{
  "paper_id": "2107.07502v2",
  "title": "Multibench: Multiscale Benchmarks For Multimodal Representation Learning",
  "published": "2021-07-15T17:54:36Z",
  "authors": [
    "Paul Pu Liang",
    "Yiwei Lyu",
    "Xiang Fan",
    "Zetian Wu",
    "Yun Cheng",
    "Jason Wu",
    "Leslie Chen",
    "Peter Wu",
    "Michelle A. Lee",
    "Yuke Zhu",
    "Ruslan Salakhutdinov",
    "Louis-Philippe Morency"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MULTIBENCH, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MULTIBENCH provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MULTIBENCH offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MULTIBENCH introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning spanning innovations in fusion paradigms, optimization objectives, and training approaches. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9 15 datasets. Therefore, MULTIBENCH presents a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MULTIBENCH, our standardized implementations, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Our perception of the natural world surrounding us involves multiple sensory modalities: we see objects, hear audio signals, feel textures, smell fragrances, and taste flavors. A modality refers to a way in which a signal exists or is experienced. Multiple modalities then refer to a combination of multiple signals each expressed in heterogeneous manners  [10] . Many real-world research problems are inherently multimodal: from the early research on audio-visual speech recognition  [48]  to the recent explosion of interest in language, vision, and video understanding  [48]  for applications such as multimedia  [102, 116] , affective computing  [101, 127] , robotics  [84, 91] , finance  [70] , dialogue  [126] , human-computer interaction  [47, 117] , and healthcare  [51, 172] . The research field of multimodal machine learning (ML) brings unique challenges for both computational and theoretical research given the heterogeneity of various data sources  [10] . At its core lies the learning of multimodal representations that capture correspondences between modalities for prediction, and has emerged as a vibrant interdisciplinary field of immense importance and with extraordinary potential. Limitations of current multimodal datasets: Current multimodal research has led to impressive advances in benchmarking and modeling for specific domains such as language and vision  [4, 103, 105, 132] . However, other domains, modalities, and tasks are relatively understudied. Many of these tasks are crucial for real-world intelligence such as improving accessibility to technology for diverse populations  [62] , accelerating healthcare diagnosis to aid doctors  [78] , and building reliable robots that can engage in human-AI interactions  [16, 83, 137] . Furthermore, current benchmarks typically focus on performance without quantifying the potential drawbacks involved with increased time and space complexity  [148] , and the risk of decreased robustness from imperfect modalities  [101, 123] . In real-world deployment, a balance between performance, robustness, and complexity is often required.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Multibench:",
      "text": "In order to accelerate research in building general-purpose multimodal models, our main contribution is MULTIBENCH (Figure  1 ), a systematic and unified large-scale benchmark that brings us closer to the requirements of real-world multimodal applications. MULTIBENCH is designed to comprehensively evaluate 3 main components: generalization across domains and modalities, complexity during training and inference, and robustness to noisy and missing modalities: 1. Generalization across domains and modalities: MULTIBENCH contains a diverse set of 15 datasets spanning 10 modalities and testing for 20 prediction tasks across 6 distinct research areas. These research areas include important tasks understudied from a multimodal learning perspective, such as healthcare, finance, and HCI. Building upon extensive data-collection efforts by domain experts, we worked with them to adapt datasets that reflect real-world relevance, present unique challenges to multimodal learning, and enable opportunities in algorithm design and evaluation. 2. Complexity during training and inference: MULTIBENCH also quantifies potential drawbacks involving increased time and space complexity of multimodal learning. Together, these metrics summarize the tradeoffs of current models as a step towards efficiency in real-world settings  [142] .\n\n3. Robustness to noisy and missing modalities: Different modalities often display different noise topologies, and real-world multimodal signals possibly suffer from missing or noisy data in at least one of the modalities  [10] . MULTIBENCH provides a standardized way to assess the risk of decreased robustness from imperfect modalities through a set of modality-specific and multimodal imperfections that reflect real-world noise, thereby providing a benchmark towards safe and robust deployment. Together, MULTIBENCH unifies efforts across separate research areas in multimodal learning to enable quick and accurate benchmarking across a wide range of datasets and metrics.\n\nTo help the community accurately compare performance and ensure reproducibility, MULTIBENCH includes an end-to-end pipeline including data preprocessing, dataset splits, multimodal algorithms, evaluation metrics, and cross-validation protocols. This includes an implementation of 20 core multimodal approaches spanning innovations in fusion paradigms, optimization objectives, and training approaches in a standard public toolkit called MULTIZOO. We perform a systematic evaluation and show that directly applying these methods can improve the state-of-the-art performance on 9 out of the 15 datasets. Therefore, MULTIBENCH presents a step towards unifying disjoint efforts in multimodal research and paves a way towards a deeper understanding of multimodal models.\n\nTable  1 : MULTIBENCH provides a comprehensive suite of 15 multimodal datasets to benchmark current and proposed approaches in multimodal representation learning. It covers a diverse range of research areas, dataset sizes, input modalities (in the form of : language, i: image, v: video, a: audio, t: time-series, ta: tabular, f : force sensor, p: proprioception sensor, s: set, o: optical flow), and prediction tasks. We provide a standardized data loader for datasets in MULTIBENCH, along with a set of state-of-the-art multimodal models. Most importantly, our public zoo of multimodal benchmarks and models will ensure ease of use, accessibility, and reproducibility. Finally, we outline our plans to ensure the continual availability, maintenance, and expansion of MULTIBENCH, including using it as a theme for future workshops and competitions and to support the multimodal learning courses taught around the world.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multibench: The Multiscale Multimodal Benchmark",
      "text": "Background: We define a modality as a single particular mode in which a signal is expressed or experienced. Multiple modalities then refer to a combination of multiple heterogeneous signals  [10] .\n\nThe first version of MULTIBENCH focuses on benchmarking algorithms for multimodal fusion, where the main challenge is to join information from two or more modalities to perform a prediction (e.g., classification, regression). Classic examples for multimodal fusion include audio-visual speech recognition where visual lip motion is fused with speech signals to predict spoken words  [48] .\n\nMultimodal fusion can be contrasted with multimodal translation where the goal is to generate a new and different modality  [162] , grounding and question answering where one modality is used to query information in another (e.g., visual question answering  [4] ), and unsupervised or self-supervised multimodal representation learning  [109, 143] . We plan future versions of MULTIBENCH to study these important topics in multimodal research in Appendix I. Each of the following 15 datasets in MULTIBENCH contributes a unique perspective to the various technical challenges in multimodal learning involving learning and aligning complementary information, scalability to a large number of modalities, and robustness to realistic real-world imperfections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "Table  1  shows an overview of the datasets provided in MULTIBENCH. We provide a brief overview of the modalities and tasks for each of these datasets and refer the reader to Appendix C for details.\n\nAffective computing studies the perception of human affective states (emotions, sentiment, and personalities) from our natural display of multimodal signals spanning language (spoken words), visual (facial expressions, gestures), and acoustic (prosody, speech tone)  [124] . It has broad impacts towards building emotionally intelligent computers, human behavior analysis, and AI-assisted education. MULTIBENCH contains 4 datasets involving fusing language, video, and audio time-series data to predict sentiment (CMU-MOSI  [181] ), emotions (CMU-MOSEI  [183] ), humor (UR-FUNNY  [64] ), and sarcasm (MUSTARD  [24] ). Complementary information may occurs at different moments, requiring models to address the multimodal challenges of grounding and alignment.\n\nHealthcare: Modern medical decision-making often involves integrating complementary information and signals from several sources such as lab tests, imaging reports, and patient-doctor conversations.\n\nMultimodal models can help doctors make sense of high-dimensional data and assist them in the diagnosis process  [5] . MULTIBENCH includes the large-scale MIMIC dataset  [78]  which records ICU patient data including time-series data measured every hour and other demographic variables (e.g., age, gender, ethnicity in the form of tabular numerical data). These are used to predict the disease ICD-9 code and mortality rate. MIMIC poses unique challenges in integrating time-varying and static modalities, reinforcing the need of aligning multimodal information at correct granularities. Robotics: Modern robot systems are equipped with multiple sensors to aid in their decision-making.\n\nWe include the large-scale MUJOCO PUSH  [90]  and VISION&TOUCH  [92]  datasets which record the manipulation of simulated and real robotic arms equipped with visual (RGB and depth), force, and proprioception sensors. In MUJOCO PUSH, the goal is to predict the pose of the object being pushed by the robot end-effector. In VISION&TOUCH, the goal is to predict action-conditional learning objectives that capture forward dynamics of the different modalities (contact prediction and robot end-effector pose). Robustness is important due to the risk of real-world sensor failures  [89] .\n\nFinance: We gathered historical stock data from the internet to create our own dataset for financial time-series prediction across 3 groups of correlated stocks: STOCKS-F&B, STOCKS-HEALTH, and STOCKS-TECH. Within each group, the previous stock prices of a set of stocks are used as multimodal time-series inputs to predict the price and volatility of a related stock (e.g., using Apple, Google, and Microsoft data to predict future Microsoft prices). Multimodal stock prediction  [136]  presents scalability issues due to a large number of modalities (18 63 100 vs 2 3 in most datasets), as well as robustness challenges arising from real-world data with an inherently low signal-to-noise ratio.\n\nHuman Computer Interaction (HCI) studies the design of computer technology and interactive interfaces between humans and computers  [43] . Many real-world problems involve multimodal inputs such as language, visual, and audio interfaces. We use the ENRICO (Enhanced Rico) dataset  [40, 93]  of Android app screens (consisting of an image as well as a set of apps and their locations) categorized by their design motifs and collected for data-driven design applications such as design search, user interface (UI) layout generation, UI code generation, and user interaction modeling. Multimedia: A significant body of research in multimodal learning has been fueled by the large availability of multimedia data (language, image, video, and audio) on the internet. MULTIBENCH includes 3 popular large-scale multimedia datasets with varying sizes and levels of difficulty: (1) AV-MNIST  [161]  is assembled from images of handwritten digits  [88]  and audio samples of spoken digits  [94] , (2) MM-IMDB  [8]  uses movie titles, metadata, and movie posters to perform multi-label classification of movie genres, and (3) KINETICS  [80]  contains video, audio, and optical flow of 306, 245 video clips annotated for 400 human actions. To ease experimentation, we split KINETICS into small and large partitions (see Appendix C).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "MULTIBENCH contains evaluation scripts for the following holistic desiderata in multimodal learning: Performance: We standardize evaluation using metrics designed for each dataset, including MSE and MAE for regression to accuracy, micro & macro F1-score, and AUPRC for classification.\n\nComplexity: Modern ML research unfortunately causes significant impacts to energy consumption  [142] , a phenomenon often exacerbated in processing high-dimensional multimodal data. As a step towards quantifying energy complexity and recommending lightweight multimodal models, MULTIBENCH records the amount of information taken in bits (i.e., data size), number of model parameters, as well as time and memory resources required during the entire training process. Realworld models may also need to be small and compact to run on mobile devices  [131]  so we also report inference time and memory on CPU and GPU (see Appendix D.2). Robustness: Real-world multimodal data is often imperfect as a result of missing entries, noise corruption, or missing modalities entirely, which calls for robust models that can still make accurate predictions despite only having access to noisy and missing signals  [101, 123] . To standardize efforts in evaluating robustness, MULTIBENCH includes the following tests: (1) Modality-specific imperfections are independently applied to each modality taking into account its unique noise topologies (i.e., flips and crops of images, natural misspellings in text, abbreviations in spoken audio). (2) Multimodal imperfections capture correlations in imperfections across modalities (e.g., missing modalities, or a chunk of time missing in multimodal time-series data). We use both qualitative measures (performance-imperfection curve) and quantitative metrics  [149]  that summarize  (1)  relative",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Unimodal Models Fusion Paradigms",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Optimization Objectives Training Procedures Data Preprocessing",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "It Insight Gives Me Much",
      "text": "Figure  2 : MULTIZOO provides a standardized implementation of a suite of multimodal methods in a modular fashion to enable accessibility for new researchers, compositionality of approaches, and reproducibility of results.\n\nrobustness measuring accuracy under imperfections and (2) effective robustness measuring the rate of accuracy drops after equalizing for initial accuracy on clean test data (see Appendix D.3 for details).",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Multizoo: A Zoo Of Multimodal Algorithms",
      "text": "To complement MULTIBENCH, we release a comprehensive toolkit, MULTIZOO, as starter code for multimodal algorithms which implements 20 methods spanning different methodological innovations in (1) data preprocessing, (2) fusion paradigms, (3) optimization objectives, and (4) training procedures (see Figure  2 ). To introduce these algorithms, we use the simple setting with 2 modalities for notational convenience but refer the reader to Appendix E for detailed descriptions and implementations. We use x 1 , x 2 for input modalities, z 1 , z 2 for unimodal representations, z mm for the multimodal representation, and ŷ for the predicted label.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Preprocessing",
      "text": "Temporal alignment  [26]  has been shown to help tackle the multimodal alignment problem for time-series data. This approach assumes a temporal granularity of the modalities (e.g., at the level of words for text) and aligns information from the remaining modalities to the same granularity. We call this approach WORDALIGN  [26]  for temporal data where text is one of the modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fusion Paradigms",
      "text": "Early and late fusion: Early fusion performs concatenation of input data before using a model (i.e., z mm = [x 1 , x 2 ]) while late fusion applies suitable unimodal models to each modality to obtain their feature representations, concatenates these features, and defines a classifier to the label (i.e., z mm = [z 1 , z 2 ])  [10] . MULTIZOO includes their implementations denoted as EF and LF respectively. Tensors are specifically designed to tackle the multimodal complementarity challenge by explicitly capturing higher-order interactions across modalities  [179] . Given unimodal representations z 1 , z 2 , tensors are defined as\n\nwhere ⊗ denotes an outer product. However, computing tensor products is expensive since their dimension scales exponentially with the number of modalities so several efficient approximations have been proposed  [71, 101, 106] . MULTIZOO includes Tensor Fusion (TF)  [179]  as well as the approximate Low-rank Tensor Fusion (LRTF)  [106] .\n\nMultiplicative Interactions (MI) generalize tensor products to include learnable parameters that capture multimodal interactions  [77] . In its most general form, MI defines a bilinear product z mm = z 1 Wz 2 +z ⊺ 1 U+Vz 2 +b where W, U, Z, and b are trainable parameters. By appropriately constraining the rank and structure of these parameters, MI recovers HyperNetworks  [61]  (unconstrained parameters resulting in a matrix output), Feature-wise linear modulation (FiLM)  [120, 188]  (diagonal parameters resulting in vector output), and Sigmoid units  [37]  (scalar parameters resulting in scalar output). MULTIZOO includes all 3 as MI-MATRIX, MI-VECTOR, and MI-SCALAR respectively. Multimodal gated units learn representations that dynamically change for every input  [25, 167, 171] . Its general form can be written as z mm = z 1 ⊙ h(z 2 ), where h represents a function with sigmoid activation and ⊙ denotes element-wise product. h(z 2 ) is commonly referred to as \"attention weights\" learned from z 2 to attend on z 1 . Attention is conceptually similar to MI-VECTOR but recent work has explored more expressive forms of h such as using a Query-Key-Value mechanism  [167]  or fully-connected layers  [25] . We implement the Query-Key-Value mechanism as NL GATE  [167] . Temporal attention models tackle the challenge of multimodal alignment and complementarity. Transformer models  [158]  are useful for temporal data by automatically aligning and capturing Algorithm 1 PyTorch code integrating MULTIBENCH datasets and MULTIZOO models. complementary features at different time-steps  [154, 174] . We include the Multimodal Transformer (MULT)  [154]  which applied a Crossmodal Transformer block using z 1 to attend to z 2 (and vice-versa) to obtain a multimodal representation z mm = [z 1→2 , z 2→1 ] = [CM(z 1 , z 2 ), CM(z 2 , z 1 )]. Architecture search: Instead of hand-designing architectures, several approaches define a set of atomic operations (e.g., linear transformation, activation, attention, etc.) and use architecture search to learn the best order of these operations for a given task  [122, 173] , which we call MFAS.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Optimization Objectives",
      "text": "In addition to the standard supervised losses (e.g., cross entropy for classification, MSE/MAE for regression), several proposed methods have proposed new objective functions based on: Prediction-level alignment objectives tackle the challenge of alignment by capturing a representations where semantically similar concepts from different modalities are close together  [9, 33, 91, 151] . Alignment objectives have been applied at both prediction and feature levels. In the former, we implement Canonical Correlation Analysis (CCA)  [7, 145, 166] , which maximizes correlation by adding a loss term L CCA = -corr (g 1 (z 1 ), g 2 (z 2 )) where g 1 , g 2 are auxiliary classifiers mapping each unimodal representation to the label. Feature-level alignment: In the latter, contrastive learning has emerged as a popular approach to bring similar concepts close in feature space and different concepts far away  [33, 91, 151] . We include REFNET  [135]  which uses a self-supervised contrastive loss between unimodal representations z 1 , z 2 and the multimodal representation z mm , i.e., L contrast = 1cos(z mm , g 1 (z 1 )) + 1cos(z mm , g 2 (z 2 )) where g 1 , g 2 is a layer mapping each modality's representation into the joint multimodal space. Reconstruction objectives based on generative-discriminative models (e.g., VAEs) aim to reconstruct the input (or some part of the input)  [91, 155] . These have been shown to better preserve task-relevant information learned in the representation, especially in settings with sparse supervised signals such as robotics  [91]  and long videos  [155] . We include the Multimodal Factorized Model (MFM)  [155]  that learns a representation z mm that can reconstruct input data x 1 , x 2 while also predicting the label, i.e., adding an objective L rec = g 1 (z mm )x 1 2 + g 2 (z mm )x 2 2 where g 1 , g 2 are auxiliary decoders mapping z mm to each raw input modality. MFM can be paired with any multimodal model from section 3.2 (e.g., learning z mm via tensors and adding a term to reconstruct input data). Improving robustness: These approaches modify the objective function to account for robustness to noisy  [101]  or missing  [89, 111, 123]  modalities. MULTIZOO includes MCTN  [123]  which uses cycle-consistent translation to predict the noisy/missing modality from present ones (i.e., a path\n\nWhile MCTN is trained with multimodal data, it only takes in one modality x 1 at test-time which makes it robust to the remaining modalities.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Training Procedures",
      "text": "Improving generalization: Recent work has found that directly training a multimodal model is sub-optimal since different modalities overfit and generalize at different rates. MULTIZOO includes Gradient Blending (GRADBLEND), that computes generalization statistics for each modality to determine their weights during fusion  [167] , and Regularization by Maximizing Functional Entropies (RMFE), which uses functional entropy to balance the contribution of each modality to the result  [53] .",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Putting Everything Together",
      "text": "In Algorithm 1, we show a sample code snippet in Python that loads a dataset from MULTIBENCH (section C.2), defines the unimodal and multimodal architectures, optimization objective, and training procedures (section 3), before running the evaluation protocol (section 2.2). Our MULTIZOO toolkit is easy to use and trains entire multimodal models in less than 10 lines of code. By standardizing the implementation of each module and disentangling the individual effects of models, optimizations, and training, MULTIZOO ensures both accessibility and reproducibility of its algorithms.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Discussion",
      "text": "Setup: Using MULTIBENCH, we load each of the datasets and test the multimodal approaches in MULTIZOO. We only vary the contributed method of interest and keep all other possibly confounding factors constant (i.e., using the exact same training loop when testing a new multimodal fusion paradigm), a practice unfortunately not consistent in previous work. Our code is available at https://github.com/pliang279/MultiBench. Please refer to Appendix G for experimental details. MULTIBENCH allows for careful analysis of multimodal models and we summarize the main take-away messages below (see Appendix H for full results and analysis).\n\nBenefits of standardization: From Table  2 , simply applying methods proposed outside of the same research area can improve the state-of-the-art performance on 9 of the 15 MULTIBENCH datasets, especially for relatively understudied domains and modalities (i.e., healthcare, finance, HCI). Generalization across domains and modalities: MULTIBENCH offers an opportunity to analyze algorithmic developments across a large suite of modalities, domains, and tasks. We summarize the following observations regarding performance across datasets and tasks (see details in Appendix H.7):\n\n1. Many multimodal methods show their strongest performance on in-domain datasets and do not generalize across domains and modalities. For example, MFAS  [122]  works well on domains it was designed for (AV-MNIST and MM-IMDB in multimedia) but does not generalize to other domains such as healthcare (MIMIC). Similarly, MULT  [154]  performs extremely well on the affect recognition datasets it was designed for but struggles on other multimodal time-series data in the finance and robotics domains. Finally, GRADBLEND  [167] , an approach specifically designed to improve generalization in multimodal learning and tested on video and audio datasets (e.g., Kinetics), does not perform well on other datasets. In general, we observe high variance in the performance of multimodal methods across datasets in MULTIBENCH. Therefore, there still does not exist a one-size-fits-all model, especially for understudied modalities and tasks. 2. There are methods that are surprisingly generalizable across datasets. These are typically general modality-agnostic methods such as LF. While simple, it is a strong method that balances simplicity, performance, and low complexity. However, it does not achieve the best performance on any dataset. 3. Several methods such as MFAS and CCA are designed for only 2 modalities (usually image and text), and TF and MI do not scale efficiently beyond 2 3 modalities. We encourage the community to generalize these approaches across datasets and modalities on MULTIBENCH.",
      "page_start": 7,
      "page_end": 89
    },
    {
      "section_name": "Training",
      "text": "Tradeoffs between modalities: How far can we go with unimodal methods? Surprisingly far! From Table  2 , we observe that decent performance can be obtained with the best performing modality.\n\nFurther improvement via multimodal models may come at the expense of around 2-3× the parameters.\n\nTradeoffs between performance and complexity: In Figure  3 (a), we summarize the performance of all methods in terms of performance and complexity. We find a strong tradeoff between these two desiderata: simple fusion techniques (e.g., LF) are actually appealing choices which score high on both metrics, especially when compared to complex (but slightly better performing) methods such as architecture search (MFAS) or Multimodal Transformers (MULT). While LF is the easiest to adapt to new datasets and domains, we encountered difficulties in adapting several possibly well-performing methods (such as MFAS or MULT) to new datasets and domains. Therefore, while their average performance is only slightly better than LF on all datasets (see Figure  3 (a)), they perform much better on well-studied datasets (see Figure  3 (b)). We hope that the release of MULTIBENCH will greatly accelerate research in adapting complex methods on new datasets (see full results in Appendix H.8).\n\nTradeoffs between performance and robustness: In Figure  4 , we plot a similar tradeoff plot between accuracy and (relative & effective) robustness. As a reminder, relative robustness directly measures accuracy under imperfections while effective robustness measures the rate at which accuracy drops after equalizing for initial accuracy on clean test data (see Appendix D.3 for details). We observe a positive correlation between performance and relative robustness (see Figure  4 (a)), implying that models starting off with higher accuracy tend to stay above other models on the performanceimperfection curve. However, we observe a negative best fit between performance and effective robustness (see Figure  4 (b)) because several well-performing methods such as MULT, CCA, and MVAE tend to drop off faster after equalizing for initial accuracy on clean test data. Furthermore, very few models currently achieve both positive relative and effective robustness, which is a crucial area for future multimodal research (see full results in Appendix H.9).",
      "page_start": 89,
      "page_end": 90
    },
    {
      "section_name": "Related Work",
      "text": "We review related work on standardizing datasets and methods in multimodal learning.\n\nComparisons with related benchmarks: To the best of our knowledge, MULTIBENCH is the first multimodal benchmark with such a large number of datasets, modalities, and tasks. Most previous multimodal benchmarks have focused on a single research area such as within affective computing  [56] , human multimodal language  [177] , language and vision-based question answering  [50, 138] , text classification with external multimodal information  [60] , and multimodal learning for educa-    4 : Tradeoff between performance and robustness. Size of circles shows variance in robustness across datasets. We show the line of best linear fit in dotted blue. While better performing methods show better relative robustness (a), some suffer in effective robustness since performance drops off faster (b). Few models currently achieve both relative and effective robustness, which suggests directions for future research.\n\ntion  [65] . MULTIBENCH is specifically designed to go beyond the commonly studied language, vision, and audio modalities to encourage the research community to explore relatively understudied modalities (e.g., tabular data, time-series, sensors, graph and set data) and build general multimodal methods that can handle a diverse set of modalities. Our work is also inspired by recent progress in better evaluation benchmarks for a suite of important tasks in ML such as language representation learning  [163, 164] , long-range sequence modeling  [150] , multilingual representation learning  [72] , graph representation learning  [74] , and robustness to distribution shift  [85] . These well-crafted benchmarks have accelerated progress in new algorithms, evaluation, and analysis in their respective research areas. Standardizing multimodal learning: There have also been several attempts to build a single model that works well on a suite of multimodal tasks  [95, 109, 143] . However, these are limited to the language and vision space, and multimodal training is highly tailored for text and images. Transformer architectures have emerged as a popular choice due to their suitability for both language and image data  [27, 73]  and a recent public toolkit was released for incorporating multimodal data on top of text-based Transformers for prediction tasks  [60] . By going beyond Transformers and text data, MULTIBENCH opens the door to important research questions involving a much more diverse set of modalities and tasks while holistically evaluating performance, complexity, and robustness. Analysis of multimodal representations: Recent work has begun to carefully analyze and challenge long-standing assumptions in multimodal learning. They have shown that certain models do not actually learn cross-modal interactions but rather rely on ensembles of unimodal statistics  [68]  and that certain datasets and models are biased to the most dominant modality  [22, 59] , sometimes ignoring others completely  [3] . These observations are currently only conducted on specific datasets and models without testing their generalization to others, a shortcoming we hope to solve using MULTIBENCH which enables scalable analysis over modalities, tasks, and models.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "Limitations: While MULTIBENCH can help to accelerate research in multimodal ML, we are aware of the following possible limitations (see detailed future directions in Appendix I): 1. Tradeoffs between generality and specificity: While it is desirable to build models that work across modalities and tasks, there is undoubtedly merit in building modality and task-specific models that can often utilize domain knowledge to improve performance and interpretability (e.g., see neurosymbolic VQA  [159] , or syntax models for the language modality  [31] ). MULTIBENCH is not at odds with research in this direction: in fact, by easing access to data, models, and evaluation, we hope that MULTIBENCH will challenge researchers to design interpretable models leveraging domain knowledge for many multimodal tasks. It remains an open question to define \"interpretability\" for other modalities beyond image and text, a question we hope MULTIBENCH will drive research in.\n\n2. Scale of datasets, models, and metrics: We plan for MULTIBENCH to be a continuously-growing community effort with regular maintenance and expansion. While MULTIBENCH currently does not include several important research areas outside of multimodal fusion (e.g., question answering  [4, 63] , retrieval  [187] , grounding  [32] , and reinforcement learning  [110] ), and is also limited by the models and metrics it supports, we outline our plan to expand in these directions in Appendix I.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Projected Expansions Of Multibench:",
      "text": "In this subsection, we describe concrete ongoing and future work towards expanding MULTIBENCH (see details in Appendix I). 1. Other multimodal research problems: We are genuinely committed to building a community around these resources and continue improving it over time. While we chose to focus on multimodal fusion by design for this first version to have a more coherent way to standardize and evaluate methods across datasets, we acknowledge the breadth of multimodal learning and are looking forward to expanding it in other directions in collaboration with domain experts. We have already included 2 datasets in captioning (and more generally for non-language outputs, retrieval): (1) Yummly-28K of paired videos and text descriptions of food recipes  [114] , and (2) Clotho dataset for audio-captioning  [45]  as well as a language-guided RL environment Read to Fight Monsters (RTFM)  [188]  and are also working towards more datasets in QA, retrieval, and multimodal RL. To help in scalable expansion, we plan for an open call to the community for suggestions and feedback about domains, datasets, and metrics. As a step in this direction, we have concrete plans to use MULTIBENCH as a theme for future workshops and competitions (building on top of the multimodal workshops we have been organizing at NAACL 2021, ACL 2020, and ACL 2019, and in multimodal learning courses (starting with the course taught annually at CMU). Since MULTIBENCH is public and will be regularly maintained, the existing benchmark, code, evaluation, and experimental protocols can greatly accelerate any dataset and modeling innovations added in the future. In our public GitHub, we have included a section on contributing through task proposals or additions of datasets and algorithms. The authors will regularly monitor new proposals through this channel. 2. New evaluation metrics: We also plan to include evaluation for distribution shift, uncertainty estimation, tests for fairness and social biases, as well as labels/metrics for interpretable multimodal learning. In the latter, we plan to include the EMAP score  [68]  as an interpretability metric assessing whether cross-modal interactions improve performance. 3. Multimodal transfer learning and co-learning: Can training data in one dataset help learning on other datasets? MULTIBENCH enables easy experimentation of such research questions: our initial experiments on transfer learning found that pre-training on larger datasets in the same domain can improve performance on smaller datasets when fine-tuned on a smaller dataset: performance on the smaller CMU-MOSI dataset improved from 75.2 to 75.8 using the same late fusion model with transfer learning from the larger UR-FUNNY and CMU-MOSEI datasets. Furthermore, recent work has shown that multimodal training can help improve unimodal performance as well  [140, 170, 180] . While previous experiments were on a small scale and limited to a single domain, we plan to expand significantly on this phenomenon (multimodal co-learning) in future versions of MULTIBENCH. 4. Multitask learning across modalities: Multitask learning across multimodal tasks with a shared set of input modalities is a promising direction that can enable statistical strength sharing across datasets and efficiency in training a single model. Using MULTIBENCH, we also ran an extra experiment on multi-dataset multitask learning. We used the 4 datasets in the affective computing domain and trained a single model across all 4 of them with adjustable input embedding layers if the input features were different and separate classification heads for each dataset's task. We found promising initial results with performance on the largest CMU-MOSEI dataset improving from 79.2 to 80.9 for a late fusion model and from 82.1 to 82.9 using a multimodal transformer model, although performance on the smaller CMU-MOSI dataset decreased from 75.2 to 70.8. We believe that these potential future studies in co-learning, transfer learning, and multi-task learning are strengths of MULTIBENCH since it shows the potential of interesting experiments and usage.\n\nIn conclusion, we present MULTIBENCH, a large-scale benchmark unifying previously disjoint efforts in multimodal research with a focus on ease of use, accessibility, and reproducibility, thereby paving the way towards a deeper understanding of multimodal models. Through its unprecedented range of research areas, datasets, modalities, tasks, and evaluation metrics, MULTIBENCH highlights several future directions in building more generalizable, lightweight, and robust multimodal models.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D Multibench Evaluation Protocol",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Broader Impact Statement",
      "text": "Multimodal data and models are ubiquitous in a range of real-world applications. MULTIBENCH and MULTIZOO is our aim to systematically categorize the plethora of datasets and models currently in use. While these contributions will accelerate research towards multimodal datasets and models as well as their real-world deployment, we believe that special care must be taken in the following regard to ensure that these models are safely deployed for real-world benefit:\n\nTime & space complexity: Modern multimodal datasets and models are large, especially when building on already large pretrained unimodal datasets and models such as BERT or ResNets. The increasing time and space complexity of these models can cause financial impacts resulting from the cost of hardware, electricity, and computation, as well as environmental impacts resulting from the carbon footprint required to fuel modern hardware. Therefore, there has been much recent interest in building lightweight machine learning models  [142] .\n\nMULTIBENCH also provides several efforts in this direction:\n\n1. Firstly, MULTIBENCH alleviates the need for separate research groups to repeat preprocessing efforts when beginning to work on a new multimodal dataset, which often takes significant time when large video & audio datasets and feature extractors are involved. 2. Secondly, our standardized implementation of core approaches in MULTIZOO prevents duplicate efforts in adapting approaches to new datasets. We found that while many authors of these multimodal methods released their code publicly on GitHub, there was still some effort needed to adapt their code and tune their models to achieve the best performance on our standardized implementation in MULTIZOO. By standardizing these experimentation efforts, we can facilitate the sharing of code and trained models, ensure reproducibility across implementations, and save time and effort in the future. 3. Finally, MULTIBENCH explicitly tests for complexity and encourages researchers to build lightweight models. While this has been less studied in multimodal research, we hope that our efforts will pave the way for greener multimodal learning.\n\nPrivacy and security: There may be privacy risks associated with making predictions from multimodal data of recorded human behaviors. The datasets potentially in question might include those in affective computing (recorded video data labeled for sentiment, emotions, and personality attributes), and healthcare (health data labeled for disease and mortality rate). Therefore, it is crucial to obtain user consent before collecting device data. In our experiments with real-world data where people are involved (i.e., healthcare and affective computing), the creators of these datasets have taken the appropriate steps to only access public data which participants/content creators have consented for released to the public (see details in Appendix C.2). We only use these datasets for research purposes. All data was anonymized and stripped of all personal (e.g., personally identifiable information) and protected attributes (e.g., race, gender).\n\nTo deploy these algorithms at scale in the real world, it is also important to keep data and features private on each device without sending it to other locations using techniques such as federated learning  [96, 100] , differential privacy  [55] , or encryption  [35] .",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Social Biases:",
      "text": "We acknowledge that there is a risk of exposure bias due to imbalanced datasets, especially when human-centric data and possibly sensitive labels are involved. For example, will models trained on imbalanced data disproportionately classify videos of a particular gender as displaying a particular emotion? Models trained on biased data have been shown to amplify the underlying social biases especially when they correlate with the prediction targets  [108] . This leaves room for future work in exploring methods tailored for specific scenarios such as mitigating social biases in words  [18] , sentences  [99] , images  [118] , and other modalities. Future research in multimodal models should also focus on quantifying the trade-offs between fairness and performance  [186] . MULTIBENCH enables the large-scale study of these crucial research questions and we outline some of our ongoing and future efforts in expanding the evaluation metrics in MULTIBENCH to take these into account in Appendix I.\n\nPossible biases within each dataset: In this section, we expand upon the previous two points regarding privacy and social biases by describing the possible biases in each domain/dataset included in MULTIBENCH.\n\n1. Affective computing: Analysis of sentiment, emotions, and personality might carry biases if care is not taken to appropriately anonymize the video data used. In MULTIBENCH, all models trained on affect recognition datasets use only pre-extracted non-invertible features that rely on general visual or audio features such as the presence of a smile or magnitude of voice. Therefore the features used in this paper cannot be used to identify the speaker  [181, 183] . Furthermore, videos within the datasets all follow the creative commons license and follow fair use guidelines of YouTube. This license allows is the standard way for content creators to grant someone else permission to use and redistribute their work. We use no information regarding gender, ethnicity, identity, or video identifier in online sources. We emphasize that the models trained to perform automated affect recognition should not in any way be used to harm individuals and should only be used as a scientific study.\n\nIn addition to privacy issues, we also studied the videos collected in these affective computing datasets and found no offensive content. While there are clearly expressions of highly negative sentiment or strong displays of anger and disgust, there are no offensive words used or personal attacks recorded in the video. All videos are related to movie or product reviews, TED talks, and TV shows. 2. Healthcare: The MIMIC dataset  [78]  has been rigorously de-identified in accordance with Health Insurance Portability and Accountability Act (HIPAA) such that all possible personal information has been removed from the dataset. Removed personal information include patient name, telephone number, address, and dates. Dates of birth for patients aged over 89 were shifted to obscure their true age. Please refer to Appendix C.2.2 for de-identification details. Again, we emphasize that any multimodal models trained to perform prediction should only be used for scientific study and should not in any way be used for real-world prediction. 3. Finance: There is no personal/human data included and there is no risk of personally identifiable information and offensive content. 4. Robotics: There is no personal/human data included and there is no risk of personally identifiable information and offensive content. 5. HCI: There is no personal/human data included and there is no risk of personally identifiable information and offensive content. 6. Multimedia: For MM-IMDb and AV-MNIST, there is no personal/human data included and there is no risk of personally identifiable information and offensive content. For Kinetics, all videos within the dataset are obtained from public YouTube videos that follow the creative commons license which allows content creators to grant permission to use and redistribute their work. We use no information regarding gender, ethnicity, identity, or video identifier in online sources. We emphasize that the models trained to perform action recognition should not in any way be used to harm individuals and should only be used as a scientific study. We also checked to make sure that these videos do not contain offensive content. All videos are related to human actions and do not contain any offensive words/audio.\n\nOverall, MULTIBENCH offers opportunities to study these potential issues at scale across modalities, tasks, datasets, and domains. We plan to continue expanding this benchmark to rigorously test for these social impacts to improve the safety and reliability of multimodal models. For example, in Appendix I.3.3, we describe some concrete extensions to include evaluations for fairness and privacy of multimodal models trained on the datasets in MULTIBENCH. Our holistic evaluation metrics will also encourage the research community to quantify the tradeoffs between performance, complexity, robustness, fairness, and privacy in human-centric multimodal models.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "B Background: Multimodal Representation Learning",
      "text": "We first provide background focusing on multimodal representation learning and several core technical challenges in this area.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "B.1 Problem Statement",
      "text": "We define a modality as a single particular mode in which a signal is expressed or experienced. Multiple modalities then refer to a combination of multiple signals each expressed or experienced in heterogeneous manners  [10] . We distinguish between the possible temporal resolution of modalities that will impact the types of approaches used: 1. Static modalities include inputs without a time dimension such as images, tabular data (i.e., a table of numerical data). 2. Temporal modalities include those coming in a sequence with a time-dimension such as language (a sequence of tokens), video (a sequence of frames/audio features/optical flow features), or time-series data (a sequence of data points indexed by time).\n\nThe first version of MULTIBENCH focuses on benchmarks and algorithms for multimodal fusion, where the main challenge is to join information from two or more modalities to perform a prediction. Classic examples include audio-visual speech recognition where visual lip motion is fused with speech signals to predict spoken words  [48] . Note that in fusion problems, it should be well-defined to predict the label with a single modality only, which marks an important distinction to tasks in question answering and grounding where one modality is used to query information in another (e.g., visual question answering  [4]  using a text question to query information in the image). We outline our plans to extend future versions of MULTIBENCH to include more multimodal challenges such as question answering, retrieval, and grounding in Appendix I.\n\nFormally, the multimodal fusion problem is defined as follows. We suppose there is a set of M modalities drawn from a joint distribution p(X 1 , ..., X M , Y ) where X m is a random variable denoting data distributed according to modality m and Y is a random variable representing the label. If modality m is a static modality, X m is a random vector without the time dimension. If modality m is a temporal modality, X m is a random vector with a time dimension which can be represented as follows: X m = (X 1 m , X 2 m , ..., X T m ) where T is the number of time-steps in the temporal modality. In multimodal fusion, a set of M modalities is drawn from a joint distribution p(X 1 , ..., X M , Y ) where X m is a random variable denoting data distributed according to modality m and Y is a random variable representing the label. A multimodal dataset is a collection of draws of (data, label) pairs from the joint distribution p(X 1 , ..., X M , Y ). We denote a dataset as {(x i1 , ..., x iM , y i )} n i=1 . These draws from the true distribution are possibly biased (e.g., across individuals, topics, or labels) and noisy (e.g., due to noisy or missing modalities). A multimodal model is a set of functions {f m ∶ m ∈ [M ]} ∪ f mm where each of the f m 's are unimodal encoders, one for each modality, and f mm is a multimodal fusion network. The unimodal encoders are specially designed with domain knowledge to learn representations from each modality (e.g., convolutional networks for images, temporal models for time-series data) resulting in unimodal representations z 1 , ..., z M . The multimodal network is designed to capture information across unimodal representations and summarize it in a multimodal representation z mm that can be used to predict the label y. The goal of multimodal fusion is to learn a model with the lowest prediction error as measured on a held-out test set, while also balancing other potential objectives such as low complexity and robustness to imperfect data.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "B.2 Technical Challenges",
      "text": "MULTIBENCH tests for the following holistic desiderata in multimodal fusion:\n\n1. Performance: We summarize the following core challenges across all prediction tasks for multimodal learning with reference to Baltrusaitis et al.,  [10] . Solving these challenges is essential in any multimodal prediction problem, regardless of domain and task. (a) Unimodal structure and granularity: The information coming from each modality follows a certain underlying structure and invariance, which needs to be processed by suitable unimodal encoders. While there are certain generally adopted unimodal encoders for commonly studied modalities such as images and text, there remain challenges in designing unimodal encoders with the right types of inductive biases for other less-studied modalities such as tabular and time-series data. Representations extracted from unimodal encoders should contain task-relevant information from that modality, expressed at the right granularity. (b) Multimodal complementarity: The information coming from different modalities have varying predictive power by themselves and also when complemented by each other. We refer to these as higher-order interactions: first-order interactions define a predictive signal from a single granular unit of information in one modality to the label (e.g., the presence of a smile indicating positive sentiment); second-order interactions define a predictive signal from a pair of granular units of information across two modalities to the label (e.g., the presence of an eye-roll together with a positive word indicating sarcasm); and nth-order interactions extend the above definition to n modalities. There are many possible interactions that explain the labels in a dataset, out of which only some may generalize to unseen data. It remains a challenge to discover these higherorder interactions using suitably expressive models. At the same time, the space of possible interactions is too large which requires suitable inductive biases in model design (see challenges regarding complexity in model design below). (c) Multimodal alignment: Information from different modalities often comes in different granularities. In order to learn predictive signals from higher-order interactions, there is a need to first identify the relations between granular units from two or more different modalities. This challenge requires a measure of the relationship between different modalities, which we call cross-modal alignment.\n\nWhen dealing with temporal data, it also requires capturing possible long-range dependencies across time, which we call temporal alignment. For example, it requires aligning the presence of an eye-roll together with a positive word to recognize sarcasm even when both signals happen at different times. This challenge extends cross-modal alignment to the temporal dimension. 2. Complexity: The space of possible interactions is very large which requires suitable inductive biases in model design. While more expressive models may perform better, these often come at the cost of time and space complexity during training and inference. To enable real-world deployment of multimodal models in a variety of settings  [142] , there is a need to build lightweight models with cheap training and inference. 3. Robustness: Information from different modalities often display different noise topologies, and real-world multimodal signals possibly suffer from missing or noisy data in at least one of the modalities  [10] . While most methods are trained on carefully curated and cleaned datasets, there is a need to benchmark their robustness in realistic scenarios. The core challenge here is to build models that still perform well despite the presence of unimodalspecific or multimodal imperfections.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Multibench Datasets",
      "text": "",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Multibench Data Loader",
      "text": "",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Multizoo Model",
      "text": "MultiBench evaluator",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Multibench Leaderboard",
      "text": "Figure  5 : MULTIBENCH provides a standardized machine learning pipeline across data processing, data loading, multimodal models, evaluation metrics, and a public leaderboard to encourage future research in multimodal representation learning. MULTIBENCH aims to present a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility.",
      "page_start": 28,
      "page_end": 40
    },
    {
      "section_name": "C Multibench Datasets",
      "text": "MULTIBENCH provides a standardized machine learning pipeline that starts from data loading to running multimodal models, providing evaluation metrics, and a public leaderboard to encourage future research in multimodal representation learning (see Figure  5 ).\n\nIn this section, we provide additional details on the distribution, release, and maintenance of each of the datasets in MULTIBENCH as well as the maintenance of MULTIBENCH as a whole.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "C.1 Dataset Selection",
      "text": "In this section, we discuss our choices of datasets in MULTIBENCH. We select each dataset based on its data collection method, input modalities, evaluation tasks, evaluation metric, and train/test splits that reflect real-world multimodal applications. We consulted with domain experts in each of the application areas to select datasets that satisfy the following properties:\n\n1. Realism in data collection, input modalities, preprocessing, and task: Each of the datasets in MULTIBENCH reflect a subset of real-world sensory modalities collected in the wild.\n\nRealism is important since it brings natural noise topologies in each modality and in the prediction task. It is crucial that these datasets reflect real-world data such that capturing these imperfections through machine learning models can potentially bridge the gap towards real-world deployment. 2. Diversity in research area: We chose these research areas through a survey of recent research papers in multimodal learning across conferences in machine learning and beyond (e.g., HCI, NLP, vision, robotics conferences). Furthermore, we consulted with domain experts in applying multimodal learning to their respective application areas to determine areas of large potential. Through engaging with domain experts we were able to select research areas and datasets that reflected realism in data collection, input modalities, preprocessing, and tasks which present challenges for machine learning models and potential for real-world transfer of learned algorithms. These research areas that are designed to span both humancentric and data-centric machine learning. In the former, we selected HCI, healthcare, and robotics since these are fast-growing research areas with increasingly specialized tracks in machine learning conferences dedicated to them. In the latter, financial data analysis is an area with an inherently low signal-to-noise ratio reflecting extremely noisy, imperfect, and uncertain real-world datasets which provide challenges for current multimodal models. We also included several multimedia datasets due to the large resources publicly available on the internet which results in multimodal datasets of the largest scale. 3. Diversity in modalities: We started with a set of commonly studied modalities such as language, image, video, and audio. For each of the following research areas, we consulted with domain experts to choose datasets that are established, but not overstudied. More importantly, we aimed for diversity in modalities to truly test the generalization capabilities of modern multimodal models outside of commonly studied domains and modalities. For example, while there is much work in HCI involving images and text, we chose a modality representing a set of mobile applications for coverage. Similarly, in robotics, we consulted with domain experts to obtain datasets with high-frequency force and proprioception sensors that provide a unique challenge to machine learning researchers. 4. Challenging for ML models: We aim to choose datasets where the current state-of-the-art performance via machine learning models is still far from human performance (if human performance is provided, otherwise judged by a domain expert). This is to ensure that there is room for improvement through community involvement in this research area. Affective computing studies the perception of human affective states (emotions, sentiment, and personalities) from our natural display of multimodal signals spanning language (spoken words), visual (facial expressions, gestures), and acoustic (prosody, speech tone)  [124] . MULTIBENCH contains 4 datasets in this category involving fusing language, video, and audio time-series data to predict sentiment (CMU-MOSI  [181]  and CMU-MOSEI  [183] ), emotions (CMU-MOSEI  [183] ), humor (UR-FUNNY  [64] ), and sarcasm (MUSTARD  [24] ).",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Community Expansion:",
      "text": "Finally, we would like to emphasize that we heavily encourage and actively seek out community participation in expanding MULTIBENCH to keep up with the incredible pace in multimodal machine learning research. We describe our plans for an open call for proposals of new research areas, datasets, and prediction tasks in section I.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "C.2 Dataset Details",
      "text": "We provide details for each of the research areas and datasets selected in MULTIBENCH. In each of the categories, we describe the research area, the datasets and their associated data collection process, their access restrictions and licenses, and any data preprocessing or feature extraction we used following current work in each of these domains.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "C.2.1 Affective Computing",
      "text": "1. MUSTARD is a multimodal video corpus for research in automated sarcasm discovery  [24] . The dataset is compiled from popular TV shows including Friends, The Golden Girls, The Big Bang Theory, and Sarcasmaholics Anonymous. MUSTARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context, which provides additional information on the scenario where the utterance occurs, thereby providing a further challenge in the long-range modeling of multimodal information. Sarcasm is specifically chosen as an annotation task since it requires careful modeling of complementary information, particularly when the semantic information from each modality do not agree with each other.\n\nData collection: According to Castro et al.,  [24] , they conducted web searches on YouTube using keywords such as Friends sarcasm, Chandler sarcasm, Sarcasm 101, and Sarcasm in TV shows to obtain videos with sarcastic content from three main TV shows: Friends, The Golden Girls, and Sarcasmaholics Anonymous. To obtain non-sarcastic videos, they used a subset of 400 videos from MELD, a multimodal emotion recognition dataset derived from the Friends TV series  [128] . Videos from The Big Bang Theory were also collected by segmenting episodes using laughter cues from its audience.\n\nAccess restrictions: While we do not have the license to this dataset, it is a public dataset free to download by the research community from https://github.com/soujanyaporia/MUStARD.\n\nLicenses: MIT, see https://github.com/soujanyaporia/MUStARD/blob/master/ LICENSE Dataset preprocessing: We followed these preprocessing steps for each modality as suggested in the original paper  [24] :\n\n1. Language: Textual utterances are represented using pretrained BERT representations  [42]  as well as Common Crawl pre-trained 300-dimensional GloVe word vectors  [119]  for each token. 2. Visual: Visual features are extracted for each frame using a pool5 layer of an ImageNet  [41]  pretrained ResNet-152  [66]  model. Every frame is first preprocessed by resizing, centercropping, and normalizing it. We also use the OpenFace facial behavioral analysis tool  [11]  to extract facial expression features.",
      "page_start": 29,
      "page_end": 30
    },
    {
      "section_name": "3.",
      "text": "Audio: Low-level features from the audio data stream are extracted using the speech processing library Librosa  [112] . We also extract COVAREP  [39]  features as is commonly used for the other datasets in the affective computing domain (see below).\n\nTrain, validation, and test splits: there are 414, 138, and 138 video segments in train, valid, and test data respectively, which gives a total of 690 data points.\n\n2. CMU-MOSI is a collection of 2, 199 opinion video clips each rigorously annotated with labels for subjectivity, sentiment intensity, per-frame, and per-opinion annotated visual features, and permilliseconds annotated audio features  [181] . Sentiment intensity is annotated in the range [-3, +3] which enables fine-grained prediction of sentiment beyond the classical positive/negative split. Each video is collected from YouTube with a focus on video blogs, or vlogs which reflect the real-world distribution of speakers expressing their behaviors through monologue videos. CMU-MOSI is a realistic real-world multimodal dataset for affect recognition and is regularly used in competitions and workshops.\n\nData collection: According to Zadeh et al.,  [181] , videos were collected from YouTube with a focus on video blogs indexed by #vlog. A total of 93 videos were randomly selected. The final set of videos contained 89 distinct speakers, including 41 female and 48 male speakers. Most of the speakers were approximately between the ages of 20 and 30 from different backgrounds (e.g., Caucasian, African-American, Hispanic, Asian). All speakers expressed themselves in English and the videos originated from either the United States of America or the United Kingdom.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The authors are part of the team who collected the CMU-MOSI dataset  [181]  so we have the license and right to redistribute this dataset. CMU-MOSI was originally downloaded from https://github.com/A2Zadeh/CMU-MultimodalSDK.\n\nLicenses: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the conditions in https://raw.githubusercontent.com/A2Zadeh/ CMU-MultimodalSDK/master/LICENSE.txt\n\nTrain, validation, and test splits: Each dataset contains several videos, and each video is further split into short segments (roughly 10 -20 seconds) that are annotated. We split the data at the level of videos so that segments from the same video will not appear across train, valid, and test splits. This enables us to train user-independent models instead of having a model potentially memorizing the average affective state of a user. There are 52, 10, and 31 videos in train, valid, and test data respectively. Splitting up these videos gives a total of 1, 284, 229, and 686 segments respectively for a total of 2, 199 data points.\n\nDataset preprocessing: We follow current work  [103, 183]  and apply the following preliminary feature extraction for the CMU-MOSI dataset:\n\n1. Language: Glove word embeddings  [119]  were used to embed a sequence of individual words from video segment transcripts into a sequence of word vectors that represent spoken text. The Glove word embeddings used are 300-dimensional word embedding trained on 840 billion tokens from the common crawl dataset, resulting in a sequence of dimension T × 300 after alignment. The timing of word utterances is extracted using P2FA forced aligner  [176] . This extraction enables alignment between text, audio, and video. 2. Visual: We use the library Facet  [75]  to extract a set of visual features including facial action units, facial landmarks, head pose, gaze tracking, and HOG features. These visual features are extracted from the full video segment at 30Hz to form a sequence of facial gesture changes throughout time, resulting in a sequence of dimension T × 35. In addition to Facet, OpenFace facial behavioral analysis tool  [11]  is used to extract the facial expression features which include facial Action Units (AU) based on the Facial Action Coding System (FACS)  [49] . 3. Audio: The software COVAREP  [39]  is used to extract acoustic features including 12 Melfrequency cepstral coefficients, pitch tracking and voiced/unvoiced segment features  [46] , glottal source parameters  [28] , peak slope parameters and maxima dispersion quotients  [79] . These visual features are extracted from the full audio clip of each segment at 100Hz to form a sequence that represents variations in tone of voice over an audio segment, resulting in a sequence of dimension T × 74.\n\n3. UR-FUNNY is the first large-scale multimodal dataset of humor detection in human speech  [64] . UR-FUNNY is a realistic representation of multimodal language (including text, visual and acoustic modalities). This dataset opens the door to understanding and modeling humor in a multimodal framework, which is crucial since humor is an inherently multimodal communicative tool involving the effective use of words (text), accompanying gestures (visual), and prosodic cues (acoustic). UR-FUNNY consists of more than 16, 000 video samples from TED talks which are among the most diverse idea-sharing channels covering speakers from various backgrounds, ethnic groups, and cultures discussing a variety of topics from discoveries in science and arts to motivational speeches and everyday events. The diversity of speakers, topics, and unique annotation targets make it a realistic dataset for multimodal language modeling.\n\nData collection: According to Hasan et al.,  [64]  1, 866 videos and their transcripts in English were collected from the TED portal, chosen from 1, 741 different speakers and across 417 topics. The laughter markup is used to filter out 8257 humorous punchlines from the transcripts. The context is extracted from the prior sentences to the punchline (until the previous humor instances or the beginning of the video is reached). Using a similar approach, 8, 257 negative samples are chosen at random intervals where the last sentence is not immediately followed by a laughter marker. After this negative sampling, there is a homogeneous 50% split in the dataset between positive and negative humor examples.\n\nAccess restrictions: This is a public dataset free to download by the research community from https://github.com/ROC-HCI/UR-FUNNY. The authors of the dataset also note that videos on www.ted.com are publicly available for download  [64] .\n\nLicenses: No license was provided with this dataset.\n\nDataset preprocessing: We follow current work  [103, 183]  and apply the same preliminary feature extraction as the CMU-MOSI dataset described above.\n\nTrain, validation, and test splits: Each dataset contains several videos, and each video is further split into short segments (roughly 10 -20 seconds) that are annotated. We split the data at the level of videos so that segments from the same video will not appear across train, valid, and test splits. This enables us to train user-independent models instead of having a model potentially memorizing the average affective state of a user. There are 1, 166, 300, and 400 videos in train, valid, and test data respectively. Splitting up these videos gives a total of 10, 598, 2, 626, and 3, 290 segments respectively for a total of 16, 514 data points, 4. CMU-MOSEI is the largest dataset of sentence-level sentiment analysis and emotion recognition in real-world online videos  [102, 183] . CMU-MOSEI contains more than 65 hours of annotated video from more than 1, 000 speakers and 250 topics. Each video is annotated for sentiment as well as the presence of 9 discrete emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed, and neutral) as well as continuous emotions (valence, arousal, and dominance). The diversity of prediction tasks makes CMU-MOSEI a valuable dataset to test multimodal models across a range of real-world affective computing tasks. The dataset has been continuously used in workshops and competitions revolving around human multimodal language.\n\nData collection: According to Zadeh et al.,  [183] , videos from YouTube are automatically analyzed for the presence of one speaker in the frame using face detection to ensure the video is a monologue and rejecting videos that have moving cameras. A diverse set of 250 frequently used topics in online videos is used as the seed for acquisition. The authors restrict the number of videos acquired from each channel to a maximum of 10 and limit the videos to have manual and properly punctuated transcriptions. After manual quality inspection, they also performed automatic checks on the quality of video and transcript using facial feature extraction confidence and forced alignment confidence, before balancing the gender in the dataset using the data provided by annotators (57% male to 43% female).",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Access Restrictions:",
      "text": "The authors are part of the team who collected the CMU-MOSEI dataset  [183]  so we have the license and right to redistribute this dataset. CMU-MOSEI was originally downloaded from https://github.com/A2Zadeh/CMU-MultimodalSDK.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Mortality Icd9 Codes",
      "text": "Figure  7 : Healthcare: Medical decision-making often involves integrating complementary signals from several sources such as lab tests, imaging reports, and patient-doctor conversations. Multimodal models can help doctors make sense of high-dimensional data and assist them in the diagnosis process  [5] . MULTIBENCH includes the MIMIC dataset  [78]  which records ICU patient data including time-series data measured every hour and other tabular numerical data about the patient (e.g., age, gender, ethnicity) to predict mortality rate and the disease ICD-9-code. Figure adapted from  [165] .\n\nLicenses: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the\"\"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the conditions in https://raw.githubusercontent.com/A2Zadeh/ CMU-MultimodalSDK/master/LICENSE.txt Dataset preprocessing: We follow current work  [103, 183]  and apply the same preliminary feature extraction as the CMU-MOSI and UR-FUNNY datasets described above.\n\nTrain, validation, and test splits: Each dataset contains several videos, and each video is further split into short segments (roughly 10 -20 seconds) that are annotated. We split the data at the level of videos so that segments from the same video will not appear across train, valid, and test splits. This enables us to train user-independent models instead of having a model potentially memorizing the average affective state of a user. There are a total of 16, 265, 1, 869, and 4, 643 segments in train, valid, and test datasets respectively for a total of 22, 777 data points.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "C.2.2 Healthcare",
      "text": "1. MIMIC-III (Medical Information Mart for Intensive Care III)  [78]  is a large, freely-available database comprising de-identified health-related data associated with over 40, 000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. Following  [129] , we organized numerous patient data into two major modalities (using the 17 features in feature set A in  [129] ): time series modality, which is a set of medical measurements of the patient taken every 1 hour in a period of 24 hours where each measurement is a vector of size 12 (12 different measured numerical values); static modality, which is a set of medical information about the patient, represented in a vector of size 5. We use these modalities for 3 tasks: mortality prediction (6-class prediction on whether the patient dies in 1 day, 2 day, 3 day, 1 week, 1 year, or longer than 1 year), and 2 ICD-9 code predictions (binary classification on whether the patient fits any ICD-9 code in group 1 (140 -239) and binary classification on whether the patient fits any ICD-9 code in group 7 460 -519).\n\nData collection: According to Johnson et al.,  [78] , MIMIC contains data associated with 53, 423 distinct hospital admissions for adult patients (aged 16 years or above) admitted to critical care units between 2001 and 2012, as well as 7, 870 neonates admitted between 2001 and 2008. The data covers 38, 597 distinct adult patients and 49, 785 hospital admissions. Data was also downloaded from several sources, including archives from critical care information systems, hospital electronic health record databases, and Social Security Administration Death Master File.\n\nPrivacy: Before data was incorporated into the MIMIC-III database, it was first de-identified in accordance with Health Insurance Portability and Accountability Act (HIPAA) standards using structured data cleansing and date shifting. The de-identification process removed all eighteen identifying data elements listed in HIPAA, such as patient name, date of birth (for patients over 89 of age), telephone number, address, and dates. Protected health information was also removed from text fields, such as diagnostic reports and physician notes. We refer the reader to  [129]  for full de-identification details.\n\nAccess restrictions: We do not have the license and right to redistribute this dataset. Accessing MIMIC requires the completion of a training course and approval for access on PhysioNet (https: //physionet.org/about/database/). However, we provide our own data preprocessing scripts for MIMIC, which transform the raw data into the standardized format for multimodal data and perform standardized splitting into the train, validation, and test splits. For a new user getting started with MIMIC data, all they would need to do is to complete the training course and obtain approval of access for scientific research from PhysioNet before they can use our public code to load all extracted features from the raw dataset in a version that can directly be used for machine learning studies.\n\nLicenses: MIT, see https://github.com/mit-lcp/mimic-code/blob/main/ LICENSE Dataset preprocessing: We followed the instructions on https://mimic.physionet. org/gettingstarted/access/ to download the dataset in the form of raw tables, then generated preprocessed data following the steps described in https://github. com/USC-Melady/Benchmarking_DL_MIMICIII (which takes 1 -2 weeks running time) to get the data used for experiments. Specifically, we will use data in the file 24hrs/series/imputed-normed-ep_1_24-stdized.npz. When accessing this data from our code repo, set the imputed_path of the npz file above in the get_data.py and the script will generate the PyTorch data loader for the tasks (where we will normalize the data).\n\nTrain, validation, and test splits: We split the data into train/valid/test sets randomly (using a fixed random seed) in a 80 ∶ 10 ∶ 10 ratio (so 28, 970 train, 3, 621 valid, and 3, 621 test data points) for a total of 36, 212 data points.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "C.2.3 Robotics",
      "text": "1. MUJOCO PUSH is a planar pushing task, in which a 7-DoF Panda Franka robot is pushing a circular puck with its end-effector in simulation. We estimate the 2D position of the unknown object on a table surface, while the robot intermittently interacts with the object. Similar to VISION&TOUCH, planar pushing is a contact-rich task. However, instead of estimating robot states, this dataset is estimating the state of the object the robot is currently interacting with. While other robotics datasets have also studied planar pushing  [14, 175] , Yu et al.,  [175]  use a Vicon tracker (instead of raw RGB images) while Bauza et al.,  [14]  only collect visual and proprioceptive data.\n\nData collection: According to Lee et al.  [90] , this dataset consists of 1000 trajectories with 250 steps at 1.0 × 10 1 Hz, of a simulated Franka Panda robot arm pushing a circular puck in MuJoCo  [152] . The pushing actions are generated by a heuristic controller that tries to move the end-effector to the center of the object. The multimodal inputs are gray-scaled images (1 × 32 × 32) from an RGB camera, forces (and binary contact information) from a force/torque sensor, and the 3D position of the robot end-effector. The task is to predict the 2-D planar object pose which we measure by MSE.\n\nAccess restrictions: While we do not have the license to this dataset, it is a public dataset free to download by the research community from https://github.com/brentyi/ multimodalfilter/.\n\nLicenses: MIT, see https://github.com/brentyi/multimodalfilter/blob/ master/LICENSE. Dataset preprocessing: Training, validation, and test data are each in their own files and can be used directly after downloading. Data is normalized using mean and variance from the train set.   [90]  and VISION&TOUCH  [92]  datasets which record the manipulation of real and simulated robotic arms equipped with visual (RGB and depth), force, and proprioception sensors. In MUJOCO PUSH, the goal is to predict the pose of the object being pushed by the robot end-effector. In VISION&TOUCH, the goal is to predict action-conditional learning objectives that capture forward dynamics of the different modalities (contact prediction and robot end-effector pose). Figure adapted from  [91] .\n\nof data points for training, validation, and test are 29, 000, 290, and 8, 700 for a total of 37990 data points.\n\n2. VISION&TOUCH is a real-world robot manipulation dataset that collects visual, force, and robot proprioception data (as well as the robot actions) for a peg insertion task. The robot is a 7-DoF, torque-controlled Franka Panda robot, which has a triangle peg attached to its end-effector. Rigidly attached to the table in front of the robot is a box with a triangle hole. The robot attempts to insert the peg into the hole, a contact-rich manipulation task that has been studied for decades due to its relevance in manufacturing. Vision, force, and proprioception are feedback modalities shown to be complementary and concurrent during contact-rich manipulation  [17] .\n\nData collection: According to Lee et al.,  [92] , the data is collected by running on the robot a random policy (that takes random actions) as well as a heuristic policy (that attempts peg insertion). Four sensor modalities are available, including robot proprioception, an RGB-D camera, and a force-torque sensor. The proprioceptive input is the robot end-effector pose as well as linear and angular velocity. They are computed using forward kinematics. RGB images and depth maps are recorded from a fixed camera (Kinect v2 camera) pointed at the robot. Input images to our model are down-sampled to 128×128. The force sensor provides 6-axis feedback on the forces and moments along the x, y, z axes. The OptoForce force sensor is mounted between the last joint and the peg. The robot action data is also collected at every timestep. The robot action is the Cartesian end-effector position displacement and z-axis roll rotation of the end-effector. There are 150 trajectories collected, each with 1000 timesteps of data collected. While the dataset originally was intended for representation learning for reinforcement learning, We use 2 tasks from the VISION&TOUCH datasets: (1) predicting binary contact in the next time step and (2) predicting end-effector position measured in MSE.\n\nAccess restrictions: While we do not have the license to this dataset, it is a public dataset free to download by the research community from https://github.com/stanford-iprl-lab/ multimodal_representation/.\n\nLicenses: MIT, see https://github.com/stanford-iprl-lab/multimodal_ representation/blob/master/LICENSE.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Stock Price Volatility",
      "text": "",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "C.2.4 Finance",
      "text": "We created the following financial datasets which consist of historical stock data retrieved from publicly available online financial databases. We record the opening price of each stock from 2000-06-01 to 2021-02-28, which creates a total of 5218 time steps. Details of each dataset are described in its own section below.\n\n1. STOCKS-F&B consists of 18 selected stocks from S&P 500 stocks categorized by GICS as Restaurants or Packaged Foods & Meats. We select MCD, SBUX, HSY, and HRL for initial experiments on this dataset, record their opening prices, and preprocess the data following the preprocessing procedures below.\n\n2. STOCKS-HEALTH consists of 63 selected stocks from S&P 500 stocks categorized by GICS as Health Care. We select MRK, WST, CVS, MCK, ABT, UNH, and TFX for initial experiments on this dataset, record their opening prices, and preprocess the data following the preprocessing procedures below.\n\n3. STOCKS-TECH consists of 100 selected stocks from S&P 500 stocks categorized by GICS as Information Technology or Communication Services. We select AAPL, MSFT, AMZN, INTC, AMD,",
      "page_start": 34,
      "page_end": 35
    },
    {
      "section_name": "Design Interface",
      "text": "Design screenshot",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Ui Layout",
      "text": "Figure  10 : Human Computer Interaction (HCI) studies the design and use of computer technology with a focus on the interactive interfaces between humans and computers. We use the ENRICO (Enhanced Rico) dataset  [40, 93]  of Android app screens (consisting of an image as well as a set of apps and their locations) categorized by their design motifs and collected for data-driven design applications such as design search, user interface (UI) layout generation, UI code generation, and user interaction modeling. Figure adapted from  [40, 93] .\n\nand MSI for initial experiments on this dataset, record their opening prices, and preprocess the data following the preprocessing procedures below.\n\nAccess restrictions: The datasets were collected from Yahoo Finance, which is publicly available but does not allow redistribution of their data. We provide automated download and preprocessing scripts for this dataset.\n\nLicenses: We could not find a finance dataset with a free redistribution license that includes historical financial data. As such, we provide automated download and preprocessing scripts as part of this project, which utilizes the open-source pandas-datareader to download raw finance data. We used the open-source code at https://github.com/pydata/pandas-datareader/ blob/master/pandas_datareader/yahoo/components.py. The automated scripts we provide are licensed under an MIT License.\n\nDataset preprocessing: Data is downloaded, converted to returns, and normalized. Labels are converted to squared returns. Each time series is split in chronological order, where the test split corresponds to the latest prices. For each data point, 500 previous returns are used to predict the squared return of the next day. The first 500 time steps are not predicted since they do not have 500 previous steps. We consider each stock as a modality; unimodal datasets have the input stock identical to the target stock. To keep memory usage practical for MULT  [154]  models, we evenly separate the stocks into 3 groups and use each group as a modality when preprocessing for MULT  [154] . C.2.5 HCI 1. ENRICO (Enhanced Rico)  [93]  is a dataset of Android app screens categorized by their design motifs. ENRICO was collected to help data-driven design applications such as design search, UI layout generation, UI code generation, and user interaction modeling. ENRICO is a subset of RICO  [40] , which is a large dataset of app screens collected by the automated and semi-automated \"crawling\" of Android apps available on the Google Play Store.\n\nThe RICO and ENRICO datasets have been used as benchmarks for data-driven models of design in scaffolding the creation of mobile apps. These constitute a set of relevant examples that help designers understand best practices and trends in building human-centered interfaces. Building multimodal models on these examples will enable systems that can predict whether a UI design will achieve its targeted goals even before it is deployed to millions of people. In the long run, this will enable the large-scale creation of personalized UI designs that can automatically adapt to diverse users and contexts.\n\nThe authors of ENRICO employed two main modalities for app classification: (1) the app screenshot and (2) the view hierarchy. The app screenshot is given in the form of an image. The view hierarchy is a type of metadata associated with some UI screens that describe the spatial and structural layout of UI elements. This view hierarchy can be treated as a set since it contains an unordered collection of UI elements each containing metadata and their spatial and structural layout.\n\nData collection: The original RICO dataset was collected using a combination of manual (i.e., crowdworkers) and automated (i.e., app crawler) methods. More information about how the apps were downloaded and captured is available in the RICO paper  [40] . The ENRICO dataset is a subset of RICO that was created by first randomly sampling 10000 screens from RICO and labeling a highquality subset (1460 screens) that can be categorized into 20 design categories. More information about the collection and annotation process is available in the ENRICO paper  [93] .\n\nAccess restrictions: While we do not have the license to this dataset, it is a public dataset free to download by the research community from https://github.com/luileito/enrico.\n\nLicenses: MIT, see https://github.com/luileito/enrico/blob/master/ LICENSE Dataset preprocessing: We extract the following features from each modality:\n\n1. Image: The authors of ENRICO used a VGG-16 network (augmented with batch normalization and dropout) to encode app screenshots. To reduce overfitting on the relatively small dataset (1460 examples), we use a VGG-11 network pre-trained on ImageNet, with a frozen feature extraction network and a slimmed-down classifier network. 2. Set: We followed prior modeling approaches  [40, 93]  to represent the view hierarchy as a set of UI elements spatially rendered as a \"wireframe\" (similar to a semantic map). The wireframe was then fed into the same VGG-11 network used to encode the screenshot.\n\nAnother possibility, which we briefly explored, is to use a set encoder  [184]  to use a permutation invariant function to compute a pooled representation of the set of mobile applications. We found that the CNN-based approach resulted in better performance, as it allowed the network to be initialized from a pre-trained checkpoint, although our experiments were initial and there is still ample room for future work to explore better encoders for this set modality.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "C.2.6 Multimedia",
      "text": "1. AV-MNIST is a multimodal dataset created by paring audio of a human reading digits from the FSDD dataset  [1]  with written digits in the MNIST dataset  [88]  with a task to predict the digit into one of 10 classes (0 -9). Since existing models can already complete the digit recognition task from either modality quite well, one common practice in previous work  [161]  is to increase the difficulty by removing 75% of energy in the visual modality via PCA and adding noise from ESC-50  [125]  to the audio modality, such that models have to leverage information from both modalities to make accurate predictions. ESC-50 is a realistic dataset collected from real-world audio of various everyday objects. Therefore, AV-MNIST serves as a good starting point of a relatively simple multimodal dataset but with underlying challenges of complementarity and noisy data. In fact, the method of injecting real-world background noises into the audio modality also inspired more tests for robustness included in MULTIBENCH. AV-MNIST has served as a popular benchmark for evaluating the effectiveness of multimodal fusion models  [122, 161] . 2. MM-IMDB is the largest publicly available multimodal dataset for genre prediction on movies  [8] . MM-IMDB starts from the movies of the MovieLens 20M dataset and expands this dataset by collecting genre, poster, and plot information for each movie. The final dataset contains ratings for 25, 959 movies. MM-IMDB is a realistic real-world multimodal dataset and is a popular benchmark for multimodal learning  [8, 81, 122] .\n\nData collection: According to Arevalo et al.,  [8] , MM-IMDB dataset is built with the IMDb ids provided by the Movielens 20M dataset that contains ratings of 27, 000 movies. Using the IMDbPY 3 library, movies that do not contain their poster image were filtered out. The resulting dataset comprises 25, 959 movies along with their plot, poster, genres, and other 50 additional metadata fields such as year, language, writer, director, aspect ratio, etc. The task is to perform multilabel classification into one of 23 movie genres.\n\nAccess restrictions: While we do not have the license to this dataset, it is a public dataset free to download by the research community from http://lisi1.unal.edu.co/mmimdb/ and https://github.com/johnarevalo/gmu-mmimdb/.\n\nLicenses: MIT, see https://github.com/johnarevalo/gmu-mmimdb/blob/ master/LICENSE Dataset preprocessing: We used the same method as  [8]  to extract features from texts and images.\n\n1. Text: We used the pretrained Google Word2vec 1  to extract text features. The final vocabulary contains 41, 612, which is the intersection of Google word2vec words and the MM-IMDB plots. We converted all text to lowercase following existing work. 2. Image: All images were scaled, and cropped when required, to 160 × 256 pixels keeping the aspect ratio. A VGG-16 model  [139]  is applied as the image feature extractor. This CNN consists of 5 convolutional layers of 5, 3, 3, 3, 3 squared filters and 2 × 2 pool sizes.\n\nEach convolutional layer has 16 hidden units. The convolutional layers are connected with a MaxoutMLP on top.   [161]  is assembled from images of handwritten digits  [88]  and audio samples of spoken digits  [94] ,\n\n(2) Multimodal IMDb (MM-IMDB)  [8]  uses movie titles, metadata, and movie posters to perform multi-label classification to a set of movie genres, and (3) KINETICS  [80]  contains video and audio of 306, 245 video clips annotated for 400 human actions. To ease experimentation, we split KINETICS into small and large partitions (see Appendix C). Figure adapted from  [8, 80] .\n\nTrain, validation, and test splits: The MM-IMDb dataset is split by genre into train, valid, and test datasets containing 15552, 2608, and 7799. The split was performed so that training, valid and test sets comprise 60%, 10%, 30% samples of each genre respectively.\n\n3. KINETICS is a series of large-scale curated video clips covering a diverse range of human actions.\n\nWe use the original Kinetics-400 dataset  [80]  which contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. This is one of the largest publicly available multimodal datasets with a total of 306, 245 video clips spanning 400 human actions. Therefore, KINETICS is suitable for testing the scalability of multimodal models to extremely large datasets. Furthermore, recognizing human actions is a core challenge in a variety of applications such as human-AI interaction, robotics, and human behavior analysis.\n\nThe sheer scale of the KINETICS dataset means that even the simplest models take up to several weeks to finish training. To enable multimodal learning from video and audio while also increasing access across researchers with limited computing resources, we subsample the KINETICS dataset into small and large partitions: Data collection: We refer the reader to Kay et al.,  [80]  for a detailed description of the dataset collection process. Briefly, the authors (1) started with a list of human actions from sources spanning existing action datasets, motion capture, and crowdsourcing, (2) obtained candidate clips from YouTube and extracted temporal positions within a video, (3) performed manual labeling for human actions with Amazon's Mechanical Turk, and (4) cleaning up and de-noising the selected videos.\n\nAccess restrictions:",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "C.3 Documentation",
      "text": "We provide documentation for MULTIBENCH in the form of datasheets for datasets  [54] :\n\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nLearning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, and healthcare. Unfortunately, current research focuses primarily on a fixed set of modalities and tasks without a concrete understanding of generalization across domains and modalities, complexity during training and inference, and robustness to noisy and missing modalities. In order to standardize multimodal research and accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MULTIBENCH, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MULTIBENCH provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.\n\nTo enable holistic evaluation, MULTIBENCH summarizes both performance as well as the potential drawbacks involving increased time and space complexity and risk of decreased robustness from other modalities. To accompany MULTIBENCH, we also provide a standardized implementation of 20 core approaches in multimodal learning unifying innovations in fusion paradigms, optimization objectives, and training approaches.\n\nMULTIBENCH datasets present significant challenges of scalability to large-scale multimodal datasets and robustness to realistic imperfections, which present fruitful opportunities for future research. We hope that MULTIBENCH will present a milestone in unifying disjoint efforts in multimodal machine learning research and paves a way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MULTIBENCH, our standardized implementation, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community. (b) Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? We describe each dataset's statistics in detail in Appendix C.2. We chose datasets to span small, medium, and large sizes. The smallest dataset contains 1, 460 instances (and training a model takes roughly a few minutes on a single GPU) while the largest one contains 306, 245 instances (and training a model takes roughly 2 weeks on a single GPU). This enables accessibility for researchers with limited computational resources, while also allowing for large-scale studies of multimodal datasets and models. (c) Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nEach of the datasets is collected in different ways that we detail in Appendix C.2. To summarize, each dataset consists of samples from a larger set since it is impossible to include all videos/stock data/medical data/robotics data in the world. Each dataset is collected with the aim to be representative of the entire population. (d) What data does each instance consist of? \"Raw\" data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\nWe describe in detail the raw data and processed features for each dataset in Appendix C.2. To summarize, MULTIBENCH contains both raw modality data as well as processed data with predefined feature extractors following current work. (e) Is there a label or target associated with each instance? If so, please provide a description.\n\nWe describe in detail the labels for each dataset in Appendix C.2. To summarize, MULTI-BENCH contains 6 research areas with a total of 15 prediction tasks spanning affect recognition, robot manipulation, stock prediction, design interface, action recognition, movie genre prediction, and digit prediction. (f) Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.\n\nNo, all datasets are provided in full. For robustness tests, we do inject noise and imperfections into each dataset to simulate the performance of machine learning models on real-world imperfections (see Appendix D.3 for details). (g) Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.\n\n(n) Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.\n\nThe following datasets relate to people: i. Affective computing: These datasets do not identify any subpopulations in their modeling decisions. However, the raw data comes in the form of videos publicly available and free to download from YouTube. Sub-population and demographic information can be inferred from these raw videos. ii. MIMIC: According to the authors  [78] : \"The median age of adult patients is 65.8 years and 55.9% patients are male.\" iii. Kinetics: This dataset does not identify any subpopulations. However, the raw data comes in the form of videos publicly available and free to download from YouTube. Sub-population and demographic information can be inferred from these raw videos. (o) Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.\n\nThe following datasets relate to people: i. Affective computing: One can see the person in the raw video, but the dataset contains no personal information. We do not explicitly use information regarding gender, ethnicity, identity, or video identifier in online sources. All pre-extracted features are non easily invertible and only rely on general visual or audio features such as the presence of a smile or magnitude of voice  [181, 183] . ii. MIMIC: The MIMIC dataset has been rigorously de-identified in accordance with Health Insurance Portability and Accountability Act (HIPAA) such that all possible personal information has been removed from the dataset. Removed personal information includes patient name, telephone number, address, and dates. Dates of birth for patients aged over 89 were shifted to obscure their true age. Please refer to Appendix C.2.2 for de-identification details. Again, we emphasize that any multimodal models trained to perform prediction should only be used for scientific study and should not in any way be used for real-world prediction. iii. Kinetics: One can see the person in the raw video, but the dataset does not contain direct personal information. We do not explicitly use information regarding gender, ethnicity, identity, or video identifier in online sources. (p) Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. MULTIBENCH contains datasets with financial and healthcare data. However, all these datasets are publicly available for research purposes. Healthcare data (MIMIC) has been rigorously de-identified in accordance with the Health Insurance Portability and Accountability Act (HIPAA) such that all possible personal information (patient name, telephone number, address, and dates, date of birth) has been removed from the dataset. Please refer to Appendix C.2.2 for de-identification details. (q) Any other comments?\n\nNo. We include sampling methods for each dataset in Appendix C.2. (d) Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\n\nWe include annotation details for each dataset in Appendix C.2. (e) Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\n\nWe include timeframes for each dataset in Appendix C.2. (f) Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\n\nFrom the authors of MIMIC  [78] : \"The project was approved by the Institutional Review Boards of Beth Israel Deaconess Medical Center (Boston, MA) and the Massachusetts Institute of Technology (Cambridge, MA). Requirement for individual patient consent was waived because the project did not impact clinical care and all protected health information was de-identified.\" (g) Does the dataset relate to people? If not, you may skip the remainder of the questions in this section.\n\nYes, the healthcare, affective computing, and Kinetics (multimedia) datasets relate to people. The other datasets in MULTIBENCH do not. (h) Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? Affective computing and Kinetics datasets are collected from YouTube videos that follow the creative commons license and follow fair use guidelines of YouTube. According to the authors for the MIMIC dataset  [78] : \"Data was downloaded from several sources, including archives from critical care information systems, hospital electronic health record databases, and Social Security Administration Death Master File.\" (i) Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how the notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\n\nAffective computing and Kinetics datasets are collected from YouTube videos that follow the creative commons license and follow fair use guidelines of YouTube. This is the standard way for content creators to grant someone else permission to use and redistribute their work. According to the authors for the MIMIC dataset  [78] : \"The project was approved by the Institutional Review Boards of Beth Israel Deaconess Medical Center (Boston, MA) and the Massachusetts Institute of Technology (Cambridge, MA). Requirement for individual patient consent was waived because the project did not impact clinical care and all protected health information was de-identified.\" (j) Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\nAffective computing and Kinetics datasets are collected from YouTube videos that follow the creative commons license and follow fair use guidelines of YouTube which allows content creators to grant someone else permission to use and redistribute their work. According to the authors for the MIMIC dataset  [78] : \"Requirement for individual patient consent was waived because the project did not impact clinical care and all protected health information was de-identified.\" (k) If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). N/A. (l) Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.",
      "page_start": 40,
      "page_end": 44
    },
    {
      "section_name": "N/A. (M) Any Other Comments?",
      "text": "N/A. 4. Preprocessing/cleaning/labeling (a) Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section.\n\nYes, we followed the convention in prior research for any preprocessing done to the datasets. We explain these steps in Appendix C.2. (b) Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \"raw\" data.\n\nYes, we include the raw data in MULTIBENCH in addition to the preprocessed features.\n\nThe raw data (usually in the form of raw text, videos, audio, time series etc) are useful for users to perform their own feature extraction and also for robustness tests on raw data itself (e.g., imperfections in the raw text through spelling errors and missing words). There are certain cases where we are not allowed to distribute the raw data: for MIMIC where users must undergo training to download the raw data, and for finance datasets where Yahoo Finance is publicly available but does not allow redistribution of raw data. For both of these datasets, we provide automated download and preprocessing scripts once the raw data is downloaded through the correct procedure by each user (see details in Appendix C.2). (c) Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.\n\nYes, we provided all links and references to preprocessing steps in Appendix C.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "(D) Any Other Comments?",
      "text": "No.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Uses",
      "text": "(a) Has the dataset been used for any tasks already? If so, please provide a description. Yes, MULTIBENCH contains several datasets that have been used in the multimodal ML community. We provide links to the original repositories of each dataset and their original citations in Appendix C.2. (b) Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\n\nWe provide links to the original repositories of each dataset and their original citations in Appendix C.2. We also include references to general multimodal methods implemented in MULTIZOO in Appendix E. Many of these methods have been tested by their original authors on a small subset of datasets in MULTIBENCH. In addition to these references, the leading authors maintain a reading list on topics in multimodal ML at  [98]  which contains links to papers, datasets, code, academic courses, conferences, and workshops relevant to the multimodal ML community. (c) What (other) tasks could the dataset be used for?\n\nIn addition to building multimodal models for the prediction tasks, datasets in MULTI-BENCH can also be used for: i. Unsupervised learning across multimodal data/unsupervised pre-training of multimodal models. ii. Interpreting relationships between modalities. iii. Designing models for robustness to noisy and missing modalities. iv. Investigating alignment between modalities. v. Other multimodal tasks including but not limited to: co-learning, translation, retrieval, and grounding  [10] . (d) Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?\n\nWe are careful to outline all possible risks associated with each dataset in Appendix C.2 and also in our broader impact statement (Appendix A). We acknowledge that there could be risks regarding the privacy and security of data, as well as the real-world deployment of these methods whenever human-centric data is involved (e.g., in healthcare, affective computing, and multimedia). We discussed data demographics in the previous section and it should be taken into consideration when making claims regarding the generalization of models to new users. We also emphasize that these multimodal datasets and methods should only be used for research purposes and not for actual real-world deployment until research can sufficiently verify their safety. Finally, we are carefully working with domain experts towards better understanding biases in these multimodal datasets and models as well as their real-world safety. (e) Are there tasks for which the dataset should not be used? If so, please provide a description.\n\nYes, we emphasize that all multimodal models trained to perform prediction on these datasets should not in any way be used to harm individuals and should only be used as a scientific study. They should not be deemed safe for real-world deployment. In particular, the models used to make predictions of affective states, human actions, health indicators, and financial indicators are particularly sensitive and should not be used to inform any real-world decisions. All results must only be used as a scientific study of machine learning methods. See more details in Appendix A. (f) Any other comments?\n\nNo.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Distribution",
      "text": "(a) Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. Yes, the benchmark will be distributed to the public research community for theoreticians and practitioners to experiment on multimodal data. (b) How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nWe plan to distribute MULTIBENCH via our public GitHub: https://github. com/pliang279/MultiBench. We also include a landing website page on https://cmu-multicomp-lab.github.io/multibench/ that includes an introduction to the benchmark, links to the relevant papers on multimodal datasets and algorithms, and a public leaderboard to keep track of current progress on these multimodal tasks. (c) When will the dataset be distributed?\n\nThe dataset is currently available for use. (d) Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nWe release the benchmark and code under an MIT license: see https://github. com/pliang279/MultiBench/blob/main/LICENSE, which allows for sharing and distribution of the code for research purposes. Each of the datasets in MULTI-BENCH has their own licenses which we detail in Appendix C.2. (e) Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. Yes, MULTIBENCH brings together a collection of several existing datasets in the multimodal research that were built by their individual authors who have original licenses for these datasets. We only included the datasets with licenses that allow for redistribution (MIT or Creative Commons license) and are freely downloadable for research purposes. We detailed all dataset licenses in Appendix C.2.\n\n(f) Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\n\nWe are not aware of any such restrictions. (g) Any other comments?\n\nNo.",
      "page_start": 46,
      "page_end": 47
    },
    {
      "section_name": "Maintenance",
      "text": "(a) Who is supporting/hosting/maintaining the dataset?\n\nThe dataset is supported and hosted by the team of authors at CMU. The team will also lead the maintenance and expansion of MULTIBENCH. The team will also work with the other collaborators on the paper who are domain experts in each research area MULTIBENCH covers, such as robotics, HCI, healthcare, and finance. (b) How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\nWe provide all contact addresses at https://cmu-multicomp-lab.github. io/multibench/. (c) Is there an erratum? If so, please provide a link or other access point.\n\nAll erratum and updates to the dataset will be tracked via GitHub commit histories at https://github.com/pliang279/MultiBench. We will also provide updates via our landing page https://cmu-multicomp-lab.github. io/multibench/. (d) Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)? Yes, we plan for long-term maintenance and expansion of the dataset. All erratum and updates to the dataset will be tracked via GitHub commit histories at https:// github.com/pliang279/MultiBench. We will also provide updates via our landing page https://cmu-multicomp-lab.github.io/multibench/.\n\nPlease refer to Appendix C.5 for details. (e) If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\n\nThe individuals in question were not notified about the data collection. For YouTube videos, they are released under a creative commons license which is the standard way for content creators to grant someone else permission to use and redistribute their work. According to the authors for the MIMIC dataset  [78] : \"The project was approved by the Institutional Review Boards of Beth Israel Deaconess Medical Center (Boston, MA) and the Massachusetts Institute of Technology (Cambridge, MA). Requirement for individual patient consent was waived because the project did not impact clinical care and all protected health information was de-identified.\" (f) Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users. Yes, we will maintain a GitHub history for all updates and older versions of datasets and code in MULTIBENCH. (g) If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.\n\nYes, we will create a system where users can create pull requests on GitHub to include their datasets and models. The authors will verify that the additions are in the scope of multimodal learning and do not break the current experimental code. We will work with these authors to ensure that their data and algorithms can be included in MULTIBENCH. (h) Any other comments?\n\nNo.",
      "page_start": 47,
      "page_end": 48
    },
    {
      "section_name": "C.9 Persistence Of Multibench",
      "text": "MULTIBENCH is publicly hosted on https://github.com/pliang279/MultiBench. For larger datasets that cannot be uploaded to GitHub, we plan to upload the processed dataset to CMU Box. We are still exploring the best options for sharing large datasets. Users need to download these processed datasets, place them into a correct folder, and run the MULTIBENCH data loader and machine learning pipeline.",
      "page_start": 49,
      "page_end": 49
    },
    {
      "section_name": "D Multibench Evaluation Protocol",
      "text": "To enable holistic evaluation, MULTIBENCH offers a comprehensive evaluation methodology to assess (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities: We describe the evaluation protocol for each desiderata in detail in each of the following subsections:",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "D.1 Performance",
      "text": "MULTIBENCH provides standardized evaluation using metrics designed for each dataset, ranging from MSE and MAE for regression to accuracy, micro & macro F1-score, and AUPRC for classification on each dataset. To assess for generalization, we compute the variance of a particular model's performance across all datasets in MULTIBENCH on which it is tested. We split these results on multiple datasets into in-domain datasets and out-domain datasets. In-domain datasets refer to model performance on datasets that it was initially proposed and tested on, while out-domain datasets refer to model performance on the remaining datasets. Comparing out-domain vs in-domain performance, as well as variance in performance across datasets as a whole, allow us to summarize the generalization statistics of each multimodal model.",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "D.2 Complexity",
      "text": "Modern ML research, unfortunately, causes significant impacts to energy consumption  [142] , a phenomenon often exacerbated in processing high-dimensional multimodal data. As a step towards quantifying energy complexity and recommending lightweight multimodal models, MULTIBENCH records the amount of information taken in bits (i.e., data size), number of model parameters, and time and memory resources required during the entire training process. To enforce consistency, the training time measured for all models on each dataset is run on the same CPUs and GPUs. We report training memory by measuring peak memory usage of the python process during the entire training process using python memory_profiler toolkit (https://pypi.org/project/ memory-profiler/). When counting the number of parameters when training a model, we only count the parameters in persistent modules during training and does not count the ephemeral networks or modules created in the middle of the training process (such as the networks trained for determining weights in GRADBLEND or the fusion architectures created as part of the architecture search process in MFAS).\n\nIn addition to training time and resources, real-world models may need to be small and compact to run on mobile devices  [131] . To account for this, MULTIBENCH also records inference time and parameters. We report inference time by measuring the time it takes for the trained model to complete inference on the entire test set of the dataset. In some cases, only parts of the parameters used in training are counted towards the inference parameters (for example, the parameters in decoders of MVAE and MFM are part of training parameters but not part of inference parameters).",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "D.3 Robustness To Imperfect Data",
      "text": "Real-world multimodal data is often imperfect as a result of missing entries, noise corruption, or missing modalities entirely. For example, multimodal dialogue systems trained on acted TV shows are susceptible to poor performance when deployed in the real world where users might be less expressive in using facial gestures. This calls for robust models that can still make accurate predictions despite only having access to a (possibly noisy  [101] ) subset of signals  [123] . To standardize efforts in evaluating the robustness of multimodal models, MULTIBENCH includes the following robustness tests as part of the evaluation:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D.3.1 Modality-Specific Imperfections",
      "text": "Modality-specific imperfections are independently applied to each modality taking into account the unique noise topologies in that source of data (i.e., flips and crops of images, natural misspellings in text, abbreviations in spoken audio). We describe all the modality-specific imperfections we implement in MULTIBENCH in the following:\n\nLanguage: Imperfections in the language modality can occur at various granularities spanning the character, word, phrase, and sentence levels. With reference to  [15] , many of these imperfections occur at the raw text data level and are usually results of spelling errors on a QWERTY keyboard as well as abbreviations in written, typed, and spoken text. Given a word w of length n and a fixed probability p ∈ (0, 1), we implement the following language-specific imperfections:\n\n1. Spelling errors: note that spelling mistakes are different from intentionally changed word forms (e.g. abbreviation used in instant messaging service) since they are unintentional  [144] . We simulate typos by replacing each letter with a letter having an adjacent position on a QWERTY keyboard with probability p. 2. Short message noise: Short Message Service (SMS) data usually include intentional corruptions of words and phrases like abbreviations, phonetic substitutions, omission of characters and words, and dialectal and informal usages  [144] . We implement the following: (a) Simulate sticky keys: given a number m, choosing m letters of a word randomly to repeat with probability p. (b) Simulate quick typing: given a number m, choosing m letters of a word randomly to omit with probability p. 3. Random permutation of letters: swapping adjacent two letters is a common natural noise when typing quickly  [15] . Random permutation of the entire word or the majority of letters is a form of synthetic noise. We implement the following: (a) Swap two random adjacent letters (except for the first and the last letter) with probability p. (b) Permute the middle chunk of a word: denote the middle chunk (all letters except the first and the last letter) as w[1 ∶ n], with probability p, produce a permutation f with the first and last letter fixed, i.e. f (0\n\nImage: Given a RGB image X ∈ Z W ×H×3 where W and H are the height and width of the image, let R, G, B be the W × H matrices of three color channels. We implement the following robustness tests in the image modality:\n\n1. Noises in digital images: various noises are naturally prevalent in digital images during image acquisition, coding, transmission, and processing steps  [19] . We implement the following: (a) Gaussian/electronic noise that normalizes histogram with respect to the gray values. We add Gaussian noise as a W × H matrix with each entry following Gaussian distribution N (0, p). (b) Impulse valued/salt-and-pepper noise that has dark pixels in bright regions and bright pixels in dark regions. To add salt-and-pepper noise, for each pixel x ∈ X, we convert x = 0 (white) or x = 255 (black) into a dead pixel with uniform distribution with probability p. (c) Periodic noise such that it looks like some repeating patterns are exposed on top of the affected image. We add periodic noise by exposing the original image to periodic patterns with probability p.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Color Errors:",
      "text": "(a) Convert the image to grayscale: 0.3R + 0.59G + 0.11B with probability p. (b) Decrease the contrast with probability p. (c) Negate the color: let X' be the inverted image then ∀i ∈",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Most Of These Transformations Are Achieved With The Python Imaging Library (Pil).",
      "text": "Video: We treat video data as a time series of images. For each image in the video, we apply the image-specific robustness tests as described above. In addition, we also apply the following tests to simulate imperfections in time-series data:\n\n1. Random drop: dropping the datapoint at random time step with probability p. 2. Structured drop: given a time step t, m consecutive time steps with at least one nonzero signal are dropped with probability p.\n\nAudio: Audio is typically represented as a time-series signal. Noises are primarily caused by imperfections in the recording device, which can cause static Gaussian noise to be added to the recorded temporal waveform at random time steps, background noise to be picked up at higher magnitudes, and certain time steps (or consecutive time steps) to be dropped from the recording. We implement the following unimodal noises in the audio modality:\n\n1. Additive white Gaussian noise: given an array of length N of a sampled audio segment, we add white gaussian noise, which is an array of N with each entry following a normal distribution with mean 0 and standard deviation p.\n\nIn addition to these imperfections applied at a single time step, we also apply the following across the entire time-series signal:\n\n1. Random drop: dropping the datapoint at random time step with probability p. 2. Structured drop: given a time step t, m consecutive time steps with at least one nonzero signal are dropped with probability p.\n\nTime-series data consists of a sequence with a time-dimension (a sequence of data points indexed by time). Following Liang et al.,  [101] , we implement the following types of noise and missing values in time-series data:\n\n1. White noise added independently at every time step (noise sampled from zero-mean Gaussian with standard deviation p). 2. Random drop: dropping the datapoint at random time step with probability p. 3. Structured drop: given a time step t, m consecutive time steps across modalities are dropped with probability p.",
      "page_start": 51,
      "page_end": 52
    },
    {
      "section_name": "Optical Flow:",
      "text": "We treat optical flow in a similar manner as time-series data and implement the same robustness tests.",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "Force And Proprioception Sensors:",
      "text": "We also treat these sensors in robotics as time-series data with a key difference -we add noise/drop time steps at a higher frequency since force and proprioception sensors often record data at a higher frequency.\n\nTabular data takes the form of rows, each of which contains information about some feature (e.g., age, ). We define the following robustness tests on tabular data:\n\n1. Random drops of elements from the table with probability p.",
      "page_start": 52,
      "page_end": 52
    },
    {
      "section_name": "Random Swaps Elements In The Table With Probability P.",
      "text": "Sets are data instances where the collection of input elements satisfy permutation invariance, which is in contrast to fixed dimensional vectors that are commonplace in machine learning on images, text, and audio. The key difference between sets and tabular data is that each element in the set is often assumed to be from the same distribution (e.g., a point cloud is a set of 3D coordinates). We define the following types of noise on an input set modality:\n\n1. Random dropping of elements from the set with probability p. 2. Adding noise to elements of the set with noise sampled from zero-mean Gaussian with standard deviation p.",
      "page_start": 52,
      "page_end": 52
    },
    {
      "section_name": "D.3.2 Multimodal Imperfections",
      "text": "Multimodal imperfections capture correlations in imperfections across modalities (e.g., missing modalities  [123] , or a chunk of time missing in multimodal time-series data  [101] ). These represent settings where data collection across modalities is correlated rather than independent.\n\n1. Correlated noise: adding noise to all modalities with probability p, where noise is defined according to the aforementioned modality-specific noises. 2. Correlated drop: dropping all modalities with probability p, where dropping patterns are defined according to the aforementioned modality-specific drops.\n\n3. Temporal drop: in the case of temporal modalities recorded in parallel (e.g., video, audio, and text recorded across time; financial time-series data recorded across days), we perform correlated drops across all modalities at random time steps with probability p. 4. Structured temporal drop: we extend temporal drop such that given a time step t, we perform temporal drop on m consecutive time steps with probability p. 5. Missing modalities: dropping an entire modality with probability p.",
      "page_start": 52,
      "page_end": 53
    },
    {
      "section_name": "D.3.3 Robustness Measure",
      "text": "We train the model on clean training data and evaluate it under increasing levels of noise added only to test data. To simulate realistic noise and imperfections in test data, we follow the modality-specific and multimodal imperfections as described above. Given a multimodal dataset with M modalities, this allows us to create M + 1 partitions of imperfect test datasets: one partition of increasing noise levels for modality-specific imperfections within each modality (which gives a total of M partitions) and one partition of multimodal imperfections across all modalities. For datasets where it is not possible to create multimodal imperfections due to the lack of a shared dimension (e.g., image and text datasets typically do not share any correlated dimension, but multimodal time-series datasets share an underlying time dimension), we implement the first M modality-specific imperfections which results in M imperfect data partitions.\n\nA qualitative visualization: Given each test partition, we take a unimodal or multimodal model trained on clean data and plot model performance on the y-axis as increasing levels of noise is added to the test data, on a range of 0 (no noise) to 1 (complete noise) along the x-axis. This allows us to visually inspect the robustness of each model as increasing imperfections are added to the test data. Visually, a robust model should maintain high accuracy (or low MSE) as much as possible despite increasing levels of noise.\n\nA quantitative metric: While the visualization technique above allows one to compare the robustness of several multimodal models across the same dataset, it does not allow us to aggregate robustness performance across the broad range of datasets and tasks in MULTIBENCH. To design such a metric, we extend the quantitative robustness measures proposed in Taori et al.,  [149]  to deal with multimodal imperfections across a range of imperfection levels σ ∈ [0.0, 1.0].\n\nWe begin by reviewing the example proposed in Taori et al.,  [149] : suppose we are given two models f 1 and f 2 , where accuracy acc clean (f 1 ) = 0.8, acc noisy (f 2 ) = 0.75 (i.e., a 5% drop in accuracy from the imperfections), and acc clean (f 2 ) = 0.9, acc noisy (f 2 ) = 0.76 (a 14% drop). Model f 2 has higher accuracy on the noisy test set, but overall sees a drop of 14% from the clean to the noisy test set. In contrast, f 1 starts off with a lower accuracy but sees only a 5% drop. To capture both these desiderata (i.e., having higher accuracy at all levels and lower drops in accuracy), Taori et al.,  [149]  introduce two notions of robustness: relative and effective robustness.\n\nRelative robustness directly measures accuracy under imperfection. A model with higher relative robustness would display higher accuracy at all levels of imperfection compared to a baseline model. We measure the relative robustness of all multimodal models as compared to a baseline LF (simple late fusion with concatenation) method since that is the most basic method tested on all datasets. We compute relative robustness of a model f using the formula\n\nwhich essentially measures the area between two performance-imperfection curves as imperfection levels σ increase from 0.0 to 1.0 (we compute a discrete approximation to the integral).\n\nEffective robustness measures the rate of accuracy drops as imperfection levels increase. However, to reliably measure the rate of accuracy drops, one must remove the confounding variable brought by differences in initial accuracies on clean test data. Taori et al.,  [149]  therefore propose to measure whether a model can offer higher accuracy on the noisy test set beyond what is expected from having higher accuracy on the original test set. Taori et al., use a log-linear fit on the set of (accuracy on noisy test data, accuracy on clean test data) points across a range of models trained on ImageNet to measure the expected accuracy on noisy test data given a new model's performance on clean test data. Graphically, effective robustness then corresponds to a model's performance on noisy test data lying above the linear trendline. Similar to relative robustness, we measure the effective robustness of multimodal models relative to the accuracy trend of the LF baseline, which we denote as β LF . We compute effective robustness of a model f using the formula\n\nwhich essentially measures the area between the performance-imperfection curve of model f and a shifted performance-imperfection curve of the LF baseline (shifted to match the initial accuracy of model f at imperfection level 0.0). A model with higher effective robustness should lie above this shifted accuracy curve at all imperfection levels σ. Again, we compute a discrete approximation to the integral.\n\nOverall, a robust multimodal model should obtain both high relative and effective robustness.",
      "page_start": 53,
      "page_end": 54
    },
    {
      "section_name": "D.4 Aggregating Measures Across Datasets And Tasks",
      "text": "MULTIBENCH benefits from benchmarking multimodal models across a diverse set of datasets, modalities, and tasks. While it is useful to analyze methods on a single dataset in isolation, it is also useful to assess the generalization and failure modes of methods across multiple datasets. Therefore, we need a way to reliably summarize the above metrics (performance, complexity, and robustness) across datasets despite their being on vastly different scales (e.g., accuracy for different numbers of categories) and orders (e.g., accuracy vs RMSE). We find that min-max normalization of results per dataset into a 0 -1 scale (where min and max are appropriately reversed for RMSE/MSE metrics) before averaging across datasets gives a reliable indicator of overall performance across multiple datasets. Model EF, LF [10] TF  [179] , LRTF  [106]  MI-MATRIX, MI-VECTOR, MI-SCALAR  [77]  NL GATE  [167]  MULT  [154]  MFAS  [122]  Objective CCA  [7]  REFNET  [135]  MFM  [155]  MVAE  [168]  MCTN  [123]  Training GRADBLEND  [167]  RMFE  [53]  E MULTIZOO: A Zoo of Multimodal Algorithms\n\nIn this section, we provide more details into our choice of standardizing multimodal representation learning as well as the implementation of our standardized library. In each category, we carefully describe the algorithm, motivate its effect in tackling one of the core challenges in section B.2, and provide references to the original code that we adapted to include in MULTIZOO.",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "E.1 Selection Of Algorithms In Multizoo",
      "text": "We begin by discussing our choices of algorithms in MULTIZOO. We consulted with domain experts in each of the application areas to select methods that satisfy the following properties:\n\n1. Diversity in areas: We chose algorithms that present novel perspectives across a suite of machine learning research domains spanning data preprocessing, fusion paradigms, optimization objectives, and training procedures. 2. Coverage of technical challenges: Each of the algorithms selected in MULTIZOO are chosen because they provide unique perspectives to the technical challenges in multimodal learning as elucidated in Appendix B.2. In Table  3 , we provided a coarse attempt in categorizing each of the technical challenges in multimodal learning. As a result, we did not include too many methods in any category (e.g., multiple methods that are based on model architectures that tackle similar challenges of learning complementary information). Even within the same category and within those tackling the same technical challenge, we attempted to select ones that were fundamentally different (e.g., architectures based on domain knowledge, general-purpose Transformers, and architecture search). 3. SOTA on a particular dataset: For each dataset chosen in MULTIBENCH, we aim to include the model that currently achieves state-of-the-art performance on that dataset. This allows us to assess the best performing model within the same domain of the dataset, as well as the best performing model outside the domain of the dataset. 4. Community expansion: Any set of initial methods that we will choose will represent only a small sample of the powerful multimodal methods out there. We will encourage community participation in expanding the methods in MULTIZOO and encourage researchers to implement new methods using a similar modular structure to reduce confounding factors, enable standardized sharing of code, and ensure reproducibility in results.",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "E.2 Data Preprocessing",
      "text": "Temporal alignment: As a preprocessing step, performing temporal alignment  [26]  has been shown to help tackle the multimodal alignment problem in the case of time-series data. This approach makes an implicit assumption on the temporal granularity of the modalities (e.g., at the level of words for text) and aligns information from the remaining modalities to the same temporal granularity. We call this approach WORDALIGN  [26]  and apply it to temporal data with text being one of the modalities. We use the temporal alignment provided in https: //github.com/A2Zadeh/CMU-MultimodalSDK. Specifically, it performs alignment at the granularity of words. Given a sentence with words w 1 , ..., w T each annotated with their start and end times (s 1 , e 1 ), (s 2 , e 2 ), ..., (s T , e T ), word-level alignment takes the non-text modality features (which are typically extracted at a higher frequency) and averages them during the intervals e 1s 1 , e 2s 2 , ..., e Ts T . This results in a text sequence of T words alongside aligned non-text modality sequences of T time-steps as well.",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "E.3 Fusion Paradigms",
      "text": "Early and late fusion have been the de-facto first-approach when tackling new multimodal problems.\n\nEarly fusion performs concatenation at the input data level before using a suitable prediction model (i.e., z mm = [x 1 , x 2 ]) and late fusion applies suitable unimodal models to each modality to obtain their feature representations, concatenates these features, and defines a classifier to the label (i.e., z mm = [z 1 , z 2 ])  [10] . MULTIZOO includes their implementations denoted as EF and LF respectively. Since these are basic building blocks in the multimodal learning field, we implement them ourselves.\n\nTensors are specifically designed to tackle the multimodal complementarity challenge by explicitly capturing higher-order interactions across modalities  [179] . Given unimodal representations z 1 , z 2 , a multimodal tensor representation is defined as\n\nwhere ⊗ denotes an outer product.\n\nHowever, computing tensor products is expensive since their dimension scales exponentially with the number of modalities. Several efficient variants have been proposed to approximate expensive full tensor products with cheaper variants while maintaining performance  [71, 101, 106] . MULTIZOO includes Tensor Fusion (TF)  [179]  as well as approximate Low-rank Tensor Fusion (LRTF)  [106] .\n\nWe use the Tensor Fusion implementation in https://github.com/Justin1904/ TensorFusionNetworks and the Low-rank Tensor Fusion implementation in https:// github.com/Justin1904/Low-rank-Multimodal-Fusion. As future work, we also plan to include more expressive higher-order tensor fusion methods  [71] .\n\nMultiplicative Interactions (MI) further generalize tensor products to include learnable parameters that capture the interactions between streams of information  [77] . In its most general form, MI defines a bilinear product z mm = z 1 Wz 2 + z ⊺ 1 U + Vz 2 + b where W, U, Z, and b are trainable parameters. By appropriately constraining the rank and structure of these parameters, MI recovers HyperNetworks  [61]  (unconstrained parameters resulting in a matrix output), Feature-wise linear modulation (FiLM)  [120, 188]  (diagonal parameters resulting in vector output), and Sigmoid units  [37]  (scalar parameters resulting in scalar output). MULTIZOO includes all 3 as MI-MATRIX, MI-VECTOR, and MI-SCALAR respectively. Since code was not released for the Multiplicative Interactions paper  [77] , we implemented the MI layer ourselves. We also referred to the implementation of Feature-wise linear modulation (FiLM)  [120]  from https://github.com/ethanjperez/film and added it as a module in MULTIBENCH, which we call FILM. While MI-VECTOR (i.e., diagonal parameters in a MI layer which results in a vector output) corresponds to the most basic implementation of FILM, the original FILM layer uses multiple non-linear layers instead of a single linear transformation in MI-VECTOR which has been shown to improve performance  [120] .\n\nGated attention models are prevalent in learning combinations of two representations that dynamically change for every input  [25, 167, 171] . Its general form can be written as z mm = z 1 ⊙ h(z 2 ), where h represents a function with sigmoid activation and ⊙ denotes the element-wise product. The output h(z 2 ) is commonly referred to as \"attention weights\" learned from z 2 used to attend on z 1 .\n\nWe implement the Query-Key-Value mechanism as NL GATE as proposed in  [167]  by referring to the implementation of in https://github.com/facebookresearch/VMZ. This attention mechanism is conceptually similar to the MI-VECTOR case above but recent work has explored more expressive forms of h such as using a Query-Key-Value mechanism  [167]  or several fully-connected layers  [25]  rather than a linear transformation in MI-VECTOR.\n\nTemporal attention models are useful in tackling the challenge of multimodal alignment and complementarity. Transformer models  [158]  have been shown to be useful for temporal multimodal data by automatically aligning and capturing complementary features at different time-steps  [154, 174] . We include the Multimodal Transformer (MULT)  [154]  which uses a Crossmodal Transformer block that uses z 1 to attend to z 2 (and vice-versa), before concatenating both representations to obtain\n\nWe use the public implementation available at https://github.com/yaohungt/ Multimodal-Transformer which includes a basic crossmodal transformer block designed for 2 modalities. To extend this to 3 modalities, the crossmodal transformer block is repeated across all 3 sets of modality pairs (i.e., z mm = [z 1→2 , z 2→1 , z 1→3 , z 3→1 , z 2→3 , z 3→2 ]). While this is still computationally feasible for 3 modalities such as the language, video, and audio datasets that MULT was originally designed for, this quickly becomes intractable for problems involving more than 3 modalities. To adapt MULT for the financial prediction task involving more than 10 modalities, we cluster all modalities into 3 groups based on similarities in their data and perform early fusion on the data within each cluster before applying MULT only on the 3 clusters of modalities. While MULT is a strong model based on performance, it poses scalability issues that should be the subject of future work (i.e., since the number of cross-modal attention blocks grows quadratically with the number of modalities).\n\nArchitecture search: Finally, instead of hand-designing multimodal architectures, several approaches define a set of atomic neural operations (e.g., linear transformation, activation, attention, etc.) and use architecture search to automatically learn the best order of these operations for a given multimodal task  [122, 173] . We focus on the more general approach, MFAS  [122] , designed for language and vision datasets.\n\nWe adapt the implementation from https://github.com/juanmanpr/mfas. While this approach is categorized under innovations in model architecture (since it primarily targets better architectures for multimodal fusion), its code in the MULTIZOO toolkit is implemented under training structures, since architecture search requires an outer loop to learn model architectures over multiple inner supervised learning loops that train an individual model architecture. Therefore, we are unable to integrate MFAS directly with the basic supervised learning training loops like we do for the other fusion paradigms described above.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "E.4 Optimization Objectives",
      "text": "In addition to the standard supervised losses (e.g., cross-entropy for classification, MSE/MAE for regression), several proposed methods have proposed new optimization objectives based on:\n\nPrediction-level alignment: There has been extensive research in defining objective functions to tackle the challenge of multimodal alignment: capturing a representation space where semantically similar concepts from different modalities are close together. While primarily useful for crossmodal retrieval  [104, 187] , recent work has also shown its utility in learning representations for prediction  [9, 33, 91, 151] . These alignment objectives have been applied at both prediction and feature levels. In the former, we implement Canonical Correlation Analysis (CCA)  [7, 166] , which computes L CCA = corr (g 1 (z 1 ), g 2 (z 2 )) where g 1 , g 2 are auxiliary classifiers mapping each unimodal representation to the label. This method corresponds to prediction-level alignment since they aim to learn representations of each modality that agree on the label, as measured by the correlation of label predictions made by each modality across a batch of samples.\n\nWe refer to the paper that most closely implements CCA-based alignment for multimodal data (specifically directly testing on the CMU-MOSI dataset)  [145] . Since the authors did not release their code, we implemented it from scratch with reference to CCA implementations from https://github. com/Michaelvll/DeepCCA and https://github.com/VahidooX/DeepCCA. Feature-level alignment: In the latter, contrastive learning has emerged as a popular approach that brings similar concepts close in feature space and different concepts far away  [33, 91, 151] . MULTI-ZOO includes REFNET  [135]  which includes a self-supervised contrastive loss between unimodal representations z 1 , z 2 and the multimodal representation z mm , i.e., L contrast = 1cos(z mm , g 1 (z 1 )) + 1cos(z mm , g 2 (z 2 )) where g 1 , g 2 is an auxiliary layer mapping each modality's representation into the joint multimodal space. The intuition here is that the unimodal representations z 1 , z 2 and the multimodal representation z mm should be aligned in the multimodal feature space as measured by cosine similarity. While the original REFNET method does not use negative samples, closely related work in multi-view contrastive learning has extended this idea to use negative samples which is more closely in line with recent work in contrastive learning  [151] .\n\nSince they did not release code, we implement REFNET ourselves on top of current supervised learning modules in MULTIZOO.\n\nReconstruction objectives: Methods based on generative-discriminative models (e.g., VAEs) include an objective to reconstruct the input (or some part of the input)  [91, 155] . These have been shown to better preserve task-relevant information learned in the representation, especially in settings with sparse supervised signals such as robotics  [91]  and long videos  [155] . We include the Multimodal Factorized Model (MFM)  [155]  which is a general approach that learns a representation z mm that can reconstruct input data x 1 , x 2 while also predicting the label. The multimodal representation is a concatenation of factorized representations z 1 , z 2 , ..., z M , and z y .\n\nSince MFM optimizes a variational lower-bound to the log likelihood, the overall objective consists of 3 terms -generative, discriminative, and prior regularization: min fi,fmm,gi,gy\n\nwhere f i 's are encoders from each modality to representations, f mm is a multimodal encoder to the joint representation z y , g i 's are decoders from latent representations back into input data, and g y is a classification head to the label. The final MMD term is a regularizer to bring the representations close to a unit Gaussian prior. The multimodal encoder f mm in MFM can be instantiated with any multimodal model from section 3.2 (e.g., learning z y via tensors and adding a term to reconstruct input data). We use the public implementation in https://github.com/pliang279/factorized, which uses a temporal attention model as f mm for multimodal time-series data. For the remaining experiments we replace f mm with a simple late fusion but also run some experiments with multimodal methods that are state-of-the-art in each domain.\n\nImproving robustness: These approaches modify the objective function to account for robustness to noisy  [101]  or missing  [89, 111, 123]  modalities. MULTIZOO includes MCTN  [123]  which uses cycle-consistent translation to predict the noisy/missing modality from present ones. The key insight is that a joint representation between modalities x 1 and x 2 can be learned by using x 1 to predict x 2 , in a vein similar to machine translation or image/text style transfer. MCTN defines a cyclic translation path x 1 → z mm → x2 → z mm → x1 and adds additional reconstruction losses L rec = x 1 -x1 2 + x 2 -x2 2 on top of the supervised learning loss. The representations z mm learned via translation are then used to predict the label. Surprisingly, the model needs to take in only x 1 at test time and is therefore robust to all levels of noise or missingness in x 2 .",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "E.5 Training Procedures",
      "text": "Improving generalization: Recent work has found that directly training a multimodal model with all modalities using supervised learning is sub-optimal since different modalities overfit and generalize at different rates. MULTIZOO includes an approach to solve this, called Gradient Blending (GRADBLEND), that computes generalization statistics for each modality to determine their weights during multimodal fusion  [167] . We use the implementation in https: //github.com/facebookresearch/VMZ and modify it to be part of the MULTIZOO training structures.\n\nWe also include a similar work, Regularization by Maximizing Functional Entropies (RMFE), which uses functional entropy to balance the contribution of each modality to the classification result  [53] . We use the public implementation from https://github.com/itaigat/ removing-bias-in-multi-modal-classifiers.",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "E.6 Domain-Specific Methods",
      "text": "Finally, we also implemented several domain-specific methods that had been applied to each domain. These include sensor fusion  [91]  and Kalman filtering  [90]  for robotics, and the multimodal Refiner network  [135]  for multimedia experiments. We refer the reader to the respective papers for algorithmic details.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "F Integrating Multibench And Multizoo: A Brief Tutorial",
      "text": "MULTIBENCH is available via our public GitHub: https://github.com/pliang279/ MultiBench. We also include a landing website page on https://cmu-multicomp-lab. github.io/multibench/ that includes an introduction to the benchmark, links to the relevant papers on multimodal datasets and algorithms, and a public leaderboard to keep track of current progress on these multimodal tasks. In this section, we provide more details for the loading of datasets ML pipeline provided by MULTIBENCH. We also describe the modular implementation of multimodal models in MULTIZOO and provide several code examples to illustrate its usage.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "F.1 Reading The Dataset",
      "text": "We provide scripts for reading each dataset supported by MULTIZOO at dataset/[dataset_name]/get_data.py in the repository. For each dataset, the user will need to first follow downloading and preprocessing instructions documented in Section C.2 or in the comments of the get_data.py. The python script contains a function (usually called get_dataloader) that takes in required arguments (such as the location of the preprocessed dataset or compressed data, etc) and it will output a tuple of three PyTorch Dataloader objects for train, valid, and test split of the dataset respectively. You can feed these dataloaders directly into training structures in MULTIZOO.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "F.2 Unimodal Models",
      "text": "In addition to the multimodal models described in Appendix E that are the main subject of study in this area, each dataset and modality typically also requires an initial processing stage either through feature extraction (see Appendix C.2 for initial feature extraction done on each dataset) and/or unimodal models on raw data/extracted features.\n\nTo standardize the implementation of unimodal models, MULTIZOO includes an implementation of several standard unimodal models that we encountered when running experiments on the diverse range of datasets and modalities in MULTIBENCH. Each unimodal model is implemented as a function class that takes in either raw data or extracted features from a modality and returns a unimodal representation tensor after applying the function. MULTIZOO includes the following unimodal methods:\n\n1. MULTI-LAYER PERCEPTRONS form the building blocks of many deep learning methods and are generally suitable for any modality that has undergone feature extraction into a vector that does not require any more processing with inductive biases. Their general structure means that they can be flexibly adapted for the tabular, set, and image, and text (e.g., see Deep Averaging Network  [76] ) modalities. They have also been used as a starting point for force and proprioception sensors in robotics if data does not come in the form of time-series  [91] . 2. CONVOLUTIONAL NETWORKS  [87]  are typically used over the image modality. They are also used on the audio modality if an initial preprocessing step of converting raw audio to spectrograms is used. 3. RESNETS  [66]  are an improvement over ConvNets to enable training of deeper models and have been used extensively for images and audio spectrograms. 4. RECURRENT NETWORKS  [134] , GRUS  [29] , and LSTMS  [69]  are suitable for temporal data in the form of text, video, audio, and time-series modalities. 5. TRANSFORMERS  [158]  have recently emerged as a strong alternative to recurrent models by using self-attention rather than an accumulative memory. They are also suitable for text, video, audio, and time-series modalities. We also implemented recently proposed VISION TRANSFORMERS  [44]  that adapt Transformer models for image classification as well. 6. DEEP SETS  [184]  was proposed as a permutation-invariant method for machine learning on sets, and was shown to outperform prior methods such as MLPs that are sensitive to the permutation of elements. 7. Finally, we also included several domain-specific methods that we encountered as we were accumulating the datasets in MULTIBENCH. Some of these methods include MAXOUT networks  [58]  used for MM-IMDB  [8]  and CAUSAL CONVOLUTION  [157]  for the highfrequency force sensors used in robotics datasets  [91, 90] .",
      "page_start": 59,
      "page_end": 60
    },
    {
      "section_name": "F.3 Multimodal Models",
      "text": "MULTIZOO includes an implementation of all multimodal methods described in Appendix E. Each multimodal method (i.e., fusion paradigm) is implemented as a Pytorch Module class taking in unimodal tensors and returning final multimodal representation vectors. We implemented several common fusion modules, such as Concatenation, Early-Concatenation (i.e., concatenate in input space), Stack, FilM, Multiplicative-Interactions (MI), Tensor Fusion, LRTF, NL-gate, and more described in Appendix E. When the training algorithm requires non-standard multimodal representations (e.g., more than one vector output from fusion module) or the unimodal encoders produce non-standard unimodal representations (i.e., not a single vector representation), special fusion modules will be needed in these situations. For example, we wrote a roboticsConcat module that performs concatenation for the VISION&TOUCH dataset due to its non-standard unimodal encoder output. We also have special fusion modules for optimization objectives or training structures such as MVAE, MFAS, and GRADBLEND. The design of modular fusion modules gives flexibility in model design, as users can reuse a previous fusion module directly in most cases but can also write their own special fusion modules easily.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "F.4 Classification Head",
      "text": "Finally, MULTIZOO includes flexible implementations of classification heads that take in the multimodal representation and return a label either directly (perhaps with some activation) for regression or a softmax over classes for classification.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "F.5 Optimization Objectives",
      "text": "The optimization objectives are modules that take in the classification or regression result produced by the model and the ground-truth (as well as other necessary inputs if applicable) and return a loss that can be used to optimize the model based on the desired objective. In most methods we simply use torch.nn.CrossEntropyLoss as the objective for classification tasks and torch.nn.MSELoss as the objective for regression tasks. However, in certain training structures, special objectives are required. For example, MULTIZOO includes implementations of objective functions such as weighted reconstruction loss and ELBO loss used in reconstruction-based methods MFM and MVAE, and there are also implementations of alignment-based objectives such as CCA and contrastive learning. The final optimization objective returns a weighted sum of these prediction objectives and auxiliary objectives, where the user is free to specify these weights as hyperparameters.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "F.6 Training Structures",
      "text": "Training Structures are the main body of MULTIZOO programs. All other modules (unimodal models, fusion paradigms, optimization objectives, classification heads, etc) can be seen as exchangeable plugins to these training structures. The training structure determines the main training algorithm, with the most common one being supervised_learning (training unimodal, multimodal, and classification parameters directly for a task-specific supervised learning objective).\n\nMore advanced methods may change this training structure either through additional optimization objectives (MVAE  [168] , MFM  [155] ) or via extensions of supervised learning through dynamic weighting of modalities (GRADBLEND  [167] ) or an outer architecture search training loop (MFAS  [122] ). Each of these methods, therefore, have their own training structure module.\n\nThese interchangeable plugin modules give a lot of flexibility in adapting each training structure to new tasks. For example, for the experiments described in Section G, the methods that are primarily based on different fusion paradigms (i.e., EF, LF, TF, LRTF, MI, NL-GATE, MULT etc all use the same training structure (supervised_learning) with different plugin fusion modules (and different unimodal encoders and heads based on datasets and tasks). Similarly, while most of these more advanced training structures were originally paired with a simple LF model in their original papers, our modular implementation makes it possible to combine advances in fusion paradigms with training structures in future work.",
      "page_start": 60,
      "page_end": 61
    },
    {
      "section_name": "F.7 Performance Evaluation",
      "text": "We standardize evaluation using metrics designed for each dataset, ranging from MSE and MAE for regression to accuracy, micro & macro F1-score, and AUPRC for classification. We use the standard PyTorch and scikit-learn implementations of these performance metrics.\n\nAlgorithm 2 PyTorch code integrating MULTIBENCH datasets and MULTIZOO models.",
      "page_start": 4,
      "page_end": 61
    },
    {
      "section_name": "F.8 Complexity Evaluation",
      "text": "We report training memory by measuring peak memory usage of the python process during the entire training process using python memory_profiler toolkit (https://pypi.org/project/ memory-profiler/). When counting the number of parameters when training a model, we only count the parameters in persistent modules during training and does not count the ephemeral networks or modules created in the middle of the training process (such as the networks trained for determining weights in GRADBLEND or the fusion architectures created as part of the architecture search process in MFAS).",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "F.9 Robustness Evaluation",
      "text": "For robustness experiments, modality-specific and multimodal imperfections are implemented as modules. A separate version of data loader is created for each dataset to test robustness, which adds custom unimodal or multimodal imperfections of increasing noise levels σ ∈ [0, 1] to the original clean test set. A testing module is also provided specifically for robustness experiments, which evaluates the model on increasing levels of noisy test datasets and prints out the metrics for visualization. In this way, MULTIZOO allows highly modular data loading and robustness evaluation that requires minimal modification to the regular training and testing workflow.\n\nMULTIZOO includes evaluation protocols summarizing these robustness results. It includes visualization functions of the performance-imperfection curves across datasets in MULTIBENCH. We also implemented relative and effective robustness as two quantitative metrics for robustness evaluation. For relative robustness, we approximate the area under the performance-imperfection curves for each model across MULTIBENCH datasets. For effective robustness, we take the performance-imperfection curve of LF evaluated on the same dataset equalized for initial accuracy on clean test data. For both metrics, we normalized performance across all models evaluated on the same dataset.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "F.10 Code Snippets",
      "text": "In Algorithm 2, we show a sample code snippet in Python that loads a dataset from MULTIBENCH (Appendix C.2), defines the unimodal and multimodal architectures, optimization objectives, and training procedures (Appendix E), before running the evaluation protocol (Appendix D). Our MUL-TIZOO toolkit is easy to use and trains entire multimodal models in less than 10 lines of code. By standardizing the implementation of each module and disentangling the individual effects of models, optimizations, and training, MULTIZOO ensures accessibility and reproducibility of its multimodal algorithms.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "G Experimental Setup",
      "text": "In this section, we provide additional details of the experimental setup. All experiments were conducted on a server with 4× Nvidia GTX 980 Ti GPUs, 5× Nvidia Tesla P40 GPUs, 2× Nvidia Tesla K40c GPUs, 4× Nvidia TITAN X GPUs, 1× Tesla T4 GPU, and 1× Tesla V100 GPU. The server also contained 32× Intel(R) Xeon(R) CPU (E5 -2670, 2.60GHz).",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "G.1 Affective Computing",
      "text": "Hyperparameters: We show the hyperparameters used for models on datasets in the Affective Computing domain in Table  4 . For each dataset we tune the following hyperparameters selected from the following ranges: the learning rate is selected between 0.00001 to 0.001 and set to be 0.0001 in the beginning; Early stopping is applied with patience 8 to 20 before overfitting happens; The input sizes and hidden sizes vary according to the different modalities and datasets. The µ t0 , µ c , and µ t1 hyperparameters in MCTN  [123]  is tuned between 0.005 to 0.1. The sequence length varies from 20 to 50. Only punchline sentences (target sentences) are used in UR-FUNNY  [64]  and MUSTARD  [24]  following the original papers.\n\nstructures (such as GRADBLEND, MFAS), we followed the same configuration as the original papers where these methods are proposed.\n\nFor MM-IMDB, used the same MaxoutLinear unimodal encoders following current work  [8] .\n\nLearning rates were tuned between 0.1 and 0.001 except for unimodal training. The default batch size is 128 while that for CCA is 800 to make sure the methods work as intended. The number of epochs was selected based on early stopping with patience equal to 7, which means if the macro F1 on the validation set did not improve for 7 epochs, training was stopped early.\n\nFor KINETICS, we use a ResNet-LSTM for the visual modality encoder and the architectures described by Wang et al  [167]  for the rest of the models. We use a learning rate of 0.0001, batch size of 16, and 15 epochs for the small dataset experiments. For the large dataset experiments, we used the setup described by Wang et al  [167] .\n\nHyperparameters were selected based on performance on the validation set. For models that had been previously proposed and tested on these datasets, we use the same hyperparameters as those reported in their paper or public code.\n\nAll experiments were repeated 10 times and a mean and standard deviation was computed.",
      "page_start": 62,
      "page_end": 64
    },
    {
      "section_name": "H Experimental Results",
      "text": "In this section, we provide additional experimental results and observations. For all experimental tables, we describe the accuracy metrics using Acc(c) where c is the number of classes. AUPRC stands for the area under the precision-recall curve which is a useful performance metric for imbalanced data in settings where one cares a lot about finding positive examples. MSE stands for mean squared error. We use up and down arrows (↑ and ↓) to indicate metrics where higher is better (Acc, AUPRC) and metrics where lower is better (MSE) respectively.",
      "page_start": 71,
      "page_end": 71
    },
    {
      "section_name": "H.1 Affective Computing",
      "text": "We show the full performance results in Table  13  and complexity results in Table  14 . Here we list some observations regarding these results:\n\n1. Language is usually the best performing modality, especially on sentiment and emotion prediction. However, the improvement of language over audio and video on humor prediction and sarcasm prediction is much less. This follows our intuition that while language is primarily useful for sentiment and emotion prediction, audio and visual are strong predictors for humor and sarcasm. 2. The best performing method over these datasets is consistently the Multimodal Transformer (MULT  [155] ), which was originally tested on predicting sentiment and emotions on the CMU-MOSI and CMU-MOSEI dataset. We find that it is a general method and generalizes to humor and sarcasm prediction as well. 3. However, while it MULT achieves the best performance, it suffers in complexity, taking more than 12× the inference time of unimodal models and 3 -4× several simpler early or late fusion multimodal baselines. 4. Some methods that work well on humor, sentiment, and emotion prediction do not generalize to sarcasm detection, such as tensor fusion (TF) and reconstruction-based models (MVAE and MFM). It is not a surprise that this coincides with sarcasm being the least studied task as well. Furthermore, we believe that it is a task with extremely complementary information (e.g., sarcasm is usually displayed via text and video/audio features contradicting each other). We hope that MULTIBENCH can encourage further research in such multimodal tasks since current methods do not generalize to these tasks. 5. Several out-of-domain methods, such as GRADBLEND do not work well. In fact we find that the variance of the GRADBLEND method is quite high and shows strong performance on several datasets but struggles on others. 6. MCTN is designed for robustness and only uses the language modality at test time. While it was shown to work well for relatively easier fusion tasks in predicting sentiment, emotions, and humor  [123] , we find that it struggles on the more challenging sarcasm prediction task.",
      "page_start": 72,
      "page_end": 72
    },
    {
      "section_name": "Mustard Video Mustard Language",
      "text": "MUStARD audio MUStARD multimodal  Figure 12 : Robustness of multimodal models with increasing levels of noise on the MUSTARD dataset in the affective computing domain.",
      "page_start": 73,
      "page_end": 73
    },
    {
      "section_name": "Cmu-Mosi Language Cmu-Mosi Video Cmu-Mosi Audio Cmu-Mosi Multimodal",
      "text": "Figure  13 : Robustness of multimodal models with increasing levels of noise on the CMU-MOSI dataset in the affective computing domain.",
      "page_start": 74,
      "page_end": 74
    },
    {
      "section_name": "Ur-Funny Video Ur-Funny Language Ur-Funny Audio Ur-Funny Multimodal",
      "text": "Figure  14 : Robustness of multimodal models with increasing levels of noise on the UR-FUNNY dataset in the affective computing domain.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Cmu-Mosei Video Cmu-Mosei Language Cmu-Mosei Audio Cmu-Mosei Multimodal",
      "text": "Figure  15 : Robustness of multimodal models with increasing levels of noise on the CMU-MOSEI dataset in the affective computing domain.\n\nWe show the robustness of multimodal models with increasing levels of noise on MUSTARD in Figure  12 , CMU-MOSI in Figure  13 , UR-FUNNY in Figure  14 , and CMU-MOSEI in Figure  15 .\n\nWe highlight the following observations:\n\n1. Unimodal and multimodal models are in general not robust to increasing noise and imperfections in these datasets. Performance drops off very quickly towards random. 2. We find that multimodal models are slightly more robust than unimodal models. For video and audio, the unimodal method is the least robust. However, for language, the unimodal model can actually be more robust than several multimodal models. In other words, multimodal models are more robust to video and audio while being less robust to language, which is the best performing modality. We believe that directly training multimodal models via supervised learning can be prone to overfitting on the most informative modality (in this case language) which causes the multimodal model to be even less robust than unimodal models in language. A similar observation was the motivation behind the GRADBLEND approach to balance overfitting and generalization across different modalities  [167] . 3. GRADBLEND  [167]  seems to be a surprisingly robust approach while also generalizing to several datasets. GRADBLEND was not in fact not initially designed for the affective computing domain, although it was designed for similar multimodal time-series data in the multimedia domain. 4. MCTN  [123]  was designed as a robust alternative to multimodal models since it uses multimodal data at training time but only language data at test time. On imperfections to video and audio, MCTN therefore stays constant and can potentially be a viable alternative that learns a unimodal model from multimodal data during training but remains unimodal at testing.",
      "page_start": 76,
      "page_end": 76
    },
    {
      "section_name": "H.2 Healthcare",
      "text": "We show the full results in Table  15  and complexity results in Table  16 . Here we list some observations regarding these results:\n\n1. We find that results across all models show small variations on MIMIC, which suggests that many current multimodal approaches may not generalize that well to the input modalities and prediction tasks that MIMIC tests for. 2. In particular, while MFAS (architecture search) is otherwise a pretty general solution that works well across quite a few datasets, it struggles on MIMIC. While there has been a recently proposed MUFASA  [173]  method that adapts architecture search specifically for healthcare datasets, we were not able to test this method on our partition of MIMIC, and it is in our top priorities to implement that approach into MULTIZOO and accurately benchmark its performance on a suite of datasets. 3. Late Fusion (LF) with simple concatenation was the best-performing model in the evaluations conducted by the previous paper that used the exact same partition as ours  [129] . It actually works quite well compared to more complex models evaluated here, as it has the best performance on ICD-9 group 7 task and is quite close to the best performing models in the other two. This may suggest that simple multimodal models such as Late Fusion is worth being tried first on healthcare datasets. 4. The reconstruction-based multimodal models such as MVAE and MFM have strong performance on this dataset, possibly due to the low dimensions of the input modalities. This suggests that reconstruction-based architectures and objectives might work well on datasets with simple or low-dimensional modalities which are easier to reconstruct. Finally, we show the robustness of multimodal models with increasing levels of noise on the MIMIC dataset in Figure  16 . We highlight the following observations:\n\n1. Unimodal and multimodal models are in general not robust to increasing noise and imperfections in the table and time-series modalities. Performance drops off very quickly towards random. 2. In general, multimodal models are slightly more robust than unimodal models. The behavior is best exhibited in the ICD-9 group 7 task where many models start off strong, but multimodal models remain more robust than the best unimodal model. This perhaps indicates that multimodal models do learn to use information from other sources when another one is noisy. 3. There is high variance in the robustness of each multimodal model even within the same dataset and modalities but across different prediction tasks. We observe that LRTF is the most robust model on the ICD-9 group 7 task but the least robust model on the ICD-9 group 1 task. This high variance is a concern especially given the close similarity across both of these tasks.",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "H.3 Robotics",
      "text": "We show the full results in Table  17  and complexity results in Table  18 . Here we list some observations regarding these results:\n\n1. We find that in all robotics tasks, there exists one modality with extremely strong unimodal performance (force in VISION&TOUCH contact task, proprioception in VISION&TOUCH End Effector task, image in MUJOCO PUSH). 2. On the VISION&TOUCH dataset, we found that Late Fusion outperforms the method of choice in the original paper for the dataset  [91]  (Sensor Fusion) on both tasks, so Late Fusion seems to generalize well to this domain. 3. In our experiments, as well as the baselines  [91] , the action modality is typically treated as a general modality without specific modeling. Future work should explore whether this is the best way to encode action as a modality in these action-conditional prediction tasks, and possibly unify these datasets with those used in embodied multimodal learning  [36, 97, 110] . 4. We plan to include several more reinforcement learning tasks for multimodal learning in robotics. It remains an open question where multimodal representations suitable for fusion-type prediction tasks are also suitable for reinforcement learning tasks. Adding such reinforcement learning tasks from multiple sensors to MULTIBENCH will enable more accurate benchmarking of the generalization capabilities of these multimodal models.\n\nFinally, we show the robustness of multimodal models with increasing levels of noise on MUJOCO PUSH in Figure  17  and on VISION&TOUCH in Figure  18 . We highlight the following observations:\n\n1. For MUJOCO PUSH we plot the MSE using a log scale on the y-axis since the error of the TF method blows up significantly much faster than the other methods. 2. We observe that multimodal methods are much more robust than unimodal methods, which match the robustness results as reported in the paper  [91]  where the trained multimodal model is robust and able to recover from external forces on the force sensor or occlusions to the image sensor. This observation is true for both datasets. 3. For VISION&TOUCH, we observe that unimodal performance is especially bad for the object pose prediction task. The remaining multimodal models are relatively robust as compared to unimodal performance. The most robust models seem to be Sensor Fusion  [91]  (SF) and Late Fusion (LF).",
      "page_start": 78,
      "page_end": 78
    },
    {
      "section_name": "H.4 Finance",
      "text": "We show the full results in Table  19  and complexity results in Table  20 . Here we list some observations regarding these results:\n\n1. We do observe better performance using multimodal models as compared to unimodal ones, which suggests that multiple financial signals do help in stock prediction. Several multimodal models do generalize to this more challenging area which presents scalability challenges due to a large number of modalities (18 63 100 as compared to 2 3 in most datasets), as well as robustness challenges arising from real-world data with an inherently low signal-to-noise ratio. 2. There has been very little research in multimodal models in this area, and no public implementations of multimodal models on actual finance data. By adapting current models on this dataset, we observe decent performance of several out of domain methods. Specifically, early fusion (EF) works well which we believe to be due to the little heterogeneity in data origins (i.e., all data comes in the form of time-series data, which is much less heterogeneous as compare to image and text datasets). 3. There remains high variance in the performance of multimodal models even within the same domain: we observe that the best multimodal is not consistent across the 3 partitions of finance datasets, which suggests that current multimodal models remain highly sensitive to the task at hand. 4. Perhaps surprisingly, our experiments on using a Transformer found that they performed worse off than LSTM models. We hypothesize that these large Transformer models might be prone to overfitting on these small and noisy datasets. 5. These datasets present scalability issues to a large number of modalities. We find that we had to adapt several methods such as Tensor Fusion (TF) and Multimodal Transformer (MULT) since they scale exponentially and quadratically with the number of modalities respectively, which does not scale to these finance datasets with more than 10 modalities. We had to adapt these models by performing an initial clustering over the modalities to form 2 3 groups, performing early fusion by concatenating the data within each group and forming 2 3 'modalities' before applying methods such as Tensor Fusion (TF) and Multimodal Transformer (MULT). This might explain their slightly worse performance, especially MULT given its strong performance and generalization to different datasets in the affective computing domain. Future research should focus on more scalable multimodal methods to a large number of modalities. Unfortunately, the bulk of multimodal research being in language and vision means that this question is relatively unexplored.\n\nFinally, we show the robustness of multimodal models with increasing levels of noise on the finance datasets in Figure  19 . We highlight the following observations:\n\n1. We again observe a similar trend where the best multimodal models (MULT and sometimes EF) are more robust than the best unimodal model. However, different from other datasets, we find that certain multimodal models can be worse in performance and robustness than the best unimodal model. LF in particular is not very robust and performs worse than the best unimodal method. 2. The Gradient Blend (GRADBLEND) method is interesting since it starts off with the best (lowest) MSE but is the least robust -its error increases really quickly and ends up worse than several models that it was initially outperforming on 0 noise levels. 3. We find that several approaches might be underfitting the data on STOCKS-HEALTH and STOCKS-TECH. These methods do not start off with a good MSE and are also not affected significantly at increasing noise levels, showing a roughly straight horizontal line in Figure  19 .",
      "page_start": 79,
      "page_end": 79
    },
    {
      "section_name": "H.5 Hci",
      "text": "We show the full results in Table  21  and results on complexity in Table  22 . Here we list some observations regarding these results:\n\n1. The ENRICO paper  [93]  does not include code or provide many details about their experiments (e.g., data splits, hyperparameters). Compared to their reported results, our reproduction resulted in better performance for the set modality and worse performance for the screenshot modality. 2. Using multiple modalities can help prediction on ENRICO, boosting performance over the best unimodal model by 4%. 3. Similar to finance, there has been very little research in multimodal models for HCI. We observe decent performance of several out of domain methods, especially GRADBLEND which offers a slight improvement over a standard LF model. 4. Certain more complex methods, unfortunately, do not work that well on this dataset. On the architecture side, more expressive methods such as TF, LRTF and MI do not offer improvements over a simple LF model. We hypothesize that these more complex models have a larger number of trainable parameters which make them more prone to overfitting to small and noisy datasets.\n\nWe show robustness results with increasing levels of noise in Figure  20 . We highlight the following observations:\n\n1. We again observe a similar trend where the best multimodal models (LF and sometimes GRADBLEND) are more robust than the best unimodal model. However, different from other datasets, we find that certain multimodal models can be worse in performance and robustness than the best unimodal model. TF in particular is not robust and performs worse than the best unimodal method. 2. LF is surprisingly robust to imperfections in the image modality and shows a very stable trend despite high levels of noise, implying that the model has learned to rely on the set modality instead when the image is imperfect.",
      "page_start": 80,
      "page_end": 80
    },
    {
      "section_name": "H.6 Multimedia",
      "text": "We show the full results in Table  23  and results on complexity in Table  24 . Here we list some observations regarding these results:\n\n1. The current SOTA on AV-MNIST is based on architecture search: MFAS  [122] . Amongst all the methods we evaluated, MFAS is still the best performing method and beats the second best method (MVAE) by 0.5%. Meanwhile, Gradient Blend (GRADBLEND) does not seem to generalize well to this dataset, as it performs worse than all other multimodal methods. 2. On MM-IMDB, we attempted several methods on the objective function side. We found that using contrastive learning (REFNET)  [135]  or canonical correlation analysis (CCA) were quite useful in improving performance, with both outperforming purely architectural baselines without alignment as an optimization objective. In particular, while the CCA approach for multimodal fusion was originally proposed for affect recognition datasets  [145] , we find that they also generalize to the multimedia domain. 3. On KINETICS, Gradient Blend (GRADBLEND)  [167]  was shown to work really well in their original paper. However, we found that this approach does not generalize well to other datasets such as AV-MNIST. We also created a smaller version of Kinetics called KINETICS-S to enable quick prototyping of multimodal models. Unfortunately, we found that GRADBLEND also struggles on the smaller partition of Kinetics. 4. For KINETICS-S, we also observed that the visual unimodal model slightly outperformed the late fusion model despite the latter using more modalities. This reflects the observations by Wang et al.,  [167]  on the original full version of the KINETICS dataset.  In-domain refers to the performance on datasets that the method was previously proposed for and out-domain shows performance on the remaining datasets. We find that many methods show strongest performance on in-domain datasets which drop when tested on different domains, modalities, and tasks. In general, we also observe high variance in the performance of multimodal methods across datasets in MULTIBENCH, which suggest open questions in building more generalizable models.",
      "page_start": 81,
      "page_end": 88
    },
    {
      "section_name": "H.7 Performance",
      "text": "In this subsection, we summarize several general observations regarding the performance of multimodal models across domains, modalities, and tasks.\n\nIn the following analysis, we aggregate the performance of models by first assigning each task a weight of 1 n where n is the number of tasks in a dataset (e.g., there are 3 tasks in the MIMIC dataset: mortality, ICD-9 group 1, and ICD-9 group 7 prediction). Then, we compute the scaled performance of each model on each task by min-max normalization -setting the best-performing model's performance to 1 and worst-performing model's performance to 0, and scaling the performance of all remaining models linearly between 0 and 1. Note that we only take the best unimodal performance into account when determining the best and worst-performing models. Then, the final performance score for each model is computed by a weighted average of its scaled performances on all tasks that model was evaluated on.\n\nBenefits of standardization: Simply applying methods in a research different area achieves stateof-the-art performance on 9 out of the 15 tasks. We find that this is especially true for domains and modalities that have been relatively less studied in multimodal research (i.e., healthcare, finance, HCI). Performance gains can be obtained using multimodal methods outside of that research area. Therefore, this motivates the benefits of standardizing and unifying areas of research in multimodal machine learning. We believe that the ever-expanding diversity of datasets in MULTIBENCH can greatly accelerate research in multimodal learning.\n\nGeneralization across domains and modalities: MULTIBENCH offers an opportunity to analyze algorithmic developments across a large suite of modalities, domains, and tasks. We illustrate these observations through 2 summary plots of the generalization performance of multimodal models. Firstly, in Figure  22 , we plot the performance of each multimodal method across all datasets that it is tested on, using the color red to indicate performance on datasets that it was initially proposed and tested on (which we label as in-domain), and blue to indicate its performance on the remaining datasets (which we label as out-domain). Secondly, in Figure  23 , we color-code the performance on each dataset depending on which research area the dataset belongs to (one of the 6 research areas covered in MULTIBENCH).\n\nWe summarize several observations regarding generalization across domains and modalities below:\n\n1. Many multimodal methods still do not generalize across domains and datasets. For examples, MFAS  [122]  works well on domains it was designed for (AV-MNIST and MM-IMDB in the multimedia domain), but does not generalize to other domains such as healthcare (MIMIC). Similarly, the method designed for robustness, MCTN  [123] , does not generalize to datasets within the affective computing domain (UR-FUNNY and MUSTARD). Finally, GRADBLEND  [167] , an approach specifically designed to improve generalization in multimodal learning and tested on video and audio datasets (e.g., Kinetics), does not perform well on other datasets. Therefore, there still does not exist a one-size-fits-all model, especially on understudied modalities and tasks. 2. From Figure  22 , we find that many methods show their strongest performance on in-domain datasets, and their performance drops when tested on different domains, modalities, and tasks. Some interesting observations are that MULT performs extremely well on the affect recognition datasets it was designed for but struggles on other multimodal time-series in the finance and robotics domains. On the other hand, MFM shows an impressive performance in generalizing to new domains, although its in-domain performance has been exceeded by several other methods. 3. From Figure  22 , we also observe high variance in the performance of multimodal methods across datasets in MULTIBENCH, which suggest open questions in building more generalizable models. We find that LF is quite stable and always achieves above-average performance. 4. There are methods that are surprisingly generalizable across datasets. These are typically general modality-agnostic methods such as LF. While simple, it is a strong method that balances simplicity, performance, and low complexity. However, it does not achieve the best performance on any dataset, which suggests that it is a good starting point but perhaps not the best eventual method. 5. From Figure  23 , we find that performance also varies significantly across research areas. 6. Several methods such as MFAS and CCA are designed for only 2 modalities (usually image and text), and TF and MI do not scale efficiently beyond 2 3 modalities. Therefore, we were unable to directly adapt these approaches to other datasets. We encourage the community to generalize these approaches across datasets and modalities on MULTIBENCH.\n\nTradeoffs between modalities: How far can we go with unimodal methods? Surprisingly far! From each of the individual tables, we observe that decent performance can be obtained with the best performing modality. Further improvement via multimodal models may come at the expense of around 2 -3× the parameters.",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "Training",
      "text": "",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "H.8 Complexity",
      "text": "We aggregate the complexity of each model by taking the weighted average of the relative complexity of the model across tasks on which it is evaluated. The weights are assigned in the same way as performance weights described in the subsection above (i.e., performing min-max normalization across models within each task and averaging across the normalized performance across all datasets that the model was tested on). The relative complexity of each model on each task is computed by dividing its training time by the best unimodal model's training time and taking the negative log 10 of this value (we take negative log because some more complex methods can take hundreds of times the training time of simpler methods). Thus, the higher the value of aggregated complexity, the faster the model trains.\n\nBased on the full results above, we summarize the overall tradeoff between performance and complexity in Figure  24 (a). We aggregate performance and complexity statistics by first performing min-max normalization within each data to a scale of 0 -1 for performance and complexity separately. Note that for metrics where lower is better (i.e., MSE or RMSE) we reverse the direction of min-max normalization. We then aggregate normalized statistics across all datasets and plot the tradeoff between performance and complexity. We highlight the following observations:\n\n1. In Figure  24 , we plot a dotted blue line of best quadratic fit to show the Pareto frontier between performance and complexity. We choose a quadratic fit since it is common to fit a curve rather than a straight line when considering the tradeoff frontier between 2 variables (related to the law of diminishing returns in economics). Using this plot, we find a strong tradeoff between these two desiderata: simple fusion techniques (e.g., early fusion EF and late fusion LF) are actually appealing choices that score high on both metrics, especially when compared to complex (but slightly better performing) methods such as architecture search MFAS or Multimodal Transformers MULT. 2. Using this quadratic curve, we find that the best unimodal model is under the curve (i.e., worse-off than the Pareto front). This implies that while unimodal models train the fastest, several multimodal methods can outperform them despite being slightly slower, and is an overall better choice when taking both performance and complexity into account. LF is an appealing choice that lies above the curve.  There is only a slight positive trend between performance and overall robustness of these multimodal models. Therefore, few well-performing models currently achieve both relative and effective robustness, which is a crucial area for future multimodal research.",
      "page_start": 90,
      "page_end": 91
    },
    {
      "section_name": "H.9 Robustness",
      "text": "In this section, we summarize our observations regarding the tradeoffs between accuracy and robustness, where we use the quantitative metrics for relative and effective robustness as described in Appendix D.3. As a reminder, relative robustness directly measures accuracy under imperfections while effective robustness measures the rate of accuracy drops with imperfection after equalizing for initial accuracy on clean test data. In Figure  25 , we plot a similar tradeoff plot between accuracy and (relative & effective) robustness. Again, we aggregate performance and complexity statistics by first performing min-max normalization within each data to a scale of 0 -1 for performance and robustness separately. We aggregate normalized statistics across all datasets and plot the tradeoff between performance and robustness. We highlight the following observations:\n\n1. We show the line of best linear fit for relative and effective robustness in dotted blue in Figure  25 . We observe a slight positive correlation between performance and relative robustness, which implies that models starting off with higher accuracy tend to stay above other models on the performance-imperfection curve. In particular, several methods such as MVAE and RMFE show strong performance and robustness. 2. However, we observe a slightly negative correlation for effective robustness. Unfortunately, several well-performing methods such as MULT, CCA, and MVAE tend to drop off faster after equalizing for initial accuracy on clean test data. 3. Finally, we plot an average of relative and effective robustness in Figure  26  as an overall quantitative measure of robustness. We observe that very few models currently achieve both relative and effective robustness, which prompts an area for future multimodal research.",
      "page_start": 92,
      "page_end": 93
    },
    {
      "section_name": "H.10 Summary Of Takeaway Messages",
      "text": "From these results, we emphasize the main take-away messages and motivate several directions for future work:\n\n1. Benefits of standardization: Applying methods in a research different area achieves state-ofthe-art performance on 9 out of the 15 datasets, especially those relatively less studied in multimodal research (i.e., healthcare, finance, HCI). This motivates the benefits of standardizing and unifying areas of research in multimodal learning. We hope that MULTIBENCH and MULTIZOO can be a step in this direction. 2. Generalization across domains and modalities:\n\n(a) Many multimodal methods still do not generalize across domains and datasets, showing high variance across datasets in MULTIBENCH. Some of these methods perform worse on out-of-domain datasets than in-domain datasets while other methods are designed in a specific manner for certain modalities and domains which makes them unable to be adapted to other datasets in straightforward ways. (b) Certain simple methods (e.g., LF) are surprisingly generalizable. However, it does not achieve the best performance on any dataset, which suggests that it is a good starting point but perhaps not the best method. 3. Decent performance can be obtained with the best performing modality, which motivates the need for new datasets that offer challenges and opportunities in multimodal modeling not achievable from unimodal methods. 4. There is a strong tradeoff between performance and complexity which suggests that future work should also focus on lightweight multimodal models that generalize throughout datasets in MULTIBENCH. 5. Tradeoffs between performance and robustness:\n\n(a) Models starting off with higher accuracy tend to stay above other models on the performance-imperfection curve. (b) However, several well-performing methods also tend to drop off faster after equalizing for initial accuracy on clean test data. (c) Overall, very few models currently achieve both relative and effective robustness, which prompts an area for future multimodal research.",
      "page_start": 93,
      "page_end": 93
    },
    {
      "section_name": "I Future Directions",
      "text": "We plan to ensure the continual availability, maintenance, and expansion of MULTIBENCH. Several immediate future directions include expansions in the datasets provided, algorithms implemented in MULTIZOO, and broadening the holistic evaluation of multimodal models.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "I.1 Datasets",
      "text": "One main area of expansion lies in the datasets supported by MULTIBENCH. We first describe the categories of multimodal datasets in the fusion domain that we plan to add in the following months.\n\nWe also plan to include several new application areas where multimodal fusion is useful, such as cross-modal retrieval, multimodal question answering, and grounding across modalities, which we will detail in the following subsections. Finally, we explain our plan for community-based expansion of datasets and models based on user feedback that will happen in parallel.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "I.1.1 Fusion",
      "text": "Within the same category of multimodal fusion, we plan to add datasets within the same application domains as well as to expand to new application domains. Within the current domains, we plan to include (1) the hateful memes challenge  [82]  as a core challenge in multimedia to ensure safer learning from ubiquitous text and images from the internet, (2) more datasets in the robotics and HCI domains where there are many opportunities for multimodal modeling, and (3) several datasets which are of broad interest but are released via licenses that restrict redistribution such as dyadic emotion recognition on IEMOCAP  [21] , deception prediction on from real-world Trial Data  [121] , and multilingual affect recognition on CMU-MOSEAS  [182]  which was only just recently released. We are currently working with the authors to integrate some of these datasets into MULTIBENCH in the near future. These new datasets will benchmark multimodal modeling in human-centric areas where privacy and fairness can be important desiderata. Furthermore, it will enable benchmarking of multimodal learning in languages other than English which is important towards building more accessible multimodal models that include the language modality.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "I.1.2 Retrieval",
      "text": "Another area of great interest lies in cross-modal retrieval  [104, 187] . In this area, the goal is to retrieve semantically similar data from a new modality using a modality as a query (e.g., given a phrase, retrieve the closest image describing that phrase). The core challenge is to perform alignment of representations across both modalities. Retrieval has been studied primarily in the multimedia space (e.g., retrieving images, video, and audio given a text query) and we hope to add some of these datasets as well as to expand datasets for cross-modal retrieval using different combinations of query and retrieved modalities.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "I.1.3 Question Answering",
      "text": "Within the domain of language and vision, there has been growing interest in language-based question answering (i.e., \"query\" modality) of entities in the visual, video, or embodied domain (i.e., \"queried\" modality). Datasets such as Visual Question Answering  [4] , Social IQ  [178] , and Embodied Question Answering  [36]  have been proposed to benchmark the performance of multimodal models in these settings. A core challenge lies in aligning words asked in the question with entities in the queried modalities, which can take the form of visual entities in images or videos, and actions in embodied environments. We plan to add these datasets as soon as possible, and also plan to add QA over multiple queried modalities such as text, images, and tables as proposed in recent work  [63, 147] .",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "I.1.4 Grounding",
      "text": "Grounding is the task of linking entities (often at their most granular level) in one modality with entities in another modality. As an example, in the domain of language and vision, a well-studied grounding task is visual referring expressions -the task of localizing an object in an image referred to by a natural language expression (e.g., half of a sandwich on the right side of a plate nearest a coffee mug)  [32] . Grounding can be seen as a more fine-grained version of retrieval where the retrieved modality of interest is at the level of sub-patches of an image. We currently do not include tasks in the grounding area since there are no datasets outside using language to query images (and their subregions). We plan to include grounding datasets in the language and vision domain but also encourage research in extending this research problem to other modalities (e.g., using language to query video/audio/sets/tables).",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "I.1.5 Reinforcement Learning",
      "text": "Learning from multiple modalities in an interactive setting is an area of interest as a step towards building more intelligent embodied agents that can perceive the visual world, language instructions, auditory feedback, and other sensor modalities. These research areas broadly span language-conditional RL (i.e., instruction following, learning a reward function from instructions, language in the observation or action space) and language-assisted RL (language as domain knowledge, language to structure policies)  [110] . Recent work has also explored audio as a modality in an agent's multisensory interaction with the world  [38] . Modern robot systems are also equipped with multiple sensors to aid in their decision-making and there has been considerable research in learning multimodal representations from multiple sensors for robot manipulation  [89] [90] [91] .\n\nThese multimodal problems are fundamentally different from those that are concerned with prediction tasks. Alongside the core challenges in learning complementary information and aligning entities in language instructions to those in the visual environment, there also lies the core challenge of learning actionable representations that link to the set of actions that can be taken and their associated long-term rewards  [110] . We plan to include these datasets in a future version of MULTIBENCH. We also encourage research in extending these multimodal tasks beyond language and vision to truly incorporate the diverse set of modalities humans use in everyday interactive tasks.",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "I.2 Models",
      "text": "By partitioning the structure of multimodal code into the distinct areas in Appendix E (data processing, unimodal and multimodal model design, optimization objectives, and training structures), MULTIZOO enables easy addition of new innovations from all areas. It is easy to add new unimodal encoders as they are developed in areas such as computer vision and natural language processing. Similarly, it is extremely simple to add multimodal methods while ensuring compatibility with existing unimodal encoders, fusion paradigms, optimization objectives, and training structures. Please refer to Appendix F for code snippets changing multimodal models, optimization objectives, and training structures.\n\nThe authors maintain a reading list for topics in multimodal ML  [98]  that is regularly updated for the latest advances in the area. We plan to periodically add proposed methods to the MULTIZOO toolkit with help from the community as well.",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "I.3 Evaluation",
      "text": "MULTIBENCH is designed with holistic evaluation in mind. Currently, MULTIBENCH supports evaluation for prediction performance, time and space complexity, and robustness to noisy and missing modalities. There are several other crucial evaluation dimensions that we plan to include in the following versions of the benchmark:",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "I.3.1 Uncertainty Estimates",
      "text": "There has been important work in building ML models that return uncertainty estimates along with their prediction targets  [52, 57]  along with recent interest in building multimodal models with similar capabilities  [20, 169] . As ML models are increasingly deployed in real-world sensitive scenarios  [12, 34, 160] , there is an increasing need to quantify when ML models do not know the right answer and potentially abstain  [107]  or defer the prediction to a human expert  [86] . As future steps, we plan to also include evaluations of uncertainty predictions into MULTIBENCH, such as using the recently proposed Uncertainty Toolkit [2,  30, 153] . This will enable the inclusion and evaluation of uncertainty-predicting multimodal models such as the ones proposed in  [20, 169] .",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "I.3.2 Robustness To Distribution Shifts",
      "text": "Distribution shifts, spanning shifts in dataset distributions and label distributions, are among core challenges currently preventing machine learning systems from being safely deployed in real-world settings  [130] . Subtle changes in the data distribution can significantly impact performance, a phenomenon exemplified by adversarial examples  [146] , and shifts in the label distribution can significantly compromise accuracy as well  [185] .\n\nDistribution shifts in multimodal settings have not been explored by the research community. Multimodal data can exhibit shifts in the marginal data distribution of each modality as well as in the joint distribution across modalities, which makes the problem inherently more complex. To enable research in benchmarking and analyzing distribution shift in multimodal settings, we plan to include:\n\n1. Data: Data partitions (or new datasets) to MULTIBENCH that test for generalization across domains and subpopulations, in a manner similar to  [85] . Building on the current datasets available in MULTIBENCH, some examples include affect recognition across different users, robotic manipulation across different physical robots, and medical diagnosis across different age groups. 2. Algorithms: On the algorithmic side, we plan to include currently established methods for distribution shift in a single modality (which has been the bulk of existing work) into MULTIZOO, which will enable both theoreticians and practitioners to analyze the new challenges that multimodal data brings to the study of distribution shift. 3. Evaluation: Finally, to evaluate robustness to distribution shift, we plan to build a standardized evaluation pipeline into MULTIBENCH (in a similar way for robustness tests currently implemented). We will also tap into insights from the experimental protocol in  [130]  which includes evaluation metrics to detect dataset shift before attempting to correct it.",
      "page_start": 95,
      "page_end": 96
    },
    {
      "section_name": "I.3.3 Fairness",
      "text": "To safely deploy human-centric multimodal models in real-world scenarios such as healthcare, HCI, legal systems, and social science, it is necessary to recognize the role they play in shaping social biases and stereotypes. Recent work has shown that word-level embeddings reflect and propagate social biases present in training corpora  [18, 23] . Machine learning systems that incorporate these word embeddings can further amplify biases  [13]  and unfairly discriminate against users, particularly those from disadvantaged social groups. Similar observations have been observed for datasets and models in the visual domain such as facial recognition  [6]  and image captioning  [67]  tasks, which has called for immediate efforts towards better documentation and risk analysis of both ML datasets  [54]  and models  [115] .\n\nWe believe that the ability to make fair judgments is even more important in a multimodal setting for the following reasons:\n\n1. Human behavior is inherently multimodal. As a result, many research problems in multimodal learning involve human-centric data and tasks such as healthcare, affective computing, HCI, multimedia, human-robot interaction. As multimodal systems (such as emotion recognition systems) are deployed in the real world, it is crucial to characterize possible social biases they encode and design algorithms to mitigate these biases. Otherwise, real harm can be brought to under-represented populations which unfair machine learning models disproportionately harm  [18] . 2. While there has been a large body of work investigating the fairness of representations learned from language and images, there is little work currently investigating this for other modalities, as well as for the wide spectrum of multimodal models integrating multiple modalities which can potentially compound biases stemming from each one  [141] .\n\nThere are many definitions of fairness and bias in ML and it is unclear which are important in which multimodal settings. While we do not have the best answer to conclusively evaluate for fairness in multimodal systems, we are making it a priority to include this feature in future versions of MULTIBENCH. In reference to  [113] , certain dimensions of fairness we are currently exploring and plan to add to MULTIBENCH include:\n\n1. Data: A better fine-grained understanding of bias in data, which we plan to achieve via human annotations for several multimodal datasets in MULTIBENCH (especially those that involve human-centric tasks such as affect recognition). 2. Algorithms: Algorithmic fairness, including training models that satisfy individual and group fairness, analyzing trained models from a geometric perspective (i.e., studying whether biases are encoded in representations learned by a model  [18, 99] ), and methods for preprocessing and post-processing data and models to satisfy fairness metrics. 3. Evaluation: Bias evaluation of trained multimodal models as well as those trained within a single modality, to determine the relationship between biases in a single modality versus",
      "page_start": 96,
      "page_end": 96
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: MULTIBENCH contains a diverse set of 15 datasets spanning 10 modalities and testing for more than",
      "page": 2
    },
    {
      "caption": "Figure 1: ), a systematic and uniﬁed large-scale benchmark",
      "page": 2
    },
    {
      "caption": "Figure 2: MULTIZOO provides a standardized implementation of a suite of multimodal methods in a modular",
      "page": 5
    },
    {
      "caption": "Figure 2: ). To introduce these algorithms, we use the simple setting with 2 modalities",
      "page": 5
    },
    {
      "caption": "Figure 3: Tradeoff between performance and complexity. Size of circles shows variance in performance",
      "page": 8
    },
    {
      "caption": "Figure 3: (a), we summarize the performance",
      "page": 8
    },
    {
      "caption": "Figure 3: (a)), they perform much better",
      "page": 8
    },
    {
      "caption": "Figure 3: (b)). We hope that the release of MULTIBENCH will greatly",
      "page": 8
    },
    {
      "caption": "Figure 4: , we plot a similar tradeoff plot",
      "page": 8
    },
    {
      "caption": "Figure 4: (a)), implying that",
      "page": 8
    },
    {
      "caption": "Figure 4: (b)) because several well-performing methods such as MULT, CCA, and",
      "page": 8
    },
    {
      "caption": "Figure 4: Tradeoff between performance and robustness. Size of circles shows variance in robustness across",
      "page": 9
    },
    {
      "caption": "Figure 5: MULTIBENCH provides a standardized machine learning pipeline across data processing, data loading,",
      "page": 28
    },
    {
      "caption": "Figure 6: Affective computing studies the perception of human affective states (emotions, sentiment, and",
      "page": 29
    },
    {
      "caption": "Figure 7: Healthcare: Medical decision-making often involves integrating complementary signals from several",
      "page": 32
    },
    {
      "caption": "Figure 8: Robotics: Modern robot systems are equipped with multiple sensors to aid in their decision-making. We",
      "page": 34
    },
    {
      "caption": "Figure 9: Finance: We scrape historical stock data from the internet and create our own dataset for ﬁnancial",
      "page": 35
    },
    {
      "caption": "Figure 10: Human Computer Interaction (HCI) studies the design and use of computer technology with",
      "page": 36
    },
    {
      "caption": "Figure 11: Multimedia: A signiﬁcant body of research in multimodal learning has been fueled by the large",
      "page": 39
    },
    {
      "caption": "Figure 12: Robustness of multimodal models with increasing levels of noise on the MUSTARD dataset in the",
      "page": 73
    },
    {
      "caption": "Figure 13: Robustness of multimodal models with increasing levels of noise on the CMU-MOSI dataset in the",
      "page": 73
    },
    {
      "caption": "Figure 14: Robustness of multimodal models with increasing levels of noise on the UR-FUNNY dataset in the",
      "page": 73
    },
    {
      "caption": "Figure 15: Robustness of multimodal models with increasing levels of noise on the CMU-MOSEI dataset in the",
      "page": 73
    },
    {
      "caption": "Figure 12: , CMU-MOSI in Figure 13, UR-FUNNY in Figure 14, and CMU-MOSEI in Figure 15.",
      "page": 74
    },
    {
      "caption": "Figure 16: Robustness of multimodal models with increasing levels of noise on the MIMIC dataset in the",
      "page": 76
    },
    {
      "caption": "Figure 16: We highlight the following observations:",
      "page": 76
    },
    {
      "caption": "Figure 17: and on VISION&TOUCH in Figure 18. We highlight the following observations:",
      "page": 77
    },
    {
      "caption": "Figure 17: Robustness of multimodal models with increasing levels of noise on the MUJOCO PUSH dataset in",
      "page": 78
    },
    {
      "caption": "Figure 18: Robustness of multimodal models with increasing levels of noise on the VISION&TOUCH dataset in",
      "page": 79
    },
    {
      "caption": "Figure 19: We highlight the following observations:",
      "page": 80
    },
    {
      "caption": "Figure 19: Robustness of multimodal models with increasing levels of noise on the stock prediction datasets in",
      "page": 81
    },
    {
      "caption": "Figure 20: We highlight the following",
      "page": 83
    },
    {
      "caption": "Figure 20: Robustness of multimodal models with increasing levels of noise on the ENRICO dataset in the HCI",
      "page": 84
    },
    {
      "caption": "Figure 21: We highlight the following observations:",
      "page": 86
    },
    {
      "caption": "Figure 21: Robustness of multimodal models with increasing levels of noise on the MM-IMDB dataset in the",
      "page": 87
    },
    {
      "caption": "Figure 22: Relative performance of each model across in-domain (red dots) and out-domain datasets (blue dots).",
      "page": 88
    },
    {
      "caption": "Figure 22: , we plot the performance of each multimodal method across all datasets that it",
      "page": 88
    },
    {
      "caption": "Figure 23: , we color-code the performance on",
      "page": 88
    },
    {
      "caption": "Figure 23: Relative performance of each model across different domains. We ﬁnd that the performance of",
      "page": 89
    },
    {
      "caption": "Figure 22: , we ﬁnd that many methods show their strongest performance on in-domain",
      "page": 89
    },
    {
      "caption": "Figure 22: , we also observe high variance in the performance of multimodal meth-",
      "page": 89
    },
    {
      "caption": "Figure 23: , we ﬁnd that performance also varies signiﬁcantly across research areas.",
      "page": 89
    },
    {
      "caption": "Figure 24: Tradeoff between performance and complexity. Size of circles shows variance in performance",
      "page": 90
    },
    {
      "caption": "Figure 24: (a). We aggregate performance and complexity statistics by ﬁrst performing min-max",
      "page": 90
    },
    {
      "caption": "Figure 24: , we plot a dotted blue line of best quadratic ﬁt to show the Pareto frontier",
      "page": 90
    },
    {
      "caption": "Figure 24: (b), we plot the",
      "page": 91
    },
    {
      "caption": "Figure 24: (a)), they",
      "page": 91
    },
    {
      "caption": "Figure 24: (b)). Therefore, it is important for future research to focus on models that",
      "page": 91
    },
    {
      "caption": "Figure 25: Tradeoff between performance and robustness. Size of circles shows variance in performance and",
      "page": 92
    },
    {
      "caption": "Figure 26: Overall tradeoff between performance and robustness obtained by averaging the relative and",
      "page": 92
    },
    {
      "caption": "Figure 25: We show the line of best linear ﬁt in dotted blue. There is only a",
      "page": 92
    },
    {
      "caption": "Figure 25: , we plot a similar tradeoff plot between accuracy",
      "page": 92
    },
    {
      "caption": "Figure 25: We observe a slight positive correlation between performance and relative",
      "page": 93
    },
    {
      "caption": "Figure 26: as an overall",
      "page": 93
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Research Area": "Affective Computing",
          "Size\nDataset\nModalities\n# Samples\nPrediction task": "{(cid:96), v, a}\n690\nS\nMUSTARD [24]\nsarcasm\n{(cid:96), v, a}\n2, 199\nM\nCMU-MOSI [181]\nsentiment\n{(cid:96), v, a}\n16, 514\nL\nUR-FUNNY [64]\nhumor\n{(cid:96), v, a}\n22, 777\nL\nCMU-MOSEI [183]\nsentiment, emotions"
        },
        {
          "Research Area": "Healthcare",
          "Size\nDataset\nModalities\n# Samples\nPrediction task": "{t, ta}\n36, 212\nL\nMIMIC [78]\nmortality, ICD-9 codes"
        },
        {
          "Research Area": "Robotics",
          "Size\nDataset\nModalities\n# Samples\nPrediction task": "{i, f, p}\n37, 990\nM\nMUJOCO PUSH [90]\nobject pose\n{i, f, p}\n147, 000\nL\nVISION&TOUCH [92]\ncontact, robot pose"
        },
        {
          "Research Area": "Finance",
          "Size\nDataset\nModalities\n# Samples\nPrediction task": "{t × 18}\n5, 218\nM\nSTOCKS-F&B\nstock price, volatility\n{t × 63}\n5, 218\nM\nSTOCKS-HEALTH\nstock price, volatility\n{t × 100}\n5, 218\nM\nSTOCKS-TECH\nstock price, volatility"
        },
        {
          "Research Area": "HCI",
          "Size\nDataset\nModalities\n# Samples\nPrediction task": "{i, s}\n1, 460\nS\nENRICO [93]\ndesign interface"
        },
        {
          "Research Area": "Multimedia",
          "Size\nDataset\nModalities\n# Samples\nPrediction task": "{v, a, o}\n2, 624\nS\nKINETICS400-S [80]\nhuman action\n{(cid:96), i}\n25, 959\nM\nMM-IMDB [8]\nmovie genre\n{i, a}\n70, 000\nM\nAV-MNIST [161]\ndigit\n{v, a, o}\n306, 245\nL\nKINETICS400-L [80]\nhuman action"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Category": "Data",
          "Method": "WORDALIGN [26]",
          "Alignment": "(cid:51)",
          "Complementarity": "(cid:55)",
          "Robustness": "(cid:55)"
        },
        {
          "Category": "Model",
          "Method": "EF, LF [10]\nTF [179], LRTF [106]\nMI-MATRIX, MI-VECTOR, MI-SCALAR [77]\nNL GATE [167]\nMULT [154]\nMFAS [122]",
          "Alignment": "(cid:55)\n(cid:55)\n(cid:55)\n(cid:55)\n(cid:51)\n(cid:55)",
          "Complementarity": "(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)\n(cid:51)",
          "Robustness": "(cid:55)\n(cid:55)\n(cid:55)\n(cid:55)\n(cid:55)\n(cid:55)"
        },
        {
          "Category": "Objective",
          "Method": "CCA [7]\nREFNET [135]\nMFM [155]\nMVAE [168]\nMCTN [123]",
          "Alignment": "(cid:51)\n(cid:51)\n(cid:55)\n(cid:55)\n(cid:55)",
          "Complementarity": "(cid:55)\n(cid:55)\n(cid:51)\n(cid:51)\n(cid:55)",
          "Robustness": "(cid:55)\n(cid:55)\n(cid:55)\n(cid:55)\n(cid:51)"
        },
        {
          "Category": "Training",
          "Method": "GRADBLEND [167]\nRMFE [53]",
          "Alignment": "(cid:55)\n(cid:55)",
          "Complementarity": "(cid:51)\n(cid:51)",
          "Robustness": "(cid:51)\n(cid:51)"
        }
      ],
      "page": 55
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "GRU Encoder",
          "Model": "GRU",
          "Parameter": "Input sizes\nHidden sizes\nNum of layers\nDropout",
          "Value": "[5, 20, 35, 74, 300, 704]\n[32, 32, 64, 128, 512, 1024]\n1 or 2\n0.0 or 0.1"
        },
        {
          "Component": "Transformer Encoder [158]",
          "Model": "Transformer [158]",
          "Parameter": "Input sizes\nHidden sizes\nNum heads\nDropout",
          "Value": "[5, 20, 35, 74, 300, 704]\n[5, 10, 20, 40, 40, 50]\n2 or 3\n0.2"
        },
        {
          "Component": "Head",
          "Model": "MLP",
          "Parameter": "Input sizes\nHidden sizes\nNum layers\nDropout",
          "Value": "[5, 20, 32, 64, 128, 256]\n[5, 20, 32, 64, 128, 256]\n2\n0.2"
        },
        {
          "Component": "MCTN [123] Encoder",
          "Model": "GRU",
          "Parameter": "Input sizes\nHidden sizes\nNum of layers\nDropout",
          "Value": "300\n[32, 64]\n1 or 2\n0.0 or 0.1"
        },
        {
          "Component": "MCTN [123] Decoder",
          "Model": "GRU",
          "Parameter": "Input sizes\nHidden sizes\nNum of layers\nDropout",
          "Value": "[32, 64]\n300\n1 or 2\n0.0 or 0.1"
        },
        {
          "Component": "MCTN [123] Seq2Seq",
          "Model": "GRU+GRU",
          "Parameter": "teaching ratio\nEmbed sizes\nµt1 , µc, µt2",
          "Value": "0.5\n32\n0.01"
        },
        {
          "Component": "Fusion",
          "Model": "LRTF [106]",
          "Parameter": "Num ranks\nOutput sizes",
          "Value": "64\n128"
        },
        {
          "Component": "",
          "Model": "MI-MATRIX [77]",
          "Parameter": "Hidden size",
          "Value": "128"
        },
        {
          "Component": "",
          "Model": "MULT [156]",
          "Parameter": "Hidden size\nNum heads",
          "Value": "40\n8 or 10"
        },
        {
          "Component": "Training",
          "Model": "",
          "Parameter": "Loss\nBatch size\nSeq Length\nNum epochs\nEarly stop\nPatience\nActivation\nOptimizer\nWeight Decay\nLearning rate",
          "Value": "MAE or Cross Entropy\n32\n50 or 20\n100 or 300\nTrue\n[8, 20]\nReLU\nAdamW\n1 × 10−4\n1 × 10−4"
        }
      ],
      "page": 62
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Static Encoder",
          "Model": "2-layer MLP",
          "Parameter": "Hidden sizes\nActivation",
          "Value": "[10, 10]\nLeakyReLU(0.2)"
        },
        {
          "Component": "Static Decoder",
          "Model": "2-layer MLP",
          "Parameter": "Layer sizes\nActivation",
          "Value": "[200, 40, 5]\nLeakyReLU(0.2)"
        },
        {
          "Component": "Time Series Encoder",
          "Model": "GRU",
          "Parameter": "Hidden dim",
          "Value": "30"
        },
        {
          "Component": "Time Series Decoder",
          "Model": "GRU",
          "Parameter": "Hidden dim",
          "Value": "30"
        },
        {
          "Component": "Classiﬁcation Head",
          "Model": "2-Layer MLP",
          "Parameter": "Hidden size\nActivation",
          "Value": "40\nLeakyReLU(0.2)"
        },
        {
          "Component": "Fusion",
          "Model": "LRTF [106]",
          "Parameter": "Output dim\nRanks",
          "Value": "100\n40"
        },
        {
          "Component": "",
          "Model": "NL-Gate [167]",
          "Parameter": "thw-dim/c-dim/tf-dim\nkey linear\nvalue linear",
          "Value": "24/30/10\n[10, 300]\n[10, 300]"
        },
        {
          "Component": "",
          "Model": "MI-Matrix [77]",
          "Parameter": "output dim",
          "Value": "100"
        },
        {
          "Component": "Training",
          "Model": "Unimodal, LF, LRTF,\nMI-Matrix, NL-gate",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning rate",
          "Value": "Cross Entropy\n40\n20\nRMSprop\n0.001"
        },
        {
          "Component": "",
          "Model": "GRADBLEND [167]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning Rate\nGB-epoch\nv-rate\nﬁnetune epoch",
          "Value": "Cross Entropy\n40\n300\nSGD\n0.005\n20\n0.8\n25"
        },
        {
          "Component": "",
          "Model": "MVAE [168]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning Rate\nCross Entropy Weight\nLatent Representation Fusion",
          "Value": "Cross Entropy + ELBO\n40\n30\nAdam\n0.001\n2.0\nProductOfExpert"
        },
        {
          "Component": "",
          "Model": "MFM [155]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning Rate\nRecon Loss Modality Weights\nCross Entropy Weight\nIntermediate Modules",
          "Value": "Cross Entropy\n+ Reconstruction(MSE)\n40\n30\nAdam\n0.001\n[1, 1]\n2.0\nMLPs [200, 100, 100],\n[200, 100, 100], [400, 100, 100]"
        },
        {
          "Component": "",
          "Model": "MFAS [122]",
          "Parameter": "Batch size\nEpochs/search iters\nNum samples/surrogates per epoch\nη max/min/Ti/Tm\nTemperature init/ﬁnal/decay\nMax progression level\nSurrogate learning rate\nSurrogate hidden size\nSurrogate embedding size\nSearch space\nOptimizer\nRepresentation Size",
          "Value": "32\n3/3/6\n15/50\n10−3/10−6/1/2\n10.0/0.2/4.0\n4\n0.001\n100\n100\n(3, 3, 2)\nAdam\n16"
        }
      ],
      "page": 65
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Pos Encoder",
          "Model": "Linear",
          "Parameter": "Hidden sizes",
          "Value": "[64, 64, 64 (residual)]"
        },
        {
          "Component": "Sensors Encoder",
          "Model": "Linear",
          "Parameter": "Hidden sizes",
          "Value": "[64, 64, 64 (residual)]"
        },
        {
          "Component": "Image Encoder",
          "Model": "CNN",
          "Parameter": "Filter sizes\nNum ﬁlters\nFilter strides\nFilter padding",
          "Value": "[5, 3, 3, 3, 3]\n[32, 32, 32, 16, 8]\n1\n[2, 1, 1, 1, 1]"
        },
        {
          "Component": "Control Encoder",
          "Model": "Linear",
          "Parameter": "Hidden sizes",
          "Value": "[64, 64, 64 (residual)]"
        },
        {
          "Component": "Fusion",
          "Model": "Early Fusion &\nUnimodal LSTM",
          "Parameter": "Hidden size\nNum layers",
          "Value": "512\n2"
        },
        {
          "Component": "",
          "Model": "Late Fusion LSTM",
          "Parameter": "Hidden size\nNum layers",
          "Value": "256\n1"
        },
        {
          "Component": "",
          "Model": "MULT [156]",
          "Parameter": "Embed size\nNum heads",
          "Value": "64\n4"
        },
        {
          "Component": "Classiﬁcation Head",
          "Model": "Linear",
          "Parameter": "Hidden size",
          "Value": "64"
        },
        {
          "Component": "Training",
          "Model": "",
          "Parameter": "Loss\nBatch size\nNum epochs\nActivation\nOptimizer\nLearning rate",
          "Value": "Mean Squared Error\n32\n20\nReLU\nAdam\n10−5"
        }
      ],
      "page": 66
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Image Encoder",
          "Model": "CNN",
          "Parameter": "Filter sizes\nNum ﬁlters\nFilter strides\nFilter padding",
          "Value": "[7, 5, 5, 3, 3, 3]\n[16, 32, 64, 64, 128, 128]\n[2, 2, 2, 2, 2, 2]\nSame"
        },
        {
          "Component": "Force Encoder",
          "Model": "Causal Convolution\n[157]",
          "Parameter": "Filter sizes\nNum ﬁlters\nFilter strides\nFilter padding",
          "Value": "[2, 2, 2, 2, 2]\n[16, 32, 64, 128, 256]\n[2, 2, 2, 2, 2]\n1"
        },
        {
          "Component": "Proprio Encoder",
          "Model": "Linear",
          "Parameter": "Hidden sizes",
          "Value": "[32, 64, 128, 256]"
        },
        {
          "Component": "Depth Encoder",
          "Model": "CNN",
          "Parameter": "Filter sizes\nNum ﬁlters\nFilter strides\nFilter padding",
          "Value": "[3, 3, 4, 3, 3, 3]\n[32, 64, 64, 64, 128, 128]\n[2, 2, 2, 2, 2, 2]\nSame"
        },
        {
          "Component": "Action Encoder",
          "Model": "Linear",
          "Parameter": "Hidden sizes",
          "Value": "[32, 32]"
        },
        {
          "Component": "Classiﬁcation Head",
          "Model": "2-Layer MLP",
          "Parameter": "Hidden size\nActivation",
          "Value": "128\nLeakyReLU(0.2)"
        },
        {
          "Component": "Fusion",
          "Model": "LRTF [106]",
          "Parameter": "Output dim\nRanks",
          "Value": "200\n40"
        },
        {
          "Component": "",
          "Model": "Sensor Fusion [91]",
          "Parameter": "z-dim",
          "Value": "128"
        },
        {
          "Component": "Training",
          "Model": "",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning rate",
          "Value": "Contact: Cross Entropy\nEnd-Effector: MSE\n64\nSensor Fusion: 50\nLRTF: 35; Others: 15\nAdam\nContact: 10−4\nEnd-Effector: 5 × 10−4"
        },
        {
          "Component": "",
          "Model": "REFNET\n[135]",
          "Parameter": "Loss\nBatch size\nOptimizer/Learning Rate\nReﬁner\nSelf Loss Weight",
          "Value": "Cross Entropy + Contrast\n40\nAdam / 0.0005\nMLP(1056, 2000, 65760)\n0.0001"
        }
      ],
      "page": 66
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Unimodal &\nEarly Fusion LSTM",
          "Parameter": "Hidden dim",
          "Value": "128"
        },
        {
          "Model": "Late Fusion LSTM",
          "Parameter": "Hidden dim",
          "Value": "16"
        },
        {
          "Model": "TRANSFORMER [158]",
          "Parameter": "Embed dim\nNum heads\nLayers",
          "Value": "9\n3\n3"
        },
        {
          "Model": "MULT [154]",
          "Parameter": "Embed dim\nNum heads\nLayers",
          "Value": "9\n3\n3"
        },
        {
          "Model": "GRADBLEND [167] LSTM",
          "Parameter": "Hidden dim",
          "Value": "128"
        },
        {
          "Model": "Training",
          "Parameter": "Loss\nBatch size\nMax seq length\nActivation\nOptimizer\nLearning rate",
          "Value": "Mean Squared Error\n16\n500\nReLU\nAdam\n10−3"
        },
        {
          "Model": "",
          "Parameter": "Num epochs",
          "Value": "2\nUnimodal, EF\nLF, Transformer,\n4\nMULT, GRADBLEND"
        }
      ],
      "page": 67
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Unimodal",
          "Parameter": "Hidden dim",
          "Value": "16"
        },
        {
          "Model": "Late Fusion",
          "Parameter": "Hidden dim",
          "Value": "32"
        },
        {
          "Model": "GRADBLEND [167]",
          "Parameter": "Hidden dim",
          "Value": "32"
        },
        {
          "Model": "REFNET\n[135]",
          "Parameter": "Hidden dim",
          "Value": "32"
        },
        {
          "Model": "MI-Matrix [77]",
          "Parameter": "Hidden dim\nInput dims",
          "Value": "32\n16, 16"
        },
        {
          "Model": "Tensor Matrix",
          "Parameter": "Hidden dim\nInput dims",
          "Value": "32\n16, 16"
        },
        {
          "Model": "LRTF [106]",
          "Parameter": "Hidden dim\nInput dims\nRank",
          "Value": "32\n16, 16\n20"
        },
        {
          "Model": "CCA [145]",
          "Parameter": "Hidden dim",
          "Value": "32"
        },
        {
          "Model": "Training",
          "Parameter": "Loss\nBatch size\nActivation\nDropout\nOptimizer\nLearning rate",
          "Value": "Class-weighted Cross Entropy\n32\nReLU\n0.2\nAdam\n10−5"
        },
        {
          "Model": "",
          "Parameter": "Num epochs",
          "Value": "50"
        }
      ],
      "page": 67
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Image Encoder",
          "Model": "LeNet-3",
          "Parameter": "Filter Sizes\nNum Filters\nFilter Strides / Filter Paddings\nMax Pooling",
          "Value": "[5, 3, 3, 3]\n[6, 12, 24, 48]\n[1, 1, 1, 1] /[2, 1, 1, 1]\n[2, 2, 2, 2]"
        },
        {
          "Component": "Image Decoder",
          "Model": "DeLeNet-3",
          "Parameter": "Filter Sizes\nNum Filters\nFilter Strides / Filter Paddings",
          "Value": "[4, 4, 4, 8]\n[24, 12, 6, 3]\n[2, 2, 2, 4]/[1, 1, 1, 1]"
        },
        {
          "Component": "Audio Encoder",
          "Model": "LeNet-5",
          "Parameter": "Filter Sizes\nNum Filters\nFilter Strides / Filter Paddings\nMax Pooling",
          "Value": "[5, 3, 3, 3, 3, 3]\n[6, 12, 24, 48, 96, 192]\n[1, 1, 1, 1, 1, 1]/[2, 1, 1, 1, 1, 1]\n[2, 2, 2, 2, 2, 2]"
        },
        {
          "Component": "Audio Decoder",
          "Model": "DeLeNet-5",
          "Parameter": "Filter Sizes\nNum Filters\nFilter Strides / Filter Paddings",
          "Value": "[4, 4, 4, 4, 4, 8]\n[96, 48, 24, 12, 6, 3]\n[2, 2, 2, 2, 2, 4]/[1, 1, 1, 1, 1, 1]"
        },
        {
          "Component": "Classiﬁcation Head",
          "Model": "2-Layer MLP",
          "Parameter": "Hidden size\nActivation",
          "Value": "100\nLeakyReLU(0.2)"
        },
        {
          "Component": "Fusion",
          "Model": "LRTF [106]",
          "Parameter": "Output dim\nRanks",
          "Value": "120\n40"
        },
        {
          "Component": "",
          "Model": "MI-Matrix [77]",
          "Parameter": "output dim",
          "Value": "240"
        },
        {
          "Component": "Training",
          "Model": "Unimodal, LF,\nLRTF, MI-Matrix",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer/Learning rate/weight decay",
          "Value": "Cross Entropy\n40\nLRTF: 30, Others: 25\nSGD/0.05/0.0001"
        },
        {
          "Component": "",
          "Model": "GRADBLEND [167]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer/Learning rate\nGB-epoch/ﬁnetune-epoch\nv-rate",
          "Value": "Cross Entropy\n40\n300\nSGD/0.05\n10/25\n0.8"
        },
        {
          "Component": "",
          "Model": "MVAE [168]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer/Learning rate\nCross Entropy Weight\nLatent Representation Fusion",
          "Value": "Cross Entropy + ELBO\n40\n20\nAdam/0.001\n2.0\nProductOfExpert"
        },
        {
          "Component": "",
          "Model": "MFM [155]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer/Learning rate\nRecon Loss Modality Weights\nCross Entropy Weight\nIntermediate Modules",
          "Value": "Cross Entropy\n+ Reconstruction(MSE)\n40\n25\nAdam/0.001\n[1, 1]\n2.0\nMLPs [200, 100, 100],\n[200, 100, 100], [400, 100, 100]"
        },
        {
          "Component": "",
          "Model": "MFAS [122]",
          "Parameter": "Batch size\nMain epochs/search iters/epochs per model\nNum samples/surrogates per epoch\nη max/min/Ti/Tm\nTemperature init/ﬁnal/decay\nMax progression level\nSurrogate learning rate\nSurrogate hidden/embedding size\nSearch space\nOptimizer\nRepresentation Size",
          "Value": "32\n3/3/6\n15/50\n10−3/10−6/ 1/2\n10.0/0.2/4.0\n4\n0.001\n100/100\n(3, 5, 2)\nAdam\n16"
        },
        {
          "Component": "",
          "Model": "CCA [145]",
          "Parameter": "Batch size\nLoss\nOptimizer/Learning Rate/Weight Decay",
          "Value": "800\nCCALoss\nAdamW/ 0.01/0.01"
        },
        {
          "Component": "",
          "Model": "REFNET\n[135]",
          "Parameter": "Loss\nBatch size\nOptimizer/Learning Rate\nReﬁner\nSelf Loss Weight",
          "Value": "Cross Entropy + Contrast\n40\nSGD / 0.05\nMLP(384, 1000, 13328)\n0.1"
        }
      ],
      "page": 68
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Text Encoder",
          "Model": "2-Layer MaxoutMLP",
          "Parameter": "Hidden size\nOutput dim\nMLP num",
          "Value": "512\n128/256/512\n2"
        },
        {
          "Component": "Image Encoder",
          "Model": "2-Layer MaxoutMLP",
          "Parameter": "Hidden size\nOutput dim\nMLP num",
          "Value": "1024\n128/256/512\n2"
        },
        {
          "Component": "Classiﬁcation Head",
          "Model": "Linear",
          "Parameter": "",
          "Value": ""
        },
        {
          "Component": "",
          "Model": "2-Layer MLP",
          "Parameter": "Hidden size\nActivation",
          "Value": "512\nReLU"
        },
        {
          "Component": "",
          "Model": "2-Layer Maxout_Linear",
          "Parameter": "Hidden size\nMLP num",
          "Value": "512\n2"
        },
        {
          "Component": "Fusion",
          "Model": "Concatenate",
          "Parameter": "",
          "Value": ""
        },
        {
          "Component": "",
          "Model": "LRTF [106]",
          "Parameter": "Output dim\nRanks",
          "Value": "512\n128"
        },
        {
          "Component": "",
          "Model": "MI-Matrix [77]",
          "Parameter": "output dim",
          "Value": "1024"
        },
        {
          "Component": "Training",
          "Model": "Unimodal, EF, LF,\nLRTF, MI-Matrix",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning rate\nWeight decay",
          "Value": "Binary Cross Entropy\n128\nText: 125, Image: 25, LF:5,\nEF/LRTF:15, MI-Matrix:20\nAdamW\nUnimodal: 0.0001, EF: 0.04,\nLF/LRTF/MI-Matrix: 0.008\n0.01"
        },
        {
          "Component": "",
          "Model": "CCA [145]",
          "Parameter": "Loss\nCCA weight\nBatch size\nNum epochs\nOptimizer\nLearning rate\nWeight decay",
          "Value": "Binary Cross Entropy + CCA\n0.001\n800\n20\nAdamW\n0.01\n0.01"
        },
        {
          "Component": "",
          "Model": "RMFE [53]",
          "Parameter": "Loss\nRegularization weight\nBatch size\nNum epochs\nOptimizer\nLearning rate\nWeight decay",
          "Value": "Binary Cross Entropy\n+ Regularization\n1e − 10\n128\n10\nAdamW\n0.01\n0.01"
        },
        {
          "Component": "",
          "Model": "REFNET [135]",
          "Parameter": "Loss\nContrast weight\nSelf-supervised weight\nBatch size\nNum epochs\nOptimizer\nLearning rate\nWeight decay",
          "Value": "Binary Cross Entropy\n+ Contrast + Self-supervised\n0.0001\n0.1\n128\n10\nAdamW\n0.01\n0.01"
        },
        {
          "Component": "",
          "Model": "MFM [155]",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning rate\nRecon Loss Modality Weight\nCross Entropy Weight\nIntermediate Modules",
          "Value": "Binary Cross Entropy\n+ Reconstruction(MSE)\n128\n10\nAdam\n0.005\n[1, 1]\n2.0\nMLP [512, 256, 256]\nMLP [512, 256, 256]\nMLP [1024, 512, 256]"
        }
      ],
      "page": 69
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Component": "Video Encoder",
          "Model": "ResNet [66] + LSTM",
          "Parameter": "ResNet Version\nLSTM Hidden size",
          "Value": "18-layer\n64"
        },
        {
          "Component": "Audio Encoder",
          "Model": "ResNet [66] + 2-Layer MLP",
          "Parameter": "ResNet Version\nMLP hidden size\nMLP output size\nMLP activation",
          "Value": "50-layer\n200\n64\nReLU"
        },
        {
          "Component": "Classiﬁcation Head",
          "Model": "Linear",
          "Parameter": "",
          "Value": ""
        },
        {
          "Component": "",
          "Model": "2-Layer MLP",
          "Parameter": "Hidden size\nActivation",
          "Value": "200\nReLU"
        },
        {
          "Component": "Fusion",
          "Model": "Concatenate",
          "Parameter": "",
          "Value": ""
        },
        {
          "Component": "Training",
          "Model": "Unimodal, LF",
          "Parameter": "Loss\nBatch size\nNum epochs\nOptimizer\nLearning rate",
          "Value": "Cross Entropy\n16\n15\nAdam\n0.0001"
        }
      ],
      "page": 70
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (a)\nUnimodal (v)",
          "MUSTARD\nAcc(2) ↑": "68.6 ± 0.4\n64.9 ± 0.4\n65.7 ± 0.7",
          "CMU-MOSI\nAcc(2) ↑": "74.2 ± 0.5\n65.5 ± 0.2\n66.3 ± 0.3",
          "UR-FUNNY\nAcc(2) ↑": "58.3 ± 0.2\n57.2 ± 0.9\n57.3 ± 0.5",
          "CMU-MOSEI\nAcc(2) ↑": "78.8 ± 1.5\n66.4 ± 0.7\n67.2 ± 0.4"
        },
        {
          "Dataset\nMetric": "EF-GRU\nLF-GRU\nEF-TRANSFORMER\nLF-TRANSFORMER\nTF [179]\nLRTF [106]\nMI-MATRIX [77]\nMULT [154]",
          "MUSTARD\nAcc(2) ↑": "66.3 ± 0.3\n66.1 ± 0.9\n65.3 ± 1.4\n66.1 ± 0.9\n62.1 ± 2.2\n65.2 ± 1.5\n61.8 ± 0.3\n71.8 ± 0.3",
          "CMU-MOSI\nAcc(2) ↑": "73.2 ± 2.2\n75.2 ± 0.8\n78.8 ± 0.4\n79.6 ± 0.4\n74.4 ± 0.2\n76.3 ± 0.3\n73.9 ± 0.4\n83.0 ± 0.1",
          "UR-FUNNY\nAcc(2) ↑": "60.2 ± 0.5\n62.5 ± 0.5\n62.9 ± 0.2\n63.4 ± 0.3\n61.2 ± 0.4\n62.7 ± 0.2\n61.9 ± 0.3\n66.7 ± 0.3",
          "CMU-MOSEI\nAcc(2) ↑": "78.4 ± 0.6\n79.2 ± 0.4\n79.6 ± 0.3\n80.6 ± 0.3\n79.4 ± 0.5\n79.6 ± 0.6\n76.5 ± 0.4\n82.1 ± 0.5"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]\nMCTN [123]",
          "MUSTARD\nAcc(2) ↑": "66.3 ± 0.3\n64.5 ± 0.4\n63.2 ± 1.4",
          "CMU-MOSI\nAcc(2) ↑": "78.1 ± 0.9\n77.2 ± 0.3\n76.9 ± 2.1",
          "UR-FUNNY\nAcc(2) ↑": "62.4 ± 1.1\n62.0 ± 0.5\n63.2 ± 0.8",
          "CMU-MOSEI\nAcc(2) ↑": "79.4 ± 0.7\n79.1 ± 0.2\n76.4 ± 0.4"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "MUSTARD\nAcc(2) ↑": "66.1 ± 0.3",
          "CMU-MOSI\nAcc(2) ↑": "75.5 ± 0.5",
          "UR-FUNNY\nAcc(2) ↑": "62.3 ± 0.3",
          "CMU-MOSEI\nAcc(2) ↑": "78.1 ± 0.3"
        }
      ],
      "page": 71
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (v)\nUnimodal (a)",
          "MUSTARD\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "43\n381\n0.12\n2347\n0.33\n0.12\n48\n56\n0.01\n2288\n0.24\n0.01\n69\n288\n0.001\n2288\n0.25\n0.001"
        },
        {
          "Dataset\nMetric": "EF-GRU\nLF-GRU\nEF-TRANSFORMER\nLF-TRANSFORMER\nTF [179]\nLRTF [106]\nMULT [154]",
          "MUSTARD\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "126\n168\n0.84\n2291\n0.34\n0.84\n74\n52\n1.52\n2307\n0.40\n1.52\n30\n601\n1.86\n2423\n0.79\n1.86\n42\n1868\n14.0\n2586\n1.02\n14.0\n46\n1370\n14.7\n2542\n1.62\n14.7\n33\n49\n0.68\n2483\n0.50\n0.68\n31\n2414\n1.93\n3345\n3.01\n1.93"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]\nMCTN [123]",
          "MUSTARD\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "40\n2138\n4.85\n2417\n1.48\n4.33\n33\n4645\n4.32\n2695\n2.11\n4.05\n100\n1026\n0.19\n2359\n1.02\n0.19"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "MUSTARD\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "100\n6012\n1.95\n2406\n0.42\n1.58"
        }
      ],
      "page": 72
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (v)\nUnimodal (a)",
          "CMU-MOSI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "30\n590\n0.17\n2347\n0.49\n0.17\n35\n71\n0.01\n2288\n0.36\n0.01\n188\n346\n0.001\n2288\n0.38\n0.001"
        },
        {
          "Dataset\nMetric": "EF-GRU\nLF-GRU\nEF-TRANSFORMER\nLF-TRANSFORMER\nTF [179]\nLRTF [106]\nMULT [154]",
          "CMU-MOSI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "106\n221\n1.42\n2291\n0.44\n1.42\n14\n60\n1.84\n2307\n0.58\n1.84\n20\n635\n2.18\n2423\n1.07\n2.18\n33\n2011\n15.1\n2586\n2.12\n15.1\n35\n384\n12.2\n2867\n2.38\n12.2\n43\n172\n0.82\n2454\n0.59\n0.82\n22\n2414\n2.38\n3345\n4.30\n2.38"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]\nMCTN [123]",
          "CMU-MOSI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "31\n1692\n5.53\n2455\n1.52\n4.98\n35\n3820\n5.31\n2564\n2.03\n4.69\n100\n1149\n0.19\n2366\n0.98\n0.19"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "CMU-MOSI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "300\n18869\n3.91\n2355\n0.59\n1.86"
        }
      ],
      "page": 72
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (v)\nUnimodal (a)",
          "UR-FUNNY\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "32\n602\n1.99\n6524\n1.82\n1.99\n29\n70\n0.14\n6528\n1.61\n0.14\n40\n1039\n0.03\n6599\n1.66\n0.03"
        },
        {
          "Dataset\nMetric": "EF-GRU\nLF-GRU\nEF-TRANSFORMER\nLF-TRANSFORMER\nTF [179]\nLRTF [106]\nMULT [154]",
          "UR-FUNNY\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "34\n612\n3.58\n6535\n2.51\n3.58\n10\n498\n2.28\n6791\n3.25\n2.28\n32\n2358\n4.87\n7086\n3.81\n4.87\n33\n6024\n34.5\n7288\n6.75\n34.5\n32\n2780\n21.3\n7165\n6.35\n21.3\n25\n2057\n1.05\n6931\n3.32\n1.05\n30\n8096\n5.01\n9572\n12.1\n5.01"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]\nMCTN [123]",
          "UR-FUNNY\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "30\n5123\n6.89\n6970\n10.3\n6.23\n32\n10670\n6.59\n7038\n12.1\n6.10\n100\n10857\n0.19\n6578\n4.39\n0.19"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "UR-FUNNY\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "100\n19212\n4.12\n6832\n3.42\n2.31"
        }
      ],
      "page": 72
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (v)\nUnimodal (a)",
          "CMU-MOSEI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "23\n561\n1.80\n5830\n1.79\n1.80\n27\n647\n0.12\n5817\n1.46\n0.12\n39\n910\n0.03\n5818\n1.48\n0.03"
        },
        {
          "Dataset\nMetric": "EF-GRU\nLF-GRU\nEF-TRANSFORMER\nLF-TRANSFORMER\nTF [179]\nLRTF [106]\nMULT [154]",
          "CMU-MOSEI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "22\n548\n3.23\n5835\n2.01\n3.23\n9\n443\n2.08\n5996\n2.55\n2.08\n30\n1658\n4.49\n6082\n2.88\n4.49\n35\n5504\n31.5\n6996\n5.65\n31.5\n30\n2784\n22.6\n6337\n5.89\n22.6\n22\n2057\n0.78\n6102\n2.45\n0.78\n32\n6033\n4.75\n7572\n10.1\n4.75"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]\nMCTN [123]",
          "CMU-MOSEI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "33\n5340\n6.65\n6088\n9.42\n5.97\n40\n11673\n6.21\n6782\n12.0\n5.89\n100\n12242\n0.19\n6526\n4.84\n0.19"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "CMU-MOSEI\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "100\n18176\n3.89\n6042\n2.63\n2.25"
        }
      ],
      "page": 72
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Most frequent",
          "MIMIC MORTALITY\nAcc(6) ↑": "76.1",
          "MIMIC ICD-9 10 − 19\nAcc(2) ↑\nAUPRC(2) ↑": "−\n83.1",
          "MIMIC ICD-9 70 − 79\nAcc(2) ↑\nAUPRC(2) ↑": "−\n52.5"
        },
        {
          "Dataset\nMetric": "Unimodal (t)\nUnimodal (ta)",
          "MIMIC MORTALITY\nAcc(6) ↑": "76.7 ± 0.3\n76.4 ± 0.2",
          "MIMIC ICD-9 10 − 19\nAcc(2) ↑\nAUPRC(2) ↑": "83.6 ± 0.1\n35.0 ± 0.9\n91.4 ± 0.0\n68.4 ± 0.1",
          "MIMIC ICD-9 70 − 79\nAcc(2) ↑\nAUPRC(2) ↑": "67.6 ± 0.4\n72.9 ± 0.3\n56.3 ± 0.3\n54.6 ± 0.4"
        },
        {
          "Dataset\nMetric": "LF\nLRTF [106]\nMI-MATRIX [77]\nNL GATE [167]\nMFAS [122]",
          "MIMIC MORTALITY\nAcc(6) ↑": "77.9 ± 0.3\n78.2 ± 0.3\n77.6 ± 0.4\n78.1 ± 0.2\n77.9 ± 0.2",
          "MIMIC ICD-9 10 − 19\nAcc(2) ↑\nAUPRC(2) ↑": "91.5 ± 0.1\n74.2 ± 0.7\n91.5 ± 0.1\n75.1 ± 0.3\n91.5 ± 0.1\n74.2 ± 0.6\n91.6 ± 0.1\n73.8 ± 0.7\n91.4 ± 0.0\n70.3 ± 1.2",
          "MIMIC ICD-9 70 − 79\nAcc(2) ↑\nAUPRC(2) ↑": "68.9 ± 0.5\n74.3 ± 0.4\n68.5 ± 0.4\n73.8 ± 0.4\n67.9 ± 0.3\n73.0 ± 0.5\n68.7 ± 0.5\n74.3 ± 0.4\n68.5 ± 0.4\n73.7 ± 0.4"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]",
          "MIMIC MORTALITY\nAcc(6) ↑": "78.2 ± 0.3\n78.0 ± 0.3",
          "MIMIC ICD-9 10 − 19\nAcc(2) ↑\nAUPRC(2) ↑": "91.5 ± 0.1\n75.0 ± 0.5\n91.6 ± 0.1\n73.5 ± 1.4",
          "MIMIC ICD-9 70 − 79\nAcc(2) ↑\nAUPRC(2) ↑": "68.8 ± 0.4\n74.4 ± 0.4\n68.7 ± 0.6\n74.0 ± 0.7"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "MIMIC MORTALITY\nAcc(6) ↑": "78.2 ± 0.2",
          "MIMIC ICD-9 10 − 19\nAcc(2) ↑\nAUPRC(2) ↑": "91.5 ± 0.1\n74.1 ± 0.4",
          "MIMIC ICD-9 70 − 79\nAcc(2) ↑\nAUPRC(2) ↑": "68.0 ± 0.7\n73.2 ± 0.5"
        }
      ],
      "page": 75
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (t)\nUnimodal (ta)",
          "MIMIC\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "20\n46.4\n0.019\n2360\n0.41\n0.019\n20\n34.6\n0.001\n2359\n0.39\n0.001"
        },
        {
          "Dataset\nMetric": "LF\nLRTF [106]\nMI-MATRIX [77]\nNL GATE [167]\nMFAS [122]",
          "MIMIC\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "20\n49.4\n0.034\n2362\n0.41\n0.034\n50\n261\n0.008\n2575\n0.41\n0.008\n20\n56.6\n0.801\n2377\n0.39\n0.801\n20\n51.4\n0.040\n2422\n0.43\n0.040\n42 × 6\n3762\n0.086∗\n2360\n1.79\n0.016"
        },
        {
          "Dataset\nMetric": "MFM [155]\nMVAE [168]",
          "MIMIC\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "25\n221\n0.323\n2438\n0.85\n0.315\n30\n486\n0.312\n2553\n0.89\n0.305"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "MIMIC\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "300\n2785\n0.063\n2575\n0.45\n0.034"
        }
      ],
      "page": 75
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (f )\nUnimodal (p)\nUnimodal (c)",
          "MUJOCO PUSH\nMSE ↓": "0.334 ± 0.034\n4.266 ± 0.085\n3.885 ± 0.004\n3.804 ± 0.005"
        },
        {
          "Dataset\nMetric": "EF-LSTM\nLF-LSTM\nTF [179]\nMULT [156]",
          "MUJOCO PUSH\nMSE ↓": "0.363 ± 0.038\n0.290 ± 0.018\n0.574 ± 0.059\n0.402 ± 0.026"
        }
      ],
      "page": 77
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (f )\nUnimodal (p)",
          "VISION&TOUCH CONTACT\nAcc(2) ↑": "83.6 ± 0.3\n93.6 ± 0.1\n85.6 ± 0.6",
          "VISION&TOUCH END EFFECTOR\nMSE (×10−4) ↓": "1.99 ± 0.160\n87.2 ± 0.477\n0.202 ± 0.022"
        },
        {
          "Dataset\nMetric": "LF\nSensor Fusion [91]\nLRTF [106]",
          "VISION&TOUCH CONTACT\nAcc(2) ↑": "93.6 ± 0.1\n93.4 ± 0.1\n93.3 ± 0.1",
          "VISION&TOUCH END EFFECTOR\nMSE (×10−4) ↓": "0.185 ± 0.011\n0.258 ± 0.011\n0.232 ± 0.031"
        },
        {
          "Dataset\nMetric": "REFNET [135]",
          "VISION&TOUCH CONTACT\nAcc(2) ↑": "93.5 ± 0.1",
          "VISION&TOUCH END EFFECTOR\nMSE (×10−4) ↓": "0.203 ± 0.025"
        }
      ],
      "page": 77
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (f )\nUnimodal (p)\nUnimodal (c)",
          "MUJOCO PUSH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "20\n738 ± 133\n3.88\n3607 ± 1\n3.46 ± 0.02\n3.88\n20\n288 ± 39\n3.33\n3595 ± 2\n0.91 ± 0.08\n3.33\n20\n252 ± 6\n3.33\n3594 ± 1\n0.87 ± 0.04\n3.33\n20\n372 ± 64\n3.33\n3594 ± 1\n0.86 ± 0.04\n3.33"
        },
        {
          "Dataset\nMetric": "EF\nLF-LSTM\nTF-LSTM [179]\nMULT [156]",
          "MUJOCO PUSH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "20\n815 ± 34\n3.92\n3654 ± 1\n4.44 ± 0.55\n3.92\n20\n856 ± 46\n1.90\n3636 ± 1\n4.32 ± 0.45\n1.90\n20\n1914 ± 31\n23.5\n4530 ± 9\n7.75 ± 0.12\n23.5\n20\n4792 ± 62\n14.6\n6530 ± 16\n22.4 ± 0.28\n14.6"
        }
      ],
      "page": 78
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (f )\nUnimodal (p)",
          "VISION&TOUCH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n2633\n1.00\n5530\n63.9\n1.00\n15\n2185\n0.13\n2426\n51.6\n0.13\n15\n2514\n0.08\n2389\n59.5\n0.08"
        },
        {
          "Dataset\nMetric": "LF\nSensor Fusion [91]\nLRTF [106]",
          "VISION&TOUCH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n2672\n1.20\n5572\n64.4\n1.20\n50\n11604\n1.10\n4467\n62.6\n1.10\n35\n8366\n1.09\n4987\n64.4\n1.09"
        },
        {
          "Dataset\nMetric": "REFNET [135]",
          "VISION&TOUCH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n3819\n135\n6067\n65.0\n1.20"
        }
      ],
      "page": 78
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Mean",
          "STOCKS-F&B\nMSE ↓": "2.140",
          "STOCKS-HEALTH\nMSE ↓": "0.575",
          "STOCK-TECH\nMSE ↓": "0.140"
        },
        {
          "Dataset\nMetric": "ARIMA\nUnimodal",
          "STOCKS-F&B\nMSE ↓": "2.199\n1.856 ± 0.093",
          "STOCKS-HEALTH\nMSE ↓": "0.620\n0.541 ± 0.010",
          "STOCK-TECH\nMSE ↓": "0.152\n0.125 ± 0.004"
        },
        {
          "Dataset\nMetric": "EF-LSTM\nLF-LSTM\nEF-TRANSFORMER\nLF-TRANSFORMER\nMULT [156]",
          "STOCKS-F&B\nMSE ↓": "1.835 ± 0.098\n1.893 ± 0.106\n2.144 ± 0.014\n2.155 ± 0.023\n2.053 ± 0.022",
          "STOCKS-HEALTH\nMSE ↓": "0.526 ± 0.017\n0.541 ± 0.018\n0.573 ± 0.006\n0.573 ± 0.006\n0.555 ± 0.005",
          "STOCK-TECH\nMSE ↓": "0.121 ± 0.003\n0.120 ± 0.008\n0.143 ± 0.003\n0.143 ± 0.004\n0.135 ± 0.003"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "STOCKS-F&B\nMSE ↓": "1.820 ± 0.138",
          "STOCKS-HEALTH\nMSE ↓": "0.537 ± 0.011",
          "STOCK-TECH\nMSE ↓": "0.138 ± 0.030"
        }
      ],
      "page": 80
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (t)",
          "STOCKS-F&B\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "9.5 ± 0.1\n0.067\n3028 ± 3\n0.50 ± 0.01\n0.067\n2"
        },
        {
          "Dataset\nMetric": "EF-LSTM\nLF-LSTM\nEF-Transformer\nLF-Transformer\nMulT [156]",
          "STOCKS-F&B\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "9.7 ± 0.1\n0.069\n3067 ± 21\n0.51 ± 0.01\n0.069\n2\n62 ± 0.4\n0.005\n2433 ± 4\n1.74 ± 0.02\n0.005\n4\n25 ± 0.3\n0.118\n2434 ± 3\n0.62 ± 0.01\n0.118\n4\n88 ± 0.3\n0.472\n2468 ± 1\n1.70 ± 0.00\n0.472\n4\n160 ± 1\n0.125\n3313 ± 1\n4.82 ± 0.06\n0.125\n4"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "STOCKS-F&B\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "409 ± 2\n0.338\n3102 ± 1\n0.44 ± 0.01\n0.069\n4"
        }
      ],
      "page": 81
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (t)",
          "STOCKS-HEALTH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "9.6 ± 0.1\n0.067\n3032 ± 15\n0.51 ± 0.01\n0.067\n2"
        },
        {
          "Dataset\nMetric": "EF-LSTM\nLF-LSTM\nEF-Transformer\nLF-Transformer\nMulT [156]",
          "STOCKS-HEALTH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "9.6 ± 0.1\n0.070\n3083 ± 2\n0.51 ± 0.02\n0.070\n2\n108 ± 1\n0.009\n2464 ± 7\n2.89 ± 0.04\n0.009\n4\n25 ± 0.4\n0.118\n2466 ± 4\n0.65 ± 0.02\n0.118\n4\n159 ± 1\n0.826\n2524 ± 1\n2.93 ± 0.01\n0.826\n4\n162 ± 1\n0.125\n3315 ± 1\n4.88 ± 0.04\n0.125\n4"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "STOCKS-HEALTH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "582 ± 4\n0.541\n3141 ± 2\n0.49 ± 0.01\n0.070\n4"
        }
      ],
      "page": 81
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (t)",
          "STOCK-TECH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "9.5 ± 0.1\n0.067\n3023 ± 1\n0.51 ± 0.01\n0.067\n2"
        },
        {
          "Dataset\nMetric": "EF-LSTM\nLF-LSTM\nEF-Transformer\nLF-Transformer\nMulT [156]",
          "STOCK-TECH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "9.6 ± 0.1\n0.070\n3075 ± 4\n0.53 ± 0.01\n0.070\n2\n92 ± 0.5\n0.007\n2453 ± 4\n2.51 ± 0.04\n0.007\n4\n25 ± 0.4\n0.118\n2453 ± 1\n0.63 ± 0.01\n0.118\n4\n135 ± 1\n0.708\n2506 ± 1\n2.52 ± 0.00\n0.708\n4\n161 ± 1\n0.125\n3315 ± 2\n4.79 ± 0.03\n0.125\n4"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "STOCK-TECH\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "500 ± 3\n0.473\n3167 ± 1\n0.44 ± 0.01\n0.070\n4"
        }
      ],
      "page": 81
    },
    {
      "caption": "Table 22: Complexity results for datasets in the HCI domain. U: unimodal models, M: multimodal fusion",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (s)",
          "ENRICO\nAcc(20) ↑": "47.0 ± 1.6\n46.1 ± 1.3"
        },
        {
          "Dataset\nMetric": "LF\nTF [179]\nLRTF [106]\nMI-MATRIX [77]",
          "ENRICO\nAcc(20) ↑": "50.8 ± 2.0\n46.6 ± 1.9\n47.1 ± 2.9\n46.7 ± 2.4"
        },
        {
          "Dataset\nMetric": "CCA [145]\nREFNET [135]",
          "ENRICO\nAcc(20) ↑": "50.1 ± 1.4\n44.4 ± 2.2"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "ENRICO\nAcc(20) ↑": "51.0 ± 1.4"
        }
      ],
      "page": 83
    },
    {
      "caption": "Table 22: Complexity results for datasets in the HCI domain. U: unimodal models, M: multimodal fusion",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (s)",
          "ENRICO\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "50\n1601\n9.6\n2796\n7.3\n19.3\n50\n1644\n9.6\n2771\n8.1\n19.3"
        },
        {
          "Dataset\nMetric": "LF\nTF [179]\nLRTF [106]\nMI-MATRIX [77]",
          "ENRICO\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "50\n1714\n19.3\n2730\n8.7\n19.3\n50\n2012\n19.3\n2718\n10.9\n19.3\n50\n1853\n19.3\n2717\n9.7\n19.3\n50\n1604\n19.3\n2730\n8.5\n19.3"
        },
        {
          "Dataset\nMetric": "CCA [145]\nREFNET [135]",
          "ENRICO\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "50\n2945\n19.3\n2923\n9.1\n19.3\n50\n1747\n25.7\n2757\n13.8\n25.7"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "ENRICO\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "50\n2618\n19.3\n2610\n12.1\n19.3"
        }
      ],
      "page": 83
    },
    {
      "caption": "Table 23: and results on complexity in Table 24. Here we list some",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (i)",
          "MM-IMDB\nMicro F1(23) ↑\nMacro F1(23) ↑": "58.6 ± 1.3\n45.6 ± 4.5\n40.1 ± 1.3\n25.3 ± 0.6"
        },
        {
          "Dataset\nMetric": "EF\nLF\nLRTF [106]\nMI-MATRIX [77]",
          "MM-IMDB\nMicro F1(23) ↑\nMacro F1(23) ↑": "58.9 ± 2.6\n49.8 ± 1.7\n58.8 ± 1.6\n49.2 ± 2.0\n59.2 ± 0.5\n49.2 ± 0.6\n58.3 ± 1.0\n48.0 ± 1.1"
        },
        {
          "Dataset\nMetric": "CCA [145]\nREFNET [135]\nMFM [155]",
          "MM-IMDB\nMicro F1(23) ↑\nMacro F1(23) ↑": "59.3 ± 1.2\n50.2 ± 0.9\n59.2 ± 2.7\n50.2 ± 1.4\n38.4 ± 1.6\n22.3 ± 1.3"
        },
        {
          "Dataset\nMetric": "RMFE",
          "MM-IMDB\nMicro F1(23) ↑\nMacro F1(23) ↑": "58.6 ± 2.3\n47.1 ± 2.0"
        }
      ],
      "page": 85
    },
    {
      "caption": "Table 23: and results on complexity in Table 24. Here we list some",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (a)",
          "AV-MNIST\nAcc(10) ↑": "65.1 ± 0.2\n42.0 ± 0.2"
        },
        {
          "Dataset\nMetric": "LF\nLRTF [106]\nMI-MATRIX [77]\nMFAS [122]",
          "AV-MNIST\nAcc(10) ↑": "71.7 ± 0.4\n71.5 ± 0.5\n71.2 ± 0.5\n72.8 ± 0.2"
        },
        {
          "Dataset\nMetric": "CCA [145]\nREFNET [135]\nMFM [155]\nMVAE [168]",
          "AV-MNIST\nAcc(10) ↑": "71.9 ± 0.4\n70.9 ± 0.6\n71.8 ± 0.4\n72.3 ± 0.2"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "AV-MNIST\nAcc(10) ↑": "68.5 ± 0.5"
        }
      ],
      "page": 85
    },
    {
      "caption": "Table 23: and results on complexity in Table 24. Here we list some",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (v)\nUnimodal (a)",
          "KINETICS-S\nAcc(5) ↑": "56.5\n39.7",
          "KINETICS-L\nAcc(400) ↑": "72.6\n19.7"
        },
        {
          "Dataset\nMetric": "LF",
          "KINETICS-S\nAcc(5) ↑": "56.1",
          "KINETICS-L\nAcc(400) ↑": "71.7"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "KINETICS-S\nAcc(5) ↑": "23.7",
          "KINETICS-L\nAcc(400) ↑": "74.7"
        }
      ],
      "page": 85
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal ((cid:96))\nUnimodal (i)",
          "MM-IMDB\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "125\n622\n0.55\n2146\n2.07\n0.55\n25\n127\n4.86\n2176\n2.14\n4.86"
        },
        {
          "Dataset\nMetric": "EF\nLF\nLRTF [106]\nMI-MATRIX [77]\nMFM [155]\nCCA [145]\nRMFE [53]\nREFNET [135]",
          "MM-IMDB\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n117\n5.05\n2010\n3.24\n5/05\n5\n45\n10.3\n2016\n3.44\n10.3\n15\n741\n10.3\n2448\n5.57\n10.3\n20\n735\n280\n4036\n3.59\n280\n10\n78\n21.3\n2038\n3.36\n10.9\n20\n1025\n9.51\n2273\n3.33\n9.51\n10\n104\n8.78\n22297\n3.46\n8.78\n10\n2207\n27.0\n2899\n3.47\n10.3"
        }
      ],
      "page": 86
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (i)\nUnimodal (a)",
          "AV-MNIST\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "25\n106\n0.02\n9549\n0.95\n0.02\n25\n158\n0.24\n11895\n1.35\n0.24"
        },
        {
          "Dataset\nMetric": "LF\nMI-MATRIX [77]\nLRTF [106]\nMFAS [122]",
          "AV-MNIST\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "25\n260\n0.26\n11917\n1.20\n0.26\n25\n289\n2.53\n11509\n1.21\n2.53\n30\n470\n0.25\n11610\n1.25\n0.25\n172 × 6\n17648\n0.14∗\n9444\n4.39\n0.07"
        },
        {
          "Dataset\nMetric": "CCA [145]\nREFNET [135]\nMFM [155]\nMVAE [168]",
          "AV-MNIST\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "25\n310\n0.25\n9548\n1.42\n0.25\n15\n1179\n14.01\n15931\n4.39\n0.28\n25\n544\n0.92\n9570\n4.76\n0.45\n20\n679\n0.81\n9755\n4.98\n0.34"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "AV-MNIST\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "300\n12539\n0.29\n12029\n1.51\n0.26"
        }
      ],
      "page": 86
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (v)\nUnimodal (a)",
          "KINETICS-SMALL\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n6702\n12.0\n12151\n13.7\n12.0\n15\n46767\n25.8\n8533\n60.9\n25.8"
        },
        {
          "Dataset\nMetric": "",
          "KINETICS-SMALL\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n20283\n37.8\n9525\n13.9\n37.8"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "KINETICS-SMALL\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "15\n20283\n37.8\n9525\n13.9\n37.8"
        }
      ],
      "page": 86
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nMetric": "Unimodal (v)\nUnimodal (a)",
          "KINETICS-LARGE\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "45\n938280\n12.0\n12151\n1918\n12.0\n45\n947380\n33.5\n8533\n8526\n33.5"
        },
        {
          "Dataset\nMetric": "",
          "KINETICS-LARGE\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "45\n2839620\n45.5\n9525\n1946\n45.5"
        },
        {
          "Dataset\nMetric": "GRADBLEND [167]",
          "KINETICS-LARGE\nEpochs\nTraining\nTraining\nTraining peak\nInference\nInference\ntrained\ntime (s)\nparams (M)\nmemory (MB)\ntime (s)\nparams (M)": "45\n2839620\n45.5\n9525\n45.5\n1946"
        }
      ],
      "page": 86
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Free spoken digit dataset (fsdd)",
      "venue": "Free spoken digit dataset (fsdd)"
    },
    {
      "citation_id": "2",
      "title": "Analyzing the behavior of visual question answering models",
      "authors": [
        "Aishwarya Agrawal",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "3",
      "title": "VQA: Visual question answering",
      "authors": [
        "Aishwarya Agrawal",
        "Jiasen Lu",
        "Stanislaw Antol",
        "Margaret Mitchell",
        "C Zitnick",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2017",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "Overview of artificial intelligence in medicine",
      "authors": [
        "Paras Malik Amisha",
        "Monika Pathania",
        "Vyas Kumar"
      ],
      "year": "2019",
      "venue": "Journal of family medicine and primary care"
    },
    {
      "citation_id": "5",
      "title": "An own-age bias in face recognition for children and older adults",
      "authors": [
        "S Jeffrey",
        "Matthew Anastasi",
        "Rhodes"
      ],
      "year": "2005",
      "venue": "Psychonomic bulletin & review"
    },
    {
      "citation_id": "6",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "7",
      "title": "Gated multimodal units for information fusion",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y Gómez",
        "Fabio González"
      ],
      "year": "2017",
      "venue": "5th International conference on learning representations 2017 workshop"
    },
    {
      "citation_id": "8",
      "title": "Learning representations by maximizing mutual information across views",
      "authors": [
        "Philip Bachman",
        "Devon Hjelm",
        "William Buchwalter"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrušaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "11",
      "title": "Proceedings of the first workshop on NLP and computational social science",
      "authors": [
        "A David Bamman",
        "Seza",
        "Jacob Dogruöz",
        "Dirk Eisenstein",
        "David Hovy",
        "Jurgens",
        "O' Brendan",
        "Alice Connor",
        "Oren Oh",
        "Svitlana Tsur",
        "Volkova"
      ],
      "year": "2016",
      "venue": "Proceedings of the first workshop on NLP and computational social science"
    },
    {
      "citation_id": "12",
      "title": "Big data's disparate impact",
      "authors": [
        "Solon Barocas",
        "Andrew Selbst"
      ],
      "year": "2016",
      "venue": "Calif. L. Rev"
    },
    {
      "citation_id": "13",
      "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with rgb-d video",
      "authors": [
        "Maria Bauzá",
        "Ferran Alet",
        "Yen-Chen Lin",
        "Tomás Lozano-Pérez",
        "Leslie Kaelbling",
        "Phillip Isola",
        "Alberto Rodriguez"
      ],
      "year": "2019",
      "venue": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with rgb-d video"
    },
    {
      "citation_id": "14",
      "title": "Synthetic and natural noise both break neural machine translation",
      "authors": [
        "Yonatan Belinkov",
        "Yonatan Bisk"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "15",
      "title": "Social robots for education: A review",
      "authors": [
        "Tony Belpaeme",
        "James Kennedy",
        "Aditi Ramachandran",
        "Brian Scassellati",
        "Fumihide Tanaka"
      ],
      "year": "2018",
      "venue": "Science robotics"
    },
    {
      "citation_id": "16",
      "title": "Neural synergy between kinetic vision and touch",
      "authors": [
        "Randolph Blake",
        "Kenith Sobel",
        "Thomas James"
      ],
      "year": "2004",
      "venue": "Psychological science"
    },
    {
      "citation_id": "17",
      "title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings",
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Zou",
        "Venkatesh Saligrama",
        "Adam Kalai"
      ],
      "year": "2016",
      "venue": "NIPS"
    },
    {
      "citation_id": "18",
      "title": "A review paper: Noise models in digital image processing",
      "authors": [
        "Ajay Kumar",
        "Brijendra Kumar"
      ],
      "year": "2015",
      "venue": "A review paper: Noise models in digital image processing"
    },
    {
      "citation_id": "19",
      "title": "Uncertainty quantification in multimodal ensembles of deep learners",
      "authors": [
        "Katherine Brown",
        "Farzana Ahamed Bhuiyan",
        "Douglas Talbert"
      ],
      "year": "2020",
      "venue": "The Thirty-Third International Flairs Conference"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Reducing unimodal biases for visual question answering",
      "authors": [
        "Remi Cadene",
        "Corentin Dancette",
        "Matthieu Cord",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "authors": [
        "Aylin Caliskan",
        "Joanna Bryson",
        "Arvind Narayanan"
      ],
      "year": "2017",
      "venue": "Science"
    },
    {
      "citation_id": "23",
      "title": "Towards multimodal sarcasm detection (an _obviously_ perfect paper)",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "Verónica Pérez-Rosas",
        "Roger Zimmermann",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Gated-attention architectures for task-oriented language grounding",
      "authors": [
        "Devendra Singh Chaplot",
        "Kanthashree Sathyendra",
        "Rama Kumar Pasumarthi",
        "Dheeraj Rajagopal",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "Minghai Chen",
        "Sen Wang",
        "Paul Liang",
        "Tadas Baltrušaitis",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Vocal quality factors: Analysis, synthesis, and perception",
      "authors": [
        "G Donald",
        "C Childers",
        "Lee"
      ],
      "year": "1991",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "28",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "29",
      "title": "Beyond pinball loss: Quantile methods for calibrated uncertainty quantification",
      "authors": [
        "Youngseog Chung",
        "Willie Neiswanger",
        "Ian Char",
        "Jeff Schneider"
      ],
      "year": "2020",
      "venue": "Beyond pinball loss: Quantile methods for calibrated uncertainty quantification",
      "arxiv": "arXiv:2011.09588"
    },
    {
      "citation_id": "30",
      "title": "Using syntax to ground referring expressions in natural images",
      "authors": [
        "Volkan Cirik",
        "Taylor Berg-Kirkpatrick",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Visual referring expression recognition: What do systems actually learn?",
      "authors": [
        "Volkan Cirik",
        "Louis-Philippe Morency",
        "Taylor Berg-Kirkpatrick"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "32",
      "title": "Unsupervised natural language inference via decoupled multimodal contrastive learning",
      "authors": [
        "Wanyun Cui",
        "Guangyu Zheng",
        "Wei Wang"
      ],
      "year": "2020",
      "venue": "Unsupervised natural language inference via decoupled multimodal contrastive learning"
    },
    {
      "citation_id": "33",
      "title": "Law and word order: Nlp in legal tech",
      "authors": [
        "Robert Dale"
      ],
      "year": "2019",
      "venue": "Natural Language Engineering"
    },
    {
      "citation_id": "34",
      "title": "Practicing differential privacy in health care: A review",
      "year": "2013",
      "venue": "Trans. Data Priv"
    },
    {
      "citation_id": "35",
      "title": "Embodied question answering",
      "authors": [
        "Abhishek Das",
        "Samyak Datta",
        "Georgia Gkioxari",
        "Stefan Lee",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Language modeling with gated convolutional networks",
      "authors": [
        "Angela Yann N Dauphin",
        "Michael Fan",
        "David Auli",
        "Grangier"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "37",
      "title": "See, hear, explore: Curiosity via audio-visual association",
      "authors": [
        "Victoria Dean",
        "Shubham Tulsiani",
        "Abhinav Gupta"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Rico: A mobile app dataset for building data-driven design applications",
      "authors": [
        "Biplab Deka",
        "Zifeng Huang",
        "Chad Franzen",
        "Joshua Hibschman",
        "Daniel Afergan",
        "Yang Li",
        "Jeffrey Nichols",
        "Ranjitha Kumar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "40",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "41",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT (1)"
    },
    {
      "citation_id": "42",
      "title": "Human-computer interaction. Harlow ua",
      "authors": [
        "Alan Dix",
        "Janet Finlay",
        "Gregory Abowd",
        "Russell Beale"
      ],
      "year": "2000",
      "venue": "Human-computer interaction. Harlow ua"
    },
    {
      "citation_id": "43",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "44",
      "title": "Clotho: An audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "Joint robust voicing detection and pitch estimation based on residual harmonics",
      "authors": [
        "Thomas Drugman",
        "Abeer Alwan"
      ],
      "year": "2011",
      "venue": "Interspeech"
    },
    {
      "citation_id": "46",
      "title": "Multimodal interfaces: A survey of principles, models and frameworks",
      "authors": [
        "Bruno Dumas",
        "Denis Lalanne",
        "Sharon Oviatt"
      ],
      "year": "2009",
      "venue": "Human machine interaction"
    },
    {
      "citation_id": "47",
      "title": "Audio-visual speech modeling for continuous speech recognition",
      "authors": [
        "Stéphane Dupont",
        "Juergen Luettin"
      ],
      "year": "2000",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "48",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "Paul Ekman"
      ],
      "venue": "Universal facial expressions of emotion"
    },
    {
      "citation_id": "49",
      "title": "A survey of current datasets for vision and language research",
      "authors": [
        "Francis Ferraro",
        "Nasrin Mostafazadeh",
        "Ting-Hao Huang",
        "Lucy Vanderwende",
        "Jacob Devlin",
        "Michel Galley",
        "Margaret Mitchell"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "50",
      "title": "On the classification of emotional biosignals evoked while viewing affective pictures: an integrated data-mining-based approach for healthcare applications",
      "authors": [
        "Christos Frantzidis",
        "Charalampos Bratsas",
        "Manousos Klados",
        "Evdokimos Konstantinidis",
        "Chrysa Lithari",
        "Ana Vivas",
        "Christos Papadelis",
        "Eleni Kaldoudi",
        "Costas Pappas",
        "Panagiotis Bamidis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "51",
      "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "year": "2016",
      "venue": "international conference on machine learning"
    },
    {
      "citation_id": "52",
      "title": "Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies",
      "authors": [
        "Itai Gat",
        "Idan Schwartz",
        "Alexander Schwing",
        "Tamir Hazan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "53",
      "title": "",
      "authors": [
        "Timnit Gebru",
        "Jamie Morgenstern",
        "Briana Vecchione",
        "Jennifer Vaughan",
        "Hanna Wallach",
        "Hal Daumé",
        "Kate Crawford"
      ],
      "year": "2018",
      "venue": "",
      "arxiv": "arXiv:1803.09010"
    },
    {
      "citation_id": "54",
      "title": "Differentially private federated learning: A client level perspective",
      "authors": [
        "Robin Geyer",
        "Tassilo Klein",
        "Moin Nabi"
      ],
      "year": "2017",
      "venue": "Differentially private federated learning: A client level perspective",
      "arxiv": "arXiv:1712.07557"
    },
    {
      "citation_id": "55",
      "title": "What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis",
      "authors": [
        "Dimitris Gkoumas",
        "Qiuchi Li",
        "Christina Lioma",
        "Yijun Yu",
        "Dawei Song"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "56",
      "title": "Probabilistic forecasts, calibration and sharpness",
      "authors": [
        "Tilmann Gneiting",
        "Fadoua Balabdaoui",
        "Adrian Raftery"
      ],
      "year": "2007",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)"
    },
    {
      "citation_id": "57",
      "title": "Maxout networks",
      "authors": [
        "Ian Goodfellow",
        "David Warde-Farley",
        "Mehdi Mirza",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2013",
      "venue": "Proceedings of the 30th International Conference on Machine Learning"
    },
    {
      "citation_id": "58",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "Multimodal toolkit",
      "authors": [
        "Ken Gu"
      ],
      "year": "2020",
      "venue": "Multimodal toolkit"
    },
    {
      "citation_id": "60",
      "title": "",
      "authors": [
        "David Ha",
        "Andrew Dai",
        "Quoc V Le",
        "Hypernetworks"
      ],
      "year": "2016",
      "venue": "",
      "arxiv": "arXiv:1609.09106"
    },
    {
      "citation_id": "61",
      "title": "Accessible ui design and multimodal interaction through hybrid tv platforms: towards a virtualuser centered design framework",
      "authors": [
        "Pascal Hamisu",
        "Gregor Heinrich",
        "Christoph Jung",
        "Volker Hahn",
        "Carlos Duarte",
        "Pat Langdon",
        "Pradipta Biswas"
      ],
      "year": "2011",
      "venue": "International Conference on Universal Access in Human-Computer Interaction"
    },
    {
      "citation_id": "62",
      "title": "Manymodalqa: Modality disambiguation and qa over diverse inputs",
      "authors": [
        "Darryl Hannan",
        "Akshay Jain",
        "Mohit Bansal"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "63",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amirali Bagher Zadeh",
        "Jianyuan Zhong",
        "Md Iftekhar Tanveer",
        "Louis-Philippe Morency",
        "Mohammed Hoque"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "64",
      "title": "Multimodal Data Collection Made Easy: The EZ-MMLA Toolkit: A Data Collection Website That Provides Educators and Researchers with Easy Access to Multimodal Data Streams",
      "authors": [
        "Javaria Hassan",
        "Jovin Leong",
        "Bertrand Schneider"
      ],
      "year": "2021",
      "venue": "Multimodal Data Collection Made Easy: The EZ-MMLA Toolkit: A Data Collection Website That Provides Educators and Researchers with Easy Access to Multimodal Data Streams"
    },
    {
      "citation_id": "65",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "66",
      "title": "Women also snowboard: Overcoming bias in captioning models",
      "authors": [
        "Anne Lisa",
        "Kaylee Hendricks",
        "Kate Burns",
        "Trevor Saenko",
        "Anna Darrell",
        "Rohrbach"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "67",
      "title": "Does my multimodal model learn cross-modal interactions? it's harder to tell than you might think! In EMNLP",
      "authors": [
        "Jack Hessel",
        "Lillian Lee"
      ],
      "year": "2020",
      "venue": "Does my multimodal model learn cross-modal interactions? it's harder to tell than you might think! In EMNLP"
    },
    {
      "citation_id": "68",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "69",
      "title": "A picture is worth a thousand words: Multimodal sensemaking of the global financial crisis",
      "authors": [
        "Markus Hollerer",
        "Dennis Jancsary",
        "Maria Grafstrom"
      ],
      "year": "2018",
      "venue": "Organization Studies"
    },
    {
      "citation_id": "70",
      "title": "Deep multimodal multilinear fusion with high-order polynomial pooling",
      "authors": [
        "Ming Hou",
        "Jiajia Tang",
        "Jianhai Zhang",
        "Wanzeng Kong",
        "Qibin Zhao"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "71",
      "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "authors": [
        "Junjie Hu",
        "Sebastian Ruder",
        "Aditya Siddhant",
        "Graham Neubig",
        "Orhan Firat",
        "Melvin Johnson"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "72",
      "title": "Transformer is all you need: Multimodal multitask learning with a unified transformer",
      "authors": [
        "Ronghang Hu",
        "Amanpreet Singh"
      ],
      "year": "2021",
      "venue": "Transformer is all you need: Multimodal multitask learning with a unified transformer",
      "arxiv": "arXiv:2102.10772"
    },
    {
      "citation_id": "73",
      "title": "Open graph benchmark: Datasets for machine learning on graphs",
      "authors": [
        "Weihua Hu",
        "Matthias Fey",
        "Marinka Zitnik",
        "Yuxiao Dong",
        "Hongyu Ren",
        "Bowen Liu",
        "Michele Catasta",
        "Jure Leskovec"
      ],
      "year": "2020",
      "venue": "Open graph benchmark: Datasets for machine learning on graphs"
    },
    {
      "citation_id": "74",
      "title": "Facial expression analysis",
      "year": "2017",
      "venue": "Facial expression analysis"
    },
    {
      "citation_id": "75",
      "title": "Deep unordered composition rivals syntactic methods for text classification",
      "authors": [
        "Mohit Iyyer",
        "Varun Manjunatha",
        "Jordan Boyd-Graber",
        "Hal Daumé"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing"
    },
    {
      "citation_id": "76",
      "title": "Multiplicative interactions and where to find them",
      "authors": [
        "M Siddhant",
        "Wojciech Jayakumar",
        "Jacob Czarnecki",
        "Jonathan Menick",
        "Jack Schwarz",
        "Simon Rae",
        "Yee Osindero",
        "Tim Teh",
        "Razvan Harley",
        "Pascanu"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "77",
      "title": "Mimic-iii, a freely accessible critical care database. Scientific data",
      "authors": [
        "E Alistair",
        "Tom Johnson",
        "Lu Pollard",
        "H Shen",
        "Mengling Lehman Li-Wei",
        "Mohammad Feng",
        "Benjamin Ghassemi",
        "Peter Moody",
        "Leo Szolovits",
        "Roger Anthony Celi",
        "Mark"
      ],
      "year": "2016",
      "venue": "Mimic-iii, a freely accessible critical care database. Scientific data"
    },
    {
      "citation_id": "78",
      "title": "Wavelet maxima dispersion for breathy to tense voice discrimination",
      "authors": [
        "John Kane",
        "Christer Gobl"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "79",
      "title": "The kinetics human action video dataset",
      "authors": [
        "Will Kay",
        "Joao Carreira",
        "Karen Simonyan",
        "Brian Zhang",
        "Chloe Hillier",
        "Sudheendra Vijayanarasimhan",
        "Fabio Viola",
        "Tim Green",
        "Trevor Back",
        "Paul Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "80",
      "title": "Supervised multimodal bitransformers for classifying images and text",
      "authors": [
        "Douwe Kiela",
        "Suvrat Bhooshan",
        "Hamed Firooz",
        "Ethan Perez",
        "Davide Testuggine"
      ],
      "year": "2019",
      "venue": "Supervised multimodal bitransformers for classifying images and text",
      "arxiv": "arXiv:1909.02950"
    },
    {
      "citation_id": "81",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "Douwe Kiela",
        "Hamed Firooz",
        "Aravind Mohan",
        "Vedanuj Goswami",
        "Amanpreet Singh",
        "Pratik Ringshia",
        "Davide Testuggine"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "82",
      "title": "Social robots as embedded reinforcers of social behavior in children with autism",
      "authors": [
        "Elizabeth S Kim",
        "Lauren Berkovits",
        "Emily Bernier",
        "Dan Leyzberg",
        "Frederick Shic",
        "Rhea Paul",
        "Brian Scassellati"
      ],
      "year": "2013",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "83",
      "title": "Embedded multimodal interfaces in robotics: applications, future trends, and societal implications",
      "authors": [
        "Elsa Kirchner",
        "Stephen Fairclough",
        "Frank Kirchner"
      ],
      "year": "2019",
      "venue": "The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions"
    },
    {
      "citation_id": "84",
      "title": "Wilds: A benchmark of in-the-wild distribution shifts",
      "authors": [
        "Pang Wei Koh",
        "Shiori Sagawa",
        "Henrik Marklund",
        "Sang Michael Xie",
        "Marvin Zhang",
        "Akshay Balsubramani",
        "Weihua Hu",
        "Michihiro Yasunaga",
        "Richard Phillips",
        "Sara Beery"
      ],
      "year": "2020",
      "venue": "Wilds: A benchmark of in-the-wild distribution shifts",
      "arxiv": "arXiv:2012.07421"
    },
    {
      "citation_id": "85",
      "title": "Second opinion needed: communicating uncertainty in medical machine learning",
      "authors": [
        "Benjamin Kompa",
        "Jasper Snoek",
        "Andrew Beam"
      ],
      "year": "2021",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "86",
      "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio"
      ],
      "year": "1995",
      "venue": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "87",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Yann Lecun",
        "Léon Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "88",
      "title": "Detect, reject, correct: Crossmodal compensation of corrupted sensors",
      "authors": [
        "Michelle Lee",
        "Matthew Tan",
        "Yuke Zhu",
        "Jeannette Bohg"
      ],
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "89",
      "title": "Multimodal sensor fusion with differentiable filters",
      "authors": [
        "Michelle Lee",
        "Brent Yi",
        "Roberto Martín-Martín",
        "Silvio Savarese",
        "Jeannette Bohg"
      ],
      "year": "2020",
      "venue": "IROS"
    },
    {
      "citation_id": "90",
      "title": "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks",
      "authors": [
        "Michelle Lee",
        "Yuke Zhu",
        "Krishnan Srinivasan",
        "Parth Shah",
        "Silvio Savarese",
        "Li Fei-Fei",
        "Animesh Garg",
        "Jeannette Bohg"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "91",
      "title": "Making sense of vision and touch: Learning multimodal representations for contact-rich tasks",
      "authors": [
        "Michelle Lee",
        "Yuke Zhu",
        "Peter Zachares",
        "Matthew Tan",
        "Krishnan Srinivasan",
        "Silvio Savarese",
        "Li Fei-Fei",
        "Animesh Garg",
        "Jeannette Bohg"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Robotics"
    },
    {
      "citation_id": "92",
      "title": "Enrico: A dataset for topic modeling of mobile ui designs",
      "authors": [
        "Asutosh Luis A Leiva",
        "Antti Hota",
        "Oulasvirta"
      ],
      "year": "2020",
      "venue": "22nd International Conference on Human-Computer Interaction with Mobile Devices and Services"
    },
    {
      "citation_id": "93",
      "title": "Tidigits speech corpus",
      "authors": [
        "Leonard Gary",
        "George Doddington"
      ],
      "year": "1993",
      "venue": "Tidigits speech corpus"
    },
    {
      "citation_id": "94",
      "title": "Visualbert: A simple and performant baseline for vision and language",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2019",
      "venue": "Visualbert: A simple and performant baseline for vision and language",
      "arxiv": "arXiv:1908.03557"
    },
    {
      "citation_id": "95",
      "title": "Federated optimization in heterogeneous networks",
      "authors": [
        "Tian Li",
        "Anit Kumar Sahu",
        "Manzil Zaheer",
        "Maziar Sanjabi",
        "Ameet Talwalkar",
        "Virginia Smith"
      ],
      "year": "2018",
      "venue": "Federated optimization in heterogeneous networks"
    },
    {
      "citation_id": "96",
      "title": "Robust navigation with language pretraining and stochastic sampling",
      "authors": [
        "Xiujun Li",
        "Chunyuan Li",
        "Qiaolin Xia",
        "Yonatan Bisk",
        "Asli Celikyilmaz",
        "Jianfeng Gao",
        "Noah Smith",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Robust navigation with language pretraining and stochastic sampling"
    },
    {
      "citation_id": "97",
      "title": "Awesome multimodal ml",
      "authors": [
        "Paul Liang"
      ],
      "year": "2020",
      "venue": "Awesome multimodal ml"
    },
    {
      "citation_id": "98",
      "title": "Towards debiasing sentence representations",
      "authors": [
        "Paul Pu Liang",
        "Irene Li",
        "Emily Zheng",
        "Chong Yao",
        "Ruslan Lim",
        "Louis-Philippe Salakhutdinov",
        "Morency"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "99",
      "title": "Think locally, act globally: Federated learning with local and global representations",
      "authors": [
        "Paul Pu Liang",
        "Terrance Liu",
        "Liu Ziyin",
        "Nicholas Allen",
        "Randy Auerbach",
        "David Brent",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Think locally, act globally: Federated learning with local and global representations"
    },
    {
      "citation_id": "100",
      "title": "Learning representations from imperfect time series data via tensor rank regularization",
      "authors": [
        "Paul Pu Liang",
        "Zhun Liu",
        "Yao-Hung Hubert Tsai",
        "Qibin Zhao",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "101",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "102",
      "title": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion",
      "authors": [
        "Paul Pu Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion"
    },
    {
      "citation_id": "103",
      "title": "Cross-modal generalization: Learning in low resource modalities via meta-alignment",
      "authors": [
        "Paul Pu Liang",
        "Peter Wu",
        "Liu Ziyin",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2020",
      "venue": "Cross-modal generalization: Learning in low resource modalities via meta-alignment",
      "arxiv": "arXiv:2012.02813"
    },
    {
      "citation_id": "104",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "105",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "106",
      "title": "Deep gamblers: Learning to abstain with portfolio theory",
      "authors": [
        "Ziyin Liu",
        "Zhikang Wang",
        "Paul Liang",
        "Russ Salakhutdinov",
        "Louis-Philippe Morency",
        "Masahito Ueda"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "107",
      "title": "Bias amplification in artificial intelligence systems",
      "authors": [
        "Kirsten Lloyd"
      ],
      "year": "2018",
      "venue": "Bias amplification in artificial intelligence systems"
    },
    {
      "citation_id": "108",
      "title": "Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "109",
      "title": "A survey of reinforcement learning informed by natural language",
      "authors": [
        "Jelena Luketina",
        "Nantas Nardelli",
        "Gregory Farquhar",
        "Jakob Foerster",
        "Jacob Andreas",
        "Edward Grefenstette",
        "Shimon Whiteson",
        "Tim Rocktäschel"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "110",
      "title": "Smil: Multimodal learning with severely missing modality",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Sergey Tulyakov",
        "Cathy Wu",
        "Xi Peng"
      ],
      "year": "2021",
      "venue": "Smil: Multimodal learning with severely missing modality"
    },
    {
      "citation_id": "111",
      "title": "Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "year": "2015",
      "venue": "Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "112",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "Ninareh Mehrabi",
        "Fred Morstatter",
        "Nripsuta Saxena",
        "Kristina Lerman",
        "Aram Galstyan"
      ],
      "year": "2019",
      "venue": "A survey on bias and fairness in machine learning",
      "arxiv": "arXiv:1908.09635"
    },
    {
      "citation_id": "113",
      "title": "Being a supercook: Joint food attributes and multimodal content modeling for recipe retrieval and exploration",
      "authors": [
        "Weiqing Min",
        "Shuqiang Jiang",
        "Jitao Sang",
        "Huayang Wang",
        "Xinda Liu",
        "Luis Herranz"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "114",
      "title": "Model cards for model reporting",
      "authors": [
        "Margaret Mitchell",
        "Simone Wu",
        "Andrew Zaldivar",
        "Parker Barnes",
        "Lucy Vasserman",
        "Ben Hutchinson",
        "Elena Spitzer",
        "Deborah Raji",
        "Timnit Gebru"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference on fairness, accountability, and transparency"
    },
    {
      "citation_id": "115",
      "title": "Large-scale concept ontology for multimedia",
      "authors": [
        "Milind Naphade",
        "John Smith",
        "Jelena Tesic",
        "Shih-Fu Chang",
        "Winston Hsu",
        "Lyndon Kennedy",
        "Alexander Hauptmann",
        "Jon Curtis"
      ],
      "year": "2006",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "116",
      "title": "Modeling multimodal human-computer interaction",
      "authors": [
        "Zeljko Obrenovic",
        "Dusan Starcevic"
      ],
      "year": "2004",
      "venue": "Computer"
    },
    {
      "citation_id": "117",
      "title": "Investigating user perception of gender bias in image search: The role of sexism",
      "authors": [
        "Jahna Otterbacher",
        "Alessandro Checco",
        "Gianluca Demartini",
        "Paul Clough"
      ],
      "year": "2018",
      "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR '18"
    },
    {
      "citation_id": "118",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "119",
      "title": "Film: Visual reasoning with a general conditioning layer",
      "authors": [
        "Ethan Perez",
        "Florian Strub",
        "Vincent Harm De Vries",
        "Aaron Dumoulin",
        "Courville"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "120",
      "title": "Deception detection using real-life trial data",
      "authors": [
        "Verónica Pérez-Rosas",
        "Mohamed Abouelenien",
        "Rada Mihalcea",
        "Mihai Burzo"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "121",
      "title": "Mfas: Multimodal fusion architecture search",
      "authors": [
        "Juan-Manuel Pérez-Rúa",
        "Valentin Vielzeuf",
        "Stéphane Pateux",
        "Moez Baccouche",
        "Frédéric Jurie"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "122",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "123",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "124",
      "title": "ESC: Dataset for Environmental Sound Classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia"
    },
    {
      "citation_id": "125",
      "title": "Emotion recognition and adaptation in spoken dialogue systems",
      "authors": [
        "Johannes Pittermann",
        "Angela Pittermann",
        "Wolfgang Minker"
      ],
      "year": "2010",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "126",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "127",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "128",
      "title": "Benchmarking deep learning models on large healthcare datasets",
      "authors": [
        "Sanjay Purushotham",
        "Chuizheng Meng",
        "Zhengping Che",
        "Yan Liu"
      ],
      "year": "2018",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "129",
      "title": "Failing loudly: An empirical study of methods for detecting dataset shift",
      "authors": [
        "Stephan Rabanser",
        "Stephan Günnemann",
        "Zachary Lipton"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "130",
      "title": "Towards multimodal deep learning for activity recognition on mobile devices",
      "authors": [
        "Valentin Radu",
        "Sourav Nicholas D Lane",
        "Cecilia Bhattacharya",
        "Mahesh Mascolo",
        "Fahim Marina",
        "Kawsar"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct"
    },
    {
      "citation_id": "131",
      "title": "Zero-shot text-to-image generation",
      "authors": [
        "Aditya Ramesh",
        "Mikhail Pavlov",
        "Gabriel Goh",
        "Scott Gray",
        "Chelsea Voss",
        "Alec Radford",
        "Mark Chen",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Zero-shot text-to-image generation",
      "arxiv": "arXiv:2102.12092"
    },
    {
      "citation_id": "132",
      "title": "Measuring social biases in grounded vision and language embeddings",
      "authors": [
        "Candace Ross",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "year": "2020",
      "venue": "Measuring social biases in grounded vision and language embeddings",
      "arxiv": "arXiv:2002.08911"
    },
    {
      "citation_id": "133",
      "title": "Learning internal representations by error propagation",
      "authors": [
        "Geoffrey David E Rumelhart",
        "Ronald Hinton",
        "Williams"
      ],
      "year": "1985",
      "venue": "Learning internal representations by error propagation"
    },
    {
      "citation_id": "134",
      "title": "Multimodal fusion refiner networks",
      "authors": [
        "Sethuraman Sankaran",
        "David Yang",
        "Ser-Nam Lim"
      ],
      "year": "2021",
      "venue": "Multimodal fusion refiner networks",
      "arxiv": "arXiv:2104.03435"
    },
    {
      "citation_id": "135",
      "title": "Multimodal deep learning for short-term stock volatility prediction",
      "authors": [
        "Marcelo Sardelich",
        "Suresh Manandhar"
      ],
      "year": "2018",
      "venue": "Multimodal deep learning for short-term stock volatility prediction",
      "arxiv": "arXiv:1812.10479"
    },
    {
      "citation_id": "136",
      "title": "Robots for use in autism research",
      "authors": [
        "Brian Scassellati",
        "Henny Admoni",
        "Maja Matarić"
      ],
      "year": "2012",
      "venue": "Annual review of biomedical engineering"
    },
    {
      "citation_id": "137",
      "title": "Vision to language: Methods, metrics and datasets",
      "authors": [
        "Naeha Sharif",
        "Uzair Nadeem",
        "Syed Afaq",
        "Ali Shah",
        "Mohammed Bennamoun",
        "Wei Liu"
      ],
      "year": "2020",
      "venue": "Machine Learning Paradigms"
    },
    {
      "citation_id": "138",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "139",
      "title": "Zero-shot learning through cross-modal transfer",
      "authors": [
        "Richard Socher",
        "Milind Ganjoo",
        "Hamsa Sridhar",
        "Osbert Bastani",
        "Christopher Manning",
        "Andrew Ng"
      ],
      "year": "2013",
      "venue": "Zero-shot learning through cross-modal transfer",
      "arxiv": "arXiv:1301.3666"
    },
    {
      "citation_id": "140",
      "title": "Worst of both worlds: Biases compound in pre-trained vision-andlanguage models",
      "authors": [
        "Tejas Srinivasan",
        "Yonatan Bisk"
      ],
      "year": "2021",
      "venue": "Worst of both worlds: Biases compound in pre-trained vision-andlanguage models",
      "arxiv": "arXiv:2104.08666"
    },
    {
      "citation_id": "141",
      "title": "Energy and policy considerations for deep learning in nlp",
      "authors": [
        "Emma Strubell",
        "Ananya Ganesh",
        "Andrew Mccallum"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "142",
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "authors": [
        "Weijie Su",
        "Xizhou Zhu",
        "Yue Cao",
        "Bin Li",
        "Lewei Lu",
        "Furu Wei",
        "Jifeng Dai"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "143",
      "title": "A survey of types of text noise and techniques to handle noisy text",
      "authors": [
        "L Subramaniam",
        "Shourya Roy",
        "Tanveer Faruquie",
        "Sumit Negi"
      ],
      "year": "2009",
      "venue": "Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data, AND '09"
    },
    {
      "citation_id": "144",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "145",
      "title": "Intriguing properties of neural networks",
      "authors": [
        "Christian Szegedy",
        "Wojciech Zaremba",
        "Ilya Sutskever",
        "Joan Bruna",
        "Dumitru Erhan",
        "Ian Goodfellow",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations"
    },
    {
      "citation_id": "146",
      "title": "Multimodal{qa}: complex question answering over text, tables and images",
      "authors": [
        "Alon Talmor",
        "Ori Yoran",
        "Amnon Catav",
        "Dan Lahav",
        "Yizhong Wang",
        "Akari Asai",
        "Gabriel Ilharco",
        "Hannaneh Hajishirzi",
        "Jonathan Berant"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "147",
      "title": "Multimodal research: Addressing the complexity of multimodal environments and the challenges for call",
      "authors": [
        "Sabine Tan",
        "Peter Kay O'halloran",
        "Wignell"
      ],
      "year": "2016",
      "venue": "ReCALL"
    },
    {
      "citation_id": "148",
      "title": "Measuring robustness to natural distribution shifts in image classification",
      "authors": [
        "Rohan Taori",
        "Achal Dave",
        "Vaishaal Shankar",
        "Nicholas Carlini",
        "Benjamin Recht",
        "Ludwig Schmidt"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "149",
      "title": "Long range arena: A benchmark for efficient transformers",
      "authors": [
        "Yi Tay",
        "Mostafa Dehghani",
        "Samira Abnar",
        "Yikang Shen",
        "Dara Bahri",
        "Philip Pham",
        "Jinfeng Rao",
        "Liu Yang",
        "Sebastian Ruder",
        "Donald Metzler"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "150",
      "title": "Contrastive multiview coding. ECCV",
      "authors": [
        "Yonglong Tian",
        "Dilip Krishnan",
        "Phillip Isola"
      ],
      "year": "2020",
      "venue": "Contrastive multiview coding. ECCV"
    },
    {
      "citation_id": "151",
      "title": "Mujoco: A physics engine for model-based control",
      "authors": [
        "Emanuel Todorov",
        "Tom Erez",
        "Yuval Tassa"
      ],
      "year": "2012",
      "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "152",
      "title": "Methods for comparing uncertainty quantifications for material property predictions",
      "authors": [
        "Kevin Tran",
        "Willie Neiswanger",
        "Junwoong Yoon",
        "Qingyang Zhang",
        "Eric Xing",
        "Zachary Ulissi"
      ],
      "year": "2020",
      "venue": "Machine Learning: Science and Technology"
    },
    {
      "citation_id": "153",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "154",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "155",
      "title": "Multimodal routing: Improving local and global interpretability of multimodal language analysis",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Martin Ma",
        "Muqiao Yang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "156",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "Aaron Van Den Oord",
        "Sander Dieleman",
        "Heiga Zen",
        "Karen Simonyan",
        "Oriol Vinyals",
        "Alex Graves",
        "Nal Kalchbrenner",
        "Andrew Senior",
        "Koray Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Wavenet: A generative model for raw audio"
    },
    {
      "citation_id": "157",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "158",
      "title": "Probabilistic neural symbolic models for interpretable visual question answering",
      "authors": [
        "Ramakrishna Vedantam",
        "Karan Desai",
        "Stefan Lee",
        "Marcus Rohrbach",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "159",
      "title": "Using clinical natural language processing for health outcomes research: Overview and actionable suggestions for future advances",
      "authors": [
        "Sumithra Velupillai",
        "Hanna Suominen",
        "Maria Liakata",
        "Angus Roberts",
        "Anoop Shah",
        "Katherine Morley",
        "David Osborn",
        "Joseph Hayes",
        "Robert Stewart",
        "Johnny Downs",
        "Wendy Chapman",
        "Rina Dutta"
      ],
      "year": "2018",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "160",
      "title": "Centralnet: a multilayer approach for multimodal fusion",
      "authors": [
        "Valentin Vielzeuf",
        "Alexis Lechervy",
        "Stéphane Pateux",
        "Frédéric Jurie"
      ],
      "year": "2018",
      "venue": "Centralnet: a multilayer approach for multimodal fusion"
    },
    {
      "citation_id": "161",
      "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge",
      "authors": [
        "Oriol Vinyals",
        "Alexander Toshev",
        "Samy Bengio",
        "Dumitru Erhan"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "162",
      "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "authors": [
        "Alex Wang",
        "Yada Pruksachatkun",
        "Nikita Nangia",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R Bowman"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "163",
      "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "authors": [
        "Alex Wang",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel Bowman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"
    },
    {
      "citation_id": "164",
      "title": "Mimic-extract: A data extraction, preprocessing, and representation pipeline for mimic-iii",
      "authors": [
        "Shirly Wang",
        "Matthew Mcdermott",
        "Geeticka Chauhan",
        "Marzyeh Ghassemi",
        "Michael Hughes",
        "Tristan Naumann"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM Conference on Health, Inference, and Learning"
    },
    {
      "citation_id": "165",
      "title": "On deep multi-view representation learning",
      "authors": [
        "Weiran Wang",
        "Raman Arora",
        "Karen Livescu",
        "Jeff Bilmes"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "166",
      "title": "What makes training multi-modal classification networks hard?",
      "authors": [
        "Weiyao Wang",
        "Du Tran",
        "Matt Feiszli"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "167",
      "title": "Multimodal generative models for scalable weakly-supervised learning",
      "authors": [
        "Mike Wu",
        "Noah Goodman"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "168",
      "title": "Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation",
      "authors": [
        "Yingda Xia",
        "Dong Yang",
        "Zhiding Yu",
        "Fengze Liu",
        "Jinzheng Cai",
        "Lequan Yu",
        "Zhuotun Zhu",
        "Daguang Xu",
        "Alan Yuille",
        "Holger Roth"
      ],
      "year": "2020",
      "venue": "Medical Image Analysis"
    },
    {
      "citation_id": "169",
      "title": "Adaptive cross-modal few-shot learning",
      "authors": [
        "Chen Xing",
        "Negar Rostamzadeh",
        "Boris Oreshkin",
        "Pedro O O Pinheiro"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "170",
      "title": "Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "171",
      "title": "Multimodal machine learning for automated icd coding",
      "authors": [
        "Keyang Xu",
        "Mike Lam",
        "Jingzhi Pang",
        "Xin Gao",
        "Charlotte Band",
        "Piyush Mathur",
        "Frank Papay",
        "Ashish Khanna",
        "Jacek Cywinski",
        "Kamal Maheshwari"
      ],
      "year": "2019",
      "venue": "Machine Learning for Healthcare Conference"
    },
    {
      "citation_id": "172",
      "title": "Multimodal fusion architecture search for electronic health records",
      "authors": [
        "Zhen Xu",
        "David So",
        "Andrew Dai",
        "Mufasa"
      ],
      "year": "2021",
      "venue": "Multimodal fusion architecture search for electronic health records",
      "arxiv": "arXiv:2102.02340"
    },
    {
      "citation_id": "173",
      "title": "Multimodal transformer for multimodal machine translation",
      "authors": [
        "Shaowei Yao",
        "Xiaojun Wan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "174",
      "title": "More than a million ways to be pushed. a high-fidelity experimental dataset of planar pushing",
      "authors": [
        "Kuan-Ting Yu",
        "Maria Bauza",
        "Nima Fazeli",
        "Alberto Rodriguez"
      ],
      "year": "2016",
      "venue": "2016 IEEE/RSJ international conference on intelligent robots and systems (IROS)"
    },
    {
      "citation_id": "175",
      "title": "Speaker identification on the scotus corpus",
      "authors": [
        "Jiahong Yuan",
        "Mark Liberman"
      ],
      "year": "2008",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "176",
      "title": "CMU multimodal SDK",
      "authors": [
        "Amir Zadeh"
      ],
      "year": "2019",
      "venue": "CMU multimodal SDK"
    },
    {
      "citation_id": "177",
      "title": "Social-iq: A question answering benchmark for artificial social intelligence",
      "authors": [
        "Amir Zadeh",
        "Michael Chan",
        "Paul Liang",
        "Edmund Tong",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "178",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "179",
      "title": "Foundations of multimodal co-learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "180",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "181",
      "title": "Moseas: A multimodal language dataset for spanish, portuguese, german and french",
      "authors": [
        "Amirali Bagher Zadeh",
        "Yansheng Cao",
        "Simon Hessner",
        "Paul Liang",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "182",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "183",
      "title": "NIPS",
      "authors": [
        "Manzil Zaheer",
        "Satwik Kottur",
        "Siamak Ravanbakhsh",
        "Barnabás Póczos",
        "Ruslan Salakhutdinov",
        "Alexander Smola"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "184",
      "title": "Domain adaptation under target and conditional shift",
      "authors": [
        "Kun Zhang",
        "Bernhard Schölkopf",
        "Krikamol Muandet",
        "Zhikun Wang"
      ],
      "year": "2013",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "185",
      "title": "Inherent tradeoffs in learning fair representations",
      "authors": [
        "Han Zhao",
        "Geoff Gordon"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "186",
      "title": "Deep supervised cross-modal retrieval",
      "authors": [
        "Liangli Zhen",
        "Peng Hu",
        "Xu Wang",
        "Dezhong Peng"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "187",
      "title": "Rtfm: Generalising to new environment dynamics via reading",
      "authors": [
        "Victor Zhong",
        "Tim Rocktäschel",
        "Edward Grefenstette"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "193",
      "title": "While LF is the easiest to adapt to new datasets and domains, we encountered difficulties in adapting several possibly well-performing methods (such as MFAS or MULT) to new datasets and domains. MFAS is designed with a specific set of atomic architectural elements in mind which makes it most suitable for convolutional networks. MULT is suitable for multimodal time-series data and it is unclear how to adapt its fusion paradigm to modalities without a temporal dimension. For a more fair comparison, in Figure 24(b), we plot the accumulated performance for methods only on the most commonly studied datasets where we experimented with more than 6 methods. We find that these well-performing methods (MFAS or MULT) show only slightly better than LF on all datasets (see Figure 24(a)), they (see Figure 24(b)). Therefore, it is important for future research to focus on models that generalize to multiple domains, modalities, and tasks",
      "venue": "While LF is the easiest to adapt to new datasets and domains, we encountered difficulties in adapting several possibly well-performing methods (such as MFAS or MULT) to new datasets and domains. MFAS is designed with a specific set of atomic architectural elements in mind which makes it most suitable for convolutional networks. MULT is suitable for multimodal time-series data and it is unclear how to adapt its fusion paradigm to modalities without a temporal dimension. For a more fair comparison, in Figure 24(b), we plot the accumulated performance for methods only on the most commonly studied datasets where we experimented with more than 6 methods. We find that these well-performing methods (MFAS or MULT) show only slightly better than LF on all datasets (see Figure 24(a)), they (see Figure 24(b)). Therefore, it is important for future research to focus on models that generalize to multiple domains, modalities, and tasks"
    },
    {
      "citation_id": "194",
      "title": "These plots do not completely capture the picture since complexity is measured via total training time (training speed), which can be prohibitively high for methods such as MFAS, MVAE, and GRADBLEND. However, these methods are primarily slow due to extra parameters or training procedures during training, and once the model is trained, test-time inference is fast and cheap. Plotting a performance-complexity tradeoff using a different complexity metric will likely result in different observations. Overall, MULTIBENCH enables a holistic evaluation of training and test-time space and memory complexity so practitioners can choose the most suitable model for their real-world application setting",
      "venue": "These plots do not completely capture the picture since complexity is measured via total training time (training speed), which can be prohibitively high for methods such as MFAS, MVAE, and GRADBLEND. However, these methods are primarily slow due to extra parameters or training procedures during training, and once the model is trained, test-time inference is fast and cheap. Plotting a performance-complexity tradeoff using a different complexity metric will likely result in different observations. Overall, MULTIBENCH enables a holistic evaluation of training and test-time space and memory complexity so practitioners can choose the most suitable model for their real-world application setting"
    }
  ]
}