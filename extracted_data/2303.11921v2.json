{
  "paper_id": "2303.11921v2",
  "title": "Context De-Confounded Emotion Recognition",
  "published": "2023-03-21T15:12:20Z",
  "authors": [
    "Dingkang Yang",
    "Zhaoyu Chen",
    "Yuzheng Wang",
    "Shunli Wang",
    "Mingcheng Li",
    "Siao Liu",
    "Xiao Zhao",
    "Shuai Huang",
    "Zhiyan Dong",
    "Peng Zhai",
    "Lihua Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Context-Aware Emotion Recognition (CAER) is a crucial and challenging task that aims to perceive the emotional states of the target person with contextual information. Recent approaches invariably focus on designing sophisticated architectures or mechanisms to extract seemingly meaningful representations from subjects and contexts. However, a long-overlooked issue is that a context bias in existing datasets leads to a significantly unbalanced distribution of emotional states among different context scenarios. Concretely, the harmful bias is a confounder that misleads existing models to learn spurious correlations based on conventional likelihood estimation, significantly limiting the models' performance. To tackle the issue, this paper provides a causality-based perspective to disentangle the models from the impact of such bias, and formulate the causalities among variables in the CAER task via a tailored causal graph. Then, we propose a Contextual Causal Intervention Module (CCIM) based on the backdoor adjustment to de-confound the confounder and exploit the true causal effect for model training. CCIM is plug-in and model-agnostic, which improves diverse state-of-the-art approaches by considerable margins. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our CCIM and the significance of causal insight.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As an essential technology for understanding human intentions, emotion recognition has attracted significant attention in various fields such as human-computer interaction  [1] , medical monitoring  [28] , and education  [40] . Previous works have focused on extracting multimodal emotion cues from human subjects, including facial expressions  [9, 10, 49] , acoustic behaviors  [2, 50, 52] , and body § Corresponding Author.  postures  [25, 53] , benefiting from advances in deep learning algorithms  [6, 7, 21, 26, 27, 43, 44, 46, 47, 54, 55, 59] . Despite the impressive improvements achieved by subjectcentered approaches, their performance is limited by natural and unconstrained environments. Several examples in Figure  1  (left) show typical situations on a visual level. Instead of well-designed visual contents, multimodal representations of subjects in wild-collected images are usually indistinguishable (e.g., ambiguous faces or gestures), which forces us to exploit complementary factors around the subject that potentially reflect emotions.\n\nInspired by psychological study  [3] , recent works  [19, 22, 23, 29, 56]  have suggested that contextual information contributes to effective emotion cues for Context-Aware Emotion Recognition (CAER). The contexts are considered to include the place category, the place attributes, the objects, or the actions of others around the subject  [20] . The majority of such research typically follows a common pipeline:  (1)  Obtaining the unimodal/multimodal representations of the recognized subject; (2) Building diverse contexts and extracting emotion-related representations; (3) Designing We show a toy experiment on the EMOTIC  [20]  and CAER-S  [22]  datasets for scene categories of angry and happy emotions. More scene categories with normalized zeroconditional entropy reveal a strong presence of the context bias. fusion strategies to combine these features for emotion label predictions. Although existing methods have improved modestly through complex module stacking  [12, 23, 51]  and tricks  [16, 29] , they invariably suffer from a context bias of the datasets, which has long been overlooked. Recalling the process of generating CAER datasets, different annotators were asked to label each image according to what they subjectively thought people in the images with diverse contexts were feeling  [20] . This protocol makes the preference of annotators inevitably affect the distribution of emotion categories across contexts, thereby leading to the context bias. Figure  1  illustrates how such bias confounds the predictions. Intrigued, most of the images in training data contain vegetated scenes with positive emotion categories, while negative emotions in similar contexts are almost nonexistent. Therefore, the baseline  [19]  is potentially misled into learning the spurious dependencies between contextspecific features and label semantics. When given test images with similar contexts but negative emotion categories, the model inevitably infers the wrong emotional states.\n\nMore intrigued, a toy experiment is performed to verify the strong bias in CAER datasets. This test aims to observe how well emotions correlate with contexts (e.g., scene categories). Specifically, we employ the ResNet-152  [15]  pre-trained on Places365  [58]  to predict scene categories from images with three common emotion categories (i.e., \"anger\", \"happy\", and \"fear\") across two datasets. The top 200 most frequent scenes from each emotion category are selected, and the normalized conditional entropy of each scene category across the positive and negative set of a specific emotion is computed  [30] . While analyzing correlations between scene contexts and emotion categories in Figure 2 (e.g., \"anger\" and \"happy\"), we find that more scene categories with the zero conditional entropy are most likely to suggest the significant context bias in the datasets, as it shows the presence of these scenes only in the positive or negative set of emotions. Concretely, for the EMOTIC dataset  [20] , about 40% of scene categories for anger have zero conditional entropy while about 45% of categories for happy (i.e., happiness) have zero conditional entropy. As an intuitive example, most party-related scene contexts are present in the samples with the happy category and almost non-existent in the negative categories. These observations confirm the severe context bias in CAER datasets, leading to distribution gaps in emotion categories across contexts and uneven visual representations.\n\nMotivated by the above observation, we attempt to embrace causal inference  [31]  to reveal the culprit that poisons the CAER models, rather than focusing on beating them. As a revolutionary scientific paradigm that facilitates models toward unbiased prediction, the most important challenge in applying classical causal inference to the modern CAER task is how to reasonably depict true causal effects and identify the task-specific dataset bias. To this end, this paper attempts to address the challenge and rescue the bias-ridden models by drawing on human instincts, i.e., looking for the causality behind any association. Specifically, we present a causality-based bias mitigation strategy. We first formulate the procedure of the CAER task via a proposed causal graph. In this case, the harmful context bias in datasets is essentially an unintended confounder that misleads the models to learn the spurious correlation between similar contexts and specific emotion semantics. From Figure  3 , we disentangle the causalities among the input images X, subject features S, context features C, confounder Z, and predictions Y . Then, we propose a simple yet effective Contextual Causal Intervention Module (CCIM) to achieve context-deconfounded training and use the do-calculus P (Y |do(X)) to calculate the true causal effect, which is fundamentally different from the conventional likelihood P (Y |X). CCIM is plug-in and model-agnostic, with the backdoor adjustment  [14]  to de-confound the confounder and eliminate the impact of the context bias. We comprehensively evaluate the effectiveness and superiority of CCIM on three standard and biased CAER datasets. Numerous experiments and analyses demonstrate that CCIM can significantly and consistently improve existing baselines, achieving a new state-of-the-art (SOTA).\n\nThe main contributions can be summarized as follows:\n\n• To our best knowledge, we are the first to investigate the adverse context bias of the datasets in the CAER task from the causal inference perspective and identify that such bias is a confounder, which misleads the models to learn the spurious correlation.\n\n• We propose CCIM, a plug-in contextual causal intervention module, which could be inserted into most CAER models to remove the side effect caused by the confounder and facilitate a fair contribution of diverse contexts to emotion understanding.\n\n• Extensive experiments on three standard CAER datasets show that the proposed CCIM can facilitate existing models to achieve unbiased predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Context-Aware Emotion Recognition. As a promising task, Context-Aware Emotion Recognition (CAER) not only draws on human subject-centered approaches  [4, 49, 52]  to perceive emotion via the face or body, but also considers the emotion cues provided by background contexts in a joint and boosting manner. Existing CAER models invariably extract multiple representations from these two sources and then perform feature fusion to make the final prediction  [12, [22] [23] [24] 29, 37, 51, 56] . For instance, Kosti et al.  [19]  establish the EMOTIC dataset and propose a baseline Convolutional Neural Network (CNN) model that combines the body region and the whole image as the context. Hoang et al.  [16]  propose an extra reasoning module to exploit the images, categories, and bounding boxes of adjacent objects in background contexts to achieve visual relationship detection. For a deep exploration of scene context, Li et al.  [23]  present a body-object attention module to estimate the contributions of background objects and a body-part attention module to recalibrate the channel-wise body feature responses. Although the aforementioned approaches achieve impressive improvements by exploring diverse contextual information, they all neglect the limitation on model performance caused by the context bias of the datasets. Instead of focusing on beating the latest SOTA, we identify the bias as a harmful confounder from a causal inference perspective and significantly improve the existing models with the proposed CCIM. Causal Inference. Causal inference is an analytical tool that aims to infer the dynamics of events under changing conditions (e.g., different treatments or external interventions)  [31] , which has been extensively studied in economics, statistics, and psychology  [11, 41] . Without loss of generality, causal inference follows two main ways: structured causal model  [32]  and potential outcome framework  [38] , which assist in revealing the causality rather than the superficial association among variables. Benefiting from the great potential of the causal tool to provide unbiased estimation solutions, it has been gradually applied to various computer tasks, such as computer vision  [5, 34, 39, 42, 45]  and natural language processing  [17, 35, 57] . Inspired by visual commonsense learning  [45] , to our best knowledge, this is the first investigation of the confounding effect through causal inference in the CAER task while exploiting causal intervention to interpret and address the confounding bias from contexts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Causal View At Caer Task",
      "text": "Firstly, we formulate a tailored causal graph to summarize the CAER framework. In particular, we follow the same graphical notation in structured causal model  [32]  due to its intuitiveness and interpretability. It is a directed acyclic graph G = {N , E} that can be paired with data to produce quantitative causal estimates. The nodes N denote variables and the links E denote direct causal effects. As shown in Figure  3 , there are five variables involved in the CAER causal graph, which are input images X, subject features S, context features C, confounder Z, and predictions Y . Note that our causal graph is applicable to a variety of CAER methods, since it is highly general, imposing no constraints on the detailed implementations. The details of the causal relationships are described below. Z → X. Different subjects are recorded in various contexts to produce images X. On the one hand, the annotators make subjective and biased guesses about subjects' emotional states and give their annotations  [18, 20] , e.g., subjects are usually blindly assigned positive emotions in vegetation-covered contexts. On the other hand, the data nature leads to an unbalanced representation of emotions in the real world  [13] . That is, it is much easier to collect positive emotions in contexts of comfortable atmospheres than negative ones. The context bias caused by the above situations is treated as the harmful confounder Z to establish spurious connections between similar contexts and specific emotion semantics. For the input images X, Z determines the biased content that is recorded, i.e., Z → X. Z → C → Y . C represents the total context representation obtained by contextual feature extractors. C may come from the aggregation of diverse context features based on different methods. The causal path Z → C represents the detrimental Z confounding the model to learn unreliable emotion-related context semantics of C. In this case, the impure C further affects the predictions Y of the emotion labels and can be reflected via the link C → Y . Although Z potentially provides priors from the training data to better estimation when the subjects' features are ambiguous, it misleads the model to capture spurious \"context-emotion\" mapping during training, resulting in biased predictions.\n\nS represents the total subject representation obtained by subject feature extractors. Depending on distinct methods, S may come from the face, the body, or the integration of their features. In CAER causal graph, we can see that the desired effect of X on Y follows from two causal paths: X → C → Y and X → S → Y . These two causal paths reflect that the CAER model estimates Y based on the context features C and subject features S extracted from the input images X. In practice, C and S are usually integrated to make the final prediction jointly, e.g., feature concatenation  [29] .\n\nAccording to the causal theory  [31] , the confounder Z is the common cause of the input images X and corresponding predictions Y . The positive effects of context and subject features providing valuable semantics follow the causal paths X → C/S → Y , which we aim to achieve. Unfortunately, the confounder Z causes the negative effect of misleading the model to focus on spurious correlations instead of pure causal relationships. This adverse effect follows the backdoor causal path X ← Z → C → Y .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Causal Intervention Via Backdoor Adjustment",
      "text": "In Figure  3 (a), existing CAER methods rely on the likelihood P (Y |X). This process is formulated by Bayes rule:\n\nwhere f s (•) and f c (•) are two generalized encoding functions that obtain the total S and C, respectively. The confounder Z introduces the observational bias via P (z|X).\n\nTo address the confounding effect brought by Z and make the model rely on pure X to estimate Y , an intuitive idea is to intervene X and force each context semantics to contribute to the emotion prediction fairly. The process can be viewed as conducting a randomized controlled experiment by collecting images of subjects with any emotion in any context. However, this intervention is impossible due to the infinite number of images that combine various subjects and contexts in the real world. To solve this, we stratify Z based on the backdoor adjustment  [31]  to achieve causal intervention P (Y |do(X)) and block the backdoor path between X and Y , where do-calculus is an effective approximation for the imaginative intervention  [14] . Specifically, we seek the effect of stratified contexts and then estimate the average causal effect by computing a weighted average based on the proportion of samples containing different context prototypes in the training data. In Figure  3 (b), the causal path from Z to X is cut-off, and the model will approximate causal intervention P (Y |do(X)) rather than spurious association P (Y |X). By applying the Bayes rule on the new graph, Eq. (  1 ) with the intervention is formulated as: As z is no longer affected by X, the intervention intentionally forces X to incorporate every z fairly into the predictions of Y , subject to the proportion of each z in the whole.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Context-Deconfounded Training With Ccim",
      "text": "To implement the theoretical and imaginative intervention in Eq. (  2 ), we propose a Contextual Causal Intervention Module (CCIM) to achieve the context-deconfounded training for the models. From a general pipeline of the CAER task illustrated in Figure  4 (b), CCIM is inserted in a plugin manner after the original integrated feature of existing methods. Then, the output of CCIM performs predictions after passing the final task-specific classifier. The implementation of CCIM is described below. Confounder Dictionary. Since the number of contexts is large in the real world and there is no ground-truth contextual information in the training set, we approximate it as a stratified confounder dictionary Z = [z 1 , z 2 , . . . , z N ], where N is a hyperparameter representing the size, and each\n\n, where N m is the number of training samples. To compute context prototypes, we use the K-Means++ with principle component analysis to learn Z so that each z i represents a form of context cluster. Each cluster z i is set to the average feature of each cluster in the K-Means++, i.e., z i = 1 Ni Ni j=1 m i j , where N i is the number of context features in the i-th cluster. Instantiation of the Proposed CCIM. Since the calculation of P (Y |do(X)) requires multiple forward passes of all z, the computational overhead is expensive. To reduce the computational cost, we apply the Normalized Weighted Geometric Mean (NWGM)  [48]     2  and 3  follow the same interpretation.\n\nexpectation at the feature level as:\n\nInspired by  [45] , we parameterize a network model to approximate the above conditional probability of Eq. (  3 ) as follows:\n\nwhere W h ∈ R dm×d h and W g ∈ R dm×d are the learnable parameters, and h = ϕ(s, c) ∈ R d h ×1 . ϕ(•) is a fusion strategy (e.g., concatenation) that integrates s and c into the joint representation h. Note that the above approximation is reasonable, because the effect on Y comes from S, C, and the confounder Z. Immediately, we approximate E z [g(z)] as a weighted integration of all context prototypes:\n\nwhere λ i is a weight coefficient that measures the importance of each z i after interacting with the origin feature h, and P (z i ) = Ni Nm . In practice, we provide two implementations of λ i : dot product attention and additive attention:\n\nAdditive\n\nwhere W t ∈ R dn×1 , W q ∈ R dn×d h , and W k ∈ R dn×d are mapping matrices.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "Datasets. Our experiments are conducted on three standard datasets for the CAER task, namely EMOTIC  [20] , CAER-S  [22] , and GroupWalk  [29]   Evaluation Metrics. Following  [19, 29] , we utilize the mean Average Precision (mAP) to evaluate the results on the EMOTIC and GroupWalk. For the CAER-S, the standard classification accuracy is used for evaluation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Zoo",
      "text": "Limited by the fact that most methods are not open source, we select four representative models to evaluate the effectiveness of CCIM, which have different network structures and contextual exploration mechanisms. EMOT-Net  [19]   work (CNN) model with two branches. Its distinct branches capture foreground body features and background contextual information, respectively. GCN-CNN  [56]  utilizes different context elements to construct an affective graph and infer the affective relationship according to the Graph Convolutional Network (GCN). CAER-Net  [22]  is a twostream CNN model following an adaptive fusion module to reason emotions. The method focuses on the context of the entire image after hiding the face and the emotion cues provided by the facial region. EmotiCon  [29]  introduces three context-aware streams. Besides the subject-centered multimodal extraction branch, they propose to use visual attention and depth maps to learn the scene and socio-dynamic contexts separately. For EMOT-Net, we re-implement the model following the available code. Meanwhile, we reproduce the results on the three datasets based on the details reported in the SOTA methods above (i.e., GCN-CNN, CAER-Net, and EmotiCon).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "Confounder Setup. Firstly, except for the annotated EMOTIC, we utilize the pre-trained Faster R-CNN  [36]  to detect the bounding box of the target subject for each training sample on both CAER-S and GroupWalk. After that, the context images are generated by masking the target subjects on the training samples based on the bounding boxes. Then, we use the ResNet-152  [15]  pre-trained on Places365  [58]  dataset to extract the context feature set",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "We comprehensively compare the CCIM-based models with recent SOTA methods, including RRLA  [23] , VRD  [16] , SIB-Net  [24] , and GRERN  [12] . The default setting uses the dot product attention of Eq. (  6 ). Results on the EMOTIC Dataset. In Table  1 , we observe that CCIM significantly improves existing models and achieves the new SOTA. Specifically, the CCIM-based EMOT-Net, GCN-CNN, CAER-Net and EmotiCon improve the mAP scores by 2.95%, 3.56%, 2.66%, and 3.85%, respectively, outperforming the vanilla methods by large margins. In this case, these CCIM-based methods achieve competitive or better performance than the recent models RRLA and VRD. We also find that CCIM greatly improves the AP scores for some categories heavily persecuted by the confounder. For instance, CCIM helps raise the results of \"Anticipation\" and \"Sympathy\" in these CAER methods by 29%∼37% and 14%∼29%, respectively. Due to the adverse bias effect, the performance of most models is usually poor on infrequent categories, such as \"Aversion\" (AP scores of about 3%∼12%) and \"Embarrassment\" (AP scores of about 1%∼10%). Thanks to CCIM, the AP scores in these two categories are achieved at about 12%∼19% and 5%∼16%. Results on the GroupWalk Dataset. As shown in Table  2 , our CCIM effectively improves the performance of EMOT-Net, GCN-CNN, CAER-Net, and EmotiCon on the Group-Walk dataset. The mAP scores for these models are increased by 2.41%, 2.99%, 2.25%, and 3.73%, respectively. Results on the CAER-S Dataset. The accuracy of different methods on the CAER-S dataset is reported in Table  3 . The performance of EMOT-Net, GCN-CNN, and CAER-Net is consistently increased by CCIM, making each context prototype contribute fairly to the emotion classification results. These models are improved by 1.31%, 1.45%, and 1.34%, respectively. Moreover, the CCIM-based EmotiCon We argue that the essence of fine-grained modeling is the potential context stratification within the sample from the perspective of backdoor adjustment. Fortunately, CCIM can better refine this stratification effect and make the models focus on contextual causal intervention across samples to measure the true causal effect. (iii) According to Tables  1  and 2 , and Figure  5 , while the causal intervention brings gains for most emotions across datasets, the performance of some categories shows slight improvements or even deteriorations. A reasonable explanation is that the few samples and insignificant confounding effects of these categories result in over-intervention. However, the minor sacrifice is tolerable compared to the overall superiority of our CCIM.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "We conduct thorough ablation studies in Table  4  to evaluate the implementation of the causal intervention. To explore the effectiveness of CCIM when combining methods that model context semantics at different granularities, we choose the baseline EMOT-Net and SOTA EmotiCon. Rationality of Confounder Dictionary Z. We first provide a random dictionary with the same size to replace the tailored confounder dictionary Z, which is initialized by  randomization rather than the average context features. Experimental results  (3, 4)  show that the random dictionary would significantly hurt the performance, proving the validity of our context prototypes. Moreover, we use the ResNet-152 pre-trained on ImageNet  [8]  to replace the default settings (1, 2) for extracting context features. The decreased results  (5, 6)  suggest that context prototypes based on scene semantics are more conducive to approximating the confounder than those based on object semantics. It is reasonable as scenes usually include objects, e.g., in Figure  1 , \"grass\" is the child of the confounder \"vegetated scenes\". Robustness of Pre-trained Backbones. The experiments  (7, 8, 9, 10)  in Table  4  show that the gain from CCIM increases as more advanced pre-trained backbone networks are used, which indicates that our CCIM is not dependent on a well-chosen pre-trained backbone φ(•). Effectiveness of Components of E z [g(z)]. First, we report the results in experiments  (11, 12)  using the additive attention weight λ i in Eq.  (7) . The competitive performance demonstrates that both attention paradigms are meaningful and usable. Furthermore, we evaluate the effectiveness of the weighted integration by separately removing the weights λ i and prior probabilities P (z i ) in the E z [g(z)].\n\nThe decreased results  (13, 14, 15, 16)  suggest that depict-",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the context bias in the CAER task. GT",
      "page": 1
    },
    {
      "caption": "Figure 1: (left) show typical situations on a visual level. In-",
      "page": 1
    },
    {
      "caption": "Figure 2: We show a toy experiment on the EMOTIC [20]",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates how such bias confounds the",
      "page": 2
    },
    {
      "caption": "Figure 3: , we disentangle the causalities among the",
      "page": 2
    },
    {
      "caption": "Figure 3: Illustration of our CAER causal graph.",
      "page": 3
    },
    {
      "caption": "Figure 3: , there are five variables involved in the",
      "page": 3
    },
    {
      "caption": "Figure 3: (a), existing CAER methods rely on the like-",
      "page": 4
    },
    {
      "caption": "Figure 3: (b), the causal path",
      "page": 4
    },
    {
      "caption": "Figure 4: (a) The generation process of the confounder dictionary",
      "page": 4
    },
    {
      "caption": "Figure 4: (b), CCIM is inserted in a plug-",
      "page": 4
    },
    {
      "caption": "Figure 5: Emotion classification accuracy (%) for each category",
      "page": 7
    },
    {
      "caption": "Figure 5: , while the causal intervention brings",
      "page": 7
    },
    {
      "caption": "Figure 6: Ablation study results for the size N of the confounder",
      "page": 7
    },
    {
      "caption": "Figure 7: Visualization results of vanilla and CCIM-based EMOT-",
      "page": 8
    },
    {
      "caption": "Figure 6: show that selecting the suitable size N for a",
      "page": 8
    },
    {
      "caption": "Figure 7: shows the following observations. In",
      "page": 8
    },
    {
      "caption": "Figure 8: Qualitative results of the vanilla and CCIM-based",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Engagement\nHappiness\nPeace\nPleasure\nSurprise": "Anticipation\nEngagement\nSadness\nNeutral\nDisgust\nNeutral\nAngry",
          "Annoyance\nDoubt/Confusion\nSadness\nSuffering": "Aversion \nEngagement\nPeace\nSuffering\nHappy\nAnger\nSad\nHappy\nNeutral",
          "Happiness\nEngagement\nPeace\nPleasure\nSurprise": "Anticipation\nEngagement\nFatigue\nSadness\nNeutral\nDisgust\nNeutral\nAngry"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Aseel Alhadlaq, Chitra Shashidhar, Wesam Atef Hatamleh, Hussam Tarazi, Prashant Kumar Shukla, and Rajnish Ratna. Humancomputer interaction with detection of speaker emotions using convolution neural networks",
      "authors": [
        "Abeer Ali Alnuaim",
        "Mohammed Zakariah"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Deep neural network for visual emotion recognition based on resnet50 using songspeech characteristics",
      "authors": [
        "Souha Ayadi",
        "Zied Lachiri"
      ],
      "year": "2022",
      "venue": "2022 5th International Conference on Advanced Systems and Emergent Technologies (IC ASET)"
    },
    {
      "citation_id": "3",
      "title": "Context in emotion perception. Current Directions in Psychological",
      "authors": [
        "Lisa Feldman",
        "Batja Mesquita",
        "Maria Gendron"
      ],
      "year": "2011",
      "venue": "Science"
    },
    {
      "citation_id": "4",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya",
        "Trisha Mittal",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Causal intervention for subject-deconfounded facial action unit recognition",
      "authors": [
        "Yingjie Chen",
        "Diqi Chen",
        "Tao Wang",
        "Yizhou Wang",
        "Yun Liang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Shape matters: deformable patch attack",
      "authors": [
        "Zhaoyu Chen",
        "Bo Li",
        "Shuang Wu",
        "Jianghe Xu",
        "Shouhong Ding",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "7",
      "title": "Towards practical certifiable patch defense with vision transformer",
      "authors": [
        "Zhaoyu Chen",
        "Bo Li",
        "Jianghe Xu",
        "Shuang Wu",
        "Shouhong Ding",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "8",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "Learning associative representation for facial expression recognition",
      "authors": [
        "Yangtao Du",
        "Dingkang Yang",
        "Peng Zhai",
        "Mingchen Li",
        "Lihua Zhang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "10",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "Amir Hossein",
        "Xiaojun Qi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "11",
      "title": "Causal inference and developmental psychology",
      "authors": [
        "Foster Michael"
      ],
      "year": "2010",
      "venue": "Developmental psychology"
    },
    {
      "citation_id": "12",
      "title": "Graph reasoning-based emotion recognition network",
      "authors": [
        "Qinquan Gao",
        "Hanxin Zeng",
        "Gen Li",
        "Tong Tong"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Shortcut learning in deep neural networks",
      "authors": [
        "Robert Geirhos",
        "Jörn-Henrik Jacobsen",
        "Claudio Michaelis",
        "Richard Zemel",
        "Wieland Brendel",
        "Matthias Bethge",
        "Felix Wichmann"
      ],
      "year": "2020",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Causal inference in statistics: A primer",
      "authors": [
        "Madelyn Glymour",
        "Judea Pearl",
        "Nicholas Jewell"
      ],
      "year": "2016",
      "venue": "Causal inference in statistics: A primer"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Context-aware emotion recognition based on visual relationship detection",
      "authors": [
        "Manh-Hung Hoang",
        "Soo-Hyung Kim",
        "Hyung-Jeong Yang",
        "Guee-Sang Lee"
      ],
      "year": "2006",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Counterfactually-augmented snli training data does not yield better generalization than unaugmented data",
      "authors": [
        "William Huang",
        "Haokun Liu",
        "Samuel R Bowman"
      ],
      "year": "2020",
      "venue": "Counterfactually-augmented snli training data does not yield better generalization than unaugmented data",
      "arxiv": "arXiv:2010.04762"
    },
    {
      "citation_id": "18",
      "title": "Bounded rationality. Annual review of political science",
      "authors": [
        "D Bryan",
        "Jones"
      ],
      "year": "1999",
      "venue": "Bounded rationality. Annual review of political science"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "20",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Towards simultaneous segmentation of liver tumors and intrahepatic vessels via crossattention mechanism",
      "authors": [
        "Haopeng Kuang",
        "Dingkang Yang",
        "Shunli Wang",
        "Xiaoying Wang",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "Towards simultaneous segmentation of liver tumors and intrahepatic vessels via crossattention mechanism",
      "arxiv": "arXiv:2302.09785"
    },
    {
      "citation_id": "22",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "23",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "Weixin Li",
        "Xuan Dong",
        "Yunhong Wang"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Sequential interactive biased network for context-aware emotion recognition",
      "authors": [
        "Xinpeng Li",
        "Xiaojiang Peng",
        "Changxing Ding"
      ],
      "year": "2021",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "25",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "Xin Liu",
        "Henglin Shi",
        "Haoyu Chen",
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Learning appearance-motion normality for video anomaly detection",
      "authors": [
        "Yang Liu",
        "Jing Liu",
        "Mengyang Zhao",
        "Dingkang Yang",
        "Xiaoguang Zhu",
        "Liang Song"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "27",
      "title": "Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models",
      "authors": [
        "Yang Liu",
        "Dingkang Yang",
        "Yan Wang",
        "Jing Liu",
        "Liang Song"
      ],
      "year": "2023",
      "venue": "Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models",
      "arxiv": "arXiv:2302.05087"
    },
    {
      "citation_id": "28",
      "title": "Hybrid emotion-aware monitoring system based on brainwaves for internet of medical things",
      "authors": [
        "Weizhi Meng",
        "Yong Cai",
        "Laurence Yang",
        "Wei-Yang Chiu"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "29",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "Rameswar Panda",
        "Jianming Zhang",
        "Haoxiang Li",
        "Joon-Young Lee",
        "Xin Lu"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "31",
      "title": "Causal inference in statistics: An overview",
      "authors": [
        "Judea Pearl"
      ],
      "year": "2009",
      "venue": "Statistics surveys"
    },
    {
      "citation_id": "32",
      "title": "Models, reasoning and inference",
      "authors": [
        "Judea Pearl"
      ],
      "year": "2000",
      "venue": "Models, reasoning and inference"
    },
    {
      "citation_id": "33",
      "title": "Automatic Differentiation In Pytorch",
      "year": "2018",
      "venue": "Pytorch"
    },
    {
      "citation_id": "34",
      "title": "Two causal principles for improving visual dialog",
      "authors": [
        "Jiaxin Qi",
        "Yulei Niu",
        "Jianqiang Huang",
        "Hanwang Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "Counterfactual inference for text classification debiasing",
      "authors": [
        "Chen Qian",
        "Fuli Feng",
        "Lijie Wen",
        "Chunping Ma",
        "Pengjun Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "authors": [
        "Kaiming Shaoqing Ren",
        "Ross He",
        "Jian Girshick",
        "Sun"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "37",
      "title": "Context-aware generation-based net for multi-label visual emotion recognition",
      "authors": [
        "Kun Shulan Ruan",
        "Yijun Zhang",
        "Hanqing Wang",
        "Weidong Tao",
        "Guangyi He",
        "Enhong Lv",
        "Chen"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "38",
      "title": "Causal inference using potential outcomes: Design, modeling, decisions",
      "authors": [
        "Donald B Rubin"
      ],
      "year": "2005",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "39",
      "title": "Unbiased scene graph generation from biased training",
      "authors": [
        "Kaihua Tang",
        "Yulei Niu",
        "Jianqiang Huang",
        "Jiaxin Shi",
        "Hanwang Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Shoelace pattern-based speech emotion recognition of the lecturers in distance education: Shoepat23",
      "authors": [
        "Dahiru Tanko",
        "Sengul Dogan",
        "Fahrettin Burak Demir",
        "Mehmet Baygin",
        "Sakir Engin Sahin",
        "Turker Tuncer"
      ],
      "year": "2022",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "41",
      "title": "Causal inference in economics and marketing",
      "authors": [
        "Hal R Varian"
      ],
      "year": "2016",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "42",
      "title": "Caspacenet: Counterfactual analysis for 6d pose estimation in space",
      "authors": [
        "Shunli Wang",
        "Shuaibing Wang",
        "Bo Jiao",
        "Dingkang Yang",
        "Liuzhen Su",
        "Peng Zhai",
        "Chixiao Chen",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "43",
      "title": "Tsa-net: Tube self-attention network for action quality assessment",
      "authors": [
        "Shunli Wang",
        "Dingkang Yang",
        "Peng Zhai",
        "Chixiao Chen",
        "Lihua Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "44",
      "title": "A survey of video-based action quality assessment",
      "authors": [
        "Shunli Wang",
        "Dingkang Yang",
        "Peng Zhai",
        "Qing Yu",
        "Tao Suo",
        "Zhan Sun",
        "Ka Li",
        "Lihua Zhang"
      ],
      "year": "2021",
      "venue": "International Conference on Networking Systems of AI (INSAI)"
    },
    {
      "citation_id": "45",
      "title": "Visual commonsense r-cnn",
      "authors": [
        "Tan Wang",
        "Jianqiang Huang",
        "Hanwang Zhang",
        "Qianru Sun"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "46",
      "title": "Adversarial contrastive distillation with adaptive denoising",
      "authors": [
        "Yuzheng Wang",
        "Zhaoyu Chen",
        "Dingkang Yang",
        "Yang Liu",
        "Siao Liu",
        "Wenqiang Zhang",
        "Lizhe Qi"
      ],
      "year": "2023",
      "venue": "Adversarial contrastive distillation with adaptive denoising",
      "arxiv": "arXiv:2302.08764"
    },
    {
      "citation_id": "47",
      "title": "Explicit and implicit knowledge distillation via unlabeled data",
      "authors": [
        "Yuzheng Wang",
        "Zuhao Ge",
        "Zhaoyu Chen",
        "Xian Liu",
        "Chuangjia Ma",
        "Yunquan Sun",
        "Lizhe Qi"
      ],
      "year": "2023",
      "venue": "Explicit and implicit knowledge distillation via unlabeled data",
      "arxiv": "arXiv:2302.08771"
    },
    {
      "citation_id": "48",
      "title": "Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "49",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "50",
      "title": "Contextual and cross-modal interaction for multi-modal speech emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Yang Liu",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "51",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Shunli Wang",
        "Yang Liu",
        "Peng Zhai",
        "Liuzhen Su",
        "Mingcheng Li",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "52",
      "title": "Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences",
      "authors": [
        "Dingkang Yang",
        "Haopeng Kuang",
        "Shuai Huang",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "53",
      "title": "Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences",
      "authors": [
        "Dingkang Yang",
        "Yang Liu",
        "Can Huang",
        "Mingcheng Li",
        "Xiao Zhao",
        "Yuzheng Wang",
        "Kun Yang",
        "Yan Wang",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "54",
      "title": "A novel ef-ficient multi-view traffic-related object detection framework",
      "authors": [
        "Kun Yang",
        "Jing Liu",
        "Dingkang Yang",
        "Hanqi Wang",
        "Peng Sun",
        "Yanni Zhang",
        "Yan Liu",
        "Liang Song"
      ],
      "year": "2023",
      "venue": "A novel ef-ficient multi-view traffic-related object detection framework",
      "arxiv": "arXiv:2302.11810"
    },
    {
      "citation_id": "55",
      "title": "Robust adversarial reinforcement learning with dissipation inequation constraint",
      "authors": [
        "Peng Zhai",
        "Jie Luo",
        "Zhiyan Dong",
        "Lihua Zhang",
        "Shunli Wang",
        "Dingkang Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Contextaware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2006",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "57",
      "title": "Debiasing distantly supervised named entity recognition via causal intervention",
      "authors": [
        "Wenkai Zhang",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun"
      ],
      "year": "2021",
      "venue": "Debiasing distantly supervised named entity recognition via causal intervention",
      "arxiv": "arXiv:2106.09233"
    },
    {
      "citation_id": "58",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "59",
      "title": "Direct field-to-pattern monolithic design of holographic metasurface via residual encoderdecoder convolutional neural network. Opto-Electronic Advances",
      "authors": [
        "Ruichao Zhu",
        "Jiafu Wang",
        "Tianshuo Qiu",
        "Dingkang Yang",
        "Bo Feng",
        "Zuntian Chu",
        "Tonghao Liu",
        "Yajuan Han",
        "Hongya Chen",
        "Shaobo Qu"
      ],
      "year": "2023",
      "venue": "Direct field-to-pattern monolithic design of holographic metasurface via residual encoderdecoder convolutional neural network. Opto-Electronic Advances"
    }
  ]
}