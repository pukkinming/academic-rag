{
  "paper_id": "2010.06362v2",
  "title": "A Generalized Zero-Shot Framework For Emotion Recognition From Body Gestures",
  "published": "2020-10-13T13:16:38Z",
  "authors": [
    "Jinting Wu",
    "Yujia Zhang",
    "Xiaoguang Zhao",
    "Wenbin Gao"
  ],
  "keywords": [
    "Generalized Zero-Shot Learning (GZSL)",
    "Emotion Recognition",
    "Body Gesture Recognition",
    "Prototype Learning",
    "Multi-task Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although automatic emotion recognition from facial expressions and speech has made remarkable progress, emotion recognition from body gestures has not been thoroughly explored. People often use a variety of body language to express emotions, and it is difficult to enumerate all emotional body gestures and collect enough samples for each category. Therefore, recognizing new emotional body gestures is critical for better understanding human emotions. However, the existing methods fail to accurately determine which emotional state a new body gesture belongs to. In order to solve this problem, we introduce a Generalized Zero-Shot Learning (GZSL) framework, which consists of three branches to infer the emotional state of the new body gestures with only their semantic descriptions. The first branch is a Prototype-Based Detector (PBD) which is used to determine whether an sample belongs to a seen body gesture category and obtain the prediction results of the samples from the seen categories. The second branch is a Stacked AutoEncoder (StAE) with manifold regularization, which utilizes semantic representations to predict samples from unseen categories. Note that both of the above branches are for body gesture recognition. We further add an emotion classifier with a softmax layer as the third branch in order to better learn the feature representations for this emotion classification task. The input features for these three branches are learned by a shared feature extraction network, i.e., a Bidirectional Long Short-Term Memory Networks (BLSTM) with a self-attention module. We treat these three branches as subtasks and use multi-task learning strategies for joint training. The comprehensive experiments are conducted on an emotion recognition dataset based on body gestures, and the performance of our framework is significantly superior to the traditional method of emotion classification and state-of-the-art zero-shot learning methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "In human-human interaction, body language is one of the most important emotional expressions.  Knapp et al.  proposed that body postures and movements are as important as facial expressions for the task of emotion analysis and understanding  [1] . At the same time, body gestures are also of great significance for the analysis of emotion intensity  [2] . However, most existing works on emotion analysis are mainly based on facial expressions and speech. As an important factor to convey emotional information, body language has not been deeply studied in emotion recognition.\n\nThe research on emotion recognition from body language mainly focuses on how to design the artificial features of body motion that are most conducive to emotion recognition. In recent years, with the development of deep learning techniques, many researchers have developed deep networks to exploit the most effective representations of body motion and achieved more accurate emotion classification results. These algorithms mainly apply the existing body detection and feature extraction techniques to the emotion classification task, but the relation between body gestures and emotions are not well established.\n\nAnother limiting factor of existing research is the lack of data due to difficulty in data collection. Most emotion recognition datasets based on body gestures contain only several hundreds of samples  [3] -  [6] , and most of the current research collects the actions performed by the experimenters in a laboratory environment. Body gestures in this collecting method mostly need to be specified by the experiment designer in advance, and the number of the gesture categories is small. However, there are rich meanings in body gestures, and the ways people express their emotions are different. When a new body gesture appears during the test, the recognition algorithm will easily make mistakes. More specifically, existing algorithms cannot give correct prediction results when an new emotional state appears. One method to address the above problem is to expand the training dataset to include as many emotional body gestures as possible. However, it is costly and laborious to collect the labeled data of all categories.\n\nZero-Shot Learning (ZSL) can establish associations between seen and unseen categories with side information such as attributes  [7] ,  [8]  and semantic vectors  [9] . It provides another solution to this problem, which is to recognize new body gesture categories using their semantic descriptions, and then infer the emotion categories from body gesture labels. Note that in ZSL tasks, training and test categories are strictly disjoint, and only data from the seen categories are used during training. However, when data from both seen and unseen categories are available during the test, ZSL methods have an inherent bias towards the seen categories. In other words, the ZSL classifiers tend to misidentify the samples from unseen categories as seen categories  [10] . To solve this problem, GZSL, a more general task is proposed, where samples from the unseen categories are mixed with the seen categories during the test  [11] . It aims to reduce the effect of such bias in a less restricted setting where training and test categories are not disjoint.\n\nAlthough the ZSL approaches for object recognition  [7] ,  [8] ,  [12]  and action recognition  [10] ,  [13]  have been largely investigated and achieved great success, there is no method applied in emotion recognition from body gestures under the ZSL or GZSL settings. Existing approaches are difficult to be directly applied to our task. Firstly, the recognition performance of these algorithms under the GZSL setting has not yet been satisfactory. How to effectively distinguish the seen and unseen categories and achieve high accuracy on both seen and unseen categories at the same time still needs to be studied. In addition, we propose to infer emotions by predicting categories of body gestures, but the existing action classifiers does not embed emotional information during training, which will cause large errors in emotion prediction.\n\nConsidering the above problems, this paper proposes a novel GZSL framework for emotion recognition from body gestures, which can recognize new emotional body gestures and new emotional states. The framework classifies the body gesture categories using their semantic descriptions, and then obtains emotion recognition results from body gesture labels. The GZSL algorithm in this framework is an extension of our previous work  [14] , which is used for zero-shot hand gesture recognition. Based on the previous work, the algorithm in this paper uses a multi-head self-attention module in the feature extraction network to extract long-range temporal correlation information. Manifold regularization is also added to the zeroshot classifier, which is beneficial to construct the tight relations between geature space and semantic space. In addition, different from the existing GZSL algorithms, our framework adds an emotion classifier and jointly trains it with the above GZSL classifiers as subtasks, in order to learn features that are conducive to emotion classification during training. Adding attention modules to these subtasks and adjusting the learning rate of each subtask reseparately are also helpful to improve the accuracy of emoiton recognition.\n\nThe contributions of this paper are as follows:\n\n• A novel framework with three branches is proposed for emotion recognition from body gestures. The first two branches are a Prototype-Based Detector (PBD) and a Stacked AutoEncoder (StAE), which are used to give the prediction results of the seen and unseen body gesture categories respectively. The third branch is an emotion classifier, which can enhance the generalization of the framework. • To the best of our knowledge, this is the first work attempting to recognize different emotions from body gestures under the GZSL setting, which is beneficial to infer a broader range of emotional states when new body gestures are given. • We adopt multi-task learning strategies, which take the three branches as subtasks and share low-level features for joint learning. Attention mechanism is also applied to these subtasks to weight features according to their importance. • We utilize an emotion dataset  [6]  which contains body movement data of multiple users, divide it into different body gesture categories, and design the semantic descriptions. Comprehensive experimental results on this dataset demonstrate the effectiveness of the proposed framework. The rest of the paper is organized as follows. A brief review of emotion recognition from body gestures and zeroshot learning is given in Section II. The overall structure and main modules of our framework are presented in detail in Section III. Section IV provides the experimental results and analysis. Finally, Section V concludes the paper, and provides the discussion and the future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Emotion Recognition From Body Language",
      "text": "The process of emotion recognition from body language generally includes human detection, pose estimation and tracking, feature extraction, and emotion classification. Human detection and pose estimation have been developed as research hotspots in computer vision area. In this paper, we mainly focus on the other two parts: feature extraction and emotion classification.\n\nMost of the emotion recognition methods on body language focus on the whole body, upper body and gestures, where the whole body movements contain the most abundant information  [15] . In traditional feature extraction methods that focus on whole body movements, geometric models such as human skeleton joints are usually established to extract spatial features. Then, the motion features such as movement direction and speed are also extracted for dynamic analysis. Castellano et al.  [4]  extracted time-domain features by calculating parameters such as palm speed, acceleration, fluidity of movement, and made use of the nearest neighbor algorithm, decision tree and Bayesian network for classification. Behoora et al.  [16]  used RGB-D cameras to obtain users' skeletal data, and then analyzed their emotions from body features such as joint position, speed, acceleration. The comparative classification algorithms included decision tree, IBK classifier, random forest and naive Bayes algorithm. Piana et al.  [17]  used Qualisys acquisition system and Microsoft Kinect to collect three-dimensional motion data of the whole body, and automatically extracted global expression features at different levels from joint movements. Then, an abstraction layer based on dictionary learning was used to further process these motion features, and finally SVM was utilized for real-time classification.\n\nIn recent years, deep learning has also been gradually applied to the research of emotion recognition from body gestures. Barros et al.  [18]  proposed a Multichannel Convolutional Neural Network (MCCNN), which took grayscale images and images processed by two different Sobel filters as input to a three-channel convolutional network to extract spatio-temporal features. Multi-channel features were integrated by a fully connected layer, and the emotional states were obtained through logistic regression. Ghayoumi et al.  [19]  used three CNNs to respectively model facial expressions, speech, and gestures in the application of robot interaction systems, and finally fused the features on the decision layer. Ly et al.  [20]  proposed an end-to-end deep learning method. The hash algorithm was used to extract the key frames of the video, and then CNN and convolution LSTM network were used to extract the features. Sun et al.  [21]  processed the original videos to extract skeleton information and obtain the temporal segments of the videos. Then, CNN, BLSTM and PCA algorithms were combined for high-level spatio-temporal feature extraction and classification.\n\nThe above research can only recognize the emotion categories of the emotional body gestures which are seen during training. However, in our method, we propose a GZSL algorithm which can recognize unseen body gestures and classify the corresponding emotions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Zero-Shot Learning And Generalized Zero-Shot Learning",
      "text": "The early works of zero-shot learning directly construct classifiers for attributes of seen and unseen classes. Lampert et al.  [7]  first proposed the task of zero-shot learning and introduced an attribute-based classification approach using high-level descriptions of all categories. Later, some other works propose to learn mappings from feature space to semantic space. For example, Norouzi et al.  [9]  mapped images to class embeddings and estimated unseen labels by combining the embedding vectors of the most possible seen classes. Romera-Paredes et al.  [12]  developed a simple yet effective approach which learned the relationships between features, attributes and categories by adopting a two-layer linear model. More recently, Kodirov et al.  [8]  adopted an encoder-decoder paradigm that was learned with an additional reconstruction constraint to project a visual feature vector into the semantic space. Morgado and Vasconcelos  [22]  proposed two semantic constraints for recognition, namely loss-based regularizer and code-word regularizer, and achieved state-ofthe-art performance. Some other methods further map both features and attributes to a shared space or align the semantic space and feature space. For example, Changpinyo et al.  [23]  introduced \"phantom\" classes to align semantic space and the model space, and the classifiers for the phantom classes were used to synthesize classifiers for unseen classes via convex combinations.\n\nThe limitation of zero-shot learning is that all test data only come from unseen classes. Therefore, a generalized zeroshot learning setting is proposed where both seen and unseen classes are available during the test. Recently, many works have been proposed to address this task by alleviating the data imbalance between seen and unseen categories. For example, Xian et al.  [24]  proposed a generative adversarial network (GAN), which synthesizes CNN features of unseen classes based on class-level semantic information. Another GANbased model achieved improvements in balancing accuracy between seen and unseen classes by combining visual-semantic mapping, semantic-visual mapping and metric learning  [25] . Some other approaches formulate the GZSL task as a crossmodal embedding problem. For instance, Felix et al.  [26]  investigated a multi-modal based algorithm that integrated both visual and semantic Bayesian classifiers and promoted a balanced classification accuracy between seen and unseen classes. Schonfeld et al.  [27]  learned latent features of images and attributes via aligned Variational Autoencoders, and the features contained the essential multi-modal information which was associated with unseen classes. Although these methods mainly target the challenge of data imbalance between seen and unseen categories, the bias still exists due to the similar treatment of all categories. Therefore, some methods train detectors that can distinguish between seen and unseen categories. Bhattacharjee et al.  [28]  proposed a novel detector based on an autoencoder with a reconstruction loss and a triplet cosine embedding loss to determine whether an input sample belongs to a seen or unseen category. This detector greatly improved the recognition performance in novel categories. Mandal et al.  [10]  introduced two separate classifiers for seen and unseen action categories, and synthesized video features for unseen categories to train an out-of-distribution detector. Compared to these methods, our framework combines the classifier for seen classes and the detector as a branch, which has a simpler structure and is more convenient for training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we first formalize the GZSL problem of emotion recognition, then introduce the details of our proposed framework. The proposed framework is shown in Fig.  1 , which consists of a shared feature extraction network and three branches: a Prototype-Based Detector (PBD), a Stacked AutoEncoder (StAE) and an emotion classifier. The feature extraction network is a Bidirectional Long Short-Term Memory Network (BLSTM) with a multi-head self-attention module, and the features are taken as the input to the three branches, respectively. The PBD trains prototypes to classify samples from the seen categories, and learns the threshold for each category to determine whether a sample belongs to this category. The StAE uses a two-layer AutoEncoder with manifold regularization to output predictions of samples from unseen classes. The emotion classifier with a softmax layer is used to enhance the generalization of the above two branches. Furthermore, multi-task learning strategies are applied to our framework and the loss functions of the three branches are jointly optimized.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Definition",
      "text": "Let D s = {x s , y s , em(y s ), a s (y s )} be the set of samples from seen body gesture classes, where x s is the body skeletal sequence, y s is the body gesture label of x s in the set of seen body gesture classes Y s , em(y s ) ∈ {1, • • • , C em } is the corresponding emotion label of y s where C em is the number of  In the GZSL task, all of the training samples come from the seen categories. The training data can be denoted as\n\n, where N tr is the number of training samples. In addition, samples in the test set are from both the seen and unseen categories, which can be denoted as X te . The goal of GZSL is to learn a classifier f : X → Y u ∪ Y s . Note that in this emotion recognition task, the body gesture label ŷ of the test sample x is first obtained through the GZSL algorithm, its emotion recognition result em(ŷ) is then achieved.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Feature Extraction With Multi-Head Self-Attention Module",
      "text": "The input sequences which include the position and orientation of the body's skeleton joints are captured by a Microsoft Kinect device. Multi-layer Bidirectional Long Short-Term Memory Networks (BLSTM)  [29]  are used for feature extraction to simultaneously capture both the past and future contextual information. A BLSTM layer is composed of two LSTM layers (a forward one and a backward one). The outputs of the BLSTM are high-level features, which will be input into three branches.\n\nIn order to extract long-range temporal correlation information from the body gesture sequences, the multi-head selfattention module is added in front of the BLSTM. The multihead attention based on the scaled dot-product attention was first proposed for machine translation tasks  [30] . The multihead structure is conducive to handling complex associations, and the interaction of distant frames can be achieved by the self-attention method.\n\nThe input of the scaled dot-product attention consists of three components: queries Q ∈ R d k ×n , keys K ∈ R d k ×n and values V ∈ R dv×n , where d k , d k and d v are the dimensions of the queries, keys and values. In our framework, we set Q, K and V using the same input skeleton sequence x ∈ R dx×lx to build a self-attention module, where d x and l x are the dimension and the length of x, respectively. The queries, keys and values are linearly projected h times, and the outputs are concatenated and projected again to obtain the final attention result, which is given by:\n\nx at = MultiHead (x, x, x)\n\nwhere\n\nFor the input data X = {x i } N i=1 , the output of the attention module is denoted as\n\nThen, the features extracted by the BLSTM network can be represented as:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Prototype-Based Detector (Pbd) With Threshold Selection",
      "text": "Traditionally, a softmax layer is added on top of the network for classification. However, the softmax-based approaches tend to misclassify unseen classes to seen classes. To solve this problem, Yang et al.  [31]  proposed a convolutional prototype learning (CPL) framework, which can improve the robustness of classification. CPL is intended to learn a few prototypes using CNN features and predict classification labels by matching representations of test samples in the prototype space with the closest prototype. Inspired by this framework, we use a fully connected network to learn a prototype for each class and add an automatic threshold selection module to determine whether a test sequence belongs to a seen category.\n\nThe features input into the PBD model can be denoted as H pbd . The features are projected to the prototype space through FC layers. The projection of the i th sample in the prototype space is denoted as p i , and the learned prototypes are defined as M = {m (k)|k = 1, • • • , C s }, where m (k) represents the prototype of the k th seen category. The parameters of FC layers and the prototypes M are jointly trained by minimizing the distance-based cross entropy (DCE) loss and prototype loss (PL).\n\nThe distance-based cross entropy (DCE) loss is based on the traditional cross entropy loss. It can be defined as:\n\nwhere\n\n2 computes the distance between the p i and the prototype of the k th category m (k), y i s is the label of the i th sample, and γ is a hyper-parameter. Minimizing the DCE loss helps improve the classification accuracy and enhance the separability among different training categories.\n\nThe prototype loss (PL) is used as a normalization to regularize the model and enhance the intra-class compactness, which is defined as:\n\nIn the PBD branch, we also learn the distance thresholds for all prototypes. The sample whose distance from the closest prototype is greater than the corresponding threshold is classified as the unseen categories. The thresholds are defined as Th = {th (k)|k = 1, • • • , C s }, where th (k) represents the threshold corresponding to the prototype m (k). The loss function of threshold selection is given by:\n\nwhere In summary, the total loss function of the PBD module is defined as:\n\nwhere β 1 , β 2 and β 3 are hyper-parameters that weight the above loss functions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Stacked Autoencoder (Stae)",
      "text": "In GZSL tasks, semantic representations of both seen and unseen categories are available during the test. In order to recognize unseen gestures, a model needs to learn the relationship between the extracted features and the high-level semantic representations. In our previous work  [14] , we used a two-layer linear SAE to learn the semantic representation of each sample and match it with the semantic prototypes. However, the linear projection is difficult to model the complex correlation between the manifold structure of the visual feature space and the semantic space. In this section, we adopt a two-layer Stacked AutoEncoder (StAE) with manifold regularizations  [32]  to capture the manifold structures in both visual and semantic spaces and construct the tight relations of the two spaces.\n\nDuring training, the input features H stae tr of the training samples are projected into the semantic space with the first layer of the encoder U T ∈ R ds×d h , where d s and d h are the dimension of the attributes and the input features. Then, the second layer uses the semantic representation matrix A s T ∈ R Cs×ds of all seen semantic prototypes to establish the relationship between the semantic space and the label space. The structure of the decoder is symmetrical to the encoder, and we use the tied weights by setting the parameters of the decoder to A s and U. This auto-encoder aims to make the mapped embeddings in the label space close to the given labels, and at the same time, retain the original input information through the reconstruction of the decoder. The objective function is formulated as:\n\nwhere Y tr is the one-hot vectors of the body gesture labels in the training set, and γ 1 is the hyper-parameter which weights the above two terms.\n\nTo model the manifold structure, we integrate two manifold regularizers  [32]  based on the instance graph and feature graph into this objective function. For instance graph G I with n instances, the similarity matrix W I ∈ R n×n can be defined as:\n\nwhere N q (h * k ) denotes the top-q nearest neighbors of the k th instance h * k . By denoting\n\n, the instance manifold regularizer can be defined as:\n\nSimilarly, for feature graph G F with d vertices which represent features, the similarity matrix W F ∈ R d×d can be defined as:\n\nwhere N r (h k * ) denotes the top-r nearest neighbors of the k th dimension of feature h k * . By denoting\n\n, the feature manifold regularizer can be defined as:\n\nwhere B=A T U T . Then the final objective function of StAE can be formulated as:\n\nIn the training stage, the instance manifold regularizer R I tr for the training set is a constant variable. So we only need to optimize the following loss function:\n\nDuring the test, because the StAE module is only used for the classification of unseen categories, we only use the semantic representations of the unseen categories A u . Considering that the feature manifold regularizer R F te for the test set is a constant variable, a similar joint prediction mechanism based on test instances can be defined as:\n\nwhere H stae te is the features of test samples, R I te represents instance manifold regularizer for the test set. By calculating derivative of the above equation and setting it to be zero, we can get the Sylvester equation as:\n\nLet\n\nThen, this problem can be solved by using Bartels-Stewart algorithm  [33] :\n\nThe prediction result Y * te is obtained by selecting the entity with the largest score of Y te :\n\nwhere Y l te is the value of the row corresponding to the l th category in Y te .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Muti-Task Learning For Emotion Recognition",
      "text": "In the above sections, we introduce two branches: PBD and StAE, for the recognition of the seen and unseen body gesture categories, respectively. We can regard these two models as parallel subtasks to improve their generalization through joint learning. On the basis of this structure, multi-task learning strategies are introduced in our framework.\n\nFirst, the subtasks share the feature extraction network, and jointly affect the parameters of the multi-head self-attention module and BLSTM during training. It is worth noting that the final task of our work is emotion classification. In order to learn the features which are more conducive to emotion recognition, we add an emotion classifier as the third subtask. This emotion recognition branch is not used for the result prediction in our framework, but only as an auxiliary task for the training of the other two branches. Specifically, it consists of multiple fully connected (FC) layers and a softmax layer. The cross-entropy loss function of this branch can be defined as:\n\nCem j=1 e g i j ,\n\nwhere g i j represents the component of the i th sample corresponding to the j th emotion category in the output vector of the FC layers, and similarly g i em i represents the component of the i th sample corresponding to its ground-truth emotion category.\n\nThen, because the features extracted by BLSTM contribute differently to each subtask, the attention mechanism is applied to all branches to focus on the most important feature components for each subtask. It uses a hidden FC layer to calculate the distribution of attention and a softmax function for normalization, and finally multiplies the attention weights with the input to obtain new features. The attention function is defined as:\n\nwhere F is the features extracted by the BLSTM network,\n\nare parameters of the hidden FC layer, and d f is the dimension of the features F. Three attention modules are trained for the above three branches, and the weighted features are represented as H pbd , H stae and H em . Then they are used for branch training respectively. However, it is difficult to train this network with a small amount of data. Especially, it is difficult to coordinate the loss weights and the learning rates of subtasks. Therefore, we divide the variables of our framework into four groups, which are shared networks and three branches. Then, we set different learning rates for the four groups, and adjust the weights of the loss function in each branch separately. The total loss function is defined as:\n\nwhere λ 1 and λ 2 are hyper-parameters which weight the loss terms of the above three branches.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "F. Label Prediction",
      "text": "During the test, the samples which belong to the seen body gesture categories are predicted by the PBD branch, and the prediction results of the samples from the unseen body gesture categories are given by the StAE branch. The process of the label prediction is shown in Alg. 1, where ∆d is defined in  (25) .\n\nFor an input sample x, the extracted features are input into the PBD branch, and its projection p in the prototype space is obtained. The body gesture category of the prototype which is closest to this projection is represented as: Ê (x) ← em (ε stae )\n\nThen, the model distinguishes the seen and unseen categories by comparing the minimum distance with the corresponding threshold:\n\nwhere ŷpbd (x) represents the intermediate prediction result of PBD branch. Furthermore, for the sample which is considered to be from an unseen category, its feature is further input into the StAE branch. The prediction result of the StAE branch ε stae (x) is given by  (20) .\n\nIn summary, the emotion recognition result Ê (x) of the test sample x is as follows:\n\nIV. EXPERIMENTS A. Experiment Settings 1) Dataset: We evaluate our framework on a public emotion dataset, MASR  [6] , which contains facial expressions and body gestures that are captured by Microsoft Kinect sensor in game scenes. In our experiments, we only use the gesture data to determine users' emotions. The skeleton data collected by Microsoft Kinect sensor is conducive to accurately extracting the 3D motion features of the human body, and it contains more information than the datasets that only collect the upper body data  [3] ,  [5] . In this dataset, each subject performs two different body gestures of each emotion state in his own style, and each gesture was repeated 5 times. After removing incomplete and wrong samples, it contains 688 videos of 5 basic emotion states (anger, fear, happiness, sadness, surprise) which are performed by 15 subjects. Because our proposed framework recognizes emotions by classifying body gestures, we define the body gesture categories for each emotion state, and provide additional gesture labels to the dataset. There were 30 body gestures in our dataset. Due to the fact that different subjects have different habitual ways of expressing emotions, the amount of the samples in each gesture category is different.\n\nThe body gesture categories in this paper and the number of their corresponding instances are shown in the Table  I .\n\nBecause the GZSL algorithms identify unseen categories by establishing the relationship between the semantic descriptions of seen and unseen body gestures, we design attribute vectors for all body gesture categories. Most of our attributes focus on the upper body, because upper body postures, especially arm movements, contain the most emotional information. The attributes which are used to encode the postures or movement trends of the hands and arms are continuous. The other attributes are binary, which are used to describe body movement trends, head movement, symmetry, speed, etc. The attributes are visualized in the form of heat map in Fig.  2 .\n\n2) Evaluation Metrics: We adopt the top-1 accuracy to evaluate the models, and the top-1 accuracies of the seen gesture classes and unseen body gesture classes are denoted as Acc s and Acc u . In order to ensure that both Acc s and Acc u are high enough, we also use harmonic mean H for the performance comparison, which can be defined as:\n\nIn addition to body gesture recognition, we also analyze the accuracy and harmonic mean in emotion recognition, which are denoted as Acc em s , Acc em u and H em . 3) Dataset Partition: In order to verify the performance of our framework, we propose two dataset partition settings.\n\nIn the first setting, we choose two body gestures as unseen categories in each emotion category. The seen and unseen categories are shown in the \"Partition1\" column in Table  I . On the basis of ensuring that each category has at least one test sample, we randomly select approximately 1/10 samples from each seen category and add them into the test set. All samples of the unseen categories only exist in the test set. In total, the training set contains 554 samples, while the test set contains 70 samples from the seen categories and 64 samples from the unseen categories. The experimental results in this setting will be analyzed in Section IV-B.\n\nThe second setting is more specific, that is, the unseen gesture categories only come from the unseen emotion categories. In this setting, we regard all the body gestures of \"angry\" as unseen categories, as shown in the \"Partition2\" column in Table  I . The training set consists of 493 samples, while the test set includes 58 samples from the seen categories and 137 samples from the unseen categories. In this setting, traditional emotion classifiers cannot give the correct results of unseen emotion categories. The experimental results in this setting are analyzed in Section IV-C.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4) Implementation Details:",
      "text": "In feature extraction network, the number of the heads in the self-attention module is set to 5. We utilize a three-layer BLSTM network to extract features, and the numbers of forward and backward LSTM neurons are set to 64. The dimension of the output features of BLSTM is 128, and it remains unchanged after the attention module of each branch. In the branch of PBD, two FC layers which has 50 and 20 units respectively are used to project the features into the prototype space. The activation function we use in the network is ReLU, and the hyper-parameter γ is set to 0.5. In StAE branch, the similarity matrices W I and W F are calculated based on the top-5 nearest neighbors. The emotion classifier consists of two FC layers, which has 50 and 5 units. During training, the batch size is set to 8. The Adam optimizer  [34]  is utilized to minimize the loss. Other hyper-parameters are experimentally set for different partitions of the dataset, which are shown in Table  II . The traditional emotion classifier is composed of a threelayer BLSTM and a softmax layer, and it only uses emotion labels for training. We also choose two state-of-the-art GZSL algorithms, CADA-VAE  [27]  and f-CLSWGAN  [24] , for the comparisons on our dataset. The features of these algorithms are also obtained using a three-layer BLSTM. We further compare our framework with our previous work  [14]  which includes a PBD and a Semantic Autoencoder (SAE). Experimental results are shown in Table  III .\n\nThe traditional emotion classifier achieves the highest emotion recognition accuracy of the seen classes. However, the accuracy of the unseen classes is the lowest. Our framework outperforms the other zero-shot learning methods in body gesture recognition, and Acc s , Acc u and H are increased by 4.29%, 7.81% and 9.79%, respectively. Because our main purpose is emotion classification, correctly predicting the emotion labels is much more important. Although our method has no advantage while testing the samples from the seen categories, Acc em u increases from 54.69% to 64.06%, and H em increases from 68.44% to 74.35%. In our framework, two separate classifiers are used for gesture prediction, thus the influence of the inherent bias are reduced. This leads to a great improvement of our algorithm on the unseen categories. The application of the emotion branch and multi-task learning strategies enhances the adaptability of our framework for the task of emotion classification.\n\n2) Ablation Analysis: We analyze the different components in our framework to verify the effectiveness of these modules.\n\nMulti-head Self-attention Module. We compare the framework without multi-head self-attention module to our framework in order to evaluate the effectiveness of this module. The experimental results are shown in Table  IV  (line 1). We can observe that the accuracies of both gesture recognition and emotion recognition is lower after dropping the self-attention module in feature extraction. This demonstrates that adding this module helps to extract more effective long-range features, and ultimately improves the classification performance.\n\nAttention Modules of Three Branches. The experimental results without the branch attention modules are shown in Table  IV  (line 2). We observe that the framework without attention modules has a higher accuracy for the seen gesture categories, but the performance on the unseen categories is lower. This demonstrates that the learned features are biased towards the PBD branch, and the training of StAE is restricted. The use of the attention mechanism helps to focus on the most important features for each task, so as to coordinate the needs and goals of all tasks and make each task achieve better results.\n\nEmotion Branch. The accuracy of the framework without the emotion branch is shown in Table  IV  (line 3). Although its Acc s is 8.57% higher than our framework, the Acc em s is 4.28% lower. The results show that adding the emotion branch can improve the generalization ability of our framework and learn shared features that are more conducive to emotion classification.\n\nLearning Rate. In this experiment, we set the same learning rate for all network parameters and optimize them uniformly. After testing, the optimal learning rate is 0.00005, and the corresponding results are shown in Table  IV  (line 4). In fact, during training, the StAE branch converges faster and is prone to overfitting. Therefore, in our framework, the learning rates of the StAE branch are set to be smaller, and the learning rate of the PBD and shared network is set to be larger, so that all subtasks can achieve the best recognition results at the same training stage.\n\n3) Parameter Analysis: In this section, we will focus on analyzing the weight coefficients of the loss functions of each branch to illustrate the role of each module on the overall framework and the impact of different weights on the performance of recognition.\n\nParameters of PBD. In this section, we discuss the influence of the weights of the loss functions in PBD branch. First, β 1 is the weight of loss pl , which will affects the intra-class compactness of the projections in the prototype space. As β 1 increases, the intra-class distance between the samples from the same categories decreases, and it is easier to distinguish seen and unseen categories using the thresholds. However, a too high β 1 will affect the optimization process of other loss functions, which in turn leads to a lower recognition accuracy of PBD and StAE. The results of different β 1 are shown in Fig.  3(a) . In addition, the learning of the thresholds determines the accuracy of distinguishing between the seen and unseen categories, which is of great significance for improving the recognition performance of the framework. The two parameters that are related to threshold learning are β 2 and β 3 . The increase of β 2 tends to increase the thresholds to include as many training samples of the same category as possible, and the increase of β 3 reduces the thresholds by removing outliers. Because the thresholds are related to the compactness of the samples, these two parameters also need to be adjusted according to the value of β 1 . In the experiment, we also find that too large β 2 and β 3 will cause the related loss functions to be dominant in the early stage of training,   and make the algorithm difficult to converge. Therefore, we set β 2 and β 3 to 0 at the beginning of training, and then change the value of these two weights in the later stage of training.",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Parameters Of Stae.",
      "text": "In StAE, we mainly analyze the influence of γ 1 , γ 2 , and γ 3 on the accuracy of the framework, and the experimental results are shown in Fig.  3(b)-(d) . The increase in γ 1 enhances the reconstruction ability of the Autoencoder and better exploits the feature representations, which will result in an increase in the accuracy of StAE recognition. However, too large γ 1 will make the StAE branch unable to converge. When the value of γ 2 is very small, it is similar to the traditional Auto-encoder. With the increase of γ 2 , the manifold structures in the visual space and the semantic space are better described. However, when γ 2 is too large, the algorithm is also difficult to converge. Finally, because γ 3 only appears in the test process, we give a more intuitive test result of the StAE branch without threshold discrimination. With the increase in γ 3 , Acc u (StAE) and Acc em u (StAE) have a synchronous trend of first increasing and then decreasing.\n\nParameters of Emotion Classier. In the Section IV-B, we compare our framework to the method without the emotion branch. The influence of different values of λ 2 is also shown in Fig.  3 (e). Acc s decreases as λ 2 increases, because the emotion classier affects the convergence and the learned thresholds of PBD. However, as λ 2 increases, a stronger correlation between different body gestures of the same emotion category is established, and the generalization of the whole framework is enhanced. So the accuracies of emotion recognition have an upward trend in a certain range.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Gzsl Experiments For Unseen Emotion Categories",
      "text": "In the second data partition setting, all of the test samples come from an unseen emotion category (anger). Because traditional emotion classifier cannot obtain the correct recognition results of unseen categories, we only compare several state-ofthe-art zero-shot learning methods with our framework. The training parameters are shown in In fact, due to the small amount of samples in some training gesture categories, Acc u and Acc em s are lower than the experimental results in Section IV-B. The value of Acc em u is higher since there is only one unseen emotion category in this experiment. As long as the test sample is detected as unseen, it can be classified into the correct emotion category. As the number of unseen emotion categories increases, this value will decrease accordingly.\n\nV. CONCLUSION In this paper, we introduce a novel generalized zero-shot learning (GZSL) framework for emotion recognition using body gestures. The framework contains three branches. The first branch is a Prototype-Based Detector (PBD), which is used to identify the seen gesture categories and determine whether the samples come from the unseen categories through the learned thresholds. The second branch is a Stacked Au-toEncoder (StAE) with manifold regularization, which is used to classify the samples that are predicted to belong to the unseen categories. The third branch is an emotion classifier to enhance the generalization of the recognition framework. The experimental results on the MASR dataset verify that our algorithm is capable of handling unseen gesture categories or unseen emotion categories. In future work, we aim to investigate Few-Shot Learning (FSL) and Generative Adversarial Networks (GAN) to fully exploit the representative information in a small number of samples and further improve the performance. Besides, expressions, physiological signals and other modals could also be combined to make comprehensive judgments to reduce the interference of gesture ambiguity.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which consists of a shared feature extraction network",
      "page": 3
    },
    {
      "caption": "Figure 1: An overview of the proposed framework. The framework consists of a shared feature extraction network and three branches: a Prototype-Based",
      "page": 4
    },
    {
      "caption": "Figure 2: 2) Evaluation Metrics: We adopt the top-1 accuracy to",
      "page": 8
    },
    {
      "caption": "Figure 2: Heat map of the categories and attributes.",
      "page": 8
    },
    {
      "caption": "Figure 3: (a). In addition, the learning of the thresholds",
      "page": 9
    },
    {
      "caption": "Figure 3: Experimental results of parameter analysis. (a)Accuracies of different β1; (b)Accuracies of different γ1; (c)Accuracies of different γ2; (d)Accuracies",
      "page": 10
    },
    {
      "caption": "Figure 3: (b)-(d). The",
      "page": 11
    },
    {
      "caption": "Figure 3: (e). Accs decreases as λ2 increases, because the emotion",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion\nCategory": "Happy",
          "Body Gesture\nCategory": "0",
          "The Instance\nNumber": "67",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "1",
          "The Instance\nNumber": "4",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "2",
          "The Instance\nNumber": "5",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "3",
          "The Instance\nNumber": "15",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "4",
          "The Instance\nNumber": "34",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "5",
          "The Instance\nNumber": "20",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "Sad",
          "Body Gesture\nCategory": "6",
          "The Instance\nNumber": "68",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "7",
          "The Instance\nNumber": "63",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "8",
          "The Instance\nNumber": "5",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "9",
          "The Instance\nNumber": "4",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "Surprise",
          "Body Gesture\nCategory": "10",
          "The Instance\nNumber": "37",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "11",
          "The Instance\nNumber": "60",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "12",
          "The Instance\nNumber": "24",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "13",
          "The Instance\nNumber": "5",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "14",
          "The Instance\nNumber": "4",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "15",
          "The Instance\nNumber": "4",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "16",
          "The Instance\nNumber": "5",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "Fear",
          "Body Gesture\nCategory": "17",
          "The Instance\nNumber": "37",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "18",
          "The Instance\nNumber": "25",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "19",
          "The Instance\nNumber": "15",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "20",
          "The Instance\nNumber": "28",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "21",
          "The Instance\nNumber": "5",
          "Partition1": "unseen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "22",
          "The Instance\nNumber": "10",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "23",
          "The Instance\nNumber": "7",
          "Partition1": "seen",
          "Partition2": "seen"
        },
        {
          "Emotion\nCategory": "Anger",
          "Body Gesture\nCategory": "24",
          "The Instance\nNumber": "55",
          "Partition1": "seen",
          "Partition2": "unseen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "25",
          "The Instance\nNumber": "20",
          "Partition1": "seen",
          "Partition2": "unseen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "26",
          "The Instance\nNumber": "2",
          "Partition1": "unseen",
          "Partition2": "unseen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "27",
          "The Instance\nNumber": "32",
          "Partition1": "seen",
          "Partition2": "unseen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "28",
          "The Instance\nNumber": "23",
          "Partition1": "seen",
          "Partition2": "unseen"
        },
        {
          "Emotion\nCategory": "",
          "Body Gesture\nCategory": "29",
          "The Instance\nNumber": "5",
          "Partition1": "unseen",
          "Partition2": "unseen"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Parameters": "lrshared\nlrpbd\nlrstae\nlremotion\nβ1\nβ2\nβ3\nγ1\nγ2\nγ3\nλ1\nλ2",
          "Values of Partition1": "0.0001\n0.0001\n0.00002\n0.00002\n4\n0.1\n1\n0.001\n0.0001\n0.1\n1\n4",
          "Values of Partition2": "0.0001\n0.00005\n0.00002\n0.00005\n2\n0.05\n1\n0.0001\n0.0001\n0.1\n1\n2"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "Emotion Classiﬁer\nCADA-VAE [27]\nf-CLSWGAN [24]\nOur Previous Work [14]\nOur Framework",
          "Accs": "/\n52.86%\n48.57%\n57.14%\n61.43%",
          "Accu": "/\n17.19%\n21.88%\n23.44%\n31.25%",
          "H": "/\n25.94%\n30.17%\n33.24%\n41.43%",
          "Accem\ns": "94.29%\n91.43%\n88.57%\n92.86%\n88.57%",
          "Accem\nu": "46.88%\n54.69%\n53.13%\n53.13%\n64.06%",
          "H em": "62.62%\n68.44%\n66.42%\n67.59%\n74.35%"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "No Multi-head Self-attention Module\nNo Attention Modules of Three Branches\nNo Emotion Branch\nSame Learning Rate\nOur Framework*",
          "Accs": "62.86%\n71.43%\n70.00%\n60.00%\n61.43%",
          "Accu": "20.31%\n18.75%\n26.56%\n21.87%\n31.25%",
          "H": "30.70%\n29.70%\n38.51%\n32.06%\n41.43%",
          "Accem\ns": "84.29%\n87.14%\n84.29%\n81.43%\n88.57%",
          "Accem\nu": "56.25%\n54.69%\n57.81%\n57.81%\n64.06%",
          "H em": "67.47%\n67.20%\n68.58%\n67.62%\n74.35%"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Nonverbal communication in human interaction",
      "authors": [
        "M Knapp",
        "J Hall",
        "T Horgan"
      ],
      "year": "2013",
      "venue": "Nonverbal communication in human interaction"
    },
    {
      "citation_id": "2",
      "title": "The contribution of general features of body movement to the attribution of emotions",
      "authors": [
        "M Meijer"
      ],
      "year": "1989",
      "venue": "Journal of Nonverbal behavior"
    },
    {
      "citation_id": "3",
      "title": "A bimodal face and body gesture database for automatic analysis of human nonverbal affective behavior",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2006",
      "venue": "18th International Conference on Pattern Recognition (ICPR'06)"
    },
    {
      "citation_id": "4",
      "title": "Recognising Human Emotions from Body Movement and Gesture Dynamics, ser",
      "authors": [
        "G Castellano",
        "S Villalba",
        "A Camurri"
      ],
      "year": "2007",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "5",
      "title": "A study on emotion recognition from body gestures using kinect sensor",
      "authors": [
        "S Saha",
        "S Datta",
        "A Konar",
        "R Janarthanan"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Communication and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Multimodal affective state recognition in serious games applications",
      "authors": [
        "A Psaltis",
        "K Kaza",
        "K Stefanidis",
        "S Thermos",
        "K Apostolakis",
        "K Dimitropoulos",
        "P Daras"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Imaging Systems and Techniques (IST"
    },
    {
      "citation_id": "7",
      "title": "Learning to detect unseen object classes by between-class attribute transfer",
      "authors": [
        "C Lampert",
        "H Nickisch",
        "S Harmeling"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Semantic autoencoder for zero-shot learning",
      "authors": [
        "E Kodirov",
        "T Xiang",
        "S Gong"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Zero-shot learning by convex combination of semantic embeddings",
      "authors": [
        "M Norouzi",
        "T Mikolov",
        "S Bengio",
        "Y Singer",
        "J Shlens",
        "A Frome",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Zero-shot learning by convex combination of semantic embeddings"
    },
    {
      "citation_id": "10",
      "title": "Out-of-distribution detection for generalized zeroshot action recognition",
      "authors": [
        "D Mandal",
        "S Narayan",
        "S Dwivedi",
        "V Gupta",
        "S Ahmed",
        "F Khan",
        "L Shao"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Zero-shot learninga comprehensive evaluation of the good, the bad and the ugly",
      "authors": [
        "Y Xian",
        "C Lampert",
        "B Schiele",
        "Z Akata"
      ],
      "year": "2019",
      "venue": "IEEE Trans Pattern Anal Mach Intell"
    },
    {
      "citation_id": "12",
      "title": "An Embarrassingly Simple Approach to Zero-Shot Learning",
      "authors": [
        "B Romera-Paredes",
        "P Torr"
      ],
      "year": "2017",
      "venue": "An Embarrassingly Simple Approach to Zero-Shot Learning"
    },
    {
      "citation_id": "13",
      "title": "Multi-task zero-shot action recognition with prioritised data augmentation",
      "authors": [
        "X Xu",
        "T Hospedales",
        "S Gong"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "A prototype-based generalized zero-shot learning framework for hand gesture recognition",
      "authors": [
        "J Wu",
        "Y Zhang",
        "X Zhao"
      ],
      "year": "2009",
      "venue": "CoRR"
    },
    {
      "citation_id": "15",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Machine learning classification of design team members' body language patterns for real time emotional state detection",
      "authors": [
        "I Behoora",
        "C Tucker"
      ],
      "year": "2015",
      "venue": "Design Studies"
    },
    {
      "citation_id": "17",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "S Piana",
        "A Stagliano",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "18",
      "title": "Multimodal emotional state recognition using sequence-dependent deep hierarchical features",
      "authors": [
        "P Barros",
        "D Jirak",
        "C Weber",
        "S Wermter"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "19",
      "title": "Emotion in Robots Using Convolutional Neural Networks, ser",
      "authors": [
        "M Ghayoumi",
        "A Bansal"
      ],
      "year": "2016",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition via body gesture: Deep learning model coupled with keyframe selection",
      "authors": [
        "S Ly",
        "G.-S Lee",
        "S.-H Kim",
        "H.-J Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Affect recognition from facial movements and body gestures by hierarchical deep spatio-temporal features and fusion strategy",
      "authors": [
        "B Sun",
        "S Cao",
        "J He",
        "L Yu"
      ],
      "year": "2018",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "22",
      "title": "Semantically consistent regularization for zero-shot recognition",
      "authors": [
        "P Morgado",
        "N Vasconcelos"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Synthesized classifiers for zero-shot learning",
      "authors": [
        "S Changpinyo",
        "W.-L Chao",
        "B Gong",
        "F Sha"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Feature generating networks for zero-shot learning",
      "authors": [
        "Y Xian",
        "T Lorenz",
        "B Schiele",
        "Z Akata"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Generative dual adversarial network for generalized zero-shot learning",
      "authors": [
        "H Huang",
        "C Wang",
        "P Yu",
        "C.-D Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Multi-modal ensemble classification for generalized zero shot learning",
      "authors": [
        "R Felix",
        "M Sasdelli",
        "I Reid",
        "G Carneiro"
      ],
      "year": "1901",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "Generalized zero-and few-shot learning via aligned variational autoencoders",
      "authors": [
        "E Schonfeld",
        "S Ebrahimi",
        "S Sinha",
        "T Darrell",
        "Z Akata"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Autoencoder based novelty detection for generalized zero shot learning",
      "authors": [
        "S Bhattacharjee",
        "D Mandal",
        "S Biswas"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "authors": [
        "A Graves",
        "J Schmidhuber"
      ],
      "year": "2005",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "30",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "U Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "Robust classification with convolutional prototype learning",
      "authors": [
        "H.-M Yang",
        "X.-Y Zhang",
        "F Yin",
        "C.-L Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Zero-shot learning using stacked autoencoder with manifold regularizations",
      "authors": [
        "J Song",
        "G Shi",
        "X Xie",
        "D Gao"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "33",
      "title": "Solution of the matrix equation ax+ xb= c [f4]",
      "authors": [
        "R Bartels",
        "G Stewart"
      ],
      "year": "1972",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "34",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    }
  ]
}