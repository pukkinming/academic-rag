{
  "paper_id": "2004.08495v1",
  "title": "Breg-Next: Facial Affect Computing Using Adaptive Residual Networks With Bounded Gradient",
  "published": "2020-04-18T00:26:36Z",
  "authors": [
    "Behzad Hasani",
    "Pooran Singh Negi",
    "Mohammad H. Mahoor"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper introduces BReG-NeXt, a residual-based network architecture using a function wtih bounded derivative instead of a simple shortcut path (a.k.a. identity mapping) in the residual units for automatic recognition of facial expressions based on the categorical and dimensional models of affect. Compared to ResNet, our proposed adaptive complex mapping results in a shallower network with less numbers of training parameters and floating point operations per second (FLOPs). Adding trainable parameters to the bypass function further improves fitting and training the network and hence recognizing subtle facial expressions such as contempt with a higher accuracy. We conducted comprehensive experiments on the categorical and dimensional models of affect on the challenging in-the-wild databases of AffectNet, FER2013, and Affect-in-Wild. Our experimental results show that our adaptive complex mapping approach outperforms the original ResNet consisting of a simple identity mapping as well as other state-of-the-art methods for Facial Expression Recognition (FER). Various metrics are reported in both affect models to provide a comprehensive evaluation of our method. In the categorical model, BReG-NeXt-50 with only 3.1M training parameters and 15 MFLOPs, achieves 68.50% and 71.53% accuracy on AffectNet and FER2013 databases, respectively. In the dimensional model, BReG-NeXt achieves 0.2577 and 0.2882 RMSE value on AffectNet and Affect-in-Wild databases, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A FFECTIVE computing seeks to develop algorithms, sys- tems and possibly devices that are capable of recognizing, interpreting, and simulating human emotions through different channels such as the face, voice, and biological signals  [1] . Facial expressions are the most important nonverbal channels used by human beings to convey internal feelings and emotions. There have been numerous efforts to develop robust and reliable automated Facial Expression Recognition (FER) systems that can understand human emotions and interact with subjects accordingly. However, currently available systems are far from reaching a comprehensive understanding of the emotional and social capabilities necessary for rich and robust Human Machine Interaction (HMI). This is predominantly due to the fact that HMI systems interact with humans in an uncontrolled environment (a.k.a. wild settings), where the scene lighting, camera view, image resolution, background, subjects' head pose, gender, and ethnicity can vary significantly  [2] ,  [3] .\n\nThree models of categorical, dimensional, and Facial Action Coding System (FACS) are proposed in the literature to quantify affective facial behaviors:\n\n1) Categorical model: Emotion is chosen from a list of affective-related categories that Ekman et al.  [4]  define as six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. 2) Dimensional model: A value is assigned to emotion over a continuous emotional scale, such as \"valence\" and \"arousal\", as defined by Russel  [5] ,\n\nwhere valence shows how positive or negative an emotion is, and arousal indicates how much an event is intriguing or calming. This model can distinguish between subtle changes in exhibiting affect and encode these small differences in the intensity of each emotion on a continuous scale (e.g., valence and arousal). 3) FACS model: All possible facial actions are described in terms of 33 Action Units (AUs)  [6] . This model only describes facial movements and does not interpret the affective states directly.\n\nIn traditional computer vision approaches, engineered features are used to describe visual patterns and build classifiers for visual object recognition. Alternately, Deep Neural Networks (DNNs) have the ability to learn and extract more discriminative features which yield in a better interpretation of the human face texture in visual data. Despite the superiority of DNNs over traditional methods, DNNs need a large amount of data for training the networks properly. Thus, because of the small number of samples in the majority of the facial expression databases, training neural networks is significantly more difficult in this task  [3] .\n\nIn machine learning, one of the main goals is to optimally estimate a function or distribution with respect to a defined measure. Based on the connectionist principle  [7] , DNNs allow us to build very complex classes of functions. A tremendous number of network topologies have been proposed in recent years and they seem to play a crucial role in improving the underlying class of functions available to DNNs. In order to make the training of DNNs smoother and faster, current methods focus on improving neuron saturation or the efficiency of the gradient flow across various network's layers. Such approaches are more noticeable in the ReLU class of non-linear functions, and the use of identity mappings in Deep Residual Networks  [8] . While having deeper architectures has shown to improve the result of classification  [9] , one possibility is to design more complex neurons to extract more useful information at each layer of the network which results in shallower networks and fewer parameters to train but a more accurate extracted information and therefore a higher recognition rate.\n\nIn this work, we address the adaptive complex neuron concept by introducing BReG-NeXt (following our previous work BReG-Net presented in  [10] ). In BReG-NeXt, instead of stacking up several identity-mapped residual units in hopes of increasing the number of parameters and eventually achieve a higher recognition rate (in architectures such as ResNet), we propose using a complex function to extract more information in each layer and therefore decrease the number of stacked layers and parameters (Figure  1 ). In Section 3, we will explain in detail that for our complex mapping formula we use:\n\nwhere α and β are trainable scalar parameters.\n\nThis mapping adds a small number of parameters to the original residual unit (two scalar parameters for each residual module) while at the same time extracts more useful features compared to the identity mapping proposed by He et al.  [9] . One important feature of this mapping is that its gradient is bounded and continues on x ∈ R (when α ∈ R -{0} and β ∈ R). Therefore, it preserves all the properties of identity mapping and at the same time it prevents the exploding or vanishing gradient problem during backpropagation. The entire network can still be trained end-to-end by ADAM optimizer with backpropagation and can be easily implemented using common DNN libraries.\n\nWe conduct comprehensive experiments on three in-thewild facial expression databases (AffectNet  [11] , Affect-inthe-wild  [12] , and FER2013  [13] ) to evaluate our method. Our experiments show that BReG-NeXt architecture significantly reduces the number of parameters compared to the identity mapping residual networks while produces better prediction rates on both categorical and dimensional models of affect. For the categorical model, we achieve 68.50% and 71.53% recognition rate on AffectNet and FER2013, respectively. For the dimensional model, we achieve 0.2577 and 0.2882 RMSE on AffectNet and Affect-in-Wild, respectively. These results outperform many state-of-the-art methods evaluated on these databases so far.\n\nThe remainder of the paper is organized as follows: Section 2 provides an overview of the related work in this field. Section 3 explains the BReG-NeXt architecture and the functions utilized to build the network. Section 4 presents experimental results and their analysis and finally, Section 5 concludes the paper with some discussions and recommendations for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we overview related work in Facial Expression Recognition on the categorical and dimensional models of affect. We also mention recent findings in regards to incorporating more complex nodes in DNNs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Facial Expression Recognition On Categorical Model Of Affect",
      "text": "Traditional approaches for automated affective computing use various engineered features such as Local Binary Patterns (LBP)  [2] , Histogram of Oriented Gradients (HOG)  [14] , Histogram of Optical Flow (HOF)  [15] , and facial landmark points  [16] . These engineered features often times lack the required generalizability power that makes the method robust to high variation in important factors such as lighting, views, resolution, subjects' ethnicity, etc. Deep learning has become a hot research topic and has achieved state-of-the-art performance for a variety of applications  [17]  as well as facial affect estimation. In this section, we briefly mention some of the deep learning-based methods used for FER.\n\nHuman emotion can be recognized using audio and visual information that express different non-verbal cues such as language, gesture, and facial expression. These modalities are either used individually or in combination. Although categorizing expressions based on visual data can achieve promising results, incorporating other models can provide extra information and further enhance the recognition rate. For instance, in the EmotiW and Audio Video Emotion Challenges (AVEC)  [18] ,  [19] , the audio model was considered to be the second most important element. Various fusion techniques for multi-modal affect recognition were proposed in these challenges. Li et al.  [20]  proposed a deep fusion CNN (DF-CNN) to explore multi-modal 2D+3D FER. Specifically, six types of 2D facial attribute maps (i.e., geometry, texture, curvature, normal components x, y, and z) were first extracted from the textured 3D face scans and then were jointly fed into the feature extraction and feature fusion subnets to learn the optimal combination weights of 2D and 3D facial representations. Also, Vielzeuf et al.  [21]  proposed a multi-modal approach for video emotion classification by combining VGG and C3D models as image descriptors.\n\nIn other DNN-based methods, Mollahosseini et al.  [3] ,  [22]  have used the Inception layer for the task of facial expression recognition and achieved significant results. Moreover, Inception layer is combined with a residual unit introduced by He et al.  [9] . They showed that the resulting architecture accelerates the training of Inception networks significantly  [23] . ResNet-based methods have been extensively investigated in the literature  [10] ,  [24] ,  [25] ,  [26]  and have shown significant results in FER. Hasani et al. proposed a modification of ResNets for the task of facial expression recognition  [25]  and valence/arousal prediction of emotions  [26] . Many of these methods use very deep architectures that required training millions of parameters as well as a considerable amount of memory and computation power to train them. Therefore, the main question here is whether having a more complex building block of neural networks results in a shallower and more efficient network or not? In this work, we address this question and investigate the impact of this concept.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dimensional Model Of Affect",
      "text": "Traditional methods for visual prediction of the dimensional model of affect have been a topic of study for years. Gunes et al.  [27]  focus on the dimensional prediction of emotions from spontaneous conversational head gestures by mapping the amount and direction of head motion and occurrences of head nods and shakes into arousal, expectation, intensity, power and valence level of the observed subject using Support Vector Regressions (SVRs). Kipp et al.  [28]  investigated (without performing automatic prediction) how basic gestural form features (e.g., preference for using the left/right hand, hand shape, palm orientation, etc.) are related to the single Pleasure-Arousal-Dominance (PAD)  [29]  dimensions of emotion. Nicolaou et al.  [30]  focus on the dimensional and continuous prediction of emotions from naturalistic facial expressions within an Output-Associative Relevance Vector Machine (RVM) regression framework by learning non-linear input and output dependencies inherent in the affective data. In  [31]  a novel technique to automatically segment emotional clips from long audiovisual interactions is proposed. Also in  [32]  extracting emotional segments from video based on the PAD model (assuming independency between the dimensions) is introduced.\n\nAs mentioned before, fewer studies have been conducted on the dimensional model of affect using DNNs as there are not many datasets with a large number of images available in this area. Nicolaou et al.  [33]  trained bidirectional Long Short Term Memory (LSTM) architecture on multiple engineered features extracted from audio, facial geometry, and shoulders. They achieved Root Mean Square Error (RMSE) of 0.15 and Correlation Coefficient (CC) of 0.79 for valence as well as RMSE of 0.21 and CC of 0.64 for arousal.\n\nHe et al.  [34]  won the AVEC 2015 challenge by training multiple stacks of bidirectional LSTMs (DBLSTM-RNN) on engineered features extracted from audio (LLDs features), video (LPQ-TOP features), 52 ECG features, and 22 EDA features. They achieved RMSE of 0.104 and CC of 0.616 for valence as well as RMSE of 0.121 and CC of 0.753 for arousal. Koelstra et al.  [35]  trained Gaussian naive Bayes classifiers on EEG, physiological signals, and multimedia features by binary classification of low/high categories for arousal, valence, and liking on their proposed database DEAP. They achieved F1-score of 0.39, 0.37, and 0.40 on arousal, valence, and liking categories respectively. Authors in  [36]  propose three CNN-based facial affect prediction method for mobile devices. In  [37]  a two-level attention with two-stage multitask learning framework is proposed for facial emotion estimation on static images using Bi-directional Recurrent Neural Networks (Bi-RNNs). In  [38]  a CNN-based method is proposed for predicting valence and arousal in images by focusing on the ocular region. Same as methods in the categorical model of affect, these methods are very deep networks with very high numbers of parameters to train.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "More Complex Nodes In Residual Networks",
      "text": "Having more complex units in residual networks has not been thoroughly investigated in the literature. This might be partially due to the fact that He et al. in  [8]  argues that by having the mapping H(x l ) = λ l x l instead of the shortcut bypass (see Figure  1 ), then for large values for λ l , the gradient in backpropagation will be exponentially large and for small values of λ l , it would be exponentially small and therefore the gradient vanishes. We will address this concern in the next section.\n\nOur initial experiments for having a complex mapping in residual units in  [10]  (BReG-Net) showed that by having H(x l ) = tan -1 (x l ), that has a bounded and continues gradient on x l ∈ R, not only do we prevent from facing vanishing/exploding gradient problem, but we have much less number of parameters to learn and also the network converges considerably faster than using the original identity mapping. Based on this work, we investigated more general forms of functions in the residual units as well as making the mapping adaptive to the input data by introducing trainable parameters for each residual unit as explained in Section 3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Breg-Next",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Residual Networks",
      "text": "Deep Residual Networks (ResNets)  [9]  are shaped by stacking several residual building block units. Each of these units can be mathematically shown as:\n\nwhere x l and x l+1 are input and output of the l-th unit, F is a residual function, and W l = {W l,k | 1≤k≤K } is a set of weights (and biases) associated with the l-th Residual Unit in which K is the number of layers in a Residual Unit. In  [9] , H(x l ) = x l is an identity mapping and f is a ReLU activation function. Very deep ResNets have shown state-of-the-art recognition rates for several challenging classification and detection tasks on ImageNet  [39]  and MS COCO  [40]  competitions. The main idea behind ResNets revolves around learning the additive residual function F with respect to H(x l ), where H(x l ) = x l . Thus, Residual Units are formulated as Equation  (1) . In this equation, if H is an identity mapping, and f is an identity function, then we will have:\n\nFor backpropagation, with E as loss function we will have:\n\nfor any deeper unit L and any shallower unit l.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Complex Mapping",
      "text": "If we replace the identity mapping with a more complex function of H l (x l ), assuming f remains an identity function, then Equation (1) will be:\n\nBy recursively applying this formulation and then calculating the backpropagation, similar to Equation (3) we will have:\n\nIn  [8]  a simple modification of H(x l ) = λ l x l is investigated (where λ l is a modulating scalar). By putting this function in Equation (  5 ), we will have:\n\nIn this case, for a very deep network (large L), if λ i > 1 for all i, this factor can be considerably large. Also, if λ i < 1 for all i, this factor can be exponentially small and therefore it vanishes, which blocks the backpropagated signal from the shortcut and forces it to flow through the weight layers. Therefore, choosing a suitable replacement for H(x) is very critical for the network's convergence. In an ideal case, all of the properties of the identity mapping function are needed to be preserved. One of the main properties of identity mapping is that it is continuous on R and it is also bounded (always equal to 1). From Equation (  6 ) we realize that the value of the derivative of the mapping function needs to be bounded and ideally less than or equal to 1. Also, this value should not be very small because similar to the case H(x l ) = λ l x l , in very deep networks the gradient would vanish along the bypass path.\n\nBased on the argument above, we investigated several functions (adaptive and non-adaptive) with bounded derivatives. Few of these functions are as follows:\n\nHowever, our experiments showed the best results with the following mapping:\n\nwhere α l and β l are trainable scalars for the l-th layer of the residual unit. This mapping is based on the result of our initial findings in  [10]  for BReG-Net. In the following we explain different aspects of this mapping. First, Equation (  8 ) is continuous and differentiable on x l ∈ R (for α l ∈ R -{0} and β l ∈ R), which means that it preserves those properties of identity mapping. Second, its partial derivative over x l is bounded:\n\n, which means that it preserves that property of identity mapping as well. Third, to prevent the exploding gradient problem (as shown in Equation (  6 )) we prefer a function that its derivative is not above 1. H satisfies this condition as well. Fourth, for reasonable values for α l and β l (which is the case for almost all of the training scenarios), ∂H ∂x l is far from becoming zero (especially when batch normalization is applied and the data is zero-centered). Therefore, it is very unlikely for the residual unit to face the vanishing gradient problem even for very deep networks. Figure  3  shows the plots for H and its derivative for different values of α and β. It can be seen that it is very unlikely for H to have nearzero value as input x l is mostly around zero after batch normalization. By putting all of the equations together our proposed complex mapping and the backpropagation will be as follows:\n\nEquation  (10)  shows that our proposed mapping flows the gradient smoothly in the backpropagation and addresses the concerns in  [8]  for complex mappings. By replacing the original identity mapping with the proposed H we will have more complex nodes in our residual neural network which results in having shallower networks and therefore fewer number of parameters to train as we show by our experiments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Adaptive Mapping",
      "text": "The reason behind defining α l and β l in Equation (  8 ) is to make each residual unit fit its own input and adjust the complex mapping accordingly. This is vital for facial affect estimation task where subtle changes in the input data are needed to be detected and recognized. Furthermore, in  [8]  it has been shown that having training parameters in the bypass (e.g., exclusive gating, shortcut-only gating, etc.) reduces the error rate comparing to only scaling the bypass without involving any training parameters. We discovered the same phenomena in our experiments where by assigning ∀i ∈ N : α i = 1, β i = 0 (i.e., H(x) = tan -1 (x)) error rates increased as shown by our experiments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Network Architecture",
      "text": "As mentioned before, we have tested various mappings and observed consistent results for our proposed complex mapping. In order to compare the effectiveness of our method, we investigate six networks (three shallow and three deep architectures): 1) ResNet-32 2) ResNet-50 3) BReG-Net-32 (BReG-Net is a special case of BReG-NeXt with α = 1 and β = 0 in Equation  8 ) 4) BReG-Net-50 5) BReG-NeXt-32 which is comparable with ResNet-32 and BReG-Net-32 in terms of number of layers 6) BReG-NeXt-50 which is our final proposed architecture that achieved best results on both categorical and dimensional models of affect and is comparable with ResNet-50 and BReG-Net-50.\n\nResNet-32: Our first baseline is ResNet-32 proposed by He et al. in  [9]  where identity mapping is used for the bypass over the 3 × 3 convolutions. The detailed structure of this baseline is provided in Table  1 . Our implementation of this network is slightly different from the one mentioned in  [9]  as we intended to make this network similar to BReG-NeXt in terms of the arrangement of residual units to have a fair comparison between the two architectures.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Resnet-50:",
      "text": "Our second baseline is ResNet-50 which is a deeper version of ResNet-32. Similar to the previous network, for a fair comparison, our implementation of this network is slightly different from  [9] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Breg-Net-32:",
      "text": "The next baseline is BReG-Net-30  [10] . BReG-Net was our first attempt in developing residual units with complex mapping. BReG-Net simply uses H(x) = tan -1 (x) for its mapping function with no additional training parameters. In other words, BReG-Net uses the complex mapping of Equation 8 while α = 1 and β = 0 are always fixed for all blocks. The number of training parameters for this architecture is 1.9M which is significant reduction compared to the previously mentioned ResNets (Table  1 ). Therefore, it is a suitable point of reference for our proposed complex mapping for BReG-NeXt. Similar to the previous networks, for a fair comparison, we reduced the layers of BReG-Net originally proposed in  [10]  as the original architecture contains more number of layers compared to its BReG-NeXt counterpart.\n\nBReG-Net-50: Our next baseline is a deeper version of BReG-Net to compare the deep versions of the architectures. Similar to the previous networks, for a fair comparison, we matched the number of layers of BReG-Net to its BReG-NeXt counterpart.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Breg-Next-32:",
      "text": "Our shallow version of BReG-NeXt has 32 layers and is comparable to ResNet-32 and BReG-Net-32 in terms of depth. In terms of the number of training parameters, however, BReG-NeXt-32 is significantly lighter than ResNet-32 with only 1.9M parameters (Table  1 ). In this architecture, down-sampling is applied after the complex mapping H at the same time that the number of feature map channels increases (conv3 and conv5 in Table  1 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Breg-Next-50:",
      "text": "Our deeper version of BReG-NeXt has 50 layers (Figure  2 ). In the experiments section, we show that this network achieves the best results in both categorical and dimensional models of affect by having only 3.1M trainable parameters.\n\nTable  1  provides a general overview of the six networks that we study in this paper. ResNet-32, BReG-Net-32, and BReG-NeXt-32 are similar networks in terms number of convolution layers and they are a shallower version of their architectures while ResNet-50, BReG-Net-50, and BReG-NeXt-50 are a deeper version of them. ResNets showed significant results in  [9]  therefore they are suitable benchmarks for our method. Considering the parameter/error-rate trade-off, we propose BReG-NeXt-50 as our final network since deeper networks did not show significant improvement as it is shown in the experiments section.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation",
      "text": "We implemented our method using a combination of Ten-sorFlow  [41] , TfLearn  [42] , and Keras  [43]  libraries. For all experiments on all databases, we crop the faces and resize them to 64×64×3 pixels. For augmentation, random horizontal flip is used followed by random changes in hue, saturation, brightness, contrast, and zooming. Augmentation is applied in 25% of the time. Zero-centering for each color channel is also utilized as the gradient flows more smoothly around zero. Similar to ResNet, we use batch normalization  [44]  after each convolution and before each activation. We use \"ELU\" activation function  [45]  as it contributes to flowing the gradient more smoothly for negative values in the backpropagation. All networks are trained from scratch. Focal loss is used for the categorical model experiments and Mean Squared Error loss is used for the dimensional model experiments. ADAM optimizer with the batch size of 128 is used for all experiments. The learning rate starts from 0.0001 and is multiplied by 0.8 after every 10 epochs. Similar to ResNet we do not use dropout  [9] ,  [44] , however, we use L2 regularizers on the convolution layers. Our code and trained network parameters will be made publicly available.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we briefly review the three face databases used for evaluating our proposed method. We then provide details of our experiments and their results using these databases evaluated on different metrics on both categorical and dimensional models of affect.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Face Databases",
      "text": "As noted earlier, many of the traditional facial expression databases are assembled in a controlled environment while for developing a practical method, these databases do not yield satisfying results. Therefore, we chose databases that are captured in the wild setting which contain a variety of backgrounds, lighting, pose, subject ethnicity, etc. These databases are AffectNet  [11] , Affect-in-Wild  [12] , and FER2013  [13]  of which AffectNet contains labels of both categorical and dimensional models. Affect-in-Wild contains only labels of the dimensional model, and FER2013 contains only labels of the categorical model. In the following, we briefly review the contents of these databases.\n\nAffectNet contains more than one million facial images collected from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages  [11] . About half of the retrieved images (around 440,000 images) are manually annotated for the presence of seven discrete facial expressions (categorical model) and the intensity of valence and arousal (dimensional model). Af-fectNet is the largest database of facial expressions, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. This database is very challenging as it contains images of people from different races and ethnicities as well as high variety in the background, lighting, pose, point of view,  Affect-in-Wild was introduced in CVPR 2017 workshop challenge  [12] . This database contains 300 videos of different subjects watching videos of various TV shows and movies.\n\nThe videos contain subjects from different genders and ethnicities with high variations in head pose and lightning. Videos in this database are annotated with valence and arousal values for each frame. A total of 254 videos of this database are selected for training and the remaining 46 videos were used for evaluating the participants in the challenge. Since the evaluation set is not publicly available, we selected 26 sequences of the training set as our validation set (in a subject-independent manner). Figure  4 (c) provides a few examples from Affect-in-Wild database.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics For Dimensional Model",
      "text": "In order to evaluate the methods in the dimensional model of affect, we calculate and report Root Mean Square Error (RMSE), Correlation Coefficient (CC), Concordance Correlation Coefficient (CCC), and sign agreement (SAGR) metrics for the methods. In the following, we briefly explain the definitions of these metrics.\n\nRoot Mean Square Error (RMSE) is the most common evaluation metric in a continuous domain which is defined as:\n\nwhere θi and θ i are the prediction and the ground-truth of i th sample, and n is the number of samples. RMSE-based evaluation metrics can heavily weigh the outliers  [46] , and they do not consider covariance of the data.\n\nPearson's Correlation Coefficient (CC) overcomes RMSE's reliance on outliers  [33]  and it is defined as:\n\nwhere COV is covariance function.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Concordance Correlation Coefficient (Ccc)",
      "text": "is another metric  [18]  and combines CC with the square difference between the means of two compared time series:\n\nwhere ρ is the Pearson correlation coefficient (CC) between two time-series (e.g., prediction and ground-truth), σ 2 θ and σ 2 θ are the variance of each time series, σ θ and σ θ are the standard deviation of each, and µ θ and µ θ are the mean value of each. Unlike CC, the predictions that are well correlated with the ground-truth but shifted in values are penalized in proportion to the deviation in the CCC.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sagr:",
      "text": "The value of valence and arousal fall within the interval of [-1,+1] and correctly predicting their signs are essential in many emotion-prediction applications. Therefore, we use Sign AGReement (SAGR) metric as proposed in  [33]  to evaluate the performance of a valence and arousal prediction system with respect to the sign agreement. SAGR is defined as:\n\nwhere δ is the Kronecker delta function, defined as:",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "As mentioned before, we have selected BReG-NeXt-50 as our final architecture. Similar to ResNet, by increasing the number of layers, the recognition rate increases in BReG-NeXt. However, considering the trade-off between FLOPs and recognition rate, after a certain point, recognition rate   ). This phenomenon is more visible for the valence predictions than it is for arousal's but it occurs for both dimensions after a certain point. We need to mention that for each depth increment we add one unit to the residual unit (F 1 in Figure  2 ). Therefore, three units are added in each increment and since there are two convolution layers in each residual unit, thus the number of layers added in each increment is six. Based on this argument we decided to choose BReG-NeXt-50 as our final proposed networks for our method.\n\nCategorical Model: Facial expression databases are usually highly skewed. This form of imbalanced data is referred to as \"intrinsic variation\", i.e., it is a direct result of the nature of expressions in the real world. Therefore, this phenomena occurs in both categorical and dimensional models of affect. For example, Caridakis et al.  [52]  reported that a bias toward the first quadrant of valence/arousal circumplex (positive arousal, positive valence) exists in the SAL database. Table  3  shows an imbalanced number of images for each emotion category in AffectNet and FER2013 databases used in this work. We face two problems while working with imbalanced data. First, training with an imbalanced distribution often causes the learning algorithms to perform poorly on the less-represented classes  [53] . Second, the imbalance in the test/validation data distribution can affect the performance metrics of the methods significantly. Jeni et al.  [54]  showed that with exception of the area under the ROC curve (AUC), all other studied evaluation metrics, i.e., Accuracy, F1-score, Cohen's kappa  [55] , Krippendorf's Alpha  [56] , and area under Precision-Recall curve (AUC-PR) are affected by skewed distributions dramatically. While AUC is unaffected by skew, precision-recall curves suggested that AUC may mask poor performance. There have been some attempts to overcome this problem. In  [10] ,  [11]  weighted loss functions are used in which the loss function heavily penalizes the networks for misclassifying examples from under-represented classes while penalizing networks less for misclassifying examples from wellrepresented classes.\n\nRecently, focal loss  [57]  has drawn attention for imbalanced data training. Focal loss is the reshaping of cross entropy loss such that it down-weights the loss assigned to well-classified examples. Focal loss focuses on training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the network during training. Formally, for binary classification, cross entropy loss is defined as CE(p t ) = -log(p t ) where p t is defined as:\n\nin which y ∈ {±1} and p ∈ [0, 1] is the model's estimated probability for the class y = 1. In focal loss, modulating factor (1 -p t ) γ , and balancing factor α t is multiplied to the cross entropy loss as follows:\n\nwhere γ ≥ 0 is called focusing parameter. In our experiments on the categorical model of affect, we use focal loss (with α t = 0.25 and γ = 2) as our loss function for the optimizer. Table  4  shows the result of our experiments in the categorical model of affect on AffectNet and FER2013 databases.  All of the reported numbers are the results of our experiments on the validation set of these databases. As can be seen, our proposed modification of the ResNet module achieves better recognition rates compared to their counterpart on original ResNet. Our method also outperforms the existing methods on both AffectNet and FER2013 databases.\n\nFor state-of-the-art methods mentioned in Table  4 , Mollahosseini et al.  [11]  uses AlexNet, Wiles et al.  [58]  achieved 74.4 for AUC, and  [22]  uses an Inception-based method to classify the expressions,  [51]  trained deep learning methods combined with SVMs, in  [37]  a two-level attention with twostage multi-task learning framework is proposed for facial emotion estimation, and in  [21]  a multi-modal approach for video emotion classification is used by combining VGG and C3D as image descriptors. It is worth to mention that our proposed method is considerably shallower than many of the methods proposed in the field. Table  5  provides additional evaluation metrics on BReG-NeXt-50 for the categorical model of affect. It can be seen that in both databases \"Happy\" is recognized more accurately compared to other emotions. This is because \"Happy\" has a considerable number of samples in both training sets and also it is considerably distinguishable from other emotions in its shape nature. On AffectNet, the rest of the emotions have close recognition rate (in terms of F1score) to the average which shows that BReG-NeXt-50 is not excessively biased towards any emotion. On FER2013, however, \"disgust\" has a lower recognition rate comparing to the other emotions. This is due to the fact that this category is barely represented in the dataset (only 1.5% of the entire data based on Table  3 ) to the extent that focal loss is not able to assign enough priority for the samples of this category.\n\nFigure  6  shows the confusion matrix of BReG-NeXt-50 on the categorical model. On AffectNet, the most confusion occurs between \"Happy\" and \"Contempt\" which is not unexpected as these two categories are very similar to the extent that distinguishing between the two is difficult even for humans. On FER2013, as mentioned earlier, the low number of samples for \"Disgust\" has caused the main confusion for the network. Other categories, however, have been distinguished well considering the fact that the database is very challenging.\n\nFigure  7  depicts a few examples of predictions made by BReG-NeXt-50 with their corresponding confidence score. It can be seen that our method performs well in predicting most of the instances and for misclassified examples, networks predictions are so a certain degree relevant to the input pictures. Also, our method performs well specifically on the difficult categories of neutral and contempt; it is able to recognize the subtle facial properties for these challenging categories. Figure  8  shows an example input image and its corresponding feature map at different depths of BReG-NeXt-50 and ResNet-50. It can be seen that at each layer, BReG-NeXt-50 performs considerably better in distinguishing important components of the face such as eyes and mouth which results in better recognition at the end.\n\nDimensional Model: Table  6  shows the results of our experiments in the dimensional model of affect on the validation set of the AffectNet and Affect-in-Wild databases. Same as the categorical model of affect, our method achieves better results (lower loss and RMSE) in total (both valence and arousal considered together) compared to their corresponding network on ResNet and BReG-Net. For state-of-the-art methods mentioned in Table  6 , Mollahosseini et al.  [11]  use  AlexNet, and  [26]  uses an Inception-ResNet-based method to classify the expressions. In  [37]  a two-level attention with two-stage multi-task learning framework is proposed for facial emotion estimation on static images using Bidirectional Recurrent Neural Networks (Bi-RNNs). In  [38]  a CNN-based method is proposed for predicting valence and arousal in images by focusing on the ocular region. The reported results in On AffectNet, the improvement over ResNet and other state-of-the-art methods is significant. Both BReG-NeXt-32 and BReG-NeXt-50 outperform ResNets for arousal and overall predictions. This shows that our complex mapping has been able to fit the training data better and recognize the subtle differences in the dimensional model of affect. For valence, our networks achieve better performance compared to their ResNet counterpart but they do not beat the stateof-the-art. This can be seen in Figure  9  as well. Where error rates for arousal are concentrated around zero while their corresponding prediction errors for valence are not as dense   On Affect-in-Wild, our method outperforms ResNet and state-of-the-art methods on all valence, arousal, and overall predictions. This improvement, however, is not as significant as AffectNet. This can be partially due to the fact that the labels for valence and arousal in Affect-in-Wild dataset are very inconsistent in consecutive frames. There are many cases in the dataset that the emotion of the face does not change at all or it changes very subtly, but the labels for the sequence change drastically. Therefore, the network is confused by these type of instances in the training set while on AffectNet -where there is less inconsistency among the labels-our method performs considerably better. Table  7  provides additional metrics for the validation set of the studied databases on BReG-NeXt-50. We defined these metrics in Section 4.2. On AffectNet, our method achieves high correlation scores for both CC (Equation (  12 )) and CCC (Equation (  13 )) as well as significant sign agreement for both valence and arousal. On Affect-in-Wild, which is a more challenging database in general, the correlation of the predictions are not high in terms of numbers but are better or comparable with the correlations reported in  [10]  for BReG-Net. For SAGR, however, our method achieves a satisfying prediction which shows that BReG-NeXt is able to correctly predict whether an emotion is either positive or negative as well as whether it is an active emotion or a passive one.\n\nFigure  9  shows the histogram of the prediction errors in the studied databases. In all cases, most error values fall in the vicinity of the zero. On Affect-in-Wild, some high error rates can be seen (especially on arousal predictions) that can be due to the inconsistency of the labels in the training labels as mentioned before. However, in general distribution  of the errors have the expected shape and they are mainly gathered around zero on both databases. Figure  10  shows some examples of BReG-NeXt-50 predictions for the dimensional model of affect. It can be seen that BReG-NeXt is able to recognize the subtle facial muscle shapes for positivity or negativity of expression as predicted values for valence shows in most cases have the same sign (with close value) compared to the ground-truth. Also, BReG-NeXt-50 shows satisfying performance for predicting whether an emotion is active or passive by performing satisfying prediction for arousal. Some of the good examples are surprised or saddened instances in Figure  10 .\n\nAs mentioned earlier, we investigated several mapping functions and among those Equation  8 showed the best results as this mapping extracts more useful features in each block and is much more flexible in learning the patterns due to the adaptive parameters in each block of the proposed architecture. Table  8  compares the investigated mappings presented in Equations 7 and 8. It can be seen that our proposed adaptive complex mapping (Equation  8 ) achieves better results in all of the conducted experiments.\n\nIn order to show the impact of adaptive complex mapping on facial affect estimation, we evaluated BReG-NeXt with fixed values for α l and β l in Equation  (8) . Table  9  shows the result of BReG-NeXt-32 and BReG-NeXt-50 when ∀i ∈ N : α i = 1, β i = 0. Therefore, Equation (8) will be simplified to H(x l ) = tan -1 (x l ) in these cases. By comparing the experimental results provided in Tables  9, 4 , and 6 it can be seen that for all cases adaptive BReG-NeXt outperform their corresponding non-adaptive ones. This improvement is more significant in the dimensional model of affect where subtle changes in the shape of facial muscles result in different values for valence and arousal. This shows that adaptive complex mapping fits each residual unit to its input feature map more than non-adaptive one resulting to extract the subtle meaningful changes in each layer. Our final trained model shows smaller values for α and β (∼0.5) in the first few adaptive complex mapping units and larger values (∼1.2) for them in the deeper units in the network. As mentioned before, our code and trained parameters will be publicly available for the research community.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduced BReG-NeXt, a new residualbased network architecture consisting of a differentiable and bounded gradient function instead of a shortcut path between the input and the output of the residual unit (identity mapping) for the task of affect estimation in both categorical and dimensional models of affect. By utilizing this complex function (called complex mapping), the networks will have more complex nodes and therefore, more useful features are extracted at each layer. Thus, the resulting network is shallower with less number of parameters to learn and fewer operations to perform.\n\nWe showed that our complex mapping needs to have bounded derivative and away from zero for the gradient to flow smoothly in the back-propagation phase which results in preventing from facing vanishing/exploding gradient. It has been shown that incorporating training parameters in the bypass route of residual units results in a better fit especially in challenging tasks such as facial affect estimation where very subtle changes in the training data are needed to be recognized by the network.\n\nWe replaced the identity mapping in original residual units with our adaptive complex mapping in Equation (  8 ). Among many other functions that we investigated, this mapping satisfies the required properties for the bypass and it also showed the best results in both affect estimation tasks in our experiments by having a significantly lower number of parameters and FLOPs compared to deep ResNets (Table  1 ). Furthermore, adding training parameters to the bypass helped to further improve the fitting and be able to distinguish the subtle changes especially in the dimensional model of affect.\n\nTo evaluate our proposed method, we conducted comprehensive experiments for facial affect estimation on categorical and dimensional models of affect. Challenging inthe-wild databases (AffectNet, FER2013, and Affect-in-Wild) were used for the experiments. We showed that our adaptive complex mapping outperforms original residual units with identity mapping and other state-of-the-art methods in the field in the majority of the cases (Tables  4  and 6 ). Considering the trade-off between the number of parameters and recognition rate, we proposed BReG-NeXt-50 as our final architecture for this task. Furthermore, we provided additional metrics in both affect models to have a better evaluation of our method. In the categorical model, BReG-NeXt-50 with only 3.1M training parameters, achieves 68.50% and 71.53% accuracy on AffectNet and FER2013 databases, respectively. And in the dimensional model, it achieves 0.2577 and 0.2882 for RMSE on AffectNet and Affect-in-Wild databases, respectively.\n\nA recommendation for future work is to apply this method to other residual-based networks in DNNs. For instance, BReG-NeXt architecture can be applied to the well-known DenseNet  [60]  architecture to enrich the feature map flowing through the network. However, memory and computational power limitations need to be considered in such networks with nested connections. Also, for a more comprehensive investigation, BReG-NeXt architecture can be expanded furthermore with 3D-CNNs with approaches similar to  [21] .",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of a) Original Residual Unit b) BReG-NeXt Resid-",
      "page": 1
    },
    {
      "caption": "Figure 2: Network conﬁguration of BReG-NeXt-50. Down-sampling on the bypass route is implemented by average pooling. H is implemented with",
      "page": 3
    },
    {
      "caption": "Figure 1: ), then for large values for λl,",
      "page": 3
    },
    {
      "caption": "Figure 3: Plots of proposed complex mapping function H (Equation (8)) and its derivative (H′) for different values of α and β (best viewed in color)",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the",
      "page": 5
    },
    {
      "caption": "Figure 2: ). In the experiments section, we show that",
      "page": 6
    },
    {
      "caption": "Figure 4: Some example images from AffectNet (a), FER2013 (b), and Affect-in-Wild (c) databases used in this study. AffectNet contains labels for",
      "page": 7
    },
    {
      "caption": "Figure 4: (a) shows some example images from AffectNet",
      "page": 7
    },
    {
      "caption": "Figure 4: (c) provides",
      "page": 7
    },
    {
      "caption": "Figure 5: Result of experimenting different depths for BReG-NeXt on",
      "page": 8
    },
    {
      "caption": "Figure 5: shows recognition rate",
      "page": 8
    },
    {
      "caption": "Figure 5: (a)). Similar behavior in dimensional model hap-",
      "page": 8
    },
    {
      "caption": "Figure 5: (b)). This phenomenon",
      "page": 8
    },
    {
      "caption": "Figure 2: ). Therefore,",
      "page": 8
    },
    {
      "caption": "Figure 6: Confusion matrix of BReG-NeXt-50 on AffectNet (a) and",
      "page": 9
    },
    {
      "caption": "Figure 6: shows the confusion matrix of BReG-NeXt-50",
      "page": 9
    },
    {
      "caption": "Figure 7: depicts a few examples of predictions made by",
      "page": 9
    },
    {
      "caption": "Figure 8: shows an example input image and its corre-",
      "page": 9
    },
    {
      "caption": "Figure 7: Example predictions of BReG-NeXt-50 on AffectNet (a) and FER2013 (b). The text below images indicates predicted label, conﬁdence, and",
      "page": 10
    },
    {
      "caption": "Figure 9: as well. Where error",
      "page": 10
    },
    {
      "caption": "Figure 8: An example input image (a) and its corresponding feature map at different depths of BReG-NeXt-50 and ResNet-50. Figures (b), (d), (f),",
      "page": 11
    },
    {
      "caption": "Figure 9: Error histogram of BReG-NeXt-50 on AffectNet (a and b), and",
      "page": 11
    },
    {
      "caption": "Figure 9: shows the histogram of the prediction errors in",
      "page": 11
    },
    {
      "caption": "Figure 10: Example predictions of BReG-NeXt-50 on AffectNet (a) and Affect-in-Wild (b). The text below images indicates predicted values for valence",
      "page": 12
    },
    {
      "caption": "Figure 10: shows some examples of BReG-NeXt-50 pre-",
      "page": 12
    },
    {
      "caption": "Figure 10: As mentioned earlier, we investigated several mapping",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "conv1": "conv2",
          "3×3, 64, stride 2": "(cid:20)\n(cid:21)\n3 × 3, 64\n× 3\n3 × 3, 64",
          "3×3, 32": "(cid:20)"
        },
        {
          "conv1": "conv3",
          "3×3, 64, stride 2": "(cid:20)",
          "3×3, 32": "(cid:20)"
        },
        {
          "conv1": "conv4",
          "3×3, 64, stride 2": "(cid:20)",
          "3×3, 32": "(cid:20)"
        },
        {
          "conv1": "conv5",
          "3×3, 64, stride 2": "(cid:20)",
          "3×3, 32": "(cid:20)\n(cid:21)\n3 × 3, 128\n× 1\n3 × 3, 128"
        },
        {
          "conv1": "conv6",
          "3×3, 64, stride 2": "-",
          "3×3, 32": "(cid:20)\n(cid:21)\n3 × 3, 128\n× 4\n3 × 3, 128"
        },
        {
          "conv1": "global\navg pooling\n+\nfully connected",
          "3×3, 64, stride 2": "8, softmax (categorical) / 2, linear (dimensional)",
          "3×3, 32": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: shows the number of training ing networks less for misclassifying examples from well-",
      "data": [
        {
          "Neutral": "Happy",
          "80,276": "146,198",
          "6,198": "8,989"
        },
        {
          "Neutral": "Sad",
          "80,276": "29,487",
          "6,198": "6,077"
        },
        {
          "Neutral": "Surprise",
          "80,276": "16,288",
          "6,198": "4,002"
        },
        {
          "Neutral": "Fear",
          "80,276": "8,191",
          "6,198": "5,121"
        },
        {
          "Neutral": "Disgust",
          "80,276": "5,264",
          "6,198": "547"
        },
        {
          "Neutral": "Anger",
          "80,276": "28,130",
          "6,198": "4,953"
        },
        {
          "Neutral": "Contempt",
          "80,276": "5,135",
          "6,198": "-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: , Molla- most of the instances and for misclassified examples, net-",
      "data": [
        {
          "AffectNet": "FER2013",
          "59.45": "65.81",
          "63.33": "67.15",
          "65.66": "67.86",
          "66.96": "69.21",
          "66.74": "69.11",
          "68.50": "71.53",
          "58.0 [11], 57.31 [47]\n48 [37], 62.11 [48]\n58 [36], 60 [49]\n61.5 [50]": "69.3 [51], 66.4 [22]\n71.2 [21]"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "Affective computing: A review"
    },
    {
      "citation_id": "2",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition from world wild web",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Salvador",
        "H Abdollahi",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "4",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "5",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "6",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen",
        "J Hager"
      ],
      "venue": "Facial action coding system"
    },
    {
      "citation_id": "7",
      "title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition",
      "authors": [
        "D Rumelhart",
        "J Mcclelland"
      ],
      "year": "1986",
      "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition"
    },
    {
      "citation_id": "8",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2005",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2006",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Bounded residual gradient networks (breg-net) for facial affect computing",
      "authors": [
        "B Hasani",
        "P Negi",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "11",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Aff-wild: Valence and arousal in-the-wild challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "CVPR Workshop"
    },
    {
      "citation_id": "13",
      "title": "Challenges in representation learning: Facial expression recognition challenge",
      "year": "2013",
      "venue": "Challenges in representation learning: Facial expression recognition challenge"
    },
    {
      "citation_id": "14",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "Human detection using oriented histograms of flow and appearance",
      "authors": [
        "N Dalal",
        "B Triggs",
        "C Schmid"
      ],
      "year": "2006",
      "venue": "ECCV"
    },
    {
      "citation_id": "16",
      "title": "Active shape models-their training and application",
      "authors": [
        "T Cootes",
        "C Taylor",
        "D Cooper",
        "J Graham"
      ],
      "year": "1995",
      "venue": "Computer vision and image understanding"
    },
    {
      "citation_id": "17",
      "title": "Deep learning: methods and applications",
      "authors": [
        "L Deng",
        "D Yu"
      ],
      "year": "2014",
      "venue": "Foundations and Trends R in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge"
    },
    {
      "citation_id": "19",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "20",
      "title": "Multimodal 2d+ 3d facial expression recognition with deep fusion convolutional neural network",
      "authors": [
        "H Li",
        "J Sun",
        "Z Xu",
        "L Chen"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Temporal multimodal fusion for video emotion classification in the wild",
      "authors": [
        "V Vielzeuf",
        "S Pateux",
        "F Jurie"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "22",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "23",
      "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "venue": "Inceptionv4, inception-resnet and the impact of residual connections on learning"
    },
    {
      "citation_id": "24",
      "title": "Learning supervised scoring ensemble for emotion recognition in the wild",
      "authors": [
        "P Hu",
        "D Cai",
        "S Wang",
        "A Yao",
        "Y Chen"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "25",
      "title": "Facial expression recognition using enhanced deep 3d convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "26",
      "title": "Facial affect estimation in the wild using deep residual and convolutional networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "27",
      "title": "Dimensional emotion prediction from spontaneous head gestures for interaction with sensitive artificial listeners",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "International conference on intelligent virtual agents"
    },
    {
      "citation_id": "28",
      "title": "Gesture and emotion: Can basic gestural form features discriminate emotions?",
      "authors": [
        "M Kipp",
        "J.-C Martin"
      ],
      "year": "2009",
      "venue": "2009 3rd international conference on affective computing and intelligent interaction and workshops"
    },
    {
      "citation_id": "29",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "30",
      "title": "Output-associative rvm regression for dimensional and continuous emotion prediction",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "31",
      "title": "Automatic segmentation of spontaneous data using dimensional labels from multiple coders",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proc. of LREC Int. Workshop on Multimodal Corpora: Advances in Capturing, Coding and Analyzing Multimodality"
    },
    {
      "citation_id": "32",
      "title": "Affective level video segmentation by utilizing the pleasure-arousal-dominance information",
      "authors": [
        "S Arifin",
        "P Cheung"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Multimodal affective dimension prediction using deep bidirectional long shortterm memory recurrent neural networks",
      "authors": [
        "L He",
        "D Jiang",
        "L Yang",
        "E Pei",
        "P Wu",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "35",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Cnn-based facial affect analysis on mobile devices",
      "authors": [
        "C Hewitt",
        "H Gunes"
      ],
      "year": "2018",
      "venue": "Cnn-based facial affect analysis on mobile devices",
      "arxiv": "arXiv:1807.08775"
    },
    {
      "citation_id": "37",
      "title": "Two-level attention with two-stage multi-task learning for facial emotion recognition",
      "authors": [
        "W Xiaohua",
        "P Muzi",
        "P Lijuan",
        "H Min",
        "J Chunhua",
        "R Fuji"
      ],
      "year": "2019",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "38",
      "title": "Oculum afficit: Ocular affect recognition",
      "authors": [
        "E Langholz"
      ],
      "year": "2019",
      "venue": "Oculum afficit: Ocular affect recognition",
      "arxiv": "arXiv:1905.09240"
    },
    {
      "citation_id": "39",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "40",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "T.-Y Lin",
        "M Maire",
        "S Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C Zitnick"
      ],
      "year": "2014",
      "venue": "Microsoft coco: Common objects in context"
    },
    {
      "citation_id": "41",
      "title": "Tensorflow: Large-scale machine learning on heterogeneous",
      "authors": [
        "A Abadi"
      ],
      "year": "2015",
      "venue": "Software available from tensorflow"
    },
    {
      "citation_id": "42",
      "title": "Tflearn",
      "authors": [
        "A Damien"
      ],
      "year": "2016",
      "venue": "Tflearn"
    },
    {
      "citation_id": "43",
      "title": "Keras",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "44",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "citation_id": "45",
      "title": "Fast and accurate deep network learning by exponential linear units (elus)",
      "authors": [
        "D.-A Clevert",
        "T Unterthiner",
        "S Hochreiter"
      ],
      "year": "2015",
      "venue": "Fast and accurate deep network learning by exponential linear units (elus)",
      "arxiv": "arXiv:1511.07289"
    },
    {
      "citation_id": "46",
      "title": "Oriented principal component analysis for large margin classifiers",
      "authors": [
        "S Bermejo",
        "J Cabestany"
      ],
      "year": "2001",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "47",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "48",
      "title": "Hero: Human emotions recognition for realizing intelligent internet of things",
      "authors": [
        "W Hua",
        "F Dai",
        "L Huang",
        "J Xiong",
        "G Gui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "49",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "50",
      "title": "Facial motion prior networks for facial expression recognition",
      "authors": [
        "Y Chen",
        "J Wang",
        "S Chen",
        "Z Shi",
        "J Cai"
      ],
      "year": "2019",
      "venue": "Facial motion prior networks for facial expression recognition",
      "arxiv": "arXiv:1902.08788"
    },
    {
      "citation_id": "51",
      "title": "Deep learning using linear support vector machines",
      "authors": [
        "Y Tang"
      ],
      "year": "2013",
      "venue": "Deep learning using linear support vector machines",
      "arxiv": "arXiv:1306.0239"
    },
    {
      "citation_id": "52",
      "title": "User and context adaptive neural networks for emotion recognition",
      "authors": [
        "G Caridakis",
        "K Karpouzis",
        "S Kollias"
      ],
      "year": "2008",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "53",
      "title": "Learning from imbalanced data",
      "authors": [
        "H He",
        "E Garcia"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Knowledge & Data Engineering"
    },
    {
      "citation_id": "54",
      "title": "Facing imbalanced datarecommendations for the use of performance metrics",
      "authors": [
        "L Jeni",
        "J Cohn",
        "F De",
        "La Torre"
      ],
      "year": "2013",
      "venue": "Facing imbalanced datarecommendations for the use of performance metrics"
    },
    {
      "citation_id": "55",
      "title": "A coefficient of agreement for nominal scales",
      "authors": [
        "J Cohen"
      ],
      "year": "1960",
      "venue": "Educational and Psychological Measurement"
    },
    {
      "citation_id": "56",
      "title": "Estimating the reliability, systematic error and random error of interval data",
      "authors": [
        "K Krippendorff"
      ],
      "year": "1970",
      "venue": "Estimating the reliability, systematic error and random error of interval data"
    },
    {
      "citation_id": "57",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "58",
      "title": "Self-supervised learning of a facial attribute embedding from video",
      "authors": [
        "O Wiles",
        "A Koepke",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Self-supervised learning of a facial attribute embedding from video",
      "arxiv": "arXiv:1808.06882"
    },
    {
      "citation_id": "59",
      "title": "Registration-free face-ssd: Single shot analysis of smiles, facial attributes, and affect in the wild",
      "authors": [
        "Y Jang",
        "H Gunes",
        "I Patras"
      ],
      "year": "2019",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "60",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "K Weinberger",
        "L Van Der Maaten"
      ],
      "year": "2017",
      "venue": "CVPR"
    }
  ]
}