{
  "paper_id": "2010.15075v2",
  "title": "Generative Adversarial Networks In Human Emotion Synthesis:A Review",
  "published": "2020-10-28T16:45:36Z",
  "authors": [
    "Noushin Hajarolasvadi",
    "Miguel Arjona Ramírez",
    "Hasan Demirel"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Synthesizing realistic data samples is of great value for both academic and industrial communities. Deep generative models have become an emerging topic in various research areas like computer vision and signal processing. Affective computing, a topic of a broad interest in computer vision society, has been no exception and has benefited from generative models. In fact, affective computing observed a rapid derivation of generative models during the last two decades. Applications of such models include but are not limited to emotion recognition and classification, unimodal emotion synthesis, and cross-modal emotion synthesis. As a result, we conducted a review of recent advances in human emotion synthesis by studying available databases, advantages, and disadvantages of the generative models along with the related training strategies considering two principal human communication modalities, namely audio and video. In this context, facial expression synthesis, speech emotion synthesis, and the audio-visual (cross-modal) emotion synthesis is reviewed extensively under different application scenarios. Gradually, we discuss open research problems to push the boundaries of this research area for future works.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning techniques are known best for their promising success in uncovering the underlying probability distributions over various data types in the field of artificial intelligence. Some of these data types are videos, images, audio samples, biological signals, and natural language corpora. The success of the deep discriminative models owes primarily to the back-propagation algorithm and piece-wise linear units  (LeCun et al., 1998; Krizhevsky et al., 2012) . In contrast, deep generative models  (Goodfellow et al., 2014)  have been less successful in addressing deep learning due to difficulties that arise by intractable approximation in the probabilistic computation of methods like maximum likelihood estimation.\n\nMany reviews studied the rapidly expanding topic of generative models and specifically Generative Adversarial Networks (GAN) by investigating various points of view. From algorithms, theory, and applications  (Gui et al., 2020; Wang et al., 2017)  and recent advances and developments  (Pan et al., 2019; Zamorski et al., 2019)  to comparative studies  (Hitawala, 2018) , GAN taxonomies  (Wang et al., 2019b) , and its variants  (Hong et al., 2019; Creswell et al., 2018; Huang et al., 2018; Kurach et al., 2018)  are investigated by the researchers. Also, few review papers discussed the subject based on a specific application like medical imaging  (Yi et al., 2019b) , audio enhancement and synthesis (Torres-Reyes and Latifi, 2019), image synthesis  (Wu et al., 2017) , and text synthesis  (Agnese et al., 2019) . Howsoever, none of the existing surveys considered GAN in view of human emotion synthesis.\n\nIt is important to note that searching the phrase \"Generative Adversarial Network\" on Web Of Science (WOS) and Scopus repositories report that 2538 and 4405 documents are published, respectively starting from 2014 up to present.  Figure 1(a)  and 1(b) show the statistical results obtained from these repositories by searching the aforementioned phrase. The large number of researches published on this topic within only 6 years inspired us to conduct a comprehensive review considering one of the significant applications of GAN models called human emotion synthesis.\n\nSynthesizing realistic data samples is of great value for both academic and industrial communities. Affective computing, a topic of a broad interest in computer vision society benefits from human emotion synthesis and data augmentation. Throughout this paper, we concentrate on the recent advances in the field of GAN and their possible acquisition in the field of human emotion recognition which is known to be useful in other research areas like computer-aided diagnosis systems, security and identity verification, multimedia tagging systems, and human-computer and human-robot interactions. Humans communicate through various verbal and nonverbal channels to show their emotional state. All of the communication modalities are of high importance once interpreting the current emotional state of the user. In this paper, we focus on the GAN-related works of speech emotion synthesis, face emotion synthesis, and audio-visual (cross-modal) emotion synthesis because face and speech are known as pioneer communication channels among humans  (Schirmer and Adolphs, 2017; Ekman et al., 1988; Zuckerman et al., 1981; Mehrabian and Ferris, 1967) . Researchers developed many GAN-based models to address problems such as data augmentation, improvement of emotion recognition rate, and enhancement of synthesized samples through unimodal  (Ding et al., 2018) ,  (Choi et al., 2018; Tulyakov et al., 2018) ,  (Kervadec et al. , 2018),  (Kim et al., 2018) ,  (Pascual et al., 2017) ,  (Latif et al., 2017) ,  (Gideon et al., 2019) ,  (Zhou and Wang, 2017) ,  (Wang and Wan, 2018)  and cross-modal analysis  (Duarte et al., 2019) ,  (Karras et al., 2017a) ,  (Jamaludin et al., 2019) ,  (Chen et al., 2017) .\n\nA specific type of neural network called GAN models was introduced in 2014 by  Goodfellow et al. (Goodfellow et al., 2014) . This model is composed of a generative model pitting against an adversary model as a two-player minimax framework. The generative model captures data distribution. Then, given a sample, the adversary or the discriminator decides if the sample is drawn from the true data distribution (real) or the model distribution (fake). The competition continues until the generated samples are indistinguishable from the genuine ones.\n\nThis review deals with the GAN-based algorithms, theory, and applications in human emotion synthesis and recognition. The remainder of the paper is organized as follows: Section 2 provides a brief introduction to GANs and their variations. This is followed by a comprehensive review of related works on human emotion synthesis tasks using GANs in section 3. This section covers unimodal and cross-modal GAN-based methods developed using audio/visual modalities. Finally, section 4 summarizes the review, identifies potential applications, and discusses challenges.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Background",
      "text": "In general, generative models can be categorized into explicit density models and implicit density models. While the former utilizes the true data distribution or its parameters to train the generative model, the latter generates sample instances without an explicit parameter assumption or direct estimation on real data distribution. Examples of explicit density modeling are maximum likelihood estima-tion and Markov Chain Method  (Kingma and Welling, 2013; Rezende et al., 2014) . GANs can be considered as implicit density modeling example  (Goodfellow et al., 2014) .\n\n2.1 Generative Adversarial Networks (GAN)  Goodfellow et al. proposed  Generative Adversarial Networks or vanilla GAN in 2014  (Goodfellow et al., 2014) . The model works based on a two-player minimax game where one player seeks to maximize a value function and the other seeks to minimize it. The game ends at a saddle point when the first agent and the second agent reach a minimum and a maximum, respectively, concerning their strategies. This model draws samples directly from the desired distribution without explicitly modeling the underlying probability density function. The general framework of this model consists of two neural network components: a generative model G capturing the data distribution and a discriminative model D estimating the probability that a sample comes from the training samples or G.\n\nLet us designate the input sample for G as z where z is a random noise vector sampled from a priori distribution p z (z). Let us denote a real sample as x r that is taken from the data distribution P r . Also, we show an output sample generated by G as x g . Then, the idea is to get maximum visual similarity between the two samples. In fact, the generator G learns a nonlinear mapping function parametrized by θ g and formulated as: G(z; θ g ). The discriminator D, gets both x r and x g to output a single scalar value O 1 = D(x; θ d ) stating the probability that whether an input sample is a real or a generated sample  (Goodfellow et al., 2014) . It is important to highlight that D(x; θ d ) is the mapping function learned by D and parametrized by θ d . The final distribution formed by generated samples is P g and it is expected to approximate P r after learning. Figure  2 (a) illustrates the general block diagram of the vanilla GAN model.\n\nHaving two distributions P r and P g on the same probability space X , the KL divergence is as follows:\n\nwhere both P r and P g are assumed to admit densities with respect to a common measure defined on X . This happens when P r and P g are absolutely continuous, that is P g ≪ P r . The KL divergence is asymmetric, i.e KL(P r P g ) ≠ KL(P g P r ) and possibly infinite when there are points such that p g (x) = 0 and p r (x) > 0 for KL(P r P g ). A more convenient approach for GAN is the Jensen-Shannon (JS) divergence which may interpreted as a symmetrical version of KL divergence and it is defined as follows: JS(P r , P g ) = KL(P r P m ) + KL(P g P m ),\n\n(2)\n\nwhere P m = (P r + P g ) 2.\n\nIn other words, a minimax game between G and D continues to obtain a normalized and symmetrical score in terms of the value function V (G, D) as follows:\n\nHere, the parameters of G are adjusted by minimizing log(1 -D(G(x g ))). In a similar way, adjusting the parameters for D is performed by maximizing log D(x r ). Minimizing log(1 -D(G(x g ))) is known  (Goodfellow et al., 2014)  to be equivalent to minimizing the JS divergence between P r and P g as expressed in Eq. (  2 ). The value function V (θ g , θ d ) determines the payoff of the discriminator. Also, the generator takes the value -V (θ g , θ d ) as its own payoff. The generator and the discriminator, each attempts to maximize its own payoff  (Goodfellow et al., 2016)  during the learning process.The general framework of this model is shown in Figure  2",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Challenges Of Gan Models",
      "text": "The training objective of GAN models is often referred to as saddle point optimization problem  (Yadav et al., 2017)  which is resolved by gradient-based methods. One challenge here is that D and G should be trained at a time so that they advance and converge together. Minimizing the generators' objective is proven to be equivalent to minimizing JS divergence if the discriminatorD is trained to its optimal point before the next update of G. This means minimizing the JS divergence does not guarantee finding the equilibrium point between G and D through the training process. This normally leads to a better performance of D as opposed to G. Consequently, at some point classifying real and fake samples becomes such an easy task that gradients of D approach zero and it becomes ineffectual in the learning procedure of G. Mode collapse is another well-known problem in training GAN models where G produces a limited set of repetitive samples due to focusing on a few limited modes of the true data distribution, namely P r during learning and approximating distribution P g . We discuss these problems in more detail in section 4.1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Variants By Architectures",
      "text": "The GAN model can be extended to a conditional GAN (CGAN) model if both the generator and discriminator are conditioned on some extra information y  (Mirza and Osindero, 2014) . Figure  2 (b) shows the block diagram of the CGAN model. The condition vector y = c is fed into both the discriminator and the generator through an additional input layer. Here, the latent variable z with prior density p z (z) and condition vector y with some value c ∈ R d are passed through one perceptron layer to learn the joint hidden representation. Conditioning on c changes the training criterion of Eq. (  3 ) and leads to the following criterion:\n\nwhere c could be target class labels or auxiliary information from other modalities.\n\nAnother type of GAN models is Laplacian Generative Adversarial Network (LAPGAN)  (Denton et al., 2015)  that formed by combining CGAN models progressively within a Laplacian pyramid representation. LAPGAN includes a set of generative convolutional models, say G 1 , . . . , G K . The synthesis procedure consists of two parts a sampling phase and a training phase. The sampling phase starts with generator G 1 that takes a noise sample z 1 and generates sample x g 1 . The generated sample is upscaled before passing to the generator of next level as a conditioning variable. G 2 takes both upscaled version of x g 1 and a noise sample z 2 to synthesize a difference sample called h 2 which is added to the upscaled version of x g 1 . This process of upsampling and addition repeats across successive levels to yield a final full resolution sample. The Figure  3  illustrates the general block diagram of the LAPGAN model. SGAN is a second example formed by top-down stacked GAN models  (Huang et al., 2017b)  to solve the low performance of GAN models in discriminative tasks with large variation in data distribution.  Huang et al. (2017b)  employ the hierarchical representation in a model trained discriminatively by stitching GAN models in a top-down framework and forcing the top-most level to take class labels and the bottom-most one to generate images. Alternatively, instead of stacking GANs on top of each other,  Karras et al. (2017b)  increased the depth of both the generator and the discriminator by adding new layers. All models are developed under conditional GAN  (Denton et al., 2015; Huang et al., 2017b; Karras et al., 2017b) .\n\nOther models modify the input to the generator slightly. For instance, in SPADE  (Park et al., 2019)  a segmentation mask is fed indirectly to the generator through an adaptive normalization layer instead of utilizing the standard input noise vector z. Also, StyleGAN  (Wang et al., 2018a)  injects z, first to an intermediate latent space that helps to avoid entanglement of the input latent space to the probability density of the training data.\n\nIn 2015,  Radford et al. (Radford et al., 2015)  proposed Deep Convolutional Generative Adversarial Network (DCGAN) in which both the generator and the discriminator were formed by a class of architecturally constrained convolutional networks. In this model, fully convolutional downsampling/upsampling layers replaced the Fully connected layers of vanilla GAN along with other architectural restrictions like using batch-normalization layers and LeakyReLU activation function in all layers of the discriminator.\n\nAnother advancement in GAN models includes using the spectral normalization layer to adjust feature response criterion by normalizing the weights in the discriminator network  (Miyato et al., 2018) . Residual connections are another novel approach fetched into the GAN models by  Gulrajani et al. (2017)  and  Miyato et al. (2018) . While models like CGANs incorporate the conditional information vector simply by concatenation, others remodeled the usage of a conditional vector by a projection approach leading to significant improvement in the quality of the generated samples  (Miyato and Koyama, 2018) .\n\nThe aforementioned GAN models expanded based on Convolutional Neural Networks (CNN). Further, along this line, a whole new research line of GAN models developed based on recent deep learning models called CapsuleNets (CapsNets)  (Sabour et al., 2017) . Let v k be the output vector of the final layer of a CapsNet that represents the presence of a visual entity by classifying to one of the K classes.  Sabour et al. (2017)  provide an updated objective function that benefits from CapsNet margin loss (L M ) and it could be expressed as follows:\n\nwhere m + , m -, and λ are down-weighting factors set to 0.9, 0.1, and 0.5, respectively to stop initial learning from shrinking the lengths of the capsule outputs in the final layer. The length of each capsule in the final layer ( v k ) can then be viewed as the probability of the image belonging to a particular class (k). Also, T k denotes the target label.  CapsuleGAN (Jaiswal et al., 2018 ) is a GAN model proposed by Jaiswal et al. (  2018 ) based on CapsNet.The authors use CapsNet in the discriminator as opposed to conventional CNNs. The final layer of this discriminator consists of a single capsule representing the probability of being a real or fake sample. They used the margin loss introduced in Eq. (  5 ) instead of the binary cross-entropy loss for training. The training criterion of the CapsuleGAN is then formulated as follows:\n\nPractically, the generator must be trained to minimize L M (D(G(z)), T = 1) rather than minimizing -L M (D(G(z)), T = 0). This helps eliminating the downweighting factor λ in L M when training the generator, which does not contain any capsules.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Variants By Discriminators",
      "text": "Stabilizing the training and avoiding mode collapse problem could be achieved by employing different loss functions for D. An entropy-based loss is proposed by  Springenberg (2015)  called Categorical GAN (CatGAN) in which the objective of discriminator changed from real-fake classification to entropy-based class predictions. WGAN  (Arjovsky et al., 2017)  and an improved version of it called WGAN-GP  (Gulrajani et al., 2017)  are two GAN models with a loss function based on Wasserstein distance used in the discriminator. The Earth-Mover (EM) distance or Wasserstein-1 is expressed as follows:\n\nwhere ∏(Pr, P g ) is the set of all joint distributions γ(x, y) whose marginals are respectively, P r and P g . Here, γ(x, y) describes how much \"mass\" needs to be transported from x to y in order to transform the distribution P r into P g . The EM distance is then the \"cost\" of the optimal transport plan.\n\nOther alternative models that benefit from a different loss metric are GAN based on Category Information (CIGAN)  (Niu et al., 2018) , hinge loss  (Miyato et al., 2018) , least-square GAN  (Mao et al., 2017) , and f-divergence GAN  (Nowozin et al., 2016) . Research developments include replacing the encoder structure of the discriminator with an autoencoder structure. In fact, a new loss objective is defined for the discriminator which corresponds to the autoencoder loss distribution instead of data distribution. Examples of such GAN frameworks are Energy-based GAN (EBGAN)  (Zhao et al., 2016)  and Boundary Equilibrium GAN (BEGAN)  Berthelot et al. (2017) . Figure  4  illustrates the block diagram of GAN models developed by modification in the discriminator.\n\nAnother interesting GAN model proposed by  Chen et al. (2016)  is Information Maximizing Generative Adversarial Net (InfoGAN), which simply modifies the discriminator to output both the fake/real classification result and the semantic features of x g illustrated as f in Figure  4(c ). The discriminator performs real/fake prediction by maximizing the mutual information between the x g and conditional vector c. Other models like CIGAN  (Niu et al., 2018)  and ACGAN  (Odena et al., 2017)  focused on improving the quality of the generated samples by employing the class labels during synthesis and then impelling D to provide entropy loss information as well as class probabilities. The Figure  4(d)  shows the structure of ACGAN.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Variants By Generators",
      "text": "The objective of generators is to transform noise input vector z to a sample x g = G(z). In the standard vanilla GAN, this objective is achieved by successively improving the state of the generated sample. The procedure stops when the desired quality is captured. Variational AutoEncoder GAN network (VAEGAN)  (Larsen et al., 2015)  is arguably the most popular GAN model proposed by varying on the generator architecture. The VAEGAN computes the reconstruction loss in a pixel-wise approach. The decoder network of VAE outputs patterns resembling the true samples (see Figure  5 (b)).\n\nOne challenge in designing GAN models is controlling the attributes of the generated data known as a mode of data. Using supplemental information leads to sample generation with control over the modification of the selected properties. The generator output then becomes x g = G(z, c). GANs lack the capability of interpreting the underlying latent space that encodes the input sample. ALI  (Dumoulin et al., 2016)  and  BiGAN (Donahue et al., 2016)  are proposed to resolve this problem by embedding an encoder network in the generator as shown in Figure  5(a) . Here, the discriminator performs real/fake prediction by distinguishing between the tuples (z g , x g ) and (z r , x r ). This can categorize the model as a discriminator variant as well.\n\nOther researchers developed the generators to solve specific tasks.  Isola et al. (2017)  designed pix2pix as an image-to-image translation network to study relations between two visual domains and  Milletari et al. (2016)  proposed VNet with Dice loss for image segmentation. The disadvantage of such networks was the aligned training with paired samples. In 2017,  Zhu et al. and Kim et al.  found a solution to perform unpaired image-to-image translation using cycle consistency loss and cross-domain relations, respectively. Here, the idea was to join two generators together to perform translation between sets of unpaired samples. Below,  CycleGAN  (Zhu et al., 2017)  and UNIT are successful examples derived from VAEGAN model. Figure  6 (c) illustrates the layout for UNIT framework. It is important to highlight that considering the generators, the conditional input may vary from class labels  (Mirza and Osindero, 2014 ) and text descriptions  (Reed et al., 2016) ,  (Xu et al., 2018)  to object location and encoded audio features or crossmodal correlations.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Applications In View Of Human Emotion",
      "text": "In this section, we discuss applications of GAN models in human emotion synthesis. We categorize related works into unimodal and cross-modal researches based on audio and video modalities to help the reader discover applications of interest without difficulty. Also, we explain each method in terms of the proposed algorithm and its advantages and disadvantages. Generally, applications of GAN for human emotion synthesis focus on two issues. The first one is data augmentation that helps obviating the need for the tedious job of collecting and labeling large scale databases and the second is improving the performance on emotion recognition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Facial Expression Synthesis",
      "text": "Facial expression synthesis using conventional methods confronts several important problems. First, most methods require paired training data, and second, the generated faces are of low resolution. Moreover, the diversity of the generated faces is limited. The works reviewed in this section are taken from the computer-visionrelated researches that focus on facial expression synthesis.\n\nOne of the foremost works on facial expression synthesis was the study by  Susskind et al. (2008)  that could embed constraints like \"raised eyebrows\" on generated samples. The authors build their framework upon a Deep Belief Network (DBN) that starts with two hidden layers of 500 units. The output of the second hidden layer is concatenated with identity and a vector of the Facial Action Coding System (FACS)  (Ekman and Friesen, 1978)  to learn a joint model of them through a Restricted Boltzmann Machine (RBM) with 1000 logistic hidden units. The trained DBN model is then used to generate faces with different identities and facial Action Units (AU).\n\nLater, with the advent of GAN models, DyadGAN is designed specifically for face generation and it can generate facial images of an interviewer conditioned on the facial expressions of their dyadic conversation partner. ExprGAN  (Ding et al., 2018)  is another model designed to solve the problems mentioned above. ExprGAN has the ability to control both the target class and the intensity of the generated expression from weak to strong without a need for training data with intensity values. This is achieved by using an expression controller module that encodes complex information like expression intensity to a real-valued vector and by introducing an identity preserving loss function.\n\nOther proposed methods before ExprGAN had the ability to synthesize facial expressions either by manipulating facial components in the input image  (Yang et al., 2011; Mohammed et al., 2009; Yeh et al., 2016)  or by using the target expression as a piece of auxiliary information  (Susskind et al., 2008; Reed et al., 2014; Cheung et al., 2014)    2, 3, 4,  and 6  -M: Metric, RS: Results, RM:Remarks -*: shows the proposed method by authors, other mentioned methods are implemented by the authors for the sake of comparison -the result reported for expression classification accuracy (Mv 2 ) belongs to the synthesized image datasets - †: Dv 9 + Dv 10 is used as the database - ‡: Dv 9 + Dv 11 is used as the database -All papers provide visual representation of the synthesized images (Mv 7 )\n\nTable  1  compares the reviewed publication based on various metrics, databases, loss functions and purposes used by researchers. Following those models and through the many variations of facial expression synthesis proposed by researchers, the GAN-based model proposed by  Song et al. (2018)  was one of the interesting and premier ones, called G2GAN. G2GAN generates photo-realistic and identitypreserving images. Furthermore, it provides fine-grained control over the target expression and facial attributes of the generated images like widening the smile of the subject or narrowing the eyes. The idea here is to feed the face geometry into the generator as a condition vector which guides the expression synthesis procedure. The model benefits from a pair of GANs that while one removes the expression, the other synthesizes it. This leverages on the ability of unpaired training.\n\nStarGAN  (Choi et al., 2018)  is the first approach with a scalable solution for multi-domain image-to-image translation using a unified GAN model (i.e only a single generator and discriminator). In this model, a domain is defined as a set  of images sharing the same attribute and attributes are the facial features like hair color, gender, and age which can be modified based on the desired value. For example, one can set hair color to be blond or brown and set the gender to be male or female. Likewise, Attribute editing GAN (AttGAN)  (He et al., 2019)  provides a GAN framework that can edit any attribute among a set of attributes for face images by employing adversarial loss, reconstruction loss, and attribute classification constraints. Also, DIAT  (Li et al., 2016) , CycleGAN  (Zhu et al., 2017)  and IcGAN  (Perarnau et al., 2016 ) could be compared as baseline models.\n\nIn 2018, G2GAN Song et al. is extended by Qiao et al.. The authors derived a model based on VAEGANs to synthesize facial expressions given a single image and several landmarks through some transferring stages. Different from ExprGAN their model does not require the target class label of the generated image. Also, unlike G2GAN, it does not require the neutral expression of a specific subject as an intermediate level in the facial expression transfer procedure. While G2GAN and its extension focus on geomterical features to guide the expression synthesis procedure,  Pumarola et al. (2018)  use facial AU as a one-hot vector to perform an unsupervised expression synthesis while smooth transition and unpaired samples are guaranteed.\n\nAnother VAEGAN-based model is the work of  (Lai and Lai, 2018)  where a novel optimization loss called symmetric loss is introduced. Symmetric loss helps preserving the symmetrical property of the face while translating from various head poses to frontal-view of the face. Similar to Lai and Lai is the FaceID-GAN  (Shen et al., 2018a)  where, in addition to the two-players of vanilla GANs and symmetry information, a classifier of face identity is employed as the third player that competes with the generator by distinguishing the identities of the real and synthesized faces.  Lai and Lai (2018)  used GAN to perform emotion-preserving representations. In the proposed approach, the generator can transform the non-frontal facial images into frontal ones while the identity and the emotion expression are preserved. Moreover, a recent publication  (Vielzeuf et al., 2019)  relies on a two-step GAN framework. The first component maps images to a 3D vector space. This vector is issued from a neural network and it represents the corresponding emotion of the image. Then, a second component that is a standard image-to-image translator uses the 3D points obtained in the first step to generate different expressions. The proposed model provides fine-grained control over the synthesized discrete expressions through the continuous vector space representing the arousal, valence, and dominance space.\n\nIt should be noted that a series of GAN models focus on 3D object/face generation. Examples of these models are Convolutional Mesh Autoencoder (CoMA)  (Ranjan et al., 2018) , MeshGAN  (Cheng et al., 2019) , UVGAN  (Deng et al., 2018) , and MeshVAE  (Litany et al., 2018) . Despite the successful performance of GANs in image synthesis, they still fall short when dealing with 3D objects and particularly human face synthesis. Here, we compare synthesized images of the aforementioned methods qualitatively in Figures  7  and 8 . Images are taken from the corresponding papers. As the images show, most of the generated samples suffer from blurring problem.\n\nIn addition to GAN-based models that synthesize single images, there are models with the ability to generate an image sequence or a video/animation. Video GAN (VGAN)  (Vondrick et al., 2016)  and Temporal GAN (TGAN)  (Saito et      (video) given the pose sequence generated by the PSGAN. The effect of noisy or abnormal poses between the generated and ground-truth poses is reduced by the semantic consistency. We show this method as PS/SCGAN in Table  5 . It is worth to mention that two of the recent and successful methods in video generation are MetaPix  (Lee et al., 2019b)  and MoCycleGAN  (Chen et al., 2019)  that used motion and temporal information for realistic video synthesis. However, these methods are not tested for facial expression generation. Table  5  lists the models developed for video or animation generation.\n\nOne of the main goals in synthesizing is augmenting the number of available samples.  Zhu et al. (2018)  used GAN models to improve the imbalanced class distribution by data augmentation through GAN models. The discriminator of the model is a CNN and the generator is based on CycleGAN. They report up to 10% increase in the classification accuracy (Mv 2 ) based on GAN-based data augmentation techniques.\n\nThe objective function or the optimization loss problem categorizes into two groups: synthesis loss and classification loss. Although the definitions provided by the authors are not always clear, we tried to list all different losses used by authors and we propose a symbolic name for each to provide harmony in the literature. The losses are used in a general point of view. That is, marking different papers by classification loss (L7) in Table  1 , does not mean necessarily that the exact same loss function is used. In other words, it shows that the classification loss is contributed in some way. A comprehensive list of these functions is given in Table  3 . Additionally, we compared some of the video synthesis models in Figure  9 .\n\nEvaluation metrics of the generative models are different from one research to another due to several reasons  (Hitawala, 2018) . First, the quality of the synthesized sample is a perceptual concept and, as a result, it cannot be accurately expressed. Usually, researchers provide the best-synthesized samples for visual comparison and thus problems like mode drop are not covered qualitatively. Second, employing human annotators to judge the visual quality can cover only a limited number of data samples. Specifically, in topics such as human emotion, experts are required for accurate annotation and having the least possible labeling error. Hence, approaches like Amazon Mechanical Turk are less reliable considering classification based on those labels. Third, general metrics like photo-metric error, geometric error, and inception score are not reported in all publications  (Salimans et al., 2016) . These problems cause the comparison among papers either unfair or impossible.\n\nThe Inception Score (IS) can be computed as follows:\n\nwhere x g denotes the generated sample, y is the label predicted by an arbitrary classifier, and KL(.) is the KL divergence to measure the distance between probability distributions as defined in Eq. (  1 ). Based on this score, an ideal model produces samples that have close congruence to real data samples as much as possible. In fact, KL divergence is the de-facto standard for training and evaluating generative models.\n\nOther widely used evaluative metrics are Structural Similarity Index Measure (SSIM) and Peak Signal to Noise Ratio (PSNR). SSIM is expressed as follows:\n\nwhere I, C, and S are luminance, contrast, and structure and they can be formulated as:\n\nHere µ x , µ y , σ x , and σ y denote mean and standard deviations of pixel intensity in a local image patch where the patch is superimposed so that its center coincides with the center of the image. Typically, a patch is considered as a square neighborhood of n × x pixels. Also, σ xy is the sample correlation coefficient between corresponding pixels in that patch. C1, C2, and C3 are small constants values added for numerical stability.\n\nPSNR or the peak signal-to-noise ratio assesses the quality between two monochrome images x g and x r . Let x g and x r be the generated image and the real image, respectively. Then, PSNR is:\n\nwhere MAX x r is the maximum possible pixel value of the image and MSE stands for Mean Square Error. PSNR is measured in dB, generated images with a better quality result in higher PSNR.\n\nIn addition to the metrics that evaluate the generated image, Generative Adversarial Metric (GAM) proposed by  Im et al. (2016)  compares two GAN models by engaging them in a rivalry. In this metric, first GAN models M 1 and M 2 are trained. Then, model M 1 competes with model M 2 in a test phase by having M 1 trying to fool discriminator of M 2 and vice versa. In the end, two ratios are calculated using the discriminative scores of these models as follows:\n\n, and\n\nwhere G 1 , D 1 , G 2 , and D 2 , are the generators and the discriminators of M 1 and M 2 , respectively. In Eq. (  12 ), ǫ(.) outputs the classification error rate. The test ratio or r test shows which model generalizes better because it discriminates based on X test . The sample ratio or r sample shows which model fools the other more easily because discriminators classify based on the synthesized samples of the opponent.\n\nThe sample ratio and the test ratio can be used to decide the winning model:\n\nTo measure the texture similarity, Peng and Yin (2019) simply calculated correlation coefficients between T g and T r that are the texture of the synthesized image and the texture of its corresponding ground truth, respectively. Let ρ be the texture similarity score. Then, the mathematical representation is as follows:\n\nwhere (i, j) specifies pixel coordinates in the texture images, and µ g and µ r are the mean value of T g and T r , respectively. Other important metrics include Fréchet Inception Distance (FID), Maximum Mean Discrepancy (MMD), the Wasserstein Critic, Tournament Win Rate and Skill Rating, and Geometry Score. FID works based on embedding the set of synthesized samples into a new feature space using a certain layer of a CNN architecture. Then, mean and covariance are estimated for both the synthesized and the real data distributions based on the assumption that the embedding layer is a continuous multivariate Gaussian distribution. Finally, FID or Wasserstein-2 distance between these Gaussians is then used to quantify the quality of generated samples :\n\nHere, (µ g , ∑ g ) and (µ r , ∑ r ) represent the mean and covariance of generated and real data distributions, respectively. Lower FID score indicates a smaller distance between the two distributions. MMD focuses on the dissimilarity between the two probability distributions by taking samples from each distribution independently. The kernel MMD is expressed as follows:\n\nwhere k is some fixed characteristic kernel function like Gaussian kernel: k(x r , x g ) = exp(∥ x rx g ∥ 2 ) that measures MMD dissimilarity between the generated and real data distributions. Also, x r and x ′ r are randomly drawn samples from real data distribution, i.e P r . Similarly, x g and x ′ g are randomly drawn from model distribution, i.e P g .\n\nThe Wasserstein Critic provides an approximation of the Wasserstein distance between the model distribution and the real data distribution. Let P r and P g be the real data and the model distributions, then:\n\nwhere f ∶ R D → R is a Lipschitz continuous function. In practice, the critic f is a neural network with clipped weights and bounded derivatives  (Borji, 2019) . In practice, this is approximated by training to achieve high values for real samples and low values for generated ones:\n\nwhere X test is a batch of testing samples, X g is a batch of generated samples, and f is the independent critic. An alternative version of this score is known as Sliced Wasserstein Distance (SWD) that estimates the Wasserstein-1 distance (see Eq. (  7 )) between real and generated images. SWD computes the statistical similarity between local image patches extracted from Laplacian pyramid representations of the images  (Karras et al., 2017b) .\n\nIn the case of the metrics of video generation, evaluating content consistency based on Average Content Distance (ACD) is defined as calculating the average pairwise L 2 distance of the per-frame average feature vectors. In addition, the motion control score (MCS) is suggested for assessing the motion generation ability of the model. Here, a spatio-temporal CNN is first trained on a training dataset. Then, this model classifies the generated videos to verify whether the generated video contained the required motion (e.g action/expression).\n\nOther metrics include but are not limited to identification classification, true/false acceptance rate  (Song et al., 2018) , expression classification accuracy/error  (Ding et al., 2018) , real/fake classification accuracy/error  (Ding et al., 2018) , attribute editing accuracy/error (He et al., 2019), and Fully Convolutional Networks. List of evaluative metrics used in the reviewed publications is given in Table  4 . For a comprehensive list on evaluative metrics of GAN models, we invite the reader to study \"Pros and Cons of GAN Evaluation Measures\" by  Borji (2019) .\n\nSynthesizing models are proposed with different aims and purposes. Texture synthesis, super-resolution images, and image in-painting are some applications. Considering face synthesis, the most important goal is the data augmentation for improved recognition performance. A complete list of such purposes and the model properties are given in Table  6 .\n\nDespite the numerous publications on image and video synthesis, yet some problems are not solved thoroughly. For example, generating high-resolution samples is an open research problem. The output is usually blurry or impaired by checkered artifacts. Results obtained for video generation or synthesis of 3D samples are far from realistic examples. Also, it is important to highlight that the supports unsupervised learning\n\nTable  6  List of purposes and characteristics of GAN models used for facial emotion synthesis by the reviewed publications number of publications focused on expression classification is greater than that of those employing identity recognition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Speech Emotion Synthesis",
      "text": "Research efforts focusing on synthesizing speech with emotion effect has continued for more than a decade now. One application of GAN models in speech synthesis is speech enhancement. A pioneer GAN-based model developed for raw speech generation and enhancement is called the Speech Enhancement GAN (SEGAN) proposed by  Pascual et al. (2017) . SEGAN provides a quick non-recursive framework that works End-to-End (E2E) with raw audio. Learning from different speakers and noise types and incorporating that information to a shared parameterizing system is another contribution of the proposed model. Similar to SEAGAN,  Macartney and Weyde (2018)  proposes a model for speech enhancement based on a CNN architecture called Wave-UNet. The Wave-UNet is used successfully for audio source separation in music and speech de-reverberation. Similar to section 3.1, we compare the results of the reviewed papers in Table  7 . Additionally, Tables 8 to 11 represent databases, loss functions, assessment metrics and characteristics used in speech synthesis.  Sahu et al. (2018)  followed a two-fold contribution. First, they train a simple GAN model to learn a high-dimensional feature vector through the distribution of a lower-dimensional representation. Second, cGAN is used to learn the distribution of the high-dimensional feature vectors by conditioning on the emotional label of the target class. Eventually, the generated feature vectors are used to assess the improvement of emotion recognition. They report that using synthesized samples generated by cGAN in the training set is helpful. Also it is concluded that using synthesized samples in the test set suggests the estimation of a lower-dimensional distribution is easier than a high-dimensional complex distribution. Employing the synthesized feature vectors from IEMOCAP database in a cross-corpus experiment on emotion classification of MSP-IMPROV database is reported to be successful.\n\nMic2Mic  (Mathur et al., 2019)  is another example of a GAN-based model for speech enhancement. This model addresses a challenging problem called microphone variability. The Mic2Mic model disentangles the variability problem from the downstream speech recognition task and it minimizes the need for training data. Another advantage is that it works with unlabeled and unpaired samples from various microphones. This model defines microphone variability as a data translation from one microphone to another for reducing domain shift between the train and the test data. This model is developed based on CycleGAN to assure that the audio sample  (Mathur et al., 2019)      Gao et al. (2018)  decomposed each speech signal into two codes: a content code that represents emotion-invariant information and a style code that represents emotion-dependent information. The content code is shared across emotion domains and should be preserved while the style code carries domain-specific information and it should change. The extracted content code of the source speech and the style code of the target domain are combined at the conversion step. Finally, they use the GAN model to enhance the quality of the generated speech.\n\nAnother widely extended research direction in speech synthesis is Voice Conversion (VC).  Hsu et al. (2017)  proposed a non-parallel VC framework called Variational Autoencoding Wasserstein Generative Adversarial Network (VAWGAN). This method directly incorporates a non-parallel VC criterion into the objective function to build a speech model from unaligned data. VAWGAN improves the synthesized samples with more realistic spectral shapes. Even if VAE-based approaches can work free of parallel data and unaligned corpora, yet they have three drawbacks. First, it is difficult to learn time dependencies in the acoustic feature sequences of source and target speech. Second, the decoder of the VAEs tends to output over-smoothed results. To overcome these limitations,  Kameoka et al. (2018a)  adopted fully convolutional neural networks to learn conversion rules that capture short-term and long-term dependencies. Also, by transplanting the spectral details of input speech into its converted version at the test phase, the proposed model avoids producing buzzy speech. Furthermore, in order to prevent losing class information during the conversion process, an information-theoretic regularizer is used.\n\nIn 2018, Kaneko and Kameoka made two modifications in CycleGAN architecture to make it suitable for voice conversion task and so the name CycleGAN-VC is selected for the modified architecture. Representing speech by using Recurrent Neural Networks (RNN) is more effective due to the sequential and hierarchical structure of the speech. Howsoever, RNN is computationally demanding considering parallel implementations. As a result, they used gated CNNs that are proven to be successful both in parallelization over sequential data and achieving high performance. The second modification is made by using identity loss to assure preserving linguistic information. Here, a 1D CNN is used as a generator and a 2D CNN as a discriminator to focus on 2D spectral texture. Aurora 4 (Hirsch and Pearce) ---\n\nTable  8  List of databases used for speech synthesis in the reviewed publications\n\nLater, they released CycleGAN-VC2  (Kaneko et al., 2019)  which is an improved version of CycleGAN-VC to fill the large gap between the real target and converted speech. Architecture is altered by using 2-1-2D CNN for the generator and PatchGAN for the discriminator. In addition, the objective function is improved by employing a two-step adversarial loss. It is known that downsampling and upsampling have a severe degradation effect on the original structure of the data. To alleviate this, a 2-1-2D CNN architecture is used in the generator where 2D convolution is used for downsampling and upsampling, and only 1D convolution is used for the main conversion process. Another difference is that while CycleGAN-VC uses a fully connected CNN as its discriminator, CycleGAN-VC2 uses PatchGAN. The last layer of PatchGAN employs convolution to make a patchbased decision for the realness of samples. The difference in objective functions between these two models is reported in Table  7 . They report that CycleGAN-VC2 outperforms its predecessor on the same database.\n\nTo overcome the shortcomings of CVAEVC  (Kameoka et al., 2018a)  and CycleGAN-VC  (Kaneko and Kameoka, 2018) , the StarGAN-VC  (Kameoka et al., 2018b)  method combines these two methods to address nonparallel many-to-many voice conversion. While CVAEVC and CycleGAN-VC require to know the attribute of the input speech at the test time, StarGAN does not need any such information. Other GAN-based methods for VC like WaveCycleGAN-VC  (Tanaka et al., 2018)  and WaveCycleGAN-VC2  (Tanaka et al., 2019b)  rely on learning based on filters that prevents quality degradation by overcoming the over-smoothing effect. The over-smoothing effect causes degradation in resolution of acoustic features of the generated speech signal. WaveCycleGAN-VC uses cycle-consistent adversarial networks to convert synthesized speech to natural waveform. The drawback of WaveCycleGAN-VC is aliasing distortion that is avoided in WaveCycleGAN-VC2 by adding identity loss. Conventional methods like VAEs, cycle-consistent GANs, and StarGAN have a common limitation. Instead of focusing on converting prosodic features like Fundamental frequency contour, they focus on the conversion of spectral features frame by frame. A fully convolutional sequence-to-sequence (seq2seq) learning approach is proposed by  Kameoka et al. (2018c)  to solve this problem. Generally, all inputs of a seq2seq model must be encoded into a fixed-length vector. In order to avoid this general limitation of seq2seq models, the authors used an attentionbased mechanism that learns where to pay attention in the input sequence for each output sequence. The advantage of seq2seq models is that one can transform a sequence into another variable-length sequence. The proposed model is called ConvS2S  (Kameoka et al., 2018c)  and its architecture comprises a pair of source and target encoders, a pair of source and target re-constructors, one target decoder, and a PostNet. The PostNet aims to restore the linear frequency spectrogram from its Mel-scaled version.\n\nSimilar to ConvS2S is ATTS2S-VC  (Tanaka et al., 2019a ) that employs attention and context preservation mechanisms in a Seq2Seq-based VC system. Although this method addresses the aforementioned problems, yet it has a lower performance in comparison to CVAE-VC, CycleGAN-VC, CycleGAN-VC2, and StarGAN. An ablation study is required to evaluate each component of seq2seq methods considering performance degradation.\n\nDespite the promising performance of deep neural networks, they are highly susceptible to malicious attacks that use adversarial examples. One can develop an adversarial example through the addition of unperceived perturbation with the intention of eliciting wrong responses from the machine learning models.  Latif et al. (2017)  conducted a study on how adversarial examples can be used to attack speech emotion recognition (SER) systems. They propose the first black-box adversarial attack on SER systems that directly perturbs speech utterances with small and imperceptible noises. Later, the authors perform emotion classification to clean audio utterances by removing that adversarial noise using a GAN model to show that GAN-based defense stands better against adversarial examples. Other examples of malicious attacks are simulating spoofing attacks  (Cai et al., 2018)  and cloning Obama's voice using GAN-based models and low-quality data  (Lorenzo-Trueba et al., 2018a) . The next target application of speech synthesis is data augmentation. Data augmentation is the task of increasing the amount and diversity of data to compensate for the lack of data in certain cases. Data augmentation can improve the generalization behavior of the classifiers. Despite its importance, only a few papers contributed fully toward this concept.\n\nOne of the researches on data augmentation for the purpose of SER improvement is the work of  Sheng et al. (2018) . Sheng et al. used a variant of cGANs model that works at frame level and uses two different conditions. The first condition is the acoustic state of each input frame that is combined as a one-hot vector with the noise input and fed into the generator. The same vector is combined with real noisy speech and fed into the discriminator. The second condition is the pairing of speech samples during the training process. In fact, parallel paired data is used for training. For example, original and clean speech is paired with manually added noisy speech or close-talk speech sample is paired with far-field recorded speech. The discriminator learns the naturalness of the sample based on the paired data. Another study with more focus on the improvement of SER is done by  Chatziagapi et al. (2019) . They adopt a cGAN called Balancing GAN (BAGAN)  (Mariani et al., 2018)  and improve it to generate synthetic spectrograms for the minority or underrepresented emotion classes. The authors modified the architecture of BAGAN by adding two dense layers to the original generator. These layers project the input to higher dimensionality. Also, the discriminator is changed by using double strides to increase the height and width of the intermediate tensors which affect the quality of the generated spectrogram remarkably.\n\nOther interesting applications like cross-language emotion transfer and singing voice synthesis are also investigated by various researches. However, these applications are not thoroughly studied and they have plenty of potential for further research. One such example is ET-GAN  (Jia et al., 2019) . This model uses a cycleconsistent GAN to learn language-independent emotion transfer from one emotion to another while it does not require parallel training samples.\n\nAlso, some works are dedicated to speech synthesis in the frequency domain. Long-range dependencies are difficult to model in the time domain. For instance, MelNet model  (Vasquez and Lewis, 2019)  proves that such dependencies can be more tractably modeled in two dimensional (2D) time-frequency representations such as spectrograms. By coupling the 2D spectrogram representation and an autoregressive probabilistic model with a multi-scale generative model, they synthesized high fidelity audio samples. This model captures local and global structures at time scales that time-domain models have yet to achieve. In a MOS comparison between MelNet and WaveNet, MelNet won with a 100% vote for a preference of the quality of the sample.\n\nIn the case of speech synthesis in feature-domain, several pieces of research are represented under VC application. For instance,  Juvela et al. (2018)  proposed generating speech from filterbank Mel-frequency cepstral coefficients (MFCC). The method starts by predicting the fundamental frequency (F 0 ) and the intonation information from MFCC using an auto-regressive model. Then, a pitch synchronous excitation model is trained on the all-pole filters obtained in turn from spectral envelope information in MFCCs. In the end, a residual GAN-based noise model is used to add a realistic high-frequency stochastic component to the modeled exci-tation signal. Degradation Mean Opinion Score (DMOS) is used to evaluate the quality of synthesized speech samples.\n\nIn order to evaluate the local and global structures of the generated samples, various metrics are employed by the researchers. In general, metrics like Mean opinion score (MOS) and Perceptual Evaluation of Speech Quality (PESQ) are used widely, while other efficient metrics like Mel-cepstral distortion (MCD) and modulation spectra distance (MSD) are less employed in the literature. Following, we provide a brief explanation of each metric with the hope of a more cohesive future comparison in the literature.\n\nMOS is a quality rating method that works based on the subjective quality evaluation test. The quality is assessed by human subjects for a given stimulus. The user rates its Quality of Experience (QoE) as a number within a categorical range with \"bad\" being the lowest perceived quality 1 and 5 being \"Excellent\" or the highest perceived quality. MOS is expressed as the arithmetic mean over QoEs.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Mos",
      "text": "where N is the total number of subjects contributed to the evaluation and r n is the QoE of the subject considering the stimuli. MOS is subject to certain biases. Number of subjects in the test and content of the samples under assessment are some of the problems. The ordinal categories code a wide range of perceptions. That is why MOS is considered to be an absolute measure of total quality, regardless of any specific quality dimension. This is useful in may applications related to communications. However, for other applications, a measure that could be more sensitive to specific quality dimensions is more suitable. Other biases include, changing the user expectation about quality over time, and the value of the smallest MOS difference that is perceptible to users and can actually claim if one method is better over another. For example,  Pascual et al. and Macartney and  Weyde achieved an MOS of 3.18 and 2.41 on the same database which provides a naive comparison of .77 MOS difference in favor of the former method. Howsoever, one question here is that whether the sample tests and the number of subjects were the same. This becomes more interesting by comparison of the methods proposed in  Tanaka et al. (2018)  and  Tanaka et al. (2019b)  where the authors achieved only a .11 MOS difference using the same database. MOS is time-consuming though, it is applicable to different quantities. For instance, likewise MOS of voice quality, the MOS of signal distortion and MOS for intrusiveness of background noise are used as a metric. PESQ is an objective speech quality assessment based on subjective quality ratings. In fact, it automatically calculates what is MOS of subjective perception. PESQ integrates disturbance over several time frequency scales. This is applied by using a method that take soptimal account of the distribution of error in time and amplitude  (Rix et al., 2001; Recommendation, 2001) . The disturbance values are aggregated using a L p norm as follows:\n\nSumming disturbance across frequency using an L p norm gives a frame-byframe measure of perceived distortion. The subjective listening tests were designed to reduce the effect of uncertainty arising from the listener's decision by highlighting which of the three components of a noisy speech signal should form the basis of their ratings of overall quality. These components are the speech signal, the back- ground noise, or both. In this method the listener successively attends and rates the synthesized speech sample on: a) the speech signal alone using a five-point scale of signal distortion (SIG), b) the background noise alone using a five-point scale of background intrusiveness (BAK), c) the overall quality using the scale of the mean opinion score (OVRL). The SIG, BAK, and OVRL scales are described in Table  12 .\n\nThe Signal to Noise Ratio (SNR) and Segmental Signal to Noise Ratio can be expressed as follows:\n\nwhere x(i) and y(i) are the ith real and synthesized samples and N is the total number of samples. Segmental signal to Noise Ratio (SSNR/SegSNR) can be expressed as the average of the SNR values of short segments (15 to 20 ms). It can be expressed as follows:\n\nwhere N and M are the segment length and the number of segments, respectively. SSNR tends to provide better results than SNR for waveform encoders and generally SSNR results are poor on vocoders.\n\nOther objective measurements include MCD that evaluates the distance between the target and converted Mel-cepstral coefficients (MCEP) sequences. Also, MSD assesses the local structural differences by calculating the root mean square error between the target and converted logarithmic modulation spectra of MCEPs averaged over all MCEP dimensions and modulation frequencies. For both metrics, smaller values indicate higher distortion between the real and converted speech. It is important to highlight that some of the successful methods like GANSynth  (Engel et al., 2019)  are not mentioned in this paper as it focuses on the musical note synthesis.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Audio-Visual Emotion Synthesis",
      "text": "Although GAN models have an impressive performance on single-domain and cross-domain generation, yet they did not achieve much success in cross-modal generation due to the lack of a common distribution between heterogeneous data. In a cross-domain generation, one generates data samples of various styles from the same modality. As a result, the generated sample and its original counterpart have a common shape structure. However, in a cross-modal generation, the pair of samples have heterogeneous features with quite different distributions.\n\nIn this section, we investigate the cross-modal research line where audio and video provide applications like talking heads, audio-video synchronization, facial animations, and visualizing the face of an unseen subject from their voice. Note that other modalities like text  (Reed et al., 2016; Gu et al., 2018; Stanton et al., 2018)  and biological signals  (Palazzo et al., 2017)  are used in combination with audio and video. However, those modalities are beyond the scope of this review paper.  Ephrat and Peleg (2017)  proposed the Vid2Speech model that uses neighboring video frames to generate sound features for each frame. Then, speech waveforms are synthesized from the learned speech features. In 2017, the authors designed a two-tower CNN  (Ephrat et al., 2017)  framework that reconstructs a naturalsounding speech signal from silent video frames of the speaking person. Their model shows that using one modality to generate samples of another modality is indeed useful because it provides the possibility of natural supervision which means segmentation of the recorded video frames and the recorded sound is not required. The two-tower CNN relies on improving the performance of a Residual neural network (ResNet) that is used as an encoder and redesigning a CNN-based decoder.  One of the foremost cross-modal GAN-based models is proposed by  Chen et al. (2017) . They explored the performance of cGANs by using various audio-visual encodings on generating sound/player of a musical instrument from the pose of the player or the sound of the instrument. This model is not tested on any emotional database and hence, it is not listed in Table  13 . Another leading and interesting research work is conducted by  Suwajanakorn et al. (2017) . An RNN is trained on weekly audio footage of President Barack Obama to map raw audio features to mouth shapes. In the end, a high-quality video with accurate lip synchronization is synthesized. The model can control fine-details like lip texture and mouth-pose.\n\nSpeech-driven video synthesis is the next application. The X2Face model proposed by  Wiles et al. 2018  uses a facial photo or another modality sample (e.g audio) to modify the pose and expression of a given face for video/image editing. They train the model in a self-supervised fashion by receiving two samples: a source sample (video) and a driving sample (video, audio or, a combination). The generated sample inherits the same identity and style (e.g hairstyle) from the source sample and gets the pose, expression of the driving sample. The authors employed an embedding network that factorizes the face representation of the source sample and applies face frontalization. Unfortunately, the authors reported only the visual generated samples and, no further metric is used as of comparison. Another noteworthy work in speech-driven video synthesis is the one presented by  Vougioukas et al. (2018) . They suggested an E2E temporal GAN that captures the facial dynamics and generates synchronized mouth movements and fine-detailed facial expressions, such as eyebrow raises, frowns, and blinks. The authors paired still images of a person with an audio speech to generate subject independent realistic videos. They use raw speech signal as the audio input. The model includes one generator comprising an RNN-based audio encoder, an identity image encoder, a frame decoder, and a noise generator. Also, there exist two discriminators: frame discriminator that simply classifies the frames into real and fake, and sequence discriminator distinguishing real videos from the fake ones. Evaluating the generated samples in frame-level and video-level helps to generate high-quality frames while the video remains synchronized with audio. In 2019,  Vougioukas et al. modified     Duarte et al. (2019)  proposed Wav2Pix model generating the facial image of a speaker without prior knowledge about the face identity. This is done by conditioning on the raw speech signal of that person. The model uses Least-Square GAN and SEAGAN to preserve the identity of the speaker half of the time. The generated images by this model are of low quality (See Figure  10 ). Also, the model is sensitive to several factors like the dimensionality and quality of the training images and the duration of the speech chunk.\n\nLikewise,  (Jamaludin et al., 2019)  in \"You said that? : Synthesising Talking Faces from Audio\" designed the Speech2Vid model that gets the still images of the target face and an audio speech segment as input. The model synthesizes a video of the target face that has a synchronized lip with the speech signal. This model consists of a VGG-M as an audio encoder, a VGG-Face as an identity image encoder, and a VGG-M in reverse order as a talking face image decoder.\n\nHere instead of raw speech data, the audio encoder uses MFCC heatmap images. The network is trained with a usual adversarial loss between the generated image and the ground truth and a content representation loss. One of the most important models developed in the cross-modal community is the SyncGAN model  (Chen et al., 2018)  capable of successfully generating synchronous data. A common problem of the aforementioned cross-modal GAN models is that they are one-directional because they learn the transfer between different modalities. This means they cannot generate a pair of synchronous data from both generating an intelligible speech signal from silent video of a speaking person Table  17  List of purposes and characteristics used for cross-modal synthesis by reviewed publications modalities simultaneously. SyncGAN addresses this problem by learning in a bidirectional mode and from synchronous latent space representing the cross-modal data. In addition to the general generator and discriminator of the vanilla GAN, the model uses a synchronizer network for estimating the probability that two input data are from the same concept. This network is trained using synchronous and asynchronous data samples to maximize the following loss function:\n\nSimilarly, CMCGAN  (Hao et al., 2018)  is a cross-modal CycleGAN that handles generating mutual generation of cross-modal audio-visual videos. Given an image/sound sample from a musical instrument outputs a sound LMS or an image of the player. Unfortunately, neither SyncGAN nor CMCGAN are tested on any multi-modal emotional database.\n\nIn Figure  10  we compared the generated samples of the reviewed publications qualitatively.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Discussion",
      "text": "In this section, we discuss the concepts that are yet not explored thoroughly about GAN-based emotion synthesis within the literature. Also, despite the active development of GAN models, there exist open research problems like mode collapse, convergence failure, and vanishing gradients. Following we discuss these problems. Also, the evolution of GAN models is shown in Figure  11 .",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Disadvantages",
      "text": "The most important drawback of GAN models is mode drop or mode collapse. Mode collapse occurs when a generator learns to generate a limited variety of samples out of the many modes available in the training data.  Roth et al. (2017)  attempted to solve the mode collapse problem by stabilizing the training procedure using regularization. Numerical analysis of general algorithms for training GAN showed that not all training methods actually converge  (Mescheder et al., 2018)  which leads to mode collapse problem. Several objective functions  (Berthelot et al. ,  2017) and structures  (Ghosh et al., 2018)  are developed to tackle this problem, however, none have solved the problem thoroughly. GANs also suffer from convergence failure. Convergence failure happens when the model parameters oscillate and they cannot stabilize during training. In the minimax game, convergence occurs when the discriminator and the generator reach the optimal point under Nash equilibrium theorem. Nash equilibrium is defined as the situation where one player will not change action irrespective of opponent action.\n\nIt is known that if the discriminator performs too accurately, the generator fails due to the vanishing gradient. In fact, the discriminator does not leak/provide enough information for the generator to continue the learning process.",
      "page_start": 36,
      "page_end": 37
    },
    {
      "section_name": "Open Research Problems",
      "text": "In addition to the theoretical problems mentioned in section 4.1, GANs have taskbased limitations. For instance, GANs cannot synthesize discrete data like onehot coded vectors. Although this problem is addressed partially in some research works  (Kusner and Hernández-Lobato, 2016; Jang et al., 2016; Maddison et al., 2016) , yet it needs more attention to unlock the full potential of GAN models. A series of novel divergence algorithms like FisherGAN  (Mroueh and Sercu, 2017)  and the model proposed by  Mroueh et al. (2017)  try to improve the convergence for training GANs. This area deserves more exploration by studying families of integral probability metrics.\n\nThe objective of a GAN model is to generate new samples that come from the same distribution as the training data. However, they do not generate the distribution that generated the training examples. As a result, they don't have a prior likelihood or a well-defined posterior. The question here is how can one estimate the uncertainty of a well-trained generator.\n\nConsidering the emotion synthesis domain, some problems are studied partially. First of all, data augmentation is not fully explored. At the time of writing this paper, there is no large scale image database generated artificially by using GAN models and released for public usage. Such a database could be compared in terms of classification accuracy and quality with the existing databases. Although methods like GANimation and StarGAN successfully generate all sorts of facial expressions, yet generating a fully labeled database requires further processing. For example, the synthesized samples should be annotated and tested against a ground truth like facial Action Units (AU) to confirm that the generated samples carry the predefined standards of a specific emotional class. This issue becomes very complicated when one deals with compound emotions and not only the basic discrete emotions. Also, generated samples are not evaluated within continuous space considering the arousal, valence, and dominance properties of the emotional state. Finally, despite the fact that some successful GAN models are proposed for video generation, the results are not realistic.\n\nIn the case of speech emotion synthesizing, majority of papers focused on raw speech and spectrgorams. As a result, feature-based synthesis is less explored. Human-likeliness of the generated speech samples is another open discussion in this research direction. Furthermore, evaluation metrics in this field is less developed and merely the ones from the traditional speech processing are used on the generated results. Research works that are focusing on cross-modal emotion generation do not exceed from few publications. This research direction requires both developing new ideas and improving the result of previous models.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Applications",
      "text": "One important application of GAN models in the computer vision society includes synthesizing Super Resolution (SR) or photo-realistic images. For example, SR-GAN  (Ledig et al., 2017)  and Enhanced SRGAN  (Wang et al., 2018b)  are generating photo-realistic natural images for an upscaling factor. Considering the facial synthesis, these applications include manipulation of facial pose using DRGAN  (Tran et al., 2017)  and TPGAN  (Huang et al., 2017a) , generating a facial portrait  (Yi et al., 2019a) , generating face of an artificial subject or manipulating the facial attributes of a specific subject  Radford et al. (2015) ,  (Choi et al., 2018) , and synthesizing/manipulating fine detail facial features like skin, lip or teeth texture  (Suwajanakorn et al., 2017) . Generally speaking, the application of GAN considering the visual modality could be categorized to texture synthesis, image super-resolution, image inpainting, face aging, face frontalization, human image synthesis, image-to-image translation, text-to-image, sketch-to-image, image editing, and video generation. Some specific applications with respect to the emotional video generation include the synthesizing of talking heads  (Tulyakov et al., 2018) ,  (Pumarola et al., 2018) .\n\nIn the case of speech emotion synthesis, as mentioned before these applications can be categorize to speech enhancement, data augmentation, and voice conversion. Other research directions like feature learning, imitation learning, and reinforcement learning are important research directions for the near future.",
      "page_start": 38,
      "page_end": 39
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we survey the state-of-the-art proposed in human emotion synthesis using GAN models. GAN models proposed first in 2014 by  Goodfellow et al. . The core idea of GANs is based on a zero-sum game in game theory. Generally, a GAN model consists of a generator and a discriminator, which are trained iteratively in an adversarial learning manner, approaching Nash equilibrium. Instead of estimating the distribution of real data samples, GANs learn to synthesize samples that adapt to the distribution of real data samples. Fields like computer vision, speech processing, and natural language processing benefit from the ability of GAN in generating infinite new samples from potential distributions.",
      "page_start": 39,
      "page_end": 39
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Percentage of publications on GAN models since 2014 based on various research areas",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) illustrates the general",
      "page": 4
    },
    {
      "caption": "Figure 2: General Structure of vanilla GAN and CGAN models; Z: input noise, G: Generator,",
      "page": 5
    },
    {
      "caption": "Figure 2: (b) shows the block diagram of the CGAN",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates the general block",
      "page": 6
    },
    {
      "caption": "Figure 3: Block diagram of LAPGAN model; Gk: k-th Generator, Dk: k-th Discriminator,",
      "page": 6
    },
    {
      "caption": "Figure 4: illustrates the block diagram of GAN models",
      "page": 8
    },
    {
      "caption": "Figure 4: (c). The discriminator performs real/fake",
      "page": 8
    },
    {
      "caption": "Figure 4: (d) shows the structure of",
      "page": 8
    },
    {
      "caption": "Figure 4: General Structure of GAN models varying by discriminator; Z: input noise, G:",
      "page": 9
    },
    {
      "caption": "Figure 5: (a). Here, the discriminator performs real/fake prediction by distinguish-",
      "page": 9
    },
    {
      "caption": "Figure 5: General Structure of UNIT GAN model; Z: input noise, A: domain A, B: domain",
      "page": 10
    },
    {
      "caption": "Figure 6: (c) illustrates the layout for UNIT framework. It is",
      "page": 10
    },
    {
      "caption": "Figure 6: General Structure of GAN models varying by generator; Z: input noise, A: domain",
      "page": 11
    },
    {
      "caption": "Figure 7: Visual comparison of the GAN models, images are in courtesy of the reviewed papers",
      "page": 17
    },
    {
      "caption": "Figure 8: Visual comparison of the GAN models, images are in courtesy of the reviewed papers",
      "page": 18
    },
    {
      "caption": "Figure 9: Evaluation metrics of the generative models are diﬀerent from one research to",
      "page": 20
    },
    {
      "caption": "Figure 9: Visual comparison of the GAN models, images are in courtesy of the reviewed papers",
      "page": 20
    },
    {
      "caption": "Figure 10: ). Also, the model",
      "page": 35
    },
    {
      "caption": "Figure 10: we compared the generated samples of the reviewed publications",
      "page": 36
    },
    {
      "caption": "Figure 11: 4.1 Disadvantages",
      "page": 36
    },
    {
      "caption": "Figure 10: Visual comparison of the cross-modal GAN models, images are in courtesy of the",
      "page": 37
    },
    {
      "caption": "Figure 11: Evolution of GAN models, the horizontal line shows the release year of the model,",
      "page": 37
    }
  ],
  "tables": [
    {
      "caption": "Table 12: DescriptionofSIG,BAK,andOVRLscalesusedinthesubjectivelisteningtests",
      "data": [
        {
          "SGI": "very unnatural/degraded",
          "BAK": "very conspicuous/intrusive",
          "OVRL": "bad"
        },
        {
          "SGI": "fairly unnatural/degraded",
          "BAK": "fairly conspicuous, somewhat intrusive",
          "OVRL": "poor"
        },
        {
          "SGI": "somewhat natural/degraded",
          "BAK": "noticeable but not intrusive",
          "OVRL": "regular"
        },
        {
          "SGI": "fairly natural,\nlittle degradation",
          "BAK": "somewhat noticeable",
          "OVRL": "good"
        },
        {
          "SGI": "very natural, no degradation",
          "BAK": "not noticeable",
          "OVRL": "excellent"
        }
      ],
      "page": 32
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey and taxonomy of adversarial neural networks for text-to-image synthesis",
      "authors": [
        "J Agnese",
        "J Herrera",
        "H Tao",
        "X Zhu"
      ],
      "year": "2019",
      "venue": "A survey and taxonomy of adversarial neural networks for text-to-image synthesis"
    },
    {
      "citation_id": "2",
      "title": "The mug facial expression database",
      "authors": [
        "N Aifanti",
        "C Papachristou",
        "A Delopoulos"
      ],
      "year": "2010",
      "venue": "11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "Recycle-GAN: Unsupervised video retargeting",
      "authors": [
        "A Bansal",
        "S Ma",
        "D Ramanan",
        "Y Sheikh"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "5",
      "title": "BEGAN: Boundary equilibrium generative adversarial networks",
      "authors": [
        "D Berthelot",
        "T Schumm",
        "L Metz"
      ],
      "year": "2017",
      "venue": "BEGAN: Boundary equilibrium generative adversarial networks"
    },
    {
      "citation_id": "6",
      "title": "Pros and cons of GAN evaluation measures",
      "authors": [
        "A Borji"
      ],
      "year": "2019",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "7",
      "title": "Exprada: Adversarial domain adaptation for facial expression analysis",
      "authors": [
        "B Bozorgtabar",
        "D Mahapatra",
        "J Thiran"
      ],
      "year": "2020",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "9",
      "title": "Attacking speaker recognition with deep generative models",
      "authors": [
        "W Cai",
        "A Doshi",
        "R Valle"
      ],
      "year": "2018",
      "venue": "Attacking speaker recognition with deep generative models"
    },
    {
      "citation_id": "10",
      "title": "Data augmentation using GANs for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Proc Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Deep cross-modal audio-visual generation",
      "authors": [
        "L Chen",
        "S Srivastava",
        "Z Duan",
        "C Xu"
      ],
      "year": "2017",
      "venue": "Proceedings of the on Thematic Workshops of ACM Multimedia"
    },
    {
      "citation_id": "12",
      "title": "SyncGAN: Synchronize the latent spaces of cross-modal generative adversarial networks",
      "authors": [
        "W Chen",
        "C Chen",
        "M Hu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Multimedia and Expo (ICME"
    },
    {
      "citation_id": "13",
      "title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
      "authors": [
        "X Chen",
        "Y Duan",
        "R Houthooft",
        "J Schulman",
        "I Sutskever",
        "P Abbeel"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Mocycle-GAN: Unpaired video-to-video translation",
      "authors": [
        "Y Chen",
        "Y Pan",
        "T Yao",
        "X Tian",
        "T Mei"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "4dfab: A large scale 4d database for facial expression analysis and biometric applications",
      "authors": [
        "S Cheng",
        "I Kotsia",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "MeshGAN: Non-linear 3d morphable models of faces",
      "authors": [
        "S Cheng",
        "M Bronstein",
        "Y Zhou",
        "I Kotsia",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "MeshGAN: Non-linear 3d morphable models of faces"
    },
    {
      "citation_id": "17",
      "title": "Discovering hidden factors of variation in deep networks",
      "authors": [
        "B Cheung",
        "J Livezey",
        "A Bansal",
        "B Olshausen"
      ],
      "year": "2014",
      "venue": "Discovering hidden factors of variation in deep networks"
    },
    {
      "citation_id": "18",
      "title": "StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "An audio-visual corpus for speech perception and automatic speech recognition",
      "authors": [
        "M Cooke",
        "J Barker",
        "S Cunningham",
        "X Shao"
      ],
      "year": "2006",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "20",
      "title": "Generative adversarial networks: An overview",
      "authors": [
        "A Creswell",
        "T White",
        "V Dumoulin",
        "K Arulkumaran",
        "B Sengupta",
        "A Bharath"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "21",
      "title": "UV-GAN: Adversarial facial uv map completion for pose-invariant face recognition",
      "authors": [
        "J Deng",
        "S Cheng",
        "N Xue",
        "Y Zhou",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
      "authors": [
        "E Denton",
        "S Chintala",
        "R Fergus"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops"
    },
    {
      "citation_id": "24",
      "title": "ExprGAN: Facial expression editing with controllable expression intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2016",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Wav2pix: speech-conditioned face generation using generative adversarial networks",
      "authors": [
        "A Duarte",
        "F Roldan",
        "M Tubau",
        "J Escur",
        "S Pascual",
        "A Salvador",
        "E Mohedano",
        "K Mcguinness",
        "J Torres",
        "Giro-I Nieto"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "Adversarially learned inference",
      "authors": [
        "V Dumoulin",
        "I Belghazi",
        "B Poole",
        "O Mastropietro",
        "A Lamb",
        "M Arjovsky",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Adversarially learned inference"
    },
    {
      "citation_id": "27",
      "title": "Facial Action Coding System",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Consulting Psychologists Press"
    },
    {
      "citation_id": "28",
      "title": "Smiles when lying",
      "authors": [
        "P Ekman",
        "W Friesen",
        "O 'sullivan"
      ],
      "year": "1988",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "29",
      "title": "Bringing portraits to life",
      "authors": [
        "H Elor",
        "D Cohen-Or",
        "J Kopf",
        "M Cohen"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia"
    },
    {
      "citation_id": "30",
      "title": "GANSynth: Adversarial neural audio synthesis",
      "authors": [
        "J Engel",
        "K Agrawal",
        "S Chen",
        "I Gulrajani",
        "C Donahue",
        "A Roberts"
      ],
      "year": "2019",
      "venue": "GANSynth: Adversarial neural audio synthesis"
    },
    {
      "citation_id": "31",
      "title": "Vid2speech: speech reconstruction from silent video",
      "authors": [
        "A Ephrat",
        "S Peleg"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Improved speech reconstruction from silent video",
      "authors": [
        "A Ephrat",
        "T Halperin",
        "S Peleg"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "33",
      "title": "Nonparallel emotional speech conversion",
      "authors": [
        "J Gao",
        "D Chakraborty",
        "H Tembine",
        "O Olaleye"
      ],
      "year": "2018",
      "venue": "Nonparallel emotional speech conversion"
    },
    {
      "citation_id": "34",
      "title": "Warp-guided GANs for single-photo facial animation",
      "authors": [
        "J Geng",
        "T Shao",
        "Y Zheng",
        "Y Weng",
        "K Zhou"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "35",
      "title": "Multi-Agent diverse generative adversarial networks",
      "authors": [
        "A Ghosh",
        "V Kulharia",
        "V Namboodiri",
        "P Torr",
        "P Dokania"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "36",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "Warde Farley",
        "D Ozair",
        "S Courville",
        "A Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A ; Courville",
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "39",
      "title": "Multi-pie",
      "authors": [
        "R Gross",
        "I Matthews",
        "J Cohn",
        "T Kanade",
        "S Baker"
      ],
      "year": "2010",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "40",
      "title": "Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models",
      "authors": [
        "J Gu",
        "J Cai",
        "S Joty",
        "L Niu",
        "G Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "A review on generative adversarial networks: Algorithms, theory, and applications",
      "authors": [
        "J Gui",
        "Z Sun",
        "Y Wen",
        "D Tao",
        "J Ye"
      ],
      "year": "2020",
      "venue": "A review on generative adversarial networks: Algorithms, theory, and applications"
    },
    {
      "citation_id": "42",
      "title": "Improved training of wasserstein GANs",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "CMCGAN: A uniform framework for cross-modal visual-audio mutual generation",
      "authors": [
        "W Hao",
        "Z Zhang",
        "H ; Guan",
        "W Zuo",
        "M Kan",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence He Z"
    },
    {
      "citation_id": "44",
      "title": "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions",
      "authors": [
        "H Hirsch",
        "D Pearce"
      ],
      "year": "2000",
      "venue": "ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop"
    },
    {
      "citation_id": "45",
      "title": "Comparative study on generative adversarial networks",
      "authors": [
        "S Hitawala"
      ],
      "year": "2018",
      "venue": "Comparative study on generative adversarial networks"
    },
    {
      "citation_id": "46",
      "title": "How generative adversarial networks and their variants work: An overview",
      "authors": [
        "Y Hong",
        "U Hwang",
        "J Yoo",
        "S Yoon"
      ],
      "year": "2019",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "47",
      "title": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
      "authors": [
        "C Hsu",
        "H Hwang",
        "Y Wu",
        "Y Tsao",
        "H Wang"
      ],
      "year": "2017",
      "venue": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks"
    },
    {
      "citation_id": "48",
      "title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments",
      "authors": [
        "G Huang",
        "M Mattar",
        "T Berg",
        "E Learned-Miller"
      ],
      "year": "2008",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "49",
      "title": "An introduction to image synthesis with generative adversarial nets",
      "authors": [
        "H Huang",
        "P Yu",
        "C Wang"
      ],
      "year": "2018",
      "venue": "An introduction to image synthesis with generative adversarial nets"
    },
    {
      "citation_id": "50",
      "title": "Beyond face rotation: Global and local perception GAN for photorealistic and identity preserving frontal view synthesis",
      "authors": [
        "R Huang",
        "S Zhang",
        "T Li",
        "R He"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "51",
      "title": "Stacked generative adversarial networks",
      "authors": [
        "X Huang",
        "Y Li",
        "O Poursaeed",
        "J Hopcroft",
        "S Belongie"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "52",
      "title": "DyadGAN: Generating facial expressions in dyadic interactions",
      "authors": [
        "Y Huang",
        "S Khan"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "53",
      "title": "Generating images with recurrent adversarial networks",
      "authors": [
        "D Im",
        "C Kim",
        "H Jiang",
        "R Memisevic"
      ],
      "year": "2016",
      "venue": "Generating images with recurrent adversarial networks"
    },
    {
      "citation_id": "54",
      "title": "Image-to-image translation with conditional adversarial networks",
      "authors": [
        "P Isola",
        "J Zhu",
        "T Zhou",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "55",
      "title": "CapsuleGAN: Generative adversarial capsule network",
      "authors": [
        "K Ito"
      ],
      "year": "2017",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "56",
      "title": "You said that?: Synthesising talking faces from audio",
      "authors": [
        "A Jamaludin",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "57",
      "title": "Categorical reparameterization with gumbel-softmax",
      "authors": [
        "E Jang",
        "S Gu",
        "B Poole"
      ],
      "year": "2016",
      "venue": "Categorical reparameterization with gumbel-softmax"
    },
    {
      "citation_id": "58",
      "title": "ET-GAN: Cross-language emotion transfer based on cycle-consistent generative adversarial networks",
      "authors": [
        "X Jia",
        "J Tai",
        "H Zhou",
        "Y Li",
        "W Zhang",
        "H Du",
        "Q Huang"
      ],
      "year": "2019",
      "venue": "ET-GAN: Cross-language emotion transfer based on cycle-consistent generative adversarial networks"
    },
    {
      "citation_id": "59",
      "title": "Speech waveform synthesis from mfcc sequences with generative adversarial networks",
      "authors": [
        "L Juvela",
        "B Bollepalli",
        "X Wang",
        "H Kameoka",
        "M Airaksinen",
        "J Yamagishi",
        "P Alku"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "a) Acvae-vc: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder",
      "authors": [
        "H Kameoka",
        "T Kaneko",
        "K Tanaka",
        "N Hojo"
      ],
      "year": "2018",
      "venue": "a) Acvae-vc: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder"
    },
    {
      "citation_id": "61",
      "title": "StarGAN-VC: Non-parallel many-to-many voice conversion using star generative adversarial networks",
      "authors": [
        "H Kameoka",
        "T Kaneko",
        "K Tanaka",
        "N Hojo"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "62",
      "title": "Convs2s-vc: Fully convolutional sequence-tosequence voice conversion",
      "authors": [
        "H Kameoka",
        "K Tanaka",
        "T Kaneko",
        "Hojo"
      ],
      "year": "2018",
      "venue": "Convs2s-vc: Fully convolutional sequence-tosequence voice conversion"
    },
    {
      "citation_id": "63",
      "title": "CycleGAN-VC: Non-parallel voice conversion using cycle-consistent adversarial networks",
      "authors": [
        "T Kaneko",
        "H Kameoka"
      ],
      "year": "2018",
      "venue": "2018 26th European Signal Processing Conference"
    },
    {
      "citation_id": "64",
      "title": "CycleGAN-VC2: Improved CycleGAN-based nonparallel voice conversion",
      "authors": [
        "T Kaneko",
        "H Kameoka",
        "K Tanaka",
        "N Hojo"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "Audio-driven facial animation by joint end-to-end learning of pose and emotion",
      "authors": [
        "T Karras",
        "Aila Laine",
        "S Herva",
        "A Lehtinen"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "66",
      "title": "Progressive growing of GANs for improved quality, stability, and variation",
      "authors": [
        "T Karras",
        "Aila Laine",
        "S Lehtinen"
      ],
      "year": "2017",
      "venue": "Progressive growing of GANs for improved quality, stability, and variation"
    },
    {
      "citation_id": "67",
      "title": "Cake: Compact and accurate kdimensional representation of emotion",
      "authors": [
        "C Kervadec",
        "V Vielzeuf",
        "S Pateux",
        "A Lechervy",
        "F Jurie"
      ],
      "year": "2018",
      "venue": "Cake: Compact and accurate kdimensional representation of emotion"
    },
    {
      "citation_id": "68",
      "title": "Deep video portraits",
      "authors": [
        "H Kim",
        "P Garrido",
        "A Tewari",
        "W Xu",
        "J Thies",
        "M Nießner",
        "P Pérez",
        "C Richardt",
        "M Zollhöfer",
        "C Theobalt"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "69",
      "title": "Learning to discover cross-domain relations with generative adversarial networks",
      "authors": [
        "T Kim",
        "M Cha",
        "H Kim",
        "J Lee",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "70",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes"
    },
    {
      "citation_id": "71",
      "title": "The cmu arctic speech databases",
      "authors": [
        "J Kominek",
        "A Black"
      ],
      "year": "2004",
      "venue": "Fifth ISCA workshop on speech synthesis"
    },
    {
      "citation_id": "72",
      "title": "Multiscale fusion of visible and thermal ir images for illumination-invariant face recognition",
      "authors": [
        "S Kong",
        "J Heo",
        "F Boughorbel",
        "Y Zheng",
        "B Abidi",
        "A Koschan",
        "M Yi",
        "M Abidi"
      ],
      "year": "2007",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "73",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "74",
      "title": "The GAN landscape: Losses, architectures, regularization, and normalization",
      "authors": [
        "K Kurach",
        "M Lucic",
        "X Zhai",
        "M Michalski",
        "S Gelly"
      ],
      "year": "2018",
      "venue": "The GAN landscape: Losses, architectures, regularization, and normalization"
    },
    {
      "citation_id": "75",
      "title": "GANs for sequences of discrete elements with the gumbelsoftmax distribution",
      "authors": [
        "M Kusner",
        "J Hernández-Lobato"
      ],
      "year": "2016",
      "venue": "GANs for sequences of discrete elements with the gumbelsoftmax distribution"
    },
    {
      "citation_id": "76",
      "title": "Emotion-preserving representation learning via generative adversarial network for multi-view facial expression recognition",
      "authors": [
        "Y Lai",
        "S Lai"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "77",
      "title": "Presentation and validation of the radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "78",
      "title": "Autoencoding beyond pixels using a learned similarity metric",
      "authors": [
        "Abl Larsen",
        "S Sønderby",
        "H Larochelle",
        "O Winther"
      ],
      "year": "2015",
      "venue": "Autoencoding beyond pixels using a learned similarity metric"
    },
    {
      "citation_id": "79",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study"
    },
    {
      "citation_id": "80",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness"
    },
    {
      "citation_id": "81",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "82",
      "title": "Photo-realistic single image super-resolution using a generative adversarial network",
      "authors": [
        "C Ledig",
        "L Theis",
        "F Huszár",
        "J Caballero",
        "A Cunningham",
        "A Acosta",
        "A Aitken",
        "A Tejani",
        "J Totz",
        "Z Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "83",
      "title": "CollaGAN: Collaborative GAN for missing image data imputation",
      "authors": [
        "D Lee",
        "J Kim",
        "W Moon",
        "J Ye"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "84",
      "title": "Metapix: Few-shot video retargeting",
      "authors": [
        "J Lee",
        "D Ramanan",
        "R Girdhar"
      ],
      "year": "2019",
      "venue": "Metapix: Few-shot video retargeting"
    },
    {
      "citation_id": "85",
      "title": "Deep identity-aware transfer of facial attributes",
      "authors": [
        "M Li",
        "W Zuo",
        "D Zhang"
      ],
      "year": "2016",
      "venue": "Deep identity-aware transfer of facial attributes"
    },
    {
      "citation_id": "86",
      "title": "Generative face completion",
      "authors": [
        "Y Li",
        "S Liu",
        "J Yang",
        "M Yang"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "87",
      "title": "Facial expression editing with continuous emotion labels",
      "authors": [
        "A Lindt",
        "P Barros",
        "H Siqueira",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "88",
      "title": "Deformable shape completion with graph convolutional autoencoders",
      "authors": [
        "O Litany",
        "A Bronstein",
        "M Bronstein",
        "A Makadia"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "89",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "90",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "91",
      "title": "Can we steal your vocal identity from the internet?: Initial investigation of cloning obama's voice using GAN, wavenet and low-quality found data",
      "authors": [
        "J Lorenzo-Trueba",
        "F Fang",
        "X Wang",
        "I Echizen",
        "J Yamagishi",
        "T Kinnunen"
      ],
      "year": "2018",
      "venue": "Can we steal your vocal identity from the internet?: Initial investigation of cloning obama's voice using GAN, wavenet and low-quality found data"
    },
    {
      "citation_id": "92",
      "title": "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods",
      "authors": [
        "J Lorenzo-Trueba",
        "J Yamagishi",
        "T Toda",
        "D Saito",
        "F Villavicencio",
        "T Kinnunen",
        "Z Ling"
      ],
      "year": "2018",
      "venue": "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods"
    },
    {
      "citation_id": "93",
      "title": "Attribute-guided face generation using conditional CycleGAN",
      "authors": [
        "Y Lu",
        "Y Tai",
        "C Tang"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "94",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "95",
      "title": "CD ROM from Department of Clinical Neuroscience, Psychology section",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "Karolinska Institutet"
    },
    {
      "citation_id": "96",
      "title": "The japanese female facial expression (jaffe) database",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba",
        "J Budynek"
      ],
      "year": "1998",
      "venue": "Proceedings of third international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "97",
      "title": "Improved speech enhancement with the wave-u-net",
      "authors": [
        "C Macartney",
        "T Weyde"
      ],
      "year": "2018",
      "venue": "Improved speech enhancement with the wave-u-net"
    },
    {
      "citation_id": "98",
      "title": "The concrete distribution: A continuous relaxation of discrete random variables",
      "authors": [
        "C Maddison",
        "A Mnih",
        "Y Teh"
      ],
      "year": "2016",
      "venue": "The concrete distribution: A continuous relaxation of discrete random variables"
    },
    {
      "citation_id": "99",
      "title": "Least squares generative adversarial networks",
      "authors": [
        "X Mao",
        "Q Li",
        "H Xie",
        "R Lau",
        "Z Wang",
        "Paul Smolley"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "100",
      "title": "BAGAN: Data augmentation with balancing gan",
      "authors": [
        "G Mariani",
        "F Scheidegger",
        "R Istrate",
        "C Bekas",
        "C Malossi"
      ],
      "year": "2018",
      "venue": "BAGAN: Data augmentation with balancing gan"
    },
    {
      "citation_id": "101",
      "title": "Mic2mic: using cycle-consistent generative adversarial networks to overcome microphone variability in speech systems",
      "authors": [
        "A Mathur",
        "A Isopoussu",
        "F Kawsar",
        "N Berthouze",
        "N Lane"
      ],
      "year": "2019",
      "venue": "Proceedings of the 18th International Conference on Information Processing in Sensor Networks"
    },
    {
      "citation_id": "102",
      "title": "Inference of attitudes from nonverbal communication in two channels",
      "authors": [
        "A Mehrabian",
        "S Ferris"
      ],
      "year": "1967",
      "venue": "Journal of consulting psychology"
    },
    {
      "citation_id": "103",
      "title": "",
      "authors": [
        "L Mescheder",
        "A Geiger",
        "S Nowozin"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "104",
      "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
      "authors": [
        "F Milletari",
        "N Navab",
        "S Ahmadi"
      ],
      "year": "2016",
      "venue": "2016 Fourth International Conference on 3D Vision (3DV"
    },
    {
      "citation_id": "105",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets"
    },
    {
      "citation_id": "106",
      "title": "cGANs with projection discriminator",
      "authors": [
        "T Miyato",
        "M Koyama"
      ],
      "year": "2018",
      "venue": "cGANs with projection discriminator"
    },
    {
      "citation_id": "107",
      "title": "Spectral normalization for generative adversarial networks",
      "authors": [
        "T Miyato",
        "T Kataoka",
        "M Koyama",
        "Y Yoshida"
      ],
      "year": "2018",
      "venue": "Spectral normalization for generative adversarial networks"
    },
    {
      "citation_id": "108",
      "title": "Visio-lization: generating novel facial images",
      "authors": [
        "U Mohammed",
        "S Prince",
        "J Kautz"
      ],
      "year": "2009",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "109",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "110",
      "title": "Advances in Neural Information Processing Systems",
      "authors": [
        "Y Mroueh",
        "T ; Sercu",
        "Gan Fisher"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "111",
      "title": "",
      "authors": [
        "Y Mroueh",
        "C Li",
        "T Sercu",
        "A Raj",
        "Y Cheng"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "112",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: a large-scale speaker identification dataset"
    },
    {
      "citation_id": "113",
      "title": "DCVGAN: Depth conditional video generation",
      "authors": [
        "Y Nakahira",
        "K Kawamoto"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "114",
      "title": "CIGAN: A novel GANs model based on category information",
      "authors": [
        "J Niu",
        "Z Li",
        "S Mo"
      ],
      "year": "2018",
      "venue": "2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation"
    },
    {
      "citation_id": "115",
      "title": "f-GAN: Training generative neural samplers using variational divergence minimization",
      "authors": [
        "S Nowozin",
        "B Cseke",
        "R Tomioka"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "116",
      "title": "Conditional image synthesis with auxiliary classifier gans",
      "authors": [
        "A Odena",
        "C Olah",
        "J Shlens"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "117",
      "title": "Generative adversarial networks conditioned by brain signals",
      "authors": [
        "S Palazzo",
        "C Spampinato",
        "I Kavasidis",
        "D Giordano",
        "M Shah"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "118",
      "title": "Recent progress on generative adversarial networks (GANs): A survey",
      "authors": [
        "Z Pan",
        "W Yu",
        "X Yi",
        "A Khan",
        "F Yuan",
        "Y Zheng"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "119",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "IEEE international conference on multimedia and Expo"
    },
    {
      "citation_id": "120",
      "title": "Semantic image synthesis with spatially-adaptive normalization",
      "authors": [
        "T Park",
        "M Liu",
        "T Wang",
        "J Zhu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "121",
      "title": "SEGAN: Speech enhancement generative adversarial network",
      "authors": [
        "S Pascual",
        "A Bonafonte",
        "J Serra"
      ],
      "year": "2017",
      "venue": "SEGAN: Speech enhancement generative adversarial network"
    },
    {
      "citation_id": "122",
      "title": "ApprGAN: Appearance-based GAN for facial expression synthesis",
      "authors": [
        "Y Peng",
        "H Yin"
      ],
      "year": "2019",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "123",
      "title": "Invertible conditional GANs for image editing",
      "authors": [
        "G Perarnau",
        "J Van De Weijer",
        "B Raducanu",
        "J Álvarez"
      ],
      "year": "2016",
      "venue": "Invertible conditional GANs for image editing"
    },
    {
      "citation_id": "124",
      "title": "GANimation: Anatomically-aware facial animation from a single image",
      "authors": [
        "A Pumarola",
        "A Agudo",
        "A Martinez",
        "A Sanfeliu",
        "F Moreno-Noguer"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "125",
      "title": "Emotional facial expression transfer from a single image via generative adversarial nets",
      "authors": [
        "F Qiao",
        "N Yao",
        "Z Jiao",
        "Z Li",
        "H Chen",
        "H Wang"
      ],
      "year": "2018",
      "venue": "Computer Animation and Virtual Worlds"
    },
    {
      "citation_id": "126",
      "title": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "authors": [
        "C Quiroz",
        "R Srinivasan",
        "Q Feng",
        "Y Wang",
        "A Martinez"
      ],
      "year": "2017",
      "venue": "Emotionet challenge: Recognition of facial expressions of emotion in the wild"
    },
    {
      "citation_id": "127",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks"
    },
    {
      "citation_id": "128",
      "title": "Perceptual evaluation of speech quality (pesq): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs",
      "authors": [
        "A Ranjan",
        "T Bolkart",
        "S Sanyal",
        "M Black"
      ],
      "year": "2001",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "129",
      "title": "Learning to disentangle factors of variation with manifold interaction",
      "authors": [
        "S Reed",
        "K Sohn",
        "Y Zhang",
        "H Lee"
      ],
      "year": "2014",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "130",
      "title": "Generative adversarial text to image synthesis",
      "authors": [
        "S Reed",
        "Z Akata",
        "X Yan",
        "L Logeswaran",
        "B Schiele",
        "H Lee"
      ],
      "year": "2016",
      "venue": "Generative adversarial text to image synthesis"
    },
    {
      "citation_id": "131",
      "title": "Stochastic backpropagation and approximate inference in deep generative models",
      "authors": [
        "D Rezende",
        "S Mohamed",
        "D Wierstra"
      ],
      "year": "2014",
      "venue": "Stochastic backpropagation and approximate inference in deep generative models"
    },
    {
      "citation_id": "132",
      "title": "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs",
      "authors": [
        "A Rix",
        "J Beerends",
        "M Hollier",
        "A Hekstra"
      ],
      "year": "2001",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings"
    },
    {
      "citation_id": "133",
      "title": "Stabilizing training of generative adversarial networks through regularization",
      "authors": [
        "K Roth",
        "A Lucchi",
        "S Nowozin",
        "T Hofmann"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "134",
      "title": "Dynamic routing between capsules",
      "authors": [
        "S Sabour",
        "N Frosst",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "135",
      "title": "On enhancing speech emotion recognition using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2018",
      "venue": "On enhancing speech emotion recognition using generative adversarial networks"
    },
    {
      "citation_id": "136",
      "title": "Temporal generative adversarial nets with singular value clipping",
      "authors": [
        "M Saito",
        "E Matsumoto",
        "S Saito"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "137",
      "title": "Improved techniques for training GANs",
      "authors": [
        "T Salimans",
        "I Goodfellow",
        "W Zaremba",
        "V Cheung",
        "A Radford",
        "X Chen"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "138",
      "title": "Emotion perception from face, voice, and touch: comparisons and convergence",
      "authors": [
        "A Schirmer",
        "R Adolphs"
      ],
      "year": "2017",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "139",
      "title": "Emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "1988",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "140",
      "title": "Tang X (2018a) FaceID-GAN: Learning a symmetry three-player GAN for identity-preserving face synthesis",
      "authors": [
        "Y Shen",
        "P Luo",
        "J Yan",
        "X Wang"
      ],
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "141",
      "title": "FaceFeat-GAN: A two-stage approach for identity-preserving face synthesis",
      "authors": [
        "Y Shen",
        "B Zhou",
        "P Luo",
        "X Tang"
      ],
      "year": "2018",
      "venue": "FaceFeat-GAN: A two-stage approach for identity-preserving face synthesis"
    },
    {
      "citation_id": "142",
      "title": "Data augmentation using conditional generative adversarial networks for robust speech recognition",
      "authors": [
        "P Sheng",
        "Z Yang",
        "H Hu",
        "T Tan",
        "Y Qian"
      ],
      "year": "2018",
      "venue": "11th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "143",
      "title": "Neural face editing with intrinsic image disentangling",
      "authors": [
        "Z Shu",
        "E Yumer",
        "S Hadap",
        "K Sunkavalli",
        "E Shechtman",
        "D Samaras"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "144",
      "title": "Geometry guided adversarial facial expression synthesis",
      "authors": [
        "L Song",
        "Z Lu",
        "R He",
        "Z Sun",
        "T Tan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "145",
      "title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks",
      "authors": [
        "J Springenberg"
      ],
      "year": "2015",
      "venue": "Unsupervised and semi-supervised learning with categorical generative adversarial networks"
    },
    {
      "citation_id": "146",
      "title": "Predicting expressive speaking style from text in endto-end speech synthesis",
      "authors": [
        "D Stanton",
        "Y Wang",
        "R Skerry-Ryan"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "147",
      "title": "Twostreamvan: Improving motion modeling in video generation",
      "authors": [
        "X Sun",
        "H Xu",
        "K Saenko"
      ],
      "year": "2020",
      "venue": "The IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "148",
      "title": "Generating facial expressions with deep belief nets",
      "authors": [
        "J Susskind",
        "G Hinton",
        "J Movellan",
        "A Anderson"
      ],
      "year": "2008",
      "venue": "Affective Computing, Emotion Modelling, Synthesis and Recognition pp"
    },
    {
      "citation_id": "149",
      "title": "Synthesizing obama: learning lip sync from audio",
      "authors": [
        "S Suwajanakorn",
        "S Seitz",
        "I Kemelmacher-Shlizerman"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "150",
      "title": "Synthetic-to-natural speech waveform conversion using cycle-consistent adversarial networks",
      "authors": [
        "K Tanaka",
        "T Kaneko",
        "N Hojo",
        "H Kameoka"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Work-shop (SLT)"
    },
    {
      "citation_id": "151",
      "title": "Atts2s-vc: Sequence-to-sequence voice conversion with attention and context preservation mechanisms",
      "authors": [
        "K Tanaka",
        "H Kameoka",
        "T Kaneko",
        "N Hojo"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "152",
      "title": "Time-domain neural post-filter for speech waveform generation",
      "authors": [
        "K Tanaka",
        "H Kameoka",
        "T Kaneko",
        "N Hojo"
      ],
      "year": "2019",
      "venue": "Time-domain neural post-filter for speech waveform generation"
    },
    {
      "citation_id": "153",
      "title": "The voice conversion challenge 2016. Annual Conference of the International Speech Communication Association",
      "authors": [
        "T Tomoki",
        "L Chen",
        "D Saito",
        "F Villavicencio",
        "M Wester",
        "Z Wu",
        "J Yamagishi"
      ],
      "year": "2016",
      "venue": "The voice conversion challenge 2016. Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "154",
      "title": "Audio enhancement and synthesis using generative adversarial networks: A survey",
      "authors": [
        "N Torres-Reyes",
        "S Latifi"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "155",
      "title": "Disentangled representation learning GAN for pose-invariant face recognition",
      "authors": [
        "L Tran",
        "X Yin",
        "X Liu"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "156",
      "title": "MocoGAN: Decomposing motion and content for video generation",
      "authors": [
        "S Tulyakov",
        "M Liu",
        "X Yang",
        "J Kautz"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "157",
      "title": "Melnet: A generative model for audio in the frequency domain",
      "authors": [
        "S Vasquez",
        "M Lewis"
      ],
      "year": "2019",
      "venue": "Melnet: A generative model for audio in the frequency domain"
    },
    {
      "citation_id": "158",
      "title": "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "S King"
      ],
      "year": "2013",
      "venue": "2013 International Conference Oriental CO-COSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation"
    },
    {
      "citation_id": "159",
      "title": "The many variations of emotion",
      "authors": [
        "V Vielzeuf",
        "C Kervadec",
        "S Pateux",
        "F Jurie"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "160",
      "title": "Generating videos with scene dynamics",
      "authors": [
        "C Vondrick",
        "H Pirsiavash",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "161",
      "title": "End-to-End speech-driven facial animation with temporal GANs",
      "authors": [
        "K Vougioukas",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2018",
      "venue": "End-to-End speech-driven facial animation with temporal GANs"
    },
    {
      "citation_id": "162",
      "title": "Realistic speech-driven facial animation with GANs",
      "authors": [
        "K Vougioukas",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "163",
      "title": "SentiGAN: Generating sentimental texts via mixture adversarial networks",
      "authors": [
        "K Wang",
        "X Wan"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "164",
      "title": "Generative adversarial networks: introduction and outlook",
      "authors": [
        "K Wang",
        "C Gou",
        "Y Duan",
        "Y Lin",
        "X Zheng",
        "F Wang"
      ],
      "year": "2017",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "165",
      "title": "Catanzaro B (2018a) High-resolution image synthesis and semantic manipulation with conditional GANs",
      "authors": [
        "T Wang",
        "M Liu",
        "J Zhu",
        "A Tao",
        "J Kautz"
      ],
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "166",
      "title": "Learning to augment expressions for few-shot fine-grained facial expression recognition",
      "authors": [
        "W Wang",
        "Y Fu",
        "Q Sun",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y Jiang",
        "X Xue"
      ],
      "year": "2020",
      "venue": "Learning to augment expressions for few-shot fine-grained facial expression recognition"
    },
    {
      "citation_id": "167",
      "title": "ESRGAN: Enhanced super-resolution generative adversarial networks",
      "authors": [
        "X Wang",
        "K Yu",
        "S Wu",
        "J Gu",
        "Y Liu",
        "C Dong",
        "Y Qiao",
        "Change Loy"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "168",
      "title": "U-Net conditional GANs for photo-realistic and identity-preserving facial expression synthesis",
      "authors": [
        "X Wang",
        "Y Wang",
        "W Li"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "169",
      "title": "Generative adversarial networks: A survey and taxonomy",
      "authors": [
        "Z Wang",
        "Q She",
        "T Ward"
      ],
      "year": "2019",
      "venue": "Generative adversarial networks: A survey and taxonomy"
    },
    {
      "citation_id": "170",
      "title": "X2face: A network for controlling face generation using images, audio, and pose codes",
      "authors": [
        "O Wiles",
        "Sophia Koepke",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "171",
      "title": "A survey of image synthesis and editing with generative adversarial networks",
      "authors": [
        "X Wu",
        "K Xu",
        "P Hall"
      ],
      "year": "2017",
      "venue": "Tsinghua Science and Technology"
    },
    {
      "citation_id": "172",
      "title": "AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks",
      "authors": [
        "T Xu",
        "P Zhang",
        "Q Huang",
        "H Zhang",
        "Z Gan",
        "X Huang",
        "X He"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "173",
      "title": "Stabilizing adversarial nets with prediction methods",
      "authors": [
        "A Yadav",
        "S Shah",
        "Z Xu",
        "D Jacobs",
        "T Goldstein"
      ],
      "year": "2017",
      "venue": "Stabilizing adversarial nets with prediction methods"
    },
    {
      "citation_id": "174",
      "title": "Pose guided human video generation",
      "authors": [
        "C Yang",
        "Z Wang",
        "X Zhu",
        "C Huang",
        "J Shi",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "175",
      "title": "Expression flow for 3d-aware face component transfer",
      "authors": [
        "F Yang",
        "J Wang",
        "E Shechtman",
        "L Bourdev",
        "D Metaxas"
      ],
      "year": "2011",
      "venue": "ACM SIGGRAPH 2011 papers"
    },
    {
      "citation_id": "176",
      "title": "Semantic facial expression editing using autoencoded flow",
      "authors": [
        "R Yeh",
        "Z Liu",
        "D Goldman",
        "A Agarwala"
      ],
      "year": "2016",
      "venue": "Semantic facial expression editing using autoencoded flow"
    },
    {
      "citation_id": "177",
      "title": "APDrawingGAN: Generating artistic portrait drawings from face photos with hierarchical gans",
      "authors": [
        "R Yi",
        "Y Liu",
        "Y Lai",
        "P Rosin"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "178",
      "title": "Generative adversarial network in medical imaging: A review",
      "authors": [
        "X Yi",
        "E Walia",
        "P Babyn"
      ],
      "year": "2019",
      "venue": "Medical image analysis"
    },
    {
      "citation_id": "179",
      "title": "A 3d facial expression database for facial behavior research",
      "authors": [
        "L Yin",
        "X Wei",
        "Y Sun",
        "J Wang",
        "M Rosato"
      ],
      "year": "2006",
      "venue": "7th international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "180",
      "title": "Generative adversarial networks: recent developments",
      "authors": [
        "M Zamorski",
        "A Zdobylak",
        "M Zieba",
        "J Światek"
      ],
      "year": "2019",
      "venue": "International Conference on Artificial Intelligence and Soft Computing"
    },
    {
      "citation_id": "181",
      "title": "TV-GAN: Generative adversarial network based thermal to visible face recognition",
      "authors": [
        "T Zhang",
        "A Wiliem",
        "S Yang",
        "B Lovell"
      ],
      "year": "2018",
      "venue": "2018 international conference on biometrics (ICB)"
    },
    {
      "citation_id": "182",
      "title": "Facial expression recognition from nearinfrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "183",
      "title": "Energy-based generative adversarial network",
      "authors": [
        "J Zhao",
        "M Mathieu",
        "Y Lecun"
      ],
      "year": "2016",
      "venue": "Energy-based generative adversarial network"
    },
    {
      "citation_id": "184",
      "title": "Mojitalk: Generating emotional responses at scale",
      "authors": [
        "X Zhou",
        "W Wang"
      ],
      "year": "2017",
      "venue": "Mojitalk: Generating emotional responses at scale"
    },
    {
      "citation_id": "185",
      "title": "Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder",
      "authors": [
        "Y Zhou",
        "B Shi"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "186",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "187",
      "title": "Emotion classification with data augmentation using generative adversarial networks",
      "authors": [
        "X Zhu",
        "Y Liu",
        "J Li",
        "T Wan",
        "Z Qin"
      ],
      "year": "2018",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "188",
      "title": "Controlling nonverbal displays: Facial expressions and tone of voice",
      "authors": [
        "M Zuckerman",
        "D Larrance",
        "N Spiegel",
        "R Klorman"
      ],
      "year": "1981",
      "venue": "Journal of Experimental Social Psychology"
    }
  ]
}