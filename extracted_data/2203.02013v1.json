{
  "paper_id": "2203.02013v1",
  "title": "Dime: Fine-Grained Interpretations Of Multimodal Models Via Disentangled Local Explanations",
  "published": "2022-03-03T20:52:47Z",
  "authors": [
    "Yiwei Lyu",
    "Paul Pu Liang",
    "Zihao Deng",
    "Ruslan Salakhutdinov",
    "Louis-Philippe Morency"
  ],
  "keywords": [
    "â€¢ Computing methodologies â†’ Machine learning",
    "Natural language processing",
    "Computer vision",
    "â€¢ Human-centered computing â†’ Visualization multimodal machine learning, interpretability, explainability, visualization, disentangled representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "What is the table made of? A: Glass \n DIME (ours) Unimodal (image) explanation for \"glass\" DIME (ours) Multimodal Interactive explanation for \"glass\" (Without disentangling) LIME explanation for \"glass\" Figure  1 : DIME is a novel method of interpreting multimodal models by disentangling the model into unimodal contributions (UC) and multimodal interactions (MI), before generating visual explanations for each. Here is an example from visualizing a trained LXMERT model on VQA: without disentangling, the explanation highlights both parts of the table and the glass bottle; with disentangling, we can see that the unimodal contributions look at the image without looking at the question and highlights the glass bottle, wine glass, and glass table, all of which could support the answer glass, while the multimodal interaction knows that only the table matters, so it focuses only on the table. Therefore, with DIME, we can be certain that the model identifies the table as the reason for answering \"glass\", and not the bottle instead. In this paper, we show that DIME can accurately perform disentanglement and generate explanations for both UC and MI to help researchers better interpret multimodal models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As machine learning models are increasingly deployed in real-world scenarios, it has motivated the development of interpretable machine learning (ML) as a research field with the goal of understanding Advantages DIME (ours) LIME  [52]  EMAP  [23]  M2Lens  [    [23, 52, 62] , DIME disentangles a multimodal model into unimodal contributions and multimodal interactions and generates accurate explanations for each, while remaining generalizable (not designed specifically for any model, modality, or task) and works for black-box models (does not require knowledge of the internal structure of the model). DIME can help human users determine the dominant factor behind a model's decisions, gain insight into what specific multimodal interactions are captured, and reveal undesirable behavior for debugging or improving the model. (*) EMAP can only sometimes distinguish the dominant factor between UC or MI: according to Hessel and Lee  [23] , EMAP can give insight on individual data points only under special conditions (such as when EMAP happens to flip the prediction).\n\nML models, performing model debugging, and using these insights to better inform the interaction between AI and humans in joint decision making  [7, 12, 18] . Recently, the promise of multimodal models for real-world representation learning in numerous applications such as multimedia  [39, 40, 45] , affective computing  [42, 50] , robotics  [33, 36] , finance  [25] , dialogue  [49] , human-computer interaction  [15, 46] , and healthcare  [64]  has invigorated research into multimodal machine learning, which brings unique challenges for both computational and theoretical research given the heterogeneity of various data sources and difficulty of capturing correspondences between modalities  [4] . Among one of these core challenges is interpretable multimodal learning with the end goal of empowering various stakeholders by providing insights into multimodal learning, improving model design, or debugging models and datasets.\n\nRecent work in interpretable multimodal learning has therefore focused on constructing interpretable multimodal models via careful model design  [48, 59, 66]  or performing post-hoc explanations of black-box multimodal models  [10, 21] . However, existing works typically focus on building interpretable models using suitable inductive biases, such as designing multimodal routing networks  [59] , graph-based fusion  [66] , or multimodal explanation networks to highlight visual importance  [48] . Some of these approaches also require the collection of specialized datasets annotated for visual explanations as intermediate steps in training interpretable models  [48] . On the other hand, with the trend towards large-scale modeling or pre-training as an alternative over individual modality-specific or task-specific models  [37, 40] , it is increasingly important to design general-purpose approaches that (1) are able to generate post-hoc explanations for arbitrary black-box models, and (2) does not assume anything about the modality or classification task itself.\n\nAs a step towards more fine-grained interpretations of generalpurpose multimodal models across arbitrary tasks, we propose DIME, an interpretation method for black-box multimodal models. While existing work has been able to generate useful explanations to help humans understand model decision-making processes  [10] , they are often only performed at one step of the entire multimodal decisionmaking process. These singular steps typically include attributing feature importance  [10, 48]  or representation importance  [59, 66] . The core idea in DIME is to provide more fine-grained interpretations by disentangling a multimodal model into unimodal contributions (UC) and multimodal interactions (MI). We show that this key insight enables more accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures  [28, 57] , and tasks  [19, 27] .\n\nThrough a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME is able to accurately perform disentanglement and generate reliable explanations for both UC and MI. Using DIME, we are able to gain a deeper understanding of model behavior on challenging multimodal tasks. For example, on VQA 2.0  [20] , we successfully use DIME to determine whether the model uses correct multimodal interactions to answer the questions, as shown in Figure  1 . By providing these model explanations to a human annotator, they are able to gain additional insights on model behavior and better determine whether UC, MI, or both are the dominant factor behind the model's predictions on individual datapoints. Furthermore, DIME presents a step towards debugging and improving these models by systematically revealing certain undesirable behaviors.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Interpretable machine learning as a research field aims to further our understanding of AI models, empower various stakeholders to build trust in AI models, perform model debugging, and use these insights to better inform the interaction between AI and humans in joint decision making  [7, 12, 18] . We cover related concepts in interpreting unimodal models and multimodal models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Interpreting Unimodal Models",
      "text": "Related work has studied approaches for better understanding unimodal models used for vision, language, and audio modalities. These approaches can be roughly categorized into interpretable ML as designing models which are understandable by design, and explainable ML which focuses on producing post-hoc explanations for black-box models  [54] . In the former, methods such as Concept Bottleneck Models  [34]  and fitting sparse linear layers  [63]  or decision trees on top of deep feature representations  [61]  have emerged as promising choices marrying the expressive power of deep features with the interpretable decision-making processes of linear models or decision trees. In the latter, approaches such as saliency maps  [55, 56] , using surrogate models to interpret local decision boundaries  [52] , feature visualizations  [16, 65] , and assigning semantic concepts  [5]  all aim to provide insight into model predictions for specific input instances. We refer the reader to Chen et al.  [12]  for a survey and taxonomy of interpretable ML approaches, as well as Bhatt et al.  [7]  for an analysis of how interpretable and explainable ML tools can be used in the real world.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Interpreting Multimodal Models",
      "text": "Similar to the interpretation of unimodal models, recent work in interpretable multimodal learning can be categorized into two sections: (1) constructing interpretable multimodal models via careful model design  [48, 59, 66]  or (2) performing post-hoc explanations of black-box multimodal models  [10, 21] . In the former, multimodal routing networks  [59] , graph-based fusion techniques  [41, 66] , multimodal explanation networks to highlight visual importance  [48] , hard-attention  [11] , and neuro-symbolic reasoning methods  [3, 60]  have emerged as strong design choices as a step towards more interpretable multimodal learning. These approaches individually focus on building interpretable components for either modality importance  [48] , cross-modal interactions  [41, 59, 66] , or the reasoning process on top of cross-modal interactions  [3, 60] . While these approaches provide reliable interpretations by virtue of model design, they are typically restricted to a certain set of modalities or tasks. On the other hand, we propose a more general approach that is able to generate post-hoc explanations for arbitrary black-box multimodal models, and does not assume anything about the modality or classification task itself.\n\nIn the latter section on post-hoc explainability of black-box multimodal models, related work has similarly gravitated towards aiming to understand either modality importance  [10, 21, 29]  or cross-modal interactions in pretrained language and vision transformer models  [9, 17, 38, 47] . Perhaps most related to our work is Wang et al.  [62]  proposing M2Lens, an interactive visual analytics system to visualize and explain black-box multimodal models for sentiment analysis through both unimodal and multimodal contributions. Our approach further disentangles the two types of contributions, which allows us to generate visualizations on each and gain insight into which input features are involved in multimodal interactions. Our approach is also not restricted to sentiment analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Representation Disentanglement",
      "text": "Related to our work is the idea of learning disentangled data representations -mutually independent latent variables that each explain a particular variation of the data  [6, 43] . Disentangled representation learning has been shown to improve both generative and discriminative performance in multimodal tasks  [58] . If the factors of variation are known, many methods learn latent attributes that individually control each variation of data by supervised training  [14, 30, 51] . If the factors are partially known or unknown, deep generative models can be used to impose an isotropic Gaussian prior on the latent variables  [24, 32, 53] , maximize the mutual information between a subset of latent variables and the data  [13] , or to encourage the distribution of representations to be factorial and hence independent  [31] . Particularly related to our work is empirical multimodally-additive function projection (EMAP)  [23] , an approach for disentangling the effects of unimodal (additive) contributions from cross-modal interactions in multimodal tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Model Biases",
      "text": "One core motivation for interpretable ML is to enable a better understanding of the model's decision-making process so as to check whether model behavior is as intended. Using these tools, researchers have uncovered several biases existing in machine learning models and datasets. These biases include undesirable associations captured either in the data or the model, which do not reflect decision-making as one would expect. For example, a line of work in visualizing and understanding multimodal models has uncovered unimodal biases in the language modality of VQA tasks  [1, 2, 8, 26] , which then inspired follow-up datasets to elevate the importance of visual understanding through VQA 2.0  [20] . Similar visualizations also led to improved performance on image captioning tasks by relying less on gender biases and spurious correlations  [22] . Our approach towards better visualizing and understanding multimodal models is also inspired by these insights, and we believe that our fine-grained and general approach will motivate future work towards removing biases from a wider range of datasets and models beyond the prototypical language and vision tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method: Dime",
      "text": "Our approach, DIME (short for DISENTANGLED MULTIMODAL EXPLANATIONS), is primarily based on disentangling a multimodal model into unimodal contributions (UC) and multimodal interactions (MI), before performing fine-grained visualizations on each disentangled factor. In this section, we introduce precise definitions of unimodal contributions and multimodal interactions, before explaining how disentanglement and interpretations are performed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Unimodal Contributions And Multimodal Interactions",
      "text": "Unimodal contributions (UC) represent information gained by only looking at one of the modalities without interacting with any other modalities, while multimodal interactions (MI) are information gained from cross-referencing inputs from multiple modalities  [23] .\n\nMultimodal models make decisions using a combination of information from both unimodal contributions and multimodal interactions. For example, in Figure  1 , the model assigns a high likelihood to \"glass\" because  (1)  just by looking at the image, there are many glass objects (unimodal contributions) and (  2 ) by cross-referencing with text, the model focuses on the glass table and assigns a high likelihood to \"glass\" (multimodal interaction). Therefore, to performed fine-grained interpretation in a multimodal model ğ‘€, we first propose a new method to disentangle the model into two submodels:\n\nwhere UC(ğ‘€) represents the unimodal contributions within ğ‘€ and MI(ğ‘€) represents the multimodal interactions within ğ‘€. We can then run visualizations on each sub-model in order to generate human-interpretable visualizations of unimodal contributions and multimodal interactions (see Figure  2  for an overview of DIME). To generate visual explanations, we choose LIME  [52] , a widely used interpretation method for black-box models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Disentanglement",
      "text": "Let ğ‘€ be the multimodal model that we wish to disentangle into unimodal contributions and multimodal interactions. For simplicity, suppose ğ‘€ takes in two modalities as input and produces pre-softmax logits on ğ¶ classes as output. Therefore, we can view ğ‘€ as a function that maps two inputs ğ‘¥ 1 , ğ‘¥ 2 from two modalities to a output logit vector ğ‘‰ , i.e., ğ‘‰ = ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 ). Our goal will be to disentangle the function ğ‘€ into a sum of two functions, one representing unimodal contributions and one representing multimodal interactions. Formally, we would like to write ğ‘€ as\n\n, where ğ‘” 1 and ğ‘” 2 are unimodal contributions from the two input modalities, respectively, and ğ‘” 12 represents multimodal interactions. By definition of multimodal interactions, we require that E ğ‘¥ 1 ğ‘” 12 (ğ‘¥ 1 , ğ‘¥ 2 ) = 0 for all ğ‘¥ 2 and E ğ‘¥ 2 ğ‘” 12 (ğ‘¥ 1 , ğ‘¥ 2 ) = 0 for all ğ‘¥ 1 so that ğ‘” 12 contains no unimodal contribution. We will show that under this definition, for each ğ‘€ there will be a unique ğ‘” 12 that satisfies these rules.\n\nWe will compute ğ‘” 1 (ğ‘¥ 1 ) + ğ‘” 2 (ğ‘¥ 2 ) using a similar method to EMAP  [23] . We define UC(ğ‘€) as\n\n(2) Theorem 1 below (equations 3-5, proof in Appendix) states that UC(ğ‘€) indeed represents\n\nThus, we can compute\n\nThis also shows that ğ‘” 12 can be uniquely determined.\n\nIn practice, to compute UC(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )) and MI(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )), we use a sampling method similar to  [23] , where we sample ğ‘ datapoints\n\n2 ) including the point we want to explain ğ‘¥ = (ğ‘¥ 1 , ğ‘¥ 2 ) as one of them, and computing each expectation in UC(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )) by approximating\n\nFigure  3  illustrates this disentanglement process. However, to compute UC(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )) and MI(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )), we will need to run forward passes through the model a total of ğ‘ 2 times. In section 3.4 we will show an algorithm that computes this more efficiently by amortizing across multiple datapoints.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interpreting Disentanglement",
      "text": "Now that we have disentangled the model into two, we will generate human-interpretable explanations on each modality using LIME  [52] . LIME works by subdividing the input into distinct features, and then randomly perturbing the features ğ‘† times to see how the perturbations on the features affect the model output logits of a specific class ğ‘. LIME then fits a linear model mapping the perturbations on each feature to the logits of ğ‘. The linear model weights on each feature gives the explanation of that feature: if the weight is positive, it means that this feature supports the decision of class ğ‘; if the weight is negative, it means that this feature is against the decision of class ğ‘; the larger the weight's absolute value, the stronger the contribution is. Visually, the weights can also be used to generate a humaninterpretable visualization: for images, each feature is typically a part of the image, so the parts with the highest absolute weights can be highlighted in green for positive and red for negative contributions. For text, each feature is typically a word, so the explanation can be summarized as a histogram of weights of each word (see Figure  2  for an example).\n\nWhen running LIME on multimodal inputs, we run LIME on one modality at a time, treating the inputs to all other modalities as constant and only perturbing the inputs to that one modality. We denote the generated explanation on model ğ‘€, datapoint (ğ‘¥ 1 , ğ‘¥ 2 ), and modality ğ‘– as LIME ğ‘– (ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )). After disentanglement into unimodal contributions UC(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )) and multimodal interactions MI(ğ‘€ (ğ‘¥ 1 , ğ‘¥ 2 )), our approaches enables the generation of four finegrained explanations:\n\n), the explanation of modality 1's unimodal contributions.\n\n), the explanation of modality 2's unimodal contributions.\n\n), the explanation of modality 1's contribution to multimodal interactions.\n\n), the explanation of modality 2's contribution to multimodal interactions.    [52] ) in order to generate fine-grained human-interpretable visualizations of each. ğ‘ is the number of samples used for EMAP, so the total procedure of running DIME on one datapoint can take ğ‘‚ (ğ‘†ğ‘ 2 ) runs of ğ‘€.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Improving Efficiency",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Unimodal Contributions",
      "text": "In order to make the process faster, we use the following algorithmic trick: we fix ğ‘ datapoints from the dataset, and then run ğ‘€ on all ğ‘ 2 combinations of the two modalities amongst the ğ‘ points, and store the resulting logits in a ğ‘ Ã— ğ‘ Ã— ğ¶ array ğ¿ (where ğ¶ is the number of classes in this task). When we want to run DIME on any one of those ğ‘ points (let's say the ğ‘–th point), for each perturbed LIME sample (WLOG let's say we're running LIME on modality 1, so modality 1 is perturbed in the LIME sample), we make a deep copy of ğ¿ called ğ¿ â€² , re-run ğ‘€ on the combination of the perturbed modality 1 input and all ğ‘ modality 2 inputs, replace the values in the ith row of ğ¿ â€² with the results, and compute UC(ğ‘€) on this LIME sample with the updated table ğ¿ â€² . Using this trick, after amortizing the one-time initial ğ‘‚ (ğ‘ 2 ) runs of ğ‘€, each followup DIME run on any of the ğ‘ points only takes ğ‘‚ (ğ‘†ğ‘ ) runs of ğ‘€. See details in Algorithm 1 in the Appendix.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we will perform a set of experiments to fully evaluate the reliability and usefulness of DIME in interpreting multimodal models. We will be using 3 datasets: a synthetic dataset, CLEVR  [27] , and VQA 2.0  [19] , and with one corresponding stateof-the-art model for each: MLP, MDETR  [28]  and LXMERT  [57] . When dealing with datasets involving image and text modalities, we will refer to the two modalities as (ğ‘‰ ,ğ‘‡ ) respectively (e.g., UC ğ‘‰ would refer to the DIME explanation on image unimodal contribution). Our experiments are designed to illustrate the following takeaway messages of using DIME to analyze multimodal models:\n\n(1) Our method can reliably disentangle the model and generate accurate explanations for both UC and MI, correlating highly with their respective ground truths (section 4.2.1). (2) In more difficult tasks such as CLEVR and VQA, and with more complex models, DIME can still disentangle the model reliably. We show that changing the text input affects UC ğ‘‰ (explanation on image unimodal contribution) little but affects MI ğ‘‰ (explanation on multimodal interactions from the image side) significantly (section 4.2.1). (  3 ) DIME gives additional insight into understanding multimodal model behavior by answering whether the model relies mostly on UC, MI, or both in making the prediction (section 4.2.2). (  4 ) DIME also enables human users to debug and improve models by identifying which input features are used in MI and revealing undesirable behavior in models (section 4.2.3). Following these results, we will discuss limitations and future works (section 4.3).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Setup",
      "text": "4.1.1 Datasets. We will use three datasets: a synthetic dataset to enable controlled variations between unimodal and multimodal interactions, as well as two large-scale multimodal datasets: CLEVR, and VQA 2.0.\n\nThe synthetic dataset ğ· is designed to model a task that requires both unimodal (additive) contributions and multimodal interactions to solve correctly. According to prior work  [23] , the dot product of two modalities requires non-additive cross-modal interaction, while the sum of two vectors is additive. Therefore, we design a synthetic dataset ğ· by randomly generating two 10-dimensional vectors following ğ‘ (0, 1) independently for each element, and then computing the sum of all elements in both vectors plus the dot product of the two vectors. If the result's absolute value is below 0.01, we discard this point; otherwise, we assign a 0/1 label based on the sign of the result. We generate 100, 000 points to form ğ· and divide it into train/valid/test splits by 8/1/1 ratio.\n\nCLEVR  [27]  is a diagnostic dataset designed for language and visual reasoning. The dataset consists of synthesized images of 3D shapes of various colors, sizes, and materials on a gray background, For each image, there are several questions about the shapes' attributes, positions, and numbers. This dataset has been widely used for diagnostic purposes to find model weaknesses.\n\nVQA 2.0  [19]  is a dataset containing various questions on realworld images. It is designed to force multimodal interactions, especially incorporating the visual aspect, by sometimes having the same question with two different answers on two different images. This dataset is interesting because models have been shown to occasionally \"guess\" correct answers purely from unimodal contributions or with the wrong visual grounding  [2, 8] . DIME will enable us to study how often models rely on undesirable unimodal biases and further understand the model's decision-making process.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Models.",
      "text": "For synthetic dataset ğ·, we train a 4-layer MLP (with input size 20 and hidden layer sizes 100, 200, 10, 2 respectively) on ğ· that reaches 97.3% accuracy on the test split.\n\nFor CLEVR dataset, we will be using a pretrained MDETR  [28]  that achieves 99.7% test accuracy.\n\nFor VQA 2.0, we will be using pretrained LXMERT  [57] , one of the best models on the dataset, with a 72.5% test accuracy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Research Questions And Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rq1:",
      "text": "Can DIME reliably disentangle a model into unimodal contributions and multimodal interactions and generate accurate explanations for both UC and MI in practice?\n\nIn section 3.2, we have theoretically shown that DIME can disentangle a model into unimodal contributions and multimodal interactions. To show that this also holds in practice (when expectation computations are replaced by sampling), we will run DIME on our trained model ğ‘€ using 1, 000 randomly selected datapoints in the test split of our synthetic dataset ğ·, on label 1 (i.e., that the sum of all elements of both vectors plus the dot-product of the two vectors are positive).\n\nFor each point (ğ‘‘ 1 , ğ‘‘ 2 ) in ğ·, since we are classifying whether the sum of all elements in ğ‘‘ 1 and ğ‘‘ 2 as well as the dot product of ğ‘‘ 1 and ğ‘‘ 2 , the ground truth UC explanation on each modality will be ğ‘‘ 1 and ğ‘‘ 2 respectively, and the ground truth MI explanation will be elementwise product ğ‘‘ 1 * ğ‘‘ 2 . Therefore, for each generated explanation on input data (ğ‘‘ 1 , ğ‘‘ 2 ), we will compute the Pearson Correlation between the explanation weights of the 10 features with the values of the 10 features of ğ‘‘ 1 , the values of the 10 features of ğ‘‘ 2 , and the 10 features in the element-wise product of ğ‘‘ 1 and ğ‘‘ 2 . In addition to DIME, we also run LIME under the same settings as an ablation and compute average correlations.\n\nThe results are shown in Table  2 . We found that within each datapoint (ğ‘‘ 1 ,ğ‘‘ 2 ), there is a strong correlation between each DIMEgenerated unimodal explanation (UC 1 , UC 2 ) and the corresponding ground truth UC explanation, but there is neither correlation between UC 1 /UC 2 and ground truth UC explanation of a different modality, nor correlation between UC 1 /UC 2 and ground truth multimodal interaction explanations. This shows that DIME-generated UC explanations indeed capture unimodal contributions only. Moreover, we found that both DIME-generated multimodal interaction explanations (MI 1 , MI 2 ) indeed correlate with the ground truth MI explanation, but not with either ground truth UC explanation. This shows that DIME-generated multimodal interaction explanation indeed captures explanations on just the multimodal interactions (i.e., the dot-product), and not any of the unimodal contributions. Meanwhile, running the original LIME on either modality just gives an explanation that weakly correlates with ground truth unimodal contributions and multimodal interactions, so the original LIME without disentangling is unable to give an accurate explanation of either unimodal contributions or multimodal interactions.\n\nIn addition to using a synthetic dataset, we show that DIME can also disentangle more complex models on multimodal tasks, such as MDETR on CLEVR and LXMERT on VQA (the latter model is far from perfect in performance). As a measure of disentanglement, we check how DIME-generated explanations would be different given the same image but different questions. From each dataset, we randomly select 100 points and generate their DIME explanations on the correct label. Then, for each point, we swap out the question with another different question on the same image and generate With disentanglement in DIME, the unimodal contribution explanations completely correlates with their respective unimodal ground truth, and the multimodal interaction explanations completely correlates with the ground truth multimodal interactions (i.e., element-wise product of the two inputs). On the other hand, running LIME without disentangling gives an explanation that confuses both unimodal contributions and multimodal interactions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset (Model)",
      "text": "UC ğ‘‰ MI ğ‘‰ CLEVR (MDETR) 0.005 0.295 VQA (LXMERT) 0.001 0.808 their DIME explanations on the same label (i.e., correct label before the swap). We compute cosine distance between the explanation weights from UC ğ‘‰ before/after the swap, as well as cosine distance between the weights from MI ğ‘‰ before/after the swap, and report average cosine distances on each dataset in Table  3 . We can see that swapping text has almost no effect on UC ğ‘‰ but affects MI ğ‘‰ significantly. Therefore, DIME is able to correctly disentangle a model into unimodal contributions and multimodal interaction for more complex models and tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Rq2:",
      "text": "Can DIME help researchers gain additional insight in whether unimodal contributions or multimodal interactions are the dominant factors behind a model's prediction? Disentangling the model into UC and MI and generating visualizations for each should provide additional insights into whether UC or MI is the main factor in the model's prediction. In the following experiments, we show that DIME can uncover which factor is dominant in a model's prediction process both across all points in the dataset (\"global\") and on each individual datapoint (\"local\").\n\nGlobal interpretation: CLEVR dataset is designed to force multimodal interactions, and MDETR has a 99.7% accuracy on CLEVR, so we expect that MDETR will be heavily reliant on multimodal interactions. To verify this, we run DIME on MDETR for 100 randomly sampled datapoints from the validation split of CLEVR, and compute the average absolute weight of the top-5 features in DIME explanations. As shown in Table  4 , the MI ğ‘‰ and ğ‘€ğ‘‰ ğ‘‡ weights are indeed significantly larger than UC ğ‘‰ and UC ğ‘‡ weights. Note that unimodal text does still give some useful information in CLEVR, such as the answer type (yes/no, attribute, or number), so that explains why UC ğ‘‡ still has a weight of about 60% that of MI ğ‘‡ . The average weight for MI ğ‘‰ , however, is over 4 times higher than UC ğ‘‰ . Therefore, using DIME, we confirmed that MDETR indeed relies mostly on multimodal interactions to solve the task.\n\nLocal interpretation: In most datasets and models, models will not be near-perfect, and they will have different dominating factors  from datapoint to datapoint. In this case, a global analysis will not suffice, and it will be necessary to look into which factor contributes more to the model's prediction on individual datapoints. We perform the following experiment to show that DIME can help users determine whether a model makes a prediction on a datapoint where (1) unimodal text is dominant, (2) unimodal image is dominant, (3) multimodal interactions are dominant, and (4) both UC and MI have significant contributions to the answer. We will use LXMERT on VQA since LXMERT is not close to perfect and often relies on different factors when predicting different datapoints.\n\nWe gave five human annotators (who have some background knowledge in machine learning but do not have any knowledge about DIME) the same set of 52 datapoints from VQA, as well as the prediction from LXMERT. For each datapoint, each human annotator is first given the LIME explanations without disentanglement as a baseline, and they are asked to categorize this point into one of the four categories above, while also rating how confident they are on their decision on a scale from one (least confident) to five (most confident). The human annotators are then presented with DIME explanations, and again they are asked to categorize each point as well as rate their confidence.\n\nThe results are shown in Table  5 . We can see that human annotators have significantly higher average confidence scores when presented with DIME explanations as compared to the baseline. Moreover, DIME result shows significantly higher Krippendorff's alpha score  [35] , which measures inter-annotator agreements, so annotators also tend to agree a lot more on their categorizations. Therefore, DIME is able to help researchers more confidently determine whether UC or MI (or both) is the dominant factor behind the model's prediction, and thus help researchers gain additional insight into model behavior.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Rq3:",
      "text": "Can DIME help us better assess the qualities of the model and gain insights on how to debug or improve model performance?",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Lime Dime",
      "text": "Average confidence score 2.23 3.77 Annotator agreement (Krippendorff's ğ›¼)  [35]  0.18 0.57\n\nTable  5 : Results of the human annotation experiments on categorizing whether LXMERT predicts a point using UC, MI, or both as the dominant deciding factor. We can see that on average, human annotators gave much higher confidence score to the DIME explanation compared to LIME without disentanglement, and human annotators also tend to agree more on decisions based on DIME.\n\nWhen trying to debug or improve a model on a task involving challenging reasoning, such as VQA, one important question researchers often ask is: do we know if our model actually learns to do the task \"the intended way\" (i.e., go through the same logical reasoning process as a human would to perform the task)? How often does our model perform as intended? Therefore, we conduct the following experiment to show that DIME may help answer this question.\n\nWe use DIME explanations to categorize the model's behavior on each datapoint into one of the following categories:\n\nWhen the model answers correctly,\n\nâ€¢ â€¢ (4) The model fully identifies the necessary parts of the image to answer the question logically through MI, but still gets the answer wrong because the model does not fully understand a concept or because the question is too difficult (even for a human being). â€¢ (5) The model only partially identifies the parts of the image that are necessary to answer the question logically through MI, thus missing some of the key parts of the image resulting in an incorrect answer. â€¢ (6) The model did not correctly identify any of the parts of the image that are necessary to answer the question logically through MI, and thus the model fails to answer the question correctly. In Figure  4 , we show examples of datapoints, model predictions, and explanations that were annotated into each of the above categories. As shown in the examples, in most cases, there will be enough evidence to categorize a datapoint just by looking at the multimodal interaction explanations from the image side (MI ğ‘‰ ), but sometimes other DIME explanations (e.g., explanations of text interactions) will be needed to gain additional understanding of the model's decision-making process.\n\nThe results of this human study are shown in Table  6 . With DIME, we were able to categorize 118 points with evidence, out of a total of 140 points (84%). This shows that DIME is able to highlight which input features are aligned or recognized by MI. We observe that,  even though the models can fully identify the correct parts of the image that are relevant to the questions half of the time (69/118), there is still a significant portion of datapoints where the model correctly aligns text and image but relies on unimodal contributions instead. This highlights several shortcomings of the model's decisionmaking process despite answering the question correctly. Therefore, the information gained from performing DIME can help researchers identify weaknesses in their models and debug or improve these models accordingly.\n\nWe also observe that the model is more likely to not be able to fully identify the correct regions of the image when the model makes the wrong prediction, which is expected.\n\nIn addition, we also found the following interesting observations when looking at the DIME explanations of the 118 points:\n\nâ€¢ LXMERT often relies too heavily on unimodal text contributions: for example, in a question involving \"car\", unimodal contributions in text will prompt the model to answer \"street\" even if the model is unable to find \"street\" in the image. Sometimes, even when the model is able to interactively identify the correct regions of the image, unimodal text contributions can still dominate over the multimodal interaction (such as the fourth example in Figure  4 , where the model answered \"glove\" due to unimodal text contributions even though the model was able to interactively identify the bat). â€¢ The model sometimes interactively identifies the wrong object that happens to share the same properties in question as the correct object (such as the third example in Figure  4 , where instead of the dog's paws, the model identified the nearby cat which also happens to be white). This coincidence happens more often than we expected, as there are 8 such cases amongst the 118 examples (7%).\n\nâ€¢ When asked about the color of an object that has two colors, LXMERT will only pick out one of the colors. DIME analysis shows that this is often due to LXMERT only identifying subregions of the object in one color while ignoring other parts of the object that are in a different color. For example, in Figure  5 , the model thinks that the hydrant is not \"silver and red\" because it did not classify the red tip as part of the hydrant. These additional observations may guide future research in improving LXMERT (and other similar models) or designing inductive biases to avoid these undesirable behaviors. The model accurately identifies both \"head\" and \"hat\" through multimodal interactions.\n\n( The model mistakenly identified the white cat as the black dog's paws, and since the cat happens to be the same color as the dog's paws, the model is able to answer the question correctly by chance.\n\n( The model is able to identify a part of the soup (the noodle and part of the shrimp), which happens to be white, so the model thinks the soup is creamy; the model fails to look at other part of the soup that shows the soup is actually not creamy.\n\nQ: What is on the child's hand? Correct A: bat Model A: glove\n\nThe model identifies the boy, the hand, and the bat; however, it still selected \"glove\" over \"bat\" due to strong unimodal contributions from text: the words \"what\", \"on\", \"hand\" basically strongly steered the model towards \"glove\".\n\nQ: What color is the streak? Correct A: red Model A: white (6) The model identifies none of the necessary parts of the image to answer the question logically.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Model Was Right Model Was Wrong",
      "text": "The model is completely unable to identify the streak, so it just answers the dominant color of the image: white. As shown by the MI ğ‘‰ explanation, the model actually thought that the red part is \"against\" the answer \"silver and red\", which means the model thought the red region isn't a part of the hydrant.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Limitations And Future Directions",
      "text": "Despite the ability of DIME in interpreting and debugging multimodal models, there remain several directions for future work:\n\n1. Models with discrete outputs: Even though DIME is designed to work for any black-box classification models, it requires the model to produce a continuous logit for each answer choice. DIME does not work well on the Neural-Symbolic VQA model  [44]  since it only produces one discrete output instead of a continuous logit. Even when we tried to convert its outputs to logits by assigning its answer a logit of 1 and all other answer choices a logit of -1, DIME often fails to produce any meaningful explanation since the perturbations are unable to change the discrete answer of the model, thus having no effect on the assigned logits.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Number Of Modalities:",
      "text": "In all experiments, DIME was applied to tasks with 2 modalities. Disentangling a model across even 3 modalities can be very costly, as we will need to run the model ğ‘ 3 times to compute unimodal contributions. Another challenge lies in interpreting the multimodal interaction, which would consist of bi-modal interactions between each pair of modalities as well as tri-modal interactions across all 3 modalities. Future work should tackle these challenges and try to expand DIME for high-modality scenarios.\n\n3. Diverse modalities: Even though the disentangling method in DIME theoretically works on any modality, our experiments have focused on image+text datasets (except the synthetic dataset experiment). This is because LIME-generated visualized explanations are relatively intuitive on image and text; it can be much harder for a human annotator to look at the results of explanations on other modalities (such as time-series of vectors) and try to make sense of them. In the future, we would like to design additional experiments to show that DIME can also be used to gain additional insight on model behavior in tasks involving modalities other than image and text as well.\n\n4. Using these insights to improve models: Since DIME is able to reveal several hidden undesirable behaviors in multimodal models, future work should aim to propose targeted solutions to these highlighted biases as a step towards improving multimodal models. For example, according to insights gained on VQA in RQ3, LXMERT can be improved by encouraging less reliance on unimodal text contribution, where insights from Cadene et al.  [8]  (which studies this research question for non-pretrained models) could be useful. Furthermore, future work could also design new training objectives which penalize models that associate wrong objects with words in MI, despite getting the correct answer.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In conclusion, DIME presents a new way to help users understand multimodal models by disentanglement into unimodal contributions and multimodal interactions before generating visual explanations for each. DIME can generate accurate disentangled explanations, help researchers and developers gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models. We hope that DIME inspires the design of multimodal models that are more trustworthy, reliable, and robust for real-world applications.\n\nA PROOF OF THEOREM 1",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Algorithm Details",
      "text": "In Algorithm 1 we describe our procedure for efficiently running batched DIME. The core idea in DIME is to provide more fine-grained interpretations by disentangling a multimodal model into unimodal contributions (UC) and multimodal interactions (MI). DIME is able to accurately perform disentanglement and generate reliable explanations for both UC and MI. Using DIME, we are able to gain additional insights on model behavior and better determine whether UC, MI, or both are the dominant factor behind the model's predictions on individual datapoints. Furthermore, DIME presents a step towards debugging and improving these models as it may reveal certain undesirable behaviors of the models.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: DIME is a novel method of interpreting multimodal models by disentangling the model into unimodal contributions (UC) and multimodal",
      "page": 1
    },
    {
      "caption": "Figure 1: By providing these model ex-",
      "page": 2
    },
    {
      "caption": "Figure 1: , the model assigns a high likelihood to",
      "page": 3
    },
    {
      "caption": "Figure 2: for an overview of DIME). To",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates this disentanglement process.",
      "page": 4
    },
    {
      "caption": "Figure 2: for an example).",
      "page": 4
    },
    {
      "caption": "Figure 2: High level illustration of DIME: we disentangle the model ğ‘€into two: unimodal contributions (UC) and multimodal interactions (MI),",
      "page": 5
    },
    {
      "caption": "Figure 3: An illustration of the disentangling process of DIME. We dis-",
      "page": 5
    },
    {
      "caption": "Figure 4: , we show examples of datapoints, model predictions,",
      "page": 8
    },
    {
      "caption": "Figure 4: , where the model answered",
      "page": 8
    },
    {
      "caption": "Figure 5: , the model thinks that the hydrant is not â€œsilver",
      "page": 8
    },
    {
      "caption": "Figure 4: Here we present examples of using DIME to categorize and explain why LXMERT makes certain predictions on datapoints in VQA 2.0.",
      "page": 9
    },
    {
      "caption": "Figure 5: In this example, the model was unable to answer correctly",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Advantages": "Works for black-box models",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ“\nâœ“\nâœ“"
        },
        {
          "Advantages": "Works for modalities from arbitrary\nclassification tasks (not restricted to\nspecific tasks or domains)",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ“\nâœ“\nâœ—"
        },
        {
          "Advantages": "Visualizes important input features",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ“\nâœ—\nâœ“"
        },
        {
          "Advantages": "Disentangles unimodal contributions\n(UC) and multimodal interactions (MI)",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ—\nâœ—\nUC only\n(RQ1, Â§4.2.1)"
        },
        {
          "Advantages": "Determines whether UC or MI (or both)\nis the dominant factor behind the\nmodelâ€™s predictions",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ—\nâœ“\nSometimes (*)\n(RQ2, Â§4.2.2)"
        },
        {
          "Advantages": "Provides insight into what features are\nbeing aligned or recognized in MI",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ—\nâœ—\nâœ—\n(RQ3, Â§4.2.3)"
        },
        {
          "Advantages": "Visualizes each of UC and MI to reveal\nundesirable model behavior",
          "DIME (ours)\nLIME [52]\nEMAP [23]\nM2Lens [62]": "âœ“\nâœ—\nâœ—\nâœ—\n(RQ3, Â§4.2.3)"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Analyzing the Behavior of Visual Question Answering Models",
      "authors": [
        "Aishwarya Agrawal",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "1955",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Blindfold baselines for embodied QA",
      "authors": [
        "Ankesh Anand",
        "Eugene Belilovsky",
        "Kyle Kastner",
        "Hugo Larochelle",
        "Aaron Courville"
      ],
      "year": "2018",
      "venue": "Blindfold baselines for embodied QA",
      "arxiv": "arXiv:1811.05013"
    },
    {
      "citation_id": "3",
      "title": "Neural module networks",
      "authors": [
        "Jacob Andreas",
        "Marcus Rohrbach",
        "Trevor Darrell",
        "Dan Klein"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "4",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal machine learning: A survey and taxonomy"
    },
    {
      "citation_id": "5",
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "authors": [
        "David Bau",
        "Bolei Zhou",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Representation Learning: A Review and New Perspectives",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "year": "2013",
      "venue": "TPAMI"
    },
    {
      "citation_id": "7",
      "title": "Explainable machine learning in deployment",
      "authors": [
        "Umang Bhatt",
        "Alice Xiang",
        "Shubham Sharma",
        "Adrian Weller",
        "Ankur Taly",
        "Yunhan Jia",
        "Joydeep Ghosh",
        "Ruchir Puri",
        "M JosÃ©",
        "Peter Moura",
        "Eckersley"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "8",
      "title": "Rubi: Reducing unimodal biases for visual question answering",
      "authors": [
        "Remi Cadene",
        "Corentin Dancette",
        "Matthieu Cord",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Behind the scene: Revealing the secrets of pre-trained vision-and-language models",
      "authors": [
        "Jize Cao",
        "Zhe Gan",
        "Yu Cheng",
        "Licheng Yu",
        "Yen-Chun Chen",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Do explanations make VQA models more predictable to a human?",
      "authors": [
        "Arjun Chandrasekaran",
        "Viraj Prabhu",
        "Deshraj Yadav",
        "Prithvijit Chattopadhyay",
        "Devi Parikh"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "11",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "Minghai Chen",
        "Sen Wang",
        "Paul Liang",
        "Tadas BaltruÅ¡aitis",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "12",
      "title": "Interpretable Machine Learning: Moving from mythos to diagnostics",
      "authors": [
        "Valerie Chen",
        "Jeffrey Li",
        "Joon Sik Kim",
        "Gregory Plumb",
        "Ameet Talwalkar"
      ],
      "year": "2022",
      "venue": "Interpretable Machine Learning: Moving from mythos to diagnostics"
    },
    {
      "citation_id": "13",
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "authors": [
        "Xi Chen",
        "Yan Duan",
        "Rein Houthooft",
        "John Schulman",
        "Ilya Sutskever",
        "Pieter Abbeel"
      ],
      "year": "2016",
      "venue": "NIPS"
    },
    {
      "citation_id": "14",
      "title": "Discovering hidden factors of variation in deep networks",
      "authors": [
        "Brian Cheung",
        "Jesse Livezey",
        "Arjun Bansal",
        "Bruno Olshausen"
      ],
      "year": "2014",
      "venue": "Discovering hidden factors of variation in deep networks",
      "arxiv": "arXiv:1412.6583"
    },
    {
      "citation_id": "15",
      "title": "Multimodal interfaces: A survey of principles, models and frameworks",
      "authors": [
        "Bruno Dumas",
        "Denis Lalanne",
        "Sharon Oviatt"
      ],
      "year": "2009",
      "venue": "Human machine interaction"
    },
    {
      "citation_id": "16",
      "title": "Visualizing higher-layer features of a deep network",
      "authors": [
        "Yoshua Dumitru Erhan",
        "Aaron Bengio",
        "Pascal Courville",
        "Vincent"
      ],
      "year": "2009",
      "venue": "University of Montreal"
    },
    {
      "citation_id": "17",
      "title": "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers",
      "authors": [
        "Stella Frank",
        "Emanuele Bugliarello",
        "Desmond Elliott"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "authors": [
        "David Leilani H Gilpin",
        "Bau",
        "Ayesha Ben Z Yuan",
        "Michael Bajwa",
        "Lalana Specter",
        "Kagal"
      ],
      "year": "2018",
      "venue": "2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)"
    },
    {
      "citation_id": "19",
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "20",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Towards transparent ai systems: Interpreting visual question answering models",
      "authors": [
        "Yash Goyal",
        "Akrit Mohapatra",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2016",
      "venue": "Towards transparent ai systems: Interpreting visual question answering models",
      "arxiv": "arXiv:1608.08974"
    },
    {
      "citation_id": "22",
      "title": "Women also snowboard: Overcoming bias in captioning models",
      "authors": [
        "Anne Lisa",
        "Kaylee Hendricks",
        "Kate Burns",
        "Trevor Saenko",
        "Anna Darrell",
        "Rohrbach"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "23",
      "title": "Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!",
      "authors": [
        "Jack Hessel",
        "Lillian Lee"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "24",
      "title": "ğ›½-vae: Learning basic visual concepts with a constrained variational framework",
      "authors": [
        "Irina Higgins",
        "Loic Matthey",
        "Arka Pal",
        "Christopher Burgess",
        "Xavier Glorot",
        "Matthew Botvinick",
        "Shakir Mohamed",
        "Alexander Lerchner"
      ],
      "year": "2016",
      "venue": "ğ›½-vae: Learning basic visual concepts with a constrained variational framework"
    },
    {
      "citation_id": "25",
      "title": "A Picture is Worth a Thousand Words: Multimodal Sensemaking of the Global Financial Crisis",
      "authors": [
        "Markus Hollerer",
        "Dennis Jancsary",
        "Maria Grafstrom"
      ],
      "year": "2018",
      "venue": "Organization Studies"
    },
    {
      "citation_id": "26",
      "title": "Revisiting visual question answering baselines",
      "authors": [
        "Allan Jabri",
        "Armand Joulin",
        "Laurens Van Der Maaten"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "27",
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "authors": [
        "Justin Johnson",
        "Bharath Hariharan",
        "Laurens Van Der Maaten",
        "Li Fei-Fei",
        "C Lawrence Zitnick",
        "Ross Girshick"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR-Modulated Detection for End-to-End Multi-Modal Understanding",
      "authors": [
        "Aishwarya Kamath",
        "Mannat Singh",
        "Yann Lecun",
        "Ishan Misra"
      ],
      "year": "2021",
      "venue": "Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR-Modulated Detection for End-to-End Multi-Modal Understanding",
      "arxiv": "arXiv:2104.12763"
    },
    {
      "citation_id": "29",
      "title": "Multimodal explanations by predicting counterfactuality in videos",
      "authors": [
        "Atsushi Kanehira",
        "Kentaro Takemoto",
        "Sho Inayoshi",
        "Tatsuya Harada"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Bayesian representation learning with oracle constraints",
      "authors": [
        "Theofanis Karaletsos",
        "Serge Belongie",
        "Gunnar RÃ¤tsch"
      ],
      "year": "2015",
      "venue": "Bayesian representation learning with oracle constraints",
      "arxiv": "arXiv:1506.05011"
    },
    {
      "citation_id": "31",
      "title": "Disentangling by Factorising",
      "authors": [
        "Hyunjik Kim",
        "Andriy Mnih"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "32",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "33",
      "title": "Embedded multimodal interfaces in robotics: applications, future trends, and societal implications",
      "authors": [
        "Elsa Kirchner",
        "Stephen Fairclough",
        "Frank Kirchner"
      ],
      "year": "2019",
      "venue": "The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions"
    },
    {
      "citation_id": "34",
      "title": "Concept bottleneck models",
      "authors": [
        "Pang Wei Koh",
        "Thao Nguyen",
        "Siang Yew",
        "Stephen Tang",
        "Emma Mussmann",
        "Been Pierson",
        "Percy Kim",
        "Liang"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "35",
      "title": "Computing Krippendorff's alpha-reliability",
      "authors": [
        "Klaus Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing Krippendorff's alpha-reliability"
    },
    {
      "citation_id": "36",
      "title": "Making sense of vision and touch: Learning multimodal representations for contact-rich tasks",
      "authors": [
        "Michelle Lee",
        "Yuke Zhu",
        "Peter Zachares",
        "Matthew Tan",
        "Krishnan Srinivasan",
        "Silvio Savarese",
        "Li Fei-Fei",
        "Animesh Garg",
        "Jeannette Bohg"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Robotics"
    },
    {
      "citation_id": "37",
      "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2019",
      "venue": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
      "arxiv": "arXiv:1908.03557"
    },
    {
      "citation_id": "38",
      "title": "What does bert with vision look at",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "Multimodal Language Analysis with Recurrent Multistage Fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "40",
      "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Zetian Wu",
        "Yun Cheng",
        "Jason Wu",
        "Leslie Chen",
        "Peter Wu",
        "Michelle Lee",
        "Yuke Zhu",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2021",
      "venue": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning"
    },
    {
      "citation_id": "41",
      "title": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion",
      "authors": [
        "Paul Pu Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion"
    },
    {
      "citation_id": "42",
      "title": "Multimodal Local-Global Ranking Fusion for Emotion Recognition",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal Local-Global Ranking Fusion for Emotion Recognition"
    },
    {
      "citation_id": "43",
      "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
      "authors": [
        "Francesco Locatello",
        "Stefan Bauer",
        "Mario Lucic",
        "Gunnar Raetsch",
        "Sylvain Gelly",
        "Bernhard SchÃ¶lkopf",
        "Olivier Bachem"
      ],
      "year": "2019",
      "venue": "Challenging common assumptions in the unsupervised learning of disentangled representations"
    },
    {
      "citation_id": "44",
      "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
      "authors": [
        "Jiayuan Mao",
        "Chuang Gan",
        "Pushmeet Kohli",
        "Joshua Tenenbaum",
        "Jiajun Wu"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "45",
      "title": "Large-scale concept ontology for multimedia",
      "authors": [
        "Milind Naphade",
        "John Smith",
        "Jelena Tesic",
        "Shih-Fu Chang",
        "Winston Hsu",
        "Lyndon Kennedy",
        "Alexander Hauptmann",
        "Jon Curtis"
      ],
      "year": "2006",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "46",
      "title": "Modeling multimodal humancomputer interaction",
      "authors": [
        "Zeljko Obrenovic",
        "Dusan Starcevic"
      ],
      "year": "2004",
      "venue": "Computer"
    },
    {
      "citation_id": "47",
      "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks",
      "authors": [
        "Letitia Parcalabescu",
        "Albert Gatt",
        "Anette Frank",
        "Iacer Calixto"
      ],
      "year": "2021",
      "venue": "Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR"
    },
    {
      "citation_id": "48",
      "title": "Multimodal explanations: Justifying decisions and pointing to the evidence",
      "authors": [
        "Dong Huk",
        "Lisa Hendricks",
        "Zeynep Akata",
        "Anna Rohrbach",
        "Bernt Schiele",
        "Trevor Darrell",
        "Marcus Rohrbach"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Emotion recognition and adaptation in spoken dialogue systems",
      "authors": [
        "Johannes Pittermann",
        "Angela Pittermann",
        "Wolfgang Minker"
      ],
      "year": "2010",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "50",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "51",
      "title": "Learning to disentangle factors of variation with manifold interaction",
      "authors": [
        "Scott Reed",
        "Kihyuk Sohn",
        "Yuting Zhang",
        "Honglak Lee"
      ],
      "year": "2014",
      "venue": "ICML"
    },
    {
      "citation_id": "52",
      "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
      "authors": [
        "Marco TÃºlio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
      "arxiv": "arXiv:1602.04938"
    },
    {
      "citation_id": "53",
      "title": "On the Latent Space of Wasserstein Auto-Encoders",
      "authors": [
        "Bernhard Paul K Rubenstein",
        "Ilya Schoelkopf",
        "Tolstikhin"
      ],
      "year": "2018",
      "venue": "On the Latent Space of Wasserstein Auto-Encoders",
      "arxiv": "arXiv:1802.03761"
    },
    {
      "citation_id": "54",
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "authors": [
        "Cynthia Rudin"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "55",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2013",
      "venue": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "arxiv": "arXiv:1312.6034"
    },
    {
      "citation_id": "56",
      "title": "Smoothgrad: removing noise by adding noise",
      "authors": [
        "Daniel Smilkov",
        "Nikhil Thorat",
        "Been Kim",
        "Fernanda ViÃ©gas",
        "Martin Wattenberg"
      ],
      "year": "2017",
      "venue": "Smoothgrad: removing noise by adding noise",
      "arxiv": "arXiv:1706.03825"
    },
    {
      "citation_id": "57",
      "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "arxiv": "arXiv:1908.07490"
    },
    {
      "citation_id": "58",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "59",
      "title": "Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Martin Ma",
        "Muqiao Yang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "60",
      "title": "Probabilistic neural symbolic models for interpretable visual question answering",
      "authors": [
        "Ramakrishna Vedantam",
        "Karan Desai",
        "Stefan Lee",
        "Marcus Rohrbach",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "61",
      "title": "NBDT: Neural-Backed Decision Tree",
      "authors": [
        "Alvin Wan",
        "Lisa Dunlap",
        "Daniel Ho",
        "Jihan Yin",
        "Scott Lee",
        "Suzanne Petryk",
        "Sarah Bargal",
        "Joseph Gonzalez"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "62",
      "title": "M2Lens: Visualizing and explaining multimodal models for sentiment analysis",
      "authors": [
        "Xingbo Wang",
        "Jianben He",
        "Zhihua Jin",
        "Muqiao Yang",
        "Yong Wang",
        "Huamin Qu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "63",
      "title": "Leveraging sparse linear layers for debuggable deep networks",
      "authors": [
        "Eric Wong",
        "Shibani Santurkar",
        "Aleksander Madry"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "64",
      "title": "Multimodal machine learning for automated ICD coding",
      "authors": [
        "Keyang Xu",
        "Mike Lam",
        "Jingzhi Pang",
        "Xin Gao",
        "Charlotte Band",
        "Piyush Mathur",
        "Frank Papay",
        "Ashish Khanna",
        "Jacek Cywinski",
        "Kamal Maheshwari"
      ],
      "year": "2019",
      "venue": "Machine Learning for Healthcare Conference"
    },
    {
      "citation_id": "65",
      "title": "Understanding neural networks through deep visualization",
      "authors": [
        "Jason Yosinski",
        "Jeff Clune",
        "Thomas Fuchs",
        "Hod Lipson"
      ],
      "year": "2015",
      "venue": "ICML Workshop on Deep Learning"
    },
    {
      "citation_id": "66",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}