{
  "paper_id": "2102.06357v1",
  "title": "Contrastive Unsupervised Learning For Speech Emotion Recognition",
  "published": "2021-02-12T06:06:02Z",
  "authors": [
    "Mao Li",
    "Bo Yang",
    "Joshua Levy",
    "Andreas Stolcke",
    "Viktor Rozgic",
    "Spyros Matsoukas",
    "Constantinos Papayiannis",
    "Daniel Bone",
    "Chao Wang"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Contrastive predictive coding",
    "Unsupervised pre-training"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is a key technology to enable more natural human-machine communication. However, SER has long suffered from a lack of public large-scale labeled datasets. To circumvent this problem, we investigate how unsupervised representation learning on unlabeled datasets can benefit SER. We show that the contrastive predictive coding (CPC) method can learn salient representations from unlabeled datasets, which improves emotion recognition performance. In our experiments, this method achieved state-of-the-art concordance correlation coefficient (CCC) performance for all emotion primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on the MSP-Podcast dataset, our method obtained considerable performance improvements compared to baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims at discerning the emotional state of a speaker, thus enabling more human-like interactions between human and machines. An agent can understand the command of a human better if it is able to interpret the emotional state of the speaker as well. Moreover, a digital assistant can prove to be a human-like companion when equipped with the capability of recognizing emotions. These fascinating applications provide key motivations underpinning the fast growing research interest in this area  [1, 2]  Despite the substantial interest from both academia and industry, SER has not found many real-world applications. One possible reason is the unsatisfactory performance of existing systems. The difficulty is caused by, and contributes to, relatively small public data sets  [3, 4]  in this domain. The lack of large scale emotion annotated data hinders the application of deep learning methods, from which many other speechrelated tasks (e.g automatic speech recognition  [5] ) have benefited greatly.\n\nIn order to circumvent the data sparsity issue of SER, we investigate the use of unsupervised pre-training. Unsupervised pre-training techniques have received increased attention over the last few years. The research interest in this direction is well-motivated: while deep-learning (DL) based methods achieve state-of-the-art results across multiple domains, these methods tend to be data-intensive. Training a large and deep neutral network usually requires very large labeled datasets. The cost of data labeling has since become a major obstacle for applying DL techniques to real-world applications, and SER is no exception. Motivated by recent developments in unsupervised representation learning, we leverage an unsupervised pre-training approach for SER.\n\nThe proposed method shows great performance improvement on two widely used public benchmarks. The improvements on recognizing valence (positivity/negativity of the tone of voice) are particularly encouraging, as valence is known to be very hard to predict from speech data alone, see e.g.  [6, 7] . Furthermore, our analysis implies, even without explicit supervision in training, emotion clusters emerge in the embedding space of the pre-trained model, confirming the suitability of unsupervised pre-training for SER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Recent studies on unsupervised representation learning have achieved great success in natural language processing  [8, 9]  and computer vision  [10, 11] . While leveraging unsupervised learning for SER has been investigated relatively little, previous attempts using autoencoders have been successful  [12, 13] . More recently, it has been shown that learning to predict future information in a time series is a useful pre-training mechanism  [14] .\n\nUnsupervised methods based on contrastive learning have established strong and feasible baselines in many domains, recently. For instance, contrastive predictive coding (CPC)  [11]  is able to extract useful representations from sequential data and achieves competitive performance on various tasks, including phone and speaker classification in speech. Our work relies on the use of a CPC network for learning acoustic representations from large unlabeled speech datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "The primary goal of this study is to learn representations that encode emotional attributes shared across frames of speech audios without supervision. We start by reviewing relevant concepts in emotion representation, then we give a brief review of the contrastive predictive coding (CPC) method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Representation",
      "text": "In general, there are two widely used approaches to represent emotion: by emotion categories (happiness, sadness, anger, etc.) or by dimensional emotion metrics (aka emotion primitives)  [3, 4, 15] . Albeit intuitive, the categories-based representation may miss the subtleties of emotion \"strength\", e.g. annoyance versus rage. The dimensional emotion metrics often include activation (aka arousal, very calm versus very active) , valence (level of positivity or negativity) and dominance (very weak versus very strong). In this work, we mainly focus on predicting dimensional emotion metrics from speech. Since emotion representation is an active research topic, we refer the interested readers to  [15, 16] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contrastive Predictive Coding",
      "text": "As the name suggests, CPC falls into the contrastive learning paradigm: positive example and negative examples are constructed, and the loss function encourages separation of positive from negative examples. We give a detailed description of CPC below.\n\nFor an audio sequence X = (x 1 , x 2 , ..., x n ), CPC uses a nonlinear encoder f to project observation x t ∈ R Dx to its latent representation z t = f (x t ), where z t ∈ R Dz . Then an autoregressive model g is adopted to aggregate t consecutive latent representations from the past into a contextual representation c t = g(z ≤t ), where c t ∈ R Dc .\n\nSince c t summarizes the past, it should be able to infer the latent representation z t+k of future observations x t+k from c t , for a small k. For this purpose, a prediction function h k for a specific k takes the context representation as the input to predict the future representation:\n\nTo form a contrastive learning problem, some negative samples (i.e. other observation x) are drawn, either from the same sequence or other sequences, and their latent representations (z) are computed.\n\nAssuming N -1 negatives are randomly sampled for each context representation, then positive and negatives form a set of N samples that contains only one positive and N -1 negatives. To guide feature learning, the CPC method proposes to discriminate the positive from negatives, which boils down to an N-way classification problem. CPC uses the infoNCE loss function: for an audio segment and a time step t, the infoNCE loss is defined as\n\nwhere τ is a scaling factor (a.k.a temperature) to control the concentration-level of the feature distribution, k is the upperbound on time extrapolation. Notice that the summation over i assumes that the randomly drawn negative samples are labeled as {1, ...N -1}, and these are different for each z t+m .\n\nIn addition, the loss function considers all the future time extrapolation up to k. Clearly, the loss (  2 ) is additive across different audio segments and time steps, hence in training, the loss (  2 ) is usually computed for batches of audio segments and all possible time steps in these segments, to utilize the mini-batch-based Adam  [17]  optimizer. Optimizing (2) results in larger inner product between a latent representation and its predicted counterpart, than any of the negatives -mismatched latent representation and predictions. Theoretical justification for the optimization objective function (2) can be found in  [11]  and  [18] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "The proposed method consists of two stages: pre-training a \"feature extractor\" model with CPC on a large un-labeled dataset, and training an emotion recognizer with features learned in the first stage. In this section, we introduce the emotion recognizer and training loss function.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attention-Based Emotion Recognizer",
      "text": "The output of CPC is a sequence of encoded vectors C = {c 1 , c 2 , ..., c L }, C ∈ R L×Dc . To predict primitive emotions for a certain speech utterance, an utterance-level embedding is desired. Since certain parts of an utterance are often more emotionally salient than others, we adopt a self-attention mechanism to focus on these periods for utilizing relevant features. Specifically, a structured self-attention  [19]  layer aggregates information from the output of CPC and produces a fixed-length vector u as the representation of the speech utterance.\n\nGiven C as input of the emotion recognizer, we follow  [19]  to compute the scaled dot-product attention representation H as\n\nwhere W Q , W K , and W V are trainable parameters, and all have shape D c × D attn . The subscripts Q, K, V stand for query, key, and value, as defined in  [19] .\n\nIn order to learn an embedding from multiple aspects, we use a multi-headed mechanism to process the input multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed to get the final embedding U ∈ R Du .\n\nwhere W O ∈ R nDattn×Du is another trainable weight matrix, and U ∈ R L×Du is the sequence representation after the multi-headed attention layer. Following the multi-headed attention layer, we compute the mean and standard deviation along the time dimension, and concatenate them as the sequence representation\n\nSubsequently, two dense layers with ReLU activation are used. We apply a dropout after these two dense layers with a small dropout probability. The final output layer is a dense layer with hidden units of the number of emotion attributes (e.g. three dimensions corresponding to activation, valence and dominance respectively).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Loss Function",
      "text": "Following  [20] , we build a loss function based on the concordance correlation coefficient (CCC,  [21] ). For two random variables X and Y , the CCC is defined as\n\nwhere ρ = σ XY σ X σ Y is the Pearson correlation coefficient, and µ and σ are the mean and standard deviation, respectively. As can be seen from (  7 ), CCC measures alignment of two random variables. In our setting, model predictions and data labels assume the role of X and Y in  (7) .\n\nSince the emotion recognizer predicts at the same time activation, valence and dominance, we use a loss function that combines CCC act , CCC val , CCC dom values for activation, valence, and dominance, respectively\n\nWe set the trade-off parameters α = β = γ = 1/3 in all our experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Corpora",
      "text": "For unsupervised pre-training, we train the CPC model on LibriSpeech dataset  [22] , which is a large scale corpus originally created for automatic speech recognition (ASR). It contains 1000 hours of English audiobook reading speech, sampled at 16kHz. In our experiment, due to computational limitations, we use an official subset \"train-clean-100\" containing 100 hours of clean speech for unsupervised pre-training. In this subset, 126 male and 125 female speaker were assigned to the training set. For each speaker, the amount of speech was limited to 25 minutes to avoid imbalances in per-speaker duration.\n\nTo evaluate the empirical emotion recognition performance, we perform experiments on the widely used MSP-Podcast dataset  [4]  and IEMOCAP dataset  [3] . MSP-Podcast is a database of spontaneous emotional speech. In our work, we used version 1.6 of the corpus, which contains 50,362 utterances amounting to 84 hours of audio recordings. Each utterance contains a single speaker with duration between 2.75s and 11s. We follow the official partition of the dataset, which has 34,280, 5,958, and 10,124 utterances in the training, validation and test sets, respectively. The dataset provides scores for activation, valence and dominance, as well as categorical emotion labels.\n\nIEMOCAP is a widely used corpus in SER research. It has audio-visual recordings from five male and five female actors. The actors were instructed to either improvise or act out certain specific emotions. The dataset contains 5,531 utterances grouped into 5 sessions, which amount to about 12 hours of audio. Similar to MSP-Podcast, this dataset provides categorical and dimensional emotion labels. In this work, we focus on predicting the dimensional emotion metrics from the speech data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiment Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Setups",
      "text": "Our experiments investigate four different setups: a). supervised only (Sup): As a simple baseline, an emotion recognizer was trained and tested on 40-dimensional log filterbank energies (LFBE) features of IEMOCAP and MSP-Podcast, respectively. LFBE features have been tested in a wide variety of applications. b). joint CPC + supervised (jointCPC): JointCPC trained CPC model and emotion recognizer in an end-to-end manner, where the CPC model aims to learn features from the raw audios directly, while the Sup setup uses hand-crafted features for the supervised task. We included this baseline to test whether it is possible to learn better features when the feature extraction part is aware of the downstream task. c). miniCPC: Compared with jointCPC, miniCPC trains the CPC model and the emotion recognizer in two separate stages on the same datasets. In this setup, we can verify whether CPC model can learn universal representations that can facilitate various downstream tasks. d). CPC pre-train + supervised (preCPC): We first pretrained a CPC model with a 100-hour subset of the Lib-riSpeech dataset. Then an attention-based emotion recognizer will be trained on features that were extracted from the learned CPC model with MSP-Podcast and IEMOCAP, re-spectively. Since the training corpus for CPC is much larger than the labeled datasets, we can test whether introducing a large out-of-domain dataset for unsupervised pretraining is useful.\n\nFor the CPC model used in the above settings, we use a four layer CNN with strides  [5, 4, 4, 2] , filter-sizes  [10, 8, 8, 4]  and 128 hidden units with ReLU activations to encode the 16KHz audio waveform inputs. A unidirectional gated recurrent unit (GRU) network with 256 hidden dimensions is used as the autoregressive model. For each output of GRU, we predict 12 timesteps in the future using 50 negative samples, sampled from the same sequence, in each prediction. We train the CPC model with fixed length utterances of 10s duration. Longer utterances are cut at 10s, and shorter ones were padded by repeating themselves.\n\nFor the emotion recognizer, an 8-head attention layer with 512 dimensional hidden states is used. The outputs of attention layer have the same dimension of the inputs. The two fully-connected layers have 128 hidden units. The drop out probability is set to 0.2 for the dropout layers.\n\nOur model was implemented in PyTorch and all methods were conducted on 8 GPUs each with a minibatch size of 8 examples for CPC pretraining. We use Adam optimizer with a weight decay of 0.00001 and a learning rate of 0.0002. We used 50 epochs for training and saved the model that perform best on validation set for testing.\n\nTo evaluate the IEMOCAP dataset, we configured 5-fold cross-validation to evaluate the model. All experiments were run five times to produce the means and standard deviations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Table  1  and 2 present the performance in terms of CCC for activation, valence and dominance on the IEMOCAP and MSP-Podcast corpora, respectively. As shown in these tables, on both datasets preCPC consistently outperforms other setups. preCPC achieves higher CCC values for all metrics than Sup, which implies that the representations learned by CPC are superior to hand-crafted features for speech emotion recognition task. Surprisingly, even pre-training the CPC model on a small dataset, miniCPC still performs better than jointCPC on both datasets. We hypothesize that this is because unsupervised pre-training learns universal representations that are less specialized towards solving a certain task. Hence, it produces representations with better generalization which might facilitate various downstream tasks. However, for the jointCPC method, a trade-off has to be made between emotion prediction capability and representation learning. Also notice that, preCPC outperforms miniCPC by a large margin. This confirms our intuition that exposing the model to more diverse acoustic conditions and speaker variations is beneficial for learning robust features.\n\nWe also plot the representations extracted by CPC from IEMOCAP to examine how suitable these representations are  for emotion. For visualization purposes, we used the categorical emotion labels when making the figure. As can be seen from Figure  1 , the CPC model representation is capable of separating sadness from anger to a good extent, even though it is trained without emotion labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Our experiment results demonstrated that CPC can learn useful features from unlabeled speech corpora that benefit emotion recognition. We have also observed significant performance improvement on widely used public benchmarks under various experiments setups, compared to baseline methods. Further, we also present a visualization that confirms the discriminative nature, with respect to emotion classes, of the CPC-learned representations. So far we mainly conducted experiments on LibriSpeech for pre-training. In the future, it would be interesting to investigate the impact of other corpora for pre-training. In particular, corpora that have more varied and expressive emotions might yield representations that are even more relevant for SER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualization of the learned representations",
      "page": 4
    },
    {
      "caption": "Figure 1: , the CPC model representation is capable of",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "danibone, wngcha}@amazon.com": "ABSTRACT"
        },
        {
          "danibone, wngcha}@amazon.com": "Speech emotion recognition (SER) is a key technology to"
        },
        {
          "danibone, wngcha}@amazon.com": "enable more natural human-machine communication. How-"
        },
        {
          "danibone, wngcha}@amazon.com": "ever, SER has long suffered from a lack of public large-scale"
        },
        {
          "danibone, wngcha}@amazon.com": "labeled datasets.\nTo circumvent\nthis problem, we investi-"
        },
        {
          "danibone, wngcha}@amazon.com": "gate how unsupervised representation learning on unlabeled"
        },
        {
          "danibone, wngcha}@amazon.com": "datasets can beneﬁt SER. We show that\nthe contrastive pre-"
        },
        {
          "danibone, wngcha}@amazon.com": "dictive coding (CPC) method can learn salient representations"
        },
        {
          "danibone, wngcha}@amazon.com": "from unlabeled datasets, which improves emotion recogni-"
        },
        {
          "danibone, wngcha}@amazon.com": "tion performance.\nIn our experiments,\nthis method achieved"
        },
        {
          "danibone, wngcha}@amazon.com": "state-of-the-art\nconcordance\ncorrelation\ncoefﬁcient\n(CCC)"
        },
        {
          "danibone, wngcha}@amazon.com": "performance for all emotion primitives (activation, valence,"
        },
        {
          "danibone, wngcha}@amazon.com": "and dominance) on IEMOCAP. Additionally, on the MSP-"
        },
        {
          "danibone, wngcha}@amazon.com": "Podcast dataset, our method obtained considerable perfor-"
        },
        {
          "danibone, wngcha}@amazon.com": "mance improvements compared to baselines."
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": "Index Terms— Speech emotion recognition, Contrastive"
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": "predictive coding, Unsupervised pre-training."
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": "1.\nINTRODUCTION"
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": "Speech emotion recognition (SER)\naims\nat discerning the"
        },
        {
          "danibone, wngcha}@amazon.com": ""
        },
        {
          "danibone, wngcha}@amazon.com": "emotional state of a speaker,\nthus enabling more human-like"
        },
        {
          "danibone, wngcha}@amazon.com": "interactions between human and machines.\nAn agent can"
        },
        {
          "danibone, wngcha}@amazon.com": "understand the command of a human better if it is able to in-"
        },
        {
          "danibone, wngcha}@amazon.com": "terpret\nthe emotional state of the speaker as well. Moreover,"
        },
        {
          "danibone, wngcha}@amazon.com": "a digital assistant can prove to be a human-like companion"
        },
        {
          "danibone, wngcha}@amazon.com": "when equipped with the capability of recognizing emotions."
        },
        {
          "danibone, wngcha}@amazon.com": "These fascinating applications provide key motivations un-"
        },
        {
          "danibone, wngcha}@amazon.com": "derpinning\nthe\nfast\ngrowing\nresearch\ninterest\nin\nthis\narea"
        },
        {
          "danibone, wngcha}@amazon.com": "[1, 2]"
        },
        {
          "danibone, wngcha}@amazon.com": "Despite the substantial\ninterest\nfrom both academia and"
        },
        {
          "danibone, wngcha}@amazon.com": "industry, SER has not\nfound many real-world applications."
        },
        {
          "danibone, wngcha}@amazon.com": "One possible reason is the unsatisfactory performance of ex-"
        },
        {
          "danibone, wngcha}@amazon.com": "isting systems. The difﬁculty is caused by, and contributes to,"
        },
        {
          "danibone, wngcha}@amazon.com": "relatively small public data sets [3, 4] in this domain. The lack"
        },
        {
          "danibone, wngcha}@amazon.com": "of large scale emotion annotated data hinders the application"
        },
        {
          "danibone, wngcha}@amazon.com": "of deep learning methods,\nfrom which many other speech-"
        },
        {
          "danibone, wngcha}@amazon.com": "related tasks (e.g automatic speech recognition [5]) have ben-"
        },
        {
          "danibone, wngcha}@amazon.com": "eﬁted greatly."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "relies on the use of a CPC network for learning acoustic rep-": "resentations from large unlabeled speech datasets.",
          "discriminate the positive from negatives, which boils down to": "an N-way classiﬁcation problem. CPC uses the infoNCE loss"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "function: for an audio segment and a time step t, the infoNCE"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "loss is deﬁned as"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "3. BACKGROUND",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "(cid:34)"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "exp(ˆz(cid:62)\nt+mzt+m)/τ"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "The primary goal of this study is to learn representations that",
          "discriminate the positive from negatives, which boils down to": "k(cid:88) m\nL = −\nlog"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "encode emotional attributes shared across frames of speech",
          "discriminate the positive from negatives, which boils down to": "exp(ˆz(cid:62)\nexp(ˆz(cid:62)\nt+mzt+m)/τ + (cid:80)N −1"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "=1"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "audios without supervision. We start by reviewing relevant",
          "discriminate the positive from negatives, which boils down to": "(2)"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "concepts in emotion representation,\nthen we give a brief re-",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "where τ is a scaling factor (a.k.a temperature) to control\nthe"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "view of the contrastive predictive coding (CPC) method.",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "concentration-level of the feature distribution, k is the upper-"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "bound on time extrapolation. Notice that the summation over"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "3.1. Emotion representation",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "i assumes that\nthe randomly drawn negative samples are la-"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "In general, there are two widely used approaches to represent",
          "discriminate the positive from negatives, which boils down to": "beled as {1, ...N − 1}, and these are different for each zt+m."
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "emotion:\nby emotion categories (happiness, sadness, anger,",
          "discriminate the positive from negatives, which boils down to": "In addition, the loss function considers all the future time ex-"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "etc.) or by dimensional emotion metrics (aka emotion prim-",
          "discriminate the positive from negatives, which boils down to": "trapolation up to k. Clearly, the loss (2) is additive across dif-"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "itives)\n[3, 4, 15]. Albeit\nintuitive,\nthe categories-based rep-",
          "discriminate the positive from negatives, which boils down to": "ferent audio segments and time steps, hence in training,\nthe"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "resentation may miss\nthe subtleties of emotion “strength”,",
          "discriminate the positive from negatives, which boils down to": "loss (2)\nis usually computed for batches of audio segments"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "e.g.\nannoyance versus rage. The dimensional emotion met-",
          "discriminate the positive from negatives, which boils down to": "and all possible time steps in these segments,\nto utilize the"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "rics often include activation (aka arousal, very calm versus",
          "discriminate the positive from negatives, which boils down to": "mini-batch-based Adam [17] optimizer."
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "very active)\n, valence (level of positivity or negativity) and",
          "discriminate the positive from negatives, which boils down to": "Optimizing (2)\nresults in larger inner product between a"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "dominance (very weak versus very strong).\nIn this work, we",
          "discriminate the positive from negatives, which boils down to": "latent representation and its predicted counterpart, than any of"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "mainly focus on predicting dimensional emotion metrics from",
          "discriminate the positive from negatives, which boils down to": "the negatives – mismatched latent representation and predic-"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "speech.\nSince emotion representation is an active research",
          "discriminate the positive from negatives, which boils down to": "tions. Theoretical\njustiﬁcation for the optimization objective"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "topic, we refer the interested readers to [15, 16].",
          "discriminate the positive from negatives, which boils down to": "function (2) can be found in [11] and [18]."
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "3.2. Contrastive predictive coding",
          "discriminate the positive from negatives, which boils down to": "4. PROPOSED METHOD"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "As the name suggests, CPC falls into the contrastive learning",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "The proposed method consists of\ntwo stages: pre-training a"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "paradigm: positive example and negative examples are con-",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "“feature extractor” model with CPC on a large un-labeled"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "structed, and the loss function encourages separation of pos-",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "dataset,\nand training an emotion recognizer with features"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "itive from negative examples. We give a detailed description",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "learned in the ﬁrst stage.\nIn this section, we introduce the"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "of CPC below.",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "emotion recognizer and training loss function."
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "For an audio sequence X = (x1, x2, ..., xn), CPC uses a",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "to its\nnonlinear encoder f to project observation xt ∈ RDx",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "4.1. Attention-based emotion recognizer"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "latent representation zt = f (xt), where zt ∈ RDz . Then an",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "autoregressive model g is adopted to aggregate t consecutive",
          "discriminate the positive from negatives, which boils down to": "The output of CPC is a sequence of encoded vectors C ="
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "latent\nrepresentations from the past\ninto a contextual\nrepre-",
          "discriminate the positive from negatives, which boils down to": "{c1, c2, ..., cL}, C ∈ RL×Dc. To predict primitive emotions"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "sentation ct = g(z≤t), where ct ∈ RDc.",
          "discriminate the positive from negatives, which boils down to": "for a certain speech utterance, an utterance-level embedding"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "Since ct summarizes the past, it should be able to infer the",
          "discriminate the positive from negatives, which boils down to": "is desired. Since certain parts of an utterance are often more"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "latent\nfrom\nrepresentation zt+k of\nfuture observations xt+k",
          "discriminate the positive from negatives, which boils down to": "emotionally salient\nthan others, we\nadopt\na\nself-attention"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "ct,\nfor a small k. For this purpose, a prediction function hk",
          "discriminate the positive from negatives, which boils down to": "mechanism to focus on these periods\nfor utilizing relevant"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "for a speciﬁc k takes the context representation as the input to",
          "discriminate the positive from negatives, which boils down to": "features.\nSpeciﬁcally, a structured self-attention [19]\nlayer"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "predict the future representation:",
          "discriminate the positive from negatives, which boils down to": "aggregates information from the output of CPC and produces"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "a ﬁxed-length vector u as\nthe representation of\nthe speech"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "(1)\nzt+k = hk(ct) = hk(g(z≤t)).",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "utterance."
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "Given C as input of\nthe emotion recognizer, we follow"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "To form a contrastive learning problem, some negative sam-",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "[19] to compute the scaled dot-product attention representa-"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "ples (i.e. other observation x) are drawn, either from the same",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "tion H as"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "sequence or other sequences, and their latent representations",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "",
          "discriminate the positive from negatives, which boils down to": "(cid:17)\n(cid:16)\n(cid:112)"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "(z) are computed.",
          "discriminate the positive from negatives, which boils down to": "(3)\nH = softmax\nDattn\nCWV\nCWQ(CWK)(cid:62)/"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "Assuming N −1 negatives are randomly sampled for each",
          "discriminate the positive from negatives, which boils down to": ""
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "context representation, then positive and negatives form a set",
          "discriminate the positive from negatives, which boils down to": "are trainable parameters, and all\nwhere WQ, WK, and WV"
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "of N samples that contains only one positive and N − 1 nega-",
          "discriminate the positive from negatives, which boils down to": "The subscripts Q, K, V\nstand for\nhave shape Dc × Dattn."
        },
        {
          "relies on the use of a CPC network for learning acoustic rep-": "tives. To guide feature learning, the CPC method proposes to",
          "discriminate the positive from negatives, which boils down to": "query, key, and value, as deﬁned in [19]."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In order to learn an embedding from multiple aspects, we": "use a multi-headed mechanism to process the input multiple",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "this subset, 126 male and 125 female speaker were assigned"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "times in parallel. The independent attention outputs are sim-",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "to the training set.\nFor each speaker,\nthe amount of speech"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "ply concatenated and linearly transformed to get the ﬁnal em-",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "was limited to 25 minutes to avoid imbalances in per-speaker"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "bedding U ∈ RDu .",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "duration."
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "To\nevaluate\nthe\nempirical\nemotion\nrecognition\nperfor-"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "(cid:17)\n(cid:16)\n(cid:112)",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "W j\nW j\nH j = softmax",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "(4)\nDattn\nV C\nQC(W j\nKC)(cid:62)/",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "mance, we perform experiments on the widely used MSP-"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "Podcast dataset [4] and IEMOCAP dataset [3]. MSP-Podcast"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "(5)\nU = Concat(H 1, H 2, ..., H n)WO",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "is a database of spontaneous emotional speech.\nIn our work,"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "is another\ntrainable weight ma-\nwhere WO ∈ RnDattn×Du",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "we used version 1.6 of the corpus, which contains 50,362 ut-"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "trix, and U ∈ RL×Du is the sequence representation after the",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "terances amounting to 84 hours of audio recordings. Each ut-"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "multi-headed attention layer.",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "terance contains a single speaker with duration between 2.75s"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "Following the multi-headed attention layer, we compute",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "and 11s. We follow the ofﬁcial partition of the dataset, which"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "the mean and standard deviation along the time dimension,",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "has 34,280, 5,958, and 10,124 utterances in the training, vali-"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "and concatenate them as the sequence representation",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "dation and test sets, respectively. The dataset provides scores"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "for activation, valence and dominance, as well as categorical"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "u = [ mean (U );\nstd (U )]\n(6)",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "emotion labels."
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "IEMOCAP is a widely used corpus in SER research.\nIt"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "Subsequently,\ntwo dense\nlayers with ReLU activation are",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "has audio-visual\nrecordings from ﬁve male and ﬁve female"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "used. We apply a dropout after these two dense layers with",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "actors. The actors were instructed to either improvise or act"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "a small dropout probability. The ﬁnal output\nlayer is a dense",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "out certain speciﬁc emotions. The dataset contains 5,531 ut-"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "layer with hidden units of\nthe number of emotion attributes",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "terances grouped into 5 sessions, which amount\nto about 12"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "(e.g.\nthree dimensions corresponding to activation, valence",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "hours of audio. Similar to MSP-Podcast, this dataset provides"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "and dominance respectively).",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "categorical and dimensional emotion labels.\nIn this work, we"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "focus on predicting the dimensional emotion metrics from the"
        },
        {
          "In order to learn an embedding from multiple aspects, we": "4.2. Loss function",
          "100 hours of clean speech for unsupervised pre-training.\nIn": ""
        },
        {
          "In order to learn an embedding from multiple aspects, we": "",
          "100 hours of clean speech for unsupervised pre-training.\nIn": "speech data."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "spectively. Since the training corpus for CPC is much larger": "than the labeled datasets, we can test whether introducing a",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "large out-of-domain dataset\nfor unsupervised pretraining is",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Methods\nCCC avg\nCCC act\nCCC val\nCCC dom"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "useful.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Sup\n.664 ± .007\n.638 ± .017\n.718 ± .004\n.635 ± .009"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "For the CPC model used in the above settings, we use a",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "jointCPC\n.562 ± .012\n.549 ± .032\n.642 ± .013\n.491 ± .016"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "four\nlayer CNN with strides [5, 4, 4, 2], ﬁlter-sizes [10, 8,",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "miniCPC\n.660 ± .005\n.673 ± .028\n.702 ± .009\n.606 ± .019"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "8, 4] and 128 hidden units with ReLU activations to encode",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "preCPC\n.731 ± .003\n.752 ± .014\n.752 ± .009\n.691 ± .009"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "the 16KHz audio waveform inputs. A unidirectional gated re-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "current unit\n(GRU) network with 256 hidden dimensions is",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Table 2: CCC scores (mean/std) on the MSP-Podcast dataset"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "used as the autoregressive model. For each output of GRU,",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "we predict 12 timesteps in the future using 50 negative sam-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Methods\nCCC avg\nCCC act\nCCC val\nCCC dom"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "ples, sampled from the same sequence, in each prediction. We",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "train the CPC model with ﬁxed length utterances of 10s dura-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Sup\n.458 ± .005\n.596 ± .007\n.266 ± .004\n.501 ± .013"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "tion. Longer utterances are cut at 10s, and shorter ones were",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "jointCPC\n.491 ± .008\n.628 ± .006\n.280 ± .006\n.568 ± .007"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "padded by repeating themselves.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "miniCPC\n.549 ± .006\n.688 ± .009\n.345 ± .005\n.615 ± .011"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "For the emotion recognizer, an 8-head attention layer with",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "preCPC\n.571 ± .004\n.706 ± .006\n.377 ± .008\n.639 ± .012"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "512 dimensional hidden states is used. The outputs of atten-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "tion layer have the same dimension of\nthe inputs.\nThe two",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "fully-connected layers have 128 hidden units. The drop out",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "40"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "probability is set to 0.2 for the dropout layers.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "Our model was implemented in PyTorch and all methods",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "20"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "were conducted on 8 GPUs each with a minibatch size of 8",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "0"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "examples for CPC pretraining. We use Adam optimizer with",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "a weight decay of 0.00001 and a learning rate of 0.0002. We",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "20"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "used 50 epochs for training and saved the model that perform",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "best on validation set for testing.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "40"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "anger"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "To evaluate the IEMOCAP dataset, we conﬁgured 5-fold",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "sadness"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "cross-validation to evaluate the model. All experiments were",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "60\n40\n20\n0\n20\n40\n60"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "run ﬁve times to produce the means and standard deviations.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Fig. 1: Visualization of the learned representations"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "6.2. Results",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "for emotion. For visualization purposes, we used the categor-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "Table 1 and 2 present the performance in terms of CCC for ac-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "ical emotion labels when making the ﬁgure. As can be seen"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "tivation, valence and dominance on the IEMOCAP and MSP-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "from Figure 1,\nthe CPC model\nrepresentation is capable of"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "Podcast corpora,\nrespectively. As shown in these tables, on",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "separating sadness from anger to a good extent, even though"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "both datasets preCPC consistently outperforms other setups.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "it is trained without emotion labels."
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "preCPC achieves higher CCC values for all metrics than Sup,",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "which implies that the representations learned by CPC are su-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "7. CONCLUSION"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "perior\nto hand-crafted features for speech emotion recogni-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "tion task.\nSurprisingly, even pre-training the CPC model on",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "Our experiment results demonstrated that CPC can learn use-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "a small dataset, miniCPC still performs better than jointCPC",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "ful features from unlabeled speech corpora that beneﬁt emo-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "on both datasets. We hypothesize that this is because unsuper-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "tion recognition. We have also observed signiﬁcant perfor-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "vised pre-training learns universal representations that are less",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "mance improvement on widely used public benchmarks un-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "specialized towards solving a certain task. Hence, it produces",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "der various experiments setups, compared to baseline meth-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "representations with better generalization which might facil-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "ods. Further, we also present a visualization that conﬁrms the"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "itate various downstream tasks. However,\nfor\nthe jointCPC",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "discriminative nature, with respect\nto emotion classes, of the"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "method, a trade-off has to be made between emotion predic-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "CPC-learned representations."
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "tion capability and representation learning. Also notice that,",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "So far we mainly conducted experiments on LibriSpeech"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "preCPC outperforms miniCPC by a large margin. This con-",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "for pre-training. In the future, it would be interesting to inves-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "ﬁrms our\nintuition that exposing the model\nto more diverse",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "tigate the impact of other corpora for pre-training.\nIn partic-"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "acoustic conditions and speaker variations\nis beneﬁcial\nfor",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "ular, corpora that have more varied and expressive emotions"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "learning robust features.",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "might yield representations\nthat are even more relevant\nfor"
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "We also plot\nthe representations extracted by CPC from",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": "SER."
        },
        {
          "spectively. Since the training corpus for CPC is much larger": "IEMOCAP to examine how suitable these representations are",
          "Table 1: CCC scores (mean/std) on the IEMOCAP dataset": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "man,\n“Unsupervised\nlearning\napproach\nto\nfeature"
        },
        {
          "8. REFERENCES": "[1] George Trigeorgis, Fabien Ringeval, Raymond Brueck-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "analysis\nfor\nautomatic\nspeech\nemotion\nrecognition,”"
        },
        {
          "8. REFERENCES": "ner, Erik Marchi, Mihalis A Nicolaou, Bj¨orn Schuller,",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "in 2018 IEEE International Conference on Acoustics,"
        },
        {
          "8. REFERENCES": "and Stefanos Zafeiriou,\n“Adieu features?\nEnd-to-end",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "Speech and Signal Processing (ICASSP).\nIEEE, 2018,"
        },
        {
          "8. REFERENCES": "speech emotion recognition using a deep convolutional",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "pp. 5099–5103."
        },
        {
          "8. REFERENCES": "recurrent network,”\nin 2016 IEEE International Con-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[13]\nJun Deng, Rui Xia,\nZixing Zhang, Yang Liu,\nand"
        },
        {
          "8. REFERENCES": "ference on Acoustics,\nSpeech and Signal Processing",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "Bj¨orn Schuller,\n“Introducing shared-hidden-layer au-"
        },
        {
          "8. REFERENCES": "(ICASSP). IEEE, 2016, pp. 5200–5204.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "toencoders for transfer learning and their application in"
        },
        {
          "8. REFERENCES": "[2] Bj¨orn W Schuller,\n“Speech emotion recognition: Two",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "acoustic emotion recognition,”\nin 2014 IEEE Interna-"
        },
        {
          "8. REFERENCES": "decades in a nutshell, benchmarks, and ongoing trends,”",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "tional Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "8. REFERENCES": "Communications of the ACM, vol. 61, no. 5, pp. 90–99,",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "cessing (ICASSP). IEEE, 2014, pp. 4818–4822."
        },
        {
          "8. REFERENCES": "2018.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[14] Zheng Lian, Jianhua Tao, Bin Liu, and Jian Huang, “Un-"
        },
        {
          "8. REFERENCES": "[3] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "supervised representation learning with future observa-"
        },
        {
          "8. REFERENCES": "Kazemzadeh,\nEmily Mower\nProvost,\nSamuel Kim,",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "arXiv"
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "tion prediction for speech emotion recognition,”"
        },
        {
          "8. REFERENCES": "Jeannette N. Chang, Sungbok Lee,\nand Shrikanth S.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "preprint arXiv:1910.13806, 2019."
        },
        {
          "8. REFERENCES": "Narayanan,\n“IEMOCAP:\ninteractive emotional dyadic",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[15] Roddy Cowie and Randolph R Cornelius,\n“Describ-"
        },
        {
          "8. REFERENCES": "Language Resources and\nmotion capture database,”",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "ing the emotional states that are expressed in speech,”"
        },
        {
          "8. REFERENCES": "Evaluation, vol. 42, pp. 335–359, 2008.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "Speech Communication,\nvol.\n40,\nno.\n1-2,\npp.\n5–32,"
        },
        {
          "8. REFERENCES": "[4] Reza Lotﬁan and Carlos Busso,\n“Building naturalistic",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "2003."
        },
        {
          "8. REFERENCES": "emotionally balanced speech corpus by retrieving emo-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[16] Georgios N Yannakakis, Roddy Cowie,\nand Carlos"
        },
        {
          "8. REFERENCES": "IEEE\ntional speech from existing podcast recordings,”",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "Busso,\n“The ordinal nature of emotions: An emerging"
        },
        {
          "8. REFERENCES": "Transactions on Affective Computing, vol. 10, pp. 471–",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "approach,” IEEE Transactions on Affective Computing,"
        },
        {
          "8. REFERENCES": "483, 2019.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "2018."
        },
        {
          "8. REFERENCES": "AUTOMATIC SPEECH\n[5] Dong Yu\nand\nLi Deng,",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[17] Diederik\nP Kingma\nand\nJimmy Ba,\n“Adam:\nA"
        },
        {
          "8. REFERENCES": "RECOGNITION, Springer, 2016.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "arXiv preprint\nmethod for\nstochastic optimization,”"
        },
        {
          "8. REFERENCES": "[6] Alan Hanjalic,\n“Extracting moods\nfrom pictures and",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "arXiv:1412.6980, 2014."
        },
        {
          "8. REFERENCES": "IEEE Signal\nsounds: Towards truly personalized TV,”",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "Processing Magazine, vol. 23, no. 2, pp. 90–100, 2006.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[18] Ben\nPoole,\nSherjil\nOzair,\nAaron\nvan\nden\nOord,"
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "A. Alemi, and G. Tucker,\n“On variational bounds of"
        },
        {
          "8. REFERENCES": "[7] Emily Mower, Angeliki Metallinou, Chi-Chun Lee,",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "mutual information,” in ICML, 2019."
        },
        {
          "8. REFERENCES": "Abe Kazemzadeh, Carlos Busso,\nSungbok Lee,\nand",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "Shrikanth Narayanan,\n“Interpreting ambiguous emo-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[19] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "8. REFERENCES": "tional expressions,”\nin 2009 3rd International Confer-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,"
        },
        {
          "8. REFERENCES": "ence on Affective Computing and Intelligent Interaction",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "and Illia Polosukhin, “Attention is all you need,” in Ad-"
        },
        {
          "8. REFERENCES": "and Workshops. IEEE, 2009, pp. 1–8.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "vances in Neural Information Processing Systems, 2017,"
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "pp. 5998–6008."
        },
        {
          "8. REFERENCES": "[8] Tom B. Brown et.al.,\n“Language models are few-shot",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "learners,” ArXiv, vol. abs/2005.14165, 2020.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[20] Felix Weninger,\nFabien Ringeval, Erik Marchi,\nand"
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "Bj¨orn W Schuller,\n“Discriminatively trained recurrent"
        },
        {
          "8. REFERENCES": "[9]\nJacob Devlin, Ming-Wei Chang, Kenton\nLee,\nand",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "neural networks\nfor\ncontinuous dimensional\nemotion"
        },
        {
          "8. REFERENCES": "Kristina Toutanova,\n“BERT: Pre-training of deep bidi-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "recognition from audio.,” in IJCAI, 2016, vol. 2016, pp."
        },
        {
          "8. REFERENCES": "rectional\ntransformers for language understanding,”\nin",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "2196–2202."
        },
        {
          "8. REFERENCES": "NAACL-HLT, 2019.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[21] Lawrence I-Kuei Lin, “A concordance correlation coef-"
        },
        {
          "8. REFERENCES": "[10] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "ﬁcient to evaluate reproducibility,” Biometrics, pp. 255–"
        },
        {
          "8. REFERENCES": "Ross Girshick,\n“Momentum contrast for unsupervised",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "268, 1989."
        },
        {
          "8. REFERENCES": "the\nvisual\nrepresentation learning,”\nin Proceedings of",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "IEEE/CVF Conference on Computer Vision and Pattern",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "[22] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-"
        },
        {
          "8. REFERENCES": "Recognition, 2020, pp. 9729–9738.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "jeev Khudanpur,\n“Librispeech: An ASR corpus based"
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "2015 IEEE Interna-\non public domain audio books,”"
        },
        {
          "8. REFERENCES": "[11] A¨aron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "tional Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "8. REFERENCES": "resentation learning with contrastive predictive coding,”",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        },
        {
          "8. REFERENCES": "",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": "cessing (ICASSP), pp. 5206–5210, 2015."
        },
        {
          "8. REFERENCES": "ArXiv, vol. abs/1807.03748, 2018.",
          "[12] Seﬁk Emre Eskimez, Zhiyao Duan, and Wendi Heinzel-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "AUTOMATIC SPEECH RECOGNITION",
      "authors": [
        "Dong Yu",
        "Li Deng"
      ],
      "year": "2016",
      "venue": "AUTOMATIC SPEECH RECOGNITION"
    },
    {
      "citation_id": "7",
      "title": "Extracting moods from pictures and sounds: Towards truly personalized TV",
      "authors": [
        "Alan Hanjalic"
      ],
      "year": "2006",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "8",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "Emily Mower",
        "Angeliki Metallinou",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Carlos Busso",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "9",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown"
      ],
      "year": "2005",
      "venue": "ArXiv"
    },
    {
      "citation_id": "10",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "11",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aäron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "Zhiyao Sefik Emre Eskimez",
        "Wendi Duan",
        "Heinzelman"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Introducing shared-hidden-layer autoencoders for transfer learning and their application in acoustic emotion recognition",
      "authors": [
        "Jun Deng",
        "Rui Xia",
        "Zixing Zhang",
        "Yang Liu",
        "Björn Schuller"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised representation learning with future observation prediction for speech emotion recognition",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang"
      ],
      "year": "2019",
      "venue": "Unsupervised representation learning with future observation prediction for speech emotion recognition",
      "arxiv": "arXiv:1910.13806"
    },
    {
      "citation_id": "16",
      "title": "Describing the emotional states that are expressed in speech",
      "authors": [
        "Roddy Cowie",
        "Randolph Cornelius"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "17",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "Roddy Georgios N Yannakakis",
        "Carlos Cowie",
        "Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "19",
      "title": "On variational bounds of mutual information",
      "authors": [
        "Ben Poole",
        "Sherjil Ozair",
        "Aaron Van Den Oord",
        "A Alemi",
        "G Tucker"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio",
      "authors": [
        "Felix Weninger",
        "Fabien Ringeval",
        "Erik Marchi",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "IJCAI"
    },
    {
      "citation_id": "22",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I-Kuei Lawrence",
        "Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "23",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}