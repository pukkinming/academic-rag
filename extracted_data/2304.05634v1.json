{
  "paper_id": "2304.05634v1",
  "title": "How You Feelin'? Learning Emotions And Mental States In Movie Scenes",
  "published": "2023-04-12T06:31:14Z",
  "authors": [
    "Dhruv Srivastava",
    "Aditya Kumar Singh",
    "Makarand Tapaswi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Movie story analysis requires understanding characters' emotions and mental states. Towards this goal, we formulate emotion understanding as predicting a diverse and multi-label set of emotions at the level of a movie scene and for each character. We propose EmoTx, a multimodal Transformer-based architecture that ingests videos, multiple characters, and dialog utterances to make joint predictions. By leveraging annotations from the MovieGraphs dataset [74], we aim to predict classic emotions (e.g. happy, angry) and other mental states (e.g. honest, helpful). We conduct experiments on the most frequently occurring 10 and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies and comparison against adapted stateof-the-art emotion recognition approaches shows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scores reveals that expressive emotions often look at character tokens while other mental states rely on video and dialog cues.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the movie The Pursuit of Happyness, we see the protagonist experience a roller-coaster of emotions from the lows of breakup and homelessness to the highs of getting selected for a coveted job. Such heightened emotions are often useful to draw the audience in through relatable events as one empathizes with the character(s). For machines to understand such a movie (broadly, story), we argue that it is paramount to track how characters' emotions and mental states evolve over time. Towards this goal, we leverage annotations from MovieGraphs  [74]  and train models to watch the video, read the dialog, and predict the emotions and mental states of characters in each movie scene.\n\nEmotions are a deeply-studied topic. From ancient Rome and Cicero's 4-way classification  [62] , to modern brain research  [33] , emotions have fascinated humanity. Psychologists use of Plutchik's wheel  [54]  or the proposal of universality in facial expressions by Ekman  [18] , structure has been provided to this field through various theories. Affective emotions are also grouped into mental (affective, be- Check the footnote below for the ground-truth emotion labels for these scenes and Appendix A for an explanation of the story.\n\nhavioral, and cognitive) or bodily states  [13] .\n\nA recent work on recognizing emotions with visual context, Emotic  [31]  identifies 26 label clusters and proposes a multi-label setup wherein an image may exhibit multiple emotions (e.g. peace, engagement). An alternative to the categorical space, valence, arousal, and dominance are also used as three continuous dimensions  [31] .\n\nPredicting a rich set of emotions requires analyzing multiple contextual modalities  [31, 34, 45] . Popular directions in multimodal emotion recognition are Emotion Recognition in Conversations (ERC) that classifies the emotion for every dialog utterance  [42, 55, 85] ; or predicting a single valence-activity score for short ∼10s movie clips  [4, 46] .\n\nWe operate at the level of a movie scene: a set of shots telling a sub-story, typically at one location, among a defined cast, and in a short time span of 30-60s. Thus, scenes are considerably longer than single dialogs  [55]  or movie Ground-truth emotions and mental states portrayed in movie scenes in Fig.  1 : A: excited, curious, confused, annoyed, alarmed; B: shocked, confident; C: happy, excited, amused, shocked, confident, nervous.\n\nclips in  [4] . We predict emotions and mental states for all characters in the scene and also by accumulating labels at the scene level. Estimation on a larger time window naturally lends itself to multi-label classification as characters may portray multiple emotions simultaneously (e.g. curious and confused) or have transitions due to interactions with other characters (e.g. worried to calm).\n\nWe perform experiments with multiple label sets: Top-10 or 25 most frequently occurring emotion labels in MovieGraphs  [74]  or a mapping to the 26 labels in the Emotic space, created by  [46] . While emotions can broadly be considered as part of mental states, for this work, we consider that expressed emotions are apparent by looking at the character, e.g. surprise, sad, angry; and mental states are latent and only evident through interactions or dialog, e.g. polite, determined, confident, helpful 1 . We posit that classification in a rich label space of emotions requires looking at multimodal context as evident from masking context in Fig.  1 . To this end, we propose EmoTx that jointly models video frames, dialog utterances, and character appearance.\n\nWe summarize our contributions as follows: (i) Building on rich annotations from MovieGraphs  [74] , we formulate scene and per-character emotion and mental state classification as a multi-label problem. (ii) We propose a multimodal Transformer-based architecture EmoTx that predicts emotions by ingesting all information relevant to the movie scene. EmoTx is also able to capture label co-occurrence and jointly predicts all labels. (iii) We adapt several previous works on emotion recognition for this task and show that our approach outperforms them all. (iv) Through analysis of the self-attention mechanism, we show that the model learns to look at relevant modalities at the right time. Selfattention scores also shed light on our model's treatment of expressive emotions vs. mental states.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "We first present work on movie understanding and then dive into visual and multimodal emotion recognition. Movie understanding has evolved over the last few years from person clustering and identification  [6, 7, 19, 29, 47, 67]  to analyzing the story. Scene detection  [11, 56, 57, 59, 68] , question-answering  [35, 70, 79] , movie captioning  [58, 80]  with names  [51] , modeling interactions and/or relationships  [21, 32, 43] , alignment of text and video storylines  [69, 78, 86]  and even long-form video understanding  [77]  have emerged as exciting areas. Much progress has been made through datasets such as Condensed Movies  [3] , MovieNet  [27] , VALUE benchmark (goes beyond movies)  [37] , and MovieGraphs  [74] . Building on the 1 Admittedly it is not always easy or possible to categorize a label as an expressed emotion or a mental state, e.g. cheerful, upset. Using Clore et al.  [13] 's classification, expressed emotions refer to affective and bodily states, while our mental states refer to behavioral and cognitive states. annotations from MovieGraphs  [74] , we focus on another pillar of story understanding complementary to the above directions: identifying the emotions and mental states of each character and the overall scene in a movie.\n\nVisual emotion recognition has relied on face-based recognition of Ekman's 6 classic emotions  [18] , and was popularized through datasets such as MMI  [50] , CK and CK+  [41, 72] . A decade ago, EmotiW  [16] , FER  [24] , and AFEW  [15]  emerged as challenging in-the-wild benchmarks. At the same time, approaches such as  [38, 39]  introduced deep learning to expression recognition achieving good performance. Breaking away from the above pattern, the Emotic dataset  [31]  introduced the use of 26 labels for emotion understanding in images while highlighting the importance of context. Combining face features and context using two-stream CNNs  [34]  or person detections with depth maps  [45]  were considered. Other directions in emotion recognition include estimating valence-arousal (continuous variables) from faces with limited context  [71] , learning representations through webly supervised data to overcome biases  [49]  or improving them further through a joint text-vision embedding space  [75] . Different from the above, our work focuses on multi-label emotions and mental states recognition in movies exploiting multimodal context both at the scene-and character-level.\n\nMultimodal datasets for emotion recognition have seen recent adoption. Acted Facial Expressions in the Wild  [15]  aims to predict emotions from faces, but does not provide any context. The Stanford Emotional Narratives Dataset  [48]  contains participant shared narratives of positive/negative events in their lives. While multimodal, these are quite different from edited movies and stories that are our focus. The Multimodal EmotionLines Dataset (MELD)  [55]  is an example of Emotion Recognition in Conversations (ERC) and attempts to estimate the emotion for every dialog utterance in TV episodes from Friends. Different from MELD, we operate at the time-scale of a cohesive story unit, a movie scene. Finally, closest to our work, Annotated Creative Commons Emotional DatabasE (LIRIS-ACCEDE)  [4]  obtains emotion annotations for short movie clips. However, the clips are quite small (8-12s) and annotations are obtained in the continuous valence-arousal space. Different from the above works, we also aim to predict character-level mental states and demonstrate that video and dialog context helps for such labels.\n\nMultimodal emotion recognition methods. RNNs have been used since early days for ERC  [28, 42, 64, 76]  (often with graph networks  [23, 82] ) as they allow effective combination of audio, visual, and textual data. Inspired by recent advances, Transformer architectures are also adopted for ERC  [12, 63] . External knowledge graphs provide useful commonsense information  [22]  while topic modeling integrated with Transformers have improved results  [85] . Multi-label prediction has also been attempted by considering a sequence-to-set approach  [81] , however that may not scale with number of labels. While we adopt a Transformer for joint modeling, our goal to predict emotions and mental states for movie scenes and characters is different from ERC. We adapt some of the above methods and compare against them in our experiments. Close to our work, the MovieGraphs  [74]  emotion annotations are used to model changing emotions across the entire movie  [46] , and for Temporal Emotion Localization  [36] . However, the former tracks one emotion in each scene, while the latter proposes a different, albeit interesting direction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "EmoTx leverages the self-attention mechanism in Transformers  [73]  to predict emotions and mental states. We first define the task (Sec. 3.1) and then describe our proposed approach (Sec. 3.2), before ending this section with details regarding training and inference (Sec. 3.3).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Statement",
      "text": "We assume that movies have been segmented automatically  [56]  or with a human-in-the-loop process  [68, 74]  into coherent scenes that are self-contained and describe a short part of the story. The focus of this work is on characterizing emotions within a movie scene that are often quite long (30-60s) and may contain several tens of shot changes.\n\nConsider such a movie scene S that consists of a set of video frames V, characters C, and dialog utterances U. Let us denote the set of video frames as V = {f t } T t=1 , where T is the number of frames after sub-sampling. Multiple characters often appear in any movie scene. We model N characters in the scene as C = {P i } N i=1 , where each character P i = {(f t , b i t )} may appear in some frame f t of the video at the spatial bounding box b i t . We assume that b i t is empty if the character P i does not appear at time t. Finally, U = {u j } M j=1 captures the dialog utterances in the scene. For this work, we use dialogs directly from subtitles and thus assume that they are unnamed. While dialogs may be named through subtitle-transcript alignment  [19] , scripts are not always available or reliable for movies. Task formulation. Given a movie scene S with its video, character, and dialog utterance, we wish to predict the emotions and mental states (referred as labels, or simply emotions) at both the scene, y V , and per-character, y P i , level. We formulate this as a multi-label classification problem with K labels, i.e. y = {y k } K k=1 . Each y k ∈ {0, 1} indicates the absence or presence of the k th label in the scene y V k or portrayed by some character y P i k . For datasets with character-level annotations, scene-level labels are obtained through a simple logical OR operation, i.e. y V = N i=1 y P i .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotx: Our Approach",
      "text": "We present EmoTx, our Transformer-based method that recognizes emotions at the movie scene and per-character level. A preliminary video pre-processing and feature extraction pipeline extracts relevant representations. Then, a Transformer encoder combines information across modalities. Finally, we adopt a classification module inspired by previous work on multi-label classification with Transformers  [40] . An overview of the approach is presented in Fig.  2 . Preparing multimodal representations. Recognizing complex emotions and mental states (e.g. nervous, determined) requires going beyond facial expressions to understand the larger context of the story. To facilitate this, we encode multimodal information through multiple lenses: (i) the video is encoded to capture where and what event is happening; (ii) we detect, track, cluster, and represent characters based on their face and/or full-body appearance; and (iii) we encode the dialog utterances as information complementary to the visual domain.\n\nA pretrained encoder ϕ V extracts relevant visual information from a single or multiple frames as\n\nSimilarly, a pretrained language model ϕ U extracts dialog utterance representations as u j = ϕ U (u j ). Characters are more involved as we need to first localize them in the appropriate frames. Given a valid bounding box b i t for person P i , we extract character features using a backbone pretrained for emotion recognition as c i t = ϕ C (f t , b i t ). Linear projection. Token representations in a Transformer often combine the core information (e.g. visual representation) with meta information such as the timestamp through position embeddings (e.g.  [65] ). We first bring all modalities to the same dimension with linear layers. Specifically, we project visual representation\n\nWe omit linear layer biases for brevity. Modality embeddings. We learn three embedding vectors E M ∈ R D×3 to capture the three modalities corresponding to (1) video, (2) characters, and (3) dialog utterances. We also assist the model in identifying tokens coming from characters by including a special character count embedding, E C ∈ R D×N . Note that the modality and character embeddings do not encode any specific meaning or imposed order (e.g. higher to lower appearance time, names in alphabetical order) -we expect the model to use this only to distinguish one modality/character from the other. Time embeddings. The number of tokens depend on the chosen frame-rate. To inform the model about relative temporal order across modalities, we adopt a discrete time binning strategy that translates real time (in seconds) to an index. Thus, video frame/segment and character box representations fed to the Transformer are associated with their  We present the detailed approach in Sec. 3 but provide a short summary here. A: Video features (in blue region), character face features (in purple region), and utterance features (in orange region) are obtained using frozen backbones and projected with linear layers into a joint embedding space. B: Here appropriate embeddings are added to the tokens to distinguish between modalities, character count, and to provide a sense of time. We also create per-emotion classifier tokens associated with the scene or a specific character. C: Two Transformer encoder layers perform self-attention across the sequence of input tokens. D: Finally, we tap the classifier tokens to produce output probability scores for each emotion through a linear classifier shared across the scene and characters.\n\nrelevant time bins. For an utterance u j , binning is done based on its middle timestamp t j . We denote the time embeddings as E T ∈ R D×⌈T * /τ ⌉ , where T * is the maximum scene duration and τ is the bin step. For convenience, E T t selects the embedding using a discretized index ⌈t/τ ⌉. Classifier tokens. Similar to the classic CLS tokens in Transformer models  [17, 87]  we use learnable classifier tokens to predict the emotions. Furthermore, inspired by Query2Label  [40] , we use K classifier tokens rather than tapping a single token to generate all outputs (see Fig.  2D ). This allows capturing label co-occurrence within the Transformer layers improving performance. It also enables analysis of per-emotion attention scores providing insights into the model's workings. In particular, we use K classifier tokens for scene-level predictions (denoted z S k ) and N × K tokens for character-level predictions (denoted z i k for character P i , one for each character-emotion pair). Token representations. Combining the features with relevant embeddings provides rich information to EmoTx. The token representations for each input group are as follows: scene cls. tokens:\n\nchar. cls. tokens:\n\nFig.  2B  illustrates this addition of embedding vectors. We also perform LayerNorm  [2]  before feeding the tokens to the Transformer encoder layers, not shown for brevity.\n\nTransformer Self-attention. We concatenate and pass all tokens through H=2 layers of the Transformer encoder that computes self-attention across all modalities  [73] . For emotion prediction, we only tap the outputs corresponding to the classification tokens as\n\nWe jointly encode all tokens spanning {k} K 1 , {i} N 1 , {t} T 1 , and {j} M 1 . Emotion labeling. The contextualized representations for the scene ẑS k and characters ẑi k are sent to a shared linear layer W E ∈ R K×D for classification. Finally, the probability estimates through a sigmoid activation σ(•) are:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training And Inference",
      "text": "Training. EmoTx is trained in an end-to-end fashion with the BinaryCrossEntropy (BCE) loss. To account for the class imbalance we provide weights ω k for the positive labels based on inverse of proportions. The scene and character prediction losses are combined as    Inference. At test time, we follow the procedure outlined in Sec. 3.2 and generate emotion label estimates for the entire scene and each character as indicated in Eq. 7.\n\nVariations. As we will see empirically, our model is very versatile and well suited for adding/removing modalities or additional representations by adjusting the width of the Transformer (number of tokens). It can be easily modified to act as a unimodal architecture that applies only to video or dialog utterances by disregarding other modalities.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Discussion",
      "text": "We present our experimental setup in Sec. 4.1 before diving into the implementation details in Sec. 4.2. A series of ablation studies motivate the design choices of our model (Sec. 4.3) while we compare against the adapted versions of various SoTA models for emotion recognition in Sec. 4.4. Finally, we present some qualitative analysis and discuss how our model switches from facial expressions to video or dialog context depending on the label in Sec. 4.5.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset And Setup",
      "text": "We use the MovieGraphs dataset  [74]  that features 51 movies and 7637 movie scenes with detailed graph annotations. We focus on the list of characters and their emotions and mental states, which naturally affords a multilabel setup. Other annotations such as the situation label, or character interactions and relationships  [32]  are ignored as they cannot be assumed to be available for a new movie.\n\nLabel sets. Like other annotations in the MovieGraphs dataset, emotions are also obtained as free-text leading to a huge variability and a long-tail of labels (over 500). We focus our experiments on three types of label sets: (i) Top-10 considers the most frequently occurring 10 emotions; (ii) Top-25 considers frequently occurring 25 labels; and (iii) Emotic, a mapping from 181 MovieGraphs emotions to 26 Emotic labels provided by  [46] .\n\nStatistics. We first present row max-normalized cooccurrence matrices for the scene and characters (Fig.  3 ). It is interesting to note how a movie scene has high cooccurrence scores for emotions such as worried and calm (perhaps owing to multiple characters), while worried is most associated with confused for a single character. Another high scoring example for a single character is curious and surprise, while a movie scene has curious with calm and surprise with happy. In Fig.  4 , we show the number of movie scenes that contain a specified number of emotions. Most scenes have 4 emotions. Appendix B features further analysis.\n\nEvaluation metric. We use the original splits from MovieGraphs. As we have K binary classification problems, we adopt mean Average Precision (mAP) to measure model performance (similar to Atomic Visual Actions  [25] ). Note that AP also depends on the label frequency.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Feature representations play a major role on the performance of any model. We describe different backbones used to extract features for video frames, characters, and dialog.\n\nVideo features f t : The visual context is important for understanding emotions  [31, 34, 45] . We extract spatial features using ResNet152  [26]  trained on ImageNet  [61] , ResNet50  [26]  trained on Place365  [84] , and spatiotemporal features, MViT  [20]  trained on Kinetics400  [10] .\n\nDialog features u j : Each utterance is passed through a RoBERTa-Base encoder  [87]  to obtain an utterance-level embedding. We also extract features from a RoBERTa model fine-tuned for the task of multi-label emotion classification (based on dialog only).\n\nCharacter features c i t : are represented based on face or person detections. We perform face detection with MTCNN  [83]  and person detection with Cascade RCNN  [8]  trained on MovieNet  [27] . Tracks are obtained using SORT  [5] , a simple Kalman filter based algorithm, and clusters using C1C  [29] . Details of the character processing pipeline are presented in Appendix C. ResNet50  [1]  trained on SFEW  [14]  and pretrained on FER13  [24]  and VGGFace  [52] , VGGm  [1]  trained on FER13 and pretrained on VGGFace, and InceptionResnetV1  [66]  trained on VG-GFace2  [9]  are used to extract face representations.\n\nFrame sampling strategy. We sample up to T =300 tokens Training details. Our model is implemented in Py-Torch  [53]  and trained on a single NVIDIA GeForce RTX-2080 Ti GPU for a maximum of 50 epochs with a batch size of 8. The hyperparameters are tuned to achieve best performance on validation set. We adopt the Adam optimizer  [30]  with an initial learning rate of 5 × 10 -5 , reduced by a factor of 10 using the learning rate scheduler ReduceLROnPlateau. The best checkpoint maximizes the geometric mean of scene and character mAP.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "We perform ablations across three main dimensions: architectures, modalities, and feature backbones. When not mentioned, we adopt the defaults: (i) MViT trained on Ki-netics400 dataset to represent video; (ii) ResNet50 trained on SFEW, FER, and VGGFace for character representations; (iii) fine-tuned RoBERTa for dialog utterance representations; and (iv) EmoTx with appropriate masking to pick modalities or change the number of classifier tokens.\n\nArchitecture ablations. We compare our architecture Finally, we compare multimodal EmoTx that uses 1 classifier token to predict all labels (EmoTx: 1 CLS) against K classifier tokens (last row). Both models achieve significant improvements, e.g. in absolute points, +8.5% for Top-10 scene labels and +2.3% for the much harder Top-25 character level labels. We believe the improvements reflect EmoTx's ability to encode multiple modalities in a meaningful way. Additionally, the variant with K classifier tokens (last row) shows small but consistent +0.5% improvements over 1 classifier token on Top-25 emotions.\n\nFig.  5  shows the scene-level AP scores for the Top-25 labels. Our model outperforms the MLP and Single Tx encoder on 24 of 25 labels and outperforms the single classifier token variant on 15 of 25 labels. EmoTx is good at recognizing expressive emotions such as excited, serious, happy and even mental states such as friendly, polite, worried. However, other mental states such as determined or helpful are challenging.\n\nModality ablations. We evaluate the impact of each modality (video, characters, and utterances) on scene-and character-level emotion prediction in Table  2 . We observe that the character modality (row 4, R4) outperforms any of the video or dialog modalities (R1-R3). Similarly, dialog features (R3) are better than video features (R1, R2), common in movie understanding tasks  [70, 74] . The choice of visual features is important. Scene features V r are consis-  tently worse than action features V m as reflected in comparisons R1, R2 or R5, R6 or R8, R9. Finally, we observe that using all modalities (R9) outperforms other combinations, indicating that emotion recognition is a multimodal task. Backbone ablations. We compare several backbones for the task of emotion recognition. The effectiveness of the fine-tuned RoBERTa model is evident by comparing pairs of rows R2, R5 and R3, R7 and R4, R8 of Table  3 , where we see a consistent improvement of 1-3%. Character representations with ResNet50-FER show improvement over VGGm-FER as seen from R5, R8 or R6, R7. Finally, comparing R8 shows the benefits provided by action features as compared to places. Details are presented in Appendix E.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Sota Comparison",
      "text": "We compare our model against published works Emo-tionNet  [75] , CAER  [34] , AttendAffectNet  [71] , and M2Fnet  [12]  by adapting them for our tasks (adaptation details are provided in Appendix F).\n\nTable  4  shows scene-level performance while the character-level performance is presented in",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Analyzing Self-Attention Scores",
      "text": "EmoTx provides an intuitive way to understand which modalities are used to make predictions. We refer to the self-attention scores matrix as α, and analyze specific rows and columns. Separating the K classifier tokens allows us to find attention-score based evidence for each predicted emotion by looking at a row α z S k in the matrix. Fig.  6  shows an example movie scene where EmoTx predicts that Forrest is happy and Mrs. Gump is worried. We see that the model pays attention to the appropriate moments and modalities to make the right predictions.\n\nExpressive emotions vs. Mental states. We hypothesize that the self-attention module may focus on character tokens for expressive emotions, while looking at the overall video frames and dialog for the more abstract mental states. We propose an expressiveness score as\n\nwhere α z S k ,c i t is the self-attention score between the scene classifier token for emotion k (z S k ) and character P i 's appearance in the video frame as b i t ; α z S k ,ft is for the video f t and α z S k ,uj is for dialog utterance u j . Higher scores indicate expressive emotions as the model focuses on the character features, while lower scores identify mental states that analyze the video and dialog context. Fig.  7  shows the averaged expressiveness score for the Top-25 emotions when the emotion is present in the scene (i.e. y k =1). We observe that mental states such as honest, helpful, friendly, confident appear towards the latter half of this plot while most expressive emotions such as cheerful, excited, serious, surprise appear in the first half. Note that the expressiveness scores in our work are for faces and applicable to our particular dataset. We also conduct a short human evaluation to understand expressiveness by annotating whether the emotion is conveyed through video, dialog, or character appearance; presented in Appendix G.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "We presented a novel task for multi-label emotion and mental state recognition at the level of a movie scene and for each character. A Transformer encoder based model, EmoTx, was proposed that jointly attended to all modalities (features) and obtained significant improvements over previous works adapted for this task. Our learned model was shown to have interpretable attention scores across modalities -they focused on the video or dialog context for mental states while looking at characters for expressive emotions. In the future, EmoTx may benefit from audio features or by considering the larger context of the movies instead of treating every scene independently.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Appendix",
      "text": "In Sec. A we refer to Fig.  1  (teaser) and share the hidden contexts in each scene reflecting upon the importance of individual modalities to capture the emotions in realworld environments. Sec. B present some statistics around emotions extracted from the MovieGraphs dataset. In Sec. C we share the character detection, tracking, and clustering pipeline used to extend the tracks provided in the MovieGraphs dataset. In Sec. D we visualize the class AP scores for top-10 and 25 emotions from MovieGraphs along with Emotic mapped emotions. Since there were several feature combinations in our work, an extended feature ablation is presented in Sec. E. Finally, Sec. F shares details of the modifications made to adapt EmotionNet  [75] , CAER  [34] , M2Fnet  [12] , and AttendAffectNet  [71]  for comparison with EmoTx. We end with another qualitative example showing the attention scores similar to Fig.  6  in Sec. G.\n\nA. The Stories behind Emotions in Fig.  1  We discuss some additional details from Fig.  1 . Prior to this, note that the emotions are grouped into three tuples, each corresponding to the frame depicted in the example -however, this was for illustrative purposes and making it easy to match emotions to the frames. We do not explicitly generate frame-level predictions.\n\nScene A is taken from the movie \"Sleepless in Seattle, 1993\", scene number 087, where Suzy is narrating an incident from a classical movie \"An affair to remember\". While narrating, she gets sentimental and starts crying. The other characters, Sam and Greg listen curiously but feel neutral and mock her by faking a cry and narrating the scene from some war movie. This makes Suzy laugh, and she asks the duo to stop before the scene ends. The reflected emotions and mental states include upset, calm, confused, excited, sad, and happy. Observing the situation, it is evident that a single emotion label does not suffice and both the visual and dialog context taken over a longer duration is important to predict emotions with mental states.\n\nScene B is taken from the movie \"Forrest Gump, 1994\", scene number 045. Forrest has joined the army and it is his introductory day. Sergeant Drill asks Forrest about his role in the army to which Forrest replies \"To do whatever you tell me Sergeant Drill\" which impresses him a lot. Then Sergeant Drill praises him by saying it is the best response he has ever heard! The original subtitles of this clip are shared in Table  6 . We hope to show that the dialog modality is crucial in understanding the real emotions since visually it appears that both the characters are angry and screaming at each other but in reality Forrest is determined, honest, and serious, while the Sergeant is excited.\n\nScene C is taken from movie \"Slumdog Millionaire, 2008\", scene number 076. The scene represents a Television Show \"Who wants to be a millionaire?\" where Jamal is being asked some question. He has given the response and is waiting for the confirmation from the anchor. The frames used in the figure reflect the moment when the anchor excitedly reveals that the answer given by Jamal is correct. However, by only looking at the faces, it appears as if Jamal is tense and he anchor is scolding him, whereas in reality, everyone is clapping and cheering for him. We show that looking at the visual frame is necessary to correctly predict the wider perspective of emotions, here corresponding to the transition from nervous and curious to surprised and amused for Jamal, and excited for the anchor.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Moviegraphs-Emotions: Dataset Features",
      "text": "The MovieGraphs dataset  [74]  contains graph-based annotations for each scene within a movie. The nodes of these graphs include characters and their details such as relationships, interactions, emotions, and other physical attributes, along with movie scene-level labels such as the overarching situation, place (scene), and a few sentence natural language description. There are a total of 51 movies divided into 7637  clips with associated graphs. The MovieGraphs dataset is provided with train, validation and test splits which contain 33/7/11 movies with 5050/1060/1527 clip graphs respectively. These clips have an average duration of 41.7s at 23.976 fps (frames per second). For each clip, we focus on characters and their emotion attributes. As the dataset consists of free-text annotations, this amounts to massive 509 unique emotion labels in the dataset, which however, can be mapped to a smaller set. Label distributions. We analyze the dataset from various perspectives and highlight some statistics. Fig.  8  shows the number of scenes that have a certain number of emotions. We observe that most scenes have 2-7 emotions, and the train, val, and test distributions are relatively similar. The absolute counts are expected to be lower due to smaller val/test sizes.\n\nFig.  9  presents the number of instances for top-10 (orange) and top-25 (orange + blue) label sets. We see a classic long-tail effect, however, by selecting the top-25, we ensure that there are sufficient instances for all labels to learn a decent representation. Fig.  10  shows the same distribution after mapping 181 emotions from MovieGraphs to the 26 emotion labels of the Emotic dataset  [31] . We used a similar mapping as shared  Character 1 corresponds to the most frequently occurring character in the movie, character 2 to the second most frequent, and so on.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Happiness",
      "text": "For our work, a character is considered as present if there is at least one emotion annotated in the scene.\n\nby  [46]  and show the details in Table  7 . Recall that we report results on this label set in our SoTA experiments in Sec. 4.4 of the main paper. We assign the character index 1, 2, . . . to the most frequent, second most frequent character, and so on. The plot in Fig.  11  shows the average number of scenes in which a character appears, or rather, has an annotated emotion from the MovieGraphs dataset. This provides interesting avenues for future research, to track emotions across the entire movie.\n\nCo-occurrence in the top-25 labels. Similar to Fig.  3  of the main paper, we show the row-normalized co-occurrence matrices for the top-25 labels in Fig.  12 . From a cursory look, we observe that the movie scene labels (left) are denser than the per-character co-occurrence (right) -this is expected as the movie scene level labels contain a combination of multiple characters.\n\nWe present a few notable differences between the scenelevel and character-level co-occurrences. Tuples here correspond to label1 selecting a row, and label2 selecting a col-",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Character Processing Pipeline",
      "text": "The face tracks provided by the MovieGraphs dataset  [74]  occasionally miss the characters due to the quality of the face detection. By watching some clips, we observed that many face tracks were broken within a clip due to missed detections and multiple track IDs were provided for the same character within a single shot. In addition, some shots had 0 detections, but could be useful to provide a wider perspective on the emotions of that character and scene. Therefore, we extend the face tracks from MovieGraph dataset by first extending the sparse ground-truth tracks within a shot and then over multiple shots within a scene through clustering.\n\nIn summary, we first recompute face detections and tracks for the movie scenes. A subset of the new face tracks are assigned a name based on overlap with the original tracks present in the dataset. Then, we cluster all detections in a clip using hierarchical clustering and assign names to remaining unnamed tracks based on the clustering. Fig.  13  shows an example where original tracks did not have a single detection (due to the dark scene) for a scene in the \"Forrest Gump, 1994\" movie.\n\nFace and person detection and tracking. New face and person detections are extracted from every movie scene of the MovieGraphs dataset. We adopt MTCNN (Multi-Task Cascaded Convolutional Neural Networks)  [83]  for face detection and Cascade-RCNN pretrained on cast annotations of MovieNet  [27]  for person detection. Since the original tracks are only for faces, we first compute person boxes using the person detector and obtain face detections within the person box in order to define a mapping between face and person detections. If multiple faces are found within a person bounding box, the face with higher detection probability is selected. The resulting bounding boxes are tracked using the Kalman-filter based SORT (Simple Online and Realtime Tracking) algorithm  [5] . Due to the mapping established between the face and person detections, the same track ID is shared between face and person tracks. For the rest of the discussion, we focus on face tracks.\n\nExtending names from original to new face tracks. Since some of the newly generated tracks coincide with the original tracks from MovieGraphs, such tracks are assigned a name based on their IoU overlap score. In particular, for every detection in the original tracks, a corresponding new detection is mapped if the IoU score between the two is greater than a threshold (0.7 in our case). Thus, names from the original detections (or track), are mapped to the new track, and a majority vote of these names is used to decide the final name for a new track.\n\nFace clustering and naming other tracks. Not all tracks are assigned a name through the above method due to missed detections in the original tracks. Thus, we perform clustering to increase the coverage. First, we extract good identity features from an InceptionResNetV1  [66]  pretrained on the VGGFace2  [9]  dataset. For clustering we use the C1C  [29]  algorithm which also uses track information for establishing must and cannot links between the face features. Individual face detections (features) are processed and clustered using C1C resulting in multiple partitions with varying number of clusters. We calculate the Silhouette score  [60]  for every partition and the one with highest score is selected as the representative partition. Now, based on the named tracks generated using the paragraph above, every cluster is assigned a probability corresponding to distinct names (via named detections) within the cluster. For clusters which do not have any named detection, equal probability is given to every name present in the scene. The cluster name-probabilities corresponding to the detections of unnamed tracks are extracted and the average of these soft scores is used to reflect the names for the newly discovered tracks. This way, we assign a name probability to new tracks and threshold it with 0.7 to select the final name for such new tracks.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Analyzing Ap Scores",
      "text": "Similar to Fig.  5  of the main paper, we present peremotion scores for the top-10 emotions in the dataset in Fig.  14 . We observe that our model with the individual classifier (CLS) tokens outperforms other approaches in 5 of 10 emotions. In Fig.  15 , we show the AP for each group of Emotic labels. We observe that challenging labels such as pain, sensitivity, perform much worse than others such as happiness, sadness, anger, etc.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E. Feature Ablation",
      "text": "We expand upon the feature ablation in Table  3  of the main paper to show the effect of additional feature combinations in Table  8 . All the trends are similar, fine-tuning RoBERTa helps consistently, ResNet50 trained on FER appears to be a good representation for characters, and the MViT trained on Kinetics400 provides better results for both the label sets, while ResNet50 trained on Places365 is a close second.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "F. Adapting Sota Methods For Our Task",
      "text": "The MovieGraphs dataset has not been used directly to predict emotions at a scene or character level. Related to using labels from MovieGraphs, Affect2MM  [46]  extracts scene-level emotion timelines for the entire movie, but relies on one emotion per scene. This is quite different from our vision of a multi-label setting where the scene and each character can present multiple emotions. For a fair compar-    ison to previous work, we chose models that have attained SoTA in image, video and multimodal emotion recognition. We share details on how these methods are adapted to make them suitable for our task.\n\nEmotionNet  [75]  is a recent SoTA for emotion recognition from web images. It uses a joint embedding training approach which uses emotional keywords associated with a given image and aligns its learned text embedding (pretrained on massive text data) with image embedding extracted from a standard feature backbone (ResNet50). To adapt EmotionNet for our task, we used word2vec  [44]  for extracting text embeddings and ResNet50 for frames. Since we use a video as input, the frame features are max-pooled to generate a single representation. We use the proposed embedding loss and provide the emotion labels as the keywords for joint embedding training. This learned ResNet50 is finetuned for multilabel emotion recognition where the individual frame features are max-pooled before passing to the logits layer. the fused features from both the streams to generate a single representation for a video. This adapted model is trained to predict multiple scene-level emotions. M2FNet  [12]  is a transformer based model originally developed for Emotion Recognition in Conversations (ERC) and features a fusion-attention mechanism to modulate the attention given to each utterance considering the audio and visual features. As this model is designed for utterance emotion recognition we apply a max-pooling operation over the final outputs of fusion attention module to generate a feature representation for all the utterances in a video. Since this model provides two strategies to consider visual features: one with the video frame and another that combines multiple faces in a frame, we use them to predict either scene-or character-level emotions separately. AttendAffectNet  [71]  proposes two multi-modal selfattention based approaches for predicting emotions from movie clips. We adapted the proposed Feature AttendAf-fectNet model in our work. It leverages the transformer encoder block where every input token represents a different modality. These modality feature vectors are generated by average pooling over respective features. Following the proposed mechanism, a classification head was attached at the end of the model for predicting multi-label emotions. We adopt the same backbone representations, MViT  [20]  pre-trained on Kinetics400  [10]  and ResNet50 pretrained on FER13  [24] , for their work to extract scene and face features respectively. SoTA results. Reflecting Tables  4  and 5  in the main paper, we present the Table  9  and Table  10  and also include standard deviation over 3 runs.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Caer (Context",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "G. Additional Qualitative Analysis",
      "text": "Fig.  16  shows another example (similar to Fig.  6  from the main paper) where we visualize the emotions for two characters Jamie and Dylan. We see that our model looks at relevant video frames, dialog utterances, and character representations while making the predictions. The scene described above is of a proposal, where the protagonist, Dylan, clears out some misunderstanding and proposes to the female lead character, Jamie, in between an ongoing flash mob (scene). As mentioned, in the Fig.  16  caption, both the characters develop emotion: happy and excited. From the facial expressions as well as from dialog utterances, it is apparent enough for the readers to predict emotions, but from model's point-of-view culminating all these signals and making sense of them, that too for complex human emotions, is a great job. User study on understanding expressiveness. We asked 2 people to look at about 30 random clips that have positive labels for angry, scared, cheerful and independently mark yes when the emotion was apparent in the video (V), dialog (D), and character/face (C), similar to a multi-label setup. Note, our model's attention scores suggest that cheerful is an expressive emotion (character tokens are helpful), while scared and angry can rely on dialog and video context.\n\nBelow, we present the fraction of times each modality was picked by the users. For angry, the annotators favored V: 62%, D: 80%, and C: 59%, due to several neutral-faced instances with harsh dialog and violent actions. Scared, V: 56%, D: 48%, C: 62%, was sometimes expressed through screaming or crying, with no modality standing out strongly. Finally, cheerful, V: 41%, D: 64%, C: 79%, was observed most prominently on character faces and through dialog. Note that this analysis aligns with our observations in Fig.  7  of the main paper that the expressiveness scores are applicable to our particular dataset.",
      "page_start": 14,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Multimodal models and multi-label emotions are neces-",
      "page": 1
    },
    {
      "caption": "Figure 1: A: excited, curious, confused, annoyed, alarmed; B: shocked,",
      "page": 1
    },
    {
      "caption": "Figure 1: To this end, we propose EmoTx that jointly models",
      "page": 2
    },
    {
      "caption": "Figure 2: Preparing multimodal representations.",
      "page": 3
    },
    {
      "caption": "Figure 2: An overview of EmoTx. We present the detailed approach in Sec. 3 but provide a short summary here. A: Video features (in",
      "page": 4
    },
    {
      "caption": "Figure 2: B illustrates this addition of embedding vectors. We",
      "page": 4
    },
    {
      "caption": "Figure 3: Row normalized label co-occurrence matrices for the",
      "page": 5
    },
    {
      "caption": "Figure 4: Bar chart showing the number of movie scenes associ-",
      "page": 5
    },
    {
      "caption": "Figure 4: , we show the number of",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the scene-level AP scores for the Top-25",
      "page": 6
    },
    {
      "caption": "Figure 5: Comparing scene-level per class AP of EmoTx against baselines (Table 1) shows consistent improvements. We also see that our",
      "page": 7
    },
    {
      "caption": "Figure 6: A scene from the movie Forrest Gump showing the multimodal self-attention scores for the two predictions: Mrs. Gump is",
      "page": 8
    },
    {
      "caption": "Figure 7: Sorted expressiveness scores for Top-25 emotions. Ex-",
      "page": 8
    },
    {
      "caption": "Figure 6: shows an example movie scene where EmoTx pre-",
      "page": 8
    },
    {
      "caption": "Figure 7: shows the av-",
      "page": 8
    },
    {
      "caption": "Figure 1: We discuss some additional details from Fig.1. Prior to",
      "page": 9
    },
    {
      "caption": "Figure 1: Note that the speaker",
      "page": 9
    },
    {
      "caption": "Figure 8: Number of scenes with a specific number of emotion",
      "page": 10
    },
    {
      "caption": "Figure 9: Number of movie scenes containing top-10 and 25 emo-",
      "page": 10
    },
    {
      "caption": "Figure 8: shows the number of scenes that have a certain",
      "page": 10
    },
    {
      "caption": "Figure 9: presents the number of instances for top-10 (or-",
      "page": 10
    },
    {
      "caption": "Figure 10: shows the same distribution after mapping 181",
      "page": 10
    },
    {
      "caption": "Figure 10: Number of movie scenes depicting each of the 26",
      "page": 10
    },
    {
      "caption": "Figure 11: Average number of scenes in which characters appear.",
      "page": 10
    },
    {
      "caption": "Figure 11: shows the average number of scenes in which",
      "page": 10
    },
    {
      "caption": "Figure 12: From a cur-",
      "page": 10
    },
    {
      "caption": "Figure 13: shows an example where original tracks did not",
      "page": 11
    },
    {
      "caption": "Figure 12: Normalized label co-occurrence matrices for the top-25 emotions associated with a movie scene (left) and character-level",
      "page": 12
    },
    {
      "caption": "Figure 13: Example face detections. The original face tracks do",
      "page": 12
    },
    {
      "caption": "Figure 5: of the main paper, we present per-",
      "page": 12
    },
    {
      "caption": "Figure 14: We observe that our model with the individual clas-",
      "page": 12
    },
    {
      "caption": "Figure 15: , we show the AP for each group of",
      "page": 12
    },
    {
      "caption": "Figure 14: AP scores for the top-10 emotions label set sorted from high to low AP score for our model with individual CLS tokens.",
      "page": 13
    },
    {
      "caption": "Figure 15: AP scores on the 26 grouped labels of the Emotic label set.",
      "page": 13
    },
    {
      "caption": "Figure 16: A scene from the movie Friends with Benefits with self-attention scores for multiple modalities for two character-level predic-",
      "page": 13
    },
    {
      "caption": "Figure 16: shows another example (similar to Fig. 6 from",
      "page": 15
    },
    {
      "caption": "Figure 16: caption, both",
      "page": 15
    },
    {
      "caption": "Figure 7: of the main paper that the expressive-",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "1",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "136",
          "99": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 7: Recall that we",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "277",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "133",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "801"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Recall that we",
      "data": [
        {
          "3": "",
          "Column_2": "2",
          "Column_3": "",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "3": "",
          "Column_2": "",
          "Column_3": "1",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "3": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "1",
          "Column_5": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Recall that we",
      "data": [
        {
          "608": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "495",
          "Column_5": "47",
          "Column_6": "9473"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "64.9\nOur model with individual CLS\n48.8\n42.942.242.2\n38.737.236.135.7\n33.4\n30.7\n25.8\n22.2\n20.320.1\n13.9\n11.8\n8.9 7.7 6.8 5.7 5.4 5.3\n3.6 3.1\n0.6": "64.9",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        },
        {
          "64.9\nOur model with individual CLS\n48.8\n42.942.242.2\n38.737.236.135.7\n33.4\n30.7\n25.8\n22.2\n20.320.1\n13.9\n11.8\n8.9 7.7 6.8 5.7 5.4 5.3\n3.6 3.1\n0.6": "",
          "Column_2": "48.8",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        },
        {
          "64.9\nOur model with individual CLS\n48.8\n42.942.242.2\n38.737.236.135.7\n33.4\n30.7\n25.8\n22.2\n20.320.1\n13.9\n11.8\n8.9 7.7 6.8 5.7 5.4 5.3\n3.6 3.1\n0.6": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "37.2",
          "Column_8": "36.1"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning Grimaces by Watching TV",
      "authors": [
        "S Albanie",
        "A Vedaldi"
      ],
      "year": "2016",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "2",
      "title": "Layer Normalization",
      "authors": [
        "Jimmy Lei",
        "Jamie Ba",
        "Geoffrey Ryan Kiros",
        "Hinton"
      ],
      "year": "2016",
      "venue": "Layer Normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "3",
      "title": "Condensed Movies: Story Based Retrieval with Contextual Embeddings",
      "authors": [
        "Max Bain",
        "Arsha Nagrani",
        "Andrew Brown",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "Asian Conference on Computer Vision (ACCV)"
    },
    {
      "citation_id": "4",
      "title": "LIRIS-ACCEDE: A video database for affective content analysis",
      "authors": [
        "Yoann Baveye",
        "Emmanuel Dellandrea",
        "Christel Chamaret",
        "Liming Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Simple Online and Realtime Tracking",
      "authors": [
        "Alex Bewley",
        "Zongyuan Ge",
        "Lionel Ott",
        "Fabio Ramos",
        "Ben Upcroft"
      ],
      "year": "2016",
      "venue": "International Conference on Image Processing"
    },
    {
      "citation_id": "6",
      "title": "Automated Video Labelling: Identifying Faces by Corroborative Evidence",
      "authors": [
        "Andrew Brown",
        "Ernesto Coto",
        "Andrew Zisserman"
      ],
      "year": "2021",
      "venue": "Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "7",
      "title": "Face, Body, Voice: Video Person-Clustering with Multiple Modalities",
      "authors": [
        "Andrew Brown",
        "Vicky Kalogeiton",
        "Andrew Zisserman"
      ],
      "year": "2021",
      "venue": "International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "8",
      "title": "Cascade R-CNN: Delving into High Quality Object Detection",
      "authors": [
        "Zhaowei Cai",
        "Nuno Vasconcelos"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "VGGFace2: A Dataset for Recognising Faces across Pose and Age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "M Omkar",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "10",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "João Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "11",
      "title": "Self-Supervised Learning for Scene Boundary Detection",
      "authors": [
        "Shixing Chen",
        "Xiaohan Nie",
        "David Fan",
        "Dongqing Zhang",
        "Vimal Bhat",
        "Raffay Hamid"
      ],
      "year": "2021",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "12",
      "title": "M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Pankaj Shah",
        "Naoyuki Wasnik",
        "Onoe"
      ],
      "year": "2009",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "13",
      "title": "The Psychological Foundations of the Affective Lexicon",
      "authors": [
        "Gerald Clore",
        "Andrew Ortony",
        "Mark Foss"
      ],
      "year": "1987",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "14",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2011",
      "venue": "International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "15",
      "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in the wild challenge",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Jyoti Joshi",
        "Michael Wagner",
        "Tom Gedeon"
      ],
      "year": "2013",
      "venue": "International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "17",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "18",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "W V Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Buffy\" -Automatic Naming of Characters in TV Video",
      "authors": [
        "Mark Everingham",
        "Josef Sivic",
        "Andrew Zisserman"
      ],
      "year": "2006",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "20",
      "title": "Multiscale Vision Transformers",
      "authors": [
        "Bo Haoqi Fan",
        "Karttikeya Xiong",
        "Yanghao Mangalam",
        "Zhicheng Li",
        "Jitendra Yan",
        "Christoph Malik",
        "Feichtenhofer"
      ],
      "year": "2021",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "21",
      "title": "Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning",
      "authors": [
        "Lifeng Fan",
        "Wenguan Wang",
        "Siyuan Huang",
        "Xinyu Tang",
        "Song-Chun Zhu"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "COSMIC: COm-monSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "24",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing (ICONIPS)"
    },
    {
      "citation_id": "25",
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions",
      "authors": [
        "Chunhui Gu",
        "Chen Sun",
        "David Ross",
        "Carl Vondrick",
        "Caroline Pantofaru",
        "Yeqing Li",
        "Sudheendra Vijayanarasimhan",
        "George Toderici",
        "Susanna Ricco",
        "Rahul Sukthankar",
        "Cordelia Schmid",
        "Jitendra Malik"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2005",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "MovieNet: A Holistic Dataset for Movie Understanding",
      "authors": [
        "Qingqiu Huang",
        "Yu Xiong",
        "Anyi Rao",
        "Jiaze Wang",
        "Dahua Lin"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "28",
      "title": "Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "Association for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "29",
      "title": "Constrained video face clustering using 1nn relations",
      "authors": [
        "Vicky Kalogeiton",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "30",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2005",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "32",
      "title": "Learning Interactions and Relationships between Movie Characters",
      "authors": [
        "Anna Kukleva",
        "Makarand Tapaswi",
        "Ivan Laptev"
      ],
      "year": "2005",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "Evolution of Human Emotions",
      "authors": [
        "Joseph Ledoux"
      ],
      "year": "2013",
      "venue": "Progress in Brain Research"
    },
    {
      "citation_id": "34",
      "title": "Context-aware Emotion Recognition Networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2009",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "TVQA: Localized, Compositional Video Question Answering",
      "authors": [
        "Jie Lei",
        "Licheng Yu",
        "Mohit Bansal",
        "Tamara Berg"
      ],
      "year": "2018",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "36",
      "title": "Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos",
      "authors": [
        "Juncheng Li",
        "Junlin Xie",
        "Linchao Zhu",
        "Long Qian",
        "Siliang Tang",
        "Wenqiao Zhang",
        "Haochen Shi",
        "Shengyu Zhang",
        "Longhui Wei",
        "Qi Tian",
        "Yueting Zhuang"
      ],
      "year": "2022",
      "venue": "In ACM Multimedia (MM)"
    },
    {
      "citation_id": "37",
      "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation",
      "authors": [
        "Linjie Li",
        "Jie Lei",
        "Zhe Gan",
        "Licheng Yu",
        "Yen-Chun Chen",
        "Rohit Pillai",
        "Yu Cheng",
        "Luowei Zhou",
        "Eric Xin",
        "William Wang",
        "Yang Wang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS): Track on Datasets and Benchmarks"
    },
    {
      "citation_id": "38",
      "title": "Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis",
      "authors": [
        "Mengyi Liu",
        "Shaoxin Li",
        "S Shan",
        "Ruiping Wang",
        "Xilin Chen"
      ],
      "year": "2014",
      "venue": "Asian Conference on Computer Vision (ACCV)"
    },
    {
      "citation_id": "39",
      "title": "Facial Expression Recognition via a Boosted Deep Belief Network",
      "authors": [
        "Ping Liu",
        "Shizhong Han",
        "Zibo Meng",
        "Yan Tong"
      ],
      "year": "2014",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification",
      "authors": [
        "Shilong Liu",
        "Lei Zhang",
        "Xiao Yang",
        "Hang Su",
        "Jun Zhu"
      ],
      "year": "2021",
      "venue": "Query2Label: A Simple Transformer Way to Multi-Label Classification",
      "arxiv": "arXiv:2107.10834"
    },
    {
      "citation_id": "41",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "42",
      "title": "An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria",
        "Di-Aloguernn"
      ],
      "year": "2019",
      "venue": "Association for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "43",
      "title": "LAEO-Net: revisiting people Looking At Each Other in videos",
      "authors": [
        "Vicky Manuel J Marin-Jimenez",
        "Pablo Kalogeiton",
        "Andrew Medina-Suarez",
        "Zisserman"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "44",
      "title": "Distributed Representations of Words and Phrases and Their Compositionality",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing Systems (ICONIPS)"
    },
    {
      "citation_id": "45",
      "title": "Emoti-Con: Context-Aware Multimodal Emotion Recognition using Frege's Principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2005",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "46",
      "title": "Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality",
      "authors": [
        "Trisha Mittal",
        "Puneet Mathur",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2021",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "47",
      "title": "From Benedict Cumberbatch to Sherlock Holmes: Character Identification in TV series without a Script",
      "authors": [
        "Arsha Nagrani",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "48",
      "title": "Modeling Emotion in Complex Stories: The Stanford Emotional Narratives Dataset",
      "authors": [
        "Desmond Ong",
        "Zhengxuan Wu",
        "Tan Zhi-Xuan",
        "Marianne Reddan",
        "Isabella Kahhale",
        "Alison Mattek",
        "Jamil Zaki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Contemplating Visual Emotions: Understanding and Overcoming Dataset Bias",
      "authors": [
        "Rameswar Panda",
        "Jianming Zhang",
        "Haoxiang Li",
        "Joon-Young Lee",
        "Xin Lu",
        "Amit Roy-Chowdhury"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "50",
      "title": "Webbased database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "51",
      "title": "Identity-Aware Multi-Sentence Video Description",
      "authors": [
        "Jae Sung Park",
        "Trevor Darrell",
        "Anna Rohrbach"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "52",
      "title": "Deep Face Recognition",
      "authors": [
        "M Omkar",
        "Andrea Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "53",
      "title": "Martin Raison",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas Kopf",
        "Edward Yang",
        "Zachary Devito"
      ],
      "venue": "Martin Raison"
    },
    {
      "citation_id": "54",
      "title": "Py-Torch: An Imperative Style, High-Performance Deep Learning Library",
      "authors": [
        "Lu Steiner",
        "Junjie Fang",
        "Soumith Bai",
        "Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "55",
      "title": "A General Pscychoevolutionary Theory of Emotion. Theories of Emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "A General Pscychoevolutionary Theory of Emotion. Theories of Emotion"
    },
    {
      "citation_id": "56",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Association of Computational Linguistics (ACL)"
    },
    {
      "citation_id": "57",
      "title": "A Local-to-Global Approach to Multi-modal Movie Scene Segmentation",
      "authors": [
        "Anyi Rao",
        "Linning Xu",
        "Yu Xiong",
        "Guodong Xu",
        "Qingqiu Huang",
        "Bolei Zhou",
        "Dahua Lin"
      ],
      "year": "2020",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "58",
      "title": "Scene Detection in Hollywood Movies and TV Shows",
      "authors": [
        "Zeeshan Rasheed",
        "Mubarak Shah"
      ],
      "year": "2003",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "59",
      "title": "Aaron Courville, and Bernt Schiele. Movie Description. IJCV",
      "authors": [
        "Anna Rohrbach",
        "Atousa Torabi",
        "Marcus Rohrbach",
        "Chris Tandon",
        "Hugo Pal",
        "Larochelle"
      ],
      "year": "2017",
      "venue": "Aaron Courville, and Bernt Schiele. Movie Description. IJCV"
    },
    {
      "citation_id": "60",
      "title": "Optimal Sequential Grouping for Robust Video Scene Detection using Multiple Modalities",
      "authors": [
        "Dror Daniel Rotman",
        "Gal Porat",
        "Ashour"
      ],
      "year": "2017",
      "venue": "International Journal of Semantic Computing"
    },
    {
      "citation_id": "61",
      "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
      "authors": [
        "J Peter",
        "Rousseeuw"
      ],
      "year": "1987",
      "venue": "Journal of Computational and Applied Mathematics"
    },
    {
      "citation_id": "62",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein",
        "Alexander Berg",
        "Li Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision (IJCV)"
    },
    {
      "citation_id": "63",
      "title": "17th and 18th Century Theories of Emotions",
      "authors": [
        "Amy Schmitter"
      ],
      "year": "2021",
      "venue": "The Stanford Encyclopedia of Philosophy"
    },
    {
      "citation_id": "64",
      "title": "DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Association for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "65",
      "title": "Multimodal Continuous Prediction of Emotions in Movies using Long Short-Term Memory Networks",
      "authors": [
        "Sarath Sivaprasad",
        "Tanmayee Joshi"
      ],
      "year": "2018",
      "venue": "International Conference on Multimedia Retrieval (ICMR)"
    },
    {
      "citation_id": "66",
      "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
      "authors": [
        "Chen Sun",
        "Austin Myers",
        "Carl Vondrick",
        "Kevin Murphy",
        "Cordelia Schmid"
      ],
      "year": "2019",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "67",
      "title": "Going Deeper with Convolutions",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "year": "2015",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "68",
      "title": "Probabilistic Person Identification in TV series",
      "authors": [
        "Makarand Tapaswi",
        "Martin Bäuml",
        "Rainer Stiefelhagen"
      ],
      "year": "2012",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "StoryGraphs: Visualizing Character Interactions as a Timeline",
      "authors": [
        "Makarand Tapaswi",
        "Martin Bäuml",
        "Rainer Stiefelhagen"
      ],
      "year": "2014",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "70",
      "title": "Book2Movie: Aligning Video scenes with Book chapters",
      "authors": [
        "Makarand Tapaswi",
        "Martin Bäuml",
        "Rainer Stiefelhagen"
      ],
      "year": "2015",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "71",
      "title": "MovieQA: Understanding Stories in Movies through Question-Answering",
      "authors": [
        "Makarand Tapaswi",
        "Yukun Zhu",
        "Rainer Stiefelhagen",
        "Antonio Torralba",
        "Raquel Urtasun",
        "Sanja Fidler"
      ],
      "year": "2006",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "72",
      "title": "AttendAffectNet: Self-Attention based Networks for Predicting Affective Responses from Movies",
      "authors": [
        "Ha Thi",
        "Phuong Thao",
        "B Balamurali",
        "Dorien Herremans",
        "Gemma Roig"
      ],
      "year": "2009",
      "venue": "International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "73",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)"
    },
    {
      "citation_id": "74",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2004",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "75",
      "title": "MovieGraphs: Towards Understanding Human-Centric Situations from Videos",
      "authors": [
        "Paul Vicol",
        "Makarand Tapaswi",
        "Lluis Castrejon",
        "Sanja Fidler"
      ],
      "year": "2009",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "76",
      "title": "Learning Visual Emotion Representations From Web Data",
      "authors": [
        "Zijun Wei",
        "Jianming Zhang",
        "Zhe Lin",
        "Joon-Young Lee",
        "Niranjan Balasubramanian",
        "Minh Hoai",
        "Dimitris Samaras"
      ],
      "year": "2009",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "77",
      "title": "Context-sensitive Multimodal Emotion Recognition from Speech and Facial Expression using Bidirectional LSTM Modeling",
      "authors": [
        "Martin Wöllmer",
        "Angeliki Metallinou",
        "Florian Eyben",
        "Björn Schuller",
        "Shrikanth Narayanan"
      ],
      "year": "2010",
      "venue": "Interspeech"
    },
    {
      "citation_id": "78",
      "title": "Towards Long-Form Video Understanding",
      "authors": [
        "Chao-Yuan Wu",
        "Philipp Krähenbühl"
      ],
      "year": "2021",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "79",
      "title": "A Graph-based Framework to Bridge Movies and Synopses",
      "authors": [
        "Yu Xiong",
        "Qingqiu Huang",
        "Lingfeng Guo",
        "Hang Zhou",
        "Bolei Zhou",
        "Dahua Lin"
      ],
      "year": "2019",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "80",
      "title": "A Joint Sequence Fusion Model for Video Question Answering and Retrieval",
      "authors": [
        "Youngjae Yu",
        "Jongseok Kim",
        "Gunhee Kim"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "81",
      "title": "End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering",
      "authors": [
        "Youngjae Yu",
        "Hyungjin Ko",
        "Jongwook Choi",
        "Gunhee Kim"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "82",
      "title": "Multi-modal Multi-label Emotion Detection with Modality and Label Dependence",
      "authors": [
        "Dong Zhang",
        "Xincheng Ju",
        "Junhui Li",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "83",
      "title": "Modeling both Contextand Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "84",
      "title": "Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "85",
      "title": "Places: A 10 Million Image Database for Scene Recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)"
    },
    {
      "citation_id": "86",
      "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "venue": "International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "87",
      "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
      "authors": [
        "Yukun Zhu",
        "Ryan Kiros",
        "Richard Zemel",
        "Ruslan Salakhutdinov",
        "Raquel Urtasun",
        "Antonio Torralba",
        "Sanja Fidler"
      ],
      "year": "2015",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "88",
      "title": "A Robustly Optimized BERT Pre-training Approach with Post-training",
      "authors": [
        "Liu Zhuang",
        "Lin Wayne",
        "Shi Ya",
        "Zhao Jun"
      ],
      "year": "2021",
      "venue": "Chinese National Conference on Computational Linguistics"
    }
  ]
}