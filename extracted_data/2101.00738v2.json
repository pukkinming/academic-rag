{
  "paper_id": "2101.00738v2",
  "title": "A Novel Policy For Pre-Trained Deep Reinforcement Learning For Speech Emotion Recognition",
  "published": "2021-01-04T02:13:26Z",
  "authors": [
    "Thejan Rajapakshe",
    "Rajib Rana",
    "Sara Khalifa",
    "Björn W. Schuller",
    "Jiajun Liu"
  ],
  "keywords": [
    "Machine Learning",
    "Deep Reinforcement Learning",
    "Speech Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Reinforcement Learning (RL) is a semi-supervised learning paradigm where an agent learns by interacting with an environment. Deep learning in combination with RL provides an efficient method to learn how to interact with the environment called Deep Reinforcement Learning (deep RL). Deep RL has gained tremendous success in gaming -such as AlphaGo, but its potential has rarely being explored for challenging tasks like Speech Emotion Recognition (SER). Deep RL being used for SER can potentially improve the performance of an automated call centre agent by dynamically learning emotion-aware responses to customer queries. While the policy employed by the RL agent plays a major role in action selection, there is no current RL policy tailored for SER. In addition, an extended learning period is a general challenge for deep RL, which can impact the speed of learning for SER. Therefore, in this paper, we introduce a novel policy -the \"Zeta policy\" which is tailored for SER and apply pre-training in deep RL to achieve a faster learning rate. Pre-training with a cross dataset was also studied to discover the feasibility of pre-training the RL agent with a similar dataset in a scenario where real environmental data is not available. The IEMOCAP and SAVEE datasets were used for the evaluation with the problem being to recognise the four emotions happy, sad, angry, and neutral in the utterances provided. The experimental results show that the proposed \"Zeta policy\" performs better than existing policies. They also support that pre-training can reduce the training time and is robust to a cross-corpus scenario.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Reinforcement Learning (RL), a semi-supervised machine learning technique, allows an agent to take actions and interact with an environment to maximise the total rewards. RL's main popularity comes from its super-human performance in solving some games like AlphaGo  [1]  and AlphaStar  [2] .\n\nRL has been also employed for audio-based applications showing its potential for audio enhancement  [3] ,  [4] , automatic speech recognition  [5] , and spoken dialogue systems  [6] ,  [7] . The potential of using RL for speech emotion recognition has been recently demonstrated for robot applications where the robot can detect an unsafe situation earlier given a human utterance  [8] . An emotion detection agent was trained to achieve an accuracy-latency tradeoff by punishing wrong classifications as well as too late predictions through the reward function. Motivated by this recent study, we employ RL for speech emotion recognition where a potential application of the proposed system could be an intelligent call centre agent, learning over time how to communicate with human customers in an emotionally intelligent way. We consider the speech utterance as \"state\" and the classified emotion as \"action\". We consider a correct classification as a positive reward and negative reward, otherwise.\n\nRL widely uses Q-learning, a simple, yet quite powerful algorithm to create a state-action mapping, namely Q-table, for the agent. This, however, is intractable for a large or continuous state and/or action space. First, the amount of memory required to save and update that table would increase as the number of states increases. Second, the amount of time required to explore each state to create the required Q-table would be unrealistic. To address these issues, the deep Q-Learning algorithm uses a neural network to approximate a Q-value function. Deep Q-Learning is an important algorithm enabling deep Reinforcement Learning (deep RL)  [9] .\n\nThe standard deep Q-learning algorithm employs a stochastic exploration strategy called -greedy, which follows a greedy policy according to the current Q-value estimate and chooses a random action with probability . Since the application of RL in speech emotion recognition (SER) is mostly unexplored, there is not enough evidence if the -greedy policy is best suited for SER. In this article, we investigate the feasibility of a tailored policy for SER. We propose a Zeta-policy 1 and provide analysis supporting its superior performance compared to -greedy and some other popular policies.\n\nA major challenge of deep RL is that it often requires a prohibitively large amount of training time and data to reach a reasonable accuracy, making it inapplicable in realworld settings  [10] . Leveraging humans to provide demonstrations (known as learning from demonstration (LfD)) in RL has recently gained traction as a possible way of speeding up deep RL  [11] ,  [12] ,  [13] . In LfD, actions demonstrated by the human are considered as the ground truth labels for a given input game/image frame. An agent closely simulates the demonstrator's policy at the start, and later on, learns to surpass the demonstrator  [10] . However, LfD holds a distinct challenge, in the sense that it often requires the agent to acquire skills from only a few demonstrations and interactions due to the time and expense of acquiring them  [14] . Therefore, LfDs are generally not scalable, especially for high-dimensional problems.\n\nWe propose the technique of pre-training the underlying deep neural networks to speed up training in deep RL. It enables the RL agent to learn better features leading to better performance without changing the policy learning strategies  [10] . In supervised methods, pre-training helps regularisation and enables faster convergence compared to randomly initialised networks  [15] . Various studies (e. g.,  [16] ,  [17] ) have explored pre-training in speech recognition and achieved improved results. However, pre-training in deep RL is hardly explored in the area of speech emotion recognition. In this paper, we present the analysis showing that pre-training can reduce the training time. In our envisioned scenario, the agent might be trained with one corpus but might need to interact with other corpora. To test the performance in those scenarios, we also analyse performance for cross-corpus pre-training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work And Background",
      "text": "Reinforcement learning has not been widely explored for speech emotion recognition. The closest match of our work is EmoRL where the authors use deep RL to determine the best position to split the utterance to send to the emotion recognition model  [8] . The RL agent chooses to decide an \"action\" whether to wait for more audio data or terminate and trigger prediction. Once the terminate action is selected, the agent stops processing the audio stream and starts classifying the emotion. The authors aim to achieve a trade-off between accuracy and latency by penalising wrong classifications as well delayed predictions through rewards. In contrast, our focus is on developing a new policy tailored for SER, and apply pre-training to achieve faster learning rate.\n\nAnother related study has used RL to develop a music recommendation system based on the mood of the listener. The users select their current mood by selecting \"pleasure\" and \"energy\", then, the application selects a song out of its repository. The users can provide feedback on the system recommended audio by answering the question \"Does this audio match the mood you set?\"  [18] . Here, the key focus is to learn the mapping of a song to the selected mood, however, in this article, we focus on the automatic determination of the emotion. Yu and Yang introduced an emotion-based target reward function  [19] , which again did not have a focus on SER.\n\nA group of studies used RL for audio enhancement. Shen et al. showed that introducing RL for enhancing the audio utterances can reduce the testing phase character error rate by about 7 % of automatic speech recognition with noise  [3] . Fakoor et al. also used RL for speech enhancement while considering the noise suppression as a black box and only taking the feedback from the output as the reward. They achieved an increase of 42 % of the signal to noise ratio of the output  [4] .\n\nOn the topic of proposing new policies, Lagoudakis and Parr used a modification of approximate policy iteration for pendulum balancing and bicycle riding domains  [20] . We propose a policy which is tailored for SER. Many studies can be found in the literature using Deep learning to capture emotion and related features from speech signal, however, none of them had a focus on RL  [21] ,  [22] ,  [23] .\n\nAn important aspect of our study is incorporating pretraining in RL to achieve a faster learning rate in SER. Gabriel et al. used human demonstrations of playing Atari games to pre-train a deep RL agent to improve the training time of the RL agent  [10] . Hester et al. in their introduction of Deep Q-Learning from Demonstrations, presented prior recorded demonstrations to a deep RL system of playing 42 games -41 of them had improved performance and 11 out of them achieved state-of-the-art performance  [24] . An evidential advance in performance and accuracy is observed in their results of the case study with Speech Commands Dataset. None of these studies focuses on using pre-training in RL for SER, which is our focus.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Reinforcement Learning",
      "text": "RL architecture mainly consist of two major components namely \"Environment\" and \"Agent\" while there are three major signals passing between those two agents as \"current state\", \"reward\" and \"next state\". An agent interacts with an unknown environment and observes a state s t ∈ S at every time step t ∈ [0, T ], where S is the state space and T is the terminal time. The agent selects an action a ∈ A, where A is the action space and then, the environment changes to s t+1 and the agent receives a reward scalar r t+1 which represents a feedback on how good the selected action on the environment is. The agent learns a policy π to map most actions a for a given state s. The objective of RL is to learn π * , an optimal policy which maximises the cumulative reward that can map the most suitable actions to a given state s.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Q-Learning",
      "text": "Before introducing Q-Learning, introduction to a number of key terms is necessary.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Free And Model Based Policy:",
      "text": "The model learns the transition probability from the current state for an action to the next state. Although it is straight forward, modelbased algorithms become impractical as the state space and action space grows. In contrary, model-free algorithms rely on trial-and-error to update its knowledge and does not require to store all the combination of states and actions.\n\nOn-Policy learning and Off-policy learning: These two policies can be described in-terms of Target Policy and Behaviour Policy. Target Policy is the policy, which an agent In On-policy learning, target policy and the behavioural policies are the same, but different in Off-policy learning. Off-policy learning enables continuous exploration resulting in learning an optimal policy, whereas on-policy learning can only offer learning sub-optimal policy.\n\nIn this paper, we consider widely used Q-Learning, which represents a model-free off-policy learning. Q-Learning maintains a Q-table which contains Q-values for each state, and action combination. This Q-table is updated each time the agent receives a reward from the environment. A straight forward way to store this information would be in a Table  1 . Q-values from the Q-table are used for the action selection process. A policy takes Q-values as input and outputs the action to be selected by the agent. Some of the policies are Epsilon-Greedy policy  [25] , Boltzmann Q Policy, Max-Boltzmann Q Policy  [26] , and Boltzmann-Gumbel exploration Policy  [27] .\n\nThe main disadvantage of Q-Learning is that the state space should be discrete and the Q-table cannot store Qvalues for continues state space. Another disadvantage is that the Q-table grows larger with increasing state space which will not be manageable at a certain point. This is known as \"curse of dimensionality\"  [26]  and the complexity of evaluating a policy scales up with O(n 3 ) when n is the number of states in a problem. Deep Q-Learning offers a solution to these challenges.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Deep Q-Learning",
      "text": "A Neural Network can be used to approximate the Q-values based on the state as input. This is more tractable than storing every possible state in a table like Table  1 .\n\nWhen the problems and states are becoming more complex, the Neural Network may need to be \"deep\", meaning a few hidden layers may not suffice to capture all the intricate details of that knowledge, hence the use of Deep Neural Networks (DNNs). Studies were carried out to incorporate DNNs to Q-Learning by replacing the Q-table with a DNN known as deep Q-Learning, and the involvement of Deep Learning to Reinforcement Learning is now known as Deep Reinforcement Learning. Deep Q-Learning process uses two neural networks models: the inferring model and the target networks. These networks have the same architecture but different weights. Every N steps, the weights from the inferring network are copied to the target network. Using both of these networks leads to more stability in the learning process and helps the algorithm to learn more effectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Warm-Up Period",
      "text": "Since the RL agent learns only by interacting with the environment and the gained reward, the RL agent needs a set of experiences to start training for the experience replay. A parameter nb warm up is used to define the number of warm-up steps to be performed before initiating RL training. During this period, actions are selected totally through a random function for a given state, and no Q-value is involved. The state, action, reward, and next state are stored in the memory buffer and are used to sample the experience replay.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Q-Learning Policies",
      "text": "Q-Learning policies are responsible for deciding the best action to be selected based on the Q-values as their input. Exploration and exploitation are used to improve the experience of the experience replay by involving randomness to the action selection. Epsilon-Greedy policy, Max-Boltzmann Q-Policy, and Linear Annealed wrapper on Epsilon-Greedy policy are some of the Q-Learning policies used today  [28] ,  [29] .\n\nThe Epsilon-Greedy policy adopts a greedy algorithm to select an action out from the Q values, The value of this policy determines the exploitation and exploration ratio. 1is the probability of choosing exploitation on action selection. A random action is selected by a uniform random distribution at exploration and an action with maximum Q-value is selected on exploitation. The Linear Annealed wrapper on the Epsilon-Greedy policy is changing the value of the Epsilon-Greedy policy at each step. An value range and the number of steps are given as parameters. This wrapper linearly changes and updates the value of the Epsilon-Greedy policy at each step.\n\nThe Max-Boltzmann policy also uses as a parameter. value is considered when determining exploitation and exploration. Exploration in Max-Boltzmann policy is similar as the Epsilon-Greedy policy. At exploitation, instead of selecting the action with maximum Q-value as in Epsilon-Greedy policy; Max-Boltzmann policy randomly selects an action from a distribution which is similar to the Q-values. This introduces more randomness yet, usage of Q-values in to the action selection process.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Scope Of Rl In Speech Emotion Recognition",
      "text": "Emotion recognition from speech has gained a fair amount of attention over the past years among machine learning researchers and many studies are being carried out to improve the performance of SER from both feature extraction and emotion classification stages  [30] . Hidden Markov Models, Support Vector Machines, Gaussian Mixture Models, and Artificial Neural Networks are some of the classifiers used for SER in the literature  [31] ,  [32] ,  [33] ,  [34] . With the tremendous success of DNN architectures, numerous studies have successfully used them and achieved good performances, e.g.,  [35] ,  [36] ,  [37] ,  [38] .\n\nUsing DNN, Supervised and guided unsupervised/selfsupervised techniques are being dominantly developed for SER, however, there is still a gap in the literature for dynamically updating SER systems. Although some studies, e. g., Learning++  [39]  and Bagging++  [40]  use incremental learning/Online learning, there is a major difference between Online learning and RL. Online learning is usually used for a constant stream of data, where after once using an example, it is discarded. Whereas, RL constitutes a series of state-action pairs that either draw a positive or a negative reward. If it draws a positive reward, the entire string of actions leading up to that positive reward is reinforced, but if it draws a negative reward, the actions are penalised.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Process Flow",
      "text": "Figure  1  shows our proposed architecture and process flow. It dominantly focuses on the process flow with the \"Zeta Policy\", yet showing the inter connection between the inferring and the target model and the interaction between the agent and the environment through action and reward.\n\nThrough out this section we will gradually elaborate on different components.\n\nInferring model is used to approximate the Q-values used in action selection for a given state by the environment. After a reward signal is received by the agent after an action is executed in the environment, attributes (state, reward, action, next state and terminal signal) are stored in the experience memory (History DB in Figure  1 ). After every N update number of steps, a batch of samples from the experience memory is used and updates the parameters of Inferring Model. Target Model is used to approximate the Q target values. Usage of Q target value is explained in section 3.4.4. Parameters of the Target model is updated after N copy number of steps. N update and N copy are hyperparameters.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Zeta Policy",
      "text": "A novel RL policy: \"Zeta Policy\" is introduced in this study, which takes inspiration from the Boltzmann Q Policy and the Max-Boltzmann Q Policy. This policy uses the current step (cs) (see Figure  1 ) of the RL training cycle and decides on how the action selection is performed. Figure  1  shows how the action selection process is performed by the Zeta Policy with the connection through other components in the RL architecture. The Zeta nb step(ζ) is a hyperparameter to the policy and it routes to the action selection process. If cs < ζ, the policy follows an exploitation and exploration process, where exploitation selects the action with the highest Q-value and exploration selects a random action from a discrete uniform random distribution to include uniform randomness to the experiences. The parameter compared with a random number n from an uniform distribution and used to determine the exploration and exploration route. If cs > ζ, a random value from a distribution similar to the Q-value distribution is picked as the selected action. Experiments were carried out to find the effect of the parameters ζ and on the performance of RL.\n\nIn the SER context, a state s is defined as an utterance in the dataset and an action a is the classified label (emotion) to the state s. A reward r is the reward returned by the environment after comparing the ground truth with the action a. If an action (classified emotion) and ground truth are similar, i. e., the agent has inferred the correct label to the given state, the reward resembles 1 and else the reward is -1. The reward is accumulated throughout the episode, and the mean reward is calculated. The higher the mean reward, the better the performance of the RL agent. The standard deviation of the reward is also calculated, since this value interprets how robust the RL predictions are.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Training For Improved Performance",
      "text": "Pre-training allows the RL agent's inferring DNN to optimise its parameters and learning the features required for a similar problem. To use a pre-trained DNN in RL, we replace the softmax layer with a Q-value output layer. As the inferring model is optimised with learnt features, there is no necessity of a warm-up period to collect experience replay. In fact, extended training time is a key shortcoming of RL  [10] . One key contribution of the present paper is to use pre-training to reduce the training time by reducing the warm-up period.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this section, we explain the experimental setup including feature extraction and model configuration. Figure  2",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "This study uses two popular datasets in SER: IEMOCAP  [41]  and SAVEE  [42] . The IEMOCAP dataset features five sessions; each session includes speech segments from two speakers and is labelled with nine emotional categories. However, we use happiness, sadness, anger, and neutral for consistency with the literature. The dataset was collected from ten speakers (five male and five female). We took a maximum of 1 200 segments from each emotion for equal distribution of emotions within the dataset. Note that the SAVEE dataset is relatively smaller compared to IEMOCAP. It is collected form 4 male speakers and has 8 labels for emotions which we filtered out keeping happiness, sadness, anger, and neutral segments for alignment with IEMOCAP and the literature.\n\nTable  2  shows the utterances' distribution of the two datasets with 30 % of each dataset being used as a subset for pre-training, and the rest being used for the RL execution.\n\n20% of the RL execution data subset is used in the testing phase. RL Testing phase executes the whole pipeline of RL algorithm but does not update the model parameters. RL which is a different paradigm of Machine Learning than Supervised Learning does not need to have a testing dataset as it contentiously interacting with an environment. But since this specific study is related to a classification problem, we included a testing phase by proving a dataset which the RL agent has not seen before.\n\nWe remove the silent sections of the audio segments and process only the initial two seconds of the audio. For segments less than two seconds in length, we use zero padding. The reason is when using the two seconds for segment length, there will be more zero-padded segments if the segment length is increased and it becomes an identifiable feature within the dataset in the model training.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "We use Mel Frequency Cepstral Coefficients (MFCC) to represent the speech utterances. MFCC are widely used in speech audio analysis  [35] ,  [43] . We extract 40 MFCCs from the Mel-spectrograms with a frame length of 2,048 and a hop length of 512 using Librosa  [44] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Recipe",
      "text": "A supervised learning approach is followed to identify the best suited DNN model as the inferring model. Even-though the last layer of supervised learning models output a probability vector, RL also learn the representaions in the hidden layers with similar mechanism. DNN architecture of the supervised learning model is similar to the DNN architecture of the inferring model except the output later. Activation function of the output layer of supervised later is Softmax whereas it is Linear Activation in RL. Different architectures containing Convolutional Neural Networks (CNNs), Long-Short Term Memory (LSTM) Recurrent Neural Networks (RNNs) and Dense Layers were evaluated with similar dataset and DNN model architecture with highest testing accuracy was selected as the Inferring DNN architecture.\n\nWe use the popular Deep Learning API Keras  [45]  with TensorFlow  [46]  as the backend for modelling and training purposes in this study. We model the RL agent's DNN with a combination of CNNs and LSTM. The use of a CNN-LSTM combined model is motivated by the ability to learn temporal and frequency components in the speech signal  [47] . We stack the LSTM on a CNN layer and pass it on to a set of fully connected layers to learn discriminative features  [35] .\n\nAs is trained with pre-training data subset before starting the RL execution. The model is trained for 64 epochs with a batch size 128. Once the pre-training is completed parameters of the pre-trained DNN model is copied to the inferring and target models in the RL Agent.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model Parameter Update",
      "text": "Mathematical formulation of Q-Learning is based on the popular Bellman's Equation for State-Value function (Eq 2).\n\nIn (2), v(s) is the value at state s, R t is the reward at time t and γ is the discount factor.\n\nEquation (  2 ) can be re-written to obtain the Bellman's State-Action Value function known as Q-function as follows; Here q π (s, a) is the Q-value of the state s following action a under the policy π.\n\nWe use a DNN to approximate the Q-values and q π (s, a) is considered as the target Q-value giving the loss function (  4 )\n\nCombining Equations (  4 ) and (  5 ), updated loss function can be written as;\n\nMinimising the Loss value L is the optimisation problem solved in the training phase. Q target is obtained by inferring the state s from the target network via the function  (1) . The output layer of the DNN model is a Dense layer with Linear activation function. Adopting the feed-forward pass in deep neural networks to the last layer l, Q value for an action a can be obtained by equation  (7) ,\n\nwhere x l-1 is the input for the layer l. Backpropergation pass in parameter optimising updates the W l and b l values in each later l to minimise the Loss value L.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "Experiments were carried out to evaluate the efficiency of the newly proposed policy \"Zeta-Policy\" and the improvement that can be gained by introducing the pre-training aspect to the RL paradigm.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison Of Policies",
      "text": "The key contribution presented in this article is the Zeta Policy. Therefore, we first compare its performance with that of three commonly used policies: the Epsilon-Greedy policy (EpsG.), the Max-Boltzmann Q Policy (MaxB.), and Linear Annealed wrapper on Epsilon-Greedy policy (LA).\n\nFigure  3  presents the results of these comparisons. Standard deviation and mean reward are plotted against the step number. We notice from Figure  3  that the Zeta policy has a lower standard deviation of the reward, which suggests that the robustness of the selected actions (i. e., classified labels/emotions) of the Zeta policy is higher than that of the compared policies.\n\nThe Zeta policy outperforms the other compared policies with higher mean reward. The mean reward of the Zeta policy converges to a value around 0.78 for the IEMOCAP dataset and 0.7 for the MSP-IMPROV dataset. These values are higher than that of other policies compared, which means that the RL Agent selects the actions more correctly than other policies. Since the inferring DNN model of the RL Agent is the same for all experiments, we can infer that the Zeta policy has played a major role in this out-performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of The Value",
      "text": "All the compared policies -Zeta Policy, Epsilon-Greedy policy, Max-Boltzmann Q policy, and the Linear Annealed wrapper on Epsilon-Greedy policy use the parameter in their action selection process. is used to decide the exploration and exploitation within the policy. 1is the probability of selecting exploitation out of exploration and exploitation. Hence, ranges between 0 and 1.\n\nSeveral experiments were carried out to find the range of with the values 0.1, 0.05, 0.025, and 0.0125. Figure  4  shows the effect of changing the value on the standard deviation of the reward and mean reward for all policies.\n\nThe Zeta policy shows a noticeable change in the standard deviation of the reward and mean reward for = 0.0125. The reason being that the Zeta policy performs well in the lower scenarios is: when cs < ζ, the Zeta policy picks actions with the exploration and exploitation strategy and is the probability of exploration. A random action from a uniform random distribution is picked in the exploration strategy. A lower means lower randomness in the period cs < ζ which has a higher probability of",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of The Number Of Steps",
      "text": "The Zeta policy uses the parameter Zeta nb step(ζ) to determine the route and the Linear Annealing uses the parameter nb step to determine the gradient of the value used in the Epsilon-Greedy component. Experiments were defined to examine the behavior of the performance of the RL agent by changing the above parameters to the values 500 000, 250 000, 100 000 and 50 000. Figure  5  was drawn with the output from experiments. Looking at the curve of the standard deviation of Zeta policy, the robustness of the RL agent has increased with the increase of the number of steps. The graph shows that the most robust curve is observed for the step size 500 000.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Pre-Training For The Reduced Warm-Up Period",
      "text": "The warm-up period is executed in the RL algorithm to collect experiences for the RL agent to sample the experience replay. But with the pre-training, inferring DNN, the RL agent is already trained to learn the features. This leads the RL agent to produce better Q values than an RL agent inferring a DNN based on randomly initialised parameters. An experiment was executed to identify the possibility of reducing the warm-up period after pre-training and yet keep the RL agent performance unchanged shown in Figure  6  that features the generated results. Observing both standard deviation of the reward and mean reward in Figure  6 , pretraining has improved the robustness and performance of the prediction. The time taken to achieve the highest performance has reduced since the warm-up period is reduced. This makes the RL training time lower and time taken for optimising the RL agent as well.\n\nSpeed-up of the training period by pre-training was calculated by considering the number of steps needed to reach the mean reward value of 0.6. Mean number of steps taken to reach mean reward of 0.6 without pre-training was 77126 whilst with pre-training (warm-up 10000) was 48623. The speed-up of training period by pre-training and reducing warm-up steps was 1.63x.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cross-Dataset Pre-Training",
      "text": "In our envisioned scenario an agent, although pre-trained with one corpus, is expected to be robust to other corpus/dialects. In order to experiment the behaviour of cross dataset pre-training on RL, we pre-trained the RL Agent with SAVEE pre-train data subset for the IEMOCAP RL agent and plotted the reward curve and standard deviation of the reward curve in Figure  7 . The graph shows that the pre-training has always improved the performance of the RL agent and cross dataset pre-training has not degraded the performance drastically. This practice can be used in realworld applications with RL implementations, where there is a lower number of training data available. The RL agent can be pre-trained with a dataset which is aligned with the problem and deployed for act with the real environment.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Accuracy Of The Predictions",
      "text": "Accuracy of a machine learning model is a popular attribute that uses to benchmark against the other parallel models. Since RL is a method of dynamic programming, it does not comprise of accuracy attribute. But, as this specific study is focused on a classification problem, we calculated the accuracy of the RL agent with the logs of the environment. Equation 8 is used to calculate the accuracy value of an episode. Accuracy value of the testing phase after RL execution of 700000 steps was calculated and tabulated in the Table  3 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Accuracy =",
      "text": "No. correct inferences No. of utterances × 100%\n\nStudying the Table  3 , it is observed that the Zeta policy outperforms the other compared policies in both datasets. Also, these results can be compared with the results of supervised learning methods even though they are diverse machine learning paradigms.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This study was carried out to discover the feasibility of using a novel reinforcement learning policy named as Zeta policy for speech emotion recognition problems. Pretraining the RL agent was also studied to reduce the training time and minimise the warm-up period. The evaluated results show that the proposed Zeta policy performs better than the existing policies. We also provided an analysis of the relevant parameters epsilon and the number of steps, which shows the operating range of these two parameters. The results also confirm that pre-training can reduce the training time to reach maximum performance by reducing the warm-up period. We show that the proposed Zeta Policy with pre-training is robust to a cross-corpus scenario. In the future, one should study a cross-language scenario and explore the feasibility of using the novel Zeta policy with other RL algorithms.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Flow of the Zeta Policy and the connection with related compo-",
      "page": 4
    },
    {
      "caption": "Figure 1: shows our proposed architecture and process ﬂow.",
      "page": 4
    },
    {
      "caption": "Figure 1: ) of the RL training cycle and",
      "page": 4
    },
    {
      "caption": "Figure 1: shows how the action selection process is performed by the",
      "page": 4
    },
    {
      "caption": "Figure 2: Deep Reinforcement Learning Architecture",
      "page": 5
    },
    {
      "caption": "Figure 3: Comparison across the policies Zeta policy (Zeta), Epsilon-Greedy policy (EpsG.), Max-Boltzmann Q-policy (MaxB.), and Linear Annealed",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of mean reward and standard deviation of reward with policies by changing the ϵ value of the Zeta policy (Zeta), the Epsilon-",
      "page": 6
    },
    {
      "caption": "Figure 5: Comparison of mean reward and standard deviation of reward with policies by changing the number of steps of the Zeta Policy (Zeta) and",
      "page": 7
    },
    {
      "caption": "Figure 3: presents the results of these comparisons. Standard",
      "page": 7
    },
    {
      "caption": "Figure 3: that the Zeta policy has a",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the effect of changing the ϵ value on the standard",
      "page": 7
    },
    {
      "caption": "Figure 6: Effect of the warm-up period and pre-training (p/t) on the performance.",
      "page": 8
    },
    {
      "caption": "Figure 7: Performance impact on pre-training (p/t) and cross dataset pre-training by the IEMOCAP (IEM) dataset and the SAVEE (SAV ) dataset",
      "page": 8
    },
    {
      "caption": "Figure 5: was drawn",
      "page": 8
    },
    {
      "caption": "Figure 6: that features the generated results. Observing both standard",
      "page": 8
    },
    {
      "caption": "Figure 7: The graph shows that the",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 2: shows the utterances’ distribution of the two classification. Activation function of the last layer is kept",
      "data": [
        {
          "Emotion": "Happy\nSad\nAngry\nNeutral",
          "IEMOCAP": "895\n1200\n1200\n1200",
          "SAVEE": "60\n60\n60\n120"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mastering the game of Go with deep neural networks and tree search",
      "authors": [
        "D Silver",
        "A Huang",
        "C Maddison",
        "A Guez",
        "L Sifre",
        "G Van Den Driessche",
        "J Schrittwieser",
        "I Antonoglou",
        "V Panneershelvam",
        "M Lanctot",
        "S Dieleman",
        "D Grewe",
        "J Nham",
        "N Kalchbrenner",
        "I Sutskever",
        "T Lillicrap",
        "M Leach",
        "K Kavukcuoglu",
        "T Graepel",
        "D Hassabis"
      ],
      "year": "2016",
      "venue": "Nature",
      "doi": "10.1038/nature16961"
    },
    {
      "citation_id": "2",
      "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
      "authors": [
        "O Vinyals",
        "I Babuschkin",
        "W Czarnecki",
        "M Mathieu",
        "A Dudzik",
        "J Chung",
        "D Choi",
        "R Powell",
        "T Ewalds",
        "P Georgiev",
        "J Oh",
        "D Horgan",
        "M Kroiss",
        "I Danihelka",
        "A Huang",
        "L Sifre",
        "T Cai",
        "J Agapiou",
        "M Jaderberg",
        "A Vezhnevets",
        "R Leblond",
        "T Pohlen",
        "V Dalibard",
        "D Budden",
        "Y Sulsky",
        "J Molloy",
        "T Paine",
        "C Gulcehre",
        "Z Wang",
        "T Pfaff",
        "Y Wu",
        "R Ring",
        "D Yogatama",
        "D Ünsch",
        "K Mckinney",
        "O Smith",
        "T Schaul",
        "T Lillicrap",
        "K Kavukcuoglu",
        "D Hassabis",
        "C Apps",
        "D Silver"
      ],
      "year": "2019",
      "venue": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
    },
    {
      "citation_id": "3",
      "title": "Reinforcement Learning Based Speech Enhancement for Robust Speech Recognition",
      "authors": [
        "Y Shen",
        "C Huang",
        "S Wang",
        "Y Tsao",
        "H Wang",
        "T Chi"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Reinforcement Learning To Adapt Speech Enhancement to Instantaneous Input Signal Quality",
      "authors": [
        "R Fakoor",
        "X He",
        "I Tashev",
        "S Zarar"
      ],
      "year": "2018",
      "venue": "Reinforcement Learning To Adapt Speech Enhancement to Instantaneous Input Signal Quality",
      "arxiv": "arXiv:1711.10791"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised Training for Sequence-to-Sequence Speech Recognition Using Reinforcement Learning",
      "authors": [
        "H Chung",
        "H Jeon",
        "J Park"
      ],
      "venue": "Proceedings of the International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "6",
      "title": "Reinforcement learning for spoken dialogue systems",
      "authors": [
        "S Singh",
        "M Kearns",
        "D Litman",
        "M Walker"
      ],
      "year": "1999",
      "venue": "Nips"
    },
    {
      "citation_id": "7",
      "title": "Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment",
      "authors": [
        "T Paek"
      ],
      "year": "2006",
      "venue": "Proc. Dialog-on-Dialog Workshop"
    },
    {
      "citation_id": "8",
      "title": "EmoRL: Continuous Acoustic Emotion Classification using Deep Reinforcement Learning",
      "authors": [
        "E Lakomkin",
        "M Zamani",
        "C Weber",
        "S Magg",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "Proceedings -IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "9",
      "title": "A Theoretical Analysis of Deep Q-Learning",
      "authors": [
        "J Fan",
        "Z Wang",
        "Y Xie",
        "Z Yang"
      ],
      "year": "2019",
      "venue": "A Theoretical Analysis of Deep Q-Learning"
    },
    {
      "citation_id": "10",
      "title": "Pre-training Neural Networks with Human Demonstrations for Deep Reinforcement Learning",
      "authors": [
        "G Cruz",
        "Y Du",
        "M Taylor"
      ],
      "year": "2019",
      "venue": "Adaptive Learning Agents (ALA)"
    },
    {
      "citation_id": "11",
      "title": "Starcraft ii: A new challenge for reinforcement learning",
      "authors": [
        "O Vinyals",
        "T Ewalds",
        "S Bartunov",
        "P Georgiev",
        "A Vezhnevets",
        "M Yeo",
        "A Makhzani",
        "H Üttler",
        "J Agapiou",
        "J Schrittwieser"
      ],
      "year": "2017",
      "venue": "Starcraft ii: A new challenge for reinforcement learning"
    },
    {
      "citation_id": "12",
      "title": "Deep qlearning from demonstrations",
      "authors": [
        "T Hester",
        "M Vecerik",
        "O Pietquin",
        "M Lanctot",
        "T Schaul",
        "B Piot",
        "D Horgan",
        "J Quan",
        "A Sendonaris",
        "I Osband"
      ],
      "year": "2018",
      "venue": "Proceedings AAAI"
    },
    {
      "citation_id": "13",
      "title": "The atari grand challenge dataset",
      "authors": [
        "V Kurin",
        "S Nowozin",
        "K Hofmann",
        "L Beyer",
        "B Leibe"
      ],
      "year": "2017",
      "venue": "The atari grand challenge dataset"
    },
    {
      "citation_id": "14",
      "title": "Learning from demonstration (programming by demonstration)",
      "authors": [
        "S Calinon"
      ],
      "year": "2018",
      "venue": "Encyclopedia of Robotics"
    },
    {
      "citation_id": "15",
      "title": "Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition",
      "authors": [
        "D Yu",
        "L Deng",
        "G Dahl"
      ],
      "year": "2010",
      "venue": "Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning"
    },
    {
      "citation_id": "16",
      "title": "Deep neural network features and semi-supervised training for low resource speech recognition",
      "authors": [
        "S Thomas",
        "M Seltzer",
        "K Church",
        "H Hermansky"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "17",
      "title": "Graph-based semi-supervised acoustic modeling in dnn-based speech recognition",
      "authors": [
        "Y Liu",
        "K Kirchhoff"
      ],
      "year": "2014",
      "venue": "2014 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "18",
      "title": "Reinforcement Learning of Listener Response for Mood Classification of Audio",
      "authors": [
        "J Stockholm",
        "P Pasquier"
      ],
      "year": "2009",
      "venue": "2009 International Conference on Computational Science and Engineering"
    },
    {
      "citation_id": "19",
      "title": "An Emotion-Based Approach to Reinforcement Learning Reward Design",
      "authors": [
        "H Yu",
        "P Yang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 16th International Conference on Networking, Sensing and Control (ICNSC)"
    },
    {
      "citation_id": "20",
      "title": "Reinforcement learning as classification: leveraging modern classifiers",
      "authors": [
        "M Lagoudakis",
        "R Parr"
      ],
      "year": "2003",
      "venue": "ser. ICML'03"
    },
    {
      "citation_id": "21",
      "title": "A deep learningbased stress detection algorithm with speech signal",
      "authors": [
        "H Han",
        "K Byun",
        "H Kang"
      ],
      "year": "2018",
      "venue": "AVSU 2018 -Proceedings of the 2018 Workshop on Audio-Visual Scene Understanding for Immersive Multimedia, Co-located with MM 2018",
      "doi": "10.1145/3264869.3264875"
    },
    {
      "citation_id": "22",
      "title": "Adversarial Machine Learning And Speech Emotion Recognition: Utilizing Generative Adversarial Networks For Robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "venue": "Adversarial Machine Learning And Speech Emotion Recognition: Utilizing Generative Adversarial Networks For Robustness"
    },
    {
      "citation_id": "23",
      "title": "Automated screening for distress: A perspective for the future",
      "authors": [
        "R Rana",
        "S Latif",
        "R Gururajan",
        "A Gray",
        "G Mackenzie",
        "G Humphris",
        "J Dunn"
      ],
      "year": "2019",
      "venue": "European Journal of Cancer Care"
    },
    {
      "citation_id": "24",
      "title": "Deep Q-learning from Demonstrations",
      "authors": [
        "T Hester",
        "M Vecerik",
        "O Pietquin",
        "M Lanctot",
        "T Schaul",
        "B Piot",
        "D Horgan",
        "J Quan",
        "A Sendonaris",
        "G Dulac-Arnold",
        "I Osband",
        "J Agapiou",
        "J Leibo",
        "A Gruslys"
      ],
      "year": "2017",
      "venue": "Deep Q-learning from Demonstrations",
      "arxiv": "arXiv:1704.03732"
    },
    {
      "citation_id": "25",
      "title": "Learning from Delayed Rewards",
      "authors": [
        "C Watkins"
      ],
      "year": "1989",
      "venue": "Learning from Delayed Rewards"
    },
    {
      "citation_id": "26",
      "title": "Explorations in Efficient Reinforcement Learning",
      "authors": [
        "M Wiering"
      ],
      "year": "1999",
      "venue": "Explorations in Efficient Reinforcement Learning"
    },
    {
      "citation_id": "27",
      "title": "Boltzmann exploration done right",
      "authors": [
        "N Cesa-Bianchi",
        "C Gentile",
        "G Lugosi",
        "G Neu"
      ],
      "year": "2017",
      "venue": "ser. NIPS'17"
    },
    {
      "citation_id": "28",
      "title": "Reinforcement learning with dynamic boltzmann softmax updates",
      "authors": [
        "L Pan",
        "Q Cai",
        "Q Meng",
        "W Chen",
        "L Huang"
      ],
      "venue": "IJCAI International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Model-Based Regularization for Deep Reinforcement Learning with Transcoder Networks",
      "authors": [
        "F Leibfried",
        "P Vrancx"
      ],
      "venue": "Model-Based Regularization for Deep Reinforcement Learning with Transcoder Networks"
    },
    {
      "citation_id": "30",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "32",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Nonlinear analysis and classification of speech under stressed conditions",
      "authors": [
        "D Cairns",
        "J Hansen"
      ],
      "year": "1994",
      "venue": "The Journal of the Acoustical Society of America",
      "doi": "10.1121/1.410601files/674/1.html"
    },
    {
      "citation_id": "34",
      "title": "Combining acoustic and language information for emotion recognition",
      "authors": [
        "C Lee",
        "S Narayanan",
        "R Pieraccini"
      ],
      "year": "2002",
      "venue": "Combining acoustic and language information for emotion recognition"
    },
    {
      "citation_id": "35",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct Modelling of Speech Emotion from Raw Speech"
    },
    {
      "citation_id": "36",
      "title": "Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Interspeech"
    },
    {
      "citation_id": "37",
      "title": "Cross Corpus Speech Emotion Classification -An Effective Transfer Learning Technique",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Cross Corpus Speech Emotion Classification -An Effective Transfer Learning Technique"
    },
    {
      "citation_id": "38",
      "title": "Speech Emotion Recognition Using Deep Neural Network Considering Verbal and Nonverbal Speech Sounds",
      "authors": [
        "K Huang",
        "C Wu",
        "Q Hong",
        "M Su",
        "Y Chen"
      ],
      "year": "2019",
      "venue": "ICASSP, IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "39",
      "title": "Learn++: an incremental learning algorithm for supervised neural networks",
      "authors": [
        "R Polikar",
        "L Upda",
        "S Upda",
        "V Honavar"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)"
    },
    {
      "citation_id": "40",
      "title": "Incremental learning by heterogeneous bagging ensemble",
      "authors": [
        "Q Zhao",
        "Y Jiang",
        "M Xu"
      ],
      "year": "2010",
      "venue": "International Conference on Advanced Data Mining and Applications"
    },
    {
      "citation_id": "41",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "42",
      "title": "Audio-visual feature selection and reduction for emotion classification",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2008",
      "venue": "Audio-visual feature selection and reduction for emotion classification"
    },
    {
      "citation_id": "43",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE transactions on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "44",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "45",
      "title": "",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "46",
      "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems",
      "authors": [
        "M Abadi",
        "A Agarwal",
        "P Barham",
        "E Brevdo",
        "Z Chen",
        "C Citro",
        "G Corrado",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "I Goodfellow",
        "A Harp",
        "G Irving",
        "M Isard",
        "Y Jia",
        "R Jozefowicz",
        "L Kaiser",
        "M Kudlur",
        "J Levenberg",
        "D Mané",
        "R Monga",
        "S Moore",
        "D Murray",
        "C Olah",
        "M Schuster",
        "J Shlens",
        "B Steiner",
        "I Sutskever",
        "K Talwar",
        "P Tucker",
        "V Vanhoucke",
        "V Vasudevan",
        "F Viégas",
        "O Vinyals",
        "P Warden",
        "M Wattenberg",
        "M Wicke",
        "Y Yu",
        "X Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems"
    },
    {
      "citation_id": "47",
      "title": "Deep Representation Learning in Speech Processing: Challenges, Recent Advances, and Future Trends",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep Representation Learning in Speech Processing: Challenges, Recent Advances, and Future Trends",
      "arxiv": "arXiv:2001.00378"
    }
  ]
}