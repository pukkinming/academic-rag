{
  "paper_id": "2503.18964v1",
  "title": "Unifying Eeg And Speech For Emotion Recognition: A Two-Step Joint Learning Framework For Handling Missing Eeg Data During Inference",
  "published": "2025-03-20T10:26:49Z",
  "authors": [
    "Upasana Tiwari",
    "Rupayan Chakraborty",
    "Sunil Kumar Kopparapu"
  ],
  "keywords": [
    "joint learning",
    "multi modal",
    "emotion recognition",
    "speech",
    "EEG",
    "missing modality"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Computer interfaces are advancing towards using multi-modalities to enable better human-computer interactions. The use of automatic emotion recognition (AER) can make the interactions natural and meaningful thereby enhancing the user experience. Though speech is the most direct and intuitive modality for AER, it is not reliable because it can be intentionally faked by humans. On the other hand, physiological modalities like EEG, are more reliable and impossible to fake. However, use of EEG is infeasible for realistic scenarios usage because of the need for specialized recording setup. In this paper, one of our primary aims is to ride on the reliability of the EEG modality to facilitate robust AER on the speech modality. Our approach uses both the modalities during training to reliably identify emotion at the time of inference, even in the absence of the more reliable EEG modality. We propose, a two-step joint multi-modal learning approach (JMML) that exploits both the intra-and inter-modal characteristics to construct emotion embeddings that enrich the performance of AER. In the first step, using JEC-SSL, intra-modal learning is done independently on the individual modalities. This is followed by an inter-modal learning using the proposed extended variant of deep canonically correlated cross-modal autoencoder (E-DCC-CAE). The approach learns the joint properties of both the modalities by mapping them into a common representation space, such that the modalities are maximally correlated. These emotion embeddings, hold properties of both the modalities there by enhancing the performance of ML classifier used for AER. Experimental results show the efficacy of the proposed approach. To best of our knowledge, this is the first attempt to combine speech and EEG with joint multi-modal learning approach for reliable AER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion perception in human-computer interaction (HCI) refers to the process of enabling computers to detect and respond to human emotions. With this, the HCI system can provide more personalized, adaptive, and empathetic interactions  [37] . This can be achieved using a combination of inputs such as voice  [6, 29] , facial expressions  [36, 22] , body gestures  [32] , physiological signals like brainwaves  [20, 12] , heart rate  [4, 48] , muscle movements  [21, 25] , skin conductivity, etc.  [35, 45, 24, 33]  to the HCI system. Speech being one of the popular modality, plays a crucial role in HCI system like customer service help-desk, healthcare, voice-based virtual assistants, chat-bots, to name a few. For instance, in call center scenario, if a user sounds low and unhappy, a virtual assistant might offer more empathetic support or simplify the instructions. In mental health, AI-driven emotion recognition tools help therapists monitor patients' emotional well-being. Despite its potential, the challenges like data scarcity, data privacy, security, cultural bias, ethical concerns, etc. are at the forefront in ongoing research in this area. As HCI continues to evolve, the integration of multi-modal information in emotion recognition aims to make technology more human-centric, fostering more natural and meaningful interactions  [40] .\n\nOver the past decade, a majority of automatic emotion recognition (AER) studies have been focused on uni-modal emotion recognition using only single modality  [38, 3, 49] . However, human emotions being a complex representation of individuals physiological and psychological state cannot be reliably captured with uni-modal signals, specially non-physiological signal (e.g speech and facial expression). Voice, though popular, is not always a reliable modality for identifying human emotion due to several reasons, (a) emotion in voice varies significantly across individuals, cultures, and languages making them ambiguous, (b) background ambient noise can distort the voice signals, (c) multiple emotions are often blended together, and (d) emotions can be subtle, as the speakers can deliberately hide their emotions. However, even so, facial expressions and speech are still the dominant and widely used external channels for expressing emotion. The study in  [31]  showed that these two modalities account for 93% of the emotional information in human communication, thus making them critical for AER. For these reasons, emotion recognition systems combine voice with other reliable modalities like physiological data, to achieve more accurate and reliable understanding of speakers' emotion. Speech based emotion recognition relies on external cues like pitch, tone, and rhythm, which can be influenced by environmental noise, cultural differences, and a person's conscious control over their voice. In contrast, brainwaves captured through eeg directly measures electrical brain activity, providing a more internal representation of emotional states  [17, 11] . Since emotions originate in the brain, eeg signals offer deeper and more direct insight into emotional states, even when they are not outwardly expressed  [20, 12] .\n\nThe multi-modal approach addresses the limitations of using speech as a lone modality for AER. For instance, while a person might mask their emotions in their voice, physiological signals like EEG can still capture the underlying neural patterns of stress, anxiety, or excitement. Additionally, EEG is less affected by language, accent, or cultural differences, making it more universally applicable. By fusing data from both modalities, machine learning models can identify correlations between brain activity and vocal features, improving classification accuracy and reducing misinterpretations. The combined information from these two independent modalities can also enhances system reliability, especially in noisy or unpredictable environments. This approach based on multi-modality can be used in applications like mental health monitoring, personalized learning, where high accuracy emotion recognition is crucial. However, EEG requires specialized hardware (e.g. EEG headsets) and is more intrusive, which poses challenges for large-scale, everyday use. Despite these challenges, the synergy between speech and EEG offers a promising path toward more effective and empathetic human-computer interactions  [2] . Speech being hands-free as well as easy to use, and EEG being reliable, here in this paper one of our primary objective is to make use of EEG brainwaves while learning better embedding space of each emotion in audio modality, and facilitate missing modality (e.g. EEG) testing. This way, one can take advantages from reliable modalities like EEG at the time of audio+EEG model training, and reliably detect emotion at the time of testing, even if a more complex modality like EEG is missing.\n\nThough there has been numerous work on EEG based multimodal emotion recognition (EMER)  [26] , either focusing on peripheral physiological signals  [50, 39, 30, 27]  or facial expressions as well as the eye movements  [23, 19, 51, 46, 47, 43] , but direct use of the complementarity and redundancy between EEG and speech for emotion recognition has not been well explored  [42, 35, 45, 24, 18] . The work in  [42]  employ various feature-level fusion on three modalities, namely facial images (FER and Ck+ datasets), speech (RAVDESS dataset), and EEG (SEED-IV dataset), respectively, for multi-modal AER. In  [35] , the authors proposed three different deep learning systems, GhostNet, a lightweight fully convolutional neural network (LFCNN) and a tree-like LSTM (tLSTM) for feature extractions from facial expression, speech and EEG modalities, respectively. Finally, decision-level fusion strategy was adopted to integrate the recognition results of the above three modalities. In  [24] , the authors used CNN to extract EEG features and bidirectional long short term memory (BiLSTM) neural networks to extract the speech features. Furthermore, these embeddings were combined using feature-level fusion followed by a softmax layer to recognize the levels in arousal and valence dimension. They used EEG signals from DEAP dataset for emotion recognition. The audio corresponding to the video stimuli has been used as other modality. The work in  [45]  developed four different baseline systems, namely, Identificationvector + Probabilistic Linear Discriminant Analysis (I-vector + PLDA), Temporal Convolutional Network (TCN), Extreme Learning Machine (ELM) and Multi-Layer Perception Network (MLP). Furthermore, two fusion strategies on featurelevel and decision-level respectively were employed to combine EEG and speech modality. The work was conducted on the private multi-modal dataset, MED4, having recording environment with natural noises and an anechoic chamber. Result shows that fusion methods also enhance the robustness of AER in the noisy environment. The work in  [18]  employed hybrid fuzzy-evolutionary computation methodologies for both uni-modal EEG and Speech data, followed by decision-level fusion to perform multi-modal emotion recognition. The work used SAVEE and MAHNOB dataset for Speech and EEG, respectively.\n\nMost of these existing EMER work combine EEG and other complementary modality either with early or late fusion strategies. These traditional fusion method has limitations like it cannot guarantee the integrity and representativeness of fusion information simultaneously. Also, there are no studies so far that have endeavored to learn speech and EEG simultaneously via coordinated learning for recognizing human emotion. This has motivated us to propose a joint multi-modal learning approach that fuses speech and EEG modalities in a coordinated subspace to learn the conflicting and complementary information simultaneously for robust and reliable AER. In this work, we jointly learn the corresponding involuntary internal brain response and the voluntary external behavior of human emotion, i.e. via EEG and speech, respectively, which we believe simultaneously fuses the complementary information from both the modalities for reliable emotion recognition.\n\nIn addition, existing multi-modal fusion strategies do not fully focus on the fusion of the inter-modal information on top of intra-modal fusion. The intra-modal fusion is motivated from the fact that no emotion can exist in reality as a single affective (or psychological) state. Instead, human emotions are comprised of a collectively related emotional states which are result of the variations on a shared emotion content  [14] . Thus to enrich the representative biomarkers for efficient emotion recognition, they must not only capture the characteristics specific to a particular emotion, but should also capture the common characteristics shared among all the emotion classes. Figure  1 , for the purpose of representation, shows two emotion classes E 1 and E 2 . Let E 1 ∩ E 2 represent the characteristics common to E 1 and E 2 . In order to discriminate E 1 and E 2 , most researchers in literature concentrate on extracting biomarkers which in some form try to capture\n\nso that E 1 and E 2 can be distinguished without ambiguity. However, there is a biomarker space, (E 1 ∩ E 2 ), common to both E 1 and E 2 which is not utilized. In this paper, we try to embark on a process that not only captures E 1 -(E 1 ∩ E 2 ) and E 2 -(E 1 ∩ E 2 ) but also utilizes the characteristics shared by E 1 and E 2 , namely, E 1 ∩ E 2 . We call this joint emotion class learning, where we not only learn characteristics specific to a class but also learn characteristics that are common across the classes. For example, two emotions, namely, A (Anger) and H (Happy) are shown as a vector in the arousal-valence space in Figure  2 . These emotion vectors can be decomposed into the valence (H x , A x ) and the arousal (H y , A y ) axis. It can be clearly observed that the two emotions have shared characteristics, equivalent of (E 1 ∩ E 2 ) along the arousal axis (H y and A y overlap) while having emotion specific characteristics along the valence axis, namely, H x and A x .\n\nIn this paper, we devise a two step joint multi-modal learning pipeline that exploits the intra-as well as inter-modal characteristics to enrich the input modalities towards improved and reliable AER. We term this approach as jmml. The approach is designed to first do a joint intra-modal learning in the respective input modalities by employing jec-ssl  [44] , followed by joint inter-modal learning using proposed extended variant of deep canonically correlated cross-modal autoencoder. We term this variant as e-dcc-cae. In joint intra-modal learning, jec-ssl consists of two components, namely Joint Emotion Class Learning (jecl) followed by Self-similarity Learning (ssl). jecl simultaneously captures both the class-specific (intra-) and classshared (inter-) characteristics while ssl decomposes the jointly learnt emotion embedding into a common latent structure such that it ensure the maximal covariance with the original input signal while retaining the joint characteristics. Furthermore, intra-modal jec-ssl embeddings are fed into e-dcc-cae to perform joint inter-modal learning, where the system jointly learns both the modalities by mapping them into a common representation space, such that the modalities are maximally correlated. A type of multi-modal learning, where the learnt representation of respective input modalities exist in their own space but are coordinated through a correlated subspace through a similarity (e.g., Euclidean distance) or structure constraint (e.g., partial order) is also called coordinated learning in some literature and is distinct from joint learning  [7] . However we use the notation of joint learning (jmml) to represent the coordinated multi-modal learning proposed in this work. In the proposed work, we make use of bimodal non-parallel Speech and EEG data where our main aim is to enrich the most direct and intuitive communication media in human interactions and widely used modality for automatic emotion recognition (AER), i.e Speech, by comple- menting it with one of the most reliable physiological modality, EEG, through our proposed jmml approach. Given the complexity of recording EEG data which limits it's availability in realistic scenarios, our system make use of the EEG only during training to enrich the Speech representation. Thus, jmml facilitates the Speech representation enhancement with embedded EEG learning without the need of EEG availability at the test time, making it more realistic for the real-world scenarios. Lastly, the final jmml based joint modal embeddings are learnt using final-stage classifier to perform EEG emotion detection. Experimental results shows the efficacy of our proposed approach. To best of our knowledge, this study is the first attempt to combine speech and EEG with joint multi-modal learning approach for reliable AER The rest of the paper is organized as follows: In Section 2 we describe the methodology which is the main contribution of this paper. In Section 3 we use speech and EEG as two modalities and demonstrate the efficacy of the proposed pipeline and discuss the experimental results in Section 4 and conclude in Section 5. We would like to mention that this work is an extension of  [44]  where we extend the jecssl framework for multi-modal inputs along with experimental results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "The proposed approach adopts a two-step joint multi-modal learning to enrich the speech representation by infusing it with EEG based complementary representation for improved SER. As a first step, it performs a joint intra-modal learning (jec-ssl) which is then followed by joint multi-modal learning (jmml) as the second step. A system using two modalities is shown in Figure  3 , a feature extraction module extracts (1) EEG biomarkers from the raw EEG signal, and (2) Acoustic biomarkers from speech signal, separately. As a first step, the jec-ssl block, acting on each modality separately, performs joint intra-modal learning. At this stage, in addition to the individual emotion class characteristics (example, H x and A x in Figure  2 ), characteristics that are common across the different classes (example,    4 : Joint Emotion Class Learning (jecl) for C classes, followed by mbpls based ssl(jec-ssl). The j th class learns from all other classes through its similarity latent space as well as its independent latent space.\n\nH y and A y in Figure  2 ) are learnt jointly. In the second step, these jec-ssl embedding, one for each modality, are fed into EDCC-CAE block which jointly learns both the modalities by mapping them into a common representation space, such that the modalities are maximally correlated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "We describe in brief the state-of-the-art biomarkers extracted from each of the two modalities. EEG Biomarkers: We extract state-of-the-art temporal and spectral EEG biomarkers  [16] . In the temporal domain, we extract four types of biomarkers, namely, (a) Fractal dimension using Higuchi (HFD) and Petrosian algorithm (PFD), (b) Hjorth mobility and complexity parameters, (c) Detrended Fluctuation Analysis (DFA), and (d) Hurst Exponent. We extract (a) Power Spectral Intensity (PSI), (b) Relative Intensity Ratio (RIR) and (c) Spectral Entropy, are extracted from standard EEG frequency bands, namely, θ (4 -8 Hz), α low (8 -10 Hz), α high (10 -13 Hz), β (13 -25 Hz) and γ (25 -40 Hz) as spectral features. Acoustic Biomarkers: We extract 88 acoustic features from each audio file with extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) using eGeMAPSv01a  [15]  configuration file of the OpenSMILE toolkit  [34] . There are a total of 18 acoustic features, namely Pitch, Jitter, Shimmer, formant related energy, MFCCs also known as low-level descriptors (LLDs) and high-level descriptors (HLDs) are computed (mean, standard deviation, skewness, kurtosis, extremes, linear regressions, etc.) for each of the LLDs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Joint Intra-Modal Learning (Jec-Ssl [44])",
      "text": "The system performs intra-modal feature enhancement within both the modalities, individually. jec-ssl  [44]  mechanism is adopted to enhance the EEG and speech representation, individually, through a subspace mapping. The learnt embeddings, thus not only capture the characteristics of different emotions, but also capture the similarities across different emotion classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Joint Emotion Class Learning (Jecl)",
      "text": "Let e(t) ∆ = {e j (t)} E j=1 represent the raw eeg signal consisting of E channels and let a(t) represent the acoustic or speech signal. Note that in our formulation we do not require e(t) and a(t) to be parallel, namely data collected for the same subject performing the same activity. In fact, in all our experiments e(t) and a(t) are not related; we only assume knowledge of emotion labels corresponding to e(t) and a(t). Let X represent the feature extracted from the raw signal (either e(t) or a(t)).\n\nThe aim of jecl is to transform the input X into an embedded space such that the resulting embedding not only captures the class-specific properties (intra-class) but also the properties shared across all the emotion classes (inter-class). As shown in Figure  4 , {1, • • • , ( j -1), j, ( j + 1), • • • , C} = C are the C emotion classes and X j represents a sample belonging to the emotion class j. To facilitate jecl, for emotion class j ∈ C, we learn, (a) an independent branch function g j ind corresponding to the j th emotion class\n\nwhere g j ind , parameterised by θ j ind , captures the j th intra-class characteristics (black arrow path in the X j block, Figure  4 ) and (b) a similarity branch function g j sim corresponding to all the C emotion classes\n\nwhere, g j sim , parameterised by θ j sim and θ j sim , captures the interclass similarities (blue arrow path in the X j block, Figure  4 ) shared across all the C emotion classes. These two branches, learnt for every emotion class j ∈ C collectively forms an Emotion Block I B (see Figure  4 ). We implemented both these functions (1) and (2) in I B as an autoencoder (AE) consisting of an encoder (E ind , E sim ), a latent (L ind , L sim ) and a decoder (D ind , D sim ) layers. Specifically, as shown in Figure  4  the shared parameter θ j sim in the latent space, L sim , of the similarity branch is shared across all the C emotion classes. The objective is to minimise the reconstruction loss l j r () which is the sum of two measures, namely, (a) the cosine similarity and (b) the Kullback-Leibler divergence (KLD). While the cosine similarity maximizes the proximity between the predicted and the target vector (enables the network to learn the variational mapping of the input) the KLD controls the sample divergence from the centroid of class j. Together this ensures that the learnt joint embedded space to be discriminative while also being closely packed. Specifically, we minimise the cost function,\n\nwhere Θ * ind , Θ * sim , Θ * sim represents the set of all the learnable parameters\n\nsim which together minimise (3). Note that X j ind and X j sim in (3) are the intra-class and inter-class reconstructed mapping for the emotion class j respectively. Furthermore, we stack inter and intra emotion class representations to obtain a joint emotion class representation, namely,\n\nWe hypothesize that X j is a better representation of the emotion than X j itself. The AE branch in each I B aims to reconstruct itself such that jointly learnt embeddings ( X j ) of a class j are discriminative and yet mapped in the close proximity of j.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Similarity Learning (Ssl)",
      "text": "For every emotion class j ∈ C, the emotion block I B j of jecl learns the joint embedding (4) (see Figure  4 ). The task is to combine all of these jointly learnt X , as input and aims to find a suitable subspace projection which maximizes the covariance between the projected input and the original sample X (target variable). Instead of learning an interpretative model for the entire data matrix X\n\n, mbpls learns model parameters for each individual data block X j . This makes mbpls capable of capturing the relative importance measure, i.e. how much each emotion block I B j contributes to the prediction of target X.\n\nThe general underlying model of PLS is an iterative process to identify K Latent Variable (LV). For every k th LV (k = 1, • • • , K), mbpls aims to find loading vectors p k and v k which project the data to LV scores t sk and u k , indicating maximal covariance. Subsequently, the explained variance is deflated to extract more LV's. Deflation for the k th LV is calculated as:\n\nmbpls assigns an importance score (i jk ) to each X j k in the prediction of X. After the completion of an iterative process, we get loading matrices P\n\n, the mbpls based projection of each input block X j and the target X can be expressed as\n\nwhere, E j and E X are the error terms  [8] .\n\nFinally, the ssl function (ϕ ss ) is implemented via mbpls based projections of jointly learnt emotion blocks to a discriminative common latent structure. Mathematically,\n\nwhere, X ′ is the mbpls based prediction of input X corresponding to target X; obtained via discriminative subspace learning of jointly learnt emotion blocks using jecl followed by mbpls (see Figure  4 ). Thus, jec-ssl outputs a joint embedding X ′ which captures both discriminative (intra) and joint (inter) emotion class characteristics. Finally, we train a final-stage classifier to perform emotion classification on these embeddings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Joint Multi-Modal Learning (Jmml)",
      "text": "Let M be the total number of modalities and let X m represent the sample belonging to the modality m. For every modality m, jec-ssl learns the intra-modal embedding, X ′ m as shown in Figure  4 . jmml aims to learn correspondence between different modalities such that the multi-modal representation incorporates complementary information from the input modalities.\n\nWe implement an extension of deep canonically correlated cross-modal autoencoder, dcc-cae  [13] , to map the modalities onto a common representation space, such that the modalities are maximally correlated. To the best of our knowledge, this work is the first attempt to jointly learn eeg and speech modalities for emotion recognition using extended variant of dcccae. Note that the performance of dcc-cae is not affected in the absence of an input modality  [13] , namely only eeg or only speech, thereby addressing the missing modality scenario during inference. However, knowledge of all the modalities is assumed to be known during training, this can be a serious drawback in using dcc-cae.\n\nTo overcome this limitation, we extend dcc-cae network to not only self-reconstruct (s-rec) itself but also cross-reconstruct (x-rec) the other modality. This way, the network training process is not explicitly dependent on prior knowledge of the missing modality during inference. We term this variant of dcccae as e-dcc-cae (extended dcc-cae) and is implemented as an AE that takes intra-modal (jec-ssl) embedding as an input and learns the multi-modal embedding such that the canonical correlation between the transformed representations is maximized. Namely,\n\nwhere f e m and f d m denote the non-linear transformations of the encoders and decoders, respectively for the modality m.\n\nwhere,\n\nand, L r is the binary cross-entropy loss function.\n\nAs can be seen in Figure  5  each DNN-D m , takes the encoded version of a modality, m, as the input, and reconstructs itself (s-rec) as well as other modalities (x-rec). Finally, for every modality m, DNN-D m reconstructs X′ m m (self) and X′ n m (cross) from the input X m , ∀m, n ∈ {1, • • • , M}, and m n.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Database And Biomarker Extraction",
      "text": "We experiment with non-parallel bimodal data (eeg and Speech) for multi-modal emotion analysis because of the absence of publicly available parallel dataset having Speech and eeg modalities recorded for the same stimuli, simultaneously. The unavailability can be attributed to the contradictory recording condition required for eeg and Speech, while the former requires a subject's head to be as still as possible, while the latter requires a significant jaw movement to speak which would in turn induce noise in eeg, if both modalities were recorded in parallel.\n\nFor eeg, we use the deap dataset  [41]  consisting of eeg data with valence (v), arousal (a), dominance (d) and liking (l) emotion labels. deap dataset elicits emotion in response to a 1 minute audio-visual stimuli. There are 32 subjects, each shown 40 clips (trials) during which 7 physiological modalities were recorded. Each trial has physiological recording of 40 channel in which first 32 channels corresponds to eeg. Additionally a self rated emotion score (between 1 and 9) for each trial is available. In our experiments we drop the 3s pre-trial data and use only 60s of the 63s data per trial. Furthermore, we follow the pre-processing adopted in  [41] , to construct binary labels + (≥ λ) and -(< λ) on the self-assessment labels with λ = 4.5 Thus each trial, had one of two labels in the v-a-d-l space, namely, v+ or v-; a+ or a-; d+ or d-; and l+ or l-. For proposed jmml approach, we make use of v-a emotion space of the deap EEG data.\n\nAs mentioned earlier, we extract nine tempo-spectral features for each of the five standard EEG bands using PyEEG  [16]  at the trial-level without any frame-level computation. Furthermore, we perform biomarker selection to reduce the high dimension and information redundancy resulted from 32 eeg channels. We evaluated all possible subset combination of EEG bands and features to select best features and bands. The final selected features are Hjorth features, HFD, PFD, Spectral Entropy, PSI, RIR in the α low , α high , β, γ bands. This resulted in a 416 dimensional EEG feature vector.\n\nFor speech data, we use fusion of two standard audio emotion datasets, namely, (A) Berlin Emotional Database (Emo-DB)  [9]  which consists of 535 acted utterances recorded by rehearsing the memorised script in fairly clean environment, eliciting 7 emotion categories and, (B) the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [28]  which consists of 1440 samples, where the participants vocalized lexically-matched statements in a neutral North American accent, eliciting 7 emotion categories. In our experiments, we reduce the multi-class speech data to binary class by considering the most misclassified emotion class pairs, i.e, Anger-Happy, and Neutral-Sad. Lastly, in order to jointly train the eeg and speech samples, the categorical emotion label of audio samples are converted into dimensional label to match with eeg labels. This is done by relabelling Anger as v-, Happy as v+, Sad as a-and Neutral as a+. We extract, as mentioned earlier a 88 dimensional acoustic feature vector for every audio sample.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Model Configuration And Training",
      "text": "Firstly, we perform jec-ssl (Figure  4 ) for each modality, namely, eeg and speech, with binary (+, -) class labels. Thereafter, the output of the jec-ssl is fed as input to perform the e-dcc-cae (Figure  5 ) based jmml.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Jecl",
      "text": "We implement jecl with two emotion blocks (I B 1 and I B 2 ) as shown in Figure  4 . AE for both independent and similarity branch consists of 3 layers with ReLU activation, namely, E, Ł and D. To keep the AE compact we stacked only single E, Ł and D layers in each branch. We tried three different setups for selecting the number of hidden neurons in each layer; same number of neurons in E, Ł and D (setup-1), compressed latent space (setup-2) with number of hidden neurons in Ł as half of that in E and D, and expanded latent space (setup-3) with number of hidden neurons in Ł as twice of that in E and D. Each of the above mentioned setups are tried with N/2, N, 2N, 4N number of hidden neurons, where N is the dimension of the input vector. We found setup-3 with 2N neurons in E and D and 4N in Ł to be working best. Further, the decoded output from both the branches are concatenated and fed to the final fully connected (fc) dense layer with linear activation and N neural units. The final output of each jecl block is the joint representation learnt per emotion class. So, we get one N-dimension jecl output vectors for each block. The latent layer Ł sim of the similarity branches from two blocks are tied together (by sharing weights) unlike the Ł ind (independent) branches. We hypothesize that this process of joint learning helps capture not only class-specific emotion properties but also similarities across different emotion classes.\n\nAs an example, assume X t be the training set that consists of two class data X 1 t and\n\nt is input to I B 2 in a sequence. At each epoch e, both I B 1 and I B 2 are trained. While the shared latent space weights (Ł sim ) are updated for every epoch, irrespective of the block the block layer weights (Ł ind , E ind , D ind , E sim , D sim ) are updated only once per epoch when that block sees an input. The jecl is implemented in Keras  [10]  and are trained using the adam optimizer with customised loss (as discussed in section 2.2.1) for a number of epochs guided by the validation loss.\n\nWe use mbpls python package, to implement the mbpls model with two data blocks consisting of N-dimensional joint embeddings learnt from both I B 1 and I B 2 of jecl. Note that mbpls is trained on jecl output and maps them to a common latent subspace. The target vector of mbpls is the original train data itself. From these two data blocks, mbpls predicts a N-dimensional vector, such that respective contribution of each emotion block is retained. This vector is the final joint emotion class embedding which is used for emotion classification. We tune mbpls LV s using range of LV s starting from 40 to 120 with an increment of 2.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E-Dcc-Cae Based Jmml",
      "text": "We implement jmml for a bimodal cross-corpus (/nonparallel) data, eeg data represented as X 1 and speech data represented as X 2 as shown in Figure  5 . jmml is implemented using a AE branch which is stacked with a DNN based encoder DNN-E m followed by a DNN based decoder DNN-D m , for each modality m. Each encoder (DNN-E 1 and DNN-E 2 ) and decoder (DNN-D 1 and DNN-D 2 ) consists of 3 hidden layers and an output layer. Each hidden layer is trained with ReLU activation. To tune the hidden units, we adopted the similar approach as discussed in Section 3.2.1 for jecl training. The output layer of each encoders (X ′oe 1 and X ′oe 2 ) has 20 units with linear activation function which is merged and correlated with cca loss  [5] . We use keras with tensorflow as backend to implement this loss. The output of penultimate layer of each encoders (X ′e 1 and X ′e 2 ) is further used by the decoders for the reconstruction. Lastly, each decoder has two output nodes with linear activation, for x-rec and s-rec respectively. For eeg, the reconstructed output X2\n\n1 is the x-rec speech representation mapped from eeg. Similarly, the reconstructed output X1 2 is the x-rec eeg representation mapped from speech. Besides learning the cross mapping, each modality specific decoder also outputs it's own mapping represented by s-rec, namely, X1 1 and X2 2 . The overall jmml is trained using Adam optimizer with a batch size of 32 with an overall loss (Equation  8 ) for a number of epochs guided by the validation loss.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "We split the data into train set (80%) and test set (20%) for both the EEG and speech data. Further 10% of the train data is used for validation. The EEG data consisted of (v+: 808, v-: 472; a+: 818, a-: 468) samples while the speech data has (v+: 210, v-: 255; a+: 140, a-: 203) samples. For EEG data, we adopted Minority Class Oversampling (MCO) to overcome the class imbalance across the v-a emotion dimensions. Additionally, we applied the same oversampling technique to the speech data so that the number of speech samples matched the samples in EEG. We build a baseline emotion recognition system using the features mentioned in Section 3.1 and Random Forest (rf) as the final stage classifier, for both EEG and speech modalities.\n\nWe evaluate the proposed jmml approach by performing the final stage emotion recognition in four different experimental setups as shown in Table  1  for both v and a emotions. We use rf as the final stage classifier in all our experiments. We perform grid search to fix rf parameters n estimators and n depth for each of our experimental setup independently, with the grid of n estimators = (i * 10), where 100 ≤ i ≤ 500, and n depth = (2 * i), where 1 ≤ i ≤ 20. We discuss each experimental setup in detail as below.\n\n1. Baseline: Firstly, we create an individual baseline for both the modalities using the 416-D tempo-spectral feature vector (X 1 ) for eeg and 88-D acoustic feature vector (X 2 ) for speech (as discussed in Section 2.1). 2. jec-ssl: This setup performs only intra-modal learning using jec-ssl(as discussed in Section 2.2.1) with original representation (X 1 for eeg and X 2 for Speech) as input. jecssl shows an absolute improvement over Baseline for both EEG and Speech modality in terms of both accuracy and F1 scores. As seen in Table  1 , for EEG modality there is an absolute improvement of +2.5% and +4.10% for v and a respectively in terms of the F1 score, while the F1 score improvement for speech modality is +4.2% for v and +2.8% for a. 3. Baseline → e-dcc-cae: We perform this experiment to evaluate the impact of inter-modal information on AER performance, explicitly, independent of intra-modal learning. The input feature vector X m for m = [1, 2] is fed directly to e-dcc-cae module with no intra-modal learning (jec-ssl) in between. We evaluate this setup performance against the Baseline system. The original eeg representation (X 1 ) and Speech representation (X 2 ) is fed to the input node of each modality, respectively. It is to be noted that the proposed e-dcc-cae system (as discussed in Section 2.3) allows the independent inference of either modalities, i.e, eeg data is not required to be present when using Speech as the modality during inference, and vice-versa. We use s-rec output vector for final evaluation in the both modalities. We concatenate s-rec output ( X1 1 for eeg and X2 2 for speech) with original input representation (X 1 for eeg and X 2 for speech) to perform the final evaluation. Concatenating the s-rec with the original input representation, namely [X m , Xm m ], results in a significant improvement in emotion classification over baseline. Specifically, an improvement of +1.2% and +2% is observed in terms of F1 score for v and a respectively as seen in Table  1  for EEG modality. For the speech modality the improvement in terms of F1 score for v and a is +3.4% and +3.5% respectively.  4 . jmml[jec-ssl → e-dcc-cae]: Finally, we report the performance of the proposed two-step jmml approach, namely, joint intra-modal learning (jec-ssl) followed by joint intermodal learning (e-dcc-cae). Similar to Baseline → e-dcccae experimental setup, we concatenate the s-rec output ( X′1 1 for eeg modality; X′2 2 for speech modality) with jecssl based input representation (X ′ 1 for eeg and X ′ 2 for speech modality) to perform the final evaluation. The jmml results in a considerable improvement in F1 score for speech modality for both v (+6.9%) and a (+7.2%) as seen in Table  1  over the Baseline.\n\nIt is to be noted that the SER performance using jmml for speech modality not only surpasses the Baseline, but it also shows a significant improvement over jec-ssl. Specifically, jmml betters jec-ssl by +2.7% = (85.6 -82.9) for v and +4.4% = (88.2 -83.8) in terms of F1 score as seen in Table 1. However, for eeg modality, the proposed jmml approach provides a very moderate performance improvement over jecssl. This minimal performance in the EEG modality can be attributed to performance saturation, where jec-ssl has already achieved SOTA performance  [44] , leaving it less room to improve further. The significant improvement (over jec-ssl) in SER performance using jmml, clearly demonstrates the efficacy of the proposed approach. As can be seen, combining the intra-modal (jec-ssl) and inter-modal characteristics (e-dcccae) through the proposed jmml approach, shows that Speech modality benefits from the EEG data during jmml learning resulting in improved SER performance. Note that we make no effort to compare our results with  [26] , because unlike  [26]  which fuses EEG and Speech modalities either at the featurelevel or at the decision-level to perform multi-modal AER we adopt a novel attempt where the Speech modality is facilitated with the EEG modality by coordinating both modalities via joint multi-modal learning and there is no attempt to fuse information.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a novel two-step joint multi-modal learning (jmml) framework, where joint-class intra-model learning is followed by an inter-modal joint learning for improved and reliable performance of automatic emotion recognition. Our work, specifically targets to facilitate the performance of emotion recognition on speech modality, which is the most popular and easy-to-use modality but unreliable, with the help of EEG, one of the most reliable modality used to identify human emotion. It should be noted that, though reliable, using EEG is challenging due to it's complex recording requirement dur-ing practical deployment. Our approach of joint inter-modal learning addresses this challenge by exploiting the correlations between EEG and Speech modalities in such a way that the nonparallel bi-modal (i.e EEG and Speech together) data is required only during model training and not during inference.\n\nDuring the two step training, the process of joint intra-modal learning (Step 1), captures, within each modality, (a) the common characteristics across all the emotion classes in addition to the characteristics very specific to a given emotion class. This is followed by the joint inter-modal learning (Step 2) using the proposed e-dcc-cae. We eliminate the need for prior knowledge of the missing modality during inference by enabling the joint inter-modal learning to not only self-reconstruct (s-rec) itself but also cross-reconstruct (x-rec) the other modality. In a nutshell, the joint inter-modal learning (using e-dcc-cae) makes use of complementary eeg modality to enhance the Speech representation during joint training of both modalities, while keeping the modalities independent of each other at the time of inference. This way, the emotions can be reliably detected at the time of inference from even a single modality. Specifically, it does not effect the performance of the emotion recognition using the speech modality even in the absence of a reliable modality like EEG during inference. This enables building usable emotion recognition systems in realistic scenarios where it is difficult to capture EEG data at the time of inference.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two class emotion representation (E1 and E2) with shared character-",
      "page": 3
    },
    {
      "caption": "Figure 2: These emo-",
      "page": 3
    },
    {
      "caption": "Figure 2: Sample representation of the shared characteristics (Hy, Ay) and class",
      "page": 3
    },
    {
      "caption": "Figure 2: ), charac-",
      "page": 3
    },
    {
      "caption": "Figure 3: System Overview.",
      "page": 4
    },
    {
      "caption": "Figure 4: Joint Emotion Class Learning (jecl) for C classes, followed by mbpls",
      "page": 4
    },
    {
      "caption": "Figure 2: ) are learnt jointly. In the second step,",
      "page": 4
    },
    {
      "caption": "Figure 4: , {1, · · · , ( j −1), j, ( j + 1), · · · ,C} = C are the C",
      "page": 4
    },
    {
      "caption": "Figure 4: ). We implemented both these func-",
      "page": 4
    },
    {
      "caption": "Figure 4: the shared parameter",
      "page": 4
    },
    {
      "caption": "Figure 4: ). The task is to",
      "page": 5
    },
    {
      "caption": "Figure 5: Joint Multi-modal Learning (jmml) for bimodal data; s-rec and x-rec",
      "page": 5
    },
    {
      "caption": "Figure 4: jmml aims to learn correspondence between differ-",
      "page": 5
    },
    {
      "caption": "Figure 5: represents the jmml, where, DNN-Em and DNN-Dm repre-",
      "page": 6
    },
    {
      "caption": "Figure 5: each DNN-Dm, takes the encoded",
      "page": 6
    },
    {
      "caption": "Figure 4: ) for each modality,",
      "page": 6
    },
    {
      "caption": "Figure 5: ) based jmml.",
      "page": 6
    },
    {
      "caption": "Figure 4: Æ for both independent and similarity",
      "page": 7
    },
    {
      "caption": "Figure 5: jmml is implemented us-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "eeg\nSpeech\nExperiment Setup\nInput\nAcc\nF1\nInput\nAcc\nF1"
        },
        {
          "Valence": "Baseline\n70.4\n69\n78.6\n78.77\nX1\nX2"
        },
        {
          "Valence": "X′\nX′\njec-ssl\n72.4\n71.5 (+2.5%)\n82.9\n82.9 (+4.2%)\n1\n2"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arousal": "eeg\nSpeech\nExperiment Setup\nInput\nAcc\nF1\nInput\nAcc\nF1"
        },
        {
          "Arousal": "Baseline\n69.5\n68.5\n81.4\n81\nX1\nX2"
        },
        {
          "Arousal": "X′\nX′\njec-ssl\n72.3\n72.6 (+4.1%)\n83.7\n83.8 (+2.8%)\n1\n2"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multiblock PLS: Block dependent prediction modeling for python",
      "authors": [
        "A Baum",
        "L Vermue"
      ],
      "year": "2019",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "2",
      "title": "Correlation of eeg images and speech signals for emotion analysis",
      "authors": [
        "P Abhang",
        "B Gawali"
      ],
      "year": "2015",
      "venue": "British Journal of Applied Science & Technology"
    },
    {
      "citation_id": "3",
      "title": "Lightsernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali",
        "S Ghaemmaghami",
        "B Champagne"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "4",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "F Agrafioti",
        "D Hatzinakos",
        "A Anderson"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "G Andrew",
        "R Arora",
        "J Bilmes",
        "K Livescu"
      ],
      "year": "2013",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "6",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Multimodal machine learning: A survey and taxonomy"
    },
    {
      "citation_id": "8",
      "title": "MB-PLS Python",
      "authors": [
        "A Baum",
        "L Vermue"
      ],
      "year": "2023",
      "venue": "MB-PLS Python"
    },
    {
      "citation_id": "9",
      "title": "A Database of German Emotional Speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "10",
      "title": "KERAS: The python deep learning library",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "KERAS: The python deep learning library"
    },
    {
      "citation_id": "11",
      "title": "Frontal eeg asymmetry as a moderator and mediator of emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2004",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "12",
      "title": "Eegbased emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "13",
      "title": "Audio-visual fusion for sentiment classification using cross-modal au-toencoder",
      "authors": [
        "S Dumpala",
        "I Sheikh",
        "R Chakraborty",
        "S Kopparapu"
      ],
      "year": "2019",
      "venue": "32nd conference on neural information processing systems (NIPS 2018)"
    },
    {
      "citation_id": "14",
      "title": "",
      "authors": [
        "P Ekman"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "15",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "16",
      "title": "PyEEG: an open source python module for EEG/MEG feature extraction. Computational intelligence and neuroscience",
      "authors": [
        "F Bao"
      ],
      "year": "2011",
      "venue": "PyEEG: an open source python module for EEG/MEG feature extraction. Computational intelligence and neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Emotion assessment: Arousal evaluation using EEG's and peripheral physiological signals",
      "authors": [
        "G Chanel"
      ],
      "year": "2006",
      "venue": "International workshop on multimedia content representation, classification and security"
    },
    {
      "citation_id": "18",
      "title": "Multi-modal emotion aware system based on fusion of speech and brain information",
      "authors": [
        "R Ghoniem",
        "A Algarni",
        "K Shaalan"
      ],
      "year": "2019",
      "venue": "Information"
    },
    {
      "citation_id": "19",
      "title": "Fusion of facial expressions and eeg for multimodal emotion recognition",
      "authors": [
        "Y Huang",
        "J Yang",
        "P Liao",
        "J Pan"
      ],
      "year": "2017",
      "venue": "Fusion of facial expressions and eeg for multimodal emotion recognition"
    },
    {
      "citation_id": "20",
      "title": "A comprehensive survey on emotion recognition based on electroencephalograph (eeg) signals",
      "authors": [
        "K Kamble",
        "J Sengupta"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "21",
      "title": "A new approach for emotions recognition through eog and emg signals",
      "authors": [
        "M Kose",
        "M Ahirwal",
        "A Kumar"
      ],
      "year": "2021",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "22",
      "title": "The role of facial movements in emotion recognition",
      "authors": [
        "E Krumhuber",
        "L Skora",
        "H Hill",
        "K Lander"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "23",
      "title": "An eegbased multi-modal emotion database with both posed and authentic facial actions for emotion analysis",
      "authors": [
        "X Li",
        "X Zhang",
        "H Yang",
        "W Duan",
        "W Dai",
        "L Yin"
      ],
      "year": "2020",
      "venue": "An eegbased multi-modal emotion database with both posed and authentic facial actions for emotion analysis"
    },
    {
      "citation_id": "24",
      "title": "Multi-modal emotion recognition based on deep learning of eeg and audio signals",
      "authors": [
        "Z Li",
        "G Zhang",
        "J Dang",
        "L Wang",
        "J Wei"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "25",
      "title": "Review of studies on emotion recognition and judgment based on physiological signals",
      "authors": [
        "W Lin",
        "C Li"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "26",
      "title": "Eeg-based multimodal emotion recognition: a machine learning perspective",
      "authors": [
        "H Liu",
        "T Lou",
        "Y Zhang",
        "Y Wu",
        "Y Xiao",
        "C Jensen",
        "D Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "27",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J Qiu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "An ongoing review of speech emotion recognition",
      "authors": [
        "J De Lope",
        "M Graña"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "30",
      "title": "Semi-supervised crosssubject emotion recognition based on stacked denoising autoencoder architecture using a fusion of multi-modal physiological signals",
      "authors": [
        "J Luo",
        "Y Tian",
        "H Yu",
        "Y Chen",
        "M Wu"
      ],
      "year": "2022",
      "venue": "Entropy"
    },
    {
      "citation_id": "31",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Communication theory"
    },
    {
      "citation_id": "32",
      "title": "A systematic review of human computer interaction (hci) research in medical and other engineering fields",
      "authors": [
        "A Milani",
        "A Cecil-Xavier",
        "G Cecil",
        "J Kennison"
      ],
      "year": "2022",
      "venue": "International journal of human computer interactions Sept"
    },
    {
      "citation_id": "33",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "34",
      "title": "openSMILE, audio feature extraction tool by audEERING",
      "year": "2019",
      "venue": "openSMILE, audio feature extraction tool by audEERING"
    },
    {
      "citation_id": "35",
      "title": "Multi-modal emotion recognition based on facial expressions, speech, and eeg",
      "authors": [
        "J Pan",
        "W Fang",
        "Z Zhang",
        "B Chen",
        "Z Zhang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Multi-modal emotion recognition based on facial expressions, speech, and eeg"
    },
    {
      "citation_id": "36",
      "title": "Recognizing dynamic facial expressions of emotion: Specificity and intensity effects in event-related brain potentials",
      "authors": [
        "G Recio",
        "A Schacht",
        "W Sommer"
      ],
      "year": "2014",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "37",
      "title": "Affective computing. Cambridge",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective computing. Cambridge"
    },
    {
      "citation_id": "38",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "39",
      "title": "Multimodal physiological-based emotion recognition",
      "authors": [
        "A Sharma",
        "S Canavan"
      ],
      "year": "2021",
      "venue": "Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event"
    },
    {
      "citation_id": "40",
      "title": "Sensory modalities are not separate modalities: plasticity and interactions",
      "authors": [
        "S Shimojo",
        "L Shams"
      ],
      "year": "2001",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "41",
      "title": "DEAP: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2011",
      "venue": "DEAP: A database for emotion analysis; using physiological signals"
    },
    {
      "citation_id": "42",
      "title": "Multimodal emotion recognition using different fusion techniques",
      "authors": [
        "G Subramanian",
        "N Cholendiran",
        "K Prathyusha",
        "N Balasubramanain",
        "J Aravinth"
      ],
      "year": "2021",
      "venue": "2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)"
    },
    {
      "citation_id": "43",
      "title": "Multimodal emotion recognition using deep neural networks",
      "authors": [
        "H Tang",
        "W Liu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2017",
      "venue": "Neural Information Processing: 24th International Conference"
    },
    {
      "citation_id": "44",
      "title": "Joint class learning with self similarity projection for eeg emotion recognition",
      "authors": [
        "U Tiwari",
        "R Chakraborty",
        "S Kopparapu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 7th Joint International Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th CO-MAD)",
      "doi": "10.1145/3632410.3632417"
    },
    {
      "citation_id": "45",
      "title": "Multi-modal emotion recognition using eeg and speech signals",
      "authors": [
        "Q Wang",
        "M Wang",
        "Y Yang",
        "X Zhang"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "46",
      "title": "Multimodal motivation modelling and computing towards motivationally intelligent e-learning systems",
      "authors": [
        "R Wang",
        "L Chen",
        "A Ayesh"
      ],
      "year": "2023",
      "venue": "CCF Transactions on Pervasive Computing and Interaction"
    },
    {
      "citation_id": "47",
      "title": "Investigating eegbased functional connectivity patterns for multimodal emotion recognition",
      "authors": [
        "X Wu",
        "W Zheng",
        "Z Li",
        "B Lu"
      ],
      "year": "2022",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "48",
      "title": "A spatial-temporal ecg emotion recognition model based on dynamic feature fusion",
      "authors": [
        "S Xiao",
        "X Qiu",
        "C Tang",
        "Z Huang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Speech emotion recognition using fusion of three multi-task learning-based classifiers: Hsf-dnn, ms-cnn and lld-rnn",
      "authors": [
        "Z Yao",
        "Z Wang",
        "W Liu",
        "Y Liu",
        "J Pan"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "50",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "51",
      "title": "Valence-arousal model based emotion recognition using eeg, peripheral physiological signals and facial expression",
      "authors": [
        "Q Zhu",
        "G Lu",
        "J Yan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th International Conference on Machine Learning and Soft Computing"
    }
  ]
}