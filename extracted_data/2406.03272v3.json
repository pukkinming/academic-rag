{
  "paper_id": "2406.03272v3",
  "title": "Multi-Microphone Speech Emotion Recognition Using The Hierarchical Token-Semantic Audio Transformer Architecture",
  "published": "2024-06-05T13:50:59Z",
  "authors": [
    "Ohad Cohen",
    "Gershon Hazan",
    "Sharon Gannot"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The performance of most emotion recognition systems degrades in real-life situations (\"in the wild\" scenarios) where the audio is contaminated by reverberation. Our study explores new methods to alleviate the performance degradation of Speech Emotion Recognition (SER) algorithms and develop a more robust system for adverse conditions. We propose processing multi-microphone signals to address these challenges and improve emotion classification accuracy. We adopt a state-of-the-art transformer model, the Hierarchical Tokensemantic Audio Transformer (HTS-AT), to handle multi-channel audio inputs. We evaluate two strategies: averaging mel-spectrograms across channels and summing patch-embedded representations. Our multi-microphone model achieves superior performance compared to single-channel baselines when tested on real-world reverberant environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "SER is widely studied in the literature. Most of the reported studies deal with clean speech data and do not consider additive noise and reverberant environments typical to real-life applications. Only a few studies address the influence of reverberation and noise on SER. In these studies, the reverberant data is artificially generated by convolving the clean utterances with Room Impulse Responsess (RIRs), either simulated  [1, 2]  or recorded in a real environment with various reverberation levels  [3, 4] . Noise may also be added to the reverberant signals.\n\nA survey of Human-Robot Interaction (HRI) in a noisy environment can be found in  [5, 6] . However, this survey does not address the SER task. Previous works have shown the significant challenges of detecting emotional speech in large and reverberant rooms. Reverberation can influence the speech signal and negatively affect the predicted results  [7] . The literature on using multiple microphones for SER is very scarce. In  [8] , the robustness of Automatic Speech Recognition (ASR) systems to emotional speech in noisy conditions is addressed. A Binaural Emotional Speech Recognition (BESR) system is proposed, enabling the simultaneous acquisition of the speaker's emotional state and transcribing the uttered speech signal. Devices equipped with multi-microphones are widely available nowadays. Adding the spatial information may improve the performance of audio processing tasks, including SER. However, learningbased algorithms are challenged by differences between training and\n\nThe project has received funding from the European Union's Horizon 2020 Research and Innovation Programme, Grant Agreement No. 871245; and from the Audition Project, Data Science Program, Council of Higher Education, Israel.\n\ntest conditions, specifically a change in the microphone array constellations, e.g., when the number of microphones in train and test conditions is different.\n\nIn early works in the SER domain, various architectures such as Convolutional Neural Networks (CNN), Deep Neural Networks (DNN), Recurrent Neural Networks (RNN), and Long Shortterm Memory (LSTM) were employed. Subsequently, combinations of CNN and RNN layers emerged, showcasing enhanced performance as compared with traditional classification methodologies  [9, 10, 11] . Additionally, the effectiveness of configurations utilizing blocks comprised of CNN, LSTM and Bidirectional Long Shortterm Memory (BiLSTM), as elucidated in  [12, 13] , was proven effective. Most of the works mainly focus on unimodal learning of emotions, either text, speech, or video  [14, 15, 16] .\n\nThe Transformer architecture, initially formulated for Natural Language Processing (NLP)  [17, 18] , has found applications in the audio processing domain, including tasks such as speech separation  [19]  and audio classification  [20] . The superior performance of the Audio Spectrogram Transformer (AST) model  [20] , an adaptation of the Vision Transformer (ViT) model  [21] , was demonstrated compared to CNN-based models. It is imperative to highlight that the AST model exclusively handles single-microphone data, whereas, in real-world scenarios, multiple microphones may often be available. A drawback of Transformer models is their reliance on large training data for convergence.\n\nIn the current work, we extend the HTS-AT  [22]  architecture to accommodate multi-channel inputs, thus enhancing robustness against reverberations. Since only a limited amount of data is available for our task, we resort to fine-tuning already-trained models. Moreover, our scheme can be fine-tuned with a certain number of microphones and tested with another microphone constellation, including microphone positions and the number of microphones. We evaluated the SER performance using three datasets, Ryerson audio-visual database of emotional speech and song (RAVDESS)  [23] , Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [24] , and Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D)  [25] . We used real-life RIRs from the Acoustic Characterisation of Environments (ACE) Challenge  [26]  to add reverberation to the speech utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Formulation",
      "text": "Let x(t) denote the anechoic signal in the discrete-time domain. An M -microphone array captures this signal after propagating in the acoustic enclosure. The received microphone signals are then given by yi(t) = {x * hi}(t), i = 1, 2, . . . , M , where hi(t) is the RIR from the source position to the position of microphone i. This work aims to classify the emotion given the observations yi(t); i = 1, . . . , M . arXiv:2406.03272v3 [eess.AS] 14 Sep 2024",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Model",
      "text": "The proposed SER model is based on the Swin-Transformer  [27] , 1 a state-of-the-art hierarchical ViT  [21]  architecture, used for a variety of computer vision tasks, that utilizes shifted windows to capture long-range image dependencies. We adopted and modified a Swin Transformer architecture, namely the Hierarchical Token-semantic Audio Transformer (HTS-AT)  [22] , which aims to improve the performance and scalability of audio tasks, such as the AudioSet dataset  [28] . The HTS-AT model is designed to achieve the best performance by reducing the number of parameters, requiring fewer GPU resources, and less training time than the AST architecture  [22] . In the current work, we propose modifying the HTS-AT architecture to suit the audio processing requirements better. The main change is to adapt the model to the multi-channel audio processing task, expanding its usefulness beyond the original single-channel design. Two alternative multi-channel pre-processing strategies are examined. The first strategy applies a summation of patch tokens derived from mel-spectrograms, and the second strategy applies averaging of mel-spectrograms. Our proposed approach first fuses the multichannel information into a single stream. Then, the regular HTS-AT architecture is applied.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing And Input Features",
      "text": "We assume that the sampling rate of the audio signals is 16 kHz. Each microphone signal is first analyzed by a short-time Fourier transform (STFT), with a window size of 1024 samples and a hop size of 160 samples. The log-absolute value of the STFT bins are then aggregated to construct the mel-spectrograms.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Architecture",
      "text": "We will now elaborate on the HTS-AT architecture. The standard transformer architecture requires extensive computational resources due to the unmodified input token sequence length across all layers. This includes maintaining a large global self-attention matrix and calculating outputs and gradients at each step. The HTS-AT architecture is introduced to address these challenges. The HTS-AT is designed for supporting multiple audio tasks, e.g., classification, sound event detection, and source localization. It introduces two critical architectural optimizations: a hierarchical structure and a windowed attention mechanism. The input audio mel-spectrogram is split into localized patch tokens using a convolutional Patch-Embed CNN layer of kernel dimensions P × P , in which the patches are ordered by time segment and frequency bin. Then, tokens propagate through a series of Swin Transformer encoder groups. At the terminus of each group, a Patch Merging layer reshapes the token sequence into its original 2D mel-spectrogram. This layer merges neighboring patches and then embeds them back into a latent space of reduced length. Consequently, the memory requirements decrease exponentially with depth. Within each Swin Transformer block, window attention is restricted to non-overlapping W × W squares, partitioning the token sequence. Calculating self-attention within each window subset substantially reduces computational complexity relative to full global attention while capturing localized relationships. As patch size increases downstream, windows encapsulate larger temporal and frequency contexts. Finally, the HTS-AT incorporates a token-semantic CNN layer after the last transformer block. This layer refines the output by grouping tokens, thus capturing in-1 github.com/microsoft/Swin-Transformer formation about their time frames and frequency bins. Consequently, this enhances classification by exploiting token relationships.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Channel Methods",
      "text": "We propose two multi-channel approaches that enable the HTS-AT model to handle single-and multi-channel inputs through finetuning. Both methods maintain HTS-AT's core architecture to allow flexibility in microphone numbers during fine-tuning and evaluation.\n\nPatch-Embed Summation: Inspired by the study in  [29] , we embraced a Patch-Embed scheme where each channel undergoes a shared embedding layer followed by summation. This maintains flexibility in the microphone numbers used during fine-tuning and testing without altering the core HTS-AT architecture. On the left side of Fig.  1 , each of the M channels is analyzed to generate mel-spectrograms, which are then concatenated along the channel depth. SpecAugment  [30]  is applied collectively across all M melspectrograms. Subsequently, each mel-spectrogram is reshaped to the dimensions of a 256 × 256 image and passes through the shared Patch-Embed layer. Next, a summation operation is performed across all M -encoded channels, making this input suitable for the pre-trained HTS-AT. The summation consolidates inter-channel information, enabling more robust representations suitable for reverberant conditions. The summed embeddings are then fed to the HTS-AT feedforward block for the actual emotion classification. Average Mel-Spectrograms: On the left side of Fig.  2 , the melspectrogram of each channel is calculated, followed by an averaging stage that produces a single-channel representation, regardless of the original number of channels. Since reverberation affects each channel differently, the averaged spectrogram will tend to be less reverberant. The averaged mel-spectrogram is then augmented and structured into an image. Subsequently, the image is fed to the Patch-Embed encoder layer, generating a feature tensor with dimensions of 4096 × 96. Similarly to the first architecture, the HTS-AT feedforward block implements the emotion classification.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Study",
      "text": "In this section, we describe the experimental setup and present a comparative study of the proposed scheme and a baseline method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "Our study comprised three speech emotion recognition datasets, namely RAVDESS  [23] , IEMOCAP  [24]  and CREMA-D  [25]  datasets.\n\nThe RAVDESS dataset comprises 24 actors, evenly distributed between male and female speakers, each uttering 60 English sentences. Hence, there are 1440 utterances expressing eight different emotions: 'sad', 'happy', 'angry', 'calm', 'fearful', 'surprised', 'neutral', and 'disgust'. All utterances are transcribed in advance. Consequently, emotions are more artificially expressed than in spontaneous conversation. In this dataset, we decided to merge the emotions 'neutral' and 'calm' as representations of 'neutral'. Therefore, the output of our model is on seven classes instead of eight. A significant drawback of the dataset is its small number of utterances.\n\nThe IEMOCAP dataset comprises approximately 12 hours of speech expressing four emotions: 'happy', 'sad', 'angry' and 'neutral'. It consists of conversations between two people that are either improvised or played according to a pre-determined transcript chosen to evoke different emotions.  The CREMA-D is a dataset of 7442 original clips from 91 actors comprising 48 male and 43 female actors. Speech utterances were selected from a set of 12 sentences. The sentences were presented using one of six different emotions 'anger', 'disgust', 'fear', 'happy', 'neutral' and 'sad'.\n\nFor the multi-channel experiments, we first split the original three datasets as follows: 80% as a training set, 10% as a validation set, and 10% as a test set. We note that the same actor may appear in several splits but not the same utterance.\n\nWe used the 'gpuRIR' Python package 2 to simulate reverberant multi-channel microphone signals (setting the number of microphones to M = 4) with reverberation time in the range of T60 = 200 -800 ms with different room sizes and randomized microphone locations. The room dimensions were uniformly distributed between 3 and 8 meters, with an aspect ratio ranging from 1 to 1.6. The room's height was fixed at 2.9 meters. The sound source was placed at a constant height of 1.75 meters, while the microphones were positioned at a fixed height of 1.6 meters. The x and y coordinates were randomly determined within the room for both the sound source and microphones, while keeping them at least 0.5 meters away from the room walls. By doing so, we were also able to enlarge the datasets. For RAVDESS, 6863 reverberant speech samples were generated for training, and 852 samples were used for validation. We follow the same procedure for IEMOCAP with 7356 train samples and 2107 validation samples and for CREMA-D with 5945 train samples and 1487 validation samples. Our objective encompasses evaluating our model in real-world reverberant environments. To achieve this, we conducted tests utilizing the ACE RIR database  [26] , which comprises seven distinct rooms characterized by varying dimensions and exhibiting diverse ranges of T60. We used the subset captured by a mobile phone equipped with three microphones, which are frequently used for speech interaction, making them a practical choice for real-world SER applications. The model was fine-tuned with the synthesized RIRs and evaluated with the test sets of the various datasets convolved with the ACE RIRs.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Algorithm Setup",
      "text": "We fine-tuned our model starting from the original HTS-AT model that was pre-trained utilizing the AudioSet dataset, which comprises 2 github.com/DavidDiazGuerra/gpuRIR Table  1 : The results for the non-reverberant single-microphone case. The fine-tuned HTS-AT model is compared with  [12] . We present the weighted average accuracy (in percent) of 20 models that achieved the highest accuracy on the validation set and tested on the test set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models",
      "text": "RAVDESS IEMOCAP CREMA-D Fine-tuned HTS-AT 90% 70.93% 75.86% BiLSTM + Attention  [12]  80% 66% over two million audio samples. Each sample is 10 seconds long and is categorized into 527 distinct sound event categories. This pretraining provides a strong foundation for audio feature extraction, which we then adapt to our specific SER. In order to adapt the pretrained AduioSet model to RAVDESS, IEMOCAP, and CREMA-D, we adjusted the number of output classes to 7, 4, and 6, respectively. In the three datasets, pre-processing and warm-up strategy were carried out as demonstrated in  [22]  by providing the HTS-AT with 64 mel-bins to compute the STFT and mel-spectrograms features with 160 hop size and 1024 window size. We modified the original AdamW optimizer of HTS-AT to the traditional Adam optimizer with a learning rate of 1e -3 , and batch size of 128. We set the number of epochs to 150 for all datasets. We used cross-entropy loss as the metric. We also follow the same original HTS-AT hyperparameter settings. The patch dimensions are set to 4 × 4, the patch window length is 256 frames, and the attention window size is 8 × 8. The architectural configuration comprises four network groups, each comprising 2, 2, 6, and 2 Swin-Transformer blocks, respectively. The initial patch embed is linearly projected to a dimension of D = 96, and correspondingly, after each transformer group, the dimension increases exponentially to 8D = 768, aligning seamlessly with the principles of AST. In part of our experiments, we reduced the depth of each network group by half, thereby reducing the number of trainable parameters by half to prevent overfitting. In addition, we added an early stopping strategy with a patience of 50 for RAVDESS and 25 for IEMOCAP and CREMA-D. The overall number of parameters for the fine-tuned HTS-AT was 15.7M for the RAVDESS and 28.6M for IEMOCAP and CREMA-D. Since only fine-tuning is required, we could train the models in less than two hours on an A6000 RTX GPU.\n\nTable  2 : Results on the test sets of the RAVDESS, IEMOCAP, and CREMA-D datasets convolved with RIRs of three microphones from the ACE database. The 'HTS-AT' columns are fine-tuned on reverberant single-channel audio. The 'Avg mel' columns depict results where mel-spectrograms were averaged across four channels during fine-tuning and tested on three channels. The 'Sum PE' columns are the Patch-Embed Summation approach fine-tuned and tested on three channels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "We begin our experimental study with the single-microphone case.\n\nWe compare the fine-tuned HTS-AT model with another model developed in our group that utilizes BiLSTM combined with an Attention mechanism  [12] , which has achieved very good results on the RAVDESS and IEMOCAP datasets. We applied both schemes to a non-reverberant single-microphone signal. The results are presented in Table  1 . Furthermore, to complete establishing the baseline using the RAVDESS and IEMOCAP datasets, we fine-tuned another singlechannel HTS-AT model by fine-tuning on the artificially reverberated datasets with a uniformly distributed T60 = 200 -800 ms. Then, we compared the three models by computing the Accuracy with statistical significance measure. 3 The evaluation was performed on the reverberant test set using the ACE database only applied to one of the microphones. Examining Fig.  3  for both the IEMOCAP and RAVDESS datasets, it is evident that the two HTS-AT variants outperform the approach in  [12] . It is also clear that for RAVDESS, the HTS-AT fine-tuned with reverberant speech significantly outperforms the HTS-AT fine-tuned using clean data. While this is also true for IEMOCAP, the differences are less significant. In addition, the significant gap observed for the RAVDESS dataset compared to IEMOCAP is due to differences in the nature of the datasets. RAVDESS contains more controlled, acted emotions, while IEMO-CAP is more naturalistic.\n\n3 github.com/luferrer/ConfidenceIntervals\n\nWe now turn to the evaluation of the multi-channel schemes. In all experiments, we tested all three datasets with the utterances convolved with the three RIRs of the ACE database. We only report the results for the remote source case to emphasize the reverberation effects. The average mel-spectrogram scheme was fine-tuned using four microphones. The Patch-Embed Summation scheme was fine-tuned using three microphones. While we used different microphone counts (M = 3 and M = 4) for fine-tuning, this was done to demonstrate the flexibility of our approach in handling varying numbers of input channels. However, we acknowledge that using the same number of microphones for all models would provide a more direct comparison. In both cases, the RIRs were generated using the simulator, as explained above. All experimental procedures followed identical training-validation splits, maintaining consistent model configurations, sizes, and hyperparameters for each dataset.\n\nTable  2  provides a comparative assessment of three model variants, all fine-tuned with reverberant speech: 1) 'HTS-AT' -a singlechannel scheme, 2) 'Avg mel' -mel-spectrograms averaged across four channels, and 3) 'Sum PE' -Patch-Embed Summation across three channels. In the table, we report the accuracy and the associated confidence interval for all seven rooms and three databases. Examining these results indicates the consistent, albeit modest, improvements achieved by the 'Sum PE' approach. Embedding each channel separately before summing, the model captures subtle interchannel differences that might be lost in averaging. This allows the model to emphasize the most informative aspects of each channel for emotion recognition, leading to improved performance. Importantly, these benefits come without a considerable increase in computational complexity compared to single-channel models.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we presented a multi-microphone transformer-based model for SER in reverberant environments. Based on the HTS-AT architecture, the model employs two strategies for handling multichannel audio inputs: averaging mel-spectrograms across channels and summing patch-embedded representations. When tested on realworld reverberant environments, the results show improved SER accuracy compared to single-channel schemes. By leveraging spatial information from multiple microphones, our model was able to exhibit a more robust behavior of the SER in challenging acoustic conditions. The consistent, but not large, improvements of the proposed multi-microphone schemes show promise for developing SER systems that can perform reliably in noisy and reverberant scenarios.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , each of the M channels is analyzed to generate",
      "page": 2
    },
    {
      "caption": "Figure 2: , the mel-",
      "page": 2
    },
    {
      "caption": "Figure 1: Scheme of Patch-Embed Summation.",
      "page": 3
    },
    {
      "caption": "Figure 2: Scheme of Average Mel-Spectrograms.",
      "page": 3
    },
    {
      "caption": "Figure 3: for both the IEMOCAP",
      "page": 4
    },
    {
      "caption": "Figure 3: Accuracy and Confidence Interval on test sets convolved with",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The results for the non-reverberant single-microphone",
      "page": 3
    },
    {
      "caption": "Table 2: Results on the test sets of the RAVDESS, IEMOCAP, and CREMA-D datasets convolved with RIRs of three microphones from",
      "page": 4
    },
    {
      "caption": "Table 1: Furthermore, to complete establishing the baseline using the",
      "page": 4
    },
    {
      "caption": "Table 2: provides a comparative assessment of three model vari-",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Investigation of noise-reverberation-robustness of modulation spectral features for speech-emotion recognition",
      "authors": [
        "T Guo",
        "S Li",
        "M Unoki",
        "S Okada"
      ],
      "year": "2022",
      "venue": "Asia-Pacific Signal and Information Processing"
    },
    {
      "citation_id": "3",
      "title": "Robustness to noise for speech emotion classification using CNNs and attention mechanisms",
      "authors": [
        "L Wijayasingha",
        "J Stankovic"
      ],
      "year": "2020",
      "venue": "Robustness to noise for speech emotion classification using CNNs and attention mechanisms"
    },
    {
      "citation_id": "4",
      "title": "Affect recognition in real-life acoustic conditions: a new perspective on feature selection",
      "authors": [
        "F Eyben",
        "F Weninger",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Affect recognition in real-life acoustic conditions: a new perspective on feature selection"
    },
    {
      "citation_id": "5",
      "title": "Real time distant speech emotion recognition in indoor environments",
      "authors": [
        "M Ahmed",
        "Z Chen",
        "E Fass",
        "J Stankovic"
      ],
      "year": "2017",
      "venue": "Int. Conf. on Mobile and Ubiquitous Systems: Computing, Networking and Services"
    },
    {
      "citation_id": "6",
      "title": "Effect of reverberation in speech-based emotion recognition",
      "authors": [
        "S Zhao",
        "Y Yang",
        "J Chen"
      ],
      "year": "2018",
      "venue": "IEEE Int. Conf. on the Science of Electrical Engineering in Israel (ICSEE)"
    },
    {
      "citation_id": "7",
      "title": "Challenges in acoustic signal enhancement for human-robot communication",
      "authors": [
        "H Löllmann",
        "H Barfuss",
        "A Deleforge",
        "S Meier",
        "W Kellermann"
      ],
      "year": "2014",
      "venue": "Speech Communication; ITG Symposium"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition in noisy and reverberant environments",
      "authors": [
        "P Heracleous",
        "K Yasuda",
        "F Sugaya",
        "A Yoneyama",
        "M Hashimoto"
      ],
      "year": "2017",
      "venue": "Int. Conf. on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "9",
      "title": "Robust emotional speech recognition based on binaural model and emotional auditory mask in noisy environments",
      "authors": [
        "M Bashirpour",
        "M Geravanchizadeh"
      ],
      "year": "2018",
      "venue": "EURASIP J. on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "Asia-Pacific Signal and Information Processing"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Characterizing types of convolution in deep convolutional recurrent neural networks for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "Characterizing types of convolution in deep convolutional recurrent neural networks for robust speech emotion recognition",
      "arxiv": "arXiv:1706.02901"
    },
    {
      "citation_id": "13",
      "title": "Study of speech emotion recognition using BLSTM with attention",
      "authors": [
        "D Sherman",
        "G Hazan",
        "S Gannot"
      ],
      "year": "2023",
      "venue": "European Signal Processing Conf. (EUSIPCO)"
    },
    {
      "citation_id": "14",
      "title": "CLSTM: Deep feature-based speech emotion recognition using the hierarchical ConvLSTM network",
      "authors": [
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Mathematics"
    },
    {
      "citation_id": "15",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "S Bharti",
        "S Varadhaganapathy",
        "R Gupta",
        "P Shukla",
        "M Bouye",
        "S Hingaa",
        "A Mahmoud"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "17",
      "title": "Facial expression recognition in videos: An cnn-lstm based model for video classification",
      "authors": [
        "M Abdullah",
        "M Ahmad",
        "D Han"
      ],
      "year": "2020",
      "venue": "Int. Conf. on Electronics, Information, and Communication (ICEIC)"
    },
    {
      "citation_id": "18",
      "title": "Overview of the transformer-based models for NLP tasks",
      "authors": [
        "A Gillioz",
        "J Casas",
        "E Mugellini",
        "O Khaled"
      ],
      "year": "2020",
      "venue": "Conf. on Computer Science and Information Systems (Fed-CSIS)"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need in speech separation",
      "authors": [
        "C Subakan",
        "M Ravanelli",
        "S Cornell",
        "M Bronzi",
        "J Zhong"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "AST: Audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "AST: Audio spectrogram transformer"
    },
    {
      "citation_id": "22",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "venue": "Int. Conf. on Learning Representations (ICLR)"
    },
    {
      "citation_id": "23",
      "title": "Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection",
      "authors": [
        "K Chen",
        "X Du",
        "B Zhu",
        "Z Ma",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans. on affective computing"
    },
    {
      "citation_id": "27",
      "title": "Estimation of room acoustic parameters: The ACE challenge",
      "authors": [
        "J Eaton",
        "N Gaubitch",
        "A Moore",
        "P Naylor"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Int. Conf. on computer vision"
    },
    {
      "citation_id": "29",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "IEEE Int. Conf. on acoustics, speech and signal processing"
    },
    {
      "citation_id": "30",
      "title": "Concurrent speaker detection: A multi-microphone transformer-based approach",
      "authors": [
        "A Eliav",
        "S Gannot"
      ],
      "year": "2024",
      "venue": "European Signal Processing Conf. (EUSIPCO)"
    },
    {
      "citation_id": "31",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "arxiv": "arXiv:1904.08779"
    }
  ]
}