{
  "paper_id": "2405.10213v3",
  "title": "Words As Trigger Points In Social Media Discussions: A Large-Scale Case Study About Uk Politics On Reddit",
  "published": "2024-05-16T16:02:42Z",
  "authors": [
    "Dimosthenis Antypas",
    "Christian Arnold",
    "Jose Camacho-Collados",
    "Nedjma Ousidhoum",
    "Carla Perez Almendros"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Political debates on social media sometimes flare up. From that moment on, users engage much more with one another; their communication is also more emotional and polarised. While it has been difficult to grasp such moments with computational methods, we suggest that trigger points are a useful concept to understand and ultimately model such behaviour. Established in qualitative focus group interviews to understand political polarisation (Mau, Lux, and Westheuser 2023), trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals show strong and negative emotional responses when certain triggering words or topics are mentioned. Our paper finds that these trigger points also exist in online debates. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million comments from subreddits related to a set of words identified as trigger points in UK politics. Analysing the comments, we find that trigger words increase user engagement and animosity, i.e., more negativity, hate speech, and controversial comments. Introducing trigger points to computational studies of online communication, our findings are relevant to researchers interested in affective computing, online deliberation, and how citizens debate politics and society in light of affective polarisation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Online debates may suddenly escalate, and users intensely engage in emotional and confrontational deliberations. While these turning points are difficult to model and predict, they are essential moments in online communication. They mark the moment when a discussion is likely to get off the rails, and social media users stop debating with one another and instead debate against one another. Our study introduces trigger points as a useful concept to understand why and when social media users engage in such behaviour.\n\nTrigger points, introduced by  Mau, Lux, and Westheuser (2023) , are rooted in theories of affective political identity and relate to deeply lying beliefs about moral expectations and social dispositions. They open the door to modelling social media user engagement more effectively and studying the conditions and causal mechanisms that lead to adverse reactions, hate speech, and abusive language in online debates.\n\nTrigger points are related to how individuals perceive themselves in society. People have subjective expectations about acceptable behaviour, such as explicit knowledge of law-like rules and, more importantly, an implicit understanding of appropriate actions. These deeply rooted beliefs of what is fair and unfair constitute a moral compass fundamental to navigating the social world. People feel triggered if these normative understandings are questioned. More formally,  Mau, Lux, and Westheuser (2023, 246)  define trigger points as \"those moments that question what is acceptable against the backdrop of individuals' understanding of the social contract. Trigger points jeopardise what individuals consider the fabric of society and their own role in it.\"\n\nThe authors suggest that trigger points can affect individuals through four mechanisms.\n\n• Inequality. When expectations about equality might not be met. Either similar people are treated unequally, or, to the contrary, people who should be treated differently are treated the same. For example, social benefits might be at eye level with minimal wage earners. • Norm transgression. It is the transgression of what is considered \"normal\" behaviour. A typical example relates to the lavish lifestyle of the privileged. • Slippery slope. There could be a fear of society developing norms about appropriate behaviour that goes in subjectively undesired directions and the impression that the individual might not have the power to do something against it. Claims such as \"Opening the borders so that a country could be flooded with immigrants\" is an example. • Behavioural demands. When society may be perceived as demanding unreasonable behaviour. The role of pronouns is a point in case where the generic masculine pronouns \"he/him\" might trigger some, and the neutral pronoun \"they/them\" might trigger others.\n\nGoing against deeply held beliefs about individuals and their societal role causes discomfort. People respond to such triggers in an affective behavioural mode, increasing a debate's intensity and emotionality.\n\nIn the social media context, we expect that this has two direct consequences (Figure  1 ). Trigger points lead to:\n\n• Hypothesis 1: more user engagement, which leads to more messages or comments. • Hypothesis 2: higher levels of animosity, which causes polarisation, negativity, anger, and hate.\n\nOur study seeks to establish the concept of trigger points in online communication. The research design and case selection closely follow this logic and examine whether and how individual words can act as trigger points in online debates. To this end, we conduct a large-scale case study and select five words that are ex-ante likely to work as trigger points in the context of UK politics. Specifically, we look into the words Rwanda, Brexit, NHS, Vaccine, Feminism. Collecting >100 million related Reddit comments, we use text analytics and NLP methods to understand whether these words lead to potentially negative, emotional, and harmful responses. The analysis builds upon a systematic comparison between a treatment and control group in three different scenarios:\n\n1. Space: specific subreddits where the selected words trigger users. 2. Contextual: words that appear in a similar context to the target words but without the triggering component. 3. Time: periods identified as relevant for each specific trigger word.\n\nWe find that the trigger words cause higher engagement and more animosity, measured as an increase in a thread's controversiality, negative sentiment, anger, and hate speech.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Existing work studies how particular political words or phrases lead to more engagement  (Brady et al. 2017; Hameleers et al. 2021 ) and posts with more emotion and animosity  (Friedman et al. 2004; Martin et al. 2021; Schroth, Bain-Chekal, and Caldwell 2005; Rho and Mazmanian 2020)  in various online settings. However, they lack a joint conceptual and theoretical framework-something we aim to provide.\n\nTrigger points are fundamental to understanding polarisation and division in online discussions. Given that they are at the root of different conflictive behaviours later on, they help understand the causes of disagreements in online conversations (De Kock and Vlachos 2021) and their escalation  (De Kock and Vlachos 2022; van der Meer et al. 2023)  triggering explicit and implicit ad hominem attacks, hate speech, abusive or offensive language on social media  (Talat and Hovy 2016; Davidson et al. 2017; Basile et al. 2019; Ousidhoum et al. 2019; Sap et al. 2019; ElSherief et al. 2021; Ocampo et al. 2023) .\n\nSimilarly, considering the broad array of harmful online behaviours, the wide range of elements they involve, and how they can be included in social media data  (Olteanu et al. 2019) ,  Blodgett et al. (2020)  report a missing paradigm that helps assess the initial causes without mainly focusing on the performance of the detection model. Studying trigger points helps track the origins of bias, assess whether a social media post can lead to toxic comments  (Dahiya et al. 2021) , and how language affects polarisation on social media (Karjus and Cuskley 2024). It lays the groundwork for understanding how polarised rhetoric may emerge and lead to intergroup conflict  (Simchon, Brady, and Van Bavel 2022) .\n\nRelated to trigger points are dog-whistles-coded expressions that carry two meanings: a seemingly harmless one, and another one with a hidden hateful or provocative connotation that only insiders can decipher  (Breitholtz and Cooper 2021) . In social sciences, the concept of dog whistle has its roots in the study of racial politics in the United States, leading politicians to develop more subtle ways of appealing to racial prejudices  (López 2013 ) and has evolved to encompass a broader range of phenomena, ranging from perpetuated systemic racism  (Filimon and and 2024) , to more broadly communication strategies of political ambiguity in identity politics  (Tolvanen 2024) , such as religion  (Perry 2023)  or immigration  (Filimon 2016) . Trigger points are similar in their equivocality and potential to evoke emotional responses. Conversely, using a dog whistle is conscious, but there is typically no intent when relying on trigger points. Moreover, while dog whistles are rhetorical devices to hide the real intention from some and create a rallyaround-the-flag effect, trigger points are not hidden communication and incite a strong sense of otherness towards those covered by the trigger.\n\nResearch on the automation of identifying affective triggers in online discussions remains limited. For instance,  Almerekhi et al. (2022)  developed a framework to identify triggers for toxic behaviour within Reddit discussions. Similar work typically uses a restricted set of lexical features, primarily sentiment analysis, and often examines the level of engagement some topics may generate  (Aldous, An, and Jansen 2023) . Others, such as  Beel et al. (2022) , explicitly investigate highly controversial topics like abor-\n\nTable  1 : Trigger words and their respective trigger mechanisms (described in the introduction).\n\ntion, climate change, and gun control, attempting to predict contentiousness within discussions, often heavily relying on Reddit comment upvote ratios as a proxy for contentiousness. Finally, some studies focus on online conversational tensions by predicting conversational derailment, considering the overall progression of a conversation rather than specific terms as causes  (Zhang et al. 2018; Hessel and Lee 2019) .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "To empirically demonstrate that individual words can trigger online debates, we look into most likely cases in the context of UK politics. Based on a curated set of likely candidate words for the political context in the UK (see Section 3.1), we test whether individual words can trigger the reactions outlined in the two hypotheses, i.e., (1) more engagement and (2) animosity. Our goal is to report behavioural mechanisms similar to those observed by  Mau, Lux, and Westheuser (2023)  when conducting interviews to analyse the perceived polarisation. If we could not find these effects in these intuitive cases, we would falsify the theory for online contexts.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Term Selection",
      "text": "We selected National Health Service (NHS), Brexit, Rwanda, Vaccine, and Feminist/Feminism. These terms are associated with high-profile public debates, policy changes, and social movements in the UK. Hence, they typically provoke significant public engagement and emotional responses. We expect them to cause the intended reactions in British social media users. the population was already evident during the tightly contested 2016 referendum  (Arnorsson and Zoega 2018) . Additionally, misinformation and disinformation have intensified these debates  (Höller 2021; Bruno, Lambiotte, and Saracco 2022) . Here, considerations about treating EU migrants differently than the national population (inequality), norm transgressions of \"criminal\" migrants, particularly from Eastern Europe, or opening the gate to \"unlimited\" immigration make Brexit a potential trigger word for analysis.\n\nNHS. Due to financial austerity measures, the UK government was accused of reducing its budget and neglecting the healthcare system (The Guardian 2022; The Independent 2024). Public opinion is divided, with some supporting that the NHS receives adequate funding, while others believe it wastes resources or that its services are in decline  (Gershlick, Charlesworth, and Taylor 2015) . This trigger word is related to going against considerations about unequal treatment (i.e., inequality as shown in Table  1 ).\n\nVaccine. Public vaccination programs, particularly those against COVID-19, have caused significant debate in recent years. Post-roll-out, debates have shifted to focus on issues such as the fairness of vaccine distribution and unreasonable demands for vaccination  (Nichol and Mermin-Bunnell 2021) . Moreover, the proliferation of misinformation and conspiracy theories has further fuelled public division  (Hayawi et al. 2022; Garett and Young 2021) , particularly in social media, making Vaccine an ideal trigger word for our study.\n\nFeminism. Misogyny can be observed on social media, where anonymity often allows users to express offensive and hateful sentiments  (Barker and Jurasz 2019) . Some social media influencers have built a large audience by sharing and promoting harmful content against women. This has led to the formation of online echo chambers  (Farrell et al. 2019)  where such views are amplified and propagated. Our research includes Feminism as a trigger word based on mechanisms related to norm transgressions, fears of a slippery slope, and unreasonable behavioural demands.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "Our study uses data from Reddit comments in English, collected via the Pushshift API  (Baumgartner et al. 2020) . We searched for exact matches of specific keywords in comments from 2019 to 2022 and then collected the entire discussion thread for each extracted comment.  2  We excluded comments in languages other than English, identified as such using a fastText-based language identifier  (Bojanowski et al. 2017) . Comments marked as \"[deleted]\" by the API were also removed. Following this, only threads that contained comments before and after the appearance of the target keywords were retained, as it is crucial to analyse the potential differences before and after the appearance of a given trigger point.\n\nAs a final filtering step, we limited our analysis to comments 30 minutes before and 30 minutes after the first mention of the triggering word. Previous research has shown that Reddit comments typically receive the majority of interactions within a short window after the comment is created  (Thukral et al. 2018) . Moreover, early responses are often important in shaping the direction of the conversation  (Singer et al. 2016; Horne, Adali, and Sikdar 2017) . Considering the fast-paced nature of Reddit, the 30-minute window filters out unrelated responses and ensures the focus of our analysis remains on the most relevant data 3  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Research Design",
      "text": "We identify the causal effect of trigger points with the difference-in-difference estimator (DiD), which systematically compares a treatment with a control group  (Angrist and Pischke 2009; Card and Krueger 1994) . It calculates the causal effect of an intervention as the difference before and after the intervention in the treatment group minus the difference before and after the intervention in the control group. In our context, we expect users in the treatment group to be highly likely to be triggered by the respective terms and those in the control group to be much less triggered by the respective words. There should be a more substantive behavioural change for the treatment group than for the control group.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Control Groups",
      "text": "We systematically compare treatment and control groups along three different dimensions. 4\n\n1. Space Discussions on Reddit are structured in different fora, also called subreddits. Aiming to investigate the effects a trigger word might have on specific subsets of users, we define the treatment group as a set of popular subreddits related to the trigger word (Table  2 ). We compare them with discussions in a control group, i.e., the rest of Reddit with many non-domain specific subreddits.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Contextual",
      "text": "We also compare trigger words with terms that appear in a similar context, but that does not go against the more profound underlying transgressions responsible for trigger responses in similar contexts. To members of the subreddits in Table  2 , using an explicit trigger word or its more harmless control term should make a real difference. We define related control group words and contrast woman to Feminism/Feminist, Red Cross to NHS, EU/European Union to Brexit, surgery to vaccine, and names of Rwanda's neighbouring countries 5  to Rwanda. Treatment and control groups are from the same subreddits in the same year.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Temporal",
      "text": "In the third experiment, we use the fact that two of our terms-Vaccine and Rwanda-only became explicit trigger words at specific points in time. For \"Rwanda\", we compare discussions in subreddits likely to spark reactions in 2020 to discussions in 2022. For Vaccine we compare threads from 2019 (before the COVID-19 pandemic) to 2022's (post COVID-19 pandemic). Again, we expect that users from the subreddits in Table  2  are triggered by those only in 2022 but not in the respective earlier years. Treatment and control groups share the same subreddits but are from different years.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Statistical Analysis",
      "text": "We estimate the trigger words' treatment effect in 60 separate experiments-12 experiments for H1 (Table  3 ) and 48 experiments for H2 (Table  5 ). For each experiment, we prepare the data in the same way. The treatment is the first mention of a trigger word in a discussion thread, i.e., there is only one well-defined treatment per thread. To test H1, we count the number of comments for each discussion thread 30 minutes before and 30 minutes after the treatment in the treatment group and the placebo treatment in the control group. We then calculate the share of comments before and after the intervention. For example, if there are 8 comments during the 30 minutes before the trigger word and 12 comments during the 30 minutes after the trigger word, we normalize the data of this discussion thread to 40% before intervention and 60% after intervention. For experiments with H2, we count the number of comments expressing animosity in a thread and then normalize the data as a share of animosity per thread before and after the trigger word.\n\nWe estimate the causal effect in an OLS-regression framework. More specifically, we specify the DiD with the twoway fixed effects estimator  (Angrist and    Pischke 2009):  proportion ∼ trigger dummy+treatment dummy+interaction where proportion is the per-thread-normalised target feature and the interaction term is given by interaction = trigger dummy × treatment dummy.\n\nSince the DiD estimator subtracts the difference before and after intervention in the control from the equivalent in the treatment group, no further control variables are necessary-if the parallel trends assumption holds  (Roth et al. 2023) . It requires that in the absence of treatment, the difference between the outcomes of the treated and control groups would follow the same trajectory over time. We investigate the plausibility of the assumption in section 4.3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Features",
      "text": "We rely on several features to examine our initial hypotheses and study the impact of trigger words in online discussions. We use comment counts for the first hypothesis (H1) about user engagement. The second hypothesis (H2) requires a measure of animosity. To identify relevant features that can operationalize animosity, we build on various language models that follow the RoBERTa architecture  (Liu et al. 2019)  and are trained in social media corpora, specifically X (Twitter). Even though comments on Reddit tend to be longer than on X, the posts on both platforms share many similarities, e.g., the use of slang, emojis, or unstructured text. Hence, we expect the trained models to perform well in either context  (Priya et al. 2019) .\n\nUser engagement (Hypothesis 1) Our first hypothesis suggests that trigger words lead to higher user engagement. We rely on the number of comments to investigate this and expect an uptick in message frequency after the trigger words are mentioned for the first time in a thread.\n\nAnimosity (Hypothesis 2) Hypothesis 2 suggests that trigger words cause more animosity in a debate. We operationalize this with controversial comments, negative sentiment, anger, and different forms of hate speech.\n\nControversiality We examine whether a comment is controversial using the Reddit controversiality feature. Reddit offers a measure that identifies a comment as controversial if it receives similar amounts of upvotes and downvotes. This metric is based on the users' approval, so care must be taken when considering already controversial subreddits. We expect trigger words to cause more controversiality. Sentiment analysis For sentiment analysis, we use twitter-roberta-base-sentiment-latest  (Loureiro et al. 2022 ) which identifies a comment as negative, neutral, or positive. This model has been fine-tuned for sentiment analysis using the SemEval 2017 Sentiment Analysis dataset  (Rosenthal, Farra, and Nakov 2019) . As we expect trigger words to cause more negative sentiments, we use sentiment analysis to investigate this, similarly to work using it to gain insights into how controversial topics such as vaccination  (Melton et   Negative emotions We leverage twitter-roberta-baseemotion-multilabel-latest (Camacho-Collados et al. 2022) to assign one or more emotions to each tweet. This model is trained on the Semeval 2018 Affect in Tweets task  (Mohammad et al. 2018) , covering 11 different emotions. 6  ) For our analysis, we focus on anger specifically, and we expect to observe more anger after a trigger word.\n\nHate speech Finally, we use the twitter-roberta-basehate-multi-class model  (Antypas and Camacho-Collados 2023)  to detect hate speech. It is trained on a combination of 13 different hate speech Twitter datasets and can predict hate towards seven target groups. 7  ) The inclusion of hate speech detection as a feature is motivated by previous research revealing hateful trends in topics such as politics  (Rieger et al. 2021 ), women's rights  (Farrell et al. 2019) , and health issues  (Walter et al. 2022) . Considering the connotations of some of the triggers, we specifically focus on sexism for Feminism/Feminist and racism for Rwanda. For NHS, Brexit and Vaccine we consider all hateful comments jointly.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Validation Of Annotation Model",
      "text": "The primary aim of our research is to use simple and computationally efficient models that reliably extract signals relevant to our hypotheses. To confirm the validity and robustness of our findings, we implemented an additional validation procedure to ensure consistency across the classifiers employed in our analysis.\n\nWe sampled 100 Reddit comments from our data collection, with 20 comments selected from the subreddits associated with each trigger point (Table  2 ). The collected comments were independently annotated by three experienced NLP and CSS researchers for sentiment, emotion, and the presence of hate speech. 8 Each comment was assigned a label based on the majority vote, while disagreements were resolved through discussion. When considering the difficulty of the annotation task, the annotations present aboveaverage agreement scores for sentiment and hate speech, with Fleiss' κ scores of 0.52 and 0.56, respectively. For the multi-label emotion annotation task, where annotators had to choose from a set of 12 labels,the Krippendorff's alpha score  (Krippendorff 2011 ) reached 0.24, which is in line with other similar multi-label tasks with a high number of labels  (Mohammad et al. 2018; Antypas et al. 2022; Ousidhoum et al. 2019) .\n\nWe fine-tune the base version of BERT-cased, XLNet, and RoBERTa for sentiment analysis, emotion recognition, and hate speech detection on the same datasets as the models we used for our experiments, as explained in Section 3.4  (Rosenthal, Farra, and Nakov 2017; Mohammad et al. 2018; Antypas and Camacho-Collados 2023)  to ensure fairness. We further compare our results to those of five pretrained models publicly available on HuggingFace. Specifically, two sentiment classifiers trained on Reddit data 9 , hate- 8 We include the annotation guidelines in Appendix A.4. 9 reddit-dBERT: https://huggingface.co/akshataupadhye/finetun ing-sentiment-model-reddit-data, reddit-XLNET: https://huggingf BERT  (Caselli et al. 2021 ) and one emotion classifier 10 . All models are evaluated in the aforementioned manually labelled dataset.\n\nTable  4  presents the average micro F1-scores achieved by the models across each task. We opted for the micro F1-score as it provides a more realistic representation of the results, given the limited sample and heavily imbalanced nature of our dataset. 11 The Twitter-RoBERTa models achieved the highest scores on all tasks.These scores are matched by XLNet-base for sentiment analysis and XLNet for emotion detection, and the fine-tuned models RoBERTa and XLNet for hate speech detection. Overall, the tested models performed comparably well, except for two off-theshelf sentiment classifiers that achieved scores below 50%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sentiment Emotion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "User Engagement (Hypothesis 1)",
      "text": "According to our first hypothesis, trigger points cause more comments. The DiD estimate reveals significant differences between the frequency of comments in the treatment and control groups for all our trigger words (Table  3 ).\n\nIn the treatment-only subset, the difference between user engagement before and after the trigger word is apparent for all terms. It is the largest with Rwanda where we observe a 10%-points (Table  3 ) increase. To identify the causal effect of the trigger word, we compare the difference in the treatment group to the difference in the control group. In every setting, we observe a significant difference in the number of comments in the threads between the treatment and control subsets, with the frequency of comments in the treatment group increasing by a larger factor according to the DiD estimate. The lowest value is a 3.2%-point increase for the contextual comparison in Brexit. The largest increase is for NHS in the contextual control group, where we observe a 13.4% point increase.\n\nWhen examining comment frequency across various control groups, the impact of specific trigger words becomes ace.co/minh21/XLNet-Reddit-Sentiment-Analysis 10 BERT-emotion: https://huggingface.co/ayoubkirouane/BERT-Emotions-Classifier 11 Detailed dataset distribution is available in Appendix A.3 more apparent. For instance, Figure  2  illustrates the distribution of comments related to the trigger Rwanda in the spacerelated experiment. It shows the overall number of messages across all threads. This trigger word leads to varying levels of engagement across different Reddit communities, with a notable increase in comments in identified communities where Rwanda is more controversial. Similar patterns are also observed in the contextual and temporal control groups for Rwanda (Appendix B, Figure  10 ). Overall, our analysis of the influence of trigger words across different control groups indicates a clear increase in engagement for each trigger word analysed, confirming our first hypothesis.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Animosity (Hypothesis 2)",
      "text": "Our second hypothesis suggests that trigger words may increase the frequency of polarised messages and lead to more animosity in the discussion. 12 Controversiality. We analyse the average controversiality of each thread and compare the proportion of controversial comments before and after the introduction of the trigger words. Our results (Table  5 ) indicate that, in general, there is an increase in controversial comments following each trigger word. Most terms display a statistically significant increase of DiD estimates when considering controversial messages in the treatment and control groups, with the biggest difference observed on the contextual comparison for Rwanda-a causal effect of 6.6% points.\n\nHowever, significant differences in controversiality do not appear across all settings. Notably, there seems to be no increase in controversial comments for Feminism, and in the temporal and contextual settings for Vaccine and NHS, respectively.\n\nFigure  4  shows the percentages of controversial threads related to the trigger word Rwanda for the space experiment, 12 An example of such a discussion is presented in Appendix E. before and after its occurrence. The data indicates a noticeable increase in thread controversiality caused by the trigger word, reflected by a DiD estimate of 2.3% points. The contextual and temporal control settings follow a similar trend, as documented in Appendix B, Figure  9 . Sentiment & emotion. We further investigate the effect trigger words have on sentiment and emotion. Based on our hypothesis, trigger words cause users to express more negatively charged emotions. In our analysis, we focus on the presence of negative sentiment and, more narrowly, the presence of anger in emotions.\n\nTable  5  indicates a clear trend for the negatively charged comments of the treatment and control groups for each trigger word. We observe a significant increase in comments conveying a negative sentiment after the trigger word appears in 11 of the 12 settings tested, with the sole exception being the contextual comparison for NHS. The largest effect in sentiment, with a DiD estimate of 5.5% points, appears for the Rwanda trigger word in the space and contextual experiments.\n\nLet us consider the vaccine trigger, Figure  3 , as an example. Here, we again plot the overall number of messages we observe. Both treatment and space control groups exhibit a similar pattern of negative sentiment density as time approaches the trigger word, with an increase in negative sentiment just before and after the trigger word is mentioned. However, the treatment group displays a noticeably higher volume of messages around the trigger term, suggesting that triggers amplify the expression of negative sentiment compared to the control condition.\n\nSimilarly to the increase in negative sentiment, trigger words systematically cause more anger. We observe a positive causal effect in all conditions, with only 3 nonsignificant effects. Substantively, we measure a causal effect that ranges from 1.4% points more anger (Feminist) to 5.5% points (Vaccine contextual comparison). As another example, the overall number of comments expressing anger in the Vaccine treatment and space control groups increase significantly following the appearance of the trigger word, as shown in Figure  12, Appendix B . This is further confirmed by the DiD estimates, which indicate a significant difference in anger-related comments in 9 out of 12 cases. Hate speech. As a final indicator of animosity, we also consider hate speech and abusive language, more specifically any kind of toxicity for Brexit, NHS, and Vaccine, and the more domain-specific racism-related hate speech for Rwanda and sexism-related hate speech for Feminism. As seen from Table  5 , the impact of trigger words regarding the occurrence of hateful or offensive comments is not as prevalent as anger and negative sentiment. We observe significant effects in those areas where we can count on a domain-specific hate speech measure, i.e., in the cases of Feminism (space and contextual comparison) and Rwanda (space group).\n\nWe illustrate our findings in a visualization, where we consider all messages irrespective of the thread in which they are posted. We observe a distinct pattern, particularly in the context of Rwanda and Feminism, where the volume of hateful content increases after the trigger. Figure  5   veals the prevalence of racist and sexist comments within the treatment groups when compared to their control group in the space experiment.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Robustness Of Results",
      "text": "We conduct a series of robustness checks designed to test the sensitivity of our results to assumptions and alternative specifications.\n\nTesting the Parallel Trends Assumption. The DiD estimator relies on the assumption of parallel trends: without the treatment, the treated and control groups would follow the same trajectory over time. While there are no formal guarantees for the assumption to hold, there are means to argue in its favour. The first argument is a theoretical one.\n\nIn neither of our experimental designs did we have reason to believe that the frequency of expressing animosity would not follow the same trend without the trigger word. Whether comparing treatment and control groups based on different Reddit fora (spaces), different words (contextual), or points in time (temporal), we suggest that without the respective trigger word, the relative difference between treatment and control group would remain the same. We implement event studies for each experiment to further empirically support our argument. The central idea is to model the respective outcome variable as it unfolds over time, which allows us to assess the dynamics of treatment impact before and after the intervention. Plotting the treatment effects at different time points before the treatment, the event study helps visualize whether the pre-treatment trends in the treatment and control groups are indeed parallel. If the trends are not parallel before the intervention, the parallel trends assumption is violated, indicating potential bias in the DID estimator  (Roth et al. 2023) .\n\nFor each of our overall 60 experiments, we divide the 30 minutes before and after the first mention of a trigger word into six distinct time intervals {  [-30, -20[, [-20, -10[, [-10, 0[, [0, 10[, [10, 20[, [20, 30[} instead of just two.  This allows us to follow, in particular, the pre-treatment trends at a higher granularity. We calculate each discussion thread's share of (hostile) messages in the six intervals. Averaging over all these data points per period, we subtract the control group from the treatment group and calculate the difference of each observation from the first observation period.\n\nFigure  7  charts the time-wise means over all 60 event studies. If the parallel trends assumption holds, the lines before the treatment at time = 0 should all remain on the horizontal line where the treatment effect = 0. Indeed, this is largely what we find. The mean treatment effect is close to 0 before the trigger word is mentioned for the first time. After the intervention, it is clearly above the horizontal 0-line.  13 Placebo Treatment. To further validate our Difference-in-Differences (DiD) estimates, we implement a placebo treatment analysis. The core logic of this approach is to check  whether the estimated treatment effects could be driven by other trends earlier or later than the actual intervention.\n\nIn the placebo analysis, we systematically shift the assumed treatment time in 10-minute intervals, ranging from 30 minutes before to 30 minutes after the actual intervention, i.e., the comment with the first mention of the trigger word in a discussion thread. At each placebo point, we re-estimate all DiD models as if the treatment had occurred at that alternative time. If our results capture a genuine causal effect, we expect a pronounced treatment effect at the actual intervention time (t = 0), and near-zero or statistically insignificant estimates at all placebo points before and after.\n\nFigure  6  visualizes these placebo tests by plotting the precision-weighted average treatment effects and their confidence intervals across all experiments measuring animosity (see Table  5 ). At the actual treatment point t = 0, which compares message content in the 30 minutes before and after the first appearance of the trigger word  ([-30, 0[ vs. [0, 30] ), we observe a clear and substantial effect (1.99). We then shift the treatment window rightward in 10-minute steps-first comparing  [-20, 10[ to [10, 40] , and so on-until the actual intervention fully exits the analysis window at t = 30. This generates a steady decline in estimated effects, which converge to zero, as expected if no real treatment occurred at these placebo times. Similarly, shifting the window to earlier times (e.g., comparing  [-60, -30[ to [-30, 0] ) reveals a decreasing trend in effect size, though some residual difference remains. In conclusion, given the clear peak of the effect at the timing t = 0, we are confident in the robustness of our approach to defining the moment of the actual treatment moment.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper establishes trigger words as a concept in online debates. Since our work represents the first systematic computational study of this concept, our research design and case selection are geared to clearly identify such behaviour in the most likely cases. To do so, we performed a largescale analysis about UK politics on Reddit using five trigger words (Rwanda, Feminism, Brexit, NHS, and Vaccine).\n\nThe results show that trigger words cause more user engagement, and this activity is marked by higher levels of animosity. This confirms the first hypothesis (H1), which suggests more user interaction after the appearance of a trigger word. Regarding our second hypothesis (H2), which concerns the impact of trigger words on the animosity of the ensuing discussions, we examine four different features that can indicate such effects. Trigger words cause more controversiality, more negative sentiment, and more anger. We also investigate the causal effect of trigger words on hate speech.\n\nWhere we can, we rely on a model capable of identifying domain-specific forms of hate -sexism in the case of Feminism and racism in the case of Rwanda-. Our findings similarly confirm that trigger words play a causal role in this aspect as well.\n\nFrom this work, we can certify that trigger points exist in online communication, and we have provided a framework to measure the extent of them in terms of user engagement and animosity. Future work will analyse this phenomenon comprehensively. We aim to broaden the scope, study how prevalent trigger points are in different countries and languages, and seek to better understand further behavioural consequences from trigger points. It is our intention to evaluate the extent to which trigger points are responsible for hateful online communication. We also strive to automate the trigger point detection process to detect the respective words in online conversations when we do not know them ex-ante.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "We refrain from using large language models (LLMs) for feature extraction mainly due to the vast size of our dataset (>100 million entries). Using LLMs would have significantly increased the computational and financial cost and the ecological impact of our experiments. Moreover, existing literature indicates that the zero-shot capabilities of LLMs do not consistently outperform smaller models for specific social media tasks like ours  (Antypas et al. 2023) .\n\nOur study does not attempt to map how pervasive trigger points are in online debates. The chosen subreddits may not fully represent the broader range of discussions these trigger words could provoke across the diverse Reddit community. Similarly, we acknowledge the limitations and potential bi-ases of our work, as we only investigate a small set of potential trigger words in the context of the UK and limit our data to comments written in English in a limited selection of subreddits. The choice of trigger words is informed by a contextual analysis of the UK politics, and the Reddit threads have been selected due to the often polarised conversations they host around such topics. However, this is the first study that analyses trigger points' concept validity in online conversations. Based on our empirical evidence, we can be confident that we have found relevant evidence that warrants further investigation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ethics Statement",
      "text": "All data used in our research is sourced from publicly available information or collected using the official Reddit API. It is provided in an aggregated way and without reporting sensitive information from individual users.\n\nThe automatic methods used can be abused by those with the power to repress people with opposing opinions. Hence, we offer a comprehensive study but do not share our data publicly to avoid potential misuse or commercial use. Researchers interested in replicating our experiments can contact us directly.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Model Selection",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A.1 Emotion Categories",
      "text": "The twitter-roberta-base-emotion-multilabel model classifies each entry in one or more of the following classes: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust.\n\nA.2 Hate Speech Categories.\n\nThe twitter-roberta-base-hate-multiclass model classifies each entry in one of the following classes: not hate, sexism, racism, religion, other, sexual orientation, disability",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.3 Validation Setting",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.4 Annotation Guidelines",
      "text": "Below we present the guidelines presented to annotators for the Sentiment Analysis, Emtion classification, and Hate Speech detection tasks.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "We will show you Reddit comments that can be positive, negative, or neutral based on their sentiment. Sentiment Categories",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Emotion Classification",
      "text": "You will be presented with Reddit comments and asked to select the option that best describes the text. You may choose as many emotions as you believe are present -from 0 to 11 -based on the content and tone of the comment.\n\nEmotion Categories:\n\n• Anger -includes annoyance, rage",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B Extended Results",
      "text": "Table  7  presents the total number of comments and threads extracted from Reddit for each keyword, along with the corresponding time periods during which the data was collected.\n\nTable  8  presents the number of comments included in each experiment for both the treatment and control groups.\n\nFigure  8  displays the number of comments posted before and after the introduction of each trigger point on the data collected.\n\nFigure  9  displays the average controversiality per thread before and after the introduction of the Rwanda trigger point in the Temporal and Contextual control settings.\n\nFigure  10  displays the distribution of comments posted before and after the introduction of the Rwanda trigger point in the Temporal and Contextual control settings.\n\nFigure  11  displays the distribution of negative comments, based on their sentiment, posted before and after the introduction of the Brexit, Feminism, Rwanda, and NHS trigger points in the Space control setting.\n\nFigure  12  displays the distribution of comments conveying anger and those that do not before and after the introduction of the Vaccine trigger point in the Space control setting.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C Extended Test Of Parallel Trends Assumption",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D Computational Resources",
      "text": "An NVIDIA GeForce RTX 4090 GPU was utilised for the experiments conducted. 170 hours spent for the inference process of sentiment analysis, emotion detection, and hate speech detection models.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "E Selected Reddit Comments",
      "text": "Below we present a Reddit discussion and how it evolves when the trigger \"feminism\" appears. The comment containing the trigger is bolded.\n\nWARNING: the following comments contain sensitive and potentially offensive language. Reader discretion is advised.\n\n\"This image though...\"\n\n\"Haunting for us, isn't it? But not for the females.\"\n\n\"Well I'm sure most females would find this horrifying too.\"\n\n\"You a female? Why are you on this Sub?\"\n\n\"Because I'm against feminism?\"\n\n\"Well, it's better if these Posts are seen by all the Men out there. Get the taste of true Chameleon female nature.\"\n\n\"Too late. I already saw it.\"\n\n\"There are many women like us on this sub .\"\n\n\"Yeah I'm a woman and I don't like it. Women are not a hive mind.\"\n\n\"Women are not a hive mind. You should go out and to one maybe.\"\n\n\"My fucking eyes. Sometimes I wish I was blind.\"\n\n\"When Male dies, it's \"\"sometimes\"\" sad; when Female dies.............CRIMEEEEE!!!! \"The fucked up world we live in\"",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Consequences of trigger points. H1 refers to hy-",
      "page": 1
    },
    {
      "caption": "Figure 1: ). Trigger points lead to:",
      "page": 2
    },
    {
      "caption": "Figure 2: Distribution of comments in the treatment and con-",
      "page": 5
    },
    {
      "caption": "Figure 3: Distribution of negatively charged comments for",
      "page": 6
    },
    {
      "caption": "Figure 4: Average controversiality per thread for the treat-",
      "page": 7
    },
    {
      "caption": "Figure 2: illustrates the distribu-",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the percentages of controversial threads",
      "page": 7
    },
    {
      "caption": "Figure 9: Sentiment & emotion. We further investigate the effect trig-",
      "page": 7
    },
    {
      "caption": "Figure 3: , as an ex-",
      "page": 7
    },
    {
      "caption": "Figure 12: , Appendix B. This is further confirmed",
      "page": 7
    },
    {
      "caption": "Figure 7: charts the time-wise means over all 60 event",
      "page": 8
    },
    {
      "caption": "Figure 5: Distribution of hateful comments in the treatment and control groups (space) of the trigger points Rwanda and",
      "page": 9
    },
    {
      "caption": "Figure 6: visualizes these placebo tests by plotting the",
      "page": 9
    },
    {
      "caption": "Figure 6: Placebo treatments every 10 minutes, ranging from",
      "page": 9
    },
    {
      "caption": "Figure 7: Testing the Parallel Trends Assumption with event",
      "page": 10
    },
    {
      "caption": "Figure 8: displays the number of comments posted before",
      "page": 14
    },
    {
      "caption": "Figure 9: displays the average controversiality per thread",
      "page": 14
    },
    {
      "caption": "Figure 10: displays the distribution of comments posted",
      "page": 14
    },
    {
      "caption": "Figure 11: displays the distribution of negative comments,",
      "page": 14
    },
    {
      "caption": "Figure 12: displays the distribution of comments convey-",
      "page": 14
    },
    {
      "caption": "Figure 8: Absolute count of comments before and after each trigger point.",
      "page": 16
    },
    {
      "caption": "Figure 9: Average controversiality per thread in the Temporal and Contextual settings and their respective treatment groups, for",
      "page": 17
    },
    {
      "caption": "Figure 10: Rwanda Treatment and Control distributions of comments in the Contextual and Temporal settings.",
      "page": 17
    },
    {
      "caption": "Figure 11: Distribution of negatively charged comments in the Treatment and Control (Space) groups.",
      "page": 18
    },
    {
      "caption": "Figure 12: Distribution of anger and non-anger comments for the treatment and control (space) groups of Vaccine trigger word.",
      "page": 19
    },
    {
      "caption": "Figure 13: Testing the Parallel Trends Assumption with event studies. Individual studies in grey, their time-wise mean in purple.",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 2: ). We compare them with",
      "data": [
        {
          "Feminist\nBe\nAf\nDiD\nCI": "Treatment\n47\n53\nOnly",
          "Rwanda\nBe\nAf\nDiD\nCI": "45\n55",
          "Brexit\nBe\nAf\nDiD\nCI": "46\n54",
          "NHS\nBe\nAf\nDiD\nCI": "46\n54",
          "Vaccine\nBe\nAf\nDiD\nCI": "47\n53"
        },
        {
          "Feminist\nBe\nAf\nDiD\nCI": "Space\n4.1\n49\n51\n(3.8, 4.5)\nContextual\n4.6\n48\n52\n(3.9, 5.2)\nTemporal\n-",
          "Rwanda\nBe\nAf\nDiD\nCI": "9.7\n50\n50\n(8.2, 11.3)\n9.8\n50\n50\n(6.6, 14.9)\n7.2\n49\n51\n(2.9, 11.6)",
          "Brexit\nBe\nAf\nDiD\nCI": "6.3\n49\n51\n(6.1, 6.6)\n3.2\n47\n53\n(2.6, 3.7)\n-",
          "NHS\nBe\nAf\nDiD\nCI": "6.1\n49\n51\n(5.7, 6.4)\n13.4\n53\n47\n(8.7, 18.1)\n-",
          "Vaccine\nBe\nAf\nDiD\nCI": "4.1\n49\n51\n(3.5, 3.9)\n5.0\n49\n51\n(3.1, 6.9)\n5.9\n50\n50\n(4.1, 7.6)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: presents the average micro F1-scores achieved",
      "data": [
        {
          "Sentiment": "0.76\n0.75\n0.76\n0.74\n0.45\n0.31\n-\n-\n0.30",
          "Emotion": "0.50\n0.47\n0.45\n0.40\n-\n-\n0.45\n-\n0.20",
          "Hate": "0.88\n0.88\n0.88\n0.85\n-\n-\n-\n0.80\n0.07"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Trigger\nSetting": "Rwanda\nSpace\nTemporal\nContext.",
          "Feature\nDiD\nCI": "2.3\nContr\n(0.8, 3.7)\n5.5\nN.Sent\n(4.0, 6.9)\n4.0\nAnger\n(2.7, 5.3)\n1.8\nRacism\n(0.0, 3.6)\n6.6\nContr\n(0.2, 13.0)\n5.5\nN.Sent\n(1.5, 9.6)\nAnger\n5\n(-4.6, 14.6)\nRacism\n14.6\n(-0.7, 29.9)\n5.9\nContr\n(0.5, 11.3)\n4.8\nN.Sent\n(0.8, 8.8)\nAnger\n2.8\n(-1.0, 6.6)\nRacism\n0.9\n(-11.1, 12.8)"
        },
        {
          "Trigger\nSetting": "Vaccine\nSpace\nTemporal\nContext.",
          "Feature\nDiD\nCI": "1.5\nContr\n(1.3, 1.8)\n3.0\nN.Sent\n(2.8, 3.2)\n2.0\nAnger\n(1.7, 2.2)\n0.4\nHate\n(0.0, 0.8)\n2.2\nContr\n(1.1, 3.4)\n5.4\nN.Sent\n(4.7, 6.1)\n4.4\nAnger\n(1.6, 7.2)\nHate\n-0.7\n(-2.4, 0.9)\nContr\n-0.3\n(-3.0, 2.3)\n2.9\nN.Sent\n(1.4, 4.4)\n5.5\nAnger\n(1.4, 9.6)\nHate\n-2.0\n(-5.3, 1.3)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Trigger\nSetting": "Brexit\nSpace\nContext.",
          "Feature\nDiD\nCI": "1.0\nContr\n(0.8, 1.3)\n3.3\nN.Sent\n(3.0, 3.5)\n2.7\nAnger\n(2.4, 3.0)\nHate\n-0.1\n(-0.5, 0.4)\n1.4\nContr\n(0.7, 2.1)\n2.6\nN.Sent\n(2.0, 3.1)\n3.8\nAnger\n(2.1, 5.5)\nHate\n-0.5\n(-1.9, 0.9)"
        },
        {
          "Trigger\nSetting": "Feminist\nSpace\nContext.",
          "Feature\nDiD\nCI": "Contr\n0.3\n(-0.2, 0.8)\n2.6\nN.Sent\n(2.2, 3.0)\n1.4\nAnger\n(1.0, 1.9)\n0.6\nSexism\n(0.1, 1.1)\nContr\n0.1\n(-0.8, 1.1)\n3.0\nN.Sent\n(2.3, 3.6)\n1.4\nAnger\n(1.0, 1.9)\n1.4\nSexism\n(0.5, 2.3)"
        },
        {
          "Trigger\nSetting": "NHS\nSpace\nContext.",
          "Feature\nDiD\nCI": "1.1\nContr\n(0.7, 1.4)\n3.1\nN.Sent\n(2.8, 3.5)\n2.7\nAnger\n(2.3, 3.1)\nHate\n-0.3\n(-0.9, 0.3)\nContr\n4.5\n(-3.2, 12.2)\nN.Sent\n3.5\n(-1.2, 8.3)\nAnger\n4.2\n(-11.6, 20.0)\nHate\n-1.4\n(-12.5, 9.6)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: Numbers, in millions, of threads, comments, and",
      "data": [
        {
          "Threads\nComments\nYear": "0.05\n1.87\n2020, 2022"
        },
        {
          "Threads\nComments\nYear": "0.37\n11.57\n2020 - 2022"
        },
        {
          "Threads\nComments\nYear": "0.34\n6.20\n2020 - 2022"
        },
        {
          "Threads\nComments\nYear": "1.42\n28.11\n2019, 2022"
        },
        {
          "Threads\nComments\nYear": "0.94\n17.79\n2020 - 2022"
        },
        {
          "Threads\nComments\nYear": "0.2\n12.4\n2020 - 2022"
        },
        {
          "Threads\nComments\nYear": "0.02\n0.2\n2020 - 2022"
        },
        {
          "Threads\nComments\nYear": "0.08\n1.5\n2020, 2022"
        },
        {
          "Threads\nComments\nYear": "0.5\n3.9\n2019, 2022"
        },
        {
          "Threads\nComments\nYear": "2.1\n19.3\n2020 - 2022"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 7: Numbers, in millions, of threads, comments, and",
      "data": [
        {
          "Trigger": "Feminist",
          "Experiment\nTreatment\nControl": "space\n16,503,956\n264,495\ncontextual\n163,740"
        },
        {
          "Trigger": "Rwanda",
          "Experiment\nTreatment\nControl": "space\n668,641\ncontextual\n2,587\n28,770\ntemp\n5,896"
        },
        {
          "Trigger": "Brexit",
          "Experiment\nTreatment\nControl": "space\n10,793,036\n688,600\ncontextual\n111,524"
        },
        {
          "Trigger": "vaccine",
          "Experiment\nTreatment\nControl": "space\n7,685,095\ncontextual\n348,641\n12,986\ntemp\n11,841"
        },
        {
          "Trigger": "NHS",
          "Experiment\nTreatment\nControl": "space\n5,275,917\n509,458\ncontextual\n1,276"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "What really matters?: characterising and predicting user engagement of news postings using multiple platforms, sentiments and topics",
      "authors": [
        "K Aldous",
        "J An",
        "B Jansen"
      ],
      "year": "2023",
      "venue": "What really matters?: characterising and predicting user engagement of news postings using multiple platforms, sentiments and topics"
    },
    {
      "citation_id": "2",
      "title": "Behaviour & Information Technology",
      "venue": "Behaviour & Information Technology"
    },
    {
      "citation_id": "3",
      "title": "PROVOKE: Toxicity trigger detection in conversations from the top 100 subreddits",
      "authors": [
        "H Almerekhi",
        "H Kwak",
        "J Salminen",
        "B Jansen"
      ],
      "year": "2022",
      "venue": "Data and Information Management"
    },
    {
      "citation_id": "4",
      "title": "Mostly Harmless Econometrics: An Empiricist's Companion",
      "authors": [
        "J Angrist",
        "J.-S Pischke"
      ],
      "year": "2009",
      "venue": "Mostly Harmless Econometrics: An Empiricist's Companion"
    },
    {
      "citation_id": "5",
      "title": "Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation",
      "authors": [
        "D Antypas",
        "J Camacho-Collados"
      ],
      "year": "2023",
      "venue": "Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation"
    },
    {
      "citation_id": "6",
      "title": "SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research",
      "authors": [
        "D Antypas",
        "A Ushio",
        "F Barbieri",
        "L Neves",
        "K Rezaee",
        "L Espinosa-Anke",
        "J Pei",
        "J Camacho-Collados"
      ],
      "year": "2023",
      "venue": "Findings of EMNLP"
    },
    {
      "citation_id": "7",
      "title": "Twitter Topic Classification",
      "authors": [
        "D Antypas",
        "A Ushio",
        "J Camacho-Collados",
        "V Silva",
        "L Neves",
        "F Barbieri"
      ],
      "year": "2022",
      "venue": "Twitter Topic Classification"
    },
    {
      "citation_id": "8",
      "title": "Proceedings of COLING",
      "authors": [
        "In Calzolari",
        "N Huang",
        "C.-R Kim",
        "H Pustejovsky",
        "J Wanner",
        "L Choi",
        "K.-S Ryu",
        "P.-M Chen",
        "H.-H Donatelli",
        "L Ji",
        "H Kurohashi",
        "S Paggio",
        "P Xue",
        "N Kim",
        "S Hahm",
        "Y He",
        "Z Lee",
        "T Santus",
        "E Bond"
      ],
      "venue": "Proceedings of COLING"
    },
    {
      "citation_id": "9",
      "title": "On the causes of Brexit",
      "authors": [
        "A Arnorsson",
        "G Zoega"
      ],
      "year": "2018",
      "venue": "European Journal of Political Economy"
    },
    {
      "citation_id": "10",
      "title": "Online misogyny",
      "authors": [
        "K Barker",
        "O Jurasz"
      ],
      "year": "2019",
      "venue": "Journal of International Affairs"
    },
    {
      "citation_id": "11",
      "title": "SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter",
      "authors": [
        "V Basile",
        "C Bosco",
        "E Fersini",
        "D Nozza",
        "V Patti",
        "F Rangel Pardo",
        "P Rosso",
        "M Sanguinetti"
      ],
      "year": "2019",
      "venue": "SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter"
    },
    {
      "citation_id": "12",
      "title": "The pushshift reddit dataset",
      "authors": [
        "J Baumgartner",
        "S Zannettou",
        "B Keegan",
        "M Squire",
        "J Blackburn"
      ],
      "year": "2020",
      "venue": "Proceedings of ICWSM"
    },
    {
      "citation_id": "13",
      "title": "Linguistic characterization of divisive topics online: case studies on contentiousness in abortion, climate change, and gun control",
      "authors": [
        "J Beel",
        "T Xiang",
        "S Soni",
        "D Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of ICWSM"
    },
    {
      "citation_id": "14",
      "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
      "authors": [
        "S Blodgett",
        "S Barocas",
        "Iii Daumé",
        "H Wallach"
      ],
      "year": "2020",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "15",
      "title": "Enriching Word Vectors with Subword Information",
      "authors": [
        "P Bojanowski",
        "E Grave",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2017",
      "venue": "TACL"
    },
    {
      "citation_id": "16",
      "title": "Emotion shapes the diffusion of moralized content in social networks",
      "authors": [
        "W Brady",
        "J Wills",
        "J Jost",
        "J Tucker",
        "J Bavel"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "17",
      "title": "Dogwhistles as Inferences in Interaction",
      "authors": [
        "E Breitholtz",
        "R Cooper"
      ],
      "year": "2021",
      "venue": "Proceedings of ReInAct 2021"
    },
    {
      "citation_id": "18",
      "title": "Brexit and bots: characterizing the behaviour of automated accounts on Twitter during the UK election",
      "authors": [
        "M Bruno",
        "R Lambiotte",
        "F Saracco"
      ],
      "year": "2022",
      "venue": "EPJ Data Science"
    },
    {
      "citation_id": "19",
      "title": "TweetNLP: Cutting-Edge Natural Language Processing for Social Media",
      "authors": [
        "J Camacho-Collados",
        "K Rezaee",
        "T Riahi",
        "A Ushio",
        "D Loureiro",
        "D Antypas",
        "J Boisson",
        "L Anke",
        "F Liu",
        "E Martínez-Cámara"
      ],
      "year": "2022",
      "venue": "TweetNLP: Cutting-Edge Natural Language Processing for Social Media"
    },
    {
      "citation_id": "20",
      "title": "Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania",
      "authors": [
        "D Card",
        "A Krueger"
      ],
      "year": "1994",
      "venue": "The American Economic Review"
    },
    {
      "citation_id": "21",
      "title": "HateBERT: Retraining BERT for Abusive Language Detection in English",
      "authors": [
        "T Caselli",
        "V Basile",
        "J Mitrović",
        "M Granitzer"
      ],
      "year": "2021",
      "venue": "Proceedings WOAH 2021"
    },
    {
      "citation_id": "22",
      "title": "Would your tweet invoke hate on the fly? forecasting hate intensity of reply threads on twitter",
      "authors": [
        "S Dahiya",
        "S Sharma",
        "D Sahnan",
        "V Goel",
        "E Chouzenoux",
        "V Elvira",
        "A Majumdar",
        "A Bandhakavi",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "Proceedings of KDD"
    },
    {
      "citation_id": "23",
      "title": "Automated hate speech detection and the problem of offensive language",
      "authors": [
        "T Davidson",
        "D Warmsley",
        "M Macy",
        "I Weber"
      ],
      "year": "2017",
      "venue": "Proceedings of ICWSM"
    },
    {
      "citation_id": "24",
      "title": "I Beg to Differ: A study of constructive disagreement in online conversations",
      "authors": [
        "C De Kock",
        "A Vlachos"
      ],
      "year": "2017",
      "venue": "Proceedings of ACL: Main Volume"
    },
    {
      "citation_id": "25",
      "title": "How to disagree well: Investigating the dispute tactics used on Wikipedia",
      "authors": [
        "C De Kock",
        "A Vlachos"
      ],
      "year": "2022",
      "venue": "Proceedings of EMNLP"
    },
    {
      "citation_id": "26",
      "title": "Latent Hatred: A Benchmark for Understanding Implicit Hate Speech",
      "authors": [
        "M Elsherief",
        "C Ziems",
        "D Muchlinski",
        "V Anupindi",
        "J Seybolt",
        "M De Choudhury",
        "D Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "27",
      "title": "Exploring misogyny across the manosphere in reddit",
      "authors": [
        "T Farrell",
        "M Fernandez",
        "J Novotny",
        "H Alani"
      ],
      "year": "2019",
      "venue": "Proceedings of WEBSCI"
    },
    {
      "citation_id": "28",
      "title": "From the dog whistle to the dog scream: The Republican party's (ab) use of discriminatory speech in electoral campaigns and party politics",
      "authors": [
        "L Filimon"
      ],
      "year": "2016",
      "venue": "Romanian Journal of Society and Politics"
    },
    {
      "citation_id": "29",
      "title": "Bans, sanctions, and dog-whistles: a review of anti-critical race theory initiatives adopted in the United States since",
      "authors": [
        "L.-M Filimon"
      ],
      "year": "2020",
      "venue": "Policy Studies"
    },
    {
      "citation_id": "30",
      "title": "The positive and negative effects of anger on dispute resolution: evidence from electronically mediated disputes",
      "authors": [
        "R Friedman",
        "C Anderson",
        "J Brett",
        "M Olekalns",
        "N Goates",
        "C Lisco"
      ],
      "year": "2004",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "31",
      "title": "Online misinformation and vaccine hesitancy",
      "authors": [
        "R Garett",
        "S Young"
      ],
      "year": "2021",
      "venue": "Translational behavioral medicine"
    },
    {
      "citation_id": "32",
      "title": "Analyzing the traits and anomalies of political discussions on reddit",
      "authors": [
        "B Gershlick",
        "A Charlesworth",
        "E Taylor",
        "A Guimaraes",
        "O Balalau",
        "E Terolli",
        "G Weikum"
      ],
      "year": "2015",
      "venue": "Proceedings of ICWSM"
    },
    {
      "citation_id": "33",
      "title": "Interacting with the ordinary people: How populist messages and styles communicated by politicians trigger users' behaviour on social media in a comparative context",
      "authors": [
        "M Hameleers",
        "D Schmuck",
        "L Bos",
        "S Ecklebe"
      ],
      "year": "2021",
      "venue": "European Journal of Communication"
    },
    {
      "citation_id": "34",
      "title": "ANTi-Vax: a novel Twitter dataset for COVID-19 vaccine misinformation detection",
      "authors": [
        "K Hayawi",
        "S Shahriar",
        "M Serhani",
        "I Taleb",
        "S Mathew"
      ],
      "year": "2022",
      "venue": "Public health"
    },
    {
      "citation_id": "35",
      "title": "Something's Brewing! Early Prediction of Controversy-causing Posts from Discussion Features",
      "authors": [
        "J Hessel",
        "L Lee"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL"
    },
    {
      "citation_id": "36",
      "title": "The human component in social media and fake news: the performance of UK opinion leaders on Twitter during the Brexit campaign",
      "authors": [
        "M Höller"
      ],
      "year": "2021",
      "venue": "European Journal of English Studies"
    },
    {
      "citation_id": "37",
      "title": "Identifying the social signals that drive online discussions: A case study of reddit communities",
      "authors": [
        "B Horne",
        "S Adali",
        "S Sikdar",
        "C Cuskley"
      ],
      "year": "2017",
      "venue": "Humanities and Social Sciences Communications"
    },
    {
      "citation_id": "38",
      "title": "Computing Krippendorff's alphareliability",
      "authors": [
        "K Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing Krippendorff's alphareliability"
    },
    {
      "citation_id": "39",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "40",
      "title": "Dog whistle politics: How coded racial appeals have reinvented racism and wrecked the middle class",
      "authors": [
        "I López"
      ],
      "year": "2013",
      "venue": "Dog whistle politics: How coded racial appeals have reinvented racism and wrecked the middle class"
    },
    {
      "citation_id": "41",
      "title": "TimeLMs: Diachronic Language Models from Twitter",
      "authors": [
        "D Loureiro",
        "F Barbieri",
        "L Neves",
        "L Espinosa Anke",
        "J Camacho-Collados"
      ],
      "year": "2022",
      "venue": "Proceedings of ACL: System Demonstrations"
    },
    {
      "citation_id": "42",
      "title": "Quantifying gender biases towards politicians on Reddit",
      "authors": [
        "S Marjanovic",
        "K Stańczak",
        "I Augenstein"
      ],
      "year": "2022",
      "venue": "PloS one"
    },
    {
      "citation_id": "43",
      "title": "Trigger words and the politics of polarization. Proceedings of the West Virginia Academy of Science",
      "authors": [
        "M Martin",
        "K Huck",
        "C Williams",
        "D Hull",
        "J Hull"
      ],
      "year": "2021",
      "venue": "Trigger words and the politics of polarization. Proceedings of the West Virginia Academy of Science"
    },
    {
      "citation_id": "44",
      "title": "Triggerpunkte: Konsens und Konflikt in der Gegenwartsgesellschaft",
      "authors": [
        "S Mau",
        "T Lux",
        "L Westheuser"
      ],
      "year": "2023",
      "venue": "Triggerpunkte: Konsens und Konflikt in der Gegenwartsgesellschaft"
    },
    {
      "citation_id": "45",
      "title": "Public sentiment analysis and topic modeling regarding COVID-19 vaccines on the Reddit social media platform: A call to action for strengthening vaccine confidence",
      "authors": [
        "C Melton",
        "O Olusanya",
        "N Ammar",
        "A Shaban-Nejad"
      ],
      "year": "2021",
      "venue": "Journal of Infection and Public Health"
    },
    {
      "citation_id": "46",
      "title": "Semeval-2018 task 1: Affect in tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th international workshop on SemEval"
    },
    {
      "citation_id": "47",
      "title": "The ethics of COVID-19 vaccine distribution",
      "authors": [
        "A Nichol",
        "K Mermin-Bunnell"
      ],
      "year": "2021",
      "venue": "Journal of Public Health Policy"
    },
    {
      "citation_id": "48",
      "title": "An In-depth Analysis of Implicit and Subtle Hate Speech Messages",
      "authors": [
        "N Ocampo",
        "E Sviridova",
        "E Cabrio",
        "S Villata"
      ],
      "year": "2023",
      "venue": "Proceedings of EACL"
    },
    {
      "citation_id": "49",
      "title": "Social data: Biases, methodological pitfalls, and ethical boundaries",
      "authors": [
        "A Olteanu",
        "C Castillo",
        "F Diaz",
        "E Kıcıman"
      ],
      "year": "2019",
      "venue": "Frontiers in big data"
    },
    {
      "citation_id": "50",
      "title": "Multilingual and Multi-Aspect Hate Speech Analysis",
      "authors": [
        "N Ousidhoum",
        "Z Lin",
        "H Zhang",
        "Y Song",
        "D.-Y Yeung"
      ],
      "year": "2019",
      "venue": "Proceedings of EMNLP-IJCNLP"
    },
    {
      "citation_id": "51",
      "title": "Mating call, dog whistle, trigger: Asymmetric alignments, race, and the use of reactionary religious rhetoric in American politics",
      "authors": [
        "S Perry",
        "S Priya",
        "R Sequeira",
        "J Chandra",
        "S Dandapat"
      ],
      "year": "2019",
      "venue": "Online Social Networks and Media"
    },
    {
      "citation_id": "52",
      "title": "Political Hashtags & the Lost Art of Democratic Discourse",
      "authors": [
        "E Rho",
        "M Mazmanian"
      ],
      "year": "2020",
      "venue": "Proceedings of CHI, CHI '20"
    },
    {
      "citation_id": "53",
      "title": "Assessing the extent and types of hate speech in fringe communities: A case study of alt-right communities on 8chan, 4chan, and Reddit",
      "authors": [
        "D Rieger",
        "A Kümpel",
        "M Wich",
        "T Kiening",
        "G Groh"
      ],
      "year": "2021",
      "venue": "Assessing the extent and types of hate speech in fringe communities: A case study of alt-right communities on 8chan, 4chan, and Reddit"
    },
    {
      "citation_id": "54",
      "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "authors": [
        "S Rosenthal",
        "N Farra",
        "P Nakov"
      ],
      "year": "2017",
      "venue": "Proceedings Se-mEval"
    },
    {
      "citation_id": "55",
      "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "authors": [
        "S Rosenthal",
        "N Farra",
        "P Nakov"
      ],
      "year": "2019",
      "venue": "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "arxiv": "arXiv:1912.00741"
    },
    {
      "citation_id": "56",
      "title": "What's trending in difference-in-differences? A synthesis of the recent econometrics literature",
      "authors": [
        "J Roth",
        "P Sant'anna",
        "A Bilinski",
        "J Poe"
      ],
      "year": "2023",
      "venue": "Journal of Econometrics"
    },
    {
      "citation_id": "57",
      "title": "Social IQa: Commonsense Reasoning about Social Interactions",
      "authors": [
        "M Sap",
        "H Rashkin",
        "D Chen",
        "R Le Bras",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of EMNLP-IJCNLP"
    },
    {
      "citation_id": "58",
      "title": "Sticks and stones may break bones and words can hurt me: Words and phrases that trigger emotions in negotiations and their effects",
      "authors": [
        "H Schroth",
        "J Bain-Chekal",
        "D Caldwell"
      ],
      "year": "2005",
      "venue": "International Journal of Conflict Management"
    },
    {
      "citation_id": "59",
      "title": "Troll and divide: the language of online polarization",
      "authors": [
        "A Simchon",
        "W Brady",
        "J Van Bavel"
      ],
      "year": "2022",
      "venue": "PNAS Nexus"
    },
    {
      "citation_id": "60",
      "title": "Evidence of online performance deterioration in user sessions on Reddit",
      "authors": [
        "P Singer",
        "E Ferrara",
        "F Kooti",
        "M Strohmaier",
        "K Lerman"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "61",
      "title": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter",
      "authors": [
        "Z Talat",
        "D Hovy"
      ],
      "year": "2016",
      "venue": "NAACL. The Economist. 2023. How a Rwandan gambit consumed the Conservative Party"
    },
    {
      "citation_id": "62",
      "title": "Decade of neglect means NHS unable to tackle care backlog",
      "authors": [
        "The Guardian"
      ],
      "year": "2022",
      "venue": "Decade of neglect means NHS unable to tackle care backlog"
    },
    {
      "citation_id": "63",
      "title": "Jeremy Hunt Warned of £2bn Real-Term Cuts to NHS Funding",
      "year": "2024",
      "venue": "Jeremy Hunt Warned of £2bn Real-Term Cuts to NHS Funding"
    },
    {
      "citation_id": "64",
      "title": "Analyzing behavioral trends in community driven discussion platforms like reddit",
      "authors": [
        "S Thukral",
        "H Meisheri",
        "T Kataria",
        "A Agarwal",
        "I Verma",
        "A Chatterjee",
        "L Dey"
      ],
      "year": "2018",
      "venue": "IEEE/ACM ASONAM"
    },
    {
      "citation_id": "65",
      "title": "On political ambiguity and anti-median platforms",
      "authors": [
        "J Tolvanen"
      ],
      "year": "2024",
      "venue": "European Economic Review"
    },
    {
      "citation_id": "66",
      "title": "Do Differences in Values Influence Disagreements in Online Discussions",
      "authors": [
        "M Van Der Meer",
        "P Vossen",
        "C Jonker",
        "P Murukannaiah"
      ],
      "year": "2023",
      "venue": "Proceedings of EMNLP"
    },
    {
      "citation_id": "67",
      "title": "Vaccine discourse in white nationalist online communication: A mixed-methods computational approach",
      "authors": [
        "D Walter",
        "Y Ophir",
        "A Lokmanoglu",
        "M Pruden"
      ],
      "year": "2022",
      "venue": "Social Science & Medicine"
    },
    {
      "citation_id": "68",
      "title": "Conversations Gone Awry: Detecting Early Signs of Conversational Failure",
      "authors": [
        "J Zhang",
        "J Chang",
        "C Danescu-Niculescu-Mizil",
        "L Dixon",
        "Y Hua",
        "N Tahin",
        "D Taraborelli"
      ],
      "year": "2018",
      "venue": "Proceedings of ACL"
    }
  ]
}