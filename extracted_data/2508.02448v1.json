{
  "paper_id": "2508.02448v1",
  "title": "Charting 15 Years Of Progress In Deep Learning For Speech Emotion Recognition: A Replication Study",
  "published": "2025-08-04T14:09:53Z",
  "authors": [
    "Andreas Triantafyllopoulos",
    "Anton Batliner",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Deep learning",
    "Benchmarking",
    "Robustness",
    "Fairness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has long benefited from the adoption of deep learning methodologies. Deeper models -with more layers and more trainable parameters -are generally perceived as being 'better' by the SER community. This raises the questionhow much better are modern-era deep neural networks compared to their earlier iterations? Beyond that, the more important question of how to move forward remains as poignant as ever. SER is far from a solved problem; therefore, identifying the most prominent avenues of future research is of paramount importance. In the present contribution, we attempt a quantification of progress in the 15 years of research beginning with the introduction of the landmark 2009 INTERSPEECH Emotion Challenge. We conduct a large scale investigation of model architectures, spanning both audio-based models that rely on speech inputs and text-baed models that rely solely on transcriptions. Our results point towards diminishing returns and a plateau after the recent introduction of transformer architectures. Moreover, we demonstrate how perceptions of progress are conditioned on the particular selection of models that are compared. Our findings have important repercussions about the state-of-the-art in SER research and the paths forward 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "D EEP learning has arguably become the 'go-to' method for speech emotion recognition (SER) in the past decade. Featured prominently in recent publications and surveys  [1, 2] , the overwhelming consensus in the community is that deep learning (DL) outperforms previous methods. For instance, all submissions to the recent Odyssey 2024 SER challenge relied heavily on DL models to either extract learnt representations or fine-tune them on the challenge dataset  [3] .\n\nArtificial neural networks (ANNs) have been trained for SER since as early as 1998 with Huber et al.  [4]  (multilayered perceptron (MLP) for two classes), see as well Nicholson et al.  [5]  who used a shallow, 2-layer MLP in 2000. The field has come a long way since then, with one key trend being the transition from handcrafted features towards end-to-end pipelines that incorporate feature extraction and modelling within a single, monolithic architecture  [6] , and another being Citation: Authors. Title. Pages.... DOI:000000/11111. 1 Code repository: https://github.com/CHI-TUM/ser-progress-replication the progression towards deeper and larger methods (i. e. , with more layers and more parameters)  [1, 7] . Some representative examples of this trend are the early work of Trigeorgis et al.  [6] , who relied on convolutional recurrent neural networks (CRNNs) operating on raw audio, and Wagner et al.  [7]  who demonstrated the superiority of transformer models that accept as input raw audio and have been pre-trained using self-supervised learning (SSL).\n\nHowever, with dozens -if not hundreds -of papers published yearly on the topics of SER and DL, it is hard to judge which advances (if any) substantially progress beyond the state-of-the-art -and why. Recent benchmark studies, such as HEAR  [8]  and SUPERB  [9] , focus only tangentially on SER, with both featuring single, small, and acted SER datasets (CREMA-D  [10]  and IEMOCAP  [11] , respectively), and only benchmarking recent DL methods. While their contribution to the general speech and audio community is undisputed, they fail to capture the progress that has been made on SER for naturalistic conditions. Moreover, as these studies are focused on benchmarking, they are limited with respect to their analysis of why some models perform better than others. This prevents them from uncovering important insights that can help guide future advances. For example, Wagner et al.  [7]  showed that transformer models provide substantial improvements to valence recognition, but this was later attributed to their implicit modelling of linguistic content, rather than the discovery of new, unknown paralinguistic concepts related to valence  [12] . On the other hand, recurring challenge series, such as ComParE 2  and Odyssey, are more suited to capturing the Zeitgeist at specific moments in time, as participants compete with one another to obtain the best results using the latest methods, without necessarily revisiting older ones.\n\nThe problem is exacerbated by the fact that the field of SER is lacking a universally-accepted benchmark of speech collected in naturalistic conditions. FAU-AIBO, the dataset adopted by the 2009 INTERSPEECH Emotion Challenge -the world's first SER challenge -was quietly abandoned over the years with very few papers using it in its original form  [13] . Similarly, the largest naturalistic SER dataset, MSP-Podcast  [14] , has seen several releases in the last few years, making comparisons across different works harder. Instead, the most widely-cited datasets became IEMOCAP  [11]  and EmoDB  [15] . IEMOCAP features only acted speech (both scripted and improvised) and lacks an established evaluation scheme (with some works relying on leave-one-speaker-out cross-validation and others preferring leave-one-session-out). EmoDB, which is also acted, lacks a common evaluation scheme, and features only restricted linguistic content. (Note that EmoDB was originally intended to lay the foundations for emotional speech synthesis, not to train SER.)\n\nAs such, SER is suffering from a 'blind spot' as pertains to the progress that has been made by switching to larger and deeper models over the years. This is a blind spot we aimed to address in our recent comparison of different models on the 2009 INTERSPEECH Emotion Challenge data  [13] . In the present work, we extend this study by additionally including MSP-Podcast as another, more recent and naturalistic, dataset of emotional speech. Furthermore, we present results for large language models (LLMs) using textual data, and supplement our performance comparison with several analyses to better understand the progress made by using DL architectures.\n\nThe remainder of our work is organised as follows. In Section II, we present relevant background, discussing benchmarking approaches and offering an overview of the most important DL trends seen in SER research in the last decade. Section III outlines our methodology and Section IV showcases our results. Section V includes a discussion of our findings and our conclusion, Section VI the limitations of our work, and Section VII our suggestions for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section presents related work, beginning with a discussion of benchmarking in the context of SER and continuing with a succinct overview of the main DL architecture families employed for SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Benchmarking",
      "text": "As discussed in Section II-B, there is a variety of architectures that have been introduced for speech analysis, and particularly for SER, but these are often evaluated on only a small number of datasets, with no consensus on common benchmarks. Typically, a single work will refer to only a few 'baseline' methods from prior work, oftentimes without reproducing the results but taking them from the literature. This makes it hard to compare different models and algorithms proposed by different groups. Crucially, it makes it hard to quantify the progress that has been made as new classes of architectures, like convolutional neural networks (CNNs) or transformers, are introduced.\n\nThe problem is further exacerbated by a gradual change in the datasets used by the majority of the community (which often, but not always, occurs for good reasons). For example, the first large-scale dataset open-sourced for SER research was FAU-AIBO  [16] , which was used for the 2009 INTERSPEECH Emotion Challenge  [17] . While large (for its time) and wellknown in the community, it was largely abandoned in the latter half of the 2010s with very few works using it (and even less in the form published for the challenge). Instead, the community transitioned to other datasets, such as IEMOCAP  [11]  (the top-cited SER dataset at the time of writing), SEWA  [18] , MSP-Podcast  [14] , or others. We note that this change was not always motivated by a quest for more 'naturalness' or 'bigger' data. Indeed, FAU-AIBO is both larger and collected in a more realistic, not explicitly prompted setting than IEMOCAP  [11] , RAVDESS  [19] , CREMA-D  [10]  and other datasets which have become more prevalent in recent years.\n\nMoreover, the matter of properly benchmarking artificial intelligence (AI) models is far from trivial. Beyond the choice of data discussed above, it is important to define an appropriate computational budget and overall engineering effort devoted to each particular method. For example, the winner of the 2009 INTERSPEECH Emotion Challenge utilised a hierarchical task tree by identifying supergroups of target emotions using expert knowledge  [20] . While this is a valid -and very innovative -approach, it is definitely more byzantine than the simple baseline which relied on 'mere' classification. Similar 'overengineered' approaches have been proposed over the years; from ones that take into account the exact problem proposed by a dataset, to ones with elaborate feature engineering. These require substantial effort to reproduce, making their inclusion in a benchmarking effort prohibitively time-consuming. In our present work, we chose to abide by the \"bitter lesson\" of Sutton  [21] , namely, that simpler approaches will scale better given more computational resources. This fits the broader landscape of contemporary SER research -that of almost complete dominance of DL. Therefore, we focused on 'simple' deep neural network (DNN) models and trained each of them with a fixed computational budget (in the form of update steps, not compute time) and hyperparameters. While not exhaustive, and potentially disadvantaging some approaches, this allows for a more fair comparison.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Ser Model Timeline",
      "text": "DNN architectures used for SER evolved substantially throughout the years. In the following, we present a list of the most popular SER models of recent years, beginning with the launch of the 2009 INTERSPEECH Emotion Challenge  [17]  and continuing to present day. As the most prominent task in computational paralinguistics (CP) research, innovation in SER has largely driven other related subfields as well, and is thus representative of the field as a whole. '09-: openSMILE feature sets -In the different iterations of the ComParE Challenge, newer versions of paralinguistic features were introduced. In general, these feature sets were larger and covered a wider gamut of acoustic and prosodic features than the initial features introduced in the 2009 INTERSPEECH Emotion Challenge. However, that period saw a parallel pursuit for smaller expert-driven feature sets  [22] . Researchers used either the dynamic low-level descriptors or their higher-order static functionals. These features were fed into MLPs or recurrent neural networks (RNNs) for further processing. '12-: ImageNet pre-training -Following the introduction of ImageNet and the first CNNs trained on it in 2012, such networks were subsequently introduced in the audio and speech domains by substituting images with (pictorial representations of) spectrograms  [23, 24]  -a practice that is relevant to this day, with audio transformer models oftentimes initialised with states pre-trained on ImageNet  [25] . '16-: End-to-end -Subsequent years saw the introduction of end-to-end models, i. e. , models trained directly on raw audio input for the target task without any prior feature preprocessing  [6] . These models were especially successful in the case of time-continuous SER, which requires predicting the emotion of very short audio frames, and usually follow the CRNN architecture. '16-: Supervised audio pre-training -In parallel to ImageNet pre-training, there were also efforts to collect similar largescale datasets for audio where networks could be pre-trained in a supervised fashion. Two notable examples are VoxCeleb and AudioSet  [26] , both collected from YouTube, with the former targeted to speaker identification and the latter to general audio tagging. VoxCeleb formed the basis for training speaker embedding models (i. e. , 'x-vectors') using time-delay neural networks (TDNNs)  [27] . AudioSet in turn inspired the use of convolutional networks, such as CNN14, which was introduced in pretrained audio neural networks (PANNs)  [28]  and shown to also transfer well to SER tasks  [29] , and later, transformerbased models such as AST  [25] . In addition, the introduction of the Whisper architecture  [30]  led to a renaissance of supervised training for automatic speech recognition (ASR). '20-: Self-supervised audio pre-training -The introduction of transformers and the advent of self-supervised pre-training for computer vision and natural language processing (NLP) also propagated to the speech and audio domain. The two dominant architectures here are wa2v2ec2.0(w2v2)  [31]  and hubert  [32] . These models have yielded significant advances in SER, as also shown in our own work  [7, 13] , which we have partially accredited to their ability to simultaneously encode linguistic and paralinguistic information  [12] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section outlines our workflow, beginning with the considered datasets, continuing with the experimental settings used for training models, and finishing with the details of the accompanying analyses.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Datasets And Tasks",
      "text": "As mentioned, we employed two different datasets. See Appendix A for a broader discussion on our dataset selection criteria. FAU-AIBO is the official dataset of the INTER-SPEECH 2009 Emotion Challenge  [17] . It is a dataset of German children's speech collected in a Wizard-of-Oz scenario and annotated on the word-level for the presence of 11 emotional/communicative states by 5 expert raters  [33, 34] . Subsequently, segmented words have been aggregated to meaningful chunks using manual semantic and prosodic criteria. Accordingly, annotated states have been mapped to 2and 5class categorisations with a set of heuristics, which forms a final dataset of 18 216 chunks used for the challenge. FAU-AIBO 2 features the classes negative (NEG) and non-negative (IDL), whereas FAU-AIBO 5 comprises anger (A), emphatic (E), neutral (N), motherese (P), and rest (R) for the remaining classes. The data is heavily imbalanced towards the neutral/nonnegative classes. The data was collected from two schools, with one school set aside for testing (Mont) and one set aside for training (Ohm); we use the same partitioning for our experiments. Additionally, we create a small validation set comprising the last two speakers of the training set (speakers are denoted by number IDs): Ohm_31 and Ohm_32 as in our previous work  [13] . Note that the data are extensively documented in S. Steidl  [34] .\n\nMSP-Podcast  [14]  is a recently-introduced data set for SER, and one of the biggest publicly-released datasets to date. However, one 'downside' of it is that new releases are made approximately every year; these releases include increases to the data size but also corrections to the annotations and files included. We used MSP-Podcast-v1.11 as the latest version of the dataset available to us, comprising 44 586 instances in the training set, 11 947 in the validation, and 20 845 in the test set. The dataset has been annotated for the emotional dimensions of arousal, valence, dominance, as well as for the emotion categories of angry, contemptful, disgusted, afraid, happy, neutral, sad, and surprised. A 7-point Likert scale was used for the emotional dimensions on the utterance level, and scores by individual annotators have been averaged to obtain a consensus vote. All versions are split into speaker independent partitions with an official train/dev/test partition; some releases feature a test-2 partition which we ignore throughout our work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Settings",
      "text": "In total, we experimented with 43 audio-based models (see Appendix B for the complete list). All audio models were trained using autrainer  [35] . To constrain our space of hyperparameters, we conducted two experimental phases. In the first exploration phase, we tested all 43 investigated models using a fixed set of hyperparameters. Specifically, we used the Adam optimiser with a learning rate of .0001 for 30 epochs. We used a batch size of 4 due to hardware constraints (access to higher-end GPUs that could fit all models at higher batch sizes was more limited to us than lower-end GPUs which fit all models at a batch size of 4). In total, this resulted in 43 runs for the exploration phase of each task.\n\nAdditionally, we conducted a tuning phase, where we further optimised a larger set of hyperparameters for the 5 bestperforming models from the exploration phase for FAU-AIBO 2 , FAU-AIBO 5 , and MSP-Podcast-v1.11, doing a grid search over optimisers {Adam, SGD} and learning rates {.01, .001, .0001}, while training each configuration for 50 epochs for FAU-AIBO and 5 epochs for MSP-Podcast-v1.11 (due to time constraints). For FAU-AIBO, we additionally explored different batch sizes {4, 8, 16}.\n\nTo account for variable-length sequences in training, we randomly cropped/padded all chunks to a fixed length of 3 seconds (different cropping/padding was applied on each instance across different epochs; random seed was fixed and can be reproduced) when using dynamic features (including the raw audio); during inference, we used the original utterances, only padding those shorter than 2 seconds with silence. Except for w2v2 and hubert, where we froze the feature extractors, all model parameters were fine-tuned. We used the weighted crossentropy loss for classification to account for class imbalance. In all cases, we used the defined validation set to select the bestperforming epoch for each model, which we then evaluated on the test set.\n\nOur search over text-based models was much more restricted. It was also inspired by recent developments in NLP, but was much more constrained in time. We evaluated BERT  [36] , RoBERTa  [37] , DistilBERT  [38] , and Electra  [39]  as \"first generation\" transformers, and Llama-2  [40] , Llama-3  [41] , and Mistral  [42]  as \"second generation\" LLMs. First generation transformers were trained with a batch size of 8, AdamW with a weight decay of .00001, and a learning rate of .00001 for 10 epochs. Second generation LLMs were trained for 5 epochs with a learning rate of .00001, AdamW with a weight decay of .01, and low-rank adaptation (LoRA) fine-tuning of only the attention projection matrices (for query, key, value, and output)  [43]  with 4-bit quantisation of model weights. For FAU-AIBO, we used the official transliteration provided by human raters. For MSP-Podcast-v1.11, we used automatic transcriptions generated with Whisper-large-v2.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Model Evaluation",
      "text": "Independently and identically distributed (IID) evaluation: Models were initially evaluated on the corresponding test set of each dataset. We report unweighted average recall (UAR) -the standard metric for SER since the 2009 INTERSPEECH Emotion Challenge which accounts for class imbalance by computing first the recall per class and then averaging.\n\nOut-of-domain (OOD) evaluation: Similar to Wagner et al.  [7] , we also evaluated OOD performance. To do so, we used the SER models trained on MSP-Podcast-v1.11, as this is the only task for which it was possible to find OOD data (due to the non-standard labels used in FAU-AIBO). Specifically, we used EmoDB  [15]  and IEMOCAP  [11] . For both datasets, we used the entire data corresponding to the four classes that the model was trained on. For IEMOCAP, we additionally relabelled samples belonging to the \"excited\" class as \"happy\". Performance was once again evaluated using UAR.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Probing Features",
      "text": "In order to better understand which paralinguistic features models rely on -especially after fine-tuning -we searched for interpretable features that are predictable from the intermediate representations of transformer models. This is known as probing  [12] . For our probing experiments, we relied on a small, easily-interpretable subset of the extended Geneva minimalistic acoustic descriptor set (eGeMAPS)  [22] . Those were: µ(P) average pitch; σ(P) standard deviation of pitch; µ(L) average loudness; µ(L) [dB] average loudness in dB; µ(J) average jitter; µ(S) average shimmer; µ(HNR) average harmonic-to-noise ratio; µ(F1) average frequency for first formant; µ(F2) average frequency for second formant; µ(F3) average frequency for third formant.\n\nConcretely, we extracted hidden representations from all layers and all transformer models that were fine-tuned on MSP-Podcast-v1.11, namely, w2v2-b, w2v2-L, w2v2-L-vox, w2v2-L-robust, w2v2-L-12-avd, hubert-b, and hubert-L. We then averaged these representations over the time dimension. Subsequently, we used a linear regression model to predict the absolute value of each feature scaled in the range [0, 1]. Models were trained and evaluated using leave-one-speaker-out (LOSO) cross-validation (CV) on EmoDB and we measured their Pearson's r.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Robustness Analysis",
      "text": "For our investigation of robustness, we created a noisy mixture of MSP-Podcast-v1.11, leveraging the 2021 version of the TAU Urban Audio-Visual Scenes Development dataset (TAU-UAVS)  [44]  as our source for additive noise. All audio was sourced from its training set, where we used the first 10 instances of each label in each city to create our mixtures. Specifically, for every test instance of MSP-Podcast-v1.11, we randomly drew one audio file from TAU-UAVS with replacement, which we added to the original audio from MSP-Podcast-v1.11 using a signal-to-noise ratio (SNR) of 0 dB. We then evaluated models trained on clean audio to measure the deterioration in performance caused by additive noise.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F. Individual Fairness",
      "text": "The standard process for computing performance in machine learning (ML) tasks is to consider each instance as an independent trial. Model performance is then measured using some metric, M, over the set of reference labels {y} N 0 and predictions {ŷ} N 0 , with N being the size of the evaluation set. However, the disparity in performance across different individuals plays a vital role in SER  [45] . Therefore, it makes sense to consider individual-level performance, and, as a corollary, individual-level fairness.\n\nTo define individual-level fairness, we began by computing the performance on a speaker-level, thus assuming that speakers are first selected independently, and only then are samples selected independently for them (see  Guyon et al. [46]  for a similar argumentation). We finally computed the metric M over the set of instances for each individual speaker in our dataset, which we denote as SP M in order to differentiate it from the standard evaluation protocol of treating each utterance independently, which we denote by U M . We call SP M the utility of each speaker, as this is the benefit that each speaker can expect from getting their emotions recognised correctly  3  . We examined this utility by taking the Gini coefficient (G(•)) as a standard measurement used in econometrics to judge the distribution of utility in a particular population; it is defined as half of the mean absolute difference relative to the mean of a particular sample  [47] . The Gini coefficient has been previously used to quantify inequality in speaker-level performance for SER  [45] . In particular, it takes the value of 0 for an equal distribution where everyone has the same utility, and 1 for a completely unequal one, where the entire utility is accumulated by one particular individual. In our case, this was computed as:\n\nwhere N is the number of speakers in the test set and {u i } N 1 is the set of utility values for all speakers, with u i = SP i M , i. e. , the speaker-level performance computed for each speaker.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Results",
      "text": "This section presents our performance comparisons for all audio models both IID and OOD. Moreover, it contains our probing analysis and results on robustness and individual fairness.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Audio-Based Models",
      "text": "We begin the presentation of our results by exclusively considering audio-based models, i. e. , models that accept speech (either in raw form or as features). A complete list of models and pre-trained states is provided in Appendix B.\n\n1) IID results: Results for the exploration phase are shown in Table  I . For FAU-AIBO, the best-performing model for the 2-class problem was CNN14, with a UAR of .692, whereas for the 5-class problem it was ResNet50, with a UAR of .428. We further observe that some models failed to converge and yield chance (or near-chance) performance -most likely caused by the choice of hyperparameters, which favour some models more than others. Notably, most of our results were below the challenge winners for FAU-AIBO (UAR: .703/.417) and several were in the same 'ballpark' as the original baseline (UAR: .677/.382). For MSP-Podcast-v1.11, the best performance was achieved by w2v2-L-12-avd with a UAR of .609; however, this model has been already trained on dimensional SER on the same data by Wagner et al.  [7] , and may therefore exhibit better performance due to this cascaded IID finetuning. The second-best model was hubert-L, with a UAR of .570. Is model ranking consistent across tasks? We considered model ranking across the seven different tasks, where we computed the pairwise Spearman's ρ computed over model performance on each pair of tasks. The results are presented in Fig.  1 . We observed moderate-to-high correlations between different tasks. Notably, performance on FAU-AIBO 5 correlated more with MSP-Podcast-v1.11 (.714) than with FAU-AIBO 2 (.580), despite the fact that the latter two tasks share identical data. This indicates that there is no 'one-winner-takesall' model. Rather, as is consistent with the 'no-free-lunch' theorem  [48] , a different model can be more appropriate for a given dataset and task. Do newer/larger models perform better? Table  II  displays the Spearman's ρ between model performance and the year each model was created, its multiply-addition computations (MACs), and its number of parameters. We observed very low correlations overall, with the highest correlation between complexity (measured in MACs) achieved for FAU-AIBO 5 being a meagre .230. We acknowledge that these correlations heavily depend on our particular selection of models. We account for this by also showing 95% bootstrap confidence intervals (CIs). The large variance in our results, ranging from low negative correlations (> -.3) to moderately high correlations (> .3), does not allow for robust conclusions.\n\nAgreement between different models: We first considered model agreement on FAU-AIBO. The average pairwise agreement (percentage of instances where two models agree) for all models of the exploration phase was 70% and 55% for the 2and 5-class models of FAU-AIBO, which rises to 80% and 57%, respectively, when considering only the top-5 ones. Overall, this demonstrates that models trained on similar data do not converge to an identical solution -a finding congruent with the literature on underspecification  [49] . Accordingly, the pairwise agreement for MSP-Podcast-v1.11 was 55%/79% for all models and for the top-5 models respectively. These numbers indicate  a moderate-to-high agreement on model predictions, especially for the best-performing models. This illustrates that, despite model size or year of publication, models trained on the same data tend to learn similar -but not identical -concepts.\n\nModel vs human performance: We also investigated whether samples that are harder to classify for humans are also harder for models. The standard FAU-AIBO release comes with annotator confidences per instance, computed by taking the percentage of annotators who agree with the gold standard; we thus defined 'difficulty' as 1 minus that confidence. We then make the following observations when considering all models of the tuning phase: a) We first adopted a model-agnostic measure of difficulty, which we defined as the number of models who disagree with the max-vote computed by all models on each instance -this is akin to the computation of difficulty for the human annotators. Spearman's ρ between this measure and annotator disagreement was moderate (.33 and .20 for the 2and 5-class problems). b) We then adopted a model-specific measure of difficulty, defined as the standard cross-entropy loss for each instance, similar to Hacohen and Weinshall  [50]  (see Rampp et al.  [51]  for our own investigations on the matter). Different models have different rankings of instance difficulty, with average pairwise ρ being .51 and .33 for the 2and 5-class problems.  c) Finally, we computed the Spearman's ρ between each model's UAR and its ρ with annotator disagreement; here, ρ was -.38/ -.07 for the 2/5-class problem, indicating that larger agreement with human annotators did not lead to better performance (rather, the opposite).\n\nWe performed a similar analysis for MSP-Podcast-v1.11. Here, we did not directly have annotator confidence, as samples where annotators disagree have been filtered out by the original authors  4  . Instead, we adopted two of the dimensional annotations (arousal and valence) as proxies for the difficulty of a sample. Our intuition was that samples with a very high, or very low, arousal and/or valence, would be easier to distinguish than others. Our model-agnostic measure of difficulty (percentage of models which agree with the maxvote) had a Spearman's ρ of -.15 with arousal and -.07 with valence. The average Spearman's ρ of the instance-level loss of each model with arousal was .17 and with valence -.  18  Collectively, our results indicate that models appear to learn differently than humans, with small agreement to what constitutes an easy or hard example. This was true for both FAU-AIBO and MSP-Podcast-v1.11. This points to the fact that, on the one hand, there is still room for further growth, but also that models may fail in ways that are considered 'catastrophic' by human observers.  ), we expect Whisper's success to be also attributed to that aspect (see Section IV-B for how text-based models performed). However, it is still the case that all models we tested remained close to or even below the original challenge baseline and winners, and especially the fusion of the top challenge submissions. This remained so even after selecting only the best-performing model out of all tested hyperparameters, essentially following a generally erroneous practice of overfitting. This was done intentionally to gauge performance under the most optimistic of settings -that of virtually unrestricted evaluation runs. We note that the original challenge participants were given 25 runs each. This shows how the gains obtained here must be further tempered to account for more runs on our side.\n\nFor MSP-Podcast-v1.11, the best performing model was a variant of w2v2-L-12-avd, trained with SGD, a learning rate of .01, and a batch size of 4, which reached a UAR of .650 [.642 -.658] on the test set. We release this model, which we hence refer to as w2v2-L-12-emo publicly 5 . The performance for other models also improved, with the best hyperparameter combination for hubert-L reaching .624[.615 -.633], illustrating once more how hyperparameter tuning can make a substantial difference in performance comparisons.\n\nFig.  2  shows all tuning results on both FAU-AIBO tasks and MSP-Podcast-v1.11. We observed a very high variability when changing hyperparameters -a well-known phenomenon impacting DL  [52] . On the positive side, this tuning allowed us to obtain much higher performance -on MSP-Podcast-v1.11 this gave rise to a UAR of .650 for w2v2-L-12-avd. However, 5 https://huggingface.co/autrainer/msp-podcast-emo-class-big4-w2v2-l-emo the high variability calls attention to the fact that a different choice of hyperparameters in the exploration phase could have resulted in both different overall performance and, crucially, a different ranking across architectures. Unfortunately this is a problem that plagues all DL research at the moment, with the only possible solution being the investment in more compute power to explore a wider space of hyperparameters (which was not possible in our case).\n\n2) OOD results: OOD results are shown in Table  V . In many cases, OOD UAR is, surprisingly, higher than IID. We interpret this a side-effect of the datasets being much 'easier' than MSP-Podcast-v1.11, as they only contain acted, and very prototypical, data. Nevertheless, it is a promising sign of model generalisation. The best OOD performance in both cases was achieved by w2v2-L-12-avd, which reached a UAR of .806 on EmoDB and .617 on IEMOCAP. The Spearman's ρ of IID with OOD performance was .909 for EmoDB and .843 for IEMOCAP. This is another positive finding as it entails that selecting the best-performing model based on IID performance alone will translate to the best-performing model on OOD as well. However, the Spearman's ρ between year, MACs, or number of parameters and OOD UAR remained low for both EmoDB and IEMOCAP (year: r = .112/.098; MACs: r = .166/.064; # parameters: .172/.096).\n\n3) Transfer learning dynamics of categorical SER models: We next considered the transfer learning dynamics of categorical SER models trained on MSP-Podcast-v1.11, as this is the largest dataset we have trained on. We computed the centred kernel alignment (CKA)  [53] , a measure of similarity for hidden representations using EmoDB as a probing dataset due to its smaller size. We did so for all transformer models, namely, w2v2-b, w2v2-L, w2v2-L-vox, w2v2-L-robust, w2v2-L-12-avd, hubert-b, and hubert-L, as well as two exemplary CNNs, CNN14 and EfficientNet. For each model, we computed the CKA between hidden representations extracted from the initial state before fine-tuning, and all subsequent intermediate states which were saved every second epoch. For transformers, we extracted the representations after each hidden transformer block, for CNN14 we did so after each of its convolutional blocks, and similarly for EfficientNet.\n\nResults are shown in Fig.  3 . Broadly, we note that all models exhibited a pattern of changing bottom-first (closer to the output), consistent both with the intuition that earlier layers    Fig.  3 : Transfer learning dynamics of models fine-tuned on 4-class SER using MSP-Podcast-v1.11. Plots show CKA between representations extracted at the initial state and at intermediate checkpoints stored every second epoch. A higher CKA (darker colour) corresponds to higher similarity between the two states, i. e. , a higher amount of feature reuse. learnt more general, and thus re-usable features  [54] , and prior results  [55, 56] . However, a closer look at individual models reveals important differences. Transformers that saw more adaptation of earlier layers, like w2v2-b, hubert-b, and w2v2-L, performed at chance-level performance. This is an indication that deviating a lot from the pretrained state can have catastrophic effects. We hypothesise that this was a sideeffect of using a learning rate that was too high for these models. Interestingly, a similar pattern was exhibited by EfficientNet; however, this model still achieved good SER performance. We hypothesise that this particular model needed to adjust its representations more as it had been only pretrained on ImageNet data. Lastly, w2v2-L-12-avd showed the least adaptation overall, which is expected as the model had already been trained on MSP-Podcast-v1.7 in a previous step (for dimensional SER) and had thus already adapted to the distribution of the data.",
      "page_start": 5,
      "page_end": 8
    },
    {
      "section_name": "4) Probing Transformer Representations:",
      "text": "Results are shown in Table  VI . We note that better-performing models showed a higher Pearson's r overall across all features, indicating that representations that contained more information about those features led to a better SER performance. Comparing errors across features also uncovers interesting insights. For instance, µ(P) had a much higher Pearson's r than σ(P), which shows that transformers were better at grasping average pitch rather than pitch variations as also shown by the high errors on jitter. Given the importance of pitch variations for SER  [57] , this might present an avenue for future improvements.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "5) Robustness To Additive Noise:",
      "text": "The Spearman's ρ between the UAR achieved for clean and noisy audio was .953. However, as before, the Spearman's ρ between noisy UAR and year of publication (.05), MACs (.10), and # of parameters (.03) was extremely low. Thus, while IID performance showed a strong correlation with robustness, this robustness did not increase based on the year of publication or model size. Detailed noise robustness results are shown in Appendix D where we observe substantial degradations of model performance even for the newest models, with the best-performing w2v2-L-12-avd dropping from a UAR of .609 to one of .546. 6) Individual fairness: We finish this section with a discussion of speaker-level performance for audio-based models. To do so, we computed the speaker-level performance for each task and used that as the utility to compute the Gini index.\n\nWe are interested in two main questions: a) what was the average equality observed for a dataset across models, as measured by the Gini index, and, b) were models that performed better on the instance-level also more fair across speakers, i. e. , what was the Spearman's ρ between the UAR of a model on the instance-level and its Gini index across speakers? We only considered models that performed better than chance, defined as a UAR larger than .01 more than the corresponding randomchance UAR, i. e. , .51 for 2-class, .26 for 4-class, and .21 for 5-class, respectively. Our results are presented in Table  VII .\n\nWe note that the only task which showed a moderate average inequality across speakers is MSP-Podcast-v1.11, with an average Gini index of .329. On the positive side, it also showed a high negative correlation between instance-level UAR and speaker-level Gini index (-.344). This means that models which performed better overall across all instances were also more fair towards different speakers. The FAU-AIBO tasks exhibited a low to moderate average Gini index and a negligible correlation between instance-level UAR and the Gini index across speakers. Based on this, we conclude that speaker inequality affects different datasets and tasks to different degrees, but it remains an open avenue for future research considering the potential of personalisation methods to improve fairness and overall performance  [45] .\n\nThe Spearman's ρ between the Gini index and year of publication (-.04), MACs (.-25), and # of parameters (-.07) for MSP-Podcast-v1.11 was either zero or mildly negative showing that bigger models were slightly more fair than smaller ones. However, this trend is contradicted by results for FAU-AIBO 2 (.29/.55/.53) and FAU-AIBO 5 (.18/.32/.26) which showed moderate-to-strong Spearman's ρ between the Gini index and year of publications, MACs, and # of parameters, pointing towards lower individual fairness for newer and bigger models.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Text-Based Models",
      "text": "In addition, we used state-of-the-art text-based models to predict emotion on both FAU-AIBO tasks and MSP-Podcast-v1.11. UAR results are shown in Table  VIII .\n\nThe highest performance for FAU-AIBO 2 was achieved by DistilBERT at .667, for FAU-AIBO 5 by Llama-3 at .401, and for MSP-Podcast-v1.11 by Llama-2 at .564, which was essentially on-par with Llama-3 at .563 and Mistral at .560. Notably, all these results were lower than the best-performing audio-based models though competitive with models trained on the exploration phase. For instance, Llama-2 results on MSP-Podcast-v1.11 would rank third in our exploration phase, trailing marginally behind hubert-L which scored at .570. This demonstrates how text contains vital information for the SER task of MSP-Podcast-v1.11.\n\nResults on FAU-AIBO were overall lower, with text-based models scoring in the middle of the 'competition'. We hypothesise that this is a side-effect of the fact that the textual content in FAU-AIBO is much more limited than in MSP-Podcast-v1.11 given the Wizard-of-Oz scenario in which it was recorded. Nevertheless, the fact that text-based models obtained better-than-chance performance is in itself remarkable, given how limited the textual content is, and indicates that there are potential systematic biases with respect to the linguistic content of certain emotions. We discuss the linguistic content of both datasets further in Appendix E.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "1) Complementarity Between Audio & Text:",
      "text": "We finally considered the complementarity between audio-based and text-based models. In the present subsection, we rely on error analysis to investigate whether text-based models, which generally underperform audio-based ones, brought additional benefits for classification or merely predicted correctly a subset of the data where audio-based models already performed well.\n\nFor MSP-Podcast-v1.11 we compared two exemplary audiobased models from the exploration phase, w2v2-L-12-avd, as the best-performing transformer, and EfficientNet, as the bestperforming non-transformer model, and the best-performing TABLE VI: Average Pearson's r over all layers and epochs when probing transformer representations for their knowledge of acoustic features using linear probes on EmoDB. A higher Pearson's r -either positive or negative -reflects the strength with which this feature is encoded in internal representations, and how it might be possibly utilised during inference. The last column shows UAR taken from Table  I  text-based model, Llama-2. We also considered FAU-AIBO 2 using w2v2-L-12-avd, CNN14, and DistilBERT as the three models showing a more balanced performance over FAU-AIBO 2 and FAU-AIBO 5 . We first examined the pairwise agreement between all model combinations, measured as the percentage of samples where two models make the same decision. For MSP-Podcast-v1.11, this lied at .556 between Llama-2 and w2v2, .451 between Llama-2 and EfficientNet, and .639 between Efficient-Net and w2v2. For FAU-AIBO 2 , we also observed a larger agreement between w2v2-L-12-avd and CNN14 (.815) than between w2v2-L-12-avd and DistilBERT (.666) or CNN14 and DistilBERT (.672).\n\nAdditionally, we measured their complementarity by comparing the UAR of each model on the entire test set (Test-1) vs its UAR on the subset of the test set that each other model classifies correctly (+; green) or incorrectly (-; red). These results are shown in Section IV-B1. On MSP-Podcast-v1.11, w2v2-L-12-avd showed higher UAR when either Llama-2 or EfficientNet performed well -a sign that its correct predictions were a superset of the correct predictions of the other two models. In contrast, EfficientNet and Llama-2 only showed a performance improvement when evaluated on the samples that w2v2-L-12-avd classified correctly, thus showing little overlap between these pairs of models.\n\nThese results lead us to conclude that the complementarity was much higher between EfficientNet and Llama-2, than between w2v2-L-12-avd and any of the other two models. We hypothesise that this happens because w2v2-L-12-avd already captured linguistics implicitly, as shown in Triantafyllopoulos et al.  [12] , whereas EfficientNet did not. Seen in a different light, this means that w2v2-L-12-avd -and, generally, other pretrained transformers -seem capable of combining 'the best of both worlds'.\n\nOverall, this investigation revealed that audio-based models agreed more with one another than they agreed with a textbased model. However, the differences for FAU-AIBO were less pronounced than for MSP-Podcast-v1.11. We hypothesise that this was the case because a) FAU-AIBO is a German dataset and w2v2-L-12-avd had not been pre-trained on German data, and b) FAU-AIBO was collected in a Wizard-of-Oz scenario where linguistics play a minor role in the expression of emotion; hence, text-based models performed worse and audio-based models could draw little additional information from linguistics.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Discussion & Conclusion",
      "text": "In this section, we jointly consider all findings presented in the previous section and connect them to related work.\n\nWe begin with the most important research question that we set out to answer: is recent progress on SER obtained by using newer and bigger DL models monotonic? Our answer was inconclusive. Bigger and newer models did not necessarily lead to better IID or OOD performance, more robustness, or more fairness. In fact, we also observed opposing trends with newer/bigger models often showing lower fairness or worse performance. However, one notable outcome from our work is that any conclusions regarding progress are pre-conditioned on the particular set of models that are evaluated. As shown in Section IV-A, varying the set of models included in the analysis can have dramatic effects on observed correlation between model performance and year or computational complexity. This has major implications for any attempts to gauge progress on a long-term horizon. Replication studies like ours are inevitably Fig.  4 : UAR for each model on MSP-Podcast-v1.11 (left) and FAU-AIBO 2 (right) on the entire test set vs on the subset that each of the three models examined here classifies correctly (+; green) or incorrectly (-; red).\n\nlimited by the effort we may put in them and our subjective biases when selecting the methods that are included. Seen in a different light, reporting CIs on measures of progress is vital to avoid a misrepresentation of the underlying findings.\n\nThe most important caveat underlying our work is the strong impact of hyperparameters on model performance, as showcased in our tuning phase. The large variability obtained for the models which were optimised during the tuning phase indicates that it is hard to identify a clear winner unless a sufficient amount of variations are tested. This point is further discussed as a limitation of our work in the next section. However, we note that similar computational constraints plague most recent AI works, highlighting, once again, the need for exploring a space of hyperparameters as wide as possible -and certainly giving compared models a 'fair chance' by considering at least a few different hyperparameters and reporting the range of results.\n\nMoreover, our work can be positioned within the context of the scaling hypothesis, which is currently driving the debate in the broader AI community  [58]    6  . Is the solution to the SER problem merely a matter of scaling to bigger models and bigger data? We have shown that bigger models do not necessarily lead to better performance, though this was only done when comparing across architectures; the scaling hypothesis is typically applied within the same architecture family (add more and wider layers). Our computational resources did not allow us to investigate this question which is certainly worth pursuing in future work. However, understanding the inner workings of more successful models can also help pave future advances independent of scaling. For instance, our probing analysis revealed that state-of-the-art transformer models still struggle with predicting pitch variability. Designing novel architectures that are able to do so, or pre-training tasks that lead to models doing so, might be another interesting avenue for future research, especially in the face of scarce computational resources.\n\nUltimately, the scaling hypothesis is an important and pressing question for our field that our work has not conclusively answered. While our results point against it, we have not nearly reached the levels of scale commonly achieved by \"frontier models\" (on the scale of GPT-4o). In the case of academic research, this amount of compute resources is potentially unattainable in the near-term future. At the very least, the recent stabilisation of MSP-Podcast, and the existence of streamlined training pipelines, like our autrainer toolkit  [35] , represent an opportunity to establish a common benchmark for naturalistic SER that will allow comparisons of long-term progress.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vi. Limitations",
      "text": "We are aware that our work suffers from certain limitations, which we aim to discuss here.\n\nInsufficient data -From the multitude of available SER datasets, we chose to focus on only two, with agreement in the ranking of model performance across datasets being moderate-to-high. See Appendix A for a longer discussion on why we chose these two particular datasets. Insufficient models -With hundreds of DL papers published in the domain of SER in recent years, it was impossible to replicate all of them. We opted to focus on the ones that were easier to replicate from a coding perspective. Our most important omission was the noninclusion of large audio-language models  [59, 60] , i. e. , models which connect an audio encoder to an LLM decoder for joint audio-text modelling. While extremely promising, there were not many models available at the time when we conducted our experiments. Insufficient tuning -Our investigation of hyperparameter tuning revealed that model performance heavily depends on the choice of parameters. Scaling up that search is contingent upon the availability of compute power. It could very well be that our choice of hyperparameters for the exploration phase may have favoured some models over others. Inconclusive outcomes -The dependence of our results on the particular selection of models and the large impact of hyperparameters highlights the uncertainty of our findings. Nevertheless, counter to overall consensus, we did not observe a clear and consistent trend favouring bigger and more recent models. Furthermore, any observed gains can be attributed to a selection of hyperparameters that favoured some models more than others. We acknowledge all of the above limitations. They are all known problems that plague the entire AI community. In fact, our work can be seen in direct connection to works highlighting issues of comparability and replicability of findings  [61, 62, 63, 64] . Our biggest takeaway is to interpret recent findings with the same care; hyperparameter tuning, rather than architectural innovation, may be responsible for reported empirical gains. In particular, we urge for caution with regards to the scaling hypothesis, i. e. , the hypothesis that bigger models trained on more data leads to better results. The same limitations that plague our work are also important for works that provide confirmatory evidence for it -perhaps even more so given the possibility of publication bias in favour of positive results. As confirmation or refutation of this hypothesis stands to be a defining model for our field, we hope that our investigation serves as a reminder that the replication of findings is oftentimes just as important as the replication of results.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vii. Future Work",
      "text": "We end our contribution by considering potential pathways for future research. A major bottleneck in such large-scale replication studies is the lack of compute capacity. For academic institutions with modest computational resources this constraint can only be overcome by intense cross-institutional collaboration; resources can be pulled to increase the number of investigated models and hyperparameters. Shared authorship for shared compute could be a model that facilitates such large-scale collaboration; perhaps we could envision scientific publications with longer lists of authors as those seen in other fields, like physics.\n\nBeyond that, it is necessary to invest more effort in interpreting the ways in which models make their decisions (interpretability/explainability) and the ways in which they fail (error analysis/fairness/robustness). These analyses can uncover the weak-points of contemporary architectures and inspire novel ways to mitigate them. For instance, the fact that transformer models seem to be worse at predicting pitch variations than pitch itself is a clue worth further investigation.\n\nFinally, it is perhaps time to begin considering what may lie beyond deep learning  [65] . Perhaps we have reaped the benefits of incorporating feature extraction with classification and learning both from data (the so-called end-to-end paradigm), and further increasing the complexity of models and the data they ingest can only offer diminishing returns. Recent progress on reasoning models offers a promising avenue to integrate pre-trained 'world knowledge' with contextual features analysis to help overcome the inherent uncertainty in predicting human emotions in a naturalistic environment. from criticism in this regard, given that its material was sourced from podcasts which aired in public domains. Even though the subjects were (probably) not under the impression of being monitored, they were certainly aware that they were speaking in front of an audience. While this is a valid context in which to measure human emotions, it is a context nonetheless.\n\nEventually, we had to surrender to the fact that finding a dataset which encompasses the whole palette of human experience, with participants further being entirely unaware of their ever being monitored, would be impossible without resorting to unsolicited surveillance. For this reason, we settled for FAU-AIBO and MSP-Podcast -both encompassing only a specific snapshot of the human experience, which, limited though it may be, is still much wider than most datasets comprising solely acted emotional speech.\n\nOur next desideratum was the coding scheme, where we distinguish between schemes driven by theory, and schemes driven by observation. The vast majority of SER datasets follow a prescriptive, top-down approach and use a set of pre-defined emotional categories. Usually, these correspond to -or at the very least are inspired from -Ekman's \"big-6\"  [66] , plus a \"neutral\" category to designate a complete lack of emotion. This process is inherently model-based. In other words, it enforces a specific, pre-defined model of human emotions on the annotation process. MSP-Podcast follows this recipe by adopting 8 categorical labels (plus 1 \"other\" class), which we further restrict to the four categories most commonly-found in SER literature {anger, happiness, neutral, sadness}.\n\nWhile this enables a seamless translation of its annotations to many other SER corpora, it is still enforcing a particular model of human emotions and their expressions on the annotators. This model is not without criticism and might be too limited to capture the entire spectrum of human experience  [67] .\n\nOn the other end of the theory-observation divide are data-driven approaches. This is the paradigm that FAU-AIBO primarily adheres to. The set of candidate labels was developed iteratively by experienced annotators following a consensus approach. Then, the data was annotated using an original set of 11 labels on the word-level. Subsequently, a majority vote for the five annotators was taken over all word-level annotations in an utterance to derive an utterance-level annotation. The amount of labels was then reduced to 5 and 2 emotions, respectively, using a model-based mapping. As such, the dataset comprises a set of very unique emotion labels which do not easily translate to other contexts -but which, importantly, are tailored to the specific context that this dataset encapsulates.\n\nThe choice of coding scheme has important repercussions about the applicability of trained models and leads to a much larger debate. In colloquial terms, our overarching goal is to build algorithms that are able to correctly classify human emotions in all contexts and in all situations. Following generic, model-based approaches eases the translation of model predictions to new contexts, but to the detriment of specificity. On the other hand, building models that are tailor-made to specific scenarios suffers from scalability and risks the fragmentation of research efforts. The search for ways to accommodate these two paradigms is a very important research direction, but one that was beyond the scope of our work.\n\nSettling for one model-based and one data-driven dataset was a good compromise.\n\nFinally, dataset size and fame were two last considerations. Given our focus on DNNs, we naturally favoured datasets with more training instances. Moreover, we aimed for datasets that are more widely-used by, or at least known to, the broader SER community. Among the ones that are not acted, MSP-Podcast and FAU-AIBO were two reasonable choices.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. Models",
      "text": "This section contains the details for all pre-trained models used in our work.\n\nTable  IX  records the configuration files used to extract openSMILE features. We accommodate both static and dynamic versions of the official feature sets. For the dynamic features, we used a 2-layered long short-term memory network (LSTM) model with 32 hidden units, followed by mean pooling over time, one hidden linear layer with 32 neurons and ReLU acivation, and one output linear layer; all hidden layers are followed by a dropout of 0.5; these models are denoted with -d -lstm. Additionally, we train 3-layered MLPs with 64 hidden units each, a dropout of 0.5, and ReLU activation for the static features; these models are denoted with -s -mlp.\n\nTable  X  contains the paths to the model definitions of CRNNs models. We used two particular instantiations introduced by Tzirakis et al.  [68]  (CRNN 18 ) and Zhao et al.  [69]  (CRNN 19 ).\n\nTable  XI  contains the paths to model code and pre-trained states used for spectrogram-based models that have been pretrained on vision data. We used AlexNet, Resnet50, all versions of VGG ( 11,13,16,19 ), EfficientNet-B0 (EfficientNet), the tiny, small, base, and large versions of ConvNeXt ( t,b,s,l ), and the tiny, base, and small versions of the Swin Transformer ( t,b,s ). In all cases, we use the best-performing model state on ImageNet as available in the torchvision-v0.16.0 package. As features, we always used the Mel-spectrograms generated for CNN14, i. e. , 64 Mels with a window size of 32 ms and a hop size of 10 ms; the resulting matrices were then replicated over the three dimensions to generate the 3-channel input that is required by models designed for computer vision tasks.\n\nTable XII lists the models pre-trained on supervised audio tasks. We used CNN14 and AST pre-trained on AudioSet, as well as the ETDNN model pre-trained for speaker identification  [27] . Furthermore, we fine-tuned Whisper, albeit only its three smallest available variants of ( t,b,s ) due to hardware constraints.\n\nFinally, Table  XIII  documents the paths of SSL models, which are largely the same as the ones used in Wagner et al.  [7] . In this work, we used the pretrained states from the base and large variants of w2v2 and hubert (w2v2-b, w2v2-L, hubert-b, hubert-L), a multilingual model trained on VoxPopuli  [70]  (w2v2-L-vox), a 'robust' version of w2v2 trained on more data  [71]  (w2v2-L-robust), as well as the pruned version of that model further finetuned for dimensional SER on MSP-Podcast that was presented in Wagner et al.  [7]  (w2v2-L-12-avd). Similar to Wagner et al.  [7] , we add an output 2-layered MLP which takes the pooled hidden embeddings of the last layer as input.\n\nOur text-based models are listed in Table  XIV . While we use the same acronym to refer to both English and German version of some models, these do not always correspond to the same model state as some models were trained for a single language. We only use the \"second-generation\" LLMs for both languages as they were trained on multilingual data.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "C. Codecs",
      "text": "We investigated the robustness of w2v2-L-12-emo on the following codecs:\n\nMP3 (MPEG-1/2 Layer-3) combines psychoacoustic modeling and compression of the modified discrete cosine transform  [72] . We used the LAME version available in ffmpeg  8  . Its psychoacoustic model was tuned on manual listening tests and focuses on maintaining the quality of specific frequency bands. AAC (Advanced Audio Codec) was designed as a followup to MP3  [72] . It improves on it by -among ohtersincreasing the frequency resolution.\n\nOpus combines a short-and a long-term (linear) predictor.\n\nEnCodec is one of the most recent, data-driven codecs relying on DNNs  [73] . It is a simple, CNN-based autoencoder with an intermediate steps of residual vector quantisation. It has been trained on a large dataset comprising multiple audio sources. SemantiCodec aims for higher compression rates by incorporating a very powerful encoder (AudioMAE) and a more powerful denoising diffusion probabilistic model (DDPM) decoder  [74] . However, the improvement in compression rates comes with a substantial penalty on computational overhead.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "D. Robustness To Additive Noise",
      "text": "Table XV presents detailed robustness results for all models of our exploration phase.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "E. Linguistic Content",
      "text": "Fig.  5  shows the word distribution for FAU-AIBO and MSP-Podcast 9  . We observe substantially higher variability for MSP-Podcast, which is consistent with its in-the-wild procurement. FAU-AIBO, on the other hand, comprises a much more constricted, intent-oriented vocabulary, featuring mostly simply commands (\"geh nach links\"), as well as the frequent repetition of the word Aibo.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We observed moderate-to-high correlations between",
      "page": 5
    },
    {
      "caption": "Figure 1: Pairwise agreement in the relative ranking of models",
      "page": 6
    },
    {
      "caption": "Figure 2: Results of the tuning phase for FAU-AIBO2 (left), FAU-AIBO5 (middle), and MSP-Podcast-v1.11 (right). Showing",
      "page": 7
    },
    {
      "caption": "Figure 2: shows all tuning results on both FAU-AIBO tasks",
      "page": 7
    },
    {
      "caption": "Figure 3: Broadly, we note that all models",
      "page": 7
    },
    {
      "caption": "Figure 3: Transfer learning dynamics of models fine-tuned on 4-class SER using MSP-Podcast-v1.11. Plots show CKA between",
      "page": 8
    },
    {
      "caption": "Figure 4: UAR for each model on MSP-Podcast-v1.11 (left) and FAU-AIBO2 (right) on the entire test set vs on the subset that",
      "page": 11
    },
    {
      "caption": "Figure 5: shows the word distribution for FAU-AIBO and",
      "page": 15
    },
    {
      "caption": "Figure 5: Wordclouds visualising bi-gram occurrence frequency for FAU-AIBO (left) and MSP-Podcast (right).",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".646\n.404\n.471\n.655\n.382\n.498\n.665\n.385\n.250\n.666\n.200\n.459": ".680\n.392\n.514"
        },
        {
          ".646\n.404\n.471\n.655\n.382\n.498\n.665\n.385\n.250\n.666\n.200\n.459": ".669\n.353\n.537"
        },
        {
          ".646\n.404\n.471\n.655\n.382\n.498\n.665\n.385\n.250\n.666\n.200\n.459": ".683\n.372\n.503"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".500\n.400": ".663\n.409\n.665\n.412\n.674\n.406\n.678\n.404\n.692\n.394",
          ".518\n.518\n.513\n.531\n.528\n.524": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".640\n.402\n.561": ".667\n.418\n.570\n.672\n.306\n.250\n.684\n.411\n.555"
        },
        {
          ".640\n.402\n.561": ".656\n.279\n.405"
        },
        {
          ".640\n.402\n.561": ".684\n.411\n.609\n.684\n.380\n.387\n.686\n.200\n.250"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "3",
      "title": "Odyssey 2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Naini",
        "L Moro-Velázquez",
        "T Thebaud",
        "P Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop",
      "doi": "10.21437/odyssey.2024-35"
    },
    {
      "citation_id": "4",
      "title": "You BEEP Machine -Emotion in Automatic Speech Understanding Systems",
      "authors": [
        "R Huber",
        "E Nöth",
        "A Batliner",
        "J.-C Buckow",
        "V Warnke",
        "H Niemann"
      ],
      "year": "1998",
      "venue": "Proceedings of the First Workshop on Text, Speech, Dialogue -TSD'98"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in speech using neural networks",
      "authors": [
        "J Nicholson",
        "K Takahashi",
        "R Nakatsu"
      ],
      "year": "2000",
      "venue": "Neural computing & applications"
    },
    {
      "citation_id": "6",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brückner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proceedings of ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Hear: Holistic evaluation of audio representations",
      "authors": [
        "J Turian",
        "J Shier",
        "H Khan",
        "B Raj",
        "B Schuller",
        "C Steinmetz",
        "C Malloy",
        "G Tzanetakis",
        "G Velarde",
        "K Mcnally"
      ],
      "year": "2022",
      "venue": "NeurIPS Competitions and Demonstrations Track"
    },
    {
      "citation_id": "9",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K -T. Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H.-Y Lee"
      ],
      "year": "2021",
      "venue": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "doi": "10.21437/Interspeech.2021-1775"
    },
    {
      "citation_id": "10",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proceedings of LREC"
    },
    {
      "citation_id": "12",
      "title": "Probing speech emotion recognition transformers for linguistic knowledge",
      "authors": [
        "A Triantafyllopoulos",
        "J Wagner",
        "H Wierstorf",
        "M Schmitt",
        "U Reichel",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "13",
      "title": "Interspeech 2009 emotion challenge revisited: Benchmarking 15 years of progress in speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "A Batliner",
        "S Rampp",
        "M Milling",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proceedings of INTERSPEECH, Kos Island"
    },
    {
      "citation_id": "14",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2736999"
    },
    {
      "citation_id": "15",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of the European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "16",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: The fau aibo emotion corpus",
      "authors": [
        "A Batliner",
        "S Steidl",
        "E Nöth"
      ],
      "year": "2008",
      "venue": "Programme of the Workshop on Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "17",
      "title": "The Interspeech 2009 Emotion Challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Proceedings of INTERSPEECH, ISCA"
    },
    {
      "citation_id": "18",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "21",
      "title": "The bitter lesson",
      "authors": [
        "R Sutton"
      ],
      "year": "2019",
      "venue": "Incomplete Ideas (blog)"
    },
    {
      "citation_id": "22",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "An Image-based Deep Spectrum Feature Representation for the Recognition of Emotional Speech",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "G Hagerer",
        "A Batliner",
        "S Steidl",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM Multimedia Conference"
    },
    {
      "citation_id": "24",
      "title": "Exploring deep spectrum representations via attentionbased recurrent and convolutional neural networks for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Y Zhao",
        "Z Zhang",
        "N Cummins",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "AST: Audio Spectrogram Transformer",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "26",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "Proceedings of ICASSP"
    },
    {
      "citation_id": "27",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Proceedings of INTERSPEECH, Shanghai"
    },
    {
      "citation_id": "28",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "The role of task and acoustic similarity in audio transfer learning: Insights from the speech emotion recognition case",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of ICASSP, IEEE"
    },
    {
      "citation_id": "30",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of ICML"
    },
    {
      "citation_id": "31",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems (NeurIPS)"
    },
    {
      "citation_id": "32",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "You stupid tin box\" -children interacting with the AIBO robot: A cross-linguistic emotional speech corpus",
      "authors": [
        "A Batliner",
        "C Hacker",
        "S Steidl",
        "E Nöth",
        "S D'arcy",
        "M Russell",
        "M Wong"
      ],
      "year": "2004",
      "venue": "Proceedings of LREC"
    },
    {
      "citation_id": "34",
      "title": "Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech",
      "authors": [
        "S Steidl"
      ],
      "year": "2009",
      "venue": "Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech"
    },
    {
      "citation_id": "35",
      "title": "Autrainer: A modular and extensible deep learning toolkit for computer audition tasks",
      "authors": [
        "S Rampp",
        "A Triantafyllopoulos",
        "M Milling",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Autrainer: A modular and extensible deep learning toolkit for computer audition tasks",
      "arxiv": "arXiv:2412.11943"
    },
    {
      "citation_id": "36",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "37",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "38",
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing"
    },
    {
      "citation_id": "39",
      "title": "Electra: Pretraining text encoders as discriminators rather than generators",
      "authors": [
        "K Clark",
        "M.-T Luong",
        "Q Le",
        "C Manning"
      ],
      "year": "2020",
      "venue": "Proceedings of ICLR, Online"
    },
    {
      "citation_id": "40",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "41",
      "title": "The Llama 3 herd of models",
      "authors": [
        "A Grattafiori",
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Vaughan"
      ],
      "year": "2024",
      "venue": "The Llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "42",
      "title": "Mistral 7b",
      "authors": [
        "A Jiang",
        "A Sablayrolles",
        "A Mensch",
        "C Bamford",
        "D Chaplot",
        "D Casas",
        "F Bressand",
        "G Lengyel",
        "G Lample",
        "L Saulnier"
      ],
      "year": "2024",
      "venue": "Mistral 7b",
      "arxiv": "arXiv:2310.06825"
    },
    {
      "citation_id": "43",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of ICLR, online"
    },
    {
      "citation_id": "44",
      "title": "A curated dataset of urban scenes for audio-visual scene analysis",
      "authors": [
        "S Wang",
        "A Mesaros",
        "T Heittola",
        "T Virtanen"
      ],
      "venue": "Proceedings of ICASSP"
    },
    {
      "citation_id": "45",
      "title": "Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proceedings of INTERSPEECH, Kos Island"
    },
    {
      "citation_id": "46",
      "title": "What size test set gives good error rate estimates?",
      "authors": [
        "I Guyon",
        "J Markhoul",
        "R Schwartz",
        "V Vapnik"
      ],
      "year": "1998",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "47",
      "title": "A formula for the gini coefficient",
      "authors": [
        "R Dorfman"
      ],
      "year": "1979",
      "venue": "The review of economics and statistics"
    },
    {
      "citation_id": "48",
      "title": "Understanding machine learning: From theory to algorithms",
      "authors": [
        "S Shalev-Shwartz",
        "S Ben-David"
      ],
      "year": "2014",
      "venue": "Understanding machine learning: From theory to algorithms"
    },
    {
      "citation_id": "49",
      "title": "Underspecification presents challenges for credibility in modern machine learning",
      "authors": [
        "A D'amour",
        "K Heller",
        "D Moldovan",
        "B Adlam",
        "B Alipanahi",
        "A Beutel",
        "C Chen",
        "J Deaton",
        "J Eisenstein",
        "M Hoffman"
      ],
      "year": "2022",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "50",
      "title": "On the power of curriculum learning in training deep networks",
      "authors": [
        "G Hacohen",
        "D Weinshall"
      ],
      "year": "2019",
      "venue": "Proceedings of ICML"
    },
    {
      "citation_id": "51",
      "title": "Does the definition of difficulty matter? scoring functions and their role for curriculum learning",
      "authors": [
        "S Rampp",
        "M Milling",
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Does the definition of difficulty matter? scoring functions and their role for curriculum learning",
      "arxiv": "arXiv:2411.00973"
    },
    {
      "citation_id": "52",
      "title": "Scalable hyperparameter transfer learning",
      "authors": [
        "V Perrone",
        "R Jenatton",
        "M Seeger",
        "C Archambeau"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems (NeurIPS)"
    },
    {
      "citation_id": "53",
      "title": "Similarity of neural network representations revisited",
      "authors": [
        "S Kornblith",
        "M Norouzi",
        "H Lee",
        "G Hinton"
      ],
      "year": "2019",
      "venue": "Proceedings of ICML, PMLR"
    },
    {
      "citation_id": "54",
      "title": "Deep learning of representations for unsupervised and transfer learning",
      "authors": [
        "Y Bengio"
      ],
      "year": "2012",
      "venue": "Proceedings of ICML Workshop on Unsupervised and Transfer Learning"
    },
    {
      "citation_id": "55",
      "title": "Rethinking cnn models for audio classification",
      "authors": [
        "K Palanisamy",
        "D Singhania",
        "A Yao"
      ],
      "year": "2020",
      "venue": "Rethinking cnn models for audio classification",
      "arxiv": "arXiv:2007.11154"
    },
    {
      "citation_id": "56",
      "title": "What is being transferred in transfer learning?",
      "authors": [
        "B Neyshabur",
        "H Sedghi",
        "C Zhang"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems (NeurIPS)"
    },
    {
      "citation_id": "57",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "58",
      "title": "Scaling laws for neural language models",
      "authors": [
        "J Kaplan",
        "S Mccandlish",
        "T Henighan",
        "T Brown",
        "B Chess",
        "R Child",
        "S Gray",
        "A Radford",
        "J Wu",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "59",
      "title": "Computer audition: From task-specific machine learning to foundation models",
      "authors": [
        "A Triantafyllopoulos",
        "I Tsangko",
        "A Gebhard",
        "A Mesaros",
        "T Virtanen",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Computer audition: From task-specific machine learning to foundation models",
      "arxiv": "arXiv:2407.15672"
    },
    {
      "citation_id": "60",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "S Latif",
        "M Usama",
        "M Malik",
        "B Schuller"
      ],
      "year": "2025",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "61",
      "title": "Winner's curse? on pace, progress, and empirical rigor",
      "authors": [
        "J Snoek",
        "A Wiltschko",
        "A Rahimi"
      ],
      "year": "2018",
      "venue": "Proceedings of ICLR (Workshop Track)"
    },
    {
      "citation_id": "62",
      "title": "Troubling trends in machine learning scholarship: Some ml papers suffer from flaws that could mislead the public and stymie future research",
      "authors": [
        "Z Lipton",
        "J Steinhardt"
      ],
      "year": "2019",
      "venue": "Queue"
    },
    {
      "citation_id": "63",
      "title": "On empirical comparisons of optimizers for deep learning",
      "authors": [
        "D Choi",
        "C Shallue",
        "Z Nado",
        "J Lee",
        "C Maddison",
        "G Dahl"
      ],
      "year": "2019",
      "venue": "On empirical comparisons of optimizers for deep learning",
      "arxiv": "arXiv:1910.05446"
    },
    {
      "citation_id": "64",
      "title": "Unreproducible research is reproducible",
      "authors": [
        "X Bouthillier",
        "C Laurent",
        "P Vincent"
      ],
      "year": "2019",
      "venue": "Proceedings of ICML, PMLR"
    },
    {
      "citation_id": "65",
      "title": "Beyond deep learning: Charting the next frontiers of affective computing",
      "authors": [
        "A Triantafyllopoulos",
        "L Christ",
        "A Gebhard",
        "X Jing",
        "A Kathan",
        "M Milling",
        "I Tsangko",
        "S Amiriparian",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Beyond deep learning: Charting the next frontiers of affective computing"
    },
    {
      "citation_id": "66",
      "title": "Basic emotions",
      "authors": [
        "P Ekman",
        "T Dalgleish",
        "M Power"
      ],
      "year": "1999",
      "venue": "Basic emotions"
    },
    {
      "citation_id": "67",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "68",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of ICASSP, IEEE"
    },
    {
      "citation_id": "69",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "70",
      "title": "Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "authors": [
        "C Wang",
        "M Rivière",
        "A Lee",
        "A Wu",
        "C Talnikar",
        "D Haziza",
        "M Williamson",
        "J Pino",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "71",
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "72",
      "title": "Mp3 and aac explained",
      "authors": [
        "K Brandenburg"
      ],
      "year": "1999",
      "venue": "Audio Engineering Society Conference: 17th International Conference: High-Quality Audio Coding"
    },
    {
      "citation_id": "73",
      "title": "High fidelity neural audio compression",
      "authors": [
        "A Défossez",
        "J Copet",
        "G Synnaeve",
        "Y Adi"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "74",
      "title": "Semanticodec: An ultra low bitrate semantic audio codec for general sound",
      "authors": [
        "H Liu",
        "X Xu",
        "Y Yuan",
        "M Wu",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2024",
      "venue": "Under review TABLE IX: Feature URLs for openSMILE"
    },
    {
      "citation_id": "75",
      "title": "TABLE X: Model URLs for end-to-end CRNNs",
      "venue": "TABLE X: Model URLs for end-to-end CRNNs"
    },
    {
      "citation_id": "76",
      "title": "TABLE XI: Model URLs for computer vision models applied on spectrograms",
      "authors": [
        "Url Model",
        "Crnn"
      ],
      "venue": "TABLE XI: Model URLs for computer vision models applied on spectrograms"
    },
    {
      "citation_id": "77",
      "title": "Model URL AlexNet",
      "venue": "Model URL AlexNet"
    },
    {
      "citation_id": "78",
      "title": "Resnet50",
      "venue": "Resnet50"
    },
    {
      "citation_id": "79",
      "title": "VGG 11",
      "venue": "VGG 11"
    },
    {
      "citation_id": "80",
      "title": "VGG 13",
      "venue": "VGG 13"
    },
    {
      "citation_id": "81",
      "title": "VGG 16",
      "venue": "VGG 16"
    },
    {
      "citation_id": "82",
      "title": "VGG 19",
      "venue": "VGG 19"
    },
    {
      "citation_id": "83",
      "title": "ConvNeXt t",
      "venue": "ConvNeXt t"
    },
    {
      "citation_id": "84",
      "title": "Swin t",
      "venue": "Swin t"
    },
    {
      "citation_id": "85",
      "title": "Swin s",
      "venue": "Swin s"
    },
    {
      "citation_id": "86",
      "title": "Swin b",
      "venue": "Swin b"
    },
    {
      "citation_id": "87",
      "title": "Model URLs for supervised audio models",
      "authors": [
        "Table Xii"
      ],
      "venue": "Model URLs for supervised audio models"
    },
    {
      "citation_id": "88",
      "title": "Model URL CNN",
      "venue": "Model URL CNN"
    },
    {
      "citation_id": "89",
      "title": "",
      "authors": [
        "Ast Mit/"
      ],
      "venue": ""
    }
  ]
}