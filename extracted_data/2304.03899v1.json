{
  "paper_id": "2304.03899v1",
  "title": "An Empirical Study And Improvement For Speech Emotion Recognition",
  "published": "2023-04-08T03:24:06Z",
  "authors": [
    "Zhen Wu",
    "Yizhe Lu",
    "Xinyu Dai"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Multimodal fusion",
    "Perspective loss",
    "Deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal speech emotion recognition aims to detect speakers' emotions from audio and text. Prior works mainly focus on exploiting advanced networks to model and fuse different modality information to facilitate performance, while neglecting the effect of different fusion strategies on emotion recognition. In this work, we consider a simple yet important problem: how to fuse audio and text modality information is more helpful for this multimodal task. Further, we propose a multimodal emotion recognition model improved by perspective loss. Empirical results show our method obtained new state-of-the-art results on the IEMOCAP dataset. The indepth analysis explains why the improved model can achieve improvements and outperforms baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is a crucial subtask of affective computing, which aims to identify the speaker's emotions from user utterances  [1] . In recent years, this task has drawn the increasing attention of researchers and industries because of its wide applications in social media analysis, humancomputer interaction, and customer service.\n\nEarly works use audio-only information to identify speech emotion  [2, 3, 4] . For example, Mirsamad et al.  [2]  employ attention-based RNN to capture emotion-aware features. Gao et al.  [4]  eliminate the influence of different speakers in the audio by adversarial neural networks. In fact, speech naturally contains multimodal audio and text information, and they embody emotions from phonetics and linguistics respectively. Realizing this characteristic, the following researchers use multimodal information to build neural models. These works  [5, 6, 7, 8, 9]  widely use cross-attention to fuse information from different modalities and achieve a considerable improvement over unimodal methods in accuracy.\n\nZhen Wu and Yizhe Lu contributed equally. This work was supported by the National Natural Science Foundation of China (No. 62206126 and 61976114). Our code is available at https://github.com/Luyizhe/ SpeechEmotion.\n\nThe above studies all focus on the emotional recognition of a single utterance. Considering the relevance between utterances, more recent works step towards the actual scenario, conversational multimodal emotion recognition. Poria et al.  [10]  adopt hierarchical modeling, and they extract utterance-level features first and then aggregate dialoguelevel information. In the following work  [1] , they propose an attention-based fusion (AT-Fusion) approach  [1]  to obtain different modal utterance representations and get the global information by additional LSTM  [11]  and self-attention mechanism  [12] . Besides, graph neural networks are also employed to fuse audio and text modality features  [13, 14] .\n\nDespite the effectiveness, the existing works rely on advanced neural networks to achieve remarkable results. They neglect the effect of fusion manners of different modalities on emotion recognition. In this work, we revisit the issue and answer one crucial question: what fusion manner is more effective for multimodal emotion recognition model?\n\nTo achieve the goal, we extract a general unimodal audio/text framework from existing research as a fundamental component and investigate the effect of various ways of fusing multimodal information at different layers. We find some simple operations have achieved very competitive performance. Further, we proposed an improved multimodal model with the perspective loss and achieved new state-ofthe-art results on the IEMOCAP  [15]  dataset. Empirical results and analysis explain why the multimodal model and our improved version can achieve better performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "We define the emotion recognition task in conversation first and then show different fusion methods. Finally, we introduce our perspective loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Definition",
      "text": "A set of dialogue U = {U 1 , U 2 , ..., U N } contains N dialogues. Each dialogue U = {(u 1 , e 1 ), (u 2 , e 2 ), ..., (u S , e S )} consists of S utterances and u i is its i th utterance. The emotion label of utterance u i is e i ∈{happy, sad, neutral, angry}. The task is to build a model to fit the N labeled dialogues U and then predict the emotion labels of dialogues in the test set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Unimodal Framework",
      "text": "Before applying different fusion methods, we introduce our unimodal audio/text framework.\n\nAs illustrated in Fig.  1 , the general unimodal framework is extracted from the existing multimodal framework  [1, 10, 16] . First, we extract utterance-level audio features or text features (see section 3.2). For the unimodal features, we then apply a bi-directional GRU (Bi-GRU) to encode all utterances and capture the past and future information of the dialogue. Next, we use the outputs of the Bi-GRU layer as the inputs of the self-attention module  [1] , which gets rich contextual information from relevant utterances. After that, the outputs of the self-attention module are sent to a fully-connected (FC) layer, which maps high-level features to dimensions of emotion categories. Note that, the FC's output is also called logits. Finally, a softmax layer can be applied to transform logits into probabilities for unimodal emotion classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fusion Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unimodal Framework Contains Multiple Stacked Components.",
      "text": "To analyze the effect of multimodal fusion at different layers, we explore four positions: x Early Fusion (EF), y Middle Fusion (MF), z Late Fusion (LF), and { Logits Fusion (LGF), as shown in Fig.  1 .\n\nIn terms of fusion strategy, there are three general methods of multimodal fusion in the literature, respectively ADD-Fusion, Concat-Fusion, and AT-Fusion. To describe them clearly, we take Logits Fusion (LGF) as an example and introduce some necessary notations. Specifically, a i ∈ R d and t i ∈ R d respectively represent the audio and text representations from unimodal position { before fusion. Here audio representation and text representation have the same dimension of d because we map original audio and text features to the same dimension early when extracting multimodal features (section 3.2).\n\nADD-Fusion. In this manner, we directly add two different modal features to obtain the fusion representation z i ∈ R d . The process is as follows:\n\nConcat-Fusion. In this strategy, different modal features are concatenated together and then mapped to the target dimension. The fused representation z i ∈ R d is as follows:\n\nwhere W f ∈ R d×2d and [; ] denotes concatenation. AT-Fusion. AT-Fusion fuses different modal features in the form of a weighted sum, where weights measure modality importance. The detailed calculation process is the same as the fusion process of  [1] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Perspective Loss",
      "text": "Usually, the fused representation z i can be used for emotion classification directly. The corresponding cross-entropy loss is marked as Single Loss:\n\nwhere ŷi and y i are respectively the predicted label and truth label of utterance u i in the dialogue U . It is common sense that audio and text modalities contain diverse features, e.g., phonetics and linguistics. Inspired by the work  [17] , we introduced additional cross-entropy losses of emotion classification respectively for audio and text modalities to encourage the model to retain helpful unimodal characteristics for emotion when fusing multimodal information. Finally, we combined these two unimodal losses and the above Single Loss to form Perspective Loss:\n\nwhere ŷa i and ŷt i respectively represent the predicted labels of audio modality and text modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Features Extraction",
      "text": "In this work, we extract the unimodal audio and text utterance features with different tools.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Features",
      "text": "Following the work  [19] , we use the software openSMILE with the config file IS13 ComParE to extract 6373 features for each utterance. Z-standardization is performed on features for normalization. We then employ an FC layer to reduce the dimension of the audio vector to 100, which is used as the final utterance-level features of audio.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Features",
      "text": "For each utterance, we use the pre-trained BERT base to encode text and adopt mean pooling of all token representations to obtain a vector with 768 dimensions. After that, we use an FC layer to reduce the vector to 100 dimensions as the utterance features of text modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hyper-Parameters And Metric",
      "text": "We set the hidden states of Bi-GRU network to 100 dimensions. The self-attention layer contains 100-dimensional states and 4 attention heads. We use ReLU as the nonlinear activation layer. The Adam optimization is adopted to optimize the model with a learning rate of 0.001 and decay of 0.00001. We train the models for 150 epochs with a batch size of 20, dropout with p = 0. weights and report the average result to reduce the randomness. Weighted accuracy (WA)  [2]  is chosen as our evaluation metric to compare our method with previous approaches.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "In this section, we show the effects of multimodality, different fusions, and perspective loss on speech emotion recognition.\n\nThen we compare the results of the proposed model with other state-of-the-art methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Unimodal V.S. Multimodal",
      "text": "Table  1  with Table  2  respectively show the results of our unimodal framework and multimodal framework on the IEMO-CAP dataset. As in Table  1 , the performance of text modality surpasses audio modality significantly. It is reasonable because we observe most speech emotions can be inferred from texts directly. After fusing audio and text modal information, multimodal models outperform unimodal models in most settings. This phenomenon shows that the information in the two modalities can be complementary. It provides the possibility of designing various multimodal fusion models to improve speech emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Effects Of Different Fusions",
      "text": "As shown in Table  2 , regarding different fusion methods, we find ADD-Fusion performs the best in the three methods wherever we conduct fusion operations. This may be attributed to that ADD-Fusion operation utilizes two modalities equally and make them complementary better in feature space. In contrast, Concat-Fusion and AT-Fusion may lose focus on certain modalities due to the weighted combination. When fusing at different layers, EF with ADD-Fusion shows better performance if using Single Loss, which indicates early fusion makes multimodal information interact more fully. In contrast, LGF performs the best when we use Perspective Loss to enhance modality characteristics. We guess Perspective Loss makes the features in LGF decoupled and contributes to emotion recognition from different views.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Single Loss V.S. Perspective Loss",
      "text": "From Table  2 , we observe that multimodal models equipped with Perspective Loss bring obvious performance improvements in most fusion settings. To understand this point, we take LGF with ADD-Fusion as an example to study why Perspective Loss is so effective. Table  3  shows the detailed results of various modalities and models on different emotions.\n\nFor the unimodal structure, the text modality has excellent recognition ability in happy and sad emotions, while the audio modality performs well on angry. It is reasonable because users often use explicit wordings to express their happy and sad emotions, which makes text modality have better predictions on these two types. In contrast, it is easy to judge whether users are angry or not according to their tones, which are usually embodied in the audio signal.\n\nFor the multimodal model using Single Loss, the performance all decreases on the above three emotions. One possible reason is that text modality dominates information and audio modality is weakened when using simple fusion. Specifically, the multimodal model (A+T) has great performance degradation (up to 7%) than the unimodal audio framework on angry. Besides, it also decreases slightly on happy and sad emotions because simple fusion mixes up multimodal features and obscures some key unimodal information.\n\nAfter using Perspective Loss, the multimodal model can retain key discriminative unimodal characteristics of these specific emotions when performing fusion. We observe the Approaches WA(%) bc-LSTM (2017)  [10]  75.60 † CATF-LSTM (2017)  [1]  80.10 † Zheng. (2019)  [20]  78.02 DANN (2020)  [21]  82.68 CONSK-GCN (2021)  [14]  84.79 ‡ Soumya. (2022)  [18]  83.80 Our Method 85.40 * model obtains the same or similar performance on happy, sad, and angry emotions. Additionally, it also achieves improvement in neutral. Finally, the two modalities play different roles and contribute to better overall performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Method V.S. Previous Works",
      "text": "Table  4  shows the performance of different methods on the IEMOCAP dataset. By comparison, our method (LGF+ADD-Fusion+Perspective Loss) achieves a performance improvement of 0.61% over the previous state-of-the-art method. This result indicates our method is simple but very effective for speech emotion recognition. Actually, previous works also use both audio and text modal features. However, they usually prematurely fuse multimodal features without perspective loss constraint, which may bear some noise information that is irrelevant to emotion. In contrast, LGF makes use of discriminative emotional information. Further, Perspective Loss makes each modality can play its own characteristics in predicting emotion. From Table  2  and Table  4 , we also see multimodal models without Perspective Loss are inferior to state-of-the-art methods. Perspective Loss facilitates emotion prediction significantly.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we empirically study speech emotion recognition from audio and text and explore the effect of different fusions of modalities. We find that the ADD-Fusion is a simple yet effective method. On this basis, we further propose Perspective Loss to retain the key and discriminative unimodal characteristics of specific emotions at fusion. The results show our method achieves state-of-the-art performance on the IEMOCAP dataset. We believe this study can be beneficial for other research on multimodal models and tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of unimodal structure.",
      "page": 2
    },
    {
      "caption": "Figure 1: , the general unimodal framework is",
      "page": 2
    },
    {
      "caption": "Figure 1: In terms of fusion strategy, there are three general meth-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 3: The accuracy of unimodal model and multimodal CAP dataset. † indicates results from audio+text modalities.",
      "data": [
        {
          "Emotion on IEMOCAP(%)": "happy\nsad\nneutral\nangry"
        },
        {
          "Emotion on IEMOCAP(%)": "72.01\n75.51\n60.16\n88.24\n90.52\n90.61\n71.88\n75.88"
        },
        {
          "Emotion on IEMOCAP(%)": "48.76\n86.53\n60.51\n84.12\n84.20\n71.43\n78.12\n65.88\n89.62\n88.57\n73.44\n81.18"
        },
        {
          "Emotion on IEMOCAP(%)": "58.24\n73.88\n59.64\n90.59\n90.29\n88.98\n76.82\n61.18\n90.52\n89.39\n76.56\n88.24"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multi-level multiple attentions for contextual multimodal sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Mazumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "ICDM"
    },
    {
      "citation_id": "3",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Multistream attention-based BLSTM with feature segmentation for speech emotion recognition",
      "authors": [
        "Yuya Chiba",
        "Takashi Nose",
        "Akinori Ito"
      ],
      "year": "2020",
      "venue": "Multistream attention-based BLSTM with feature segmentation for speech emotion recognition"
    },
    {
      "citation_id": "5",
      "title": "Domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition",
      "authors": [
        "Yuan Gao",
        "Jiaxing Liu",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "Krishna Dn",
        "Ankita Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks"
    },
    {
      "citation_id": "7",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "Haiyang Xu",
        "Hui Zhang",
        "Kun Han",
        "Yun Wang",
        "Yiping Peng",
        "Xiangang Li"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Multimodal speech emotion recognition using cross attention with aligned audio and text",
      "authors": [
        "Yoonhyung Lee",
        "Seunghyun Yoon",
        "Kyomin Jung"
      ],
      "year": "2020",
      "venue": "Multimodal speech emotion recognition using cross attention with aligned audio and text"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Subhadeep Dey",
        "Kyomin Jung"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Attentive modality hopping mechanism for speech emotion recognition",
      "authors": [
        "Seunghyun Yoon",
        "Subhadeep Dey",
        "Hwanhee Lee",
        "Kyomin Jung"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "12",
      "title": "Long shortterm memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "13",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "14",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "CONSK-GCN: Conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Yahui Fu",
        "Shogo Okada",
        "Longbiao Wang",
        "Lili Guo",
        "Yaodong Song",
        "Jiaxing Liu",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "ICME"
    },
    {
      "citation_id": "16",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "17",
      "title": "HiGRU: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "18",
      "title": "Improving review representations with user attention and product attention for sentiment classification",
      "authors": [
        "Zhen Wu",
        "Xin-Yu Dai",
        "Cunyan Yin",
        "Shujian Huang",
        "Jiajun Chen"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "19",
      "title": "Multimodal transformer with learnable frontend and self-attention for emotion recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "21",
      "title": "Conversational emotion analysis via attention mechanisms",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang"
      ],
      "year": "2019",
      "venue": "Conversational emotion analysis via attention mechanisms"
    },
    {
      "citation_id": "22",
      "title": "Context-dependent domain adversarial neural network for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang",
        "Zhanlei Yang",
        "Rongjun Li"
      ],
      "venue": "Context-dependent domain adversarial neural network for multimodal emotion recognition"
    }
  ]
}