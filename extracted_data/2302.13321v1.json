{
  "paper_id": "2302.13321v1",
  "title": "Multi-Modality In Music: Predicting Emotion In Music From High-Level Audio Features And Lyrics",
  "published": "2023-02-26T13:38:42Z",
  "authors": [
    "Tibor Krols",
    "Yana Nikolova",
    "Ninell Oldenburg"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper aims to test whether a multimodal approach for music emotion recognition (MER) performs better than a unimodal one on high-level song features and lyrics. We use 11 song features retrieved from the Spotify API, combined lyrics features including sentiment, TF-IDF and Anew to predict valence and arousal  (Russell, 1980)  scores on the Deezer Mood Detection Dataset (DMDD)  (Delbouys et al., 2018)  with 4 different regression models. We find that out of the 11 high-level song features, mainly 5 contribute to the performance, multi-modal features do better than audio alone when predicting valence. We made our code publically available 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The emotional effect of music is a fact both wellstudied  (Hunter & Schellenberg, 2010; Balkwill & Thompson, 1999; Deng et al., 2015)  and intimately familiar for most people. Furthermore, cross-cultural consistency in emotion recognition in music  (Balkwill & Thompson, 1999; Fritz et al., 2009)  indicates that fundamental cognitive capacities are at play in the process. With the rise of music digitization and affective computing technologies, there has been an increasing interest in extracting emotional information from music for research, therapy  (Dingle et al., 2015; Bernatzky et al., 2011)  or music recommendation  (Deng et al., 2015; Park et al., 2015)  purposes. This research falls under the term Music Emotion Recognition or MER  (Chen et al., 2015) . Nowadays, even music streaming platforms like Spotify come with an API that makes a wide range of features accessible and therefore open to the public.\n\nBut which features can actually predict the emotion of a song and how well perform Spotify's annotations? Building on existing literature presented in Section 2 we hypothesize that a multimodal approach combining high-level auditory and lyrics-extracted features performs better than a uni-modal one (Y.-H.  Yang, Lin, Cheng, et al., 2008; Hu & Downie, 2010b , 2010a) . We introduce our MER model in Section 3 before presenting and discussing the results of our exploratory and regression experiments in Sections 4 and 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The work on automatic information or prediction methods from music began with the rise of digital music. The annual Music Information Research Evaluation eXchange (MIREX)  2  included audio music mood as a classification task for the first time in 2007  (MIREX, 2007)  already finding that ground-truth labeling and human mood judgment play a critical role on this task  (Hu et al., 2008) . The used range of features already varied from spectral, so-called low-level features (raw frequency signal, for example, spectral centroids and SD (X.  Yang et al., 2018) ) to higher-level features like \"danceability\"  (Hu et al., 2008) , rhythm, or timbre (X.  Yang et al., 2018)  that can be extracted from low-level features and are being described as closer to human perception  (Panda et al., 2021) . From there on, systems were consequently further being improved  (Kim et al., 2010) .\n\nWhile the literature broadly distinguishes between three tasks of MER, i) classification into emotion categories, ii) emotion as a regression problem, and iii) variation detection throughout a song (Y.-H.  Yang & Chen, 2012 ), we will focus on the second problem of MER as regression since it is seen as more accurate in regards to the represented emotion  (Guan et al., 2012) . For this, Russell (1980)'s two-dimensional model of emotion as a numerical value of valence (negative to positive) and arousal (passive to active) is the most widely used metric  (Huang et al., 2016; Trigeorgis et al., 2016; Wang et al., 2011) . For example, anger is high arousal and low valence emotion. This framework is preferred for its simplicity and is used in the present study, although it does not capture the full range of emotions  (Cowen et al., 2020) , which is why other MER studies  (Hu & Downie, 2010a; Hu et al., 2009)  use a larger number of affective categories.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mer As Regression Task",
      "text": "The first approaches for this regression problem were done by Y.-H.  Yang, Lin, Su, and Chen (2008)  where they reached a valence-arousal (VA) R 2 score of up to 0.282 and 0.538 with lowlevel auditory features using a Support Vector Machine Regressor (SVR), similar to the SVR approach presented in  Soleymani et al. (2014)  that reached a VA score of 0.42 and 0.52 based on supra-segmental features. Later,  Bai et al. (2016) 's Random Forest Regressor (RFR) reached a VA score of 0.293 and 0.625 on 548 dimensions of low-and higher-level audio features. Other approaches include a Multiple Linear Regression (MLR) (X.  Yang et al., 2018; Vatolkin & Nagathil, 2019) , or a Multilayer Perceptron (MLP)  (Soleymani et al., 2014; Bhattarai & Lee, 2019) .\n\nThe rise of fast and efficient deep learning (DL) models across a wide range of applications also impacted MER. One of the first DL bi-modal classification models was presented in  (Jeon et al., 2017)  using an end-to-end approach for lyrics and raw audio without any feature engineering, fusioning the modalities mid-level. Their model outperforms uni-modal approaches at that time with an accuracy of 80%. They also find that lyrics predict the emotion more precisely than audio. A study by  Delbouys et al. (2018) , investigates the performance of a classical feature engineering approach vs. DL. On the Deezer Mood Detection Dataset (DMDD), they extract low-level auditory features and word embeddings from a list of VA-related words and compare the mid-level (second layer or later) and late-level (weighted outputs) fusion of both of these feature classes against an end-to-end DL approach. They can show that i) mid-level fu-sion on DL has comparable results to the feature engineering and that ii) the multi-modal mid-level fusion outperforms both uni-modal models (audio vs. textual).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Lyrics As Prediction Metric",
      "text": "D.  Yang and Lee (2004) , who are often seen as the first to combine audio and lyrics in MER modeling, use a Bag-of-Words (BoW) approach combined with the General Inquirer psychological lexical annotation to model lyrics.  Hu et al. (2009) ;  Hu and Downie (2010a, 2010b) ;  Hu et al. (2017) ;  Laurier et al. (2008)  expanded lyrics features to include n-gram counts, stylistic features, and ANEW sentiment annotation  (Bradley & Lang, 1998)  and found that lyrics tend to outperform auditory features on emotion classification and regression tasks, though the best results were achieved by combining the features. As mentioned above, the same pattern holds in  Delbouys et al. (2018) 's DL model on word embeddings.\n\nIn their recent,  Han, Kong, Han, and Wang (2022)  report that BoW, TF-IDF, and word embeddings are the most used text representation methods in MER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Higher-Level Features",
      "text": "For the relation of features to the two predicted categories,  Gabrielsson and Lindström (2001)  and  Juthi et al. (2020)  describe arousal as being related to tempo (fast/slow), pitch (high/low), loudness level (high/low), and timbre (bright/soft) as well as valence being related to mode (major/minor) and harmony (consonant/dissonant).  Vatolkin and Nagathil (2019) , however, state the importance of also rhythm for the prediction of valence.  Panda et al. (2021)  were the first ones to assess the correlation of 12 high-level Spotify features on the prediction accuracy of valence and arousal. They found that only three (energy, valence and acousticness) were relevant.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Spotify",
      "text": "Spotify, with its 422 million active monthly users is the largest music streaming service on the market  (Ruby, 2022) . It provides easy access to 70+ million songs  (Ruby, 2022)  as well as an API that makes it possible to extract a customized dataset of annotated musical pieces 3 . The annotation procedure of such is unknown to the public as this is one of Spotify's major revenue streams  (Hucker, 2021) . However, the annotation technology seems automated was developed by The Echo Nest (acquired by Spotify in 2014) based on expert-annotated data. (The Echo Nest Blog, 2013).\n\n3 Experimental Setup",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "The song dataset used in the present study is the Deezer Mood Detection Dataset (DMDD)  (Delbouys et al., 2018)  which holds VA scores for 18,644 songs and is based on the Million Song Dataset (Bertin-Mahieux, Ellis, Whitman, & Lamere, 2011) as well as tags from LastFM 4  that are related to mood  (Hu & Downie, 2010b) . The VA scores were obtained by applying an extended ANEW (XANEW) dataset with 14,000 words and their respective VA scores  (Warriner et al., 2013; Bradley & Lang, 1998)  to the tags from LastFM.\n\nTo represent the auditory modality, we retrieved the auditory features that X.  Yang et al. (2018)  describe as high-level features for all available songs from the DMDD via the Spotify API 5  , resulting in 13,445 songs with 11 auditory features each. Of these, the features acousticness, danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo and valence are continuous, while mode is binary, and key is categorical. We created dummy variables for key resulting in 23 auditory features overall. A detailed explanation of the features can be found in Appendix A. Spotify's valence annotation is derived differently from our ground-truth valence, avoiding circularity, and is also used as a predictive feature for emotion in  Panda et al. (2021) .\n\nLastly, for the textual modality, we scraped the song lyrics for the available songs from genius.com as suggested by  Li, Mou, and Chang (2018) , further reducing the dataset to a final number of 12,471 songs, of which we used ca. 60% for training, 20% for validation and 20% for testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Extracting Lyrics Features",
      "text": "To represent the lyrical information, we created three types of features.\n\n1. Sentiment information consisting of positive, negative, neutral and compound scores was obtained with Vader sentiment analysis  (Hutto & Gilbert, 2014) .\n\n2. TF-IDF features, which are widely used  (Han et al., 2022)  and aim to capture the general lexical features of the song texts relating to both valence and arousal. TF-IDF unigram counts were obtained after lemmatizing the lyrics texts and reduced to 100 principal components using PCA.\n\n3. Extended ANEW (XANEW) features aimed to capture the lexical basis of valence and arousal. This data set was introduced in 3.1. We generated two count vectors for each pre-processed lyric text and multiplied the counts by the respective VA scores. These count features were also reduced with PCA to 100 dimensions. These features were not included in the final models (see 3.2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Selection",
      "text": "Based on these features, we selected feature subsets for the two different modalities (see Figure  2 ). Firstly, we tested for significance in an MLR (similar to our baseline, see Section 3.3.1) to get the optimal combination among the auditory features resulting in the five significant predictors danceability, energy, instrumentalness, valence, and mode.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Models",
      "text": "Based on the literature presented in Section 2, we compare different regression models to evaluate the performance of uni-and multi-modal approaches for MER on high-level auditory features and lyrics. The regression learning algorithms used are all from the scikit-learn package  (Pedregosa et al., 2011) : LinearRegression, RandomForestRegression, SupportVectorRegression, MultilayerPerceptronRegressor, all trained on the optimal parameters we obtained over GridSearchCV.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline",
      "text": "We created a Multiple Linear Regression (MLR) baseline for both uni-and multi-modal prediction of VA scores. For the audio baseline, we firstly we used all auditory features as input. Secondly, we performed the MLR on only statistically significant (p < 0.05) coefficients. Then for the lyrics baseline, we performed the MLR on the selected lyrics features (sentiment and TF-IDF). Lastly, we combined the significant audio features and the selected lyrics features in MLR to form the multimodal baseline.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Post-Hoc Feature Analysis",
      "text": "Additionally, for investigating the set of most significant features overall, we applied Recursive Feature Elimination (RFE)  (Granitto et al., 2006)  using the sklearn.featureselection.RFE package  (Pedregosa et al., 2011)  on an MLR model. Furthermore, we created a MLR model with all audio features + compound sentiment scores included, for the purpose of analyzing coefficients (see Table  2 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "We evaluate the predictive power of different regression models in terms of R 2 as applied in the aforementioned literature (e.g.  Hu et al. (2008 Hu et al. ( , 2009)) ;  Kim et al. (2010) ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Results",
      "text": "Table  1  provides a summary of test results for our regression models with different modalities. The highest scores overall are achieved with multimodal features for both valence and arousal. This is more pronounced for valence: multimodality significantly improves R 2 compared to both audio-and lyrics-only features. Within the uni-modal models however, audio features are slightly better at predicting valence than lyrics features.\n\nThe arousal results are more complicated. Although the highest arousal score is achieved with multi-modal RFR, all the other regression algo- rithms perform better at arousal prediction with uni-modal (audio) features. This suggests that adding lyrics features may interfere and compound error for arousal, because we also see that the lyrics-only arousal scores are practically 0.\n\nNo single regression algorithm emerged as the best, although SVR consistently performed worst. This might be due to insufficient hyperparameter optimization, as testing a larger number of parameter combinations was very time-consuming for this type of model.\n\nSurprisingly, for lyrics-only valence and arousal as well as multi-modal valence, our more complex regression algorithms did not outperform the baseline MLR.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Analysis",
      "text": "Based on RFE, the most important features were valence, the positive and neutral sentiment score and several PCs from the TF-IDF analysis for both target variables (see Figure  2 ) as well as danceability for valence and energy and the negative sentiment score for arousal.\n\nConsidering the p-values of coefficients in MLR, valence has 7 significant predictors, where arousal has 6 as can be seen in Table  2 . We chose to illustrate these results using the MLP, as it has a consistent performance and takes little time to optimize and train. We see that using selected features gives a small but consistent boost to the R 2 for all modalities, so we trained our final models using these features. Overall, we can see from",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Selected Features Performance",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "Our main results can be briefly summarized in two notions. One, both uni-modal lyrics features and uni-modal audio features reasonably predict valence, although a multi-modal approach outperforms either modality individually. And two, predicting arousal is hard to do with lyrics features, since audio features alone perform almost as well as the multi-modal approach.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Effect Of Features",
      "text": "Valence. As seen in Table  2 , danceability, energy, valence (Spotify), mode, acousticness, liveness and compound sentiment positively contribute to the valence score, while the first three are by far the strongest predictors.\n\nArousal. The features energy, speechiness, mode, and valence positively contribute, while danceability and instrumentalness have negative coefficients. Energy is by far the strongest predictor.\n\nBoth of these findings are consistent with  Panda et al. (2021)  in regards to energy, valence and acousticness. However, unlike them, we found that danceability, instrumentalness, mode and speechiness are also relevant for MER. Especially the difference in findings about the importance of danceability is interesting. Where in our baseline model danceablility has one of the highest coefficients,  Panda et al. (2021)  found that danceability has the lowest feature weight.\n\nDanceability captures tempo, rhythm and beat aspects of a song, where perceptual features such as perceived loudness and timbre contribute to energy according to Spotify's descriptions (see appendix A). Danceability being a strong predictor of valence supports previous findings about the importance of rhythmic properties for predicting valence (Y.-H.  Yang, Lin, Su, & Chen, 2008; Vatolkin & Nagathil, 2019) . Our finding that energy is a strong predictor of arousal indirectly supports research stating timbre is related to arousal  (Gabrielsson & Lindström, 2001; Y.-H. Yang, Lin, Su, & Chen, 2008) . As energy also captures perceived loudness, the association between loudness and arousal  (Gabrielsson & Lindström, 2001; Y.-H. Yang, Lin, Su, & Chen, 2008 ) is also supported by our results.\n\nThe finding that Spotify's valence is the strongest predictor of valence is an indication that it is indeed somehow measured by the same under-lying construct, despite being constructed in different ways. Furthermore, we find a positive coefficient of mode and predicted valence, which corresponds to traditional associations of major keys being related to positive emotions and minor keys to negative ones  (Gabrielsson & Lindström, 2001; Panda, Malheiro, & Paiva, 2020) .\n\nThe positive coefficient of sentiment score for predicting valence could be interpreted as that songs with more positive lyrics have higher valence scores and songs with more negative lyrics have more lower valence scores.\n\nSurprisingly, XANEW features were among the least predictive features based on RFE and VA validation scores, which is why they were removed from the final models. The ground-truth VA scores are based on tag analysis with XANEW  (Delbouys et al., 2018) , and studies Hu and Downie (2010a) yielded good results with ANEW features. The main difference between XANEW features and VA scores is that the former is based on tags from LastFM, where the latter is based on the lyrics. This suggests that words in lyrics of the songs substantially differ from the tags people give a song on LastFM. TF-IDF may have been more successful in finding distinguishing words because it takes into account the entire lyric vocabulary.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Multi-Modal Vs. Uni-Modal",
      "text": "Valence. Our results for modality are generally consistent with the literature ((Y.-H.  Yang, Lin, Cheng, et al., 2008; Hu & Downie, 2010a; Delbouys et al., 2018) ), where multi-modal MER models outperform uni-modal ones. However, in our case this is only unambiguously true for valence, where lyrics and audio features appear to have complementary effects when combined. The fact that audio seems to contribute more to valence (based on uni-modal scores) is consistent with  Ali and Peynircioglu (2006) ,  Delbouys et al. (2018)  and  Xue et al. (2015) , as well as anecdotal evidence  (Heinrich, 2019)  that people pay more attention to music than lyrics, which would influence the listener annotations that our target variables are based on. On the other hand, older studies like  Hu and Downie (2010a, 2010b)  do report that lyrics can outperform audio for valence prediction. However they make use of more extensive feature engineering and more fine-grained emotion categories (predicted in binary classification tasks), which limits comparability.\n\nArousal. With regards to arousal prediction, the multi-modal advantage is not clear. Only one of our regression models performs better multimodally, while the other three perform better with only audio features. Lyrics features seem to either not contribute or actively interfere with audio features to give lower multi-modal performance. This is consistent with  Delbouys et al. (2018) 's findings that arousal is mostly determined by audio features such as rhythm and tempo.  Malheiro et al. (2018)  succeed in classifying songs with regards to both arousal and valence exclusively from lyrics, but with a small dataset and very extensive feature engineering, especially including more stylistic and structural features, which we pre-processed away in our BoW TF-IDF approach. On the other hand,  Delbouys et al. (2018) , who do not report preprocessing lyrics but feed them directly into a variety of neural networks, achieve comparably low arousal scores for their lyrics-only models on the same dataset.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Summary And Future Directions",
      "text": "Lastly, we want to shed light on possible reasons for our model's performance as well as limitations and give an outlook for future research direction.\n\nEarly Feature Fusion. As stated in 3.2, we chose to combine our features at feature level, following  Hu and Downie (2010a, 2010b)  and  Panda et al. (2021) . However, Y.-H.  Yang, Lin, Cheng, et al. (2008)  states that a potential difficulty of early fusion is achieving a truly common representation. Especially the combination of radically different feature types like TF-IDF vectors and auditory features could be responsible for our results. An exploratory investigation of late-fusion approaches (see e.g. Y.-H.  Yang, Lin, Cheng, et al. (2008) ;  Delbouys et al. (2018)  for this dataset is left for future investigation. Another interesting direction for future research is investigating interaction effects of the features we used.\n\nDeep Learning as State-of-the-Art. Interestingly, we find that out results align very well with the findings of  Delbouys et al. (2018) 's comparision of classical feature engineering based approaches and DL models on the same dataset. We reach competetive scores on valence on the audio-only prediction with a MLP (0.176) compared to their convolutional network (0.179) while our multi-modal MLR (0.236) even outperforms their middle-fusion DL network (0.219) on va-lence. This suggests that using higher-level features makes our model more interpretable, while not compromising on performance.\n\nVague Annotation Standard. That we obtain similar results as  Delbouys et al. (2018)  indicates that part of the limitations may be related to the dataset itself. The fact that Spotify's valence doesn't correlate to the predicted valence very well (R 2 = 0.35) means it also may be hard to measure and evaluate accurately. The VA annotations obtained through LastFM tags in the  Delbouys et al. (2018)  were not checked or validated with human annotation, so their quality is unknown. Furthermore, the target VA variables assume that there is a single value for one whole song rather than displaying a possible variation of valence and arousal within the piece, while X.  Yang et al. (2018)  suggest that a song's sentiment can vary throughout the piece, which falls under the MER subdomain of music emotion variation detection (MEVD)  (Han et al., 2022) .\n\nAudio and Predictions. Even though we find some improvement to the overall scores when including the selected Spotify features, other approaches that directly source from the song's spectral features (e.g.  Hu et al. (2008) ;  Hu and Downie (2010a) ;  Soleymani et al. (2014) ), seem to achieve better results at least on valence scores of up to 0.42 already in 2014. This leads to the conclusion in alignment with  Panda et al. (2021)  that there is still some room for improvement and clarity for Spotify features and that valence and arousal are still rather hard to predict from high-level features.\n\nLyrics and Predictions. Already the first music mood classification task, MIREX (2007), found that MER depends highly on the ground-truth labeling and human judgement, including how much people pay attention to the lyrics. An example for this is \"Hey-Ya\" by OutKast, that sounds happy and upbeat (which corresponds to a high valence and arousal score in our dataset), whereas the artists themselves attribute a more negative meaning to it (Archive-Corey-Moss, 2004). Henceforth, the true, multi-modal valence of such songs is hard to determine, both for human judges as well as predictive models. Aggravating factors to this are the lyrics shortness and the use of rhetorical figures like metaphors, oxymorons, and irony that makes it even harder to capture the lyrics' true sentiment. For future direction we suggest a more advanced sentiment analysis.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "We have shown that we can combine audio and textual modalities to predict valence and arousal for a large dataset of songs, improving in particular valence prediction relative to uni-modal approaches. Predicting arousal from lyrics proved unsuccessful with the features we used and may require more extensive feature engineering or DL approaches. On average, audio-only models were better than multi-modal ones at predicting arousal based on auditory features such as energy, speechiness and mode.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Pipeline of our model. Scores indicate R2 on the test data.",
      "page": 3
    },
    {
      "caption": "Figure 2: Feature Selection and Analysis Process",
      "page": 4
    },
    {
      "caption": "Figure 2: ) as well as danceabil-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 3: that all sub-",
      "data": [
        {
          "Audio": "",
          "MLR": "RFR",
          "0.170": "0.171",
          "0.193": "0.204"
        },
        {
          "Audio": "",
          "MLR": "SVR",
          "0.170": "0.165",
          "0.193": "0.203"
        },
        {
          "Audio": "",
          "MLR": "MLP",
          "0.170": "0.176",
          "0.193": "0.203"
        },
        {
          "Audio": "Lyrics",
          "MLR": "MLR",
          "0.170": "0.139",
          "0.193": "0.029"
        },
        {
          "Audio": "",
          "MLR": "RFR",
          "0.170": "0.121",
          "0.193": "0.027"
        },
        {
          "Audio": "",
          "MLR": "SVR",
          "0.170": "0.042",
          "0.193": "-0.074"
        },
        {
          "Audio": "",
          "MLR": "MLP",
          "0.170": "0.117",
          "0.193": "0.020"
        },
        {
          "Audio": "Multi-modal",
          "MLR": "MLR",
          "0.170": "0.236",
          "0.193": "0.190"
        },
        {
          "Audio": "",
          "MLR": "RFR",
          "0.170": "0.224",
          "0.193": "0.207"
        },
        {
          "Audio": "",
          "MLR": "SVR",
          "0.170": "0.208",
          "0.193": "0.154"
        },
        {
          "Audio": "",
          "MLR": "MLP",
          "0.170": "0.235",
          "0.193": "0.196"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: that all sub-",
      "data": [
        {
          "Constant": "Danceability",
          "-1.6885*": "0.6915*",
          "-0.9836*": "-0.3266*"
        },
        {
          "Constant": "Energy",
          "-1.6885*": "0.6378*",
          "-0.9836*": "1.4254*"
        },
        {
          "Constant": "Loudness",
          "-1.6885*": "-0.0091",
          "-0.9836*": "-0.0073"
        },
        {
          "Constant": "Speechiness",
          "-1.6885*": "-0.1101",
          "-0.9836*": "0.3952*"
        },
        {
          "Constant": "Acousticness",
          "-1.6885*": "0.1649*",
          "-0.9836*": "0.0207"
        },
        {
          "Constant": "Instrumentalness",
          "-1.6885*": "0.0929",
          "-0.9836*": "-0.3278*"
        },
        {
          "Constant": "Liveness",
          "-1.6885*": "0.1916*",
          "-0.9836*": "0.0207"
        },
        {
          "Constant": "Valence",
          "-1.6885*": "1.0901*",
          "-0.9836*": "0.5158*"
        },
        {
          "Constant": "Tempo",
          "-1.6885*": "0.0005",
          "-0.9836*": "0.0004"
        },
        {
          "Constant": "Mode",
          "-1.6885*": "0.0977*",
          "-0.9836*": "0.1272*"
        },
        {
          "Constant": "Compound sentiment",
          "-1.6885*": "0.2275*",
          "-0.9836*": "-0.0051"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: that all sub-",
      "data": [
        {
          "Audio": "",
          "all\nfeaturesA": "selectedA",
          "0.163": "0.176",
          "0.193": "0.203"
        },
        {
          "Audio": "Lyrics",
          "all\nfeaturesA": "all\nfeaturesL",
          "0.163": "0.091",
          "0.193": "0.009"
        },
        {
          "Audio": "",
          "all\nfeaturesA": "selectedL",
          "0.163": "0.117",
          "0.193": "0.019"
        },
        {
          "Audio": "Multi",
          "all\nfeaturesA": "all\nfeaturesA +\nall\nfeaturesL",
          "0.163": "0.230",
          "0.193": "0.193"
        },
        {
          "Audio": "",
          "all\nfeaturesA": "+\nselectedA\nselectedL",
          "0.163": "0.235",
          "0.193": "0.196"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Acousticness": "Danceability",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "Describes how suitable a track is for dancing based on a combination of musical elements including\ntempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0\nis most danceable."
        },
        {
          "Acousticness": "Energy",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of\nintensity and activity.\nTypically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a\nBach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic\nrange, perceived loudness, timbre, onset rate, and general entropy."
        },
        {
          "Acousticness": "Instrumentalness",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "Predicts whether a track contains no vocals. ”Ooh” and ”aah” sounds are treated as instrumental in this\ncontext. Rap or spoken word tracks are clearly ”vocal”. The closer the instrumentalness value is to 1.0,\nthe greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent\ninstrumental tracks, but conﬁdence is higher as the value approaches 1.0."
        },
        {
          "Acousticness": "Key",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "The key the track is in.\nIntegers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 =\nC/Dx, 2 = D, and so on. If no key was detected, the value is -1. -1 <= x <= 11"
        },
        {
          "Acousticness": "Liveness",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "Detects the presence of an audience in the recording. Higher\nliveness values represent an increased\nprobability that\nthe track was performed live. A value above 0.8 provides strong likelihood that\nthe\ntrack is live."
        },
        {
          "Acousticness": "Loudness",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "The overall\nloudness of a track in decibels (dB). Loudness values are averaged across the entire track\nand are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the\nprimary psychological correlate of physical strength (amplitude). Values typically range between -60\nand 0 db."
        },
        {
          "Acousticness": "Mode",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "Mode indicates the modality (major or minor) of a track,\nthe type of scale from which its melodic\ncontent is derived. Major is represented by 1 and minor is 0."
        },
        {
          "Acousticness": "Speechiness",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "Speechiness detects the presence of spoken words in a track.\nThe more exclusively speech-like the\nrecording (e.g.\ntalk show, audio book, poetry),\nthe closer to 1.0 the attribute value. Values above 0.66\ndescribe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe\ntracks that may contain both music and speech, either in sections or layered, including such cases as rap\nmusic. Values below 0.33 most likely represent music and other non-speech-like tracks."
        },
        {
          "Acousticness": "Tempo",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the\nspeed or pace of a given piece and derives directly from the average beat duration."
        },
        {
          "Acousticness": "Valence",
          "A conﬁdence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high conﬁdence\nthe track is acoustic. 0 <= x <= 1": "A measure from 0.0 to 1.0 describes the musical positiveness conveyed by a track. Tracks with high\nvalence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more\nnegative (e.g. sad, depressed, angry). 0 <= x <= 1"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LinReg": "Valence\nArousal",
          "RF": "Valence\nArousal",
          "SVR": "Valence\nArousal",
          "MLP": "Valence\nArousal"
        },
        {
          "LinReg": "0.0705\n0.0188\n-0.0146\n-0.0138\n0.0478\n-0.0025\n-0.0028\n-0.0291\n0.0931\n0.0159\n0.0238\n-0.0187\n0.0304\n-0.0251",
          "RF": "0.0631\n0.0251\n0.0060\n-0.0091\n-0.0049\n-0.0765\n0.0427\n-7.0790e-06\n0.0842\n0.0267\n0.0536\n-0.0056\n0.0796\n0.0019",
          "SVR": "-0.0138\n-0.0099\n-0.0255\n-0.0336\n0.0076\n-0.0003\n0.0765\n-0.0056\n0.0044\n0.0817\n0.0284\n-0.0223\n0.0890\n0.0041",
          "MLP": "-0.8320\n-2.0411\n0.0121\n-0.0026\n0.0536\n0.0041\n0.0962\n0.0167\n0.0216\n0.0921\n0.0518\n-0.0038\n0.0911\n0.0089"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Songs and emotions: are lyrics and melodies equal partners?",
      "authors": [
        "S Ali",
        "Z Peynircioglu"
      ],
      "year": "2006",
      "venue": "Psychology of Music",
      "doi": "10.1177/0305735606067168"
    },
    {
      "citation_id": "2",
      "title": "Road To The Grammys: The Making Of Outkast's 'Hey Ya!",
      "year": "2004",
      "venue": "Road To The Grammys: The Making Of Outkast's 'Hey Ya!"
    },
    {
      "citation_id": "3",
      "title": "Dimensional music emotion recognition by valence-arousal regression",
      "authors": [
        "J Bai",
        "J Peng",
        "J Shi",
        "D Tang",
        "Y Wu",
        "J Li",
        "K Luo"
      ],
      "year": "2016",
      "venue": "2016 IEEE 15th International Conference on Cognitive Informatics Cognitive Computing (ICCI*CC)",
      "doi": "10.1109/ICCI-CC.2016.78620632"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "L.-L Balkwill",
        "W Thompson"
      ],
      "year": "1999",
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "A Cross-Cultural Investigation of the Perception of Emotion in Music: Psychophysical and Cultural Cues",
      "venue": "A Cross-Cultural Investigation of the Perception of Emotion in Music: Psychophysical and Cultural Cues",
      "doi": "10.2307/402858111"
    },
    {
      "citation_id": "6",
      "title": "Emotional foundations of music as a nonpharmacological pain management tool in modern medicine",
      "authors": [
        "G Bernatzky",
        "M Presch",
        "M Anderson",
        "J Panksepp"
      ],
      "year": "2011",
      "venue": "Neuroscience & Biobehavioral Reviews",
      "doi": "10.1016/j.neubiorev.2011.06.0051"
    },
    {
      "citation_id": "7",
      "title": "The Million Song Dataset",
      "authors": [
        "T Bertin-Mahieux",
        "D Ellis",
        "B Whitman",
        "P Lamere"
      ],
      "year": "2011",
      "venue": "The Million Song Dataset"
    },
    {
      "citation_id": "8",
      "title": "Automatic Music Mood Detection Using Transfer Learning and Multilayer Perceptron",
      "authors": [
        "B Bhattarai",
        "J Lee"
      ],
      "year": "2019",
      "venue": "INTERNATIONAL JOURNAL of FUZZY LOGIC and INTELLIGENT SYSTEMS",
      "doi": "10.5391/IJFIS.2019.19.2.882"
    },
    {
      "citation_id": "9",
      "title": "Affective Norms for English Words (ANEW): Instruction Manual and Affective Ratings",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1998",
      "venue": "Affective Norms for English Words (ANEW): Instruction Manual and Affective Ratings"
    },
    {
      "citation_id": "10",
      "title": "Music emotion recognition using deep Gaussian process",
      "authors": [
        "S.-H Chen",
        "Y.-S Lee",
        "W.-C Hsieh",
        "J.-C Wang"
      ],
      "year": "2015",
      "venue": "2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)",
      "doi": "10.1109/APSIPA.2015.74153211"
    },
    {
      "citation_id": "11",
      "title": "What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures",
      "authors": [
        "A Cowen",
        "X Fang",
        "D Sauter",
        "D Keltner"
      ],
      "year": "2020",
      "venue": "What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures",
      "doi": "10.1073/pnas.1910704117"
    },
    {
      "citation_id": "13",
      "title": "Music Mood Detection Based On Audio And Lyrics With Deep Neural Net",
      "authors": [
        "R Delbouys",
        "R Hennequin",
        "F Piccoli",
        "J Royo-Letelier",
        "M Moussallam"
      ],
      "year": "2018",
      "venue": "Music Mood Detection Based On Audio And Lyrics With Deep Neural Net",
      "arxiv": "arXiv:1809.07276"
    },
    {
      "citation_id": "14",
      "title": "Emotional States Associated with Music: Classification, Prediction of Changes, and Consideration in Recommendation",
      "authors": [
        "J Deng",
        "C Leung",
        "A Milani",
        "L Chen"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Interactive Intelligent Systems",
      "doi": "10.1145/2723575"
    },
    {
      "citation_id": "15",
      "title": "The influence of music on emotions and cravings in clients in addiction treatment: A study of two clinical samples. The Arts in Psychotherapy",
      "authors": [
        "G Dingle",
        "P Kelly",
        "L Flynn",
        "F Baker"
      ],
      "year": "2015",
      "venue": "The influence of music on emotions and cravings in clients in addiction treatment: A study of two clinical samples. The Arts in Psychotherapy",
      "doi": "10.1016/j.aip.2015.05.0051"
    },
    {
      "citation_id": "16",
      "title": "The Music Information Retrieval Evaluation eXchange (MIREX)",
      "authors": [
        "J Downie"
      ],
      "year": "2006",
      "venue": "The Music Information Retrieval Evaluation eXchange (MIREX)"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "D-Lib Magazine"
      ],
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "November)",
      "year": "2013",
      "venue": "The Echo Nest Blog"
    },
    {
      "citation_id": "20",
      "title": "Universal Recognition of Three Basic Emotions in Music",
      "authors": [
        "T Fritz",
        "S Jentschke",
        "N Gosselin",
        "D Sammler",
        "I Peretz",
        "R Turner",
        ". Koelsch"
      ],
      "year": "2009",
      "venue": "Current Biology",
      "doi": "10.1016/j.cub.2009.02.0581"
    },
    {
      "citation_id": "21",
      "title": "The influence of musical structure on emotional expression",
      "authors": [
        "A Gabrielsson",
        "E Lindström"
      ],
      "year": "2001",
      "venue": "Music and emotion: Theory and research"
    },
    {
      "citation_id": "22",
      "title": "Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products",
      "authors": [
        "P Granitto",
        "C Furlanello",
        "F Biasioli",
        "F Gasperi"
      ],
      "year": "2006",
      "venue": "Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products"
    },
    {
      "citation_id": "23",
      "title": "Music Emotion Regression based on Multi-modal Features",
      "authors": [
        "D Guan",
        "X Chen",
        "D Yang"
      ],
      "year": "2012",
      "venue": "Proc. International Symposium on Computer Music Modeling and Retrieval"
    },
    {
      "citation_id": "24",
      "title": "A survey of music emotion recognition",
      "authors": [
        "D Han",
        "Y Kong",
        "J Han",
        "G Wang"
      ],
      "year": "2022",
      "venue": "Frontiers of Computer Science",
      "doi": "10.1007/s11704-021-0569-4"
    },
    {
      "citation_id": "25",
      "title": "22 \"Fun\" Songs That Actually Have Some Pretty Dark Lyrics",
      "authors": [
        "S Heinrich"
      ],
      "year": "2019",
      "venue": "22 \"Fun\" Songs That Actually Have Some Pretty Dark Lyrics"
    },
    {
      "citation_id": "26",
      "title": "A framework for evaluating multimodal music mood classification",
      "authors": [
        "X Hu",
        "K Choi",
        "J Downie"
      ],
      "year": "2017",
      "venue": "Journal of the Association for Information Science and Technology",
      "doi": "10.1002/asi.236492"
    },
    {
      "citation_id": "27",
      "title": "Improving mood classification in music digital libraries by combining lyrics and audio",
      "authors": [
        "X Hu",
        "J Downie"
      ],
      "year": "2007",
      "venue": "Proceedings of the 10th annual joint conference on Digital libraries",
      "doi": "10.1145/1816123.1816146"
    },
    {
      "citation_id": "28",
      "title": "When Lyrics Outperform Audio for Music Mood Classification: A Feature Analysis",
      "authors": [
        "X Hu",
        "J Downie"
      ],
      "year": "2007",
      "venue": "ISMIR"
    },
    {
      "citation_id": "29",
      "title": "Lyric Text Mining in Music Mood Classification",
      "authors": [
        "X Hu",
        "J Downie",
        "A Ehmann"
      ],
      "year": "2009",
      "venue": "Lyric Text Mining in Music Mood Classification"
    },
    {
      "citation_id": "30",
      "title": "The 2007 MIREX Audio Mood Classification Task: Lessons Learned",
      "authors": [
        "X Hu",
        "J Downie",
        "C Laurier",
        "M Bay",
        "A Ehmann"
      ],
      "year": "2008",
      "venue": "The 2007 MIREX Audio Mood Classification Task: Lessons Learned"
    },
    {
      "citation_id": "31",
      "title": "Bi-Modal Deep Boltzmann Machine Based Musical Emotion Classification",
      "authors": [
        "M Huang",
        "W Rong",
        "T Arjannikov",
        "N Jiang",
        "Z Xiong"
      ],
      "year": "2016",
      "venue": "Artificial Neural Networks and Machine Learning -ICANN 2016",
      "doi": "10.1007/978-3-319-44781-0242"
    },
    {
      "citation_id": "32",
      "title": "Uncovering How the Spotify Algorithm Works. Retrieved",
      "authors": [
        "M Hucker"
      ],
      "year": "2021",
      "venue": "Uncovering How the Spotify Algorithm Works. Retrieved"
    },
    {
      "citation_id": "33",
      "title": "Music and Emotion",
      "authors": [
        "P Hunter",
        "E Schellenberg"
      ],
      "year": "2010",
      "venue": "Music Perception",
      "doi": "10.1007/978-1-4419-6114-3_5"
    },
    {
      "citation_id": "34",
      "title": "VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text",
      "authors": [
        "C Hutto",
        "E Gilbert"
      ],
      "year": "2014",
      "venue": "Proceedings of the International AAAI Conference on Web and Social Media"
    },
    {
      "citation_id": "35",
      "title": "Music Emotion Recognition via End-to-End Multimodal Neural Networks",
      "authors": [
        "B Jeon",
        "C Kim",
        "A Kim",
        "D Kim",
        "J Park",
        "J.-W Ha"
      ],
      "year": "2017",
      "venue": "Music Emotion Recognition via End-to-End Multimodal Neural Networks"
    },
    {
      "citation_id": "36",
      "title": "Music Emotion Recognition with the Extraction of Audio Features Using Machine Learning Approaches",
      "authors": [
        "J Juthi",
        "A Gomes",
        "T Bhuiyan",
        "I Mahmud"
      ],
      "year": "2020",
      "venue": "Proceedings of ICETIT 2019",
      "doi": "10.1007/978-3-030-30577-2272"
    },
    {
      "citation_id": "37",
      "title": "Music Emotion Recognition: A State Of The Art Review",
      "authors": [
        "Y Kim",
        "E Schmidt",
        "R Migneco",
        "B Morton",
        "P Richardson",
        "J Scott",
        ". Turnbull"
      ],
      "year": "2010",
      "venue": "Music Emotion Recognition: A State Of The Art Review"
    },
    {
      "citation_id": "38",
      "title": "Multimodal Music Mood Classification Using Audio and Lyrics",
      "authors": [
        "C Laurier",
        "J Grivolla",
        "P Herrera"
      ],
      "year": "2008",
      "venue": "Seventh International Conference on Machine Learning and Applications",
      "doi": "10.1109/ICMLA.2008.962"
    },
    {
      "citation_id": "39",
      "title": "Prediction of Genres and Emotions by Song Lyrics. Deep Learning",
      "authors": [
        "S Li",
        "C Mou",
        "C Chang"
      ],
      "year": "2018",
      "venue": "Prediction of Genres and Emotions by Song Lyrics. Deep Learning"
    },
    {
      "citation_id": "40",
      "title": "Emotionally-Relevant Features for Classification and Regression of Music Lyrics",
      "authors": [
        "R Malheiro",
        "R Panda",
        "P Gomes",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "Conference Name: IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.25985697"
    },
    {
      "citation_id": "41",
      "title": "2007:Main Page -MIREX Wiki",
      "authors": [
        "Mirex"
      ],
      "year": "2007",
      "venue": "2007:Main Page -MIREX Wiki"
    },
    {
      "citation_id": "42",
      "title": "Audio Features for Music Emotion Recognition: a Survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "Conference Name: IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.30323736"
    },
    {
      "citation_id": "43",
      "title": "How Does the Spotify API Compare to the Music Emotion Recognition Stateof-the-Art?",
      "authors": [
        "R Panda",
        "H Redinho",
        "C Gonc ¸alves",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2021",
      "venue": "Proceedings of the 18th Sound and Music Computing Conference",
      "doi": "10.5281/zenodo.5045100"
    },
    {
      "citation_id": "44",
      "title": "A Music Recommendation Method with Emotion Recognition Using Ranked Attributes",
      "authors": [
        "S.-H Park",
        "S.-Y Ihm",
        "W.-I Jang",
        "A Nasridinov",
        "Y.-H Park"
      ],
      "year": "2015",
      "venue": "Computer Science and its Applications",
      "doi": "10.1007/978-3-662-45402-21511"
    },
    {
      "citation_id": "45",
      "title": "Scikitlearn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        ". Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "46",
      "title": "Spotify Stats 2022 -(Facts, Data, Infographics",
      "authors": [
        "D Ruby"
      ],
      "year": "2022",
      "venue": "Spotify Stats 2022 -(Facts, Data, Infographics"
    },
    {
      "citation_id": "47",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h00777141"
    },
    {
      "citation_id": "48",
      "title": "Emotional Analysis of Music: A Comparison of Methods",
      "authors": [
        "M Soleymani",
        "A Aljanaki",
        "Y.-H Yang",
        "M Caro",
        "F Eyben",
        "K Markov",
        ". Wiering"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia",
      "doi": "10.1145/2647868.2655019"
    },
    {
      "citation_id": "49",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2016.74726692"
    },
    {
      "citation_id": "50",
      "title": "Evaluation of Audio Feature Groups for the Prediction of Arousal and Valence in Music",
      "authors": [
        "I Vatolkin",
        "A Nagathil"
      ],
      "year": "2019",
      "venue": "Applications in Statistical Computing: From Music Data Analysis to Industrial Quality Improvement",
      "doi": "10.1007/978-3-030-25147-5_19"
    },
    {
      "citation_id": "51",
      "title": "Music Emotion Classification of Chinese Songs Based on Lyrics Using Tf*idf and Rhyme",
      "authors": [
        "X Wang",
        "X Chen",
        "D Yang",
        "Y Wu"
      ],
      "year": "2011",
      "venue": "Music Emotion Classification of Chinese Songs Based on Lyrics Using Tf*idf and Rhyme"
    },
    {
      "citation_id": "52",
      "title": "Norms of valence, arousal, and dominance for 13,915 English lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-012-0314-x"
    },
    {
      "citation_id": "53",
      "title": "Multimodal Music Mood Classification by Fusion of Audio and Lyrics",
      "authors": [
        "H Xue",
        "L Xue",
        "F Su"
      ],
      "year": "2015",
      "venue": "MultiMedia Modeling",
      "doi": "10.1007/978-3-319-14442-936"
    },
    {
      "citation_id": "54",
      "title": "Disambiguating Music Emotion Using Software Agents",
      "authors": [
        "D Yang",
        "W.-S Lee"
      ],
      "year": "2004",
      "venue": "Disambiguating Music Emotion Using Software Agents"
    },
    {
      "citation_id": "55",
      "title": "Review of data features-based music emotion recognition methods",
      "authors": [
        "X Yang",
        "Y Dong",
        "J Li"
      ],
      "year": "2018",
      "venue": "Multimedia Systems",
      "doi": "10.1007/s00530-017-0559-4"
    },
    {
      "citation_id": "57",
      "title": "Machine Recognition of Music Emotion: A Review",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": "10.1145/2168752.2168754"
    },
    {
      "citation_id": "59",
      "title": "Toward Multi-modal Music Emotion Classification",
      "authors": [
        "Y.-H Yang",
        "Y.-C Lin",
        "H.-T Cheng",
        "I.-B Liao",
        "Y.-C Ho",
        "H Chen"
      ],
      "year": "2008",
      "venue": "Advances in Multimedia Information Processing -PCM 2008",
      "doi": "10.1007/978-3-540-89796-581"
    },
    {
      "citation_id": "60",
      "title": "A Regression Approach to Music Emotion Recognition",
      "authors": [
        "Y.-H Yang",
        "Y.-C Lin",
        "Y.-F Su",
        "H Chen"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASL.2007.911513"
    }
  ]
}