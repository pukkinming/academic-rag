{
  "paper_id": "2407.04051v3",
  "title": "Funaudiollm: Voice Understanding And Generation Foundation Models For Natural Interaction Between Humans And Llms",
  "published": "2024-07-04T16:49:02Z",
  "authors": [
    "Keyu An",
    "Qian Chen",
    "Chong Deng",
    "Zhihao Du",
    "Changfeng Gao",
    "Zhifu Gao",
    "Yue Gu",
    "Ting He",
    "Hangrui Hu",
    "Kai Hu",
    "Shengpeng Ji",
    "Yabin Li",
    "Zerui Li",
    "Heng Lu",
    "Haoneng Luo",
    "Xiang Lv",
    "Bin Ma",
    "Ziyang Ma",
    "Chongjia Ni",
    "Changhe Song",
    "Jiaqi Shi",
    "Xian Shi",
    "Hao Wang",
    "Wen Wang",
    "Yuxuan Wang",
    "Zhangyu Xiao",
    "Zhijie Yan",
    "Yexin Yang",
    "Bin Zhang",
    "Qinglin Zhang",
    "Shiliang Zhang",
    "Nan Zhao",
    "Siqi Zheng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports highprecision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, the advancement in artificial intelligence (AI) has dramatically transformed how humans interact with machines, such as  GPT-4o (OpenAI, 2023)  and Gemini-1.5  (Reid et al., 2024)  and so on  (Bai et al., 2023b; Chu et al., 2023) . This transformation is particularly evident in the realm of voice processing, where capabilities such as high-precision speech recognition  (Radford et al., 2023) , emotion recognition  (Ma et al., 2024b) , and voice generation  (Wang et al., 2023a; Du et al., 2024a)  are paving the way for more intuitive and human-like interactions. In this report, we introduce FunAudioLLM, an innovative framework designed to facilitate natural voice interactions between humans and large language models (LLMs)  (Team, 2023; Bai et al., 2023a; Touvron et al., 2023) . At the core of FunAudioLLM are our two groundbreaking models: SenseVoice, for voice understanding, and CosyVoice, for voice generation.\n\nSenseVoice is our state-of-the-art voice understanding model, which excels in multiple domains of voice processing. We offer both SenseVoice-Small and SenseVoice-Large variants. We have open-sourced SenseVoice-Small, which supports multilingual recognition in Chinese, English, Cantonese, Japanese, and Korean, delivering extremely low inference latency by employing a nonautoregressive end-to-end architecture. This design choice results in a performance that is more than 5 times faster than Whisper-small and more than 15 times faster than Whisper-large  (Radford et al., 2023) . On the other hand, SenseVoice-Large supports speech recognition in over 50 lan-Figure  1 : An overview of our FunAudioLLM models for voice understanding and generation.\n\nguages, with significant advantages in recognizing Chinese and Cantonese. In addition to speech recognition, SenseVoice offers state-of-the-art capabilities in emotion recognition and audio event detection  (Mesaros et al., 2021) , making it an ideal choice for creating low-latency, human-like voice interaction systems.\n\nOur suite of applications is further enriched by CosyVoice  (Du et al., 2024a) , a family of fundamental speech generation models designed to produce natural-sounding voices for a variety of contexts. CosyVoice excels in generating multi-lingual voices tailored to specific speakers, zero-shot adaptation to new speakers  (Wang et al., 2023a) , cross-lingual voice cloning  (Zhang et al., 2023) , creating emotionally resonant voices  (Shin et al., 2022) , and offering nuanced control over speech output through instructional text  (Ji et al., 2024) . CosyVoice supports five languages: Chinese, English, Japanese, Cantonese, and Korean. CosyVoice comes in three open-source models: CosyVoice-base-300M, which specializes in accurately representing speaker identity, zero-shot learning, and crosslingual voice cloning; CosyVoice-instruct-300M, which focuses on generating emotionally expressive voices and allows for meticulous adjustments via instructional text, extending its capabilities to controllability over various aspects such as speaker identity  (Shimizu et al., 2023) , speaking style  (Ji et al., 2024) , and fine-grained paralinguistic features  (Kanda et al., 2024) ; and CosyVoice-sft-300M, which has been fine-tuned on seven multilingual speakers and is ready for immediate deployment.\n\nBy integrating SenseVoice, CosyVoice, and LLMs like Qwen  (Team, 2023) , FunAudioLLM offers a range of rich application demos. These include Speech-to-Speech Translation  (Berard et al., 2018) , which allows users to speak in foreign languages using their own voice; Emotional Voice Chat  (Xue et al., 2024) , which enables the model to understand and respond to emotions for more human-like interactions; Interactive Podcast  (Laban et al., 2022) , wherein users can engage in live discussions with multiple large models; and AudioBook  (Chalamandaris et al., 2014) , allowing the model to perform expressive, multi-character narration for audiobooks.\n\nOverall, FunAudioLLM leverages the strengths of SenseVoice and CosyVoice to push the boundaries of voice interaction technology, enabling more natural and seamless communication between humans and large language models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Funaudiollm Models",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Overview Of Funaudiollm",
      "text": "FunAudioLLM consists of two foundation models for voice understanding and generation, named SenseVoice and CosyVoice, respectively. SenseVoice supports multi-lingual speech recognition, which is trained on over 300k hours. Specifically, SenseVoice-Small is efficient in inference, in which the recognition latency is less than 80ms and is more than 5 and 15 times faster than Whisper-Small and Whisper-large, respectively, and SenseVoice-Large supports high-precision ASR for over 50 languages. Furthermore, SenseVoice supports rich transcription, including state-of-the-art emotion recognition, audio event detection, inverse text normalization  (Pusateri et al., 2017)  and punctuation  (Chen et al., 2020) .\n\nOur voice generation model, CosyVoice, can generate multi-lingual speeches, which is trained on over 170k hours and five languages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO). CosyVoice generated samples can achieve a WER of less 2% and speaker similarity of over 75%, which achieves the quality level of human parity. CosyVoice supports zeroshot in-context learning, which enables voice cloning with a prompt speech of even 3 seconds. The timbre, emotion, prosody and style can be reproduced within or cross languages. We also released an instruction model, which can control speaker identity, speaking style (e.g., emotion) and other finegrained paralinguistic features with natural textural instructions. An overview of FunAudioLLM models is shown in Figure  1 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Voice Understanding Model: Sensevoice",
      "text": "SenseVoice is a speech foundation model with multiple voice understanding capabilities, including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and audio event classification (AEC) or audio event detection (AED). Two models with different sizes and architectures are proposed to suit different requirements: SenseVoice-Small, an encoder-only speech foundation model for rapid speech understanding, and SenseVoice-Large, an encoder-decoder  (Vaswani et al., 2017)  speech foundation model for more accurate speech understanding with more languages supported, as illustrated in Figure  2 .\n\nSenseVoice-Small is a non-autoregressive encoder-only model for multi-lingual multi-style ASR and multiple speech understanding tasks. Given the input waveform, we first compute the 80dimensional log-mel filter-bank, and then stack consecutive frames and down-sample them by a factor of 6. The extracted feature is then mapped to the dimension D of the encoder, denoted as X speech ∈ R T ×D , where T is the length of the down-sampled feature. The encoder is implemented as a memory-equipped self-attention network (SAN-M)  (Gao et al., 2020) . To specify the task, we prepend four embeddings to the speech feature as the input to the encoder: X = concat(e LID , e SER , e AEC , e ITN/NoITN , X speech )\n\n(1)\n\nX ∈ R (T +4)×D and P ∈ R (T +4)×|V ′ | . V ′ is the vocabulary including tokens for ASR and other tasks. e LID , e SER , e AEC , e ITN/NoITN are embeddings of four special tokens:\n\n⟨LID⟩ indicates the LID task. If ⟨LID⟩ is prepended, the model is trained to predict the language token, at the corresponding position of the output. In the training stage, we randomly replace ⟨LID⟩ with the ground truth language token according to probability 0.8 so that the model can either predict the language token, or be configured with a specified language token in the inference stage.\n\n⟨SER⟩ indicates the SER task. If ⟨SER⟩ is prepended, the model is trained to predict the speech emotion label, at the corresponding position of the output.\n\n⟨AEC⟩ indicates the AEC task. If ⟨AEC⟩ is prepended, the model is trained to predict the audio event label, at the corresponding position of the output.\n\n⟨ITN⟩ or ⟨NoITN⟩ specify the transcription style. If ⟨ITN⟩ is provided, the model is trained to transcript with inverse text normalization (ITN) and punctuation. If ⟨NoITN⟩ is provided, the model is trained to transcript without ITN and punctuation.\n\nIn the training stage, the LID, SER, and AEC tasks are optimized using the cross-entropy loss. The ASR task is optimized using the CTC loss  (Graves et al., 2006) .\n\nSenseVoice-Large is an autoregressive encoder-decoder model for multi-lingual ASR and multiple speech understanding tasks. Similar to Whisper  (Radford et al., 2023) , SenseVoice-Large specifies tasks by a sequence of input tokens to the decoder. Specifically, we specify whether to predict language, speech emotion, and audio events with timestamps by including ⟨LID⟩, ⟨SER⟩, ⟨AED⟩ tokens respectively. Compared to SenseVoice-Small, the advantage of SenseVoice-Large is the transcription accuracy and supporting for a vast number of languages (50+).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Semantic Speech Tokenizer",
      "text": "A speech tokenizer transforms vocal signals into discrete tokens, enabling their modeling and prediction by autoregressive transformers for speech generation. Our preliminary experiments indicated that the choice of speech tokenizer is pivotal for overall system performance as well as the requirements of both data quality and volume. We evaluated three classes of speech tokenizers: 1) those based on residual quantization like SoundStream  (Zeghidour et al., 2022) , Encodec  (Défossez et al., 2022)  and FunCodec  (Du et al., 2024b) ; 2) those utilizing multi-grouped quantization, such as Hifi-Codec  (Yang et al., 2023) ; and 3) \"semantic\" speech tokens, specifically HuBERT  (Hsu et al., 2021) .\n\nAll the above tokenizers are trained in the unsupervised or self-supervised manners. Thus, their association to semantic content is often tenuous, contributing to an unstable synthesis process and a substantial demand for clean training data. Moreover, unsupervised tokenizers are susceptible to data noise, necessitating meticulously curated clean data sets.\n\nBuilding on the success of SenseVoice models, we introduce a supervised semantic speech tokenizer, denoted as S 3  (Du et al., 2024a) . Using the pre-trained SenseVoice-Large model as a foundation, we incorporate a vector quantizer subsequent to the encoder's initial six layers, delineated in Figure  3 . Importantly, the integration of an additional positional embedding post-quantization enhances temporal information. The combination of Encoder 1 and vector quantizer is considered as the speech tokenizer, employing the index of the closest code vector as speech tokens. The vector quantizer utilizes a solitary codebook with an expansive dictionary containing 4,096 entries. The derived token sequence exhibits a frequency of 50 Hz, thereby reducing the computational load on text-to-token generation within language models.\n\nSince the speech tokenizer is trained to minimize the recognition errors of rich text in an end-to-end manner, the extracted tokens have a strong semantic relationship to textual and paralinguistic information. Furthermore, our S 3 tokenizer benefits from supervised training, enhancing its robustness to data noise and reducing the reliance on pristine data collection. Consequently, a broader spectrum of data can be utilized for training the model.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Voice Generation Model: Cosyvoice",
      "text": "CosyVoice, a family of fundamental speech generation models  (Du et al., 2024a) , utilizes S 3 tokens to synthesize natural-sounding voices suitable for various applications. As a versatile model, CosyVoice excels in tasks such as generating multi-lingual voices tailored to specific speakers, adapting to new speakers without training (zero-shot in-context learning), replicating voices across different languages (cross-lingual voice cloning), creating emotionally resonant voices, and offering nuanced influence over speech output through instructional text. CosyVoice supports five languages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO).\n\nWe released three open-source models. The first, CosyVoice-base-300M, excels in accurately representing speaker identity, adapting to contexts without any finetuning, and cloning voices across languages. The second, CosyVoice-instruct-300M, is adept in generating emotionally expressive voices and allows for meticulous adjustments via instructional text. Lastly, CosyVoice-sft-300M has been fine-tuned on seven multi-lingual speakers and is ready for immediate deployment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "System Overview",
      "text": "CosyVoice incorporates an autoregressive Transformer-based language model (LM) to generate speech tokens for the input text. An ordinary differential equation based (ODE-based) diffusion model, flow matching  (Lipman et al., 2023) , reconstructs Mel spectrum from the generated tokens. Subsequently, a HiFTNet-based vocoder (Li et al., 2023) is followed to synthesize waveforms from the reconstructed Mel spectrum. Dashed models are optional for certain applications, such as crosslingual cloning and speaker fine-tuned inference.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Training",
      "text": "At the training stage, the autoregressive language model (LM) is trained using a teacher-forcing paradigm. In this process, tokenized text and a left-shifted version of the speech tokens are provided as input to predict the subsequent speech tokens.\n\nThe flow matching model is developed to estimate the conditional probabilities P (S|X, v, S ref ), where X and v denote the speech tokens and speaker embeddings  (Wang et al., 2023b) , respectively. S and S ref represent the Mel spectrum of target and reference speech, respectively. A convolutional Transformer U-Net  (Mehta et al., 2023)  is employed to ascertain the vector field between the prior distribution and the desired one, which is derived from the optimal transport ODE. The straightforward nature of resolving the OT-ODE allows for a significantly reduced number of iterations during the inference stage, typically only five to ten iterations are required to produce a satisfactory Mel spectrogram. We also employ the classifier-free guidance (CFG)  (Ho & Salimans, 2022)  technique and mask out the 70%∼100% proceeding feature conditions to boost the in-context learning ability.\n\nFor the synthesis of waveforms from the predicted Mel spectrograms, we utilize a vocoder based on HiFTNet  (Li et al., 2023) . Modifications have been made on HiFTNet to support streaming generation, including the replacement and redesign of certain components. Complete details regarding these adjustments are available in our released code.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Zero-Shot In-Context Learning",
      "text": "CosyVoice models exhibit zero-shot in-context learning capabilities, allowing for the replication of an arbitrary voice with only a brief reference speech sample. This process entails the careful construction of input sequences for the token language model (LM), depicted in Figure  5 . For prompt speech and input text in the same language, we merge them to form a unified input, treating the prompt speech tokens as pre-generated. With this input sequence, the autoregressive LM iteratively predicts subsequent tokens until it encounters the \"end of sequence\" token E . However, when the prompt speech and input text differ linguistically, we omit the text and tokens associated with the prompt to prevent prosodic characteristics of the original language from influencing the target language. It is important to note that the prompt text, which corresponds to the prompt speech's content, can be transcribed either through human annotation or ASR models, such as SenseVoice. Similar to the prompt text, the prompt tokens are extracted from the prompt speech with S 3 tokenizer.\n\nAfter generating the speech tokens, they are appended after the prompt tokens, forming a composite condition for the flow-matching model. Additionally, the speaker embedding and the Mel spectrogram of the prompt speech are incorporated to further enhance timbre and environmental consistency.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Instruction Fine-Tuning",
      "text": "To enable further controllability on CosyVoice, we experiment with integrating additional instruction fine-tuning  (Ji et al., 2023) . CosyVoice-instruct extends CosyVoice-base with enhanced instructionfollowing capabilities. Specifically, it supports controllability over various aspects such as speaker identity (i.e., speaker's characteristics), speaking style (including emotion, gender, speaking rate, and pitch), and fine-grained paralinguistic features. These features include the ability to insert laughter, breaths, speaking while laughing, and emphasizing certain words. Table  3  shows some examples of speaker identity, speaking style, and fine-grained paralinguistic features.\n\nSpeaker Identity 1. Selene 'Moonshade', is a mysterious, elegant dancer with a connection to the night. Her movements are both mesmerizing and deadly.<endofprompt>Hope is a good thing. 2. Theo 'Crimson', is a fiery, passionate rebel leader. Fights with fervor for justice, but struggles with impulsiveness.<endofprompt>You don't know about real loss.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Speaking Style",
      "text": "1. A happy girl with high tone and quick speech.<endofprompt>The sun is shining brightly today. 2. A sad woman with normal tone and slow speaking speed.<endofprompt>I failed my important exam.\n\nFine-grained Paralinguistics 1. Well that's kind of scary [laughter]. 2. I don't think I over eat yeah [breath] and um I do exercise regularly. 3. Well that pretty much covers <laughter>the subject</laughter> well thanks for calling me. 4. The team's <strong>unity</strong> and <strong>resilience</strong> helped them win the championship.\n\nTable  3 : Examples of speaker identity, speaking style, and fine-grained paralinguistics.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Set For Sensevoice",
      "text": "Figure  6  provides an overview of the dataset utilized for training the SenseVoice models. The SenseVoice-Small model was trained on an extensive audio data corpus of approximately 300,000 hours, covering 5 languages including Chinese, Cantonese, English, Japanese, and Korean. To further enhance the multilingual ability of SenseVoice-Large, an additional 100,000 hours of diverse multilingual data were integrated into the training corpus. To obtain rich transcription labels from speech data, we leveraged open-source models for audio event detection (AED) 1  and speech emo- tion recognition (SER) 2  to generate pseudo labels, yielding an extensive rich transcribe dataset. Specifically, the AED data amounted to 150 million entries, while the SER data comprised 30 million entries.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Language",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training Set For Cosyvoice",
      "text": "To train the CosyVoice models, we have amassed a considerable dataset comprising multiple languages. Throughout the collection process, we utilize specialized in-house tools for speech detection, signal-to-noise ratio (SNR) estimation, speaker diarization, and separation. Subsequently, pseudo text labels are generated using SenseVoice-Large and Paraformer. These labels undergo a refinement process with the aid of force-alignment (FA) models, which helps eliminate low-quality data and enhances the accuracy of punctuation. A comprehensive breakdown of the training data's duration across various languages is presented in Table  4 .\n\nFor the CosyVoice-instruct model, we fine-tuned CosyVoice-base using instruction training data without incorporating speaker embedding in the autoregressive language model. Table  5    Results in Table  6  show the comparison of Whisper, SenseVoice and Paraformer  (Gao et al., 2022 (Gao et al., , 2023;; Shi et al., 2024)  on popular open speech recognition benchmark datasets, including AISHELL-1  (Bu et al., 2017) , AISHELL-2  (Du et al., 2018) , WenetSpeech  (Zhang et al., 2022) , Librispeech  (Panayotov et al., 2015) , and Common Voice  (Ardila et al., 2019) . It can be seen that SenseVoice-S and SenseVoice-L outperform their Whisper counterparts by a significant margin in most test sets except Librispeech.\n\nFigure  7  illustrates the comparative performance of SenseVoice-Large and Whisper-Large-V3 on a broader range of languages, with or without ground truth LID as input. While SenseVoice-Large performs comparably with Whisper-Large-V3 in general, SenseVoice-Large obtains significantly better performance in languages like Cantonese (Yue), Catalan (CA), and Marathi (MR).\n\nThe evaluation of inference efficiency is shown in Table  7 . The Real-time factor (RTF, the ratio of the transcribing time to the audio length) and 10s Audio Latency (the average time cost when transcribing a 10s audio.) are benchmarked on an A800 machine, with a decoding batch size of 1. For the encoder-decoder-based model (Whipser-S, Whipser-L-V3, and SenseVoice-L), we perform beam search in decoding with a beam size of 5. Owing to its non-autoregressive architecture, SenseVoice-S obtains extremely low inference latency-more than 5 times faster compared to Whisper-small and more than 15 times faster compared to Whisper-L-V3. SenseVoice-L shows close performance with Whipser-L-V3.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "We evaluate the SER ability of the SenseVoice on 7 popular emotion recognition datasets, including CREMA-D  (Cao et al., 2014) , MELD  (Poria et al., 2019) , IEMOCAP  (Busso et al., 2008)  Podcast  (Martinez-Lucas et al., 2020) , CASIA  (Zhang & Jia, 2008) , MER2023  (Lian et al., 2023)  and ESD  (Zhou et al., 2021) . These corpora cover both Chinese and English, and scenarios like acts, TV dramas, and daily conversation. We report unweighted average accuracy (UA), weighted average accuracy (WA), macro F1 Score (F1), and weighted average F1 (WF1), and compare them with some recently published SER benchmarks (EmoBox  (Ma et al., 2024a) , Emo-Superb  (Wu et al., 2024)  and MerBench  (Lian et al., 2024) ) from literature in Table  8 . We show that SenseVoice achieves a good performance on all test sets and all metrics even without fine-tuning on the target domain.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Audio Event Detection",
      "text": "Both SenseVoice-Small and SenseVoice-Large models can classify the audio event in the speech, including music, applause, and laughter. The SenseVoice-L can further predict the start and end position of the audio event, while the SenseVoice-Small can only predict what happened in the audio, with at most one event per utterance. SenseVoice-Small can detect more kinds of events, such as coughing, sneezing, breathing, and crying which could occur during human-machine interaction.\n\nESC-50 Baby Cry Detection Coswara Inhome Talkshow Figure  9 : F1(%) Score comparison of the SenseVoice with the audio event detection models BEATS and PANNs on different audio event detection tasks.\n\nWe compare SenseVoice with the SOTA audio event detection models BEATs  (Chen et al., 2023a)  and PANNs  (Kong et al., 2020)  on different tasks, including environment sound classification (ESC-50)(Piczak, 2015), baby cry/laugh detection 5  , coughing detection (Coswara)  (Sharma et al., 2020)   6  and in-home talkshow event detection. As SenseVoice only predicts the event of our interest, which may not include event categories in other models, we use the F1 score on each event for evaluation. Qwen-audio is also evaluated for comparison.\n\nWe find that SenseVoice serves as a good audio event classification or detection model, though BEATs and PANNs may have better F1 scores, which may be attributed to two reasons. Firstly, BE-TAS and PANNs can modify the detection threshold to trade-off the accuracy and recall rate to obtain a higher F1 score, but threshold modification is much more difficult for SenseVoice and Qwen-Audio (An interesting discovery is that SenseVoice and Qwen-Audio always have a much higher accuracy than the recall rate, which could be more friendly for the human-machine interaction). Secondly, SenseVoice is trained with ASR data with AED pseudo labeling rather than AED-specific data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Preserving Semantic Information By S 3 Tokenizer",
      "text": "To assess the S 3 tokenizer's ability to preserve semantic information, we compared the recognition performance of the quantizer-augmented SenseVoice-L against its original version and the Whisper-Large V3 model. The models underwent evaluation using the Common Voice zh-CN and en benchmarks, with the findings detailed in Table  9 .\n\nFrom the table, we can see that our S 3 tokens demonstrate robust recognition performance in both the Chinese and English test sets. Notably, on the common voice zh-CN set, S 3 tokens surpass the performance of the Whisper-Large V3 model, achieving a 4.14% relative reduction in error rate. This suggests a substantial correlation between S 3 tokens and semantic content. It is worth noting that there is only a single codebook in the S 3 tokenizer with a dictionary size of 4,096 entries.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Whisper-L-V3",
      "text": "SenseVoice Table  9 : The evaluation on S 3 tokens' capability to preserve semantic information. We employ character and word error rates for zh-CN and en languages on the Common Voice benchmarks.\n\nPlease note that the SenseVoice-L model in this table is an intermediate version, and is not identical to the one presented in Table  6 .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Evaluation On Generation Quality Of Cosyvoice",
      "text": "We evaluate the quality of CosyVoice's speech synthesis by examining content consistency and speaker similarity. The \"test-clean\" subset of LibriTTS  (Zen et al., 2019)  and the test set of AISHELL-3  (Shi et al., 2021)  are employed to construct evaluation set for English and Chinese, respectively. For each text in these sets, we randomly select a prompt speech. Content consistency was evaluated using Whisper-Large V3  (Radford et al., 2023)  for English and Paraformer  (Gao et al., 2022)  for Chinese recognition. Speaker similarity was quantified by calculating the cosine similarity between speaker embeddings of the generated and prompt speeches, extracted using ERes2Net  (Chen et al., 2023b) .\n\nSimilar to other autoregressive language models, we employ a random sampling decoding strategy for our token LM and assessed the synthesis process using five different random seed values: 0, 7, 42, 123, and 1,337. The resultant evaluation metrics were averaged to determine the mean and standard deviation. Additionally, we conducted an ASR re-ranking to demonstrate potential performance improvements in offline mode.\n\nTables  10  and 11",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Evaluation On Emotion Controllability Of Cosyvoice",
      "text": "To verify the emotion controllability, we use the public speech emotion recognition model emo2vec 7    (Ma et al., 2024b) . We generate and evaluate 100 English utterances for each of the six emotions: happy, angry, sad, surprised, fearful, and disgusted. The content of the synthesized text is designed to match the target emotion. We then measure the accuracy of the predicted emotions from the synthesized speech for each emotion.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Applications",
      "text": "The FunAudioLLM is an innovative framework designed to facilitate natural voice interactions between humans and large language models (LLMs). By integrating SenseVoice, CosyVoice, and LLMs, FunAudioLLM offers a variety of rich application demos, including speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration. The demos are available at https://fun-audio-llm.github.io.\n\nBy combining SenseVoice, LLMs, and CosyVoice, we can effortlessly perform speech-to-speech translation (S2ST), as illustrated in Figure  10 . SenseVoice is used to recognize the input speech in its original language, the LLM translates the source language to the target language, and CosyVoice synthesizes the target speech with cross-lingual voice cloning. This allows users to speak in foreign languages using their own voice.   Through the analytical capabilities of LLMs to structure and identify emotions within books, and synthesizing this with CosyVoice, we achieve audiobooks with enhanced expressiveness, as illustrated in Figure  13 . The LLM is used for narrative and dialogue analysis, character analysis, and fine-grained sentiment analysis, while CosyVoice synthesizes the speech with enhanced expressiveness.\n\nFigure  13 : A diagram of Expressive Audiobook.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Limitations",
      "text": "SenseVoice has certain limitations that need to be addressed. Firstly, the ASR performance generally remains much lower for under-resourced languages. Secondly, SenseVoice is not designed for streaming transcription. Therefore, future work may focus on developing streamable voice understanding models based on SenseVoice.\n\nCosyVoice also has several limitations. Firstly, it supports a limited number of languages. While it can express emotions and speaking styles based on explicit instructions, it cannot infer the appropriate emotion or style based on the semantic content of the text. Additionally, CosyVoice does not perform well when tasked with singing. There's still room for improvement in achieving expressive emotional changes while maintaining the original timbre of the voice.\n\nAnother limitation is that the two innovative models within FunAudioLLM are not trained end-toend with LLMs. This pipeline approach may introduce error propagation, which could affect overall performance.\n\n7 Authors (alphabetical order of family name)",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.",
      "page": 2
    },
    {
      "caption": "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various",
      "page": 3
    },
    {
      "caption": "Figure 2: SenseVoice-Small is a non-autoregressive encoder-only model for multi-lingual multi-style ASR",
      "page": 3
    },
    {
      "caption": "Figure 3: Importantly, the integration of an additional positional embedding post-quantization enhances",
      "page": 4
    },
    {
      "caption": "Figure 3: An illustration of our supervised semantic speech tokenizer.",
      "page": 5
    },
    {
      "caption": "Figure 4: A semantic diagram of CosyVoice models.",
      "page": 6
    },
    {
      "caption": "Figure 5: Sequence construction for (a) zero-shot in-context learning and (b) cross-lingual voice",
      "page": 6
    },
    {
      "caption": "Figure 5: For prompt",
      "page": 7
    },
    {
      "caption": "Figure 6: provides an overview of the dataset utilized for training the SenseVoice models. The",
      "page": 7
    },
    {
      "caption": "Figure 6: Hours of SenseVoice training data across languages (in log scale).",
      "page": 8
    },
    {
      "caption": "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID",
      "page": 9
    },
    {
      "caption": "Figure 7: illustrates the comparative performance of SenseVoice-Large and Whisper-Large-V3 on a",
      "page": 9
    },
    {
      "caption": "Figure 8: Weighted Average Accuracy (WA(%)) comparison with other open source SER models.",
      "page": 10
    },
    {
      "caption": "Figure 9: F1(%) Score comparison of the SenseVoice with the audio event detection models BEATS",
      "page": 11
    },
    {
      "caption": "Figure 10: SenseVoice is used to recognize the input speech in",
      "page": 14
    },
    {
      "caption": "Figure 10: A diagram of Speech-to-Speech Translation.",
      "page": 14
    },
    {
      "caption": "Figure 11: SenseVoice recognizes the input speech and its emotion and audio",
      "page": 14
    },
    {
      "caption": "Figure 11: A diagram of Emotional Voice Chat.",
      "page": 14
    },
    {
      "caption": "Figure 12: We can use an LLM plugin",
      "page": 14
    },
    {
      "caption": "Figure 12: A diagram of Interactive Podcast.",
      "page": 15
    },
    {
      "caption": "Figure 13: The LLM is used for narrative and dialogue analysis, character analysis, and",
      "page": 15
    },
    {
      "caption": "Figure 13: A diagram of Expressive Audiobook.",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "This report introduces FunAudioLLM, a model family designed to enhance natu-"
        },
        {
          "Abstract": "ral voice interactions between humans and large language models (LLMs). At its"
        },
        {
          "Abstract": "core are two innovative models: SenseVoice, which handles multilingual speech"
        },
        {
          "Abstract": "recognition,\nemotion recognition,\nand audio event detection;\nand CosyVoice,"
        },
        {
          "Abstract": "which facilitates natural speech generation with control over multiple languages,"
        },
        {
          "Abstract": "timbre,\nspeaking style, and speaker\nidentity.\nSenseVoice-Small delivers excep-"
        },
        {
          "Abstract": "tionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-"
        },
        {
          "Abstract": "precision ASR for over 50 languages, while CosyVoice excels in multi-lingual"
        },
        {
          "Abstract": "voice\ngeneration,\nzero-shot\nin-context\nlearning,\ncross-lingual\nvoice\ncloning,"
        },
        {
          "Abstract": "and instruction-following capabilities.\nThe models\nrelated to SenseVoice and"
        },
        {
          "Abstract": "CosyVoice have been open-sourced on Modelscope and Huggingface, along with"
        },
        {
          "Abstract": "the corresponding training,\ninference, and fine-tuning codes released on GitHub."
        },
        {
          "Abstract": "By integrating these models with LLMs, FunAudioLLM enables applications such"
        },
        {
          "Abstract": "as speech-to-speech translation, emotional voice chat,\ninteractive podcasts, and"
        },
        {
          "Abstract": "expressive audiobook narration,\nthereby pushing the boundaries of voice interac-"
        },
        {
          "Abstract": "tion technology. Demos are available at https://fun-audio-llm.github.io,"
        },
        {
          "Abstract": "and the code can be accessed at https://github.com/FunAudioLLM."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "guages, with significant advantages in recognizing Chinese and Cantonese.\nIn addition to speech"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "recognition, SenseVoice offers state-of-the-art capabilities in emotion recognition and audio event"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "detection (Mesaros et al., 2021), making it an ideal choice for creating low-latency, human-like voice"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "interaction systems."
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "Our suite of applications is further enriched by CosyVoice (Du et al., 2024a), a family of fundamen-"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "tal speech generation models designed to produce natural-sounding voices for a variety of contexts."
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "CosyVoice excels in generating multi-lingual voices tailored to specific speakers, zero-shot adapta-"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "tion to new speakers (Wang et al., 2023a), cross-lingual voice cloning (Zhang et al., 2023), creating"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "emotionally resonant voices (Shin et al., 2022), and offering nuanced control over speech output"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "through instructional\ntext\n(Ji et al., 2024). CosyVoice supports five languages: Chinese, English,"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "Japanese, Cantonese, and Korean. CosyVoice comes in three open-source models: CosyVoice-base-"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "300M, which specializes in accurately representing speaker identity, zero-shot\nlearning, and cross-"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "lingual voice cloning; CosyVoice-instruct-300M, which focuses on generating emotionally expres-"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "sive voices and allows for meticulous adjustments via instructional text, extending its capabilities to"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "controllability over various aspects such as speaker identity (Shimizu et al., 2023), speaking style (Ji"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "et al., 2024), and fine-grained paralinguistic features (Kanda et al., 2024); and CosyVoice-sft-300M,"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "which has been fine-tuned on seven multilingual speakers and is ready for immediate deployment."
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "By integrating SenseVoice, CosyVoice, and LLMs like Qwen (Team, 2023), FunAudioLLM offers a"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "range of rich application demos. These include Speech-to-Speech Translation (Berard et al., 2018),"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "which allows users to speak in foreign languages using their own voice; Emotional Voice Chat (Xue"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "et al., 2024), which enables the model to understand and respond to emotions for more human-like"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "interactions; Interactive Podcast (Laban et al., 2022), wherein users can engage in live discussions"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "with multiple large models; and AudioBook (Chalamandaris et al., 2014), allowing the model\nto"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "perform expressive, multi-character narration for audiobooks."
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "Overall, FunAudioLLM leverages the strengths of SenseVoice and CosyVoice to push the bound-"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "aries of voice interaction technology, enabling more natural and seamless communication between"
        },
        {
          "Figure 1: An overview of our FunAudioLLM models for voice understanding and generation.": "humans and large language models."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "speech understanding tasks, including Automatic Speech Recognition (ASR), Language Identifica-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "tion (LID), Speech Emotion Recognition (SER), and Audio Event Detection (AED). SenseVoice-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "Small [Top]: An encoder-only model optimized for rapid speech understanding. It offers high-speed"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "processing while supporting 5 languages. SenseVoice-Large [Bottom]: An encoder-decoder model"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "aimed at achieving more precise speech understanding across a broader range of languages. It excels"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "in accuracy and supports an extensive set of language capabilities."
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "similarity of over 75%, which achieves the quality level of human parity. CosyVoice supports zero-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "shot in-context learning, which enables voice cloning with a prompt speech of even 3 seconds. The"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "timbre, emotion, prosody and style can be reproduced within or cross languages. We also released an"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "instruction model, which can control speaker identity, speaking style (e.g., emotion) and other fine-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "grained paralinguistic features with natural\ntextural\ninstructions. An overview of FunAudioLLM"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "models is shown in Figure 1."
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "2.2\nVoice Understanding Model: SenseVoice"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "SenseVoice is a speech foundation model with multiple voice understanding capabilities, including"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "automatic speech recognition (ASR), spoken language identification (LID), speech emotion recog-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "nition (SER), and audio event classification (AEC) or audio event detection (AED). Two models"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "with different sizes and architectures are proposed to suit different requirements: SenseVoice-Small,"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "an encoder-only speech foundation model for rapid speech understanding, and SenseVoice-Large,"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "an encoder-decoder (Vaswani et al., 2017) speech foundation model for more accurate speech un-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "derstanding with more languages supported, as illustrated in Figure 2."
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "SenseVoice-Small\nis a non-autoregressive encoder-only model\nfor multi-lingual multi-style ASR"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "and multiple speech understanding tasks.\nGiven the input waveform, we first compute the 80-"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "dimensional\nlog-mel filter-bank, and then stack consecutive frames and down-sample them by a"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "factor of 6. The extracted feature is then mapped to the dimension D of the encoder, denoted as"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "Xspeech ∈ RT ×D, where T is the length of the down-sampled feature. The encoder is implemented"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "as a memory-equipped self-attention network (SAN-M) (Gao et al., 2020). To specify the task, we"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "prepend four embeddings to the speech feature as the input to the encoder:"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "(1)\nX = concat(eLID, eSER, eAEC, eITN/NoITN, Xspeech)"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "(2)\nP = Softmax(LinearD→|V ′|(Encoder(X)))"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "X ∈ R(T +4)×D and P ∈ R(T +4)×|V ′|. V ′\nis the vocabulary including tokens for ASR and other"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "tasks. eLID, eSER, eAEC, eITN/NoITN are embeddings of four special tokens:"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "⟨LID⟩ indicates the LID task.\nIf ⟨LID⟩ is prepended,\nthe model\nis trained to predict\nthe language"
        },
        {
          "Figure 2: SenseVoice is a comprehensive speech foundation model designed to perform various": "token, at the corresponding position of the output. In the training stage, we randomly replace ⟨LID⟩"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "the language token, or be configured with a specified language token in the inference stage."
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "⟨SER⟩ indicates the SER task.\nIf ⟨SER⟩ is prepended,\nthe model\nis trained to predict\nthe speech"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "emotion label, at the corresponding position of the output."
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "⟨AEC⟩ indicates the AEC task.\nIf ⟨AEC⟩ is prepended,\nthe model\nis trained to predict\nthe audio"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "event label, at the corresponding position of the output."
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "⟨ITN⟩ or ⟨NoITN⟩ specify the transcription style.\nIf ⟨ITN⟩ is provided,\nthe model\nis trained to"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "transcript with inverse text normalization (ITN) and punctuation. If ⟨NoITN⟩ is provided, the model"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "is trained to transcript without ITN and punctuation."
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "In the training stage, the LID, SER, and AEC tasks are optimized using the cross-entropy loss. The"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "ASR task is optimized using the CTC loss (Graves et al., 2006)."
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "SenseVoice-Large is an autoregressive encoder-decoder model for multi-lingual ASR and multiple"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "speech understanding tasks.\nSimilar\nto Whisper\n(Radford et al., 2023), SenseVoice-Large speci-"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "fies tasks by a sequence of input\ntokens to the decoder. Specifically, we specify whether to predict"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "language, speech emotion, and audio events with timestamps by including ⟨LID⟩, ⟨SER⟩, ⟨AED⟩"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "tokens respectively. Compared to SenseVoice-Small, the advantage of SenseVoice-Large is the tran-"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "scription accuracy and supporting for a vast number of languages (50+)."
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "Table 1 gives examples of transcriptions of Whisper, SenseVoice-S, SenseVoice-L, and the ground"
        },
        {
          "with the ground truth language token according to probability 0.8 so that the model can either predict": "truth of the ASR task."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "Table 1 gives examples of transcriptions of Whisper, SenseVoice-S, SenseVoice-L, and the ground"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "truth of the ASR task."
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "Whisper"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "Absolute shock, but\nin a great way. Wow. That was awesome. That was awesome. What way to open a"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "· · ·\nsong. That was awesome. Awesome."
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "SenseVoice-S"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "< music > Absolute shocked but in a great way my. < happy > That was awesome,\nthat was awesome"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "what way to open a song that was awesome, awesome, · · ·"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "SenseVoice-L"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "< music > Absolutely shocked but\nin a great way. That was awesome, < music > that was awesome"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "< happy > what way to open a song, that was awesome, awesome, · · ·"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "Ground Truth"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "Absolutely shocked, but\nin a great way. Who am I? Wow. That was awesome. That was awesome. What"
        },
        {
          "scription accuracy and supporting for a vast number of languages (50+).": "· · ·\nway to open a song. That was awesome. Awesome."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "Control",
          "Fine-": "grained"
        },
        {
          "Style&Speaker": "✗",
          "Fine-": "✓"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "✗",
          "Fine-": "✓"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "✓",
          "Fine-": "✗"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "✓",
          "Fine-": "✗"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "✗",
          "Fine-": "✗"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "✓",
          "Fine-": "✓"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        },
        {
          "Style&Speaker": "✓",
          "Fine-": "✓"
        },
        {
          "Style&Speaker": "",
          "Fine-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ASR Decoder": ""
        },
        {
          "ASR Decoder": ""
        },
        {
          "ASR Decoder": "Speech Tokens"
        },
        {
          "ASR Decoder": "Vector Quantizer"
        },
        {
          "ASR Decoder": ""
        },
        {
          "ASR Decoder": ""
        },
        {
          "ASR Decoder": ""
        },
        {
          "ASR Decoder": "Speech"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Spk": "Input Text\nLID\nGenerated Speech Token"
        },
        {
          "Spk": "Emb"
        },
        {
          "Spk": "(b) Cross-lingual Voice Cloning"
        },
        {
          "Spk": "Figure 5: Sequence construction for\n(a) zero-shot\nin-context\nlearning and (b) cross-lingual voice"
        },
        {
          "Spk": "cloning. LID represents language identifier."
        },
        {
          "Spk": "2.4.1\nSystem Overview"
        },
        {
          "Spk": "CosyVoice incorporates an autoregressive Transformer-based language model\n(LM)\nto generate"
        },
        {
          "Spk": "speech tokens for\nthe input\ntext. An ordinary differential equation based (ODE-based) diffusion"
        },
        {
          "Spk": "model, flow matching (Lipman et al., 2023), reconstructs Mel spectrum from the generated tokens."
        },
        {
          "Spk": "Subsequently, a HiFTNet-based vocoder (Li et al., 2023) is followed to synthesize waveforms from"
        },
        {
          "Spk": "the reconstructed Mel spectrum. Dashed models are optional for certain applications, such as cross-"
        },
        {
          "Spk": "lingual cloning and speaker fine-tuned inference."
        },
        {
          "Spk": "2.4.2\nModel Training"
        },
        {
          "Spk": "At\nthe training stage,\nthe autoregressive language model\n(LM)\nis trained using a teacher-forcing"
        },
        {
          "Spk": "paradigm. In this process, tokenized text and a left-shifted version of the speech tokens are provided"
        },
        {
          "Spk": "as input to predict the subsequent speech tokens."
        },
        {
          "Spk": "The flow matching model\nis developed to estimate the conditional probabilities P (S|X, v, Sref ),"
        },
        {
          "Spk": "where X and v denote the speech tokens and speaker embeddings (Wang et al., 2023b), respectively."
        },
        {
          "Spk": "S and Sref represent the Mel spectrum of target and reference speech, respectively. A convolutional"
        },
        {
          "Spk": "Transformer U-Net (Mehta et al., 2023) is employed to ascertain the vector field between the prior"
        },
        {
          "Spk": "distribution and the desired one, which is derived from the optimal transport ODE. The straightfor-"
        },
        {
          "Spk": "ward nature of resolving the OT-ODE allows for a significantly reduced number of iterations during"
        },
        {
          "Spk": "the inference stage,\ntypically only five to ten iterations are required to produce a satisfactory Mel"
        },
        {
          "Spk": "spectrogram. We also employ the classifier-free guidance (CFG) (Ho & Salimans, 2022) technique"
        },
        {
          "Spk": "and mask out the 70%∼100% proceeding feature conditions to boost the in-context learning ability."
        },
        {
          "Spk": "For the synthesis of waveforms from the predicted Mel spectrograms, we utilize a vocoder based"
        },
        {
          "Spk": "on HiFTNet (Li et al., 2023). Modifications have been made on HiFTNet to support streaming gen-"
        },
        {
          "Spk": "eration, including the replacement and redesign of certain components. Complete details regarding"
        },
        {
          "Spk": "these adjustments are available in our released code."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.4.3\nZero-shot In-context Learning": "CosyVoice models exhibit zero-shot in-context learning capabilities, allowing for the replication of"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "an arbitrary voice with only a brief reference speech sample. This process entails the careful con-"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "struction of input sequences for the token language model (LM), depicted in Figure 5. For prompt"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "speech and input\ntext\nin the same language, we merge them to form a unified input,\ntreating the"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "prompt speech tokens as pre-generated. With this input sequence, the autoregressive LM iteratively"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "predicts subsequent\ntokens until\nit encounters the “end of sequence” token E . However, when the"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "prompt speech and input\ntext differ linguistically, we omit\nthe text and tokens associated with the"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "prompt\nto prevent prosodic characteristics of the original\nlanguage from influencing the target\nlan-"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "guage. It is important to note that the prompt text, which corresponds to the prompt speech’s content,"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "can be transcribed either through human annotation or ASR models, such as SenseVoice. Similar to"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "the prompt text, the prompt tokens are extracted from the prompt speech with S 3 tokenizer."
        },
        {
          "2.4.3\nZero-shot In-context Learning": "After generating the speech tokens,\nthey are appended after\nthe prompt\ntokens,\nforming a com-"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "posite condition for\nthe flow-matching model. Additionally,\nthe speaker embedding and the Mel"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "spectrogram of\nthe prompt speech are incorporated to further enhance timbre and environmental"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "consistency."
        },
        {
          "2.4.3\nZero-shot In-context Learning": "2.4.4\nInstruction Fine-tuning"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "To enable further controllability on CosyVoice, we experiment with integrating additional instruction"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "fine-tuning (Ji et al., 2023). CosyVoice-instruct extends CosyVoice-base with enhanced instruction-"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "following capabilities. Specifically,\nit supports controllability over various aspects such as speaker"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "identity (i.e., speaker’s characteristics), speaking style (including emotion, gender, speaking rate,"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "ter, breaths, speaking while laughing, and emphasizing certain words. Table 3 shows some examples"
        },
        {
          "2.4.3\nZero-shot In-context Learning": "of speaker identity, speaking style, and fine-grained paralinguistic features."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "ter, breaths, speaking while laughing, and emphasizing certain words. Table 3 shows some examples"
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "of speaker identity, speaking style, and fine-grained paralinguistic features."
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "Speaker Identity"
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "1. Selene ’Moonshade’,"
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "are both mesmerizing and deadly.<endofprompt>Hope is a good thing."
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "2. Theo ’Crimson’,"
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "impulsiveness.<endofprompt>You don’t know about real loss."
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "Speaking Style"
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": ""
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": ""
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "Fine-grained Paralinguistics"
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "1. Well that’s kind of scary [laughter]."
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "2. I don’t think I over eat yeah [breath] and um I do exercise regularly."
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "3. Well that pretty much covers <laughter>the subject</laughter> well thanks for calling me."
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": ""
        },
        {
          "and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh-": "pionship."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": ""
        },
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": "Duration (hr)"
        },
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": "130,000"
        },
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": "30,000"
        },
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": "5,000"
        },
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": "4,600"
        },
        {
          "to generate pseudo labels, yielding an extensive rich transcribe dataset.": "2,200"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: show the comparison of Whisper, SenseVoice and Paraformer (Gao et al.,",
      "data": [
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "4\nExperimental Results"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "4.1\nMultilingual Speech Recognition"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Metrics. We use Character Error Rate (CER) to evaluate the models in five languages: Chinese,"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Cantonese, Japanese, Korean, and Thai, and use the Word Error Rate (WER) for all other languages."
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Both the ground truth transcriptions and the recognition outputs are standardized using text normal-"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "ization before the error rate calculation,\nin alignment with the methodology used by Whisper. All"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Chinese characters were converted into the simplified Chinese version,\ntogether with an additional"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "text normalization pipeline3."
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Results\nin Table 6 show the\ncomparison of Whisper, SenseVoice\nand Paraformer\n(Gao et\nal.,"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "2022, 2023; Shi et al., 2024) on popular open speech recognition benchmark datasets,\nincluding"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "AISHELL-1 (Bu et al., 2017), AISHELL-2 (Du et al., 2018), WenetSpeech (Zhang et al., 2022),"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Librispeech (Panayotov et al., 2015), and Common Voice (Ardila et al., 2019).\nIt can be seen that"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "SenseVoice-S and SenseVoice-L outperform their Whisper counterparts by a significant margin in"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "most test sets except Librispeech."
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "Figure 7 illustrates the comparative performance of SenseVoice-Large and Whisper-Large-V3 on a"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "broader range of languages, with or without ground truth LID as input. While SenseVoice-Large"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "performs comparably with Whisper-Large-V3 in general, SenseVoice-Large obtains significantly"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "better performance in languages like Cantonese (Yue), Catalan (CA), and Marathi (MR)."
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "The evaluation of inference efficiency is shown in Table 7. The Real-time factor (RTF, the ratio of the"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "transcribing time to the audio length) and 10s Audio Latency (the average time cost when transcrib-"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "ing a 10s audio.)\nare benchmarked on an A800 machine, with a decoding batch size of 1. For the"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "encoder-decoder-based model\n(Whipser-S, Whipser-L-V3, and SenseVoice-L), we perform beam"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "search in decoding with a beam size of 5. Owing to its non-autoregressive architecture, SenseVoice-"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "S obtains extremely low inference latency—more than 5 times faster compared to Whisper-small"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "and more than 15 times faster compared to Whisper-L-V3. SenseVoice-L shows close performance"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "with Whipser-L-V3."
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "4.2\nSpeech Emotion Recognition"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "We evaluate the SER ability of the SenseVoice on 7 popular emotion recognition datasets, including"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "CREMA-D(Cao et al., 2014), MELD(Poria et al., 2019),\nIEMOCAP(Busso et al., 2008), MSP-"
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "3https://github.com/speechio/chinese_text_normalization/blob/master/python/cn_tn."
        },
        {
          "Figure 7: Comparison of SenseVoice and Whisper on Common Voice, with or without LID": "py"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 7: Comparison of model architecture, parameter scale, supported languages, and inference",
      "data": [
        {
          "Whisper-S": "10.04",
          "Whisper-L-V3": "5.14",
          "SenseVoice-S": "2.96",
          "SenseVoice-L": "2.09",
          "Paraformer-zh": "1.95"
        },
        {
          "Whisper-S": "8.78",
          "Whisper-L-V3": "4.96",
          "SenseVoice-S": "3.80",
          "SenseVoice-L": "3.04",
          "Paraformer-zh": "2.85"
        },
        {
          "Whisper-S": "25.62",
          "Whisper-L-V3": "18.87",
          "SenseVoice-S": "7.44",
          "SenseVoice-L": "6.73",
          "Paraformer-zh": "6.97"
        },
        {
          "Whisper-S": "16.66",
          "Whisper-L-V3": "10.48",
          "SenseVoice-S": "7.84",
          "SenseVoice-L": "6.01",
          "Paraformer-zh": "6.74"
        },
        {
          "Whisper-S": "3.13",
          "Whisper-L-V3": "1.82",
          "SenseVoice-S": "3.15",
          "SenseVoice-L": "2.57",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "7.37",
          "Whisper-L-V3": "3.50",
          "SenseVoice-S": "7.18",
          "SenseVoice-L": "4.28",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "19.60",
          "Whisper-L-V3": "12.55",
          "SenseVoice-S": "10.78",
          "SenseVoice-L": "7.68",
          "Paraformer-zh": "10.30"
        },
        {
          "Whisper-S": "14.85",
          "Whisper-L-V3": "9.39",
          "SenseVoice-S": "14.71",
          "SenseVoice-L": "9.00",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "38.97",
          "Whisper-L-V3": "10.41",
          "SenseVoice-S": "7.09",
          "SenseVoice-L": "6.78",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "19.51",
          "Whisper-L-V3": "10.34",
          "SenseVoice-S": "11.96",
          "SenseVoice-L": "9.19",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "10.48",
          "Whisper-L-V3": "5.59",
          "SenseVoice-S": "8.28",
          "SenseVoice-L": "5.21",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "20.68",
          "Whisper-L-V3": "9.66",
          "SenseVoice-S": "10.56",
          "SenseVoice-L": "7.57",
          "Paraformer-zh": "-"
        },
        {
          "Whisper-S": "",
          "Whisper-L-V3": "",
          "SenseVoice-S": "",
          "SenseVoice-L": "",
          "Paraformer-zh": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Comparison of model architecture, parameter scale, supported languages, and inference",
      "data": [
        {
          "Table 6: Performance comparisons among different models on Chinese and English Open Corpus.": "Model"
        },
        {
          "Table 6: Performance comparisons among different models on Chinese and English Open Corpus.": "Whisper-S"
        },
        {
          "Table 6: Performance comparisons among different models on Chinese and English Open Corpus.": "Whisper-L-V3"
        },
        {
          "Table 6: Performance comparisons among different models on Chinese and English Open Corpus.": "Paraformer-zh"
        },
        {
          "Table 6: Performance comparisons among different models on Chinese and English Open Corpus.": "SenseVoice-S"
        },
        {
          "Table 6: Performance comparisons among different models on Chinese and English Open Corpus.": "SenseVoice-L"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Comparison of model architecture, parameter scale, supported languages, and inference",
      "data": [
        {
          "Paraformer-zh": "SenseVoice-S",
          "Non-autoregressive": "Non-autoregressive",
          "220M": "234M",
          "zh": "zh,yue,en,ja,ko",
          "0.009": "0.007",
          "100": "70"
        },
        {
          "Paraformer-zh": "SenseVoice-L",
          "Non-autoregressive": "Autoregressive",
          "220M": "1587M",
          "zh": "50+",
          "0.009": "0.110",
          "100": "1623"
        },
        {
          "Paraformer-zh": "Table 7: Comparison of model architecture, parameter scale, supported languages, and inference",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "efficiency of SenseVoice, Paraformer, and Whisper.",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "Podcast(Martinez-Lucas et al., 2020), CASIA(Zhang & Jia, 2008), MER2023(Lian et al., 2023)",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "and ESD(Zhou et al., 2021). These corpora cover both Chinese and English, and scenarios like acts,",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "TV dramas, and daily conversation. We report unweighted average accuracy (UA), weighted average",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "accuracy (WA), macro F1 Score (F1), and weighted average F1 (WF1), and compare them with some",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "recently published SER benchmarks (EmoBox (Ma et al., 2024a), Emo-Superb(Wu et al., 2024) and",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "MerBench (Lian et al., 2024)) from literature in Table 8. We show that SenseVoice achieves a good",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        },
        {
          "Paraformer-zh": "performance on all test sets and all metrics even without fine-tuning on the target domain.",
          "Non-autoregressive": "",
          "220M": "",
          "zh": "",
          "0.009": "",
          "100": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoBox": "WA",
          "Emo-Superb": "F1",
          "MerBench": "WF1",
          "SenseVoice-L": "F1",
          "SenseVoice-S": "F1"
        },
        {
          "EmoBox": "59.6",
          "Emo-Superb": "–",
          "MerBench": "–",
          "SenseVoice-L": "95.5",
          "SenseVoice-S": "70.3"
        },
        {
          "EmoBox": "76.5",
          "Emo-Superb": "67.7",
          "MerBench": "–",
          "SenseVoice-L": "89.8",
          "SenseVoice-S": "65.7"
        },
        {
          "EmoBox": "84.6",
          "Emo-Superb": "–",
          "MerBench": "–",
          "SenseVoice-L": "92.2",
          "SenseVoice-S": "81.0"
        },
        {
          "EmoBox": "72.9",
          "Emo-Superb": "–",
          "MerBench": "69.7",
          "SenseVoice-L": "73.2",
          "SenseVoice-S": "67.9"
        },
        {
          "EmoBox": "51.9",
          "Emo-Superb": "–",
          "MerBench": "46.5",
          "SenseVoice-L": "50.9",
          "SenseVoice-S": "44.6"
        },
        {
          "EmoBox": "65.2",
          "Emo-Superb": "–",
          "MerBench": "67.5",
          "SenseVoice-L": "55.6",
          "SenseVoice-S": "52.8"
        },
        {
          "EmoBox": "43.4",
          "Emo-Superb": "38.4",
          "MerBench": "–",
          "SenseVoice-L": "45.0",
          "SenseVoice-S": "46.4"
        },
        {
          "EmoBox": "",
          "Emo-Superb": "",
          "MerBench": "",
          "SenseVoice-L": "",
          "SenseVoice-S": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 9: The evaluation on S3 tokens’ capability to preserve semantic information. We employ",
      "data": [
        {
          "4.4": "",
          "Preserving Semantic Information by S3 Tokenizer": "To assess the S 3 tokenizer’s ability to preserve semantic information, we compared the recognition"
        },
        {
          "4.4": "",
          "Preserving Semantic Information by S3 Tokenizer": "performance of the quantizer-augmented SenseVoice-L against its original version and the Whisper-"
        },
        {
          "4.4": "",
          "Preserving Semantic Information by S3 Tokenizer": "Large V3 model. The models underwent evaluation using the Common Voice zh-CN and en bench-"
        },
        {
          "4.4": "marks, with the findings detailed in Table 9.",
          "Preserving Semantic Information by S3 Tokenizer": ""
        },
        {
          "4.4": "",
          "Preserving Semantic Information by S3 Tokenizer": "From the table, we can see that our S 3 tokens demonstrate robust recognition performance in both"
        },
        {
          "4.4": "the Chinese and English test sets. Notably, on the common voice zh-CN set, S 3 tokens surpass the",
          "Preserving Semantic Information by S3 Tokenizer": ""
        },
        {
          "4.4": "performance of",
          "Preserving Semantic Information by S3 Tokenizer": "the Whisper-Large V3 model, achieving a 4.14% relative reduction in error"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 9: The evaluation on S3 tokens’ capability to preserve semantic information. We employ",
      "data": [
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": ""
        },
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": "ChatTTS as it doesn’t release voice cloning capabilities."
        },
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": "Model"
        },
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": "Original"
        },
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": "ChatTTS"
        },
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": "CosyVoice"
        },
        {
          "rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and": "+ 5× re-ranking"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 12: shows the comparison of emotion control accuracy between CosyVoice-base and",
      "data": [
        {
          "Original\n2.52\n25\n74.15": "ChatTTS\n3.87\n111\n-"
        },
        {
          "Original\n2.52\n25\n74.15": "CosyVoice\n3.82±0.24\n24.4±2.24\n81.58±0.16"
        },
        {
          "Original\n2.52\n25\n74.15": "+ 5× re-ranking\n1.84\n11\n81.58"
        },
        {
          "Original\n2.52\n25\n74.15": "Table 11: The comparison of original and CosyVoice generated speeches on the AISHELL-3 test set"
        },
        {
          "Original\n2.52\n25\n74.15": "in terms of character error rate (CER) and speaker similarity (SS). “±” joins the mean and standard"
        },
        {
          "Original\n2.52\n25\n74.15": "deviation for each evaluation metric."
        },
        {
          "Original\n2.52\n25\n74.15": "that ChatTTS has a better generation ability on Chinese than English in terms of CER. Although"
        },
        {
          "Original\n2.52\n25\n74.15": "ChatTTS and CosyVoice achieves a similar CER, ChatTTS produces more insertion and deletion"
        },
        {
          "Original\n2.52\n25\n74.15": "errors, This is due to the problem of speaker leaking, where modal particles of another speaker is"
        },
        {
          "Original\n2.52\n25\n74.15": "generated unexpectedly. On the contrary, CosyVoice doesn’t suffer\nthis problem with much less"
        },
        {
          "Original\n2.52\n25\n74.15": "insertion and deletion errors. With ASR re-ranking, CosyVoice reached a remarkably low CER of"
        },
        {
          "Original\n2.52\n25\n74.15": "1.84%. As seen with English, CosyVoice also exhibited greater speaker similarity than the original"
        },
        {
          "Original\n2.52\n25\n74.15": "utterances, showcasing its effective voice-cloning proficiency."
        },
        {
          "Original\n2.52\n25\n74.15": "4.6\nEvaluation on Emotion Controllability of CosyVoice"
        },
        {
          "Original\n2.52\n25\n74.15": "To verify the emotion controllability, we use the public speech emotion recognition model emo2vec7"
        },
        {
          "Original\n2.52\n25\n74.15": "(Ma et al., 2024b). We generate and evaluate 100 English utterances for each of the six emotions:"
        },
        {
          "Original\n2.52\n25\n74.15": "happy, angry, sad, surprised, fearful, and disgusted. The content of the synthesized text is designed"
        },
        {
          "Original\n2.52\n25\n74.15": "to match the target emotion. We then measure the accuracy of\nthe predicted emotions from the"
        },
        {
          "Original\n2.52\n25\n74.15": "synthesized speech for each emotion."
        },
        {
          "Original\n2.52\n25\n74.15": "Table\n12\nshows\nthe\ncomparison\nof\nemotion\ncontrol\naccuracy\nbetween CosyVoice-base\nand"
        },
        {
          "Original\n2.52\n25\n74.15": "CosyVoice-instruct.\nFor CosyVoice-instruct,\nthe input consists of content\ntext accompanied by a"
        },
        {
          "Original\n2.52\n25\n74.15": "speaking style instruction (e.g., “Happy.<endofprompt>Content Text”).\nIn contrast, CosyVoice-"
        },
        {
          "Original\n2.52\n25\n74.15": "base only receives\nthe content\ntext as\ninput.\nThe results\nindicate that CosyVoice-instruct with"
        },
        {
          "Original\n2.52\n25\n74.15": "emotional\ninstructions demonstrates\na\nsignificant\nimprovement\nover both CosyVoice-base\nand"
        },
        {
          "Original\n2.52\n25\n74.15": "CosyVoice-instruct without emotional instructions."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": ""
        },
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": "dev clean"
        },
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": "2.77"
        },
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": "2.79"
        },
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": "2.44"
        },
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": "2.51"
        },
        {
          "introduced by CosyVoice synthesized samples. The findings from our evaluation": "1.93"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 12: A diagram of Interactive Podcast.": "Through the analytical capabilities of LLMs to structure and identify emotions within books, and"
        },
        {
          "Figure 12: A diagram of Interactive Podcast.": "synthesizing this with CosyVoice, we achieve audiobooks with enhanced expressiveness, as illus-"
        },
        {
          "Figure 12: A diagram of Interactive Podcast.": "trated in Figure 13. The LLM is used for narrative and dialogue analysis, character analysis, and"
        },
        {
          "Figure 12: A diagram of Interactive Podcast.": "fine-grained sentiment analysis, while CosyVoice synthesizes the speech with enhanced expressive-"
        },
        {
          "Figure 12: A diagram of Interactive Podcast.": "ness."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "performance.": "Authors (alphabetical order of family name)"
        },
        {
          "performance.": "• Keyu An"
        },
        {
          "performance.": "• Qian Chen"
        },
        {
          "performance.": "• Chong Deng"
        },
        {
          "performance.": "• Zhihao Du"
        },
        {
          "performance.": "• Changfeng Gao"
        },
        {
          "performance.": "• Zhifu Gao"
        },
        {
          "performance.": "• Yue Gu"
        },
        {
          "performance.": "• Ting He"
        },
        {
          "performance.": "• Hangrui Hu"
        },
        {
          "performance.": "• Kai Hu"
        },
        {
          "performance.": "• Shengpeng Ji"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nAcknowledgment": "We extend our heartfelt appreciation to the developers and contributors of the following open-source"
        },
        {
          "8\nAcknowledgment": "projects: FunASR, FunCodec, Whisper, ESPNet, WeNet, SLAM-LLM, Matcha-TTS, and Tortoise."
        },
        {
          "8\nAcknowledgment": "Their\ninnovative efforts and valuable code contributions have significantly inspired our work and"
        },
        {
          "8\nAcknowledgment": "facilitated our research. We are also grateful\nto numerous other projects not explicitly mentioned"
        },
        {
          "8\nAcknowledgment": "here, which have equally provided considerable assistance and played an instrumental role in the"
        },
        {
          "8\nAcknowledgment": "success of our endeavors."
        },
        {
          "8\nAcknowledgment": "References"
        },
        {
          "8\nAcknowledgment": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,\nJosh Meyer,"
        },
        {
          "8\nAcknowledgment": "Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber.\nCommon voice: A"
        },
        {
          "8\nAcknowledgment": "massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019."
        },
        {
          "8\nAcknowledgment": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,"
        },
        {
          "8\nAcknowledgment": "Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,"
        },
        {
          "8\nAcknowledgment": "Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi"
        },
        {
          "8\nAcknowledgment": "Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu,"
        },
        {
          "8\nAcknowledgment": "Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan,"
        },
        {
          "8\nAcknowledgment": "Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-"
        },
        {
          "8\nAcknowledgment": "gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609,"
        },
        {
          "8\nAcknowledgment": "2023a."
        },
        {
          "8\nAcknowledgment": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang"
        },
        {
          "8\nAcknowledgment": "Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities."
        },
        {
          "8\nAcknowledgment": "CoRR, abs/2308.12966, 2023b."
        },
        {
          "8\nAcknowledgment": "Alexandre Berard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin.\nEnd-to-end"
        },
        {
          "8\nAcknowledgment": "automatic speech translation of audiobooks.\nIn ICASSP, pp. 6224–6228. IEEE, 2018."
        },
        {
          "8\nAcknowledgment": "Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin"
        },
        {
          "8\nAcknowledgment": "speech corpus and a speech recognition baseline. In 2017 20th conference of the oriental chapter"
        },
        {
          "8\nAcknowledgment": "of\nthe international coordinating committee on speech databases and speech I/O systems and"
        },
        {
          "8\nAcknowledgment": "assessment (O-COCOSDA), pp. 1–5. IEEE, 2017."
        },
        {
          "8\nAcknowledgment": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Ebrahim (Abe) Kazemzadeh, Emily Mower Provost,"
        },
        {
          "8\nAcknowledgment": "Samuel Kim,\nJeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan.\nIemocap:\nin-"
        },
        {
          "8\nAcknowledgment": "teractive emotional dyadic motion capture database.\nLanguage Resources and Evaluation, 42:"
        },
        {
          "8\nAcknowledgment": "335–359, 2008."
        },
        {
          "8\nAcknowledgment": "Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur, Ani Nenkova, and Ragini"
        },
        {
          "8\nAcknowledgment": "IEEE Transactions on\nVerma. Crema-d: Crowd-sourced emotional multimodal actors dataset."
        },
        {
          "8\nAcknowledgment": "Affective Computing, 5(4):377–390, 2014. doi: 10.1109/TAFFC.2014.2336244."
        },
        {
          "8\nAcknowledgment": "Aimilios Chalamandaris, Pirros Tsiakoulis, Sotiris Karabetsos, and Spyros Raptis.\nUsing audio"
        },
        {
          "8\nAcknowledgment": "books for training a text-to-speech system.\nIn LREC, pp. 3076–3080. European Language Re-"
        },
        {
          "8\nAcknowledgment": "sources Association (ELRA), 2014."
        },
        {
          "8\nAcknowledgment": "Qian Chen, Mengzhe Chen, Bo Li, and Wen Wang. Controllable time-delay transformer for real-"
        },
        {
          "8\nAcknowledgment": "time punctuation prediction and disfluency detection.\nIn ICASSP, pp. 8069–8073. IEEE, 2020."
        },
        {
          "8\nAcknowledgment": "Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che,"
        },
        {
          "8\nAcknowledgment": "Xiangzhan Yu, and Furu Wei.\nBeats: Audio pre-training with acoustic tokenizers.\nIn ICML,"
        },
        {
          "8\nAcknowledgment": "volume 202 of Proceedings of Machine Learning Research, pp. 5178–5193. PMLR, 2023a."
        },
        {
          "8\nAcknowledgment": "Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Qian Chen, and Jiajun Qi. An enhanced res2net"
        },
        {
          "8\nAcknowledgment": "with local and global feature fusion for speaker verification.\nIn Interspeech. ISCA, 2023b."
        },
        {
          "8\nAcknowledgment": "Yunfei Chu,\nJin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and"
        },
        {
          "8\nAcknowledgment": "Jingren Zhou.\nQwen-audio: Advancing universal audio understanding via unified large-scale"
        },
        {
          "8\nAcknowledgment": "audio-language models, 2023."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "into industrial scale. arXiv preprint arXiv:1808.10583, 2018."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Yue Gu, Ziyang Ma, and"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zhijie Yan.\nCosyvoice: A scalable multilingual zero-shot\ntext-to-speech synthesizer based on"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "supervised semantic tokens. arxiv, 2024a."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng. Funcodec: A fundamental, reproducible and"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "integrable open-source toolkit for neural speech codec.\nIn ICASSP, 2024b."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Alexandre D´efossez,\nJade Copet, Gabriel Synnaeve, and Yossi Adi.\nHigh fidelity neural audio"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "compression. arXiv:2210.13438, 2022."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zhifu Gao, Shiliang Zhang, Ming Lei,\nand Ian McLoughlin.\nSAN-M: memory equipped self-"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "the International\nattention for end-to-end speech recognition.\nIn 21st Annual Conference of"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Speech Communication Association,\nInterspeech 2020, Virtual Event, Shanghai, China, Octo-"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "ber 25-29, 2020, pp. 6–10. ISCA, 2020."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and accurate parallel"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "transformer\nfor non-autoregressive end-to-end speech recognition.\nIn Interspeech, pp. 2063–"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "2067. ISCA, 2022."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Zuo, Zhihao Du, Zhangyu Xiao, and Shiliang Zhang. Funasr: A fundamental end-to-end speech"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "recognition toolkit.\nIn INTERSPEECH, 2023."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Alex Graves, Santiago Fern´andez, Faustino J. Gomez, and J¨urgen Schmidhuber.\nConnectionist"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "temporal classification:\nlabelling unsegmented sequence data with recurrent neural networks."
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "the Twenty-Third International Conference (ICML 2006),\nIn Machine Learning, Proceedings of"
        },
        {
          "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research": "Pittsburgh, Pennsylvania, USA, June 25-29, 2006, volume 148, pp. 369–376. ACM, 2006."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Li,\nJinming Zhao, Ye Liu, Bin Liu,\nJiangyan Yi, Meng Wang, Erik Cambria, Guoying Zhao,"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Bj¨orn W. Schuller, and Jianhua Tao. Mer 2023: Multi-label\nlearning, modality robustness, and"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "semi-supervised learning, 2023."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and Jianhua Tao."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Merbench: A unified evaluation benchmark for multimodal emotion recognition, 2024."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.\nFlow"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "matching for generative modeling.\nIn ICLR. OpenReview.net, 2023."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li, Jiaxin Ye, Xie"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Chen, and Thomas Hain. Emobox: Multilingual multi-corpus speech emotion recognition toolkit"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "and benchmark.\nIn Proc. INTERSPEECH, 2024a."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen. emo-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "tion2vec: Self-supervised pre-training for speech emotion representation. Proc. ACL Findings,"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "2024b."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Luz Martinez-Lucas, Mohammed Abdelwahab, and Carlos Busso. The MSP-Conversation Corpus."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "In Proc. Interspeech 2020, pp. 1823–1827, 2020."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "´"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Shivam Mehta, Ruibo Tu, Jonas Beskow,\nEva Sz´ekely, and Gustav Eje Henter. Matcha-tts: A fast"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "TTS architecture with conditional flow matching. CoRR, abs/2309.03199, 2023."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "Annamaria Mesaros, Toni Heittola, Tuomas Virtanen, and Mark D. Plumbley. Sound event detec-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying": "tion: A tutorial.\nIEEE Signal Process. Mag., 38(5):67–83, 2021."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "R., Prasanta Kumar Ghosh, and Sriram Ganapathy. Coswara — a database of breathing, cough,"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "and voice sounds for covid-19 diagnosis.\nIn Interspeech 2020. ISCA, 2020."
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Xian Shi, Yexin Yang, Zerui Li, Yanni Chen, Zhifu Gao, and Shiliang Zhang.\nSeaco-paraformer:"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "A non-autoregressive asr system with flexible and effective hotword customization ability.\nIn"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "(ICASSP), pp. 10346–10350. IEEE, 2024."
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. AISHELL-3: A multi-speaker mandarin TTS"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "corpus.\nIn Interspeech, pp. 2756–2760. ISCA, 2021."
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya Ko-"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "matsu, and Kentaro Tachibana. Prompttts++: Controlling speaker identity in prompt-based text-"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "to-speech using natural language descriptions. CoRR, abs/2309.08140, 2023."
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Yookyung Shin, Younggun Lee, Suhee Jo, Yeongtae Hwang, and Taesu Kim. Text-driven emotional"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "style control and cross-speaker style transfer in neural TTS.\nIn INTERSPEECH, pp. 2313–2317."
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "ISCA, 2022."
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Chao Zhang. SALMONN: Towards generic hearing abilities for large language models.\nIn The"
        },
        {
          "Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala": "Twelfth International Conference on Learning Representations, 2024."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": "Chen, Chenchen Zeng, et al. Wenetspeech: A 10000+ hours multi-domain mandarin corpus for"
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": "In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech"
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": ""
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": ""
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": ""
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": "Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing"
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": "Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Speak foreign languages with"
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": "your own voice: Cross-lingual neural codec language modeling. CoRR, abs/2303.03926, 2023."
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": ""
        },
        {
          "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Whisper-L-V3": "w lid",
          "SenseVoice-L": "w lid"
        },
        {
          "Whisper-L-V3": "12.55",
          "SenseVoice-L": "7.68"
        },
        {
          "Whisper-L-V3": "9.39",
          "SenseVoice-L": "9.00"
        },
        {
          "Whisper-L-V3": "10.51",
          "SenseVoice-L": "6.78"
        },
        {
          "Whisper-L-V3": "10.34",
          "SenseVoice-L": "9.19"
        },
        {
          "Whisper-L-V3": "5.59",
          "SenseVoice-L": "5.21"
        },
        {
          "Whisper-L-V3": "10.77",
          "SenseVoice-L": "8.45"
        },
        {
          "Whisper-L-V3": "4.74",
          "SenseVoice-L": "4.63"
        },
        {
          "Whisper-L-V3": "5.46",
          "SenseVoice-L": "5.16"
        },
        {
          "Whisper-L-V3": "5.67",
          "SenseVoice-L": "5.23"
        },
        {
          "Whisper-L-V3": "7.22",
          "SenseVoice-L": "6.97"
        },
        {
          "Whisper-L-V3": "5.80",
          "SenseVoice-L": "4.12"
        },
        {
          "Whisper-L-V3": "5.70",
          "SenseVoice-L": "6.57"
        },
        {
          "Whisper-L-V3": "13.20",
          "SenseVoice-L": "5.62"
        },
        {
          "Whisper-L-V3": "4.28",
          "SenseVoice-L": "5.23"
        },
        {
          "Whisper-L-V3": "5.92",
          "SenseVoice-L": "6.88"
        },
        {
          "Whisper-L-V3": "5.95",
          "SenseVoice-L": "7.47"
        },
        {
          "Whisper-L-V3": "9.04",
          "SenseVoice-L": "9.70"
        },
        {
          "Whisper-L-V3": "16.88",
          "SenseVoice-L": "10.06"
        },
        {
          "Whisper-L-V3": "12.04",
          "SenseVoice-L": "11.09"
        },
        {
          "Whisper-L-V3": "10.84",
          "SenseVoice-L": "12.01"
        },
        {
          "Whisper-L-V3": "13.40",
          "SenseVoice-L": "12.27"
        },
        {
          "Whisper-L-V3": "12.49",
          "SenseVoice-L": "13.23"
        },
        {
          "Whisper-L-V3": "14.24",
          "SenseVoice-L": "13.25"
        },
        {
          "Whisper-L-V3": "31.13",
          "SenseVoice-L": "13.51"
        },
        {
          "Whisper-L-V3": "13.73",
          "SenseVoice-L": "16.98"
        },
        {
          "Whisper-L-V3": "11.60",
          "SenseVoice-L": "20.75"
        },
        {
          "Whisper-L-V3": "25.21",
          "SenseVoice-L": "28.63"
        },
        {
          "Whisper-L-V3": "50.43",
          "SenseVoice-L": "25.85"
        },
        {
          "Whisper-L-V3": "34.86",
          "SenseVoice-L": "40.33"
        },
        {
          "Whisper-L-V3": "40.15",
          "SenseVoice-L": "43.80"
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities"
    },
    {
      "citation_id": "4",
      "title": "End-to-end automatic speech translation of audiobooks",
      "authors": [
        "Alexandre Berard",
        "Laurent Besacier",
        "Ali Kocabiyikoglu",
        "Olivier Pietquin"
      ],
      "year": "2018",
      "venue": "End-to-end automatic speech translation of audiobooks"
    },
    {
      "citation_id": "5",
      "title": "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline",
      "authors": [
        "Hui Bu",
        "Jiayu Du",
        "Xingyu Na",
        "Bengu Wu",
        "Hao Zheng"
      ],
      "year": "2017",
      "venue": "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "7",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "8",
      "title": "Using audio books for training a text-to-speech system",
      "authors": [
        "Aimilios Chalamandaris",
        "Pirros Tsiakoulis",
        "Sotiris Karabetsos",
        "Spyros Raptis"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "9",
      "title": "Controllable time-delay transformer for realtime punctuation prediction and disfluency detection",
      "authors": [
        "Qian Chen",
        "Mengzhe Chen",
        "Bo Li",
        "Wen Wang"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Beats: Audio pre-training with acoustic tokenizers",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Wanxiang Che",
        "Xiangzhan Yu",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "11",
      "title": "An enhanced res2net with local and global feature fusion for speaker verification",
      "authors": [
        "Yafeng Chen",
        "Siqi Zheng",
        "Hui Wang",
        "Luyao Cheng",
        "Qian Chen",
        "Jiajun Qi"
      ],
      "venue": "An enhanced res2net with local and global feature fusion for speaker verification"
    },
    {
      "citation_id": "12",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models"
    },
    {
      "citation_id": "13",
      "title": "Aishell-2: Transforming mandarin asr research into industrial scale",
      "authors": [
        "Jiayu Du",
        "Xingyu Na",
        "Xuechen Liu",
        "Hui Bu"
      ],
      "year": "2018",
      "venue": "Aishell-2: Transforming mandarin asr research into industrial scale",
      "arxiv": "arXiv:1808.10583"
    },
    {
      "citation_id": "14",
      "title": "Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Yue Gu",
        "Ziyang Ma",
        "Zhijie Yan"
      ],
      "year": "2024",
      "venue": "Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens"
    },
    {
      "citation_id": "15",
      "title": "Funcodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec",
      "authors": [
        "Zhihao Du",
        "Shiliang Zhang",
        "Kai Hu",
        "Siqi Zheng"
      ],
      "venue": "Funcodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec"
    },
    {
      "citation_id": "16",
      "title": "High fidelity neural audio compression",
      "authors": [
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "year": "2022",
      "venue": "High fidelity neural audio compression",
      "arxiv": "arXiv:2210.13438"
    },
    {
      "citation_id": "17",
      "title": "SAN-M: memory equipped selfattention for end-to-end speech recognition",
      "authors": [
        "Zhifu Gao",
        "Shiliang Zhang",
        "Ming Lei",
        "Ian Mcloughlin"
      ],
      "year": "2020",
      "venue": "21st Annual Conference of the International Speech Communication Association, Interspeech 2020, Virtual Event"
    },
    {
      "citation_id": "18",
      "title": "Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition",
      "authors": [
        "Zhifu Gao",
        "Shiliang Zhang",
        "Ian Mcloughlin",
        "Zhijie Yan"
      ],
      "year": "2022",
      "venue": "Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Funasr: A fundamental end-to-end speech recognition toolkit",
      "authors": [
        "Zhifu Gao",
        "Zerui Li",
        "Jiaming Wang",
        "Haoneng Luo",
        "Xian Shi",
        "Mengzhe Chen",
        "Yabin Li",
        "Lingyun Zuo",
        "Zhihao Du",
        "Zhangyu Xiao",
        "Shiliang Zhang"
      ],
      "year": "2023",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "20",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)"
    },
    {
      "citation_id": "21",
      "title": "Classifier-free diffusion guidance",
      "authors": [
        "Jonathan Ho",
        "Tim Salimans"
      ],
      "year": "2022",
      "venue": "Classifier-free diffusion guidance"
    },
    {
      "citation_id": "22",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "23",
      "title": "Textrolspeech: A text style control speech corpus with codec language text-tospeech models",
      "authors": [
        "Shengpeng Ji",
        "Jialong Zuo",
        "Minghui Fang",
        "Ziyue Jiang",
        "Feiyang Chen",
        "Xinyu Duan",
        "Baoxing Huai",
        "Zhou Zhao"
      ],
      "year": "2023",
      "venue": "Textrolspeech: A text style control speech corpus with codec language text-tospeech models"
    },
    {
      "citation_id": "24",
      "title": "Controlspeech: Towards simultaneous zero-shot speaker cloning and zero-shot language style control with decoupled codec",
      "authors": [
        "Shengpeng Ji",
        "Jialong Zuo",
        "Minghui Fang",
        "Siqi Zheng",
        "Qian Chen",
        "Wen Wang",
        "Ziyue Jiang",
        "Hai Huang",
        "Xize Cheng",
        "Rongjie Huang",
        "Zhou Zhao"
      ],
      "year": "2024",
      "venue": "Controlspeech: Towards simultaneous zero-shot speaker cloning and zero-shot language style control with decoupled codec"
    },
    {
      "citation_id": "25",
      "title": "Making flow-matching-based zero-shot text-to-speech laugh as you like",
      "authors": [
        "Naoyuki Kanda",
        "Xiaofei Wang",
        "Sefik Eskimez",
        "Manthan Thakker",
        "Hemin Yang",
        "Zirun Zhu",
        "Min Tang",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yufei Xia",
        "Jinzhu Li",
        "Yanqing Liu",
        "Sheng Zhao",
        "Michael Zeng"
      ],
      "year": "2024",
      "venue": "Making flow-matching-based zero-shot text-to-speech laugh as you like"
    },
    {
      "citation_id": "26",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Qiuqiang Kong",
        "Yin Cao",
        "Turab Iqbal",
        "Yuxuan Wang",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2020.3030497"
    },
    {
      "citation_id": "27",
      "title": "Newspod: Automatic and interactive news podcasts",
      "authors": [
        "Philippe Laban",
        "Elicia Ye",
        "Srujay Korlakunta",
        "John Canny",
        "Marti Hearst"
      ],
      "year": "2022",
      "venue": "IUI"
    },
    {
      "citation_id": "28",
      "title": "Hiftnet: A fast high-quality neural vocoder with harmonic-plus-noise filter and inverse short time fourier transform",
      "authors": [
        "Aaron Yinghao",
        "Cong Li",
        "Xilin Han",
        "Nima Jiang",
        "Mesgarani"
      ],
      "year": "2023",
      "venue": "Hiftnet: A fast high-quality neural vocoder with harmonic-plus-noise filter and inverse short time fourier transform"
    },
    {
      "citation_id": "29",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mingyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao",
        "Ye Liu",
        "Bin Liu",
        "Jiangyan Yi",
        "Meng Wang",
        "Erik Cambria",
        "Guoying Zhao",
        "Björn Schuller",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Multi-label learning, modality robustness, and semi-supervised learning"
    },
    {
      "citation_id": "30",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition"
    },
    {
      "citation_id": "31",
      "title": "Flow matching for generative modeling",
      "authors": [
        "Yaron Lipman",
        "Ricky Chen",
        "Heli Ben-Hamu",
        "Maximilian Nickel",
        "Matthew Le"
      ],
      "year": "2023",
      "venue": "ICLR. OpenReview.net"
    },
    {
      "citation_id": "32",
      "title": "Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Ziyang Ma",
        "Mingjie Chen",
        "Hezhao Zhang",
        "Zhisheng Zheng",
        "Wenxi Chen",
        "Xiquan Li",
        "Jiaxin Ye",
        "Xie Chen",
        "Thomas Hain"
      ],
      "year": "2024",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "33",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proc. ACL Findings"
    },
    {
      "citation_id": "34",
      "title": "The MSP-Conversation Corpus",
      "authors": [
        "Luz Martinez-Lucas",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "35",
      "title": "Matcha-tts: A fast TTS architecture with conditional flow matching",
      "authors": [
        "Shivam Mehta",
        "Ruibo Tu",
        "Jonas Beskow",
        "Éva Székely",
        "Gustav Henter"
      ],
      "year": "2023",
      "venue": "Matcha-tts: A fast TTS architecture with conditional flow matching"
    },
    {
      "citation_id": "36",
      "title": "Sound event detection: A tutorial",
      "authors": [
        "Annamaria Mesaros",
        "Toni Heittola",
        "Tuomas Virtanen",
        "Mark Plumbley"
      ],
      "year": "2021",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "37",
      "title": "GPT-4 technical report",
      "year": "2023",
      "venue": "GPT-4 technical report"
    },
    {
      "citation_id": "38",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "39",
      "title": "ESC: dataset for environmental sound classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "year": "2015",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "40",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "A mostly data-driven approach to inverse text normalization",
      "authors": [
        "Ernest Pusateri",
        "Bharat Ambati",
        "Elizabeth Brooks",
        "Ondrej Plátek",
        "Donald Mcallaster",
        "Venki Nagesha"
      ],
      "year": "2017",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "42",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "of Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "43",
      "title": "",
      "authors": [
        "Machel Reid",
        "Nikolay Savinov",
        "Denis Teplyashin",
        "Dmitry Lepikhin",
        "Timothy Lillicrap",
        "Jean-Baptiste Alayrac",
        "Radu Soricut",
        "Angeliki Lazaridou",
        "Orhan Firat",
        "Julian Schrittwieser",
        "Ioannis Antonoglou",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Andrew Dai",
        "Katie Millican",
        "Ethan Dyer",
        "Mia Glaese",
        "Thibault Sottiaux",
        "Benjamin Lee",
        "Fabio Viola",
        "Malcolm Reynolds",
        "Yuanzhong Xu",
        "James Molloy",
        "Jilin Chen",
        "Michael Isard",
        "Paul Barham",
        "Tom Hennigan",
        "Ross Mcilroy",
        "Melvin Johnson",
        "Johan Schalkwyk",
        "Eli Collins",
        "Eliza Rutherford",
        "Erica Moreira",
        "Kareem Ayoub",
        "Megha Goel",
        "Clemens Meyer",
        "Gregory Thornton",
        "Zhen Yang",
        "Henryk Michalewski",
        "Zaheer Abbas",
        "Nathan Schucher"
      ],
      "venue": ""
    },
    {
      "citation_id": "44",
      "title": "Coswara -a database of breathing, cough, and voice sounds for covid-19 diagnosis",
      "authors": [
        "Neeraj Sharma",
        "Prashant Krishnan",
        "Rohit Kumar",
        "Shreyas Ramoji",
        "Raj Srikanth",
        "Chetupalli",
        "R Nirmala",
        "Prasanta Kumar Ghosh",
        "Sriram Ganapathy"
      ],
      "venue": "Coswara -a database of breathing, cough, and voice sounds for covid-19 diagnosis"
    },
    {
      "citation_id": "45",
      "title": "Seaco-paraformer: A non-autoregressive asr system with flexible and effective hotword customization ability",
      "authors": [
        "Xian Shi",
        "Yexin Yang",
        "Zerui Li",
        "Yanni Chen",
        "Zhifu Gao",
        "Shiliang Zhang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "AISHELL-3: A multi-speaker mandarin TTS corpus",
      "authors": [
        "Yao Shi",
        "Hui Bu",
        "Xin Xu",
        "Shaoji Zhang",
        "Ming Li"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "47",
      "title": "Prompttts++: Controlling speaker identity in prompt-based textto-speech using natural language descriptions",
      "authors": [
        "Reo Shimizu",
        "Ryuichi Yamamoto",
        "Masaya Kawamura",
        "Yuma Shirahata",
        "Hironori Doi",
        "Tatsuya Komatsu",
        "Kentaro Tachibana"
      ],
      "year": "2023",
      "venue": "Prompttts++: Controlling speaker identity in prompt-based textto-speech using natural language descriptions"
    },
    {
      "citation_id": "48",
      "title": "Text-driven emotional style control and cross-speaker style transfer in neural TTS",
      "authors": [
        "Yookyung Shin",
        "Younggun Lee",
        "Suhee Jo",
        "Yeongtae Hwang",
        "Taesu Kim"
      ],
      "year": "2022",
      "venue": "In INTERSPEECH"
    },
    {
      "citation_id": "49",
      "title": "SALMONN: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "M Zejun",
        "Chao Zhang"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "Qwen technical report",
      "authors": [
        "Qwen Team"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "51",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurélien Rodriguez",
        "Armand Joulin",
        "Edouard Grave",
        "Guillaume Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models"
    },
    {
      "citation_id": "52",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "53",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Neural codec language models are zero-shot text to speech synthesizers"
    },
    {
      "citation_id": "54",
      "title": "CAM++: A fast and efficient network for speaker verification using context-aware masking",
      "authors": [
        "Hui Wang",
        "Siqi Zheng",
        "Yafeng Chen",
        "Luyao Cheng",
        "Qian Chen"
      ],
      "year": "2023",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "55",
      "title": "Emo-superb: An in-depth look at speech emotion recognition",
      "authors": [
        "Haibin Wu",
        "Huang-Cheng Chou",
        "Kai-Wei Chang",
        "Lucas Goncalves",
        "Jiawei Du",
        "Jyh-Shing Roger Jang",
        "Chi-Chun Lee",
        "Hung-Yi Lee"
      ],
      "year": "2024",
      "venue": "Emo-superb: An in-depth look at speech emotion recognition"
    },
    {
      "citation_id": "56",
      "title": "E-chat: Emotion-sensitive spoken dialogue system with large language models",
      "authors": [
        "Yuhao Hongfei Xue",
        "Bingshen Liang",
        "Shiliang Mu",
        "Mengzhe Zhang",
        "Qian Chen",
        "Lei Chen",
        "Xie"
      ],
      "year": "2024",
      "venue": "E-chat: Emotion-sensitive spoken dialogue system with large language models"
    },
    {
      "citation_id": "57",
      "title": "Hificodec: Group-residual vector quantization for high fidelity audio codec",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Jinchuan Tian",
        "Chao Weng",
        "Yuexian Zou"
      ],
      "year": "2023",
      "venue": "Hificodec: Group-residual vector quantization for high fidelity audio codec"
    },
    {
      "citation_id": "58",
      "title": "Soundstream: An end-to-end neural audio codec",
      "authors": [
        "Neil Zeghidour",
        "Alejandro Luebs",
        "Ahmed Omran",
        "Jan Skoglund",
        "Marco Tagliasacchi"
      ],
      "year": "2022",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "59",
      "title": "Libritts: A corpus derived from librispeech for text-tospeech",
      "authors": [
        "Heiga Zen",
        "Viet Dang",
        "Rob Clark"
      ],
      "year": "2019",
      "venue": "Libritts: A corpus derived from librispeech for text-tospeech",
      "arxiv": "arXiv:1904.02882"
    },
    {
      "citation_id": "60",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "Binbin Zhang",
        "Hang Lv",
        "Pengcheng Guo",
        "Qijie Shao",
        "Chao Yang",
        "Lei Xie",
        "Xin Xu",
        "Hui Bu",
        "Xiaoyu Chen",
        "Chenchen Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "Jtflm Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "62",
      "title": "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling"
    },
    {
      "citation_id": "63",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset"
    },
    {
      "citation_id": "64",
      "title": "A Auxiliary Results of SenseVoice on Common Voice",
      "venue": "A Auxiliary Results of SenseVoice on Common Voice"
    }
  ]
}