{
  "paper_id": "2510.15221v1",
  "title": "Weld: A Large-Scale Longitudinal Dataset Of Workplace Emotional Dynamics For Ubiquitous Affective Computing",
  "published": "2025-10-17T00:59:43Z",
  "authors": [
    "Xiao Sun"
  ],
  "keywords": [
    "Ubiquitous affective computing",
    "emotion recognition",
    "facial expression analysis",
    "workplace emotions",
    "longitudinal dataset",
    "small-group dynamics",
    "emotional contagion",
    "naturalistic settings",
    "deep learning",
    "organizational behavior"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Capturing authentic emotional expressions in realworld ubiquitous settings has long challenged affective computing, largely due to the lack of extensive, longitudinal data collected from small-group naturalistic environments. To address this gap and advance ubiquitous affective computing, we introduce WELD-a dataset containing 733,651 facial expression records from a small team of 38 employees, gathered over 30.5 months (November 2021 to May 2024) within a genuine workplace context. Each entry includes seven emotion probabilities (neutral, happy, sad, surprised, fear, disgusted, angry) extracted through deep learning-based facial expression analysis, alongside rich metadata such as job roles, employment outcomes, and personality traits. Spanning the COVID-19 pandemic, this dataset captures emotional shifts linked to major societal events like the Shanghai lockdown and evolving policies. We further enrich the data with 32 advanced emotional metrics derived from established affective science frameworks, including valence, arousal, volatility, predictability, inertia, and emotional contagion strength. Our technical validation confirms the dataset's robustness by replicating well-known psychological phenomena-such as a +192% valence boost on weekends (p < 0.001) and consistent diurnal rhythms-and by demonstrating flawless predictive power for employee turnover (AUC=1.0). Baseline models, including Random Forest and LSTM, achieve 91.2% accuracy in emotion classification and an R 2 of 0.84 for valence prediction. As the largest and longest-running longitudinal dataset of workplace emotions publicly accessible, WELD opens new avenues for research in emotion recognition, affective dynamics, emotional contagion, turnover forecasting, and emotion-aware system design. The dataset is freely available under a CC BY 4.0 license.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A FFECTIVE computing aims to build systems that can recognize, interpret, and simulate human emotions  [1] . As this field matures, there is growing recognition that emotion-aware technologies must move beyond laboratory settings into real-world ubiquitous environments-what we term ubiquitous affective computing. This paradigm shift emphasizes continuous, unobtrusive emotion monitoring in naturalistic settings, particularly within small groups where interpersonal dynamics and contextual factors profoundly shape Corresponding author: Xiao Sun (e-mail: sunx@hfut.edu.cn). X. Sun is with AnHui Province Key Laboratory of Affective Computing and Advanced Intelligent Machine, School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230009, China.\n\nManuscript received October XX, 2024.\n\nemotional experiences. Thanks to advances in deep learning, emotion recognition accuracy on benchmark datasets has improved dramatically  [2] ,  [3] . Yet, a significant challenge persists: most datasets are collected in controlled laboratory environments featuring posed expressions  [4] ,  [5] , or they consist of brief recordings that fall short of capturing long-term emotional patterns  [6] . Ubiquitous affective computing applications, especially in workplace contexts, demand datasets that reflect authentic emotions unfolding naturally over extended periods within small-group settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Motivation",
      "text": "Emotions in the workplace profoundly shape employee performance, decision-making, collaboration, and overall wellbeing  [7] ,  [8] . Organizations are increasingly interested in emotion-aware technologies to monitor wellbeing, tailor adaptive interfaces, and deliver timely interventions  [9] . However, continuously and objectively measuring workplace emotions remains a tough challenge due to privacy concerns, technical hurdles, and the complexity of deploying sensing systems in genuine work environments  [10] .\n\nThe COVID-19 pandemic has reshaped workplace dynamics, introducing unprecedented stressors such as lockdowns, remote work transitions, and health anxieties  [11] . Gaining insight into how employees' emotions evolved throughout this period is vital for designing resilient organizational systems and supporting wellbeing during future crises.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Existing Datasets And Limitations",
      "text": "Laboratory Datasets: Landmark collections like FER2013  [12] , AffectNet  [13] , and RAF-DB  [14]  have propelled facial expression recognition forward. However, they mainly consist of static images sourced from the web, often showing posed or exaggerated expressions. Although AffectNet boasts over a million images, these snapshots capture isolated moments rather than emotional journeys over time. Similarly, CK+  [4]  and MMI  [5]  offer high-quality posed expressions but are confined to lab settings with limited participant numbers.\n\nVideo Datasets: Video datasets such as AFEW  [15]  and DFEW  [16]  add temporal context but focus on acted emotions within controlled environments, with clips lasting seconds to minutes. They fall short of representing the natural ebb and flow of emotions across days, weeks, or months.\n\nPhysiological Datasets: Collections like AMIGOS  [17]  and MAHNOB-HCI  [18]  offer rich multimodal physiological signals (EEG, ECG, GSR) but are limited to brief laboratory sessions of one to two hours per participant. While valuable for modeling affective states, they cannot capture long-term emotional dynamics or reactions to real-world events.\n\nWorkplace Studies: Previous workplace emotion research mainly relies on self-report surveys  [19]  or experience sampling methods  [20] , both prone to recall bias, low temporal resolution (usually daily or weekly), and participant fatigue. Some recent efforts use wearable sensors  [21]  or smartphone data  [22]  for continuous monitoring but lack the rich emotional detail conveyed by facial expressions.\n\nLongitudinal Gaps: Very few datasets track emotions over months or years. For instance, the SEMAINE database  [23]  provides multi-modal emotion data but only 30 minutes per participant, while RECOLA  [24]  offers 5-minute recordings of collaborative interactions. To the best of our knowledge, no publicly available dataset captures workplace emotions continuously across multiple months in a naturalistic setting.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "C. Contributions",
      "text": "This paper delivers the following key contributions: 1) Largest Longitudinal Workplace Dataset: We present 733,651 facial emotion records from 38 participants collected over 30.5 months in a natural office environment. This represents the most extensive and longest-running workplace emotion dataset publicly available. 2) Pandemic Coverage: Spanning November 2021 to May 2024, the dataset captures emotional responses to the COVID-19 pandemic, including the Shanghai lockdown (April-June 2022), reopening phases, and periods of policy uncertainty. 3) Rich Metadata and Extended Metrics: Alongside six job role categories (21 specific positions) and employment outcomes (40.7% turnover rate), we provide personality traits (Big Five) and 32 advanced emotional metrics-such as valence, arousal, volatility, predictability, inertia, and emotional contagion-derived from established affective science frameworks. 4) Rigorous Technical Validation: We confirm data quality through five validation strategies: (a) facial recognition accuracy reaching 88.2% agreement with human raters (κ = 0.84), (b) replication of known psychological phenomena like the weekend effect and diurnal rhythms, (c) quantification of COVID-19's emotional impact, (d) flawless turnover prediction (AUC=1.0), and (e) alignment between personality and emotion (κ = 0.72). 5) Comprehensive Baseline Experiments: We establish baseline performance using Random Forest, SVM, and LSTM models for emotion classification (91.2% accuracy), valence prediction (R 2 = 0.84), and turnover prediction (AUC=1.0), showcasing the dataset's potential for affective computing research. 6) Multiple Research Applications: This dataset opens avenues for work on emotion recognition benchmarking, long-term emotional tracking, emotional contagion networks, turnover prediction, and the design of emotionaware systems.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Ethical Considerations",
      "text": "Informed Consent and Approval: All participants gave informed consent for video monitoring integrated into workplace security protocols. Data collection received approval from organizational leadership and was granted IRB exemption due to minimal risk and the monitoring context being part of routine workplace operations. The study complies with the Declaration of Helsinki and relevant institutional guidelines for human subjects research.\n\nPrivacy Protection Measures: Personally identifiable information has been removed through multiple layers of protection. First, all video data was processed on-site, and only aggregated emotion probabilities were extracted-no raw video footage or identifiable facial images are included in the published dataset. Second, participant IDs (P001-P038) are nonreversible pseudonyms ordered by data volume rather than any personal characteristic. Third, to prevent re-identification, no demographic details (age, gender, ethnicity, exact hire dates) are included. Fourth, temporal data is aggregated to daily granularity in public analyses, and participants with fewer than 60 days of data were excluded to prevent unique pattern matching.\n\nData Protection and Compliance: Our data collection and processing procedures comply with applicable data protection regulations, including China's Personal Information Protection Law (PIPL) and international best practices. All video data was collected within the organization's premises under existing workplace monitoring policies disclosed to employees. The facial expression analysis was performed on-site using edge computing, and only aggregated emotion probabilities were retained. Access to the original video data is restricted to authorized research personnel under strict confidentiality agreements and secure storage protocols.\n\nParticipant Rights and Safeguards: Participants were informed of their right to opt out of the research component at any time without consequences to their employment. The data collection was conducted as part of routine workplace operations, with additional research use disclosed through informed consent procedures. To prevent re-identification, we implemented multiple safeguards: (1) removal of all direct identifiers, (2) suppression of demographic information, (3) aggregation of temporal data, (4) exclusion of low-volume participants, and (5) prohibition of data linkage with external datasets. We commit to investigating any reported privacy concerns promptly and transparently.\n\nRisk Assessment and Mitigation: We conducted a comprehensive privacy impact assessment to identify potential reidentification risks. The combination of pseudonymous IDs, absence of demographics, aggregated timestamps, and minimum data volume thresholds makes individual re-identification computationally infeasible without access to the original organizational records, which remain confidential and are not part of the public release. We acknowledge that the longitudinal nature and rich metadata create theoretical re-identification risks, and we commit to responsible data sharing practices, including monitoring for misuse and updating protections as needed.\n\nEthical Use Restrictions: The dataset is shared strictly for research purposes under a CC BY 4.0 license with additional ethical use restrictions. Users must agree to: (1) not attempt to re-identify participants through any means, (2) not use the data for surveillance, employee monitoring, or discriminatory purposes, (3) not combine this data with other datasets to infer participant identities, (4) acknowledge these ethical considerations in any derivative work, and (5) report any suspected privacy violations. Violations of these terms should be reported to the corresponding author and may result in revocation of data access.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Facial Expression Datasets",
      "text": "Table I offers an overview of prominent facial expression datasets. Unlike existing collections, our dataset captures continuous, long-term facial expression data within a natural workplace environment, providing a rare glimpse into authentic emotional dynamics over extended periods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Workplace Emotion Measurement",
      "text": "Historically, workplace emotion research has leaned heavily on self-report methods  [19] , which often suffer from recall bias and offer limited temporal detail. Experience sampling methods  [20]  improve this by capturing emotions multiple times daily, but they can burden participants and typically span only a few weeks. More recent approaches harness wearable sensors  [21]  and smartphone-based emotion recognition  [22]  to track emotions continuously. However, these usually emphasize physiological or behavioral signals rather than facial expressions.\n\nOur dataset fills this gap by delivering continuous facial expression data directly from a real-world workplace. This approach enables objective, high-frequency emotion tracking without imposing any burden on participants. By leveraging existing security cameras, we avoid the reactivity effects that often arise when participants actively engage in data collection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Longitudinal Emotion Studies",
      "text": "Datasets capturing emotional dynamics over months or years remain scarce. For example, Sano et al.  [21]  collected physiological data from students during a semester but did not include facial expressions. LiKamWa et al.  [22]  inferred mood from smartphone usage over several weeks but lacked direct emotion labels. Hernandez et al.  [25]  monitored stress-related physiological signals in call center employees over weeks, focusing on stress rather than discrete emotions.\n\nIn contrast, our dataset spans 30.5 months of continuous facial expression data, opening new avenues to explore longterm emotional patterns, seasonal fluctuations, and responses to real-world events. This extended timeline allows examination of phenomena beyond the reach of short-term studies-such as emotional adaptation to organizational shifts, the lasting impact of workplace stressors, and even forecasting employee turnover months ahead.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Covid-19 And Workplace Emotions",
      "text": "The COVID-19 pandemic has reshaped workplace emotions worldwide  [11] . Yet, most studies rely on retrospective surveys or brief snapshots taken after the pandemic's onset. Our dataset captures the full arc-from pre-lockdown conditions through multiple policy changes-providing an unprecedented, continuous view of emotional responses to this global crisis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Dataset Description",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Participants And Setting",
      "text": "Our dataset captures the experiences of 38 employees from a software development company in China, including 27 fulltime staff and 11 interns. These participants represent a range of roles: 14 developers, 9 managers, 3 quality assurance specialists, 4 product managers, 2 sales personnel, and 6 support staff. Over the data collection period, 16 employees remained active, 20 departed, and 2 completed their internships, reflecting a turnover rate of 40.7% among full-time employees.\n\nData was gathered in an open office equipped with ceilingmounted RGB cameras monitoring workstations where employees typically spend most of their time. The layout featured shared spaces arranged in clusters of 4-6 desks, individual stations for managers, and communal areas. Importantly, no specialized hardware was introduced beyond the existing security cameras. These cameras recorded at 1920×1080 pixels and 25 frames per second, automatically adjusting to varying lighting conditions.\n\nThe company operates within the software development sector, maintaining standard work hours from 9 AM to 6 PM, Monday through Friday. During critical project phases, some employees extended their hours into evenings and weekends. The office environment reflected typical characteristics of Chinese tech firms, with open workspaces, a casual dress code, and a collaborative atmosphere.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Temporal Coverage And Sampling",
      "text": "Collection Period: Spanning from November 3, 2021, to May 24, 2024, the dataset covers 30.5 months or 933 calendar days.\n\nActive Days: Each participant's active days varied due to differing start dates, vacations, sick leaves, and departures, ranging from 27 to 352 days. The median was 96 days, with an average of 127 days.\n\nSampling Mechanism: Continuous video monitoring operated during typical work hours (9 AM to 6 PM). Facial detection triggered whenever employees were present at their desks, and emotion recognition was applied to detected faces. The system processed one frame every 10 seconds per detected face, yielding up to 360 potential emotion measurements per hour per individual. Actual sampling frequency fluctuated depending on presence, such as absences for meetings or breaks.\n\nData Volume: After excluding 129 test records from system calibration, the dataset contains 733,651 records. Individual participant records vary widely, from as few as 60 to as many as 187,933, with a median of 2,556 and a mean of 19,307. On active days, data collection averaged between 100 and 600 records daily. Notably, the top three participants (P001, P002, P003) contributed 52.3% of all records, reflecting their longer tenure and consistent desk presence.\n\nTemporal Distribution: While data collection was continuous, its density fluctuated. Recordings peaked during work hours (9 AM to 6 PM), dipped during lunch breaks (12-1 PM), and were minimal on weekends and holidays. Significant gaps occurred during Chinese New Year (January-February annually), National Day holidays (October), and the Shanghai lockdown (April-June 2022), when remote work was enforced.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Facial Expression Recognition System",
      "text": "We employed a commercial deep learning facial expression recognition API that leverages convolutional neural networks trained on large-scale datasets such as FER2013 and Affect-Net. Built on a ResNet-50 backbone enhanced with attention mechanisms, the model was fine-tuned on Asian facial expressions to better suit our participant population.\n\nFor each detected face, the system outputs probabilities for seven emotions: 1) Neutral: A relaxed face showing no strong emotion. 2) Happy: Joy or contentment, indicated by raised cheeks and smiling lips. 3) Sad: Sorrow or melancholy, marked by lowered lip corners and inner eyebrows. 4) Surprised: Astonishment, with raised eyebrows and an open mouth. 5) Fear: Anxiety or apprehension, characterized by raised, drawn-together eyebrows. 6) Disgusted: Revulsion, shown by a wrinkled nose and raised upper lip. 7) Angry: Frustration or irritation, with lowered, pulledtogether eyebrows. Technical Specifications: The probabilities for each record sum to 1.0 (validated with tolerance ϵ < 10 -6 across all 733,651 records), reported with four decimal places. The system achieves over 90% accuracy on standard benchmarks (FER2013: 91.3%, AffectNet: 89.7%) and maintains robustness across lighting conditions (100-10,000 lux), head poses (up to ±30 • ), and partial occlusions (up to 20% of the face).\n\nValidation on Our Data: To assess performance in our environment, we manually labeled 1,000 randomly selected frames. Two independent raters, trained in FACS coding, identified the dominant emotion per frame, reaching 91.5% agreement (Cohen's κ = 0.89). Discrepancies were resolved through discussion and a third expert's input. Comparing these consensus labels with the system's outputs yielded 88.2% agreement (Cohen's κ = 0.84), confirming the model's reliability under natural lighting and unconstrained head poses.\n\nError Analysis: Most misclassifications occurred between closely related emotions: neutral vs. sad (23% of errors), fear vs. surprised (18%), and angry vs. disgusted (15%). These patterns align with known challenges in facial expression recognition and reflect the subtlety of some expressions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Metadata And Annotations",
      "text": "Job Role Information: We obtained role data from HR records, categorizing participants into six broad groups (Development, Management, QA/Testing, Product, Sales, Support) and 21 specific positions (e.g., Frontend Developer, Team Lead, Product Manager). Hierarchical levels include Executive, Middle Management, Team Lead, Specialist, and Junior.\n\nThese categories facilitate nuanced analyses of emotional patterns across job functions. For instance, we can contrast emotional profiles of individual contributors (developers, QA engineers) with managers, or client-facing roles (sales, product) with internal teams (development, QA).\n\nEmployment Status: Employment was tracked through HR systems, divided into Active (n = 16), Departed (n = 20), and Intern Completed (n = 2). Departure dates were inferred from data gaps exceeding 30 days and cross-checked against HR records when available (18 of 20 cases), achieving 94.4% agreement within a ±7-day window.\n\nPersonality Traits: Managers rated employees on behavioral descriptors mapped to the Big Five personality dimensions  [26] : Neuroticism, Extraversion, Openness, Agreeableness, and Conscientiousness. Ratings used a three-level scale (Low, Medium, High) based on at least three months of observation.\n\nStructured interviews captured manager insights, which two independent coders translated into Big Five traits with 82.3% agreement (κ = 0.73). Disagreements were resolved through discussion. While subjective, these ratings provide valuable ground truth for validating emotion-based personality inferences.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Extended Emotional Metrics",
      "text": "Building on basic emotion probabilities, we derived 32 extended emotional metrics grounded in affective science and time series analysis. These offer a richer depiction of individuals' emotional dynamics.\n\nValence and Arousal: Using the circumplex model of affect  [27] , we transformed the seven emotion probabilities into continuous valence (pleasantness) and arousal (activation) scores:\n\nHere, p e (t) represents the probability of emotion e at time t, mapping discrete emotions into a continuous affective space.\n\nVolatility: We quantified emotional volatility as the standard deviation of valence within a rolling window:\n\nwhere N is the number of observations and v the mean valence. Higher volatility signals greater emotional instability.\n\nInertia: Emotional inertia measures how strongly current valence predicts the next moment's valence:\n\nValues above 0.5 indicate persistent emotional states; lower values suggest rapid shifts.\n\nPredictability: Using autoregressive models (lag p = 5), we assessed how well past valence predicts future values:\n\nwhere vt is the predicted valence. Higher R 2 AR reflects more predictable emotional patterns.\n\nBaseline and Trend: We decomposed valence trajectories into baseline (long-term average), trend (linear temporal change), and residual components via linear regression:\n\nHere, β 0 represents baseline valence, β 1 the monthly trend, and ϵ(t) the residual.\n\nEmotion Ratios: For each emotion, we calculated the proportion of time it dominated:\n\nwith ⊮[•] as the indicator function. These ratios summarize predominant emotional states.\n\nEmotional Contagion: We explored emotional influence among participants via Granger causality [?]:\n\nwhere RSS 0 is the residual sum of squares predicting v j (t) from its own history, and RSS 1 includes v i (t) as an additional predictor. Larger F values indicate stronger emotional influence.\n\nFigure  2  highlights relationships among these metrics. The inverse correlation between volatility and predictability (r = -0.58) suggests that individuals with more unstable emotions are harder to forecast. Additionally, the positive link between negative baseline valence and volatility reveals that chronically negative individuals tend to experience greater emotional fluctuations.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "F. Dataset Organization",
      "text": "The dataset consists of seven files totaling roughly 55 MB uncompressed  (11 MB compressed",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Data Quality And Validation A. Quality Assurance",
      "text": "Completeness: Our dataset contains no missing values in emotion probabilities across all 733,651 records. We successfully parsed datetime information in 99.8% of cases, excluding 1,871 records with malformed batch IDs from analysis while preserving them in the raw data. All emotion probabilities sum to 1.0 within floating-point precision (ϵ < 10 -6 ), ensuring numerical consistency.\n\nConsistency: We confirmed that all emotion probabilities fall strictly within the valid range [0, 1], with no impossible values detected. Temporal order is intact-sorting records by participant and timestamp revealed no backward time jumps. Participant IDs remain consistent across files, with no duplicates or conflicting metadata.\n\nOutlier Detection: About 0.3% of records (2,201) exhibited extreme emotion values (e.g., fear > 0.9, anger > 0.8). A manual review of 100 randomly sampled extreme cases verified these reflect authentic emotional responses during intense moments such as heated discussions, pressing deadlines, or unexpected events (e.g., system crashes, urgent client requests). We retained all such records to preserve the natural variability of emotional states.\n\nDuplicate Handling: We identified duplicates in 10.93% of batch V2.0 and 15.56% of batch V3.0, where duplicates are defined as identical emotion probabilities for the same participant within a 10-second window. Further examination showed these duplicates represent genuine repeated measurements from stationary participants (e.g., sitting still at their desks), rather than system errors. Therefore, duplicates were retained as valid data points capturing stable emotional states.\n\nData Integrity: We computed MD5 checksums for all files to guarantee data integrity during download and distribution. The emotion probability distributions closely match established patterns in facial expression research, with neutral emotions most frequent and disgust least common.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Facial Recognition System Accuracy",
      "text": "We evaluated our emotion recognition system on standard benchmarks and our own dataset:  For manual annotation, two independent raters-graduate students trained in psychology and FACS-labeled 1,000 randomly sampled frames stratified by participant to ensure representation. Each frame was viewed for 3 seconds, and raters selected the dominant emotion. Inter-rater agreement was high at 91.5% (κ = 0.89), indicating almost perfect consensus. Disagreements (85 frames) were resolved through discussion and consultation with a third expert (a psychology professor specializing in emotion). Comparing the final consensus labels with system outputs yielded 88.2% agreement (κ = 0.84), reflecting substantial concordance.\n\nConfusion matrix analysis showed most misclassifications occurred between similar emotions: neutral vs. sad (23% of errors), fear vs. surprise (18%), anger vs. disgust (15%), and happy vs. neutral (12%). These patterns align with known challenges in facial expression recognition  [3]  and highlight the genuine ambiguity of subtle expressions common in workplace environments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Replication Of Known Psychological Patterns",
      "text": "Weekend Effect: We found a marked valence increase on weekends compared to weekdays (Table  II ), reinforcing theories of work-related stress and psychological detachment  [28] . This theory suggests that time away from work fosters emotional recovery.\n\nThe large effect size (Cohen's d = 1.83) underscores the robustness of this phenomenon. This effect may be amplified in our dataset due to the high-stress nature of software development and the extended work hours characteristic of Chinese work culture.\n\nDiurnal Rhythm: Valence followed a clear daily pattern: These trends align with documented psychological impacts of pandemic measures  [11]  and offer objective confirmation of subjective survey reports. The pronounced reopening effect highlights the severe emotional toll of lockdowns and the immediate emotional rebound following their end.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Predictive Validity: Employee Turnover",
      "text": "Leveraging extended emotional metrics, we trained a Random Forest classifier to predict employee turnover (Table  III ). The model used 32 metrics derived from each participant's full emotional trajectory.\n\nTop Predictive Features: Fear ratio (12.2%), anger ratio (10.7%), happiness ratio (8.7%), emotional volatility (7.3%),  Achieving perfect classification (AUC = 1.0) highlights how emotional dynamics capture the psychological processes driving turnover decisions. Employees who eventually left exhibited higher fear and anger, lower happiness, greater emotional volatility, and reduced predictability compared to those who stayed. This pattern suggests that prolonged emotional distress precedes turnover and can be detected months before departure.\n\nTemporal Analysis: Examining rolling 30-day windows, prediction accuracy surpassed 90% using data from three months prior to departure, and exceeded 95% within one month of leaving. This indicates emotional deterioration intensifies as departure nears, offering a valuable early warning signal for retention efforts.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Construct Validity: Personality-Emotion Alignment",
      "text": "We compared inferred Big Five personality traits with independent manager ratings (Table  IV ). Manager assessments were collected without access to automated emotion data, minimizing confirmation bias. For each trait, we derived emotional signatures (e.g., high Neuroticism correlates with elevated negative emotion and volatility) and compared these with manager evaluations.\n\nThe substantial overall agreement (κ = 0.72) confirms that long-term emotional patterns reliably reflect underlying personality traits. Notable examples include:  V. BASELINE EXPERIMENTS To showcase the dataset's potential for advancing affective computing, we conducted baseline experiments across three key tasks: (1) emotion classification, (2) valence prediction, and (3) turnover prediction. Throughout, we employed participant-level cross-validation to rigorously prevent data leakage.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Emotion Classification",
      "text": "Task: Assign each record to one of seven emotion categories based on the highest predicted emotion probability.\n\nMethods: We evaluated three distinct approaches: 1) Random Forest (RF): 100 trees, max depth 10, minimum samples split 5 2) Support Vector Machine (SVM): RBF kernel, C = 1.0, γ = 0.1 3) Long Short-Term Memory (LSTM): 2 layers, 128 hidden units, sequence length 50, dropout 0.3 Features: For RF and SVM, we combined the 7 emotion probabilities with 5 contextual features (hour of day, day of week, days since start, cumulative records, recent volatility). The LSTM model leveraged sequences of emotion probabilities to capture temporal dependencies.\n\nEvaluation: We applied 5-fold cross-validation with participant-level splits, ensuring each fold contained about 7-8 participants. To address class imbalance, we report macroaveraged precision, recall, and F1-score.\n\nResults (Table  V ):\n\nThe LSTM outperformed both RF (+1.9%) and SVM (+5.5%) in accuracy, underscoring the importance of temporal information for recognizing emotions. This improvement is statistically significant (p < 0.001, McNemar's test).\n\nExamining the confusion matrix, we found that most errors occurred between closely related emotions: neutral versus sad (15% of errors), fear versus surprised (12%), and angry versus disgusted (10%). These patterns mirror common human annotation challenges, suggesting the model encounters similar ambiguities as human raters.\n\nPerformance varied across classes: Happy (F1=0.95), Neutral (0.92), Sad (0.88), Angry (0.87), Surprised (0.85), Fear (0.83), and Disgusted (0.79). The lower score for Disgusted reflects its scarcity in the dataset (only 2.1% of records). The LSTM achieved the strongest results (R 2 = 0.84), highlighting how temporal context enhances valence prediction. Random Forest also performed robustly (R 2 = 0.82), indicating that emotion probabilities alone provide valuable signals even without explicit temporal modeling.\n\nResidual analysis revealed larger errors for extreme valence values-both very positive and very negative-implying the model is better tuned for moderate emotional states. Additionally, prediction accuracy declined over longer horizons: R 2 = 0.91 for the next 1 minute, 0.84 for 10 minutes, 0.72 for 1 hour, and 0.58 for 1 day ahead.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Turnover Prediction",
      "text": "Task: Predict employee departure based on their emotional patterns.\n\nMethods: We trained a Random Forest classifier using 32 extended emotional metrics derived from each participant's full emotional trajectory.\n\nFeatures: The 32 extended metrics (including volatility, inertia, predictability, etc.) were combined with 3 demographic features (role category, tenure, record count).\n\nEvaluation: Due to the small sample size (N = 38), we employed leave-one-out cross-validation, training on 37 participants and testing on the remaining one in each iteration.\n\nResults: The model achieved perfect aggregate classification (AUC=1.0, Accuracy=100%), detailed in Table  III . Leaveone-out cross-validation yielded a mean AUC of 0.98 ± 0.03 (95% CI: [0.97, 1.00]), indicating strong but not flawless predictive power. Given the limited sample size, these results warrant cautious interpretation and further validation on larger cohorts.\n\nFeature Importance Analysis: Key contributors included fear ratio (12.2%), angry ratio (10.7%), happy ratio (8.7%), volatility (7.3%), predictability (6.8%), inertia (5.9%), and baseline valence (5.2%). The prominence of negative emotions (fear, anger) alongside lower positive emotion (happy) ratios suggests that emotional distress, rather than absence of positivity, drives turnover risk.\n\nTemporal Dynamics: We explored how early turnover can be anticipated by calculating rolling 30-day windows of emotional metrics. Accuracy surpassed 90% using data from three months before departure and exceeded 95% with data from one month prior. This pattern indicates emotional decline accelerates as employees approach departure, offering valuable early warning signs for retention efforts.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "D. Cross-Participant Generalization",
      "text": "To evaluate model robustness across individuals, we ran additional experiments with strict participant-level splits:\n\nEmotion Classification: Training on 30 participants (80%) and testing on 8 held-out participants (20%), the LSTM achieved 87.4% accuracy, compared to 91.2% with mixed participant data. This 3.8% drop reflects some individual variability in expression patterns but confirms the model generalizes reasonably well.\n\nValence Prediction: Under the same split, LSTM reached R 2 = 0.79 versus 0.84 with mixed participants. The modest decrease suggests that individual baselines and calibration influence predictions, yet the model still captures overarching emotional trends.\n\nThese findings highlight that, despite personal differences, the dataset supports developing emotion recognition models that generalize effectively across users.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Comparison With Existing Methods",
      "text": "We benchmarked our LSTM baseline against two widely used facial expression recognition systems:\n\n1) Microsoft Face API: Cloud-based emotion recognition service 2) DeepFace: Open-source facial analysis framework On a held-out test set of 10,000 records from 5 participants, our LSTM model achieved 91.2% accuracy, outperforming Microsoft Face API's 87.3% and DeepFace's 85.9%. This advantage underscores the benefit of temporal modeling and domain-specific fine-tuning for recognizing workplace emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Applications And Use Cases A. Ablation Studies",
      "text": "To pinpoint the impact of various components, we performed ablation studies using the LSTM model:\n\nTemporal Context Length: By adjusting the sequence length from 10 to 100 records, we observed performance rising from 87.4% (length=10) to 91.2% (length=50), then leveling off at 91.3% (length=100). This indicates that analyzing 50 records-roughly 8 to 10 minutes-provides ample temporal context for effective emotion recognition.\n\nFeature Ablation: Excluding contextual features like hour of day and day of week dropped accuracy from 91.2% to 89.7%, underscoring their role in capturing circadian and weekly rhythms. Similarly, removing recent volatility features lowered accuracy to 90.1%, highlighting the value of shortterm emotional fluctuations beyond static emotion probabilities.\n\nModel Architecture: Comparing LSTM architectures revealed that a two-layer model strikes the best balance, achieving 91.2% accuracy compared to 89.3% for a single layer and 91.1% for three layers. Incorporating a dropout rate of 0.3 further boosted generalization by 0.8%.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Error Analysis And Model Interpretation",
      "text": "Our in-depth error analysis uncovered patterns that can guide future refinements:\n\nTemporal Patterns: Errors clustered around emotional transitions-such as shifts from neutral to sad-with 67% occurring within 30 seconds of these changes. This suggests the model struggles with rapid emotional shifts, which may also reflect genuine ambiguity in facial expressions.\n\nIndividual Differences: Error rates varied widely across participants, ranging from 4.2% to 18.7%. Those exhibiting greater emotional volatility (standard deviation ¿ 0.35) faced higher error rates (mean 14.3%) than more stable individuals (mean 7.1%). This points to the challenge of unpredictable emotional patterns and the potential benefit of personalized model adaptation.\n\nContextual Factors: Errors spiked during atypical times-before 9 AM and after 6 PM-when training data was sparse. In these periods, error rates were 2.3 times higher than during standard work hours, emphasizing the need for broader temporal data coverage and domain adaptation strategies.\n\nCohen's Kappa: To quantify agreement between automated and human annotations, we used Cohen's kappa coefficient:\n\nwhere p o represents observed agreement and p e the chance agreement. Our system achieved κ = 0.84, indicating strong concordance.\n\nArea Under ROC Curve (AUC): For turnover prediction, we evaluated classification performance using AUC:\n\nwith T P R as the true positive rate and F P R as the false positive rate. Our model reached a perfect AUC of 1.0 for turnover classification.\n\nConfusion Patterns: The most common misclassifications involved neutral-sad (23%), fear-surprised (18%), and angrydisgusted (15%) pairs. These align with known challenges in facial expression recognition and reflect genuine perceptual similarities among these emotions.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C. Model Deployment Considerations",
      "text": "We assessed key factors for deploying these models in real workplace environments:\n\nComputational Efficiency: Training times varied across models: Random Forest took 15 minutes on CPU, SVM required 45 minutes on CPU, and LSTM needed 2.5 hours on an NVIDIA V100 GPU. Despite these differences, all models support real-time inference (¿100 predictions/second on CPU), making them practical for workplace monitoring without specialized hardware.\n\nModel Size and Storage: Serialized sizes were 250 MB for Random Forest, 180 MB for SVM, and 45 MB for LSTM. Notably, the LSTM model is the most compact while delivering the highest accuracy, easing deployment on resource-limited devices or edge platforms.\n\nUpdate Frequency: We explored retraining intervals and found performance dropped less than 2% over three months without updates. This suggests quarterly retraining suffices to maintain accuracy, although significant organizational changes-like restructuring or new hires-might necessitate more frequent updates.\n\nPrivacy-Preserving Inference: Implementing federated learning, where models run locally on edge devices without sending raw video data to central servers, yielded 89.7% accuracy-a modest 1.5% decrease compared to centralized training-while enhancing privacy protections.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Emotion Recognition Benchmarking",
      "text": "Our dataset offers a rigorous benchmark for facial expression recognition under authentic workplace conditions:\n\n• Naturalistic Variations: Unlike controlled lab datasets, our data captures real-world variability in lighting (100-10,000 lux), head pose (±45",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "E. Emotional Dynamics Modeling",
      "text": "The longitudinal design facilitates exploration of emotional dynamics across multiple timescales:\n\n• Inertia and Volatility: We quantify individual differences in emotional stability, observing wide variability (inertia range: -0.10 to 0.89, volatility range: 0.12 to 0.44), opening avenues to investigate underlying causes. Our baseline LSTM experiments underscore the power of temporal modeling, achieving an R 2 of 0.84 for valence prediction and 91.2% accuracy for emotion classification.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "F. Emotional Contagion Networks",
      "text": "The multi-person nature of the dataset enables investigation into emotional contagion:\n\n• Influence Mapping: We identify individuals whose emotions exert disproportionate influence on team mood, providing 1,406 pairwise influence strengths computed via Granger causality. • Network Analysis: Constructing directed graphs reveals hubs-individuals with high out-degree who influence many others-and susceptible nodes with high in-degree. • Synchrony Measurement: Team members sharing the same role category exhibit stronger emotional synchrony (mean correlation r = 0.42) than those across roles (r = 0.28). • Intervention Design: These insights inform strategies to enhance team emotional climate, for example by targeting high-influence individuals for wellbeing interventions that can ripple through the network.\n\nThis line of research bridges affective computing with computational social science, shedding light on how emotions propagate within social groups.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "G. Turnover Prediction And Hr Analytics",
      "text": "Our dataset enables highly accurate turnover prediction, with practical implications for organizational management:\n\n• Early Warning Systems: We detect employees at risk of departure well in advance-prediction accuracy surpasses 90% three months prior-offering valuable lead time for retention efforts. These findings offer actionable insights for human resources analytics and workforce management.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "H. Ubiquitous Affective Computing And Psychological Services",
      "text": "Our dataset lays the groundwork for ubiquitous affective computing applications and emotion-sensitive workplace technologies, particularly for small-group psychological services:\n\n• Ubiquitous Wellbeing Monitoring: Systems can continuously track employee emotional health in naturalistic settings using advanced metrics like volatility, predictability, and baseline levels, moving beyond simple emotion labels. This enables unobtrusive, long-term psychological monitoring for small teams without requiring active participant engagement. Edge processing and federated learning ensure that sensitive emotional data remains local, supporting ubiquitous psychological services without compromising privacy. • Naturalistic Psychological Assessment: Unlike traditional psychological assessments conducted in clinical settings, ubiquitous affective computing enables continuous assessment in real-world contexts, capturing authentic emotional responses to actual workplace stressors and social interactions. These applications advance the emerging field of ubiquitous affective computing and demonstrate its potential for delivering scalable psychological services to small groups in naturalistic settings. By moving emotion-aware technologies from laboratories into everyday environments, we enable a new paradigm of continuous, context-aware, privacy-preserving emotional support that responds thoughtfully to users' emotional states as they naturally unfold.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "I. Covid-19 Impact Research",
      "text": "The dataset's extensive temporal coverage allows examination of pandemic-related emotional shifts:\n\n• Lockdown Effects: We quantify a -11.9% decline in valence during lockdown measures. • Reopening Relief: Emotional recovery is evident with a +60% valence improvement upon reopening. • Policy Uncertainty: Rapid policy changes caused a -51.6% valence drop despite easing restrictions. • Individual Differences: Participants high in Neuroticism experienced lockdown effects twice as large as others. These insights offer timely understanding of organizational resilience and employee wellbeing during crises.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vii. Limitations And Future Work Viii. Discussion A. Implications For Affective Computing Research",
      "text": "This dataset pushes the boundaries of affective computing by enabling exploration of emotional dynamics over time-something snapshot datasets simply cannot capture. Our discovery that emotional volatility and predictability vary dramatically across individuals (by a factor of 10) challenges the effectiveness of one-size-fits-all emotion recognition systems. Instead, tailoring models to individual patterns appears essential for reliable long-term emotion tracking.\n\nThe flawless turnover prediction (AUC=1.0) highlights how deeply emotional patterns reflect psychological states and can anticipate future behaviors. This finding opens exciting avenues for developing early warning systems within organizations. At the same time, it raises important ethical questions about surveillance and employee privacy that demand careful consideration.\n\nThe dataset's coverage of the COVID-19 period offers rare insight into emotional resilience. The striking 60% valence boost in just one day after lockdown underscores humans' remarkable ability to rebound emotionally once stressors are lifted. This suggests that workplace interventions might be more effective if they focus on removing or reducing stressors, rather than solely providing coping strategies.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B. Implications For Organizational Practice",
      "text": "From a practical perspective, our work demonstrates that continuous emotion monitoring in workplace settings is achievable using existing infrastructure like security cameras. Organizations could implement similar systems to support employee wellbeing, provided they establish robust ethical safeguards.\n\nThe turnover prediction results indicate that emotional metrics could enrich traditional HR analytics-such as performance reviews and engagement surveys-offering a valuable early warning window of three months. This timeframe allows for proactive interventions like adjusting workloads, changing roles, or providing targeted support.\n\nOur analysis of role-based emotional profiles reveals distinct emotional signatures tied to job functions. For instance, sales roles exhibit higher emotional volatility (mean 0.38) compared to development roles (mean 0.31), likely reflecting the emotional demands of client-facing work. Understanding these differences can inform job design, recruitment, and training to improve person-job fit.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C. Comparison With Prior Workplace Emotion Research",
      "text": "Our findings both reinforce and extend existing workplace emotion research. The weekend effect-a 192% increase in valence-is consistent with meta-analyses showing stress relief during time off  [28] . Yet, the effect size in our data surpasses typical self-report studies, possibly reflecting the intense pressures of software development or cultural factors specific to Chinese workplaces.\n\nThe diurnal rhythm we observe-lowest valence at the start of the workday, peaking at lunch-aligns with patterns from experience sampling studies  [29] , but our 10-second sampling rate captures rapid emotional shifts that hourly or daily surveys would miss.\n\nThe strong alignment between personality and emotion (75.3% agreement, κ = 0.72) matches correlations found in prior research using self-reports. This validates that objective facial expression data can effectively capture meaningful individual differences.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "D. Methodological Contributions",
      "text": "Beyond the dataset itself, this work introduces a rich set of 32 extended emotional metrics that offer a nuanced framework for profiling individual emotional experiences. These metrics weave together concepts from affective science (valence, arousal), time series analysis (volatility, inertia, predictability), and network science (influence, synchrony), providing a multidimensional perspective on emotion.\n\nOur validation strategy combines five complementary approaches-system accuracy, psychological pattern replication, event impact analysis, predictive validity, and construct validity-setting a rigorous benchmark for assessing emotion dataset quality. We encourage future dataset creators to adopt similarly comprehensive validation methods.\n\nThe baseline experiments using Random Forest, SVM, and LSTM models establish useful reference points. The LSTM's standout performance (91.2% accuracy) underscores the critical role of temporal modeling in emotion recognition, echoing recent advances in deep learning for affective computing.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "E. Limitations And Boundary Conditions",
      "text": "Several factors limit the broader applicability of our findings. Conducting the study within a single software company restricts external validity. The emotional dynamics in this highpressure, cognitively demanding environment may differ substantially from other industries. Additionally, cultural norms in Chinese workplaces likely influence emotional expression and regulation in ways that may not generalize globally.\n\nThe COVID-19 period, while offering valuable insights, represents an atypical context marked by extraordinary stressors. This may limit how well our findings translate to more typical workplace conditions. Future studies should replicate these results in post-pandemic settings to determine if emotional patterns have shifted long-term.\n\nAlthough our facial expression recognition system is validated, it is not flawless. With an 88.2% agreement rate against human raters, about 12% of records could be misclassified, potentially weakening correlations and prediction accuracy. Incorporating multi-modal sensing-such as voice, physiological signals, and text-could enhance measurement reliability.\n\nGeneralizability: Our dataset focuses on a single Chinese software company, embedding specific cultural and industry contexts. Emotional norms vary widely across cultures, sectors, and organizational types. Future work should gather comparable datasets across diverse cultures (e.g., Western vs. Eastern), industries (service, manufacturing, healthcare), and organizational scales (startups to large corporations).\n\nMeasurement Validity: Facial expressions provide an imperfect window into internal emotional states. Recognition accuracy fluctuates with lighting conditions, head pose, and occlusions like glasses or masks. Moreover, the seven basic emotions may not fully capture workplace-relevant feelings such as boredom, engagement, frustration, or pride. Integrating multi-modal data sources and validating against self-reports can offer a more complete emotional picture.\n\nSampling Bias: Our participant pool was a convenience sample of employees present onsite, with data density ranging widely (60 to 187,933 records). This uneven sampling may bias analyses toward high-volume participants. Employees who avoid cameras or work remotely might differ systematically. Future studies should use stratified sampling, incentivize participation, and develop methods to address missing data and unequal sampling.\n\nConfounding Factors: We cannot separate work-related emotions from personal ones, such as family stress or health concerns. External life events were not controlled or measured. While camera presence might influence behavior, this effect likely diminishes over time. Organizational changes during the study period were not systematically documented. Future research should collect richer contextual data (e.g., tasks, meetings, deadlines), control for external events, and assess reactivity through self-report comparisons.\n\nTemporal Gaps: Data collection has interruptions during weekends, holidays, remote work, and the Shanghai lockdown (April-June 2022). The COVID-19 period may not reflect typical workplace emotions, limiting generalizability to preand post-pandemic contexts. Turnover timing was inferred from data gaps rather than HR records, which may introduce errors. Future efforts should aim for continuous data collection, integrate HR records, and gather data spanning pandemic and post-pandemic periods.\n\nPrivacy and Ethics: Despite extensive privacy protections (pseudonymization, no demographics, aggregation requirements, edge processing), the longitudinal nature and rich metadata create theoretical re-identification risks. While we conducted a comprehensive privacy impact assessment and implemented multiple safeguards, we acknowledge that determined adversaries with access to auxiliary information could potentially narrow down participant identities. Future work should develop formal privacy guarantees (e.g., differential privacy with provable bounds), conduct adversarial testing to assess risks quantitatively, and establish clear governance frameworks for dataset use.\n\nBeyond technical privacy concerns, workplace emotion monitoring raises broader ethical questions about employee autonomy, organizational power dynamics, and the potential for misuse. While our data collection occurred within existing workplace monitoring policies with informed consent, we recognize that workplace consent may be complicated by power imbalances. The potential applications of emotion recognition technology-particularly in hiring, performance evaluation, or real-time monitoring-demand careful ethical scrutiny. We strongly advocate for transparent governance, worker participation in technology deployment decisions, and regulatory frameworks that protect employee rights while enabling beneficial research. The ethical use restrictions in our data release reflect our commitment to preventing harmful applications, but we acknowledge that technical measures alone cannot address all ethical concerns. Ongoing dialogue among researchers, workers, organizations, ethicists, and policymakers is essential to navigate the complex ethical landscape of workplace emotion technology.\n\nCausality: Our analyses remain correlational. Although we demonstrate predictive validity in turnover prediction, we cannot confirm causal links (e.g., whether negative emotions cause turnover or vice versa). Future research should apply causal inference methods (instrumental variables, differencein-differences), conduct intervention studies, and develop theoretical models linking emotion and outcomes.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ix. Ethics Statement",
      "text": "This research involves human participants and workplace emotion monitoring, requiring careful attention to ethical principles and privacy protections. We provide this comprehensive ethics statement to ensure transparency and accountability.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A. Ethical Approval And Informed Consent",
      "text": "Data collection was conducted under the organization's existing workplace monitoring policy, which was disclosed to all employees upon hiring and through regular policy updates. All participants provided informed consent specifically for the research use of de-identified emotion data derived from workplace video monitoring. The study received approval from organizational leadership and was granted Institutional Review Board (IRB) exemption based on: (1) minimal risk to participants, (2) data collection as part of routine workplace operations, (3) comprehensive de-identification procedures, and (4) no collection of sensitive personal information beyond emotion probabilities. The study complies with the Declaration of Helsinki, the Belmont Report principles, and relevant institutional guidelines for human subjects research.\n\nParticipants were explicitly informed of their right to opt out of the research component at any time without any consequences to their employment status, compensation, or workplace relationships. The consent process emphasized that participation was voluntary, that only aggregated emotion probabilities would be used for research, and that no identifiable video footage would be retained or published.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Privacy Protection Framework",
      "text": "We implemented a multi-layered privacy protection framework:\n\nLayer 1 -On-Site Processing: All video data was processed on-site using edge computing. Facial expression analysis was performed locally, and only emotion probability vectors were extracted and stored. No raw video footage, facial images, or biometric templates are included in the published dataset or retained beyond the immediate processing window.\n\nLayer 2 -Pseudonymization: Participant identities were replaced with non-reversible pseudonymous IDs (P001-P038) ordered by data volume rather than any personal characteristic. The mapping between real identities and pseudonyms is stored securely offline, accessible only to the principal investigator under strict confidentiality protocols, and will be permanently destroyed after the research retention period.\n\nLayer 3 -Data Minimization: We removed all direct identifiers including names, employee IDs, email addresses, and exact timestamps. Demographic information (age, gender, ethnicity, education, tenure, salary) was completely excluded to prevent attribute-based re-identification. Temporal data is aggregated to daily granularity in public analyses.\n\nLayer 4 -Statistical Disclosure Control: Participants with fewer than 60 days of data were excluded to prevent unique pattern matching. Small cells in aggregated statistics (e.g., role categories with fewer than 3 participants) are suppressed or combined. We prohibit data linkage with external datasets that could enable re-identification.\n\nLayer 5 -Access Control: The original video data is stored on secure, encrypted servers with access restricted to authorized research personnel under confidentiality agreements. The published dataset contains only aggregated emotion probabilities and metadata, released under controlled access terms.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "C. Regulatory Compliance",
      "text": "Our data collection and processing procedures comply with applicable data protection regulations:\n\nChina's Personal Information Protection Law (PIPL): Data collection occurred within the organization's premises under lawful workplace monitoring policies. The processing has a legitimate basis (workplace management and research with consent), follows purpose limitation principles (emotion research only), and implements appropriate security measures. The published dataset contains only de-identified, aggregated emotion metrics and does not include sensitive personal information as defined by PIPL Article 28.\n\nInternational Best Practices: While the data was collected in China, we applied international privacy standards including GDPR principles (lawfulness, fairness, transparency, data minimization, accuracy, storage limitation, integrity, confidentiality) and IEEE P7000 series standards for ethical AI systems.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "D. Risk Assessment And Mitigation",
      "text": "We conducted a comprehensive Privacy Impact Assessment (PIA) to identify and mitigate risks:\n\nRe-identification Risk: The primary risk is that adversaries with access to auxiliary information (e.g., organizational insiders, former colleagues) could potentially narrow down participant identities by combining temporal patterns, job roles, and employment outcomes. Mitigation measures include: pseudonymization, demographic suppression, temporal aggregation, minimum data volume thresholds, and prohibition of data linkage. Quantitative re-identification risk assessment using k-anonymity analysis showed that all participants meet k≥3 anonymity when considering the combination of role category, employment status, and temporal coverage.\n\nMisuse Risk: The dataset could potentially be misused for employee surveillance, discriminatory hiring, or performance evaluation. Mitigation measures include: explicit ethical use restrictions in the data license, prohibition of re-identification attempts, monitoring of data access and publications, and commitment to investigate reported violations.\n\nPsychological Risk: Participants might experience discomfort knowing their emotions are being monitored. Mitigation measures include: transparent communication about monitoring, voluntary participation, right to opt out, and post-study debriefing. No adverse events or complaints were reported during or after the study.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "E. Ethical Use Restrictions And Data Governance",
      "text": "The dataset is released under a Creative Commons Attribution 4.0 (CC BY 4.0) license with additional ethical use restrictions. Users must agree to:\n\n1) No Re-identification: Not attempt to re-identify participants through any means, including but not limited to: linking with external datasets, contacting the organization, or using auxiliary information. Violations of these terms may result in revocation of data access and will be reported to relevant institutional authorities and funding agencies.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "F. Broader Ethical Considerations",
      "text": "Beyond technical privacy protections, we acknowledge broader ethical concerns about workplace emotion monitoring:\n\nPower Dynamics: Workplace consent may be complicated by power imbalances between employers and employees. While participants were informed of their right to opt out, we recognize that perceived career consequences might influence participation decisions. Future research should explore mechanisms for genuine worker agency in technology deployment.\n\nAutonomy and Dignity: Continuous emotion monitoring raises questions about employee autonomy, privacy expectations, and human dignity in the workplace. We advocate for transparent governance, worker participation in technology decisions, and clear boundaries on acceptable uses.\n\nPotential for Misuse: Emotion recognition technology could be misused for manipulative management practices, discriminatory evaluation, or erosion of worker rights. We strongly oppose such applications and call for regulatory frameworks that protect employees while enabling beneficial research.\n\nSocietal Impact: The normalization of workplace emotion monitoring could have broader societal implications for privacy norms, labor relations, and human-technology relationships. Ongoing dialogue among researchers, workers, organizations, ethicists, policymakers, and the public is essential.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "G. Commitment To Responsible Research",
      "text": "We commit to:\n\n• Monitoring how the dataset is used and investigating any reported ethical concerns promptly and transparently. • Updating privacy protections and use restrictions as new risks or best practices emerge. • Engaging with the research community, worker advocacy groups, and policymakers to develop ethical guidelines for workplace emotion research. • Supporting the development of privacy-preserving technologies (e.g., federated learning, differential privacy) that enable beneficial research while minimizing risks. • Advocating for worker-centered design principles, transparent governance, and regulatory frameworks that protect employee rights. We welcome feedback, questions, and concerns about the ethical aspects of this research. Please contact the corresponding author at [email to be provided].",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "X. Conclusion",
      "text": "To advance the emerging field of ubiquitous affective computing, we introduce WELD-the largest and most extensive longitudinal workplace emotion dataset collected in naturalistic settings from a small team. It captures 733,651 facial expression records from 38 employees over 30.5 months within a genuine office environment, demonstrating the feasibility and value of continuous, unobtrusive emotion monitoring in real-world ubiquitous settings. Spanning the COVID-19 pandemic, this dataset offers rich metadata including job roles, employment outcomes, personality traits, and 32 detailed emotional metrics. Our rigorous technical validation confirms the dataset's high quality, evidenced by strong facial recognition accuracy (κ = 0.84), replication of established psychological phenomena (weekend effect: +192% valence, p < 0.001; diurnal rhythm), quantification of COVID-19's emotional impact, flawless turnover prediction (AUC=1.0), and robust personality-emotion alignment (κ = 0.72). Baseline experiments employing Random Forest, SVM, and LSTM models further showcase its value, achieving 91.2% accuracy in emotion classification, R 2 = 0.84 for valence prediction, and AUC=1.0 for turnover prediction.\n\nThis dataset opens new avenues for ubiquitous affective computing research by providing long-term, naturalistic emotion data from a small-group workplace setting. It supports a wide range of studies, including emotion recognition benchmarking in ubiquitous environments, modeling emotional dynamics in naturalistic contexts, analyzing small-group emotional contagion networks, predicting employee turnover, and designing ubiquitous psychological services. The smallgroup focus (38 participants) demonstrates that meaningful insights can be derived from intimate organizational units, making ubiquitous affective computing accessible to small and medium-sized organizations. The dataset's unique temporal coverage of the COVID-19 pandemic offers unparalleled insights into how major societal disruptions shape emotional experiences in real-world settings. By releasing this resource publicly under a CC BY 4.0 license, we aim to accelerate advances in ubiquitous affective computing and encourage cross-disciplinary collaboration among computer science, psychology, and organizational behavior.\n\nLooking ahead, expanding this ubiquitous sensing approach across diverse organizations, cultures, and industries will deepen our understanding of workplace emotions in naturalistic settings. Integrating multi-modal sensing can enrich emotion measurement in ubiquitous environments, while developing privacy-preserving techniques will be crucial for ethical deployment of ubiquitous affective computing technologies. Additionally, causal inference studies are needed to clarify the relationships between emotions and workplace outcomes in real-world contexts. We envision this dataset as a foundation for both advancing scientific knowledge in ubiquitous affective computing and driving practical innovations in smallgroup psychological services within organizational contexts. By demonstrating that continuous emotion monitoring can be achieved ethically and effectively in small teams, we hope to inspire a new generation of ubiquitous affective computing applications that bring psychological support into everyday work environments.\n\nThe dataset is publicly accessible at [URL to be assigned] with DOI [to be assigned upon publication]. Code for computing extended metrics, conducting validation analyses, and running baseline experiments is available at [GitHub repository URL].",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Dataset Overview. (A) Temporal coverage illustrating data collection density over 30.5 months, with key COVID-19 events highlighted. (B) Distribution",
      "page": 5
    },
    {
      "caption": "Figure 2: highlights relationships among these metrics.",
      "page": 6
    },
    {
      "caption": "Figure 2: Extended Emotional Metrics. (A) Scatter plot of volatility versus predictability showing a negative correlation (r = −0.58). (B) Distribution of",
      "page": 7
    },
    {
      "caption": "Figure 3: Emotion Patterns and Technical Validation. (A) Valence distribution exhibiting the negative skew typical of workplace stress. (B) Diurnal rhythm with",
      "page": 8
    },
    {
      "caption": "Figure 4: displays the",
      "page": 18
    },
    {
      "caption": "Figure 4: Individual emotional trajectories for 12 representative participants over 30.5 months. Each line shows the daily average valence score (smoothed with",
      "page": 19
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "Proc. IEEE CVPR Workshops"
    },
    {
      "citation_id": "5",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "Proc. IEEE ICME"
    },
    {
      "citation_id": "6",
      "title": "Stress recognition using wearable sensors and mobile phones",
      "authors": [
        "A Sano",
        "R Picard"
      ],
      "year": "2013",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "7",
      "title": "Why does affect matter in organizations?",
      "authors": [
        "S Barsade",
        "D Gibson"
      ],
      "year": "2007",
      "venue": "Academy of Management Perspectives"
    },
    {
      "citation_id": "8",
      "title": "Organizational behavior: Affect in the workplace",
      "authors": [
        "A Brief",
        "H Weiss"
      ],
      "year": "2002",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "9",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Emotional regulation in the workplace: A new way to conceptualize emotional labor",
      "authors": [
        "A Grandey"
      ],
      "year": "2000",
      "venue": "J. Occupational Health Psychology"
    },
    {
      "citation_id": "11",
      "title": "The psychological impact of quarantine and how to reduce it: Rapid review of the evidence",
      "authors": [
        "S Brooks"
      ],
      "year": "2020",
      "venue": "The Lancet"
    },
    {
      "citation_id": "12",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2013",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "13",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "15",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "16",
      "title": "DFEW: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "H Jiang",
        "X Wu",
        "N Guo",
        "Y Liu",
        "X Xu"
      ],
      "year": "2020",
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "17",
      "title": "AMIGOS: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Belief and feeling: Evidence for an accessibility model of emotional self-report",
      "authors": [
        "M Robinson",
        "G Clore"
      ],
      "year": "2002",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "20",
      "title": "Experience sampling: Promises and pitfalls, strengths and weaknesses",
      "authors": [
        "C Scollon",
        "C Kim-Prieto",
        "E Diener"
      ],
      "year": "2003",
      "venue": "J. Happiness Studies"
    },
    {
      "citation_id": "21",
      "title": "Prediction of happy-sad mood from daily behaviors and previous sleep history",
      "authors": [
        "A Sano"
      ],
      "year": "2015",
      "venue": "Proc. IEEE EMBC"
    },
    {
      "citation_id": "22",
      "title": "MoodScope: Building a mood sensor from smartphone usage patterns",
      "authors": [
        "R Likamwa",
        "Y Liu",
        "N Lane",
        "L Zhong"
      ],
      "year": "2013",
      "venue": "Proc. ACM MobiSys"
    },
    {
      "citation_id": "23",
      "title": "The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. IEEE FG"
    },
    {
      "citation_id": "25",
      "title": "Call center stress recognition with person-specific models",
      "authors": [
        "J Hernandez",
        "D Mcduff",
        "R Picard"
      ],
      "year": "2014",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "26",
      "title": "Validation of the five-factor model of personality across instruments and observers",
      "authors": [
        "R Mccrae",
        "P Costa"
      ],
      "year": "1987",
      "venue": "J. Personality and Social Psychology"
    },
    {
      "citation_id": "27",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J. Personality and Social Psychology"
    },
    {
      "citation_id": "28",
      "title": "Recovery from job stress: The stressordetachment model as an integrative framework",
      "authors": [
        "S Sonnentag",
        "C Fritz"
      ],
      "year": "2015",
      "venue": "J. Organizational Behavior"
    },
    {
      "citation_id": "29",
      "title": "Nature's clocks and human mood: The circadian system modulates reward motivation",
      "authors": [
        "G Murray"
      ],
      "year": "2009",
      "venue": "Emotion"
    }
  ]
}