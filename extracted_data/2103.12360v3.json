{
  "paper_id": "2103.12360v3",
  "title": "Discovering Emotion And Reasoning Its Flip In Multi-Party Conversations Using Masked Memory Network And Transformer",
  "published": "2021-03-23T07:42:09Z",
  "authors": [
    "Shivani Kumar",
    "Anubhav Shrimal",
    "Md Shad Akhtar",
    "Tanmoy Chakraborty"
  ],
  "keywords": [
    "Emotion Recognition",
    "Emotion-Flip Reasoning",
    "Multi-Party Conversations"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Efficient discovery of a speaker's emotional states in a multi-party conversation is significant to design human-like conversational agents. During a conversation, the cognitive state of a speaker often alters due to certain past utterances, which may lead to a flip in their emotional state. Therefore, discovering the reasons (triggers) behind the speaker's emotion-flip during a conversation is essential to explain the emotion labels of individual utterances. In this paper, along with addressing the task of emotion recognition in conversations (ERC), we introduce a novel task -Emotion-Flip Reasoning (EFR), that aims to identify past utterances which have triggered one's emotional state to flip at a certain time. We propose a masked memory network to address the former and a Transformer-based network for the latter task. To this end, we consider MELD, a benchmark emotion recognition dataset in multi-party conversations for the task of ERC, and augment it with new ground-truth labels for EFR. An extensive comparison with five state-of-the-art models suggests improved performances of our models for both the tasks. We further present anecdotal evidence and both qualitative and quantitative error analyses to support the superiority of our models compared to the baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Early studies in the area of emotion analysis  [1, 2]  primarily focused on emotion recognition in standalone text  [3, 4, 5]  which has been shown to be effective in a wide 1 arXiv:2103.12360v3 [cs.CL] 31 Dec 2021 range of applications such as e-commerce  [6] , social media  [7, 8] , and health-care  [9] .\n\nRecently, the problem of emotion analysis has been extended to the conversation domain -usually dubbed as Emotion Recognition in Conversation, aka ERC  [10] , where the inputs are no longer standalone; instead, they appear as a sequence of utterances uttered by more than one speaker. The aim of ERC is to extract the expressed emotion of every utterance in a conversation (or dialogue).\n\nMotivation. A thorough analysis of the ERC task points to multiple important research avenues beyond traditional emotion recognition. One such avenue is the explainability of the speaker's emotional dynamics within a conversation. Our emotional state changes very often as human beings, which is universal for any conversation among multiple interlocutors. In general, the flip in emotion usually happens due to two factors -implicit (or external) and explicit (or internal). The implicit factor corresponds to the external or unknown instigator, and it is extremely difficult to identify the exact reason for an emotion-flip. For example, someone's emotion can change without any verbal signal.\n\nOn the contrary, the explicit factor always has some instigators, e.g., another speaker's visual or verbal signals that trigger the flip in a person's emotion.\n\nExploring reasons behind emotion-flips of a speaker has wide variety of applications.\n\nFor example, a dialogue agent can utilize this as feedback for the response generation, as and when, it senses an emotion-flip due to one of its generated responses. A positive emotional-flip (e.g., sadness → joy) can be treated as a reward, whereas the system can penalize the agent for a negative emotional-flip (e.g., neutral → angry). A few researchers like Lin et al.  [11]  explored the domain of empathetic response generation.\n\nIn their work, they first captured the user emotions to get an emotion distribution which is used to optimize the response. On similar lines, Shin et al.  [12]  assigned a higher reward to a generative model if it generated an utterance that improved the user's sentiment. They proposed a new reward function under the reinforcement learning framework to do so. Ma et al.  [13]  highlighted more such systems in their survey paper. Young et al.  [14]  used audio context to train a generative model. They first learnt an auxiliary response retrieval task for audio representation learning followed by discovering additional context by fusing word-level modalities. Other than empathetic response generation, another possible application of identifying the triggers for an emotion-flip is in the domain of affect monitoring. An organization or an individual can reason upon the emotion-flip in a conversation and make an informed decision for a downstream task.\n\nNovel Problem Definition. To this end, we introduce a novel problem, called Emotion-Flip Reasoning, aka EFR, in conversations. The goal is to find all utterances that trigger a flip in emotion of a speaker within a dialogue. A few example scenarios are presented in Figure  1 . The first dialogue in Figure  1a  exhibits five emotion-flips, i.e., u 1 (neutral) → u 3 (joy), u 2 (neutral) → u 4 (joy), u 4 (joy) → u 6 (sadness), u 5 (joy) → u 7 (sadness), and u 6 (sadness) → u 8 (neutral); and the utterances which trigger the emotion flips are u 3 , u 3 , u 6 , u 6 and u 7 , respectively. Note that some emotion-flips might not be triggered by other speakers in the dialogue; instead, the target utterance can act as a self-trigger.\n\nWe show such a scenario in Figure  1d  in which utterance u 3 is the only reason for the emotion-flip observed. On the other hand, Figure  1b  shows a case where more than one trigger is instigating an emotion-flip while Figure  1c  presents an example where more than two speakers are involved in the conversation. In such a case, the trigger can come from any of the speakers' utterances.\n\nFormally, the EFR task can be defined as follows: Let U = [u 1 , u 2 , ..., u n ] be a  effectively fuses the dialogue-level global conversational and speaker-level local conversational contexts to learn an enhanced representation for each utterance. Furthermore, we employ a memory network  [15]  to leverage the historical relationship among all the previous utterances and their associated emotions as additional information. We hypothesize that the memory content at state t can model the dialogue-level emotional dynamics among the speakers so far. Thus, it can be a vital piece of information for the future utterances corresponding to the states t + y, where y = 1, utterance, we predict trigger/non-trigger for all the past utterances.\n\nFor evaluation, we augment MELD  [16] , a benchmark ERC dataset with groundtruth EFR labels. The resultant dataset, called MELD-FR, contains 8, 387 trigger utterances for 5, 430 emotion-flips. We evaluate our models on MELD and MELD-FR for the ERC and EFR tasks, respectively. We also perform extensive ablation and comparative studies with five baselines and different variations of our models and obtain state-of-the-art results for both tasks. A side-by-side diagnostics and anecdotes further explore the errors incurred by the competing models and explain why our models outperform the baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contributions.",
      "text": "Our major contributions are four-fold:\n\na. We propose a novel task, called emotion-flip reasoning (EFR), in the conversational dialogue.\n\nb. We develop a new ground-truth dataset for EFR, called MELD-FR.\n\nc. We benchmark MELD-FR through a Transformer-based model and present a strong baseline for the EFR task.\n\nd. We develop a masked memory network based architecture for ERC, which outperforms several recent baselines.\n\nReproducibility. The code and dataset are available at https://github.com/ LCS2-IIITD/Emotion-Flip-Reasoning.\n\nOrganization. The rest of the paper is organized as follows. We present the related works in Section 2. Dataset construction along with the annotation guidelines is presented in Section 3. We describe our proposed methodology in Section 4. Experimental results and error analyses are discussed in Section 5. Finally, we conclude in Section 6\n\nwith immediate future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Interpretability of emotion recognition in the linguistic domain is a relatively new research direction, and only a handful of studies have addressed this issue. Lee et al.  [17]  studied the cause of the expressed emotion in a text, usually coined as 'emotion-cause analysis.' The task is to identify a span in the text that causes a specific emotion. For example, in the sentence 'waiting at the airport is awful, but at the end, I get to see my son.', the joy emotion is caused by the phrase 'I get to see my son.' In contrast, the disgust emotion is caused by the phrase 'waiting at the airport.' Gui et al.  [18]  explored the linguistic phenomenon of the emotion cause and developed an SVM-based model for cause extraction. Poria et al.  [19]  proposed a hybrid model to explain sarcastic sentences.\n\nAt the abstract level, the two tasks, namely emotion-cause analysis  [17] , and emotionflip reasoning may seem related; however, at the surface level, they differ significantly.\n\nThe emotion-cause analysis aims to find a phrase in the text that provides the clues (or causes) for the expressed emotion. In comparison, our proposed EFR task deals with multiple speakers in a conversational dialogue and aims at extracting the cause (or reason) of an emotion-flip for a speaker. In our case, the triggers are one or more utterances from the dialogue history as opposed to phrases. Figure  2  illustrates an example from MELD-FR with annotated EFR and emotion-cause labels. It can be observed that the reason behind the emotion disgust in utterance u 5 is the name of the game which was mentioned in utterance u 1 . On the other hand, the emotion-flip from surprise to disgust (from utterances u 2 to u 5 ) was triggered by the utterances u 1 , u 3 , and u 4 .\n\nEmotion knowledge is a piece of vital information in understanding the actual meaning behind a statement. A lot of studies have been done in this field. Mencattini et al.  [20]  tried to solve the problem of emotion detection in speech by using PLS regression model, optimized according to specific features selection procedures. Researchers also used other modalities like visual cues and electroencephalography (EEG) signals to obtain the emotional knowledge of the actor. Zhang et al.  [21]  suggested the use of a variant of the evolutionary firefly algorithm for feature optimization followed by various classical supervised classifiers. Cui et al.  [22]  used EEG signals and learned an end-to-end Regional-Asymmetric Convolutional Neural Network (RACNN) for emotion recognition, which consists of temporal, regional, and asymmetric feature extractors.\n\nHowever, all these models mentioned here work for standalone instances that do not have any context. In the current work, we tackle the task of emotion recognition in conversations, which heavily depends on contextual information.\n\nEmotion recognition in conversation is an active field of study, and several interesting models have been proposed for extracting emotion of every utterance  [10, 23, 24, 25, 26, 27] . Hazarika et al.  [10]  proposed a Conversational Memory Network (CMN)\n\nto leverage the contextual utterances in a dyadic conversation for emotion recognition.\n\nThe authors showed that maintaining the conversational history in a memory helped CMN in predicting emotions more precisely. In comparison, ICON  [23]  models the self and inter-speaker influences in the conversation for emotion recognition. Similar to the CMN architecture, ICON also incorporates a memory network to exploit the contextual information within a dialogue. Ghosal et al.  [26]  aimed to address the issues of context propagation for emotion recognition through an application of graph convolutional network. The inter-speaker dynamics are effectively handled through a complete graph, where each vertex represents an utterance in the dialogue, and the edge-weight reflects the correspondence between two vertices (or utterances). Zhong et al.  [25]  proposed to infuse the commonsense knowledge to improve the prediction capability of the ERC model. They argued that the model leverages the knowledge to enrich the hidden representation. Zhang et al.  [28]  proposed to capture the intra-utterance and inter-utterance interaction dynamics. They used quantum-inspired interactive networks, which leverage the mathematical formalism of quantum theory and the LSTM network, to learn such interaction dynamics. Wang et al.  [29]  modeled the ERC task as sequence tagging. Their architecture used transformer, LSTMs, and CRF to perform the ERC task.\n\nLi et al.  [27]  used a generalized neural tensor block followed by a two-channel classifier for performing context compositionality and sentiment classification, respectively.\n\nHow are our methods different? Following the effective utilization of the memory network for emotion recognition, we also adapt it to leverage the emotional relationship among several interlocutors in this work. However, in contrast to CMN and ICON, we employ it to supplement the global and local emotional dynamics in dialogues captured through a series of recurrent layers. We also handle multiple speakers instead of dyadic conversations, as addressed in many of the existing studies. Moreover, our study advances on the path of explainability by mining the reason behind an emotion-flip of a speaker. To the best of our knowledge, such study has not been carried out in the context of conversation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset Construction",
      "text": "We employ a recently released dataset, called MELD  [16]  and extend it for our EFR task. It consists of 13, 708 utterances spoken by multiple speakers across 1, 433 dialogues from the popular sitcom F.R.I.E.N.D.S.  1  Each utterance has an associated emotion label representing one of Ekman's six basic emotions: anger, fear, disgust, sadness, joy, and surprise along with a label for no emotion, i.e., neutral. Though MELD is a multi-modal dataset, in this work, we employ textual modality only.\n\nFor EFR, we augment MELD with new ground-truth labels (dubbed as MELD-FR).\n\nFor this, we take inspiration from the Cognitive Appraisal Theory by Richard Lazarus  [30]  which states that emotions are a result of our evaluations or appraisals of an event.\n\nWe extend the concept of appraisals and try to identify these in our dialogue instances.\n\nWe consider the utterances that contain possible appraisals as triggers. We employed three annotators who had vast experiences in the ERC task  2  . Below, we explain the steps carried out for the EFR annotation.\n\n1. For each speaker s j , we identify their utterances u sj i in a dialogue where a flip in emotion has occurred, i.e., the speaker's last emotion and the current emotion are different.\n\n2. For each identified utterance u sj i , we analyse the dialogue context and mark all the utterances u k (where 1 ≤ k ≤ i) as triggers that are responsible for the emotionflip in utterance u sj i . For some of the cases, the reason for the emotion-flip of a speaker was not apparent in the dialogue, and the flip was self-driven by the speaker. We leave such cases and do not mark any triggers for them.\n\nAnnotation Guidelines. For annotating triggers, we define a set of guidelines as furnished below. We define a trigger as any utterance in the contextual history of the target utterance (the utterance for which the trigger is to be identified) that follows the following properties:\n\n1. The whole utterance or a part of utterance directly influences a change in emotion of the target speaker.\n\n2. The utterance can be uttered by a different speaker or the target speaker.\n\n3. The target utterance can also be classified as a trigger utterance if it contributes to the emotion-flip of the target speaker. For example, if a person's emotion changes from neutral to sad because of some sad message that she is conveying herself, then the target utterance is the one responsible for the shift.\n\n4. There can be more than one trigger responsible for a single emotion-flip.\n\n5. Since we deal with textual data only, it is possible that the reason behind an emotion-flip is not evident from the data (for example, when the flip occurs due to a visual stimulus). In such cases, no utterance can be marked as a trigger.\n\nWe calculate the alpha-reliability inter-annotator agreement  [31]  between each pair of annotators, α AB = 0.824, α AC = 0.804, and α BC = 0.820. To find out the overall agreement score, we take the average score, α = 0.816. Figure  1  shows a few example scenarios from our dataset. While Figures  1a, 1b , and 1d illustrate the case when two participants are involved in a conversation, Figure  1c  shows an example where more than two speakers are involved. For our work, we considered only those dialogues where speakers experience at least one emotion-flip. After removing dialogues with no emotion-flip, we were left with 834 dialogues in the training set, which account for 4, 001 utterances with emotion flips. These dialogues were annotated by three annotators according to the above guidelines for identifying triggers. Among three annotators, two of the annotators were male, and one was female. All of them were researchers with 3-10 years of experience. They belong to the age group of 30-40. Though we employed three expert annotators in our annotation phase, the process does not require experts (linguistics, social scientists, etc.). Since the annotation guidelines that we provide above are very generic, they can easily be extended to other dialogue datasets by crowdsourcing. We wanted to prepare our labeled dataset as accurately as possible as it would be the first dataset of its kind.\n\nSimilarly, we obtained the trigger annotations for the development and test sets. We show a brief statistic of the datasets in Table  1 . The resultant dataset, called MELD-FR, contains 8, 387 trigger utterances for 5, 430 emotion-flips. We also show the EFR trigger distribution considering their distance from the target utterance in Figure  3  that for the majority of the emotion-flips, the triggers appear in the past few utterances only.\n\nAfter the annotation process, we analyze the directionality of the emotion-flips in MELD-FR. Table  2  shows the statistics of the emotion-flip from the source emotion (row) to the target emotion (column). We make some interesting observations. We consider the set of emotion-joy and surprise as 'positive' and the set-disgust, fear, anger, and sadness as 'negative' emotions. We analyse the emotion flips based on these positive and negative emotion sets. There are in total 2612 emotion flips which result in an emotion from the positive set, whereas 2818 emotion flips result in an emotion from the negative set. Out of these flips, the most prominent emotion-flip pairs are neutral to joy (616) when the resultant (or target) emotion is positive, and neutral to anger (370) when the target is negative. We also analyse the flips occurring from a positive to positive emotions (e.g. joy to surprise) or from negative to negative emotions (e.g.   of the emotion flips that result in a positive emotion originate from neutral (1103), while most emotions flips that result in a negative emotion originate from joy (907). When the target emotion of the emotion-flip is a positive emotion, it is mostly joy (1020), whereas for the negative case, it is mostly anger (757).\n\nWe also observe that the most frequent reasons for positive emotion-flip are excitement, cheer, or being impressed by someone else. For negative emotion-flip, awkwardness, loss, or being annoyed are the frequent reasons.",
      "page_start": 8,
      "page_end": 12
    },
    {
      "section_name": "Proposed Methodology",
      "text": "In this section, we describe our proposed models in detail. Since the input-output mapping of the two tasks is different, we model them differently. For ERC, at each time step t, we learn an emotion label for utterance u t ; whereas, in EFR, we make a sequence of predictions corresponding to each previous utterance u i , where i ≤ t, denoting the trigger/reason behind emotion-flip at the target utterance u t . We employ an utterance-level memory network for ERC, and an instance-level Transformer-based  [32]  encoder for EFR. Figure  4  presents a high-level architecture of both the models.\n\nThe remaining section elaborates on the individual models.  for the target utterance u t . We repeat the same process for each utterance in the dialogue; however, if no emotion-flip is observed for the target utterance u t in the training set, we do not process the error gradients in the backward pass. We show the instance creation process for all the example dialogues (as shown in Figure  1 ) in Table 3. In the first example (Figure  1a ), there are five utterances, u 3 , u 4 , u 6 , u 7 and u 8 , where the emotion of the speakers has flipped; thus, we have created five instances for each target emotion-flip; the corresponding trigger utterances are u 3 , u 3 , u 6 , u 6 and u 7 , respectively. In the second one (Figure  1b ), we show an instance where triggers can come from multiple utterances. Here, the emotion of the speaker has flipped once in utterance u 8 , and the triggers were identified as the utterances u 5 and u 7 . We also show an example when more than two interlocutors are involved in a conversation (Figure  1c ). In the particular example, there are two emotion flips at utterances u 4 and u 5 . The triggers utterances for the same are u 3 and u 4 , respectively. In the last example (Figure  1d ), we show an instance where the emotion-flip is a result of a self trigger, i.e., for the emotion-flip observed at utterance u 3 , the trigger is the same utterance itself.\n\nAfter compiling the EFR instances, we employ the Transformer model for classification. We obtain the encoder output h i for each utterance of an instance, and concatenate it with the encoder output of the target utterance h t , i.e., ∀i, ĥi = h i ⊕ h t .\n\nSince emotion-flip reasoning has a strong correspondence with the emotion label, we supplement each contextual utterance with its emotion label to learn an appropriate representation for the trigger classification. Subsequently, we classify each utterance as trigger/non-trigger for the target utterance.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Emotion Recognition In Conversation -Masked Memory Network (Erc-Mmn)",
      "text": "Recall that a dialogue D can have n utterances spoken by m distinct speakers, and each of these utterances has an associated emotion label E. Unlike EFR, we model ERC as the sequence-labeling problem, where for each utterance in the dialogue sequence, we predict its corresponding label. In our model, we employ m separate speaker-level forward GRUs (sGRU sj ) 3  to capture the utterance pattern of each speaker s j ∈ S.\n\nThe hidden representations of each sGRU sj are arranged in the dialogue order and subsequently fed to cGRU for emotion recognition.\n\nFor each speaker s j ∈ S, we compute a d-dimensional speaker-level hidden representation as follows:\n\nwhere v sj k ∈ V sj denotes the utterance spoken by speaker s j in the dialogue, and p is the number of utterances uttered by speaker s j . Evidently,\n\nUsing stacked GRUs, our model becomes agnostic to the number of speakers present in the dialog. Note that the modeling of a speaker-level GRU, e.g., sGRU sj , is in isolation with other speaker-level GRUs (i.e., sGRU s k , where j = k). However, a natural conversation does not happen in isolation; therefore, to provide a mechanism for the interaction among the speakers and to model the natural conversation, we leverage the dialogue-level context in the learning process of the speaker dynamics. The dialoguelevel context is maintained through a global GRU (gGRU) which is shared across all the speakers within a dialogue. For each utterance u Masked Memory Network. To learn the dialogue conversation efficiently, the role of each utterance u i in the dialogue needs to be carefully examined. Some utterances have lesser importance in the dialogue context, whereas others have a long-lasting effect. In general, there is a higher chance that important utterances may participate in predicting emotions for multiple utterances. In our manual analysis of the dataset, we also observe a reasonable correspondence between a few utterances and the emotional labels for multiple utterances in the dialogue. Hence, we hypothesize that the information regarding these few utterances may be exploited by the future utterances u l , i < l, in the dialogue for emotion prediction. To simulate this, we employ a memory network  [15]  that maintains the information captured during the previous states. At each state t, 4  the memory network learns over the state t -1 memory content through a forward-GRU (mGRU) and updates it according to the current query q t . The updated memory at state t is then utilized by the network in the emotion recognition for the utterance u i (represented by the query q t ). Furthermore, it also acts as input for the next state t + 1.\n\nWe argue that the memory content accumulated over the emotion labels reveals the relationship among previous utterances, and the future utterances leverage it to establish their relationships with the previous utterances.\n\nHere, we employ a masked interactive attention mechanism  [33]  to incorporate the information regarding the current query. For each state t, we compute the masked attention weights β t considering input I t (I t = mGRU(O t-1 )) and the query q t = hi . Subsequently, the attended vector is computed for each hidden representation of input I t .\n\nSince the masked attention weight β t signifies the probability over the first t input hidden representations (i.e., I 1 , I 2 , ..., I t ) and β t = 1, we compute the attended vector for the first t hidden representations only and bypass the rest of the input representations (i.e., I t+1 , ..., I n ). The two sets of representations, i.e., t attended and n -t bypassed, form the memory output O t .\n\nThe attended vectors, i.e., O t [1..t], are then utilized as the memory context at state t for the subsequent processing corresponding to the utterance u i . We apply mean-pooling over O t  [1. .t] to compute ōi , and concatenate it with the enriched speaker-dialogue hidden representation hi for the final predictions, i.e., E i = Sof tmax(ō i ⊕ hi ).",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Model Evaluation",
      "text": "We evaluate our models on MELD-FR. Figure  3  shows the distribution of triggers based on their distance from the target utterance. We notice that most of the triggers are the utterances that are spoken just before the target. This phenomenon corresponds to the natural conversations where an emotion-flip occurs immediately after a trigger statement is said. However, there are cases where the trigger lies beyond the last utterance of the target speaker. After analyzing the distribution carefully, we restrict the context_size = 5 for the experiments involving instance-level EFR classification.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Baseline Methods",
      "text": "In this work, we employ a set of baseline methods for the comparative study  [10, 23, 26, 34, 35]  • CMN  [10] : It utilizes memory networks to store the speaker-level contextual history within a dialogue. The authors showed that maintaining the conversational history in a memory helped CMN in predicting emotions more precisely. They also used these memories in capturing inter-speaker dependencies.\n\n• ICON  [23] : It maintains a memory network to preserve the interaction between the self and inter-speaker influences in dyadic conversations. It models this interaction into the global memory in a hierarchical way. Finally, the memory is used as a contextual summary which aid in predicting the emotional labels.\n\n• DGCN  [26] : It models the inter-speaker dynamics in a dialogue via a graph convolutional network. This work also leverages the self and inter-speaker dependencies of the participants for modeling conversations. using graphs, the authors claim to have modeled context propagation in an efficient way.\n\n• AGHMN  [34] : It incorporates an attention GRU mechanism that controls the flow of information through a modified GRU cell based on the attention-weights, computed over the historical utterances in a dialogue.\n\n• Pointer Network  [35]  : They are often used to generate output sequence when the length of output sequence depends on the length of the input sequence. Pointer networks have been applied to solve various combinatorial optimization and search problems such as Convex hull, and travelling salesman problem. Here, we use it to map our input sequence of utterances of a dialogue into a sequence of emotions or triggers.\n\nThese baselines are readily suitable for ERC, and a few of them reported their performance on the MELD dataset. However, we reproduced the results of these baselines on MELD-FR using the official repository of each baseline.  5  On the other hand, by definition, for each utterance u i , EFR aims to predict a classification (trigger/nontrigger) label for each of the previous utterances (u 1 , ..., u i ), i.e., the model has to predict a vector of labels of length i. Since EFR is a new task and has no direct baseline model, we extend the above ERC baselines to predict a vector of labels. We augment the output layer with i independent softmax functions corresponding to each contextual utterance to achieve this. We keep the rest of the architecture as the original.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Experimental Setup",
      "text": "We fine-tune BERT  [36]  for ERC and extract its last layer hidden representation as utterance representation. We keep the standard vector dimension of BERT to represent an utterance. All reported results are averaged over 5 runs.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Hyperparameter Tuning",
      "text": "While training our models, we paid specific attention to tuning our hyperparameters appropriately. After manual tuning, the hyperparameters that we found to give us the best results are mentioned in Table  6 . For the masked memory network, the most important hyperparameter to tune was the number of memory hops. We experimented with hops ranging from 1 to 5. We observed that the performance kept increasing up to 4 hops and decreased when we increased the hops to 5. This can be seen in Table  4 .\n\nFor the Transformer-based network, an important hyperparameter was the number of encoder layers. We set the number of encoder layers to 6 as they gave the best results.\n\nLess than six layers gave a slightly lesser performance, whereas as we increased the number of layers from 6, the results more or less remained the same. This is illustrated in Table  5 . Since we use BERT embeddings to represent utterances, the input hidden size for all our models is 768. The hidden dimension remains 768 until the start of classification layers. We used 3 and 5 linear layers for the ERC and EFR tasks for the masked memory networks, respectively. For the Transformer-based model, we employed a single linear layer as our classification layer for both tasks. The size of the hidden representations for all the models can be seen in Table  6 .\n\nERC-MMN 51.9 52.3 53.6 55.7 54.9",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "System",
      "text": "Classification FC layers #Layer Dimensions\n\nEFR-TX 1 1536→2 ERC T rue →EFR Table  7 : Number of classification FC layers used in our models along with the hidden vector size. contain more than 15 utterances. For the EFR task, we set this sequence length equal to 5. We carefully selected this sequence length as more than 95% triggers lie in this distance range from the target utterance. This phenomenon can be seen in Figure  3  of the main text.\n\nOther hyperparameters such as batch size, learning rate, dropout, and maximum epochs were also tuned manually after observing the model's performance with a range of values for them. For instance, if the learning rate for ERC-MMN was set more than 1e -6 , the learning was in a zig-zag fashion, whereas a lower learning rate slowed the learning to a large extent. A similar trend was observed towards the learning rate for the other models too. If we look at the maximum number of epochs mentioned in Table  6 , we can see that the masked memory network converged faster than the Transformerbased model. This may be due to the increased parameters in the Transformer model.\n\nWe mention the maximum epochs required for our models to converge. Some may reach the best performance before these many epochs with the mentioned learning rate. We use dropouts to reduce the effect of overfitting.\n\nWe show the number of FC layers used in our models for classification in Table  7 .\n\nWe also mention the size for the input and output vectors. We set these values after careful considerations and several experiments.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we present our experimental results and comparative study for both ERC and EFR tasks.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Single-Task Learning Framework",
      "text": "In this setup, both tasks are trained and evaluated separately. We evaluate two tasks on both Transformer-based (TX) and masked memory network-based (MMN) architectures. Table  8  summarizes the results. The MMN based systems, i.e., ERC-MMN and EFR-MMN (jointly denoted as (ERC/EFR)-MMN in Table  8 ), obtain F1-scores of 55.78% and 33.42%, respectively. The modeling of EFR-MMN as utterance-level classification follows the same procedure adopted for the baselines of EFR (c.f. Baseline section). Though the MMN architecture yields moderate performance on ERC, it underperforms on the EFR task, possibly due to the way the task was modeled. This motivated us to model EFR as an instance-level classification, as mentioned in Table  3  and the methodology section. Subsequently, we train a Transformer-based architecture EFR-TX and obtain a 44.79% F1-score on the test set. The improvement of more than 11% in F1-score justifies our EFR modeling as instance-level classification. We argue Adam optimizer. We call this model EFR-ERC multi .\n\nUnfortunately, EFR-ERC multi does not benefit much for ERC and a slight improvement of ∼ 1.4% F1-score for EFR compared to EFR-MMN. However, the obtained multitask performance on EFR is below-par compared to EFR-TX. Similar to the earlier case, we attribute this performance drop to the different ways in which we model the tasks. We also perform multitasking on the best baseline, DGCN, where we observe that EFR slightly improves (from 52.9% to 53.0%), whereas we observe a 0.7% drop in ERC (51.8% to 51.1%).",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Other Cascade Models",
      "text": "Along with the experiments mentioned in Table  8 , we also perform other experiments to play around with our architectures, specifically for the EFR task. In Section 5, we explain two types of cascade models for the EFR task (ERC→EFR cas and ERC T rue →EFR). Here, we will show two more cascade models that we tried for the EFR task.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Early-Fusion Cascade:",
      "text": "In this setting, we introduce emotion labels in the input layer of our model. We concatenate the emotion representation (a 7-dimensional one-hot vector) with the utterance representation (a 768-dimensional BERT vector) and then feed it to the transformer-based network. The first row of Table  9  shows the results obtained using this model.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Late-Fusion Cascade:",
      "text": "In this setting, we introduce emotion labels in the penultimate layer of our model. We concatenate the emotion representation (a 7dimensional one-hot vector) with the representation obtained from the transformer encoder. We then feed this representation to the classification layers for classification. The second row of Table  9  shows the results obtained using this model.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Comparative Analysis",
      "text": "We utilize the publicly available implementations of the baselines for the comparative study and report the performance in Table  8 . For ERC, DGCN turns out to be the",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Early Fusion Late Fusion",
      "text": "",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Trigger F1",
      "text": "35.1 51.5 Another critical observation is that due to class-imbalance, all baselines find it difficult to identify the disgust emotion. Similarly, three out of five baselines fail to classify any fear emotion as well. In contrast, our proposed models correctly identify both disgust and fear emotions for at least a few utterances. Furthermore, except for surprise and neutral, our proposed model outperforms the baselines in remaining five emotion labels.\n\nAs shown in Table  8 , the higest performance for ERC is achieved by our model which contains memory network along with stacked GRUs and masked attention. We also obtain similar results for the multitask framework for ERC. We believe this performance boost is the result of the use of memory network in an effective manner. As can be seen from the baseline results, AGHMN and Pointer Network do not perform at par with the others since they do not contain any memory component. While CMN, ICON and our method perform better. Moreover, we notice that the ERC task performs in a comparative fashion in both standalone and multitask settings. On the other hand, our model performs better for the EFR task when emotions are being learnt simultaneously. This suggests that the ERC task assists the EFR task.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Generalizability",
      "text": "To analyze the performance of our model on an out-of-distribution generalization test set, we consider another dataset, IEMOCAP  [37] . It contains crowdsourced conversations revolving around 16 topics. For the construction of our test set, we randomly pick two conversations from each topic. We then create instances from these dialogues as illustrated in Table  3  and manually annotate them with EFR labels (inter-annotator agreement, α = 0.813)  6  . Table  10  gives us a brief statistics of the IEMOCAP-FR dataset. We test our model trained on MELD-FR on IEMOCAP-FR and report the results in Table  11 . For ERC, our model produces the best results. However, the results are significantly less than the results obtained on MELD-FR. This reduction can be attributed to the inherent differences in the dialogues present in the two datasets.\n\nIEMOCAP contains more than 50 utterances in a dialogue on average whereas MELD contains an average of 9 utterances per dialogue. Secondly, the emotion distribution between the two sets also differ in a major way. IEMOCAP does not contain any disgust emotion, and the neutral emotion is not as commonly present in it as it is in MELD-FR.\n\nConsequently, the task of emotion recognition becomes challenging for this dataset.\n\nOn the other hand, our model and the baselines perform surprisingly well for the task of EFR, comparable to the EFR performance on MELD-FR. This performance can be attributed to the fact that even if the emotion distribution differs in IEMOCAP-FR, the distribution of triggers is still very similar to MELD-FR.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Flip",
      "text": "Table  15 : Actual and predicted labels of triggers for a dialogue having five utterances (u 1 , ..., u 5 ) from the test set. There is an emotion-flip for Rachel (neutral→anger) in u 5 and its triggers are u 3 and u 4 . We mark them as triggers because Ross tricked her into believing that his apartment had an elevator and still acted like nothing happened, thus instigating an emotion-flip.\n\nutterances as neutral, out of which only four are correct. In comparison, our proposed model does not misclassify any emotion as neutral in the dialogue. It can be related to the biasness of DGCN towards the majority emotions.\n\nSimilarly, Table  15  shows a dialogue for EFR. There are two speakers, Ross and Rachel, and we observe an emotion-flip (neutral→anger) for Rachel in utterance u 5 considering her previous utterance u 2 . For the target utterance u 5 , actual trigger utterances are u 3 and u 4 . Our proposed model, ERC T rue →EFR, correctly identifies both triggers; however, it also misidentifies one utterance, i.e., u 5 , as trigger. For the same dialogue, DGCN misclassifies one trigger utterance as non-trigger and one non-trigger utterance as trigger.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Conclusion",
      "text": "A change in speaker's emotion is usually conditioned on either internal (within conversation) or external (outside conversation) factors. This paper focused on the internal factors and defined Emotion-Flip Reasoning (EFR) in conversations that highlight all the contextual instigators responsible for an emotion-flip -a first study of its kind in the conversational dialogue.\n\nSince emotion recognition (ERC) and EFR are closely related, we addressed both of them in the current study. We proposed a masked memory network and a Transformerbased architecture for the ERC and EFR tasks, respectively. Further, we leveraged the interaction between the two tasks through a multitask framework. As a by-product of the study, we developed a new benchmark dataset, MELD-FR, for the EFR task. It is an extension of MELD emotion recognition dataset with trigger labels for each emotion flips. In total, we manually annotated MELD-FR with 8,500 trigger utterances for 5,400 emotion-flips. We performed an extensive evaluation on the MELD-FR dataset and compared the experimental results with various existing state-of-the-art systems.\n\nResults suggest that the proposed system obtained better performance as compared to the state-of-the-art.\n\nUse Cases and Future Directions. One of the objectives of ERC is to develop an empathetic dialogue system, where the knowledge of emotion can assist the dialogue agent in generating empathetic responses; thus, the user enjoys a more natural conversation. Our proposed EFR task is the next step towards the same objective. The information of emotion-flips can be viewed as a feedback mechanism to the dialogue agent. If the dialogue agent senses a flip of emotion during the conversation and has the information of possible reason (trigger) of that flip, it can refine its subsequent response.\n\nFor example, if it encounters a negative emotion-flip (neutral to anger), it can refrain from generating a similar (trigger) response to avoid negative emotion-flip. On the other hand, for a positive emotion-flip (anger to joy), the confidence of the dialogue system in generating such responses will increase in the future.\n\nEFR opens a new thread in the explainability of the speaker's emotion-dynamics in conversation, and our effort is just a baby step towards this direction. In the future, we would like to explore emotion-flip reasoning in more detail. Instead of considering the contextual utterances as possible triggers, we can define a label set to capture the extent of appraisals. We will explore this direction in the future. Additionally, we will explore the effect of multimodal signals like audio and video in detecting emotion flips.",
      "page_start": 27,
      "page_end": 27
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The ﬁrst dialogue in Figure 1a exhibits ﬁve emotion-ﬂips, i.e., u1 (neutral)",
      "page": 3
    },
    {
      "caption": "Figure 1: d in which utterance u3 is the only reason for the",
      "page": 3
    },
    {
      "caption": "Figure 1: b shows a case where more than one",
      "page": 3
    },
    {
      "caption": "Figure 1: c presents an example where more",
      "page": 3
    },
    {
      "caption": "Figure 1: Examples of emotion-ﬂip reasoning.",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates an",
      "page": 6
    },
    {
      "caption": "Figure 2: A sample dialogue to illustrate the difference between emotion-cause extraction in conversation",
      "page": 7
    },
    {
      "caption": "Figure 1: shows a few example",
      "page": 10
    },
    {
      "caption": "Figure 1: c shows an example where more",
      "page": 10
    },
    {
      "caption": "Figure 3: We observe",
      "page": 10
    },
    {
      "caption": "Figure 3: Distribution of triggers w.r.t their distance from the target utterance.",
      "page": 12
    },
    {
      "caption": "Figure 4: presents a high-level architecture of both the models.",
      "page": 13
    },
    {
      "caption": "Figure 4: The proposed masked memory network based (ERC-MMN) and Transformer-based (EFR-TX)",
      "page": 13
    },
    {
      "caption": "Figure 1: for the EFR task.",
      "page": 14
    },
    {
      "caption": "Figure 1: ) in Table 3. In",
      "page": 14
    },
    {
      "caption": "Figure 1: a), there are ﬁve utterances, u3, u4, u6, u7 and u8, where",
      "page": 14
    },
    {
      "caption": "Figure 1: b), we show an instance where triggers can",
      "page": 14
    },
    {
      "caption": "Figure 1: c). In the particular example, there are two emotion ﬂips at utterances u4 and u5. The",
      "page": 14
    },
    {
      "caption": "Figure 1: d), we show an instance where the emotion-ﬂip is a result of a self trigger, i.e., for the",
      "page": 14
    },
    {
      "caption": "Figure 3: shows the distribution of trig-",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Disgust",
          "Total": "196"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Joy",
          "Total": "1077"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Surprise",
          "Total": "864"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Anger",
          "Total": "674"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Fear",
          "Total": "209"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Neutral",
          "Total": "1917"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Sadness",
          "Total": "493"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness": "Total",
          "Total": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1a": "Figure 1b",
          "u3\nu4\nu6\nu7\nu8": "u8",
          "{u1, u2, u3}\n{u1, u2, u3, u4}\n{u1, u2, u3, u4, u5, u6}\n{u1, u2, u3, u4, u5, u6, u7}\n{u1, u2, u3, u4, u5, u6, u7, u8}": "{u1, u2, u3, u4, u5, u6, u7, u8}",
          "{0, 0, 1}\n{0, 0, 1, 0}\n{0, 0, 0, 0, 0, 1}\n{0, 0, 0, 0, 0, 1, 0}\n{0, 0, 0, 0, 0, 0, 1, 0}": "{0, 0, 0, 0, 1, 0, 1, 0}"
        },
        {
          "Figure 1a": "Figure 1c",
          "u3\nu4\nu6\nu7\nu8": "u4\nu5",
          "{u1, u2, u3}\n{u1, u2, u3, u4}\n{u1, u2, u3, u4, u5, u6}\n{u1, u2, u3, u4, u5, u6, u7}\n{u1, u2, u3, u4, u5, u6, u7, u8}": "{u1, u2, u3, u4}\n{u1, u2, u3, u4, u5}",
          "{0, 0, 1}\n{0, 0, 1, 0}\n{0, 0, 0, 0, 0, 1}\n{0, 0, 0, 0, 0, 1, 0}\n{0, 0, 0, 0, 0, 0, 1, 0}": "{0, 0, 1, 0}\n{0, 0, 0, 1, 0}"
        },
        {
          "Figure 1a": "Figure 1d",
          "u3\nu4\nu6\nu7\nu8": "u3",
          "{u1, u2, u3}\n{u1, u2, u3, u4}\n{u1, u2, u3, u4, u5, u6}\n{u1, u2, u3, u4, u5, u6, u7}\n{u1, u2, u3, u4, u5, u6, u7, u8}": "{u1, u2, u3}",
          "{0, 0, 1}\n{0, 0, 1, 0}\n{0, 0, 0, 0, 0, 1}\n{0, 0, 0, 0, 0, 1, 0}\n{0, 0, 0, 0, 0, 0, 1, 0}": "{0, 0, 1}"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 6: For the masked memory network, the most",
      "data": [
        {
          "Hops": "1\n2\n3\n4\n5"
        },
        {
          "Hops": "51.9\n52.3\n53.6\n55.7\n54.9"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Encoder layers": "1\n2\n3\n4\n5\n6\n7\n8"
        },
        {
          "Encoder layers": "33.1\n37.7\n40.2\n41.4\n42.6\n44.8\n44.6\n44.5"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Batch-size": "Learning rate",
          "8": "1e-04",
          "128": "5e-07"
        },
        {
          "Batch-size": "Epochs",
          "8": "100",
          "128": "1000"
        },
        {
          "Batch-size": "Dropout",
          "8": "0.5",
          "128": "0.2"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ERC-MMN\nEFR-ERCmulti(ERC)": "EFR-MMN\nEFR-ERCmulti(EFR)",
          "3": "5",
          "1536→768→384→7": "1536→1536→1536→768→384→2"
        },
        {
          "ERC-MMN\nEFR-ERCmulti(ERC)": "ERC-TX",
          "3": "1",
          "1536→768→384→7": "1536→7"
        },
        {
          "ERC-MMN\nEFR-ERCmulti(ERC)": "EFR-TX\nERCT rue →EFR",
          "3": "1",
          "1536→768→384→7": "1536→2"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMN†\nICON†\nDGCN†\nDGCN†\nmulti\nAGHMN†\nPointer Network†": "(ERC/EFR)-MMN\n(ERC/EFR)-TX",
          "0.0\n0.0\n0.0\n0.0\n0.0\n3.0": "20.2\n0.0",
          "48.6\n36.8\n48.1\n39.2\n40.1\n15.1": "48.7\n4.0",
          "54.0\n45.5\n52.9\n43.7\n43.1\n17.0": "50.4\n5.0",
          "33.7\n37.0\n31.6\n37.1\n11.7\n13.1": "42.9\n1.9",
          "8.6\n0.0\n4.5\n0.0\n0.0\n0.0": "9.8\n0.0",
          "75.9\n69.6\n75.8\n71.7\n63.0\n63.2": "71.9\n61.2",
          "19.9\n11.0\n15.5\n12.1\n25.0\n7.0": "29.6\n0.0",
          "51.7\n50.1\n51.8\n51.1\n44.2\n35.1": "55.7\n29.5",
          "37.5\n37.3\n52.9\n53.0\n52.3\n49.0": "33.4\n44.8"
        },
        {
          "CMN†\nICON†\nDGCN†\nDGCN†\nmulti\nAGHMN†\nPointer Network†": "EFR-ERCmulti",
          "0.0\n0.0\n0.0\n0.0\n0.0\n3.0": "18.8",
          "48.6\n36.8\n48.1\n39.2\n40.1\n15.1": "48.6",
          "54.0\n45.5\n52.9\n43.7\n43.1\n17.0": "49.3",
          "33.7\n37.0\n31.6\n37.1\n11.7\n13.1": "43.7",
          "8.6\n0.0\n4.5\n0.0\n0.0\n0.0": "11.2",
          "75.9\n69.6\n75.8\n71.7\n63.0\n63.2": "72.1",
          "19.9\n11.0\n15.5\n12.1\n25.0\n7.0": "32.0",
          "51.7\n50.1\n51.8\n51.1\n44.2\n35.1": "55.7",
          "37.5\n37.3\n52.9\n53.0\n52.3\n49.0": "34.8"
        },
        {
          "CMN†\nICON†\nDGCN†\nDGCN†\nmulti\nAGHMN†\nPointer Network†": "ERCT rue →EFR",
          "0.0\n0.0\n0.0\n0.0\n0.0\n3.0": "-",
          "48.6\n36.8\n48.1\n39.2\n40.1\n15.1": "-",
          "54.0\n45.5\n52.9\n43.7\n43.1\n17.0": "-",
          "33.7\n37.0\n31.6\n37.1\n11.7\n13.1": "-",
          "8.6\n0.0\n4.5\n0.0\n0.0\n0.0": "-",
          "75.9\n69.6\n75.8\n71.7\n63.0\n63.2": "-",
          "19.9\n11.0\n15.5\n12.1\n25.0\n7.0": "-",
          "51.7\n50.1\n51.8\n51.1\n44.2\n35.1": "-",
          "37.5\n37.3\n52.9\n53.0\n52.3\n49.0": "53.9"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMN\nICON\nDGCN\nDGCNmulti\nAGHMN\nPointer Network": "(ERC/EFR)-MMN\n(ERC/EFR)-TX",
          "0.0\n0.0\n0.0\n0.0\n0.0\n0.0": "0.0\n0.0",
          "7.1\n14.2\n15.5\n11.3\n7.3\n12.4": "19.3\n11.9",
          "0.0\n0.0\n2.3\n2.2\n0.0\n0.0": "3.2\n1.3",
          "56.4\n49.7\n54.2\n53.4\n45.8\n32.2": "52.7\n36.5",
          "2.1\n9.1\n6.0\n5.1\n0.0\n2.6": "10.2\n4.2",
          "2.3\n8.0\n11.5\n13.1\n11.2\n6.0": "17.1\n9.5",
          "28.2\n28.4\n30.8\n29.2\n24.4\n18.1": "33.7\n21.2",
          "35.6\n36.1\n49.6\n48.4\n49.3\n44.7": "31.2\n40.1"
        },
        {
          "CMN\nICON\nDGCN\nDGCNmulti\nAGHMN\nPointer Network": "EFR-ERCmulti",
          "0.0\n0.0\n0.0\n0.0\n0.0\n0.0": "0.0",
          "7.1\n14.2\n15.5\n11.3\n7.3\n12.4": "17.5",
          "0.0\n0.0\n2.3\n2.2\n0.0\n0.0": "2.2",
          "56.4\n49.7\n54.2\n53.4\n45.8\n32.2": "51.5",
          "2.1\n9.1\n6.0\n5.1\n0.0\n2.6": "8.3",
          "2.3\n8.0\n11.5\n13.1\n11.2\n6.0": "17.7",
          "28.2\n28.4\n30.8\n29.2\n24.4\n18.1": "31.4",
          "35.6\n36.1\n49.6\n48.4\n49.3\n44.7": "32.8"
        },
        {
          "CMN\nICON\nDGCN\nDGCNmulti\nAGHMN\nPointer Network": "ERCT rue →EFR",
          "0.0\n0.0\n0.0\n0.0\n0.0\n0.0": "-",
          "7.1\n14.2\n15.5\n11.3\n7.3\n12.4": "-",
          "0.0\n0.0\n2.3\n2.2\n0.0\n0.0": "-",
          "56.4\n49.7\n54.2\n53.4\n45.8\n32.2": "-",
          "2.1\n9.1\n6.0\n5.1\n0.0\n2.6": "-",
          "2.3\n8.0\n11.5\n13.1\n11.2\n6.0": "-",
          "28.2\n28.4\n30.8\n29.2\n24.4\n18.1": "-",
          "35.6\n36.1\n49.6\n48.4\n49.3\n44.7": "52.8"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": ""
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": "13/0\n157/110\n12/32\n26/19\n4/0\n104/154\n9/10"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": "7/0\n31/40\n115/87\n32/21\n4/0\n44/80\n5/10"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": "16/0\n32/45\n31/56\n118/74\n6/0\n64/101\n16/7"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": "1/0\n4/5\n5/9\n7/2\n4/0\n14/24\n7/3"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": "28/0\n72/54\n42/44\n50/19\n14/0\n705/808\n32/18"
        },
        {
          "Disgust\nJoy\nSurprise\nAnger\nFear\nNeutral\nSadness\n15/0\n5/5\n4/10\n14/9\n1/0\n19/33\n3/4": "7/0\n18/25\n9/12\n19/17\n6/0\n68/100\n42/15"
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "u1": "u2",
          "Phoebe": "Phoebe",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Go back in time and listen to Phoebe!",
          "sadness": "anger",
          "anger": "anger",
          "joy": "joy"
        },
        {
          "u1": "u3",
          "Phoebe": "Monica",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Alright here’s something. It says to try holding the baby close\nto your body and then swing her rapidly from side to side.",
          "sadness": "neutral",
          "anger": "neutral",
          "joy": "neutral"
        },
        {
          "u1": "u4",
          "Phoebe": "Rachel",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Ok.",
          "sadness": "neutral",
          "anger": "neutral",
          "joy": "neutral"
        },
        {
          "u1": "u5",
          "Phoebe": "Monica",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "It worked!",
          "sadness": "surprise",
          "anger": "surprise",
          "joy": "anger"
        },
        {
          "u1": "u6",
          "Phoebe": "Rachel",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Oh! Oh! No, just stopped to throw up a little bit.",
          "sadness": "sadness",
          "anger": "sadness",
          "joy": "neutral"
        },
        {
          "u1": "u7",
          "Phoebe": "Rachel",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Oh come on! What am I gonna do? Its been hours and it won’t\nstop crying!",
          "sadness": "sadness",
          "anger": "sadness",
          "joy": "neutral"
        },
        {
          "u1": "u8",
          "Phoebe": "Monica",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Umm ’she’ Rach not ’it’, ’she’.",
          "sadness": "neutral",
          "anger": "neutral",
          "joy": "neutral"
        },
        {
          "u1": "u9",
          "Phoebe": "Rachel",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Yeah I’m not so sure.",
          "sadness": "neutral",
          "anger": "neutral",
          "joy": "neutral"
        },
        {
          "u1": "u10",
          "Phoebe": "Monica",
          "Well alright! We already tried feeding her, changing her, burping\nher. Oh! Try this one.": "Oh my god! I am losing my mind!",
          "sadness": "anger",
          "anger": "anger",
          "joy": "anger"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "u1": "u2",
          "Ross": "Rachel",
          "Okay": "Ross didn’t you say that\nthere\nwas an elevator in here?",
          "Context\nFlip": "",
          "neutral": "neutral",
          "N-Trigger": "N-Trigger"
        },
        {
          "u1": "u3",
          "Ross": "Ross",
          "Okay": "Uhh yes I did but there isn’t okay\nhere we go!",
          "Context\nFlip": "",
          "neutral": "sadness",
          "N-Trigger": "N-Trigger"
        },
        {
          "u1": "u4",
          "Ross": "Ross",
          "Okay": "Okay go left left left",
          "Context\nFlip": "",
          "neutral": "surprise",
          "N-Trigger": "Trigger"
        }
      ],
      "page": 28
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "2",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "EmoNet: Fine-grained emotion detection with gated recurrent neural networks",
      "authors": [
        "M Abdul-Mageed",
        "L Ungar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "SemEval-2019 task 3: Emo-Context contextual emotion detection in text",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th Interna-tional Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "5",
      "title": "All-in-one: Emotion, sentiment and intensity prediction using a multi-task ensemble framework",
      "authors": [
        "M Akhtar",
        "D Ghosal",
        "A Ekbal",
        "P Bhattacharyya",
        "S Kurohashi"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Emotion detection in email customer care",
      "authors": [
        "N Gupta",
        "M Gilbert",
        "G Fabbrizio"
      ],
      "year": "2010",
      "venue": "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text"
    },
    {
      "citation_id": "7",
      "title": "Emotion analysis on twitter: The hidden challenge",
      "authors": [
        "L Dini",
        "A Bittar"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)"
    },
    {
      "citation_id": "8",
      "title": "How intense are you? predicting intensities of emotions and sentiments using stacked ensemble",
      "authors": [
        "M Akhtar",
        "A Ekbal",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "9",
      "title": "Fine-grained emotion detection in health-related online posts",
      "authors": [
        "H Khanpour",
        "C Caragea"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "11",
      "title": "Moel: Mixture of empathetic listeners",
      "authors": [
        "Z Lin",
        "A Madotto",
        "J Shin",
        "P Xu",
        "P Fung"
      ],
      "year": "2019",
      "venue": "Moel: Mixture of empathetic listeners",
      "arxiv": "arXiv:1908.07687"
    },
    {
      "citation_id": "12",
      "title": "Generating empathetic responses by looking ahead the user's sentiment",
      "authors": [
        "J Shin",
        "P Xu",
        "A Madotto",
        "P Fung"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Y Ma",
        "K Nguyen",
        "F Xing",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Dialogue systems with audio context",
      "authors": [
        "T Young",
        "V Pandelea",
        "S Poria",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "End-to-end memory networks",
      "authors": [
        "S Sukhbaatar",
        "A Sszlam",
        "J Weston",
        "R Fergus"
      ],
      "year": "2015",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "A text-driven rule-based system for emotion cause detection",
      "authors": [
        "S Lee",
        "Y Chen",
        "C.-R Huang"
      ],
      "year": "2010",
      "venue": "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text"
    },
    {
      "citation_id": "18",
      "title": "Event-driven emotion cause extraction with corpus construction",
      "authors": [
        "L Gui",
        "D Wu",
        "R Xu",
        "Q Lu",
        "Y Zhou"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "A deeper look into sarcastic tweets using deep convolutional neural networks",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "P Vij"
      ],
      "year": "2016",
      "venue": "A deeper look into sarcastic tweets using deep convolutional neural networks",
      "arxiv": "arXiv:1610.08815"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure",
      "authors": [
        "A Mencattini",
        "E Martinelli",
        "G Costantini",
        "M Todisco",
        "B Basile",
        "M Bozzali",
        "C Natale"
      ],
      "year": "2014",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2014.03.019"
    },
    {
      "citation_id": "21",
      "title": "Intelligent facial emotion recognition using moth-firefly optimization",
      "authors": [
        "L Zhang",
        "K Mistry",
        "S Neoh",
        "C Lim"
      ],
      "year": "2016",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2016.08.018"
    },
    {
      "citation_id": "22",
      "title": "Eeg-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2020.106243"
    },
    {
      "citation_id": "23",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "arxiv": "arXiv:1910.04980"
    },
    {
      "citation_id": "25",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "26",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "27",
      "title": "Bieru: bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "W Li",
        "W Shao",
        "S Ji",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Bieru: bidirectional emotional recurrent unit for conversational sentiment analysis",
      "arxiv": "arXiv:2006.00492"
    },
    {
      "citation_id": "28",
      "title": "Quantum-inspired interactive networks for conversational sentiment analysis",
      "authors": [
        "Y Zhang",
        "Q Li",
        "D Song",
        "P Zhang",
        "P Wang"
      ],
      "year": "2019",
      "venue": "Quantum-inspired interactive networks for conversational sentiment analysis"
    },
    {
      "citation_id": "29",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Y Wang",
        "J Zhang",
        "J Ma",
        "S Wang",
        "J Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "30",
      "title": "Stress, appraisal, and coping",
      "authors": [
        "R Lazarus",
        "S Folkman"
      ],
      "year": "1984",
      "venue": "Stress, appraisal, and coping"
    },
    {
      "citation_id": "31",
      "title": "Computing krippendorff's alpha-reliability",
      "authors": [
        "K Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing krippendorff's alpha-reliability"
    },
    {
      "citation_id": "32",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Interactive attention networks for aspect-level sentiment classification",
      "authors": [
        "D Ma",
        "S Li",
        "X Zhang",
        "H Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2019",
      "venue": "Real-time emotion recognition via attention gated hierarchical memory network",
      "arxiv": "arXiv:1911.09075"
    },
    {
      "citation_id": "35",
      "title": "",
      "authors": [
        "O Vinyals",
        "M Fortunato",
        "N Jaitly",
        "Pointer Networks"
      ],
      "year": "2015",
      "venue": "",
      "arxiv": "arXiv:1506.03134"
    },
    {
      "citation_id": "36",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "37",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    }
  ]
}