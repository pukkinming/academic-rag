{
  "paper_id": "2509.04490v1",
  "title": "Facial Emotion Recognition Does Not Detect Feeling Unsafe In Automated Driving",
  "published": "2025-09-01T11:27:47Z",
  "authors": [
    "Abel van Elburg",
    "Konstantinos Gkentsidis",
    "Mathieu Sarrazin",
    "Sarah Barendswaard",
    "Varun Kotian",
    "Riender Happee"
  ],
  "keywords": [
    "Emotion",
    "Perceived Risk",
    "Fear",
    "Comfort",
    "Facial Emotion Recognition",
    "Skin Conductance"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Trust and perceived safety play a crucial role in the public acceptance of automated vehicles. To understand perceived risk, an experiment was conducted using a driving simulator under two automated driving styles and optionally introducing a crossing pedestrian. Data was collected from 32 participants, consisting of continuous subjective comfort ratings, motion, webcam footage for facial expression, skin conductance, heart rate, and eye tracking. The continuous subjective perceived risk ratings showed significant discomfort associated with perceived risk during cornering and braking followed by relief or even positive comfort on continuing the ride. The dynamic driving style induced a stronger discomfort as compared to the calm driving style. The crossing pedestrian did not affect discomfort with the calm driving style but doubled the comfort decrement with the dynamic driving style. This illustrates the importance of consequences of critical interactions in risk perception. Facial expression was successfully analyzed for 24 participants but most (15/24) did not show any detectable facial reaction to the critical event. Among the 9 participants who did, 8 showed a Happy expression, and only 4 showed a Surprise expression. Fear was never dominant. This indicates that facial expression recognition is not a reliable method for assessing perceived risk in automated vehicles. To predict perceived risk a neural network model was implemented using vehicle motion and skin conductance. The model correlated well with reported perceived risk, demonstrating its potential for objective perceived risk assessment in automated vehicles, reducing subjective bias and highlighting areas for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Comfort, and more specifically, perceived safety and trust are key in the public adoption of vehicle automation. The transition to automation, where the driver becomes a passenger, highlights driving style as a crucial element influencing perceived safety and trust. Interfaces informing users of vehicle automation operation can also enhance perceived safety and trust. To design automated vehicle control systems and interfaces to enhance perceived safety and trust, we need validated methods to assess these in the context of automated driving. As outlined in section II-B subjective evaluation is still the golden standard to assess perceived safety and trust. Questions asked during and after experimental sessions can disclose multiple aspects related to perceived risk, trust in automation, understanding of automation, and acceptance including the intention to use automation. However subjective measures are prone to bias, and their collection may influence the user perception in an undesired manner. Hence, objective measures provide a compelling additional approach to measure perceived risk. Several physiological parameters have been shown to correlate with perceived risk. Galvanic Skin Response (GSR) has proven to be a reliable indicator for arousal and stress  [1] ,  [2] . This correlation has also been confirmed in the context of driving  [3] ,  [4] , making GSR a promising objective measure of perceived risk. However, GSR responds similarly to different types of arousal such as feeling unsafe, motion sickness, and workload. The same holds for the measurement of heart activity and pupil diameter. In this respect facial emotion recognition (FER) provides a compelling alternative able to capture multiple emotions. Facial emotion recognition is often based on the discrete set of universal emotions defined by Paul Ekman  [5] . These emotions are expressed with the same facial expressions for all humans, and this set consists of anger, fear, disgust, happiness, surprise, and sadness. Especially fear and surprise are of interest, as these could indicate a person feeling unsafe or uncomfortable. However, as outlined in section II-B, only a few studies explored FER in the context of automated driving.\n\nThis paper contributes to this research area by collecting a comprehensive dataset and analyzing subjective comfort ratings, facial expressions, and GSR aiming to demonstrate the effectiveness of FER in measuring feeling unsafe in automated driving and using GSR as an objective measurement for overall comfort.\n\nAn experiment was conducted in a driving simulator, where participants were subjected to a scenario as if being driven by an automated vehicle. Perceived risk was varied with a 2x2 within-subject experimental design with a calm and dynamic driving style and stopping for a stop sign without a pedestrian and with a crossing pedestrian. We measured continuous perceived comfort via a knob, a camera recorded the face, and physiological data was collected. In particular, we explored whether fear and surprise, as detected with facial emotion recognition, represent periods of high perceived risk. Also we explored if GSR could be used as an alternative to subjective measurements.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. State Of The Art",
      "text": "This section reviews the literature on facial expressions and emotions (section II-A) and current assessment methods of perceived safety and trust in automated driving (section II-B).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Facial Emotion Recognition",
      "text": "As humans, we can intuitively read a human face and recognise emotions such as sadness or anger, even in people we never met before. To label facial features representing emotional state, Paul Ekman proposed the Facial Action Coding System (FACS)  [6] , which is widely used in the field of computer vision  [7] .\n\nFor labelling emotions from facial expressions, one of the most considered sets of general physiologically distinct emotions is proposed by Paul Ekman: anger, fear, disgust, happiness, surprise, and sadness  [7] ,  [5] . Ekman argues that the facial expressions that display these emotions are universal among all humans, no matter when or where they grew up. This approach has been implemented, resulting in \"discrete classification\" where emotion is either classified as neutral or one of the above six emotions. The discrete classification approach is widely accepted, but categorizing facial expressions into discrete basic emotions is not undisputed. An important argument is that emotions vary in intensity and often blend together, making discrete classification incomplete  [8] ,  [9] .\n\nA popular alternative is the circumplex model of affect  [10] . This 2D model features arousal on one axis and valence on the other, where arousal represents the intensity, and valence represents the pleasantness of emotion  [7] .  [11]  used the arousal-valence model to study the effects of emotion on driving task performance and reported a strong non-linear effect of both arousal and valence where performance drops with extremely positive and negative self-assessed emotions. We found no study using facial expression to detect arousal and valence and therefore did not explore the circumplex model in this paper.\n\nRegarding expressed emotions in real-world situations, research indicated another important concept, commonly referred to as \"display rules\"  [12] ,  [13] ,  [14] . Display rules are procedures learned to manage facial expressions for specific emotions, i.e. the displayed arousal  [12] . Research has shown that display rules can differ significantly between cultures, with multiple experiments showing differences between for example Japanese and North American culture when it comes to the intensity of displaying different emotions  [13] ,  [14] . Research on display rules also tends to make use of the basic set of emotions as defined by Ekman. So the universality of this set seems generally accepted. Display rules represent an active (aware) display of emotions and can, therefore, be considered to represent subjective rather than objective measures of emotions. Display rules may be relevant in traffic in eye-contact but seem less suitable to reflect perceived risk in automated driving. Hence display rules are not explored in this paper.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Assessment Of Perceived Safety",
      "text": "The measurement of perceived safety and trust in automated vehicles is predominantly subjective and often through questionnaires during or after experiencing automated driving in a simulator, experiencing a physically automated vehicle or receiving a description of automation without specific experience. However, questionnaire-based research is subject to various biases, including social desirability, misunderstanding, and cognitive dissonance.\n\nQuestionnaires do not provide continuous measurement and may require some interruption of the experiment. To address this, researchers used so-called direct input devices for (semi-) continuous subjective assessment.  [15]  used a slider to measure perceived safety in fog in manual and automated car following.  [16]  evaluated continuous pressing-based and discrete smartphone-based for comfort assessment in an automated driving simulator, where the discrete approach showed better measurement repeatability and lower measurement bias than the continuous approach.  [17]  evaluated perceived discomfort during automated driving in a fixed base simulator with a handheld manual input device. Results show discomfort in critical interactions representing elevated perceived risk.  [18]  evaluated continuous perceived risk through finger press force measurement in critical events during automation use in a simulator and found a good correspondence of peak finger press force with post-event perceived risk.\n\nHowever, both question-based and continuous perceived risk assessments are subjective and, therefore, prone to bias. In addition their assessment may affect the experiment outcome. This underscores the need for objective measurement of perceived risk.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Objective Measurements",
      "text": "A range of studies explored physiological measures providing such objective measures of perceived risk, motion sickness, workload and other states in driving conditions. Below we summarize key findings with a focus on perceived risk in automated driving. The Galvanic Skin Response (GSR), is also referred to as Electrodermal Activity (EDA), or Skin Conductance (SC). GSR reflects the electrical conductance of the skin and detects fluctuations caused by the triggering of sweat glands, which are controlled by the sympathetic part of the autonomic nervous system and cannot be controlled consciously. GSR has been shown to correlate with arousal, workload and attention  [1] . Opposed to other physiological signals like pupil diameter or heart rate, GSR is not controlled by the parasympathetic system. Because of this, it is a promising metric to reflect fear. Heart activity can be recorded with surface electrodes at the thorax (electrocardiography -ECG) or by measuring blood flow or skin temperature. In driving studies, in particular, the ECG has been evaluated, and various measures derived from the ECG are found to correlate to stress, mental effort and physical effort. In particular, the heart rate (HR) and heart-rate variability (HRV) show good correlations but require somewhat longer observation windows as compared to GSR making ECG less suitable to detect brief moments of high perceived risk. Eye activity can also capture cognitive workload and stress through increased pupil diameter and reduced blink rate. Brain activity (Electroencephalography -EEG) can detect a wealth of processes and states, but is rarely applied in simulators and cars due to electromagnetic interference and intrusiveness of the caps and electrode arrays used in high-end systems.\n\nThe previously mentioned research by  [17]  found physiological signals to capture events that provoke moderate to high reported discomfort (e.g. close approach to vehicle driving ahead, intersection with a fast vehicle approaching from the right, approach to red traffic light, entering highway) but not for slowly evolving and long-lasting situations. In particular, they found increased pupil diameter, reduced blink frequency, and reduced HRV in uncomfortable situations. GSR results were inconclusive and dependent on electrode placement. Facial expression analysis was briefly reported to indicate surprise and tension in close approach situations.  [18]  found that pupil diameter correlated with perceived risk during the more risky events. Heart rate also increased during events but no quantified relation between heart rate and perceived risk magnitude was found  [18] .  [19]  showed pupil diameter to increase with perceived risk in an experimental automated vehicle.  [20]  related EEG to emotional states in a self-driving car experiment. Participants experienced five scenarios to measure initial trust, trust escalation, trust reduction, trust mutation, and trust rebuilding. Both subjective rating and EEG showed substantial changes between these phases, where the EEG seemed to reflect changes in trust.\n\nFacial emotion recognition can be non-invasive as it does not require electrodes or specific devices attached to the body.  [21]  analysed facial expression of passengers driven by an automated driving system, a male driver, or a female driver in a moving base driving simulator in critical driving conditions. From the total of 31,859 images 31,180 could successfully be classified (97.8%). The results showed neutral face expressions for 94.3% and happiness for 4.4% of all images. The remaining 1.3% were attributed to sadness (0.9%), surprise (0.2%), contempt (¡0.1%) and anger (¡0.1%) -disgust and fear were classified not at all. These facial expressions differed significantly between the 3 driving styles whereas heart rate variability showed no significant differences.  [22]  implemented facial emotion recognition and evaluated emotions in eight participants when using automation in urban conditions by showing driving videos with varying maneuvers. Sadness was reported most, followed by neutrality, happiness and surprise. Fear was never detected.\n\nSummarizing the above, physiological signals and facial expressions all correlate to the driver's state and may reflect perceived risk in automated driving. In this paper, we will explore facial expression since it has hardly been explored, and collect GSR, ECG and eye activity as objective reference to validate facial expression. We also note that the above studies indicate challenges for all physiological data collection, and in defining baseline levels. Therefore, we also measure continuous comfort ratings as a robust but subjective reference.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Driving Simulator",
      "text": "The driving simulator used, was built on a MOOG Hexapod, located in the Siemens facility in Leuven, with six degrees of freedom (Figure  1 ). The scene is displayed on a wide-screen monitor with a resolution of 3440 by 1440 and a diameter of 34 inch / 86 centimeter. On the bottom right there was a big red emergency stop button for the participant. All participants were made aware of this option, but none used it or mentioned feeling a need to do so. Fig.  1 : Driving simulator with hexapod, chair and wide-screen.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Sensors",
      "text": "During this experiment, measurements from five different sensors were recorded.\n\n1) Comfort-Knob: The Comfort-Knob is a rotational potentiometer to continuously measure the subjective comfort of the participant (Figure  23 ). It is a turning knob where the participant gives a value between 0 and 10. The scale was defined as 0 being extremely uncomfortable, 5 being neutral, and 10 being extremely comfortable. This is based on SAE J1060 for subjective ratings related to ride comfort in motor vehicles, where 0 up to and including 4 is considered unacceptable, 5 is borderline, and 6 to 10 is acceptable  [23] . Except for the knob not turning past the limits of 0 and 10, there was no haptic feedback. Figure  22  was constantly present in the top-middle of the screen to remind the participant which way to turn the knob according to their state. This illustration was made simple to limit distraction. On the device itself is a LED screen that shows the current value in 2 decimals. Participants were instructed not to focus on the exact value, as this would be distracting.\n\n2) Galvanic Skin Response (GSR), heart activity (ECG) & eye tracking: The Galvanic Skin Response was measured with two electrodes at the fingers, heart activity was measured with three electrodes at the thorax, and eye gaze and pupil diameter were measured with a head mounted eye racker as detailed in appendix C.\n\n3) Webcam: The face was recorded via a webcam located above the screen (see Figure  24 ). The frames are in color and saved as a video of 30 frames per second with a resolution of 640 by 480 pixels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Driving Scenario",
      "text": "The scenarios are based on a real-vehicle experiment conducted at the Griesheim airfield where the same driver consistently drove the vehicle interacting with a real pedestrian  [24] .  The experiment scenario starts with a straight line, followed by a wide right-hand corner, right after which a stop sign is placed where a stationary vehicle is facing the other direction. After that, the drive continues along a straight line. A birdseye view of the experiment with pointers and indicators for the different objects and roads can be found in Figure  3 . In the experiment, the participants experienced this scenario in 4 different conditions designed to vary the level of perceived risk. We tested two driving styles, one being more dynamic and one being more calm. Both these styles were experienced with and without a pedestrian crossing at the stop sign. The view of the pedestrian crossing the road from the perspective of the participant can be seen in Figures  2c  and 2d . In terms of dynamics, the only difference between conditions with and without a pedestrian is the stationary period. Other than that the dynamics are the same. This can be seen in the acceleration profiles in Figure  4 . In Figure  5  the time-to-collision (TTC) is plotted for both driving styles. Here it is visible that the dynamic driving style is more critical, with the TTC decreasing faster and reaching a lower value before the vehicle stops.\n\nThe simulator scenarios are created in Simcenter Prescan. These were generated using an in-house real2sim pipeline developed by Siemens. Real2sim refers to the process of translating real-world driving data-captured by a fully instrumented vehicle equipped with lidar, radar, and camera sensors-into high-fidelity simulation environments. As each object in the Simcenter Prescan environment is individually configurable, the reconstructed scenes could be selectively modified; for example, the pedestrian was removed for the no-pedestrian condition. This approach enables controlled variations while maintaining a close link to the original real-world conditions.\n\nThe simulator motion platform was controlled using measured vehicle motion from the track test, including linear accelerations and angular velocities from the IMU installed in the vehicle, forward velocity from the GPS and positions in the Fig.  3 : Birds-eye view of the scene, with the different road types, straight line, and right-hand turn, indicated with blue and yellow boxes respectively. The objects in the scene are indicated with red arrows and letters: A is the ego vehicle in the starting position, B is the stop sign, C is the pedestrian before crossing, and D is the stationary vehicle. Note that the pedestrian is not in the scene for every condition.\n\nGNSS coordinate system that was also used to build the road network in Simcenter Prescan, and create the visualization for the simulator.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Procedure",
      "text": "At first, participants were asked to fill out a form asking for consent followed by their data regarding demographics and pre-existing trust in automated vehicles (see appendix A). Fig.  4 : Linear Acceleration profiles of the four different conditions. The colors are the same as on the birds-eye view in Figure  3 . Blue is the straight line, yellow is the right-hand turn, and the red striped line marks the point where the vehicle stops. If the pedestrian is in the scene, the red part is the stationary time for the pedestrian to cross. Fig.  5 : Time-to-collision with pedestrian for both driving styles. Both axes are in seconds, with the x-axis representing the time point in the run, and the y-axis the TTC. The colors are the same as on the birds-eye view in Figure  3 . Blue is the straight line, yellow is the right-hand turn, and the red striped line marks the point where the vehicle stops. If the pedestrian is in the scene, the red part is the stationary time for the pedestrian to cross.\n\nThe experiment started with two conditions of 'without pedestrian' (dynamic and calm), in random order and without informing the order of the participant. After the two of these conditions, participants were asked to identify each of the driving styles, whether is was calm or dynamic. All but one participant identified these first two runs correctly. Then they were told that these two driving styles would be repeated, and the order would be randomized. They were not informed about the pedestrian crossing the road. This added effect of surprise was intended to stimulate physiological reactions. With the pedestrian, when asked to identify the driving style, all participants were correct.\n\nThen, all four different conditions (calm with pedestrian, calm without pedestrian, dynamic with pedestrian, dynamic without pedestrian) were repeated in random order, where participants were asked to identify the driving style after each run. All participants identified the driving styles correctly when the pedestrian was present. Only seven participants out of thirty-two, incorrectly identified the driving styles without the pedestrian. Before starting these four runs, participants were told that now it was possible that they would impact a pedestrian. This was not true, the presented experiments were identical, but aimed to keep participants engaged rather than assuming an identical and safe performance. However, one participant stated that participants should not have been told about this possibility, as now the participant was \"warned\" about the dynamic driving style being more aggressive. When this participant was told that it was the same run as they had experienced before, the reaction was surprised as the experience was that it was more aggressive.\n\nAfter the last run the participants were assisted in removing the sensors and getting out of the simulator, and asked how they felt about the experience. Afterwards, they were given the tablet again with a Google Form containing the same questions on trust and comfort in automated vehicles as before the experiment.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Questionnaire",
      "text": "Before the experiment demographic data was collected.\n\nParticipants were asked for gender and age, if they had ever experienced a driving simulator before, and how familiar they were with automated driving systems. A specific questionnaire was designed to assess pre-existing trust in automation, as well as post-experiment trust. The questions were based on  [25]  who performed a literature study and a focus group with experts, and iteratively developed a questionnaire on acceptance of automated driving, for SAE level 3 and level 5. We adopted 8 questions from their questionnaire for SAE level 5. Each question was given as a statement, to which the participant would give their answer on a 5-point scale from 1 (fully disagree) to 5 (fully agree). After each question, the option was given to explain the answer. The same questions were presented after the experiment to asses a possible change.\n\nParticipants were asked to elaborate only in case any response had changed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "F. Facial Emotion Recognition",
      "text": "The model selected for this paper is a Dual-Direction Attention Mixed Feature Network (DDAFN) by  [26] . This model achieves start-of-the-art results on multiple datasets, and the developers released both code and the trained weights on GitHub, with the confusion matrices showing the performance on different datasets. They recently released an improved version under the name DDAMFN++ in the same repository, which is used in this paper. It was validated on the Realworld Affective Faces DataBase (RAF-DB)  [27] ,  [28] . This is a dataset with almost 30-thousand images from the internet, annotated with emotion labels by 40 independent persons, making it an extremely valuable dataset to train and validate emotion recognition models. Overall, the performance is good, but the accuracy for Fear and Disgust is relatively low. Disgust is not expected to be relevant for this application, but Fear is. According to the confusion matrix, Fear is relatively often classified as Sad or Surprise. This can be taken into account for the data analyses. They also tested for cross-database performance, and achieved 75.6% accuracy on the RAF-DB with a model that was trained on AffectNet-7. This is a very strong achievement for cross-database performance  [26]  where other models like the PAtt-lite model described below struggle. The cross-database performance suggests this is a suitable model for our application, where it has to classify unseen data.\n\nWe also implemented the lightweight patch and attention network (PAtt-lite) by  [29] . They report state-of-the-art performance on multiple datasets and published code on GitHub, including pre-trained weights which were tested preparing this paper. However on our simulator data, it always predicted Neutral, and even using self-recorded webcam footage while performing multiple very expressive facial expressions, all were labeled as Neutral.\n\nThe Dual-Direction Attention Mixed Feature Network (DDAMFN++) by  [26]  is trained on extracted faces from the dataset, which means using the model also requires to first extracting the faces from the frames. To detect and extract the faces, RetinaFace was used  [30] . Retinaface is the same model as the developers of the DDAMFN++ model used for the preprocessing of their dataset  [26] . Retinaface is state-of-the-art for face detection. It detects 5 facial landmarks: eyes, nose, and the corners of the mouth. These landmarks are given in a dictionary with coordinates, in Figure  6 -upper left the detected facial landmarks are plotted, showingthat the model can detect these also for faces with glasses or facial hair.\n\nAfter this detection, the face has to be cropped out of the frame. The DDAMFN++ model expects an input of size 112 by 112 pixels, which is also enforced by the pre-processing function. The facial area, as detected by RetinaFace, is not always square, and when it is not, it means the extracted face gets deformed a bit by the function. So instead of using the facial area given by RetinaFace, a different approach is used. Pre-defined coordinates are set, of where the facial landmarks should approximately be in the bounding box. Then a spatial transformation matrix is generated that brings the facial landmarks as close to these coordinates as possible, without deforming the image. With this spatial transformation matrix, the bounding box is used to extract the face from the image, which can be seen in Figure  6 -upper right.\n\nThe architecture of the DDAMFN model is explained in more detail by  [26] . To visualize the decisions that CNNbased models make, a technique called Gradient-weighted Class Activation Mapping (Grad-CAM) is applied. This is a technique to visualize the regions in an image that are important for the prediction the model makes by generating a heatmap  [31] . This heatmap is made by taking the gradients of the target class, which in this case is the predicted emotion, with respect to the features of the last convolutional layer. Each feature map is weighted, and by multiplying the feature maps with their weights a map is created highlighting the important areas. Figure  6 -lower right shows the extracted face that is given as input, with the generated heatmap plotted over it, illustrating that the model is well-focused on facial features.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Results",
      "text": "Subjective continuous comfort ratings are presented, confirming that the experiment was successful in eliciting different levels of comfort associated with perceived risk. Then the facial expression recognition results are presented, to answer the main research question. Finally, a comfort prediction model is presented combining vehicle motion and GSR data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Descriptive Analyses",
      "text": "The experiment was conducted with 32 participants. Many said they enjoyed the experience, and only two participants mentioned some unease that could be classified as minor motion sickness, but they did not mention sickness and reported no symptoms that would be classified higher than 1 on the MIsery SCale (MISC).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Continuous Subjective Comfort Ratings",
      "text": "All four conditions were experienced twice by the participants, and all individual continuous comfort ratings are plotted in  Figures 13, 14, 15, and 16 . These figures show a mostly reasonable to good consistency for the two repeated runs within participants. As described in appendix B inconsistent responses and outliers were removed based on dynamic time warping and visual inspection.\n\nThe average and standard deviation over participants after this outlier removal can be found in Figure  7a . All conditions show a highly significant divergence from their baseline (p < .001). This derives from t-tests, using the first 10 seconds as a baseline, where the braking event is not yet anticipated. Hence, the desired effect of eliciting time-varying comfort levels is achieved. In particular, the calm driving style without pedestrian is perceived as much more comfortable compared to the dynamic style with pedestrian.\n\nThe above averaging over participants smoothened out some dynamic effects as individual plots were not perfectly aligned in time. Hence we analysed extreme values in Figure  7b  where each dot represents an extreme value for one run for one participant. The substantial scatter again illustrates a high variety in timing and magnitude of individual responses. The absolute minima per condition are compared in Figure  8 . To quantify the effect of driving styles and pedestrian presence on the minimal perceived comfort, both an ANOVA and a pair-wise t-test test were performed. All effects were highly significant (p¡.008) with the exception of Calm no pedestrian, versus Calm with pedestrian.\n\nThis illustrates a strong interaction where the pedestrian has no effect in the calm driving style, but leads to the most severe discomfort in the dynamic driving style. It was noted by some participants that the car braking timely for the pedestrian with the calm driving style, actually increased their trust. This is also visible in the acquired data, as we see that, even though there is a small dip in the perceived risk, generally comfort increases after the vehicle has successfully stopped. This confirms what was stated by Fig.  7 : Continuous comfort ratings  [20] , that trust can be brought down by undesired behavior of the vehicle, but also be increased again if the vehicle shows trustworthy behavior, like braking in time for the pedestrian.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Facial Emotion Recognition",
      "text": "For facial emotion recognition, we only analyzed the most critical condition, which is the dynamic driving style with the crossing pedestrian. This condition elicits the strongest discomfort during cornering and braking, so if there is any effect on facial expression it will be most evident in this condition.\n\nThe state-of-the-art DDAFN++ model was successfully implemented. Of the 32 participants, 24 were included in the analyses. Exclusions were due to inconsistencies in subjective ratings, as detailed in appendix B, or the inability of the facial recognition software to reliably detect expressions, caused by factors such as participants wearing glasses, looking downward, or skin tone. For the remaining 24, with minimal lighting conditions, the face could always be detected, and expressions were successfully classified with corresponding emotion labels from the universal set of emotions. However for most of these participants (15/24) the facial expression was always classified as Neutral, and they did not show any detectable reaction in their facial expressions in the two repeated events.\n\nExample results are shown in Figures  11  and 12 . The emotion detection model gives a confidence score to each emotion, for each frame in the video. Figure  9  shows the total percentage of each emotion, after accumulating all confidence scores. As expected, Neutral is the dominant emotion. However, it was also expected to see Fear and Surprise during and after the critical event. After Neutral, the most dominant emotions are Happy and Sad. Disgust, Surprise, and Anger contribute very little, where Fear is the least detected emotion of all. We know from previous performance that the DDAFN++ model can sometimes perceive Fear as Sad, but even when we take this into account the contribution of the facial expression for Fear seems to be minimal.\n\nRepresentative emotions as a function of time are shown in Figure  10  and all plots can be found in appendix E. The emotions are plotted together with the time-to-collision, as measure of criticality. The moment during the right-hand turn where the pedestrian enters the visual frame, is marked by a red dot (see Figure  2c ).\n\nFigure  10  shows major differences between the first and the second exposure to the same scenario in four participants and appendix E shows similar differences in other participants. Of the participants showing any reaction, only one participant repeated this reaction with the same intensity during the second round (see 12). Of the others, half showed a similar reaction but less intense, and half did not show a similar reaction at all. This suggests that the element of not knowing what will happen is important. This is important for future research, as it is common to perform repetitions in these type of experiments, which may enhance statistical power, but hamper realism. This also highlights a disparity with the continuous comfort rating which was quite consistent for the two repetitions of each condition.\n\nTo further validate the detected facial expressions, a qualitative analysis was conducted for each participant by crossreferencing the continuous comfort ratings with the recorded video footage. The analysis confirmed that the DDAFN++based emotion detection accurately reflected the visible expressions and did not overlook any apparent facial reactions during the events (see Figures  11  and 12 ).  The plots show emotion confidence scores during the dynamic condition involving a pedestrian in the scene, after applying a moving average filter. The red dot marks the moment when the pedestrian and stop sign enter the participant's visual frame (see Fig.  2c ). Time-to-collision (TTC) is scaled by a factor of 0.1 for visualization purposes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Neural Network For Comfort Prediction",
      "text": "D-A shows that GSR can discriminate the two driving styles, which indicates that GSR is a physiological measurement that can be used for comfort prediction in the tested conditions. Experiments and models have shown that vehicle motion can predict perceived risk as an important component of comfort in automated driving  [18] ,  [32] . Within such models, the time to collision (TTC), defined as distance divided by approach speed, is a well-known surrogate safety measure associated with both actual and perceived risk (see  [33] ,  [18] ). This section presents a model predicting the measured continuous comfort using vehicle motion, TTC and/or GSR as inputs.\n\nAs elaborated in appendix D, GSR is split into Tonic and Phasic components and both are used as possible features for the model to train on, together with the time-to-collision (TTC) and longitudinal and lateral vehicle accelerations.\n\nGSR, TTC and accelerations are passed through their own feature extraction models consisting of three Convolutional layers and two LSTM layers. The outputs of these models are combined in the attention layer. Finally, a fully connected layer is used to obtain a prediction for the continuous comfort rating. The mean absolute error of the prediction versus the true value is used as a loss function to train the model. For an input of 5 seconds of data, which had a frequency of 10Hz, the model outputs one score to predict the perceived risk. As a true label, the score given by the participants at the end of these 5 seconds was taken. The participant perceived risk scores were available in 2 decimal precision but rounded to 1 decimal, as turning the knob by a smaller amount on purpose was practically impossible.\n\nIn pre-processing, the data was resampled to 10Hz and then the sequences of 5 seconds were created. The data was then standardized using a standard scalar, which gives the data zero mean and a standard deviation of 1. For the GSR data, this was not done on all data combined, because this would make the GSR signal for participants with a low mean almost zero. For this reason, the GSR signals were standardized individually before splitting into tonic and phasic components. 20% of the data was set aside as a validation set, and 80% was used as training data. In this split, it was ensured that both sets contained roughly the same distribution for the labels. This can be seen in Figure  40 . We also see that the dataset is not balanced, with much less data near the extremes. To account for this, different strides were applied when sampling the data, resulting in a division shown in Figure  41 .\n\nThe performance of the model was compared for the different features, looking at the loss (mean absolute error) and accuracy on both the fitted training set and the validation set. For the accuracy, a margin of error was given, since it can be considered correct if the model predicts a value close to the label. This margin of error was chosen to be plus or minus 1 around label 5, and linearly building up to 2 on both outer labels, 0 and 10. This variable margin was selected because at the edges the exact number becomes less important, a perceived risk rating below 2 means extremely low perceived risk and above 8 extremely high perceived risk.\n\nThe different feature combinations were compared in Table  I . We see that in terms of accuracy, the best performance is the model that is trained on both vehicle motion and GSR. However, only vehicle motion does reach a slightly lower mean absolute error, although this difference is very small with the model that combines the two. In terms of fitting the model to the training data, using only vehicle motion seems to work the least well, and only GSR provides a better fit than the combined model. Here it should be taken into account, that the accelerations were the same for every run, meaning that also between the training and testing data, there will be a lot of highly similar samples, if not exactly the same. This is generally not desired, as it will make the model less applicable to data outside of this experiment, thus it is not possible to really test its validity. Another thing that can be noted is that because the labels were from different participants, giving different ratings, all while experiencing the same accelerations, the model gets exactly the same input, but with different output labels to converge to. This limits the extent to which the model can fit to the data. This was confirmed when trying to run the model for 1000 epochs, which improved the fitting on the vehicle motion data only slightly both for the MAE (-0,023) and the accuracy (+1.4%). When doing the same with the model only trained on GSR data (which varies between participants), a much bigger improvement was found on both the MAE (-0,248) and accuracy (+7.1%). This shows that to predict individual perceived risk levels, it helps to incorporate physiological data, and GSR is a viable predictor. TTC is another feature that always shows the same pattern in time within a driving style, because TTC is taken from environmental variables and so does not change between participants. In this NN a time varying TTC was only defined when the pedestrian was visible (in all other cases it was set at its maximum of 10 s). It can be seen that TTC has no positive effect on the performance, it just results in more overfitting with an improved fit of the training set but a detoriated fit of the test set. TTC was thus left out in further processing. GSR could be used to discriminate between the two driving styles, which was shown to be the biggest factor between low perceived risk and high perceived risk drives, and it shows promising results for predicting individual perceived risk levels. To demonstrate the performance on individual data, Leave-One-Out-Cross-Validation (LOOCV) was applied. The model was trained omitting data from one participant, and then correlated to the omitted data. For the GSR only model, all correlations were positive, and 11/14 were significant (p¡0.01). For only vehicle motion, all correlations were significant and much stronger than for the GSR. For the combined feature model, all 14 correlations were positive and significant, showing highly similar performance to the model with only vehicle motion. This shows that these models capture data of unseen individual participants.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Discussion",
      "text": "Developments in automated driving promise to enhance the quality of life with fewer traffic accidents, and more time that can be spent on other activities. These benefits rely on acceptance by the public, and thereby require high levels of perceived safety and trust in automated driving. This is why understanding perceived risk in automated vehicles is critical for further development in this area. This paper contributes to the development and validation of objective methods for assessing emotional states of users of vehicle automation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Data Collection",
      "text": "A comprehensive dataset was collected from a vehicle simulator experiment that elicited different levels of comfort in relation to perceived risk. This dataset includes continuous subjective comfort ratings, vehicle motion from the real-world drive on which the simulation was based, webcam footage monitoring the person's facial expression, and Galvanic Skin Response. Heart rate (variability), and eye-tracking were collected but not yet analyzed.\n\nThe continuous subjective comfort ratings were analyzed and their relation to calm and dynamic driving styles and the presence of a crossing pedestrian were quantified. Results demonstrate that the experiment successfully elicited different levels of comfort evidenced in the continuous comfort rating and GSR but not through facial emotion recognition. In the braking event without the pedestrian crossing, the dynamic driving style elicited a stronger discomfort as compared to the calm driving style. Adding the crossing pedestrian did not significantly affect discomfort with the calm driving style but leads to the most severe discomfort with the dynamic driving style. With or without pedestrian the accelerations are identical and the stop sign, which is present in both cases indicates the desired stopping location. Hence the perception of relative motion is equivalent. The main difference lies in the perceived consequence of inadequate braking which is fundamentally more severe with a pedestrian crossing the road. This aligns with ample studies relating perceived risk both to the probability and the severity of consequences of events. Complex interactions between effects of driving style and pedestrian crossing are also reported by  [34] , for a simulator experiment with an aggressive and a defensive driving style with pedestrians crossing while asking about the participants' desire for control, trust in automation, and acceptance. They found that the preferred driving style depends strongly on the crossing event type where participants did not always prefer the defensive style. A significant difference between their experiment and ours is that they did not have a moving base simulator, whereby the dynamics of the driving styles were not physically experienced by the participants.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. Facial Emotion Recognition For Perceived Risk Detection",
      "text": "For facial emotion recognition (FER), the state-ofthe-art Dual-Direction Attention Mixed Feature Network (DDAFN++) model was successfully implemented  [26] . Under minimal lighting conditions, with the simulator monitor as the only light source, faces could consistently be detected and emotions classified for 24 participants. This demonstrates the feasibility of applying FER in a driving simulator setting mimicking automated driving with road-monitoring users.\n\nFER assigned labels frame-by-frame using the widely accepted basic emotions: Neutral, Happy, Sad, Angry, Disgust, Surprise, and Fear. However, most participants (15 out of 24) did not exhibit detectable changes in facial expressions during the most critical events, even though their subjective comfort ratings and GSR responses showed clear signs of discomfort. This lack of facial reaction suggests that FER, in its current form, does not reliably capture (dis)comfort related to perceived risk.\n\nContrary to expectations, Fear was the least detected emotion (see Figure  9 ). Among the nine participants showing nonneutral emotions, eight displayed a dominant Happy expression, typically after the pedestrian crossing. This may reflect a sense of relief following the stressful event (decreasing TTC), although no signs of Fear were detected.\n\nThe absence of Fear and the occurrence of Happiness may be partly explained by participants' awareness of the simulation environment, reducing their perception of real danger. Additionally, technical factors may have influenced FER performance: the webcam was positioned above the monitor, often above participants' eye level, while some participants wore large eye-tracking glasses or tilted their heads forward, obscuring key facial features. Reflections from the monitor further complicated expression detection, especially around the eyes-critical areas for emotion recognition  [26] .\n\nOverall, these results suggest that FER is currently not a reliable method for assessing comfort and perceived risk in automated driving. Similar findings from other studies  [21] ,  [22]  indicate that this limitation may be general, rather than specific to the present experiment.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Gsr As A Proxy For Comfort",
      "text": "A neural network was implemented to predict continuous comfort from vehicle motion and GSR. It was shown that both GSR and vehicle motion can successfully fit the dataset, where vehicle motion shows better performance on the test set, but also limitations on how well it can fit the training set. More importantly, the LOOCV analyses showed the model was able to follow trends of subjective perceived risk well for data from unseen participants, where for all participants, the self-asserted perceived risk and the predicted perceived risk show a positive correlation. This was the case both with and without including the GSR features. This might be caused by the fact that the vehicle motions are highly correlated with the subjective perceived risk in this experiment. Since the dynamics are always the same, the model learns this relation and the varying GSR signal becomes of little influence. This model was taken from literature and not fine-tuned, meaning it is likely possible to get better results from this dataset in future research. These findings do already show the potential of achieving objective perceived risk assessment in automated vehicles, losing the biases that are inherent to subjective assessment, and paving the way for future studies in this area.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Future Work",
      "text": "State-of-the-art models can classify emotions from facial expressions, but most research is based on laboratory settings or internet images, where display rules-learned behaviors influenced by culture, sex, and age-may exaggerate expressions  [35] ,  [36] . In our experiment, many participants showed no detectable facial reaction despite strong subjective discomfort, highlighting the greater reliability of physiological signals like GSR. Future research could explore facial expressions in real-world or more unpredictable simulated environments, as repeated exposure to the same scenario may have reduced emotional responses in our study. Nevertheless, the consistency of continuous comfort ratings and GSR supports the validity of our design.\n\nThis study used discrete facial emotion classification due to the availability of rich datasets and pre-trained models. However, alternative approaches, such as analyzing Facial Action Units (AUs)  [37] , could provide more sensitive measures, as they capture subtle facial movements linked to emotions like surprise. Mapping expressions to arousal and valence dimensions, already explored in other modalities like eye tracking  [38] , also offers promise but would require specialized datasets for automated driving contexts.\n\nCombining GSR and facial expressions, as suggested by  [39] , may help distinguish positive and negative arousal. However, as our study observed, participants sometimes smiled during discomfort, complicating interpretation. Future studies could refine objective methods for perceived risk assessment, minimizing reliance on subjective ratings. Herein haptic feedback devices as currently being developed at Siemens, instead of visual comfort knobs, can provide less intrusive, bias-free measurement.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study investigated the potential of facial emotion recognition (FER) and Galvanic Skin Response (GSR) as objective measures for perceived risk and comfort in automated driving. A comprehensive simulator experiment successfully elicited varying levels of discomfort associated with perceived risk, confirmed by subjective comfort ratings and GSR data. Although a state-of-the-art FER model (DDAFN++) was successfully implemented under minimal lighting conditions, facial expressions largely remained neutral during critical events, and fear-related emotions were rarely detected. In some cases, Happy expressions were observed following the pedestrian crossing, potentially reflecting relief rather than heightened risk perception. These findings suggest that FER, under current conditions, is not a reliable proxy for assessing perceived risk. Technical factors, including camera placement and limited facial visibility due to eyewear and lighting reflections, may have impacted detection performance. In contrast, GSR signals consistently tracked discomfort levels and, when combined with vehicle motion data, enabled promising predictions of perceived risk through a neural network model. Overall, these results highlight the limitations of FER for comfort assessment in automated driving and underscore the value of physiological signals, particularly GSR, as objective and reliable indicators.\n\nFuture work should further explore physiological and behavioral markers, improve facial expression analysis methods, and refine experimental protocols to better capture emotional responses under more realistic and unpredictable conditions.\n\nMathieu Sarrazin received his first M.Sc. in Electromechanical-Electrotechnical Engineering in 2009 from the University of Gent, Campus Kortrijk, and a second M.Sc. in Mechanical Engineering with a minor in Automotive Engineering in 2012 from KU Leuven. In 2013, he has received the Marie Curie Fellowship award, recognizing his excellence in advanced research. Since 2012, he has been a key member of LMS International, now Siemens Industry Software (SISW). With over a decade of experience in applied research, Mathieu has made significant contributions to technologies related to electro-mechanical drivetrains, NVH, model-based system testing, hybrid and electric vehicles, autonomous systems, mechatronics, system identification, converter-machine interactions, control strategies, and condition monitoring. In 2014, he received the Siemens Technology Award, and in 2016, he was honored as Siemens Researcher of the Month for his outstanding research achievements. Previously, he served as an R&D manager, leading the definition, planning, and execution of national and European research projects. Currently, Mathieu is Senior Technical Specialist and focuses on developing compelling value propositions and executing effective marketing strategies to accelerate the adoption and success of Siemens' innovative products in the marketplace.\n\nSarah Barendswaard received her B.Sc. and M.Sc. degrees (cum laude) in Aerospace Engineering from Delft Univeristy of Technoogy. Her doctoral research focused on developing innovative haptic shared control systems for automotive applications, specifically creating a personalised lane-keeping assistance system that achieved a five-fold improvement in driver acceptance rates. Currently, she serves as a Research Engineer at Siemens, where her work centers on advancing occupant comfort modeling and comfort enhancement strategies for autonomous vehicles. Her research interests include humanmachine interfaces, haptic feedback systems, passenger comfort optimization and modelling in automated driving systems.\n\nVarun Kotian received his MSc degree in Mechanical Engineering from Delft University of Technology in 2021. Currently, he is a PhD candidate at the Delft University of Technology, specializing in motion perception and motion sickness modelling. His research focuses on enhancing human comfort and safety in vehicles, particularly in the context of autonomous vehicles.\n\nRiender Happee received the Ph.D. degree from TU Delft, The Netherlands, in 1992. He investigated road safety and introduced biomechanical human models for impact and comfort at TNO Automotive (1992-2007). Currently, he investigates the human interaction with automated vehicles focusing on motion comfort, perceived safety and acceptance at the Delft University of Technology, the Netherlands, where he is full Professor.",
      "page_start": 12,
      "page_end": 14
    },
    {
      "section_name": "Appendix A Questionnaire",
      "text": "Rate the following 8 statements from 1 (do not agree at all) to 5 (completely agree). When an automated vehicle is mentioned, automation level SAE4+ is meant. This means that the vehicle can drive fully automated, without human supervision. After every question you have the option to provide comments/context to your given answer. If anything is unclear, please ask the researcher.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "1) I Trust The Technology Behind Automated Vehicles To Work Correctly.",
      "text": "Not at all 1 2 3 4 5 Completely Why?\n\n2) I think riding in an automated vehicle would be comfortable.\n\nNot at all 1 2 3 4 5 Completely Why?\n\n3) I would feel relaxed while being in an automated vehicle.\n\nNot at all 1 2 3 4 5 Completely Why?\n\n4) I believe automated vehicles could reduce traffic accidents.\n\nNot at all 1 2 3 4 5 Completely Why?\n\n5) I would rather trust a fully automated vehicle than today's drivers.  [25]  Not at all 1 2 3 4 5 Completely Why?\n\n6) I would be concerned about safety, if I was driven by a fully automated vehicle.  [25]  Not at all 1 2 3 4 5 Completely Why? 7) I would not engage in non-driving related activities, but monitor the driving system.  [25]  Not at all 1 2 3 4 5 Completely Why?\n\n8) I would use a fully automated vehicle if they are available.  [25]  Not at all 1 2 3 4 5 Completely Why?\n\n9) Feel free to share any additional thoughts or comments on fully automated vehicles:",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Appendix B Continuous Rating Repeatability",
      "text": "All conditions were experienced two times by the participants, and all individual results are plotted in  Figures 13, 14, 15, and 16  where each figure captures one condition. The xaxis is time in seconds, and the y-axis the perceived risk score between 0 and 10. The blue line shows the score given during the first round, and the red line represents the second round.\n\nThe data for Participant 20 during the second round for the calm driving style without pedestrian was lost, which is why this plot also has no red line. Participant 11 misunderstood the working of the knob during the first round, meaning this knob data is also not viable. All data for participant 19 was also excluded from further analyses, showing unrealistic scores and after the experiment the participant commented that it was normal if a car also \"nudged\" a pedestrian. This was considered very exceptional, and not relevant to how we desire automated vehicles to behave. Participants 31 and 32 seem to only adjust the perceived risk knob at the very end of the run, long after the critical condition presented itself, or not at all, leading to believe that they might have misunderstood the assignment, or forgot to use the knob till the very end. Their knob data was also excluded.\n\nThe main purpose of repeating conditions was to check for consistency. If the subjective score in time is consistent over runs, it is more likely to show a true representation of the participants' state. Consistency was assessed visually by looking at the graphs, and dynamic time-warping (DTW) was applied starting at 10 seconds to account for initial offsets.      APPENDIX C SENSORS FOR THE EXPERIMENT GSR and heart activity (ECG) were measured using a NeXus-10 MKII from MindMedia.\n\nThe GSR was measured via two electrodes attached to two fingertips of the same hand with velcro straps, as can be seen in Figure  19 . For this experiment, the participant was asked what was their dominant hand, and how they would operate the comfort knob. The velcro straps were attached to the fingertips on the hand that would not operate the comfort knob, to limit electrode movements that could affect the measurements. With the GSR electrodes, it is important that there is some time (1-2 minutes) between attaching the sensor and starting the measurement, as the skin under and around the velcro straps has to accommodate to the presence of the straps. During this experiment, after the velcro straps were attached, the other sensors were explained, and a test run was performed, ensuring sufficient time before starting the measurements.\n\nThe GSR data was processed using the NeuroKit2 python package  [42]  as described in appendix D.\n\nThe ECG was measured via three electrodes placed on the torso of the participant. The electrodes were color-coded and placed as illustrated in Figure  20 . The specific sticky electrodes used in this experiment were the F9060 electrodes (for adults, 48x50 millimeter) produced by FIAB. The participant attached these stickers themselves, under clear instruction from the researcher. ECG was collected for future research and not analysed in this paper. Fig.  19 : GSR sensors attached to the fingertips of the nondominant hand. They were not color-coded because it does not matter which goes on which finger, as long as they are on the same hand.  The Comfort-Knob is a rotational potentiometer connected to continuously measure the subjective comfort of the participant. It is a turning knob where the participant gives a value between 0 and 10. The scale was defined as 0 being extremely uncomfortable, 5 being neutral, and 10 being extremely comfortable. This is based on the SAE J1060 for subjective ratings related to ride comfort in motor vehicles, where 0 up to and including 4 is considered unacceptable, 5 is borderline, and 6 to 10 is acceptable  [23] . Except for the knob not turning past the limits of 0 and 10, there was no haptic feedback for the participant to feel how far they had turned the knob. Figure  22  was constantly present in the topmiddle of the screen to remind the participant which way to turn the knob according to their state. This illustration was made very simple to limit distraction. On the device itself is a LED screen that also shows the current value in 2 decimals. Participants were instructed not to focus on the exact value, as this would be distracting. A picture of the comfort knob is presented in Figure  23 .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Appendix D Gsr Analysis",
      "text": "For processing of the GSR data, the NeuroKit2 python package was used  [42] . This is an open source package for processing physiological signals. It is completely opensource, and invites anyone to contribute. Contributors are from all over the world and can be found with their affiliation here. They incorporate different processing methods, both self-developed and implemented from literature. They also incorporate methods from BioSPPy, another python package for physiological signal processing  [43] . BioSPPy is marked as archived on github, whereas the Neurokit package is still being updated.\n\nFirst a low-pass Butterworth filter was applied to clean the signal. As movement artifacts are likely to occur due to the dynamic nature of the experiment, the decision was made to go for a more aggressive approach, also smoothing the signal after the Butterworth filter conform  [44] . The difference between these two approaches can be seen in Figure  25 . The Neurokit plot is only a Butterworth filter, and the BioSPPy plot also has smoothing. Note that a small offset was given to the plots, to make them better visible. This offset was implemented manually purely for this image, and is not present in the data itself. To split these components different methods are used (see  [45]  for a discussion). Different methods can give varying outputs, as can also be seen in Figure  26 . According to literature sparsEDA performs best for classifying event-related stress, also specifically in the context of active driving through the Phasic component  [46] ,  [47] . However, a downside is a less accurate Tonic component  [47] . CvxEDA  [48]  has a better tonic component and also performs well in the classification of the Phasic component, and was therefore selected for this paper. In the Phasic component, peak detection was applied using the popular method by  [49] , whom performed emotion classifi-cation from short physiological signals. These peaks represent the Skin Conductance Responses, and are expressed in three components. The onset, where the rise starts, the amplitude at its peak, and the half-recovery time, when the level is halfway back to the base level. A summary of this processing is given in Figure  27 . Here we also see that SCR has a zero baseline due to the applied high pass filtering which simplifies the analysis. Before analysis, the GSR data was inspected. From one participant, the data showed a response that was unlikely to be physiological. This response, of which one run is shown in Figure  28 , showed abnormally fast and large drops in the GSR, also showing response amplitudes much higher than 3 µSiemens  [50] . Something else that was noticed, was that a lot of responses were relatively low. Normal GSR values do not go below 1µSiemens  [50] , which did occur in this dataset. A problem could have been that the velcro straps were not tight enough, resulting in the electrode to momentarily lose contact with the skin. A cut-off was implemented, excluding participants whose data dropped below 1 µSiemens, even if it was momentarily because the full response was deemed no longer reliable. This resulted in excluding 6 participants, which is close to 20% of the data. To avoid this, one-time-use sticky electrodes could be used. Fig.  28 : Faulty GSR with extreme drops.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "A. Gsr Relates To Driving Style",
      "text": "The Facial Expression analyses did not provide the desired results, even though the continuous subjective ratings showed significant drops in comfort, in particular during the dynamic driving style. To validate whether this effect was only present in the subjective ratings, or also present in the physiological data, the GSR was analysed.\n\nThe most significant variable of the experiment was found to be the driving style, and to not let the validation of GSR depend on the individual subjective perceived risk scores, an event-based analysis was done. As explained above the phasic component, or the Skin Conductance Response (SCR), is known to be an indicator of events that cause arousal or stress. Peak detection was performed, resulting in dictionaries with onsets, the moment in time that the peak starts, peaks, the moment the phasic component is at its highest, and the amplitudes, the difference between the peak value and the onset value. This detection is also depicted in Figure  27 .\n\nAs features, the times and amplitude of the highest peak for each run were given to a Random Forest Classifier. The classifier then predicted if this was a calm or dynamic drive.\n\nBecause results for such classifiers can depend on the random seed that is used, the fitting and prediction were repeated 10 times with a different seed. The classifier could always reach a perfect fit on the training data, with a mean accuracy of 100% and a standard deviation of 0. On the testing data, the mean accuracy was 74.3% with a standard deviation of 4.7%. This analysis was not fine-tuned, as the goal was merely to show that GSR can be utilized to classify events with high perceived risk in this dataset. By fine-tuning further, both in feature selection and the classifier itself, it is likely possible to achieve better results.\n\nThese GSR analyses confirm that the comfort drop, though not visible in the facial expressions of most participants, is present in physiological data.         20% of the data was set aside as a validation set, and 80% was used as training data. In this split, it was ensured that both sets contained roughly the same distribution for the labels. This can be seen in Figure  40 . The stride of the sequence sampling is how much the window shifts to get the next sequence. This stride was made dependent on the label. For the labels that were far from the center a smaller stride was applied, and for the more common labels a larger stride was applied. This balanced the data out more. A minimum stride of 0.5 seconds was chosen, which is 10% of the sequence length, to avoid different sequences from being too much alike which would result in overfitting. This stride of 0.5 seconds was applied to sequences with a label below 3 and above 8, as these were the most extreme regions with the lowest amount of samples. For labels between 3 and 5, and between 6 and 8, a stride of 2 seconds was applied. For the very common region between 5 and 6, a stride of 4 seconds was applied. These different strides resulted in a division as can be seen in Figure  41 . It is clear that the labels were now represented much more equally in the dataset.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Appendix E Facial Expression As Function Of Time",
      "text": "",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The scene is displayed on a wide-screen",
      "page": 3
    },
    {
      "caption": "Figure 1: Driving simulator with hexapod, chair and wide-screen.",
      "page": 3
    },
    {
      "caption": "Figure 23: ). It is a turning knob where",
      "page": 3
    },
    {
      "caption": "Figure 22: was constantly present",
      "page": 3
    },
    {
      "caption": "Figure 24: ). The frames are in color and",
      "page": 3
    },
    {
      "caption": "Figure 2: Screen captures from the participants perspective.",
      "page": 4
    },
    {
      "caption": "Figure 4: In Figure 5 the time-to-collision (TTC)",
      "page": 4
    },
    {
      "caption": "Figure 3: Birds-eye view of the scene, with the different road",
      "page": 4
    },
    {
      "caption": "Figure 4: Linear Acceleration profiles of the four different conditions. The colors are the same as on the birds-eye view in Figure",
      "page": 5
    },
    {
      "caption": "Figure 5: Time-to-collision with pedestrian for both driving styles. Both axes are in seconds, with the x-axis representing the time",
      "page": 5
    },
    {
      "caption": "Figure 3: Blue is the straight",
      "page": 5
    },
    {
      "caption": "Figure 6: -upper left the detected",
      "page": 6
    },
    {
      "caption": "Figure 6: -upper right.",
      "page": 7
    },
    {
      "caption": "Figure 6: -lower right shows the extracted face that is",
      "page": 7
    },
    {
      "caption": "Figure 6: Original image with facial landmarks detected (top",
      "page": 7
    },
    {
      "caption": "Figure 7: a. All conditions",
      "page": 7
    },
    {
      "caption": "Figure 8: Distribution of absolute minima of the continuous",
      "page": 7
    },
    {
      "caption": "Figure 7: Continuous comfort ratings",
      "page": 8
    },
    {
      "caption": "Figure 9: shows the",
      "page": 8
    },
    {
      "caption": "Figure 10: and all plots can be found in appendix E. The",
      "page": 8
    },
    {
      "caption": "Figure 10: shows major differences between the first and",
      "page": 8
    },
    {
      "caption": "Figure 9: Pie charts illustrating the distribution of detected",
      "page": 9
    },
    {
      "caption": "Figure 10: Facial emotion recognition confidence scores over",
      "page": 9
    },
    {
      "caption": "Figure 2: c). Time-to-collision (TTC) is scaled by a factor of",
      "page": 9
    },
    {
      "caption": "Figure 11: Facial emotion recognition participant 14, where the",
      "page": 9
    },
    {
      "caption": "Figure 12: Facial emotion recognition participant 15, which was",
      "page": 9
    },
    {
      "caption": "Figure 40: We also see that the dataset is not",
      "page": 10
    },
    {
      "caption": "Figure 41: The performance of the model was compared for the dif-",
      "page": 10
    },
    {
      "caption": "Figure 9: ). Among the nine participants showing non-",
      "page": 11
    },
    {
      "caption": "Figure 13: Individual subjective perceived risk responses for the calm drive without pedestrian in the scene. Blue lines represent the first and red lines the the second round",
      "page": 16
    },
    {
      "caption": "Figure 14: All subjective perceived risk responses for the calm drive with the pedestrian in the scene. Each blue line represents the response during the first round, and the",
      "page": 17
    },
    {
      "caption": "Figure 15: All subjective perceived risk responses for the dynamic drive without the pedestrian in the scene. Each blue line represents the response during the first round,",
      "page": 18
    },
    {
      "caption": "Figure 16: All subjective perceived risk responses for the dynamic drive with the pedestrian in the scene. Each blue line represents the response during the first round, and",
      "page": 19
    },
    {
      "caption": "Figure 17: The left plot shows a run with high consistency, resulting",
      "page": 20
    },
    {
      "caption": "Figure 17: Two plots showing the calculated paths between the",
      "page": 20
    },
    {
      "caption": "Figure 18: The distribution of the DTW distances.",
      "page": 20
    },
    {
      "caption": "Figure 18: This gives an overview and a general idea of what can be",
      "page": 20
    },
    {
      "caption": "Figure 19: For this experiment, the participant was asked",
      "page": 21
    },
    {
      "caption": "Figure 20: The specific sticky electrodes",
      "page": 22
    },
    {
      "caption": "Figure 19: GSR sensors attached to the fingertips of the non-",
      "page": 22
    },
    {
      "caption": "Figure 20: Illustration on where to place the electrodes for ECG",
      "page": 22
    },
    {
      "caption": "Figure 21: The Pupil Invisible glasses with eye tracker from",
      "page": 22
    },
    {
      "caption": "Figure 22: was constantly present in the top-",
      "page": 22
    },
    {
      "caption": "Figure 23: Fig. 22: The illustration that was in the top middle of the",
      "page": 22
    },
    {
      "caption": "Figure 23: The Comfort Knob. A marking was made on the",
      "page": 23
    },
    {
      "caption": "Figure 24: shows the first-person view of the participant",
      "page": 23
    },
    {
      "caption": "Figure 24: First person view from the participants. The webcam",
      "page": 23
    },
    {
      "caption": "Figure 25: The Neurokit",
      "page": 23
    },
    {
      "caption": "Figure 25: The raw GSR signal and the two cleaned signals.",
      "page": 23
    },
    {
      "caption": "Figure 26: According to literature sparsEDA performs best for",
      "page": 23
    },
    {
      "caption": "Figure 26: The Tonic (left) and Phasic (right) components of a",
      "page": 23
    },
    {
      "caption": "Figure 27: Here we also see that SCR has a zero baseline",
      "page": 24
    },
    {
      "caption": "Figure 27: An overview of the processing of a GSR signal. First",
      "page": 24
    },
    {
      "caption": "Figure 28: , showed abnormally fast and large drops in the",
      "page": 24
    },
    {
      "caption": "Figure 28: Faulty GSR with extreme drops.",
      "page": 24
    },
    {
      "caption": "Figure 27: As features, the times and amplitude of the highest peak",
      "page": 24
    },
    {
      "caption": "Figure 29: All emotions confidence scores during the dynamic",
      "page": 25
    },
    {
      "caption": "Figure 30: All emotions confidence scores during the dynamic",
      "page": 25
    },
    {
      "caption": "Figure 31: All emotions confidence scores during the dynamic",
      "page": 25
    },
    {
      "caption": "Figure 32: All emotions confidence scores during the dynamic",
      "page": 25
    },
    {
      "caption": "Figure 33: All emotions confidence scores during the dynamic",
      "page": 26
    },
    {
      "caption": "Figure 34: All emotions confidence scores during the dynamic",
      "page": 26
    },
    {
      "caption": "Figure 35: All emotions confidence scores during the dynamic",
      "page": 26
    },
    {
      "caption": "Figure 36: All emotions confidence scores during the dynamic",
      "page": 26
    },
    {
      "caption": "Figure 37: Part 1/3 of the architecture.",
      "page": 27
    },
    {
      "caption": "Figure 38: Part 2/3 of the architecture.",
      "page": 27
    },
    {
      "caption": "Figure 39: Part 3/3 of the architecture.",
      "page": 27
    },
    {
      "caption": "Figure 40: The stride of the sequence sampling",
      "page": 27
    },
    {
      "caption": "Figure 40: Data distribution of the training and validation set,",
      "page": 27
    },
    {
      "caption": "Figure 41: Data distribution of the training and validation set,",
      "page": 28
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "calm noped": "30,21",
          "calm ped": "66,97",
          "dynamic noped": "18,85",
          "dynamic ped": "34,83"
        },
        {
          "calm noped": "15,01",
          "calm ped": "6,13",
          "dynamic noped": "38,7",
          "dynamic ped": "61,53"
        },
        {
          "calm noped": "6,47",
          "calm ped": "18,31",
          "dynamic noped": "19,56",
          "dynamic ped": "37,47"
        },
        {
          "calm noped": "10,85",
          "calm ped": "110,08",
          "dynamic noped": "53,61",
          "dynamic ped": "14,19"
        },
        {
          "calm noped": "35,05",
          "calm ped": "42,33",
          "dynamic noped": "56,95",
          "dynamic ped": "10,76"
        },
        {
          "calm noped": "28,04",
          "calm ped": "5,67",
          "dynamic noped": "46",
          "dynamic ped": "13,69"
        },
        {
          "calm noped": "13,35",
          "calm ped": "12,9",
          "dynamic noped": "20,21",
          "dynamic ped": "25,26"
        },
        {
          "calm noped": "8,07",
          "calm ped": "11,88",
          "dynamic noped": "38,52",
          "dynamic ped": "6,72"
        },
        {
          "calm noped": "28,55",
          "calm ped": "29,18",
          "dynamic noped": "75,99",
          "dynamic ped": "7,71"
        },
        {
          "calm noped": "9,26",
          "calm ped": "43,7",
          "dynamic noped": "8,79",
          "dynamic ped": "15,41"
        },
        {
          "calm noped": "38,66",
          "calm ped": "22,5",
          "dynamic noped": "57,96",
          "dynamic ped": "41,51"
        },
        {
          "calm noped": "24,86",
          "calm ped": "48,38",
          "dynamic noped": "34,92",
          "dynamic ped": "16,83"
        },
        {
          "calm noped": "7,83",
          "calm ped": "5,6",
          "dynamic noped": "3,29",
          "dynamic ped": "8,67"
        },
        {
          "calm noped": "",
          "calm ped": "5,36",
          "dynamic noped": "7,5",
          "dynamic ped": "1,45"
        },
        {
          "calm noped": "21,04",
          "calm ped": "9,9",
          "dynamic noped": "15,42",
          "dynamic ped": "62,89"
        },
        {
          "calm noped": "19,17",
          "calm ped": "15,54",
          "dynamic noped": "115,36",
          "dynamic ped": "9,83"
        },
        {
          "calm noped": "26,09",
          "calm ped": "5,69",
          "dynamic noped": "9,72",
          "dynamic ped": "20,93"
        },
        {
          "calm noped": "21,44",
          "calm ped": "24,93",
          "dynamic noped": "25,01",
          "dynamic ped": "19,93"
        },
        {
          "calm noped": "21,72",
          "calm ped": "9,73",
          "dynamic noped": "28,6",
          "dynamic ped": "28,61"
        },
        {
          "calm noped": "32,78",
          "calm ped": "64,16",
          "dynamic noped": "46,8",
          "dynamic ped": "17,05"
        },
        {
          "calm noped": "14,08",
          "calm ped": "108,59",
          "dynamic noped": "8,77",
          "dynamic ped": "120,66"
        },
        {
          "calm noped": "143,46",
          "calm ped": "13,81",
          "dynamic noped": "120,31",
          "dynamic ped": "6,5"
        },
        {
          "calm noped": "43,42",
          "calm ped": "17,53",
          "dynamic noped": "40,05",
          "dynamic ped": "30,64"
        },
        {
          "calm noped": "36,73",
          "calm ped": "11,42",
          "dynamic noped": "38,85",
          "dynamic ped": "12,88"
        },
        {
          "calm noped": "30,17",
          "calm ped": "8,46",
          "dynamic noped": "56,27",
          "dynamic ped": "65,94"
        },
        {
          "calm noped": "81,51",
          "calm ped": "18,21",
          "dynamic noped": "63,86",
          "dynamic ped": "19,76"
        },
        {
          "calm noped": "18,58",
          "calm ped": "34,63",
          "dynamic noped": "11,13",
          "dynamic ped": "9,92"
        },
        {
          "calm noped": "29,04",
          "calm ped": "28,24",
          "dynamic noped": "33,62",
          "dynamic ped": "23,56"
        }
      ],
      "page": 21
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The electrodermal system",
      "authors": [
        "M Dawson",
        "A Schell",
        "D Filion"
      ],
      "year": "2007",
      "venue": "Handbook of psychophysiology"
    },
    {
      "citation_id": "2",
      "title": "Gsr based generic stress prediction system",
      "authors": [
        "D Jaiswal",
        "D Chatterjee",
        "R Ramakrishnan",
        "A Pal"
      ],
      "year": "2023",
      "venue": "Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing & the 2023 ACM International Symposium on Wearable Computing"
    },
    {
      "citation_id": "3",
      "title": "Detection of driver stress in real-world driving environment using physiological signals",
      "authors": [
        "K Wang",
        "Y Murphey",
        "Y Zhou",
        "X Hu",
        "X Zhang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 17th International Conference on Industrial Informatics (INDIN)"
    },
    {
      "citation_id": "4",
      "title": "Stress level classification using statistical analysis of skin conductance signal while driving",
      "authors": [
        "M Memar",
        "A Mokaribolhassan"
      ],
      "year": "2021",
      "venue": "SN Applied Sciences"
    },
    {
      "citation_id": "5",
      "title": "Basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "6",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "7",
      "title": "On driver behavior recognition for increased safety: a roadmap",
      "authors": [
        "L Davoli",
        "M Martalò",
        "A Cilfone",
        "L Belli",
        "G Ferrari",
        "R Presta",
        "R Montanari",
        "M Mengoni",
        "L Giraldi",
        "E Amparore"
      ],
      "year": "2020",
      "venue": "Safety"
    },
    {
      "citation_id": "8",
      "title": "Automatic detection of emotion valence on faces using consumer depth cameras",
      "authors": [
        "A Savran",
        "R Gur",
        "R Verma"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "9",
      "title": "An experimental study on physiological parameters toward driver emotion recognition",
      "authors": [
        "H Leng",
        "Y Lin",
        "L Zanzi"
      ],
      "year": "2007",
      "venue": "Ergonomics and Health Aspects of Work with Computers: International Conference, EHAWC 2007, Held as Part of HCI International"
    },
    {
      "citation_id": "10",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "11",
      "title": "Modeling of operators' emotion and task performance in a virtual driving environment",
      "authors": [
        "H Cai",
        "Y Lin"
      ],
      "year": "2011",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "12",
      "title": "Pan-cultural elements in facial displays of emotion",
      "authors": [
        "P Ekman",
        "E Sorenson",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Science"
    },
    {
      "citation_id": "13",
      "title": "Cultural similarities and differences in display rules",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1990",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "14",
      "title": "Variations of emotional display rules within and across cultures: A comparison between canada, usa, and japan",
      "authors": [
        "S Safdar",
        "W Friedlmeier",
        "D Matsumoto",
        "S Yoo",
        "C Kwantes",
        "H Kakai",
        "E Shigemasu"
      ],
      "year": "2009",
      "venue": "Canadian Journal of Behavioural Science/Revue canadienne des sciences du comportement"
    },
    {
      "citation_id": "15",
      "title": "Why do drivers maintain short headways in fog? a driving-simulator study evaluating feeling of risk and lateral control during automated and manual car following",
      "authors": [
        "M Saffarian",
        "R Happee",
        "J Winter"
      ],
      "year": "2012",
      "venue": "Ergonomics"
    },
    {
      "citation_id": "16",
      "title": "Development and evaluation of comfort assessment approaches for passengers in autonomous vehicles",
      "authors": [
        "H Su",
        "J Brooks",
        "Y Jia"
      ],
      "year": "2023",
      "venue": "Development and evaluation of comfort assessment approaches for passengers in autonomous vehicles"
    },
    {
      "citation_id": "17",
      "title": "Komfopilot-comfortable automated driving",
      "authors": [
        "M Beggiato",
        "F Hartwich",
        "P Roßner",
        "A Dettmann",
        "S Enhuber",
        "T Pech",
        "D Gesmann-Nuissl",
        "K Mößner",
        "A Bullinger",
        "J Krems"
      ],
      "year": "2020",
      "venue": "Smart automotive mobility: reliable technology for the mobile human"
    },
    {
      "citation_id": "18",
      "title": "Modelling perceived risk and trust in driving automation reacting to merging and braking vehicles",
      "authors": [
        "X He",
        "J Stapel",
        "M Wang",
        "R Happee"
      ],
      "year": "2022",
      "venue": "Transportation research part F: traffic psychology and behaviour"
    },
    {
      "citation_id": "19",
      "title": "Perceived risk and acceptance of automated vehicles users to unexpected hazard situations in real driving conditions",
      "authors": [
        "E Pérez-Moreno",
        "J Naranjo",
        "M Hernández",
        "T Ruíz",
        "A Valle",
        "A Cruz",
        "F Serradilla",
        "F Jiménez"
      ],
      "year": "2025",
      "venue": "Behaviour & Information Technology"
    },
    {
      "citation_id": "20",
      "title": "Social acceptability of autonomous vehicles: unveiling correlation of passenger trust and emotional response",
      "authors": [
        "C Park",
        "M Nojoumian"
      ],
      "year": "2022",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "21",
      "title": "Automated driving system, male, or female driver: Who'd you prefer? comparative analysis of passengers' mental conditions, emotional states & qualitative feedback",
      "authors": [
        "P Wintersberger",
        "A Riener",
        "A.-K Frison"
      ],
      "year": "2016",
      "venue": "Proceedings of the 8th international conference on automotive user interfaces and interactive vehicular applications"
    },
    {
      "citation_id": "22",
      "title": "Automatic emotion recognition for the calibration of autonomous driving functions",
      "authors": [
        "J Sini",
        "A Marceddu",
        "M Violante"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "23",
      "title": "Accurate ride comfort estimation combining accelerometer measurements, anthropometric data and neural networks",
      "authors": [
        "M Cieslak",
        "S Kanarachos",
        "M Blundell",
        "C Diels",
        "M Burnett",
        "A Baxendale"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "24",
      "title": "Perceived comfort and safety in automated driving based on physiological signals: Findings from a proving ground study",
      "authors": [
        "J Scharringa",
        "K Gkentsidis",
        "M Sarrazin",
        "R Happee",
        "K Janssens"
      ],
      "venue": "Perceived comfort and safety in automated driving based on physiological signals: Findings from a proving ground study"
    },
    {
      "citation_id": "25",
      "title": "Development of the questionnaire on the acceptance of automated driving (qaad): Data-driven models for level 3 and level 5 automated driving",
      "authors": [
        "K Weigl",
        "C Schartmüller",
        "A Riener",
        "M Steinhauser"
      ],
      "year": "2021",
      "venue": "Transportation research part F: traffic psychology and behaviour"
    },
    {
      "citation_id": "26",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "S Zhang",
        "Y Zhang",
        "Y Zhang",
        "Y Wang",
        "Z Song"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "27",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "28",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition",
      "authors": [
        "J Ngwe",
        "K Lim",
        "C Lee",
        "T Ong"
      ],
      "year": "2023",
      "venue": "Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition",
      "arxiv": "arXiv:2306.09626"
    },
    {
      "citation_id": "30",
      "title": "Retinaface: Single-stage dense face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "Y Zhou",
        "J Yu",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Retinaface: Single-stage dense face localisation in the wild"
    },
    {
      "citation_id": "31",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "32",
      "title": "A new computational perceived risk model for automated vehicles based on potential collision avoidance difficulty (pcad)",
      "authors": [
        "X He",
        "R Happee",
        "M Wang"
      ],
      "year": "2024",
      "venue": "Transportation Research Part C: Emerging Technologies"
    },
    {
      "citation_id": "33",
      "title": "Comparative assessment of safety indicators for vehicle trajectories on highways",
      "authors": [
        "F Mullakkal-Babu",
        "M Wang",
        "H Farah",
        "B Van Arem",
        "R Happee"
      ],
      "year": "2017",
      "venue": "Transportation Research Record"
    },
    {
      "citation_id": "34",
      "title": "Driving behavior analysis: A human factors perspective on automated driving styles",
      "authors": [
        "J Peintner",
        "C Himmels",
        "T Rock",
        "C Manger",
        "O Jung",
        "A Riener"
      ],
      "year": "2024",
      "venue": "2024 IEEE Intelligent Vehicles Symposium (IV)"
    },
    {
      "citation_id": "35",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "36",
      "title": "The repertoire of nonverbal behavior: Categories, origins, usage, and coding",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "semiotica"
    },
    {
      "citation_id": "37",
      "title": "Facial expressions as indicator for discomfort in automated driving",
      "authors": [
        "M Beggiato",
        "N Rauh",
        "J Krems"
      ],
      "year": "2020",
      "venue": "Intelligent Human Systems Integration 2020: Proceedings of the 3rd International Conference on Intelligent Human Systems Integration (IHSI 2020): Integrating People and Intelligent Systems"
    },
    {
      "citation_id": "38",
      "title": "Driver emotion recognition with a hybrid attentional multimodal fusion framework",
      "authors": [
        "L Mou",
        "Y Zhao",
        "C Zhou",
        "B Nakisa",
        "M Rastgoo",
        "L Ma",
        "T Huang",
        "B Yin",
        "R Jain",
        "W Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Driver's emotions detection with automotive systems in connected and autonomous vehicles (cavs).,\" in CHIRA",
      "authors": [
        "B Meza-García",
        "N Rodríguez-Ibáñez"
      ],
      "year": "2021",
      "venue": "Driver's emotions detection with automotive systems in connected and autonomous vehicles (cavs).,\" in CHIRA"
    },
    {
      "citation_id": "40",
      "title": "Exact indexing of dynamic time warping",
      "authors": [
        "E Keogh",
        "C Ratanamahatana"
      ],
      "year": "2005",
      "venue": "Knowledge and information systems"
    },
    {
      "citation_id": "41",
      "title": "Toward accurate dynamic time warping in linear time and space",
      "authors": [
        "S Salvador",
        "P Chan"
      ],
      "year": "2007",
      "venue": "Intelligent Data Analysis"
    },
    {
      "citation_id": "42",
      "title": "NeuroKit2: A python toolbox for neurophysiological signal processing",
      "authors": [
        "D Makowski",
        "T Pham",
        "Z Lau",
        "J Brammer",
        "F Lespinasse",
        "H Pham",
        "C Schölzel",
        "S Chen"
      ],
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "43",
      "title": "BioSPPy: Biosignal processing in Python",
      "authors": [
        "C Carreiras",
        "A Alves",
        "A Lourenc ¸o",
        "F Canento",
        "H Silva",
        "A Fred"
      ],
      "year": "2022",
      "venue": "BioSPPy: Biosignal processing in Python"
    },
    {
      "citation_id": "44",
      "title": "The scientist and engineer's guide to digital signal processing",
      "authors": [
        "S Smith"
      ],
      "year": "1997",
      "venue": "The scientist and engineer's guide to digital signal processing"
    },
    {
      "citation_id": "45",
      "title": "Innovations in electrodermal activity data collection and signal processing: A systematic review",
      "authors": [
        "H Posada-Quintero",
        "K Chon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "46",
      "title": "A comparative analysis of eda decomposition methods for improved emotion recognition",
      "authors": [
        "P Kumar",
        "P Govarthan",
        "N Ganapathy",
        "J Ronickom"
      ],
      "year": "2023",
      "venue": "Journal of Mechanics in Medicine and Biology"
    },
    {
      "citation_id": "47",
      "title": "Feature extraction for stress detection in electrodermal activity",
      "authors": [
        "E Lutin",
        "R Hashimoto",
        "W Raedt",
        "C Van Hoof"
      ],
      "year": "2021",
      "venue": "BIOSIGNALS"
    },
    {
      "citation_id": "48",
      "title": "cvxeda: A convex optimization approach to electrodermal activity processing",
      "authors": [
        "A Greco",
        "G Valenza",
        "A Lanata",
        "E Scilingo",
        "L Citi"
      ],
      "year": "2015",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "49",
      "title": "Emotion recognition system using short-term monitoring of physiological signals",
      "authors": [
        "K Kim",
        "S Bang",
        "S Kim"
      ],
      "year": "2004",
      "venue": "Medical and biological engineering and computing"
    },
    {
      "citation_id": "50",
      "title": "A guide for analysing electrodermal activity (eda) & skin conductance responses (scrs) for psychological experiments",
      "authors": [
        "J Braithwaite",
        "D Watson",
        "R Jones",
        "M Rowe"
      ],
      "year": "2013",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "51",
      "title": "Abel van Elburg received his MSc degree in Robotics Engineering from Delft University of Technology in 2024",
      "venue": "Abel van Elburg received his MSc degree in Robotics Engineering from Delft University of Technology in 2024"
    },
    {
      "citation_id": "52",
      "title": "Electrical and Computer Engineering from Democritus University of Thrace and an Advanced Master's degree (MSc) in Artificial Intelligence from KU Leuven",
      "venue": "Since 2022, he has been involved in multiple European and Belgian-funded research projects on behalf of Siemens Digital Industries Software, specializing in occupant comfort assessment, and perception for autonomous vehicles"
    }
  ]
}