{
  "paper_id": "2503.12623v2",
  "title": "Maven: Multi-Modal Attention For Valence-Arousal Emotion Network",
  "published": "2025-03-16T19:32:32Z",
  "authors": [
    "Vrushank Ahire",
    "Kunal Shah",
    "Mudasir Nazir Khan",
    "Nikhil Pakhale",
    "Lownish Rai Sookha",
    "M. A. Ganaie",
    "Abhinav Dhall"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic emotion recognition in the wild remains challenging due to the transient nature of emotional expressions and temporal misalignment of multi-modal cues. Traditional approaches predict valence and arousal and often overlook the inherent correlation between these two dimensions. The proposed Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates visual, audio, and textual modalities through a bi-directional cross-modal attention mechanism. MAVEN uses modality-specific encoders to extract features from synchronized video frames, audio segments, and transcripts, predicting emotions in polar coordinates following Russell's circumplex model. The evaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance correlation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline model with a CCC of 0.22. The multistage architecture captures the subtle and transient nature of emotional expressions in conversational videos and improves emotion recognition in real-world situations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a critical challenge in affective computing with applications spanning human-computer interaction, healthcare, education, and entertainment. While traditional approaches have focused on categorical emotion classification (e.g., happiness, sadness, anger), dimensional models representing emotions along continuous valence (pleasure-displeasure) and arousal (activation-deactivation) scales have gained prominence for their ability to represent subtle emotional nuances  [21, 46] . These dimensional models support the idea that emotions exist on a continuous spectrum, enriching the understanding of the complexity of human emotional experiences.\n\nThe detection and analysis of human emotions in natural settings pose significant challenges due to variations in expressions, individual differences, cultural influences, and the often subtle nature of emotional cues  [18, 52] . This complexity requires a multi-modal approach, as people naturally express and perceive emotions through various channels, including facial expressions, voice intonation, language, and gestures  [5, 53] . Integrating complementary information from each modality enhances the robustness and accuracy of emotion recognition systems. While facial expressions might reveal visible emotional cues, speech prosody can uncover subtle emotional undertones, and linguistic content can provide contextual information essential for accurate interpretation  [12, 32] .\n\nThe field has witnessed significant advancements in emotion recognition through deep learning approaches in recent years. Initial research emphasizes unimodal systems in facial expression analysis using efficient deep learning models like EfficientNet and extracting frame-level features through EmotiEffNet  [39, 40] . Approaches such as EmoFAN-VR achieved notable success in controlled environments but often struggled with in-the-wild scenarios characterized by varying illumination, occlusions, and pose variations  [8, 21] . Researchers explored other modalities separately, creating specialized models for emotion recognition in speech using spectral features and recurrent architectures such as Vesper  [4]  and text-based systems using natural language processing techniques.\n\nThe shift towards multi-modal systems reflects how humans perceive and express emotions through various channels simultaneously  [53] . Initial multi-modal approaches employed simple fusion strategies, such as feature concate-nation or decision-level integration, which failed to capture the inter-dependencies between modalities  [5] . Advanced techniques were developed, including tensor-based fusion, specifically Tucker Tensor Regression and Tensor Regression Networks  [34] , bilinear pooling, and various ensemble techniques like EVAEF  [29]  . While these approaches have enhanced performance, they often treat different modalities as independent sources of information. To address this, Meng et al.  [33]  uses temporal encoders, specifically transformer-based and Long Short-Term Memory (LSTM)based networks, to better understand how emotions evolve over time in a video.\n\nAttention mechanisms have emerged as a promising approach to address these limitations by enabling models to focus on relevant features across modalities. Early attention-based methods primarily implemented selfattention within individual modalities or simple crossmodal attention between pairs of modalities. Praveen et al.  [37]  implemented a joint cross-attention fusion model, and Zhang et al.  [54]  introduced TEMMA, a multi-head attention module. These approaches demonstrated improved performance but created limited pathways for information exchange, failing to utilize the complementary nature of multi-modal data completely. Most existing works have concentrated on categorical emotion recognition  [16] , with comparatively less exploration of attention mechanisms for continuous valence-arousal prediction.\n\nThe Affective Behavior Analysis in-the-wild (ABAW) competitions have significantly advanced research in this domain by providing standardized benchmarks and challenging in-the-wild datasets  [13, 24, 26, 27] . Despite advancements, several significant challenges still remain unaddressed. First, existing fusion approaches often struggle to combine information across modalities while maintaining temporal coherence effectively  [6] . Second, attention mechanisms have not been fully exploited to capture the complex inter-relationships between different emotional cues  [51] . Third, the direct regression of valence-arousal values in Cartesian coordinates may not optimally align with psychological models of emotion  [46] . We propose a novel Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) with several key innovations to address these limitations. The key contributions are: • We employ state-of-the-art (SOTA) modality-specific encoders: Swin Transformer for visual data, HuBERT for audio, and RoBERTa for text, to extract robust feature representations from each data stream. • The proposed model employs a cross-modal attention mechanism that uses six distinct attention pathways and enables interactions between all modality pairs through weighted attention from other modalities. This design allows each modality to both inform and be informed by others, creating a rich information exchange network. • This is followed by self-attention within its modalityspecific encoder, utilizing a multi-headed attention module akin to that used in the Bidirectional Encoder Representations from Transformers (BEiT) model. • Additionally, we exploit polar coordinate form, representing emotions as angle (θ) and intensity (I) rather than directly predicting valence-arousal values. This representation aligns naturally with psychological models of the emotion circumplex  [38] . • Extensive experiments on the Aff-Wild2 dataset  [18]  demonstrate that the proposed approach significantly outperforms existing methods as measured by the Concordance Correlation Coefficient (CCC)  [19] .\n\nThe results validate the effectiveness of our bidirectional multi-modal attention architecture and polar coordinate prediction framework for capturing complex emotional expressions in conversational videos.\n\nThis paper is organized as follows: Section 2 reviews related work on emotion recognition, multi-modal fusion, and attention mechanisms. Section 3 presents our proposed methodology, which includes the multi-modal attention architecture and the polar coordinate prediction framework. Section 4 details the experimental setup, results based on the Aff-Wild2 dataset, and the ablation studies and analyzes the contributions of individual components. Finally, Section 5 concludes the paper and discusses future directions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "This section reviews the evolution of emotion recognition approaches, starting with traditional unimodal methods, progressing to multi-modal fusion techniques, and examining the role of attention mechanisms. It also explores the shift from categorical to dimensional models of emotion representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unimodal Emotion Recognition",
      "text": "Early research in emotion recognition often focused on single modalities such as visual cues, audio signals, or textual content.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual-Based Approaches",
      "text": "Visual emotion recognition primarily focuses on analyzing facial expressions and body gestures. Early methods relied on handcrafted features to capture spatio-temporal information, but these often required significant prior knowledge and demonstrated a disconnect between the extracted features and actual emotional expressions. Recently, deep learning models, particularly Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), have become the preferred approach  [12, 54, 55] . For facial expressions, models like SISTCM (Super Imagebased Spatio-Temporal Convolutional Model)  [44]  and twostream LSTM models have been developed to capture local spatio-temporal features and global temporal cues related to emotional changes, with SISTCM utilizing 2D convolution for efficiency. The advancements in body gesture recognition have emerged through representation methods based on body joint movements, including the Attention-based Channel-wise Convolutional Model (ACCM), which employs channel-wise convolution and attention mechanisms to learn key joint features. Beyond facial expressions and body gestures, other visual cues like eye movement and body posture also play a significant role in emotion recognition  [7] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio-Based Approaches",
      "text": "Audio-based emotion recognition, also known as Speech Emotion Recognition (SER), involves analyzing speech signals to identify emotional states. Traditional methods focused on extracting low-level features such as Melfrequency cepstral coefficients (MFCCs), pitch, and energy  [4] . Statistical analysis was then frequently applied to these handcrafted features. However, with the advancement of deep learning, techniques such as Deep Neural Networks (DNNs), CNNs using spectrograms or MFCCs as inputs, and RNNs like LSTM networks and Gated Recurrent Units (GRUs) have demonstrated significant improvements in SER performance. Additionally, attention mechanisms have gained popularity in this field, allowing models to focus on the most important parts of the speech signal.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Text-Based Approaches",
      "text": "Text-based emotion recognition, often referred to as sentiment analysis, examines written text to identify the emotions or sentiments expressed within it. Early methods primarily relied on lexicon-based approaches and traditional machine learning classifiers, such as Support Vector Machines, which were trained using features like bag-of-words or Term Frequency -Inverse Document Frequency (TF-IDF). Deep learning techniques have revolutionized textbased emotion recognition. CNNs can extract local patterns, while RNNs, LSTMs, and GRUs-effectively capture sequential dependencies in text. Pre-trained word embeddings like Word2Vec and GloVe are used to represent words semantically. Transformer networks have also achieved SOTA results in text-based emotion recognition by effectively modeling long-range dependencies and contextualmodal approaches, which can be affected by noise, occlusions, or the masking of emotions  [21] , researchers have increasingly turned their attention to multi-modal emotion recognition (MER)  [5] . MER utilizes information from various modalities, including audio, visual (such as facial expressions and body gestures), and text, to achieve more accurate emotion recognition. The fundamental principle behind this approach is that different modalities can provide complementary insights into human emotions.\n\nVarious multi-modal fusion techniques have been explored:\n\n• Early Fusion (Feature Fusion): This approach involves concatenating features extracted from different modalities at an early stage and then feeding the combined feature vector into a classifier. For example, audio and visual features might be extracted using CNNs and then combined before being processed by an RNN. • Late Fusion (Decision Fusion): In late fusion, separate classifiers are trained for each modality, and their individual predictions (e.g., probability scores) are then combined using methods like averaging, weighted averaging, or voting to make a final emotion prediction. • Hybrid Approaches: These methods combine aspects of early and late fusion. For instance, features from different modalities might be processed separately by subnetworks, and the resulting intermediate representations are then fused before the final classification layer. Deep learning models have been instrumental in developing advanced multi-modal fusion strategies. Techniques like concatenation, element-wise addition or multiplication, and more sophisticated methods using attention mechanisms or bilinear pooling are commonly employed to integrate information from multiple modalities.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Attention Mechanisms For Emotion Recognition",
      "text": "Attention mechanisms have become a crucial component in modern emotion recognition systems, enabling models to selectively focus on the most relevant parts of the input data across different modalities and time steps  [26, 29] . In visual emotion recognition, attention can help the model focus on emotion-relevant facial regions or specific body joints. In In multi-modal emotion recognition, attention mechanisms play a vital role in learning the inter-modal relationships and determining the contribution of each modality to the final prediction. Cross-attention mechanisms allow the model to attend to relevant information in one modality based on the content of another modality, facilitating efficient information fusion. Self-attention mechanisms enable the model to capture intra-modal dependencies by attending to different parts of the same modality. Multi-head attention, as used in Transformer networks  [6] , allows the model to attend to different aspects of the input simultaneously.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dimensional Models Of Emotion",
      "text": "Early research in emotion recognition primarily focused on identifying a specific set of basic emotions, such as anger, disgust, fear, happiness, sadness, surprise, and sometimes neutrality  [54] . However, researchers have shifted towards dimensional models of emotion  [34, 43] . These models depict emotions as points within a continuous space characterized by dimensions like valence (pleasantness) and arousal (intensity)  [52] . Additionally, some models incorporate a third dimension, which is dominance (the level of control).\n\nThe Valence-Arousal (VA) space is the most commonly used dimensional representation of emotions. In this model, valence ranges from negative to positive, while arousal varies from passive to active. This two-dimensional framework allows for a circumplex representation of emotions, where different specific emotions can be mapped to distinct regions within the space. The Affective Behavior Analysis in-the-wild (ABAW) competitions have played a significant role in advancing valence-arousal estimation in real-world scenarios by providing large-scale annotated datasets, such as Aff-Wild and Aff-Wild2  [13, 24, [26] [27] [28] .\n\nBuilding on previous advancements, we propose MAVEN, a novel multi-modal attention framework for valence-arousal estimation. The next section describes our proposed model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Model: Maven",
      "text": "This section details MAVEN (Multi-modal Attention for Valence-arousal Emotion Network), our novel approach to continuous emotion recognition in conversational videos. Figure  3  illustrates the overall architecture of our proposed model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overview",
      "text": "MAVEN integrates information from three modalities: visual, audio, and text, employing an attention mechanism that enhances the exchange of information between these modalities. The system consists of five key components: • Modality-specific feature extractors for visual, audio, and textual data",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "• A Comprehensive Cross-Modal Attention Mechanism With Bidirectional Information Flow • A Bidirectional Multi-Headed Self-Attention Module For Each Modality • A Beit-Based Encoder Refinement Layer • A Polar Coordinate-Based Emotion Prediction Framework",
      "text": "Modality-Specific Feature Extraction",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Features",
      "text": "We employ the Swin Transformer  [31]  to extract visual features from facial regions in each video frame. The Swin Transformer combines local attention with shifted windows, enabling efficient modeling of hierarchical visual features while maintaining linear computational complexity relative to image size.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Patch Embedding",
      "text": "Given a video sequence {V 1 , V 2 , ..., V T } with T frames, each frame V i ∈ R H×W ×3 is split into non-overlapping patches of size P × P :\n\nThese patches are then projected into a d v -dimensional embedding space:\n\nwhere W p ∈ R (P 2 •3)×dv and b p ∈ R dv are learnable parameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Shifted Window Multi-Head Self-Attention (Sw-Msa)",
      "text": "To model long-range dependencies efficiently, attention is applied within local windows that shift across layers to capture cross-window interactions:\n\nwhere\n\nand h as the number of attention heads.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Visual Feature Output",
      "text": "After processing through L transformer layers with hierarchical merging, we obtain the final visual feature sequence:\n\nwhere F V represents the sequence of visual features with dimension d v = 1024.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Audio Features",
      "text": "For audio processing, we utilize HuBERT  [10] , a selfsupervised speech representation learning model with superior performance in capturing acoustic characteristics relevant to emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "Given the audio signal A(t) corresponding to the video frames, we first compute log-mel spectrogram features:\n\nwhere T ′ is the number of time frames in the spectrogram and F is the number of mel frequency bands.\n\nwhere T ′ is the number of time frames in the spectrogram and F is the number of mel frequency bands.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Audio Encoder",
      "text": "A CNN maps X A to initial hidden representations:\n\nwhere T ′′ is the reduced temporal dimension and d a is the feature dimension.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Audio Feature Output",
      "text": "After processing through the HuBERT model, we obtain the final audio feature sequence:\n\nwhere F A represents the sequence of audio features with dimension d a = 768.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Text Features",
      "text": "We employ RoBERTa  [30] , a robust variant of BERT, to extract semantic features from transcribed speech, capturing emotional markers from linguistic content.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Token Embedding",
      "text": "For a sequence of tokens {t 1 , t 2 , ..., t S } derived from the transcribed speech, we compute embeddings as:\n\nwhere E token is the token embedding function, E pos is the positional embedding function, and d t is the embedding dimension.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multi-Head Self-Attention",
      "text": "The token embeddings are processed through multiple layers of self-attention:\n\nwhere each attention head computes the above for i = 1, . . . , h.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Text Feature Output",
      "text": "The processed textual features are obtained as:\n\nwhere F T represents the sequence of text features with dimension d t = 768.\n\nTo align text features with visual and audio features that have temporal dimension T , we apply temporal interpolation:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Cross-Modal Attention",
      "text": "The core contribution of our approach is the bidirectional cross-modal attention mechanism, which enables information exchange between all pairs of modalities. This mechanism generates six distinct attention pathways: (visual → audio, visual → text, audio → visual, audio → text, text → visual, text → audio). This allows each modality to both provide information to and receive information from the others.\n\nFor each modality pair (m, n) where m, n ∈ {visual, audio, text} and m ̸ = n, we compute cross-modal attention as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dimension Of The Keys And Queries",
      "text": "For example, the attention from audio to visual features is computed as:\n\nSimilarly, we compute attention for the other five directional pathways, where A V →A , A V →T , A A→T , A T →V , and A T →A are obtained via cross-attention mechanisms using modality-specific query, key, and value projections of F V , F A , and",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Beit-Based Encoder Refinement",
      "text": "Following cross-modal attention, we apply a two-step refinement process: (1) Bidirectional Self-Attention Refinement, where intra-modal dependencies are strengthened, and (2) BEiT-based Encoder Refinement, which further contextualizes and integrates multi-modal features. To enhance intra-modal relationships, we apply self-attention within each modality:\n\nThe self-attention module employs a multi-head attention mechanism followed by residual connections and layer normalization: SelfAtt(F ) = LayerNorm (MHA(F, F, F ) + F )  (18)  where:\n\n• MHA(•) denotes the multi-head self-attention operation, • F represents the input feature matrix for a given modality (F V , F A , F T ), • LayerNorm(•) ensures stable training by normalizing the updated features.\n\nAfter intra-modal refinement, the features are concatenated and projected into a unified multi-modal representation:  (20)  where F concat ∈ R T ×(dv+da+dt) , W proj ∈ R (dv+da+dt)×dproj , and b proj ∈ R dproj are learnable parameters.\n\nTo capture long-range dependencies and contextualized representations, we employ two BEiT-based transformer encoders  [2] . The attention mechanism is formulated as:\n\nwhere: • h t and h t ′ are hidden states at positions t and t ′ , • W d , W ′ d , b d , and v d are learnable parameters, • l t is the refined representation at position t.\n\nThe final multi-modal feature representation is obtained after passing through two consecutive BEiT-based encoders:\n\nThis process ensures that the final multi-modal embedding captures context-aware emotional cues, allowing for robust downstream emotion prediction.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Polar Coordinate Emotion Prediction",
      "text": "The final component of our proposed architecture is a specialized emotion prediction framework that operates in polar coordinates, aligning with the valence-arousal emotion circumplex model widely used in affective computing.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Feature Pooling",
      "text": "First, we apply temporal average pooling to obtain a fixeddimensional representation:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emotion Predictor",
      "text": "A feedforward neural network with three fully connected layers and ReLU activations processes the pooled features:\n\nwhere\n\n, and b 3 ∈ R 2 are learnable parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Polar Conversion",
      "text": "The network predicts two values: intensity I and angle θ. These are converted to valence and arousal coordinates:\n\nThis parametrization aligns with psychological models of emotion, where similar emotions are adjacent in the emotion circumplex. It also enforces the constraint that emotions with similar valence-arousal values have similar representations in the model's internal space.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training And Optimization",
      "text": "The model is trained to minimize the Mean Squared Error (MSE) between predicted and ground-truth valence-arousal values:\n\nwhere v i and a i are the ground-truth valence and arousal values, vi and âi are the predicted values, and N is the number of samples.\n\nWe employ the AdamW optimizer with a learning rate of 1 × 10 -4 and weight decay of 1 × 10 -2 . To prevent overfitting, we apply dropout with a rate of 0.2 after each fully connected layer in the emotion predictor.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Results",
      "text": "In this section, we present the experimental results to evaluate the performance of the proposed model. The analysis focuses on understanding the effectiveness of the model across CCC metric.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our approach on the Aff-Wild2 dataset , the largest in-the-wild audiovisual database for valence-arousal estimation, containing 545 videos with 2.8M frames, each annotated with continuous valence and arousal values in [-1, 1]  [28] . Following the ABAW competition's train/validation/test split, we extract facial regions using a face detector, aligning them to 112×112 pixels. Audio is sampled at 48kHz, and text transcriptions are generated via automatic speech recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "We train the model for 200 epochs using the Adam optimizer with a learning rate of 1 × 10 -4 , a batch size of 16, and a weight decay of 1 × 10 -3 . The training is conducted on NVIDIA A100 GPUs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Following the standard protocol in the ABAW competition, we use the Concordance Correlation Coefficient (CCC) as the primary evaluation metric:  x + σ 2 y + (µ x -µ y ) 2 where ρ is the Pearson correlation coefficient, σ x and σ y are the standard deviations, and µ x and µ y are the means of the predicted and ground truth values, respectively.\n\nThe overall performance is measured by the average CCC of valence and arousal: MAVEN achieves a CCC Avg of 0.3061, surpassing the baseline model's 0.22, demonstrating the effectiveness of the multi-modal attention framework.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Comparison",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To analyze MAVEN's design choices, we conduct ablation studies by removing individual modalities and evaluating their impact. Table  2  presents the performance of uni-modal variants on the Aff-Wild2 dataset, measured using the Concordance Correlation Coefficient (CCC). The contributions of each modality are as follows: • Visual Modality: Achieves the highest average CCC (0.1754), excelling in arousal detection (0.2459) by capturing subtle facial cues like widened eyes or furrowed brows, critical for emotion recognition in conversational videos. • Audio Modality: Yields a lower average CCC (0.0299), with better valence performance (0.1283) through prosody and pitch, but struggles in noisy in-the-wild settings, limiting its reliability. • Text Modality: Provides minimal impact (CCC Avg: 0.0013), offering contextual cues to disambiguate emotions like sarcasm, though sparse emotional markers in transcripts constrain its contribution. These results highlight the necessity of multi-modal integration, as MAVEN's full model achieves a significantly higher CCC (0.3061), leveraging the synergistic strengths of all modalities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces MAVEN, a multi-modal attention architecture for valence and arousal recognition. It features a bidirectional cross-modal attention mechanism for effective information exchange, a self-attention refinement module for improved modality specificity, and a polar coordinate prediction framework for intuitive emotional representation. Achieving a SOTA CCC of 0.3061, the model demonstrates that both the attention mechanism and polar coordinate framework significantly enhance emotion recognition by effectively utilizing complementary multi-modal cues. Future work for our proposed methodology lies in enhancing the temporal modeling capabilities of the framework to better capture the dynamics of emotion evolution.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Valence-Arousal Emotion Circumplex",
      "page": 2
    },
    {
      "caption": "Figure 2: Valence-Arousal Distribution for a video sample from",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the overall architecture of our proposed",
      "page": 4
    },
    {
      "caption": "Figure 3: Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) Architecture. The model processes visual, audio,",
      "page": 5
    },
    {
      "caption": "Figure 4: Example frames from the Aff-Wild2 dataset showing",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "2Department of Data Science and AI, Monash University, Melbourne, Australia"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "spectrum, enriching the understanding of the complexity of"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "human emotional experiences."
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "The detection and analysis of human emotions in nat-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "ural\nsettings pose significant challenges due to variations"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "in expressions,\nindividual differences, cultural\ninfluences,"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "and the often subtle nature of emotional cues [18, 52]. This"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "complexity requires a multi-modal approach, as people nat-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "urally express and perceive emotions through various chan-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "nels,\nincluding facial\nexpressions,\nvoice\nintonation,\nlan-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "guage,\nand gestures\n[5, 53].\nIntegrating complementary"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "information from each modality enhances\nthe robustness"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "and accuracy of emotion recognition systems. While fa-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "cial expressions might reveal visible emotional cues, speech"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "prosody can uncover subtle emotional undertones, and lin-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "guistic content can provide contextual information essential"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "for accurate interpretation [12, 32]."
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "The field\nhas witnessed\nsignificant\nadvancements\nin"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "emotion recognition through deep learning approaches in"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "recent years.\nInitial\nresearch emphasizes unimodal\nsys-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "tems in facial expression analysis using efficient deep learn-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "ing models like EfficientNet and extracting frame-level fea-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "tures through EmotiEffNet\n[39, 40]. Approaches such as"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "EmoFAN-VR achieved notable success\nin controlled en-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": ""
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "vironments but often struggled with in-the-wild scenarios"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "characterized by varying illumination, occlusions, and pose"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "variations [8, 21].\nResearchers explored other modalities"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "separately, creating specialized models for emotion recog-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "nition in speech using spectral features and recurrent archi-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "tectures\nsuch as Vesper\n[4] and text-based systems using"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "natural language processing techniques."
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "The shift\ntowards multi-modal systems reflects how hu-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "mans perceive and express emotions through various chan-"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "nels\nsimultaneously [53].\nInitial multi-modal approaches"
        },
        {
          "1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India": "employed simple fusion strategies, such as feature concate-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "the inter-dependencies between modalities [5]. Advanced"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "techniques were developed,\nincluding tensor-based fusion,"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "specifically Tucker Tensor Regression and Tensor Regres-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "sion Networks [34], bilinear pooling, and various ensemble"
        },
        {
          "nation or decision-level integration, which failed to capture": "techniques like EVAEF [29] . While these approaches have"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "enhanced performance,\nthey often treat different modali-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "ties\nas\nindependent\nsources of\ninformation.\nTo address"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "this, Meng et al. [33] uses temporal encoders, specifically"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "transformer-based and Long Short-Term Memory (LSTM)-"
        },
        {
          "nation or decision-level integration, which failed to capture": "based networks,\nto better understand how emotions evolve"
        },
        {
          "nation or decision-level integration, which failed to capture": "over time in a video."
        },
        {
          "nation or decision-level integration, which failed to capture": "Attention mechanisms\nhave\nemerged\nas\na\npromis-"
        },
        {
          "nation or decision-level integration, which failed to capture": "ing\napproach\nto\naddress\nthese\nlimitations\nby\nenabling"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "models\nto\nfocus\non\nrelevant\nfeatures\nacross modalities."
        },
        {
          "nation or decision-level integration, which failed to capture": "Early attention-based methods primarily implemented self-"
        },
        {
          "nation or decision-level integration, which failed to capture": "attention within\nindividual modalities\nor\nsimple\ncross-"
        },
        {
          "nation or decision-level integration, which failed to capture": "modal attention between pairs of modalities.\nPraveen et"
        },
        {
          "nation or decision-level integration, which failed to capture": "al.\n[37]\nimplemented a joint cross-attention fusion model,"
        },
        {
          "nation or decision-level integration, which failed to capture": "and Zhang et al. [54] introduced TEMMA, a multi-head at-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "tention module. These approaches demonstrated improved"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "performance but created limited pathways for\ninformation"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "exchange,\nfailing to utilize the complementary nature of"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "multi-modal data completely. Most existing works have"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "concentrated on categorical emotion recognition [16], with"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "comparatively less exploration of attention mechanisms for"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "continuous valence-arousal prediction."
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "The Affective Behavior Analysis\nin-the-wild (ABAW)"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "competitions have significantly advanced research in this"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "domain by providing standardized benchmarks and chal-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "lenging in-the-wild datasets [13, 24, 26, 27]. Despite ad-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "vancements, several significant challenges still remain un-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "addressed.\nFirst,\nexisting fusion approaches often strug-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "gle to combine information across modalities while main-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "taining temporal coherence effectively [6]. Second, atten-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "tion mechanisms have not been fully exploited to capture"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "the complex inter-relationships between different emotional"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "cues [51].\nThird,\nthe direct\nregression of valence-arousal"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "values\nin Cartesian coordinates may not optimally align"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "with psychological models of emotion [46]. We propose a"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "novel Multi-modal Attention for Valence-Arousal Emotion"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "Network (MAVEN) with several key innovations to address"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "these limitations. The key contributions are:"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "• We employ state-of-the-art (SOTA) modality-specific en-"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "coders: Swin Transformer\nfor visual data, HuBERT for"
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "audio, and RoBERTa for\ntext,\nto extract\nrobust\nfeature"
        },
        {
          "nation or decision-level integration, which failed to capture": "representations from each data stream."
        },
        {
          "nation or decision-level integration, which failed to capture": ""
        },
        {
          "nation or decision-level integration, which failed to capture": "• The proposed model\nemploys\na\ncross-modal\nattention"
        },
        {
          "nation or decision-level integration, which failed to capture": "mechanism that uses six distinct attention pathways and"
        },
        {
          "nation or decision-level integration, which failed to capture": "enables interactions between all modality pairs through"
        },
        {
          "nation or decision-level integration, which failed to capture": "weighted attention from other modalities. This design al-"
        },
        {
          "nation or decision-level integration, which failed to capture": "lows each modality to both inform and be informed by"
        },
        {
          "nation or decision-level integration, which failed to capture": "others, creating a rich information exchange network."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "chines, which were trained using features like bag-of-words"
        },
        {
          "representation.": "2.1. Unimodal Emotion Recognition",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "or Term Frequency -\nInverse Document Frequency (TF-"
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "IDF). Deep learning techniques have revolutionized text-"
        },
        {
          "representation.": "Early research in emotion recognition often focused on sin-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "based emotion recognition.\nCNNs can extract\nlocal pat-"
        },
        {
          "representation.": "gle modalities such as visual cues, audio signals, or textual",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "terns, while RNNs, LSTMs, and GRUs- effectively capture"
        },
        {
          "representation.": "content.",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "sequential dependencies in text.\nPre-trained word embed-"
        },
        {
          "representation.": "2.1.1. Visual-Based Approaches",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "dings like Word2Vec and GloVe are used to represent words"
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "semantically.\nTransformer networks have\nalso achieved"
        },
        {
          "representation.": "Visual emotion recognition primarily focuses on analyzing",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "SOTA results in text-based emotion recognition by effec-"
        },
        {
          "representation.": "facial expressions and body gestures.\nEarly methods\nre-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "tively modeling long-range dependencies and contextual\n-"
        },
        {
          "representation.": "lied on handcrafted features to capture spatio-temporal\nin-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "modal approaches, which can be affected by noise, occlu-"
        },
        {
          "representation.": "formation, but these often required significant prior knowl-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "sions, or\nthe masking of emotions\n[21],\nresearchers have"
        },
        {
          "representation.": "edge and demonstrated a disconnect between the extracted",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "increasingly turned their attention to multi-modal emotion"
        },
        {
          "representation.": "features and actual emotional expressions. Recently, deep",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "recognition (MER) [5]. MER utilizes information from var-"
        },
        {
          "representation.": "learning models,\nparticularly Convolutional Neural Net-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "ious modalities,\nincluding audio, visual (such as facial ex-"
        },
        {
          "representation.": "works\n(CNNs)\nand Recurrent Neural Networks\n(RNNs),",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "pressions and body gestures), and text, to achieve more ac-"
        },
        {
          "representation.": "have become\nthe preferred approach [12, 54, 55].\nFor",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "curate emotion recognition. The fundamental principle be-"
        },
        {
          "representation.": "facial\nexpressions, models\nlike SISTCM (Super\nImage-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "hind this approach is that different modalities can provide"
        },
        {
          "representation.": "based Spatio-Temporal Convolutional Model) [44] and two-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "complementary insights into human emotions."
        },
        {
          "representation.": "stream LSTM models have been developed to capture local",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "Various multi-modal\nfusion techniques have been ex-"
        },
        {
          "representation.": "spatio-temporal features and global temporal cues related to",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "plored:"
        },
        {
          "representation.": "emotional changes, with SISTCM utilizing 2D convolution",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "• Early Fusion (Feature Fusion): This approach involves"
        },
        {
          "representation.": "for efficiency.\nThe advancements in body gesture recog-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "concatenating features extracted from different modalities"
        },
        {
          "representation.": "nition have emerged through representation methods based",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "at an early stage and then feeding the combined feature"
        },
        {
          "representation.": "on body joint movements,\nincluding the Attention-based",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "vector into a classifier. For example, audio and visual fea-"
        },
        {
          "representation.": "Channel-wise Convolutional Model\n(ACCM), which em-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "tures might be extracted using CNNs and then combined"
        },
        {
          "representation.": "ploys channel-wise convolution and attention mechanisms",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "before being processed by an RNN."
        },
        {
          "representation.": "to learn key joint features. Beyond facial expressions and",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "• Late Fusion (Decision Fusion):\nIn late fusion,\nseparate"
        },
        {
          "representation.": "body gestures, other visual cues\nlike eye movement and",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "classifiers are trained for each modality, and their\nindi-"
        },
        {
          "representation.": "body posture also play a significant role in emotion recog-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "vidual predictions (e.g., probability scores) are then com-"
        },
        {
          "representation.": "nition [7].",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "bined using methods like averaging, weighted averaging,"
        },
        {
          "representation.": "2.1.2. Audio-Based Approaches",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "or voting to make a final emotion prediction."
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "• Hybrid Approaches: These methods combine aspects of"
        },
        {
          "representation.": "Audio-based emotion recognition,\nalso known as Speech",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "early and late fusion. For instance, features from differ-"
        },
        {
          "representation.": "Emotion Recognition\n(SER),\ninvolves\nanalyzing\nspeech",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "ent modalities might be processed separately by subnet-"
        },
        {
          "representation.": "signals\nto\nidentify\nemotional\nstates.\nTraditional meth-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "works, and the resulting intermediate representations are"
        },
        {
          "representation.": "ods focused on extracting low-level\nfeatures such as Mel-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "then fused before the final classification layer."
        },
        {
          "representation.": "frequency cepstral\ncoefficients\n(MFCCs),\npitch,\nand en-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "Deep learning models have been instrumental\nin devel-"
        },
        {
          "representation.": "ergy [4].\nStatistical analysis was then frequently applied",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "oping advanced multi-modal fusion strategies. Techniques"
        },
        {
          "representation.": "to these handcrafted features. However, with the advance-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "like concatenation, element-wise addition or multiplication,"
        },
        {
          "representation.": "ment of deep learning, techniques such as Deep Neural Net-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "and more\nsophisticated methods using attention mecha-"
        },
        {
          "representation.": "works (DNNs), CNNs using spectrograms or MFCCs as in-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "nisms or bilinear pooling are commonly employed to in-"
        },
        {
          "representation.": "puts, and RNNs like LSTM networks and Gated Recurrent",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "tegrate information from multiple modalities."
        },
        {
          "representation.": "Units (GRUs) have demonstrated significant improvements",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "in SER performance. Additionally, attention mechanisms",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "2.2. Attention Mechanisms\nfor Emotion Recogni-"
        },
        {
          "representation.": "have gained popularity in this field, allowing models to fo-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "tion"
        },
        {
          "representation.": "cus on the most important parts of the speech signal.",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "Attention mechanisms have become a crucial component in"
        },
        {
          "representation.": "2.1.3. Text-Based Approaches",
          "machine learning classifiers,\nsuch as Support Vector Ma-": ""
        },
        {
          "representation.": "",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "modern emotion recognition systems, enabling models to"
        },
        {
          "representation.": "Text-based emotion recognition, often referred to as senti-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "selectively focus on the most relevant parts of the input data"
        },
        {
          "representation.": "ment analysis, examines written text\nto identify the emo-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "across different modalities and time steps [26, 29]. In visual"
        },
        {
          "representation.": "tions or sentiments expressed within it. Early methods pri-",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "emotion recognition, attention can help the model focus on"
        },
        {
          "representation.": "marily relied on lexicon-based approaches and traditional",
          "machine learning classifiers,\nsuch as Support Vector Ma-": "emotion-relevant\nfacial\nregions or specific body joints.\nIn"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "used dimensional representation of emotions. In this model,": "valence\nranges\nfrom negative\nto positive, while\narousal"
        },
        {
          "used dimensional representation of emotions. In this model,": "varies from passive to active. This two-dimensional frame-"
        },
        {
          "used dimensional representation of emotions. In this model,": "work allows for a circumplex representation of emotions,"
        },
        {
          "used dimensional representation of emotions. In this model,": "where different specific emotions can be mapped to distinct"
        },
        {
          "used dimensional representation of emotions. In this model,": "regions within the space. The Affective Behavior Analysis"
        },
        {
          "used dimensional representation of emotions. In this model,": "in-the-wild (ABAW) competitions have played a significant"
        },
        {
          "used dimensional representation of emotions. In this model,": "role in advancing valence-arousal estimation in real-world"
        },
        {
          "used dimensional representation of emotions. In this model,": "scenarios by providing large-scale annotated datasets, such"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "as Aff-Wild and Aff-Wild2 [13, 24, 26–28]."
        },
        {
          "used dimensional representation of emotions. In this model,": "Building\non\nprevious\nadvancements,\nwe\npropose"
        },
        {
          "used dimensional representation of emotions. In this model,": "MAVEN,\na\nnovel multi-modal\nattention\nframework\nfor"
        },
        {
          "used dimensional representation of emotions. In this model,": "valence-arousal estimation. The next section describes our"
        },
        {
          "used dimensional representation of emotions. In this model,": "proposed model."
        },
        {
          "used dimensional representation of emotions. In this model,": "3. Proposed Model: MAVEN"
        },
        {
          "used dimensional representation of emotions. In this model,": "This\nsection details MAVEN (Multi-modal Attention for"
        },
        {
          "used dimensional representation of emotions. In this model,": "Valence-arousal Emotion Network), our novel approach to"
        },
        {
          "used dimensional representation of emotions. In this model,": "continuous emotion recognition in conversational videos."
        },
        {
          "used dimensional representation of emotions. In this model,": "Figure 3 illustrates the overall architecture of our proposed"
        },
        {
          "used dimensional representation of emotions. In this model,": "model."
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "3.1. Overview"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "MAVEN integrates information from three modalities: vi-"
        },
        {
          "used dimensional representation of emotions. In this model,": "sual, audio, and text, employing an attention mechanism"
        },
        {
          "used dimensional representation of emotions. In this model,": "that enhances the exchange of\ninformation between these"
        },
        {
          "used dimensional representation of emotions. In this model,": "modalities. The system consists of five key components:"
        },
        {
          "used dimensional representation of emotions. In this model,": "• Modality-specific feature extractors for visual, audio, and"
        },
        {
          "used dimensional representation of emotions. In this model,": "textual data"
        },
        {
          "used dimensional representation of emotions. In this model,": "• A comprehensive cross-modal attention mechanism with"
        },
        {
          "used dimensional representation of emotions. In this model,": "bidirectional information flow"
        },
        {
          "used dimensional representation of emotions. In this model,": "• A bidirectional multi-headed self-attention module\nfor"
        },
        {
          "used dimensional representation of emotions. In this model,": "each modality"
        },
        {
          "used dimensional representation of emotions. In this model,": "• A BEiT-based encoder refinement layer"
        },
        {
          "used dimensional representation of emotions. In this model,": "• A polar coordinate-based emotion prediction framework"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "Modality-Specific Feature Extraction"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "3.2. Visual Features"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "We\nemploy the Swin Transformer\n[31]\nto extract visual"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "features\nfrom facial\nregions\nin each video frame.\nThe"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "Swin Transformer\ncombines\nlocal\nattention with shifted"
        },
        {
          "used dimensional representation of emotions. In this model,": "windows, enabling efficient modeling of hierarchical visual"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "features while maintaining linear computational complexity"
        },
        {
          "used dimensional representation of emotions. In this model,": "relative to image size."
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "3.2.1. Patch Embedding"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "Given a video sequence {V1, V2, ..., VT } with T frames,"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "is\nsplit\ninto non-overlapping\neach frame Vi ∈ RH×W ×3"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "patches of size P × P :"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "P 2 ×(P 2·3)"
        },
        {
          "used dimensional representation of emotions. In this model,": "(1)\nXp = PatchPartition(Vi) ∈ R HW"
        },
        {
          "used dimensional representation of emotions. In this model,": ""
        },
        {
          "used dimensional representation of emotions. In this model,": "These patches are then projected into a dv-dimensional"
        },
        {
          "used dimensional representation of emotions. In this model,": "embedding space:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cross-Modal Attention": "Enhanced Video Features",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "Video-to-Audio Attention",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "Video-to-Text Attention",
          "BEIT Multi-Headed Attention": "Projection Layer"
        },
        {
          "Cross-Modal Attention": "",
          "BEIT Multi-Headed Attention": "BEiT-3 Model"
        },
        {
          "Cross-Modal Attention": "Enhanced Audio Features",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "Audio-to-Video Attention",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "",
          "BEIT Multi-Headed Attention": "Multi-Head"
        },
        {
          "Cross-Modal Attention": "Audio-to-Text Attention",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "",
          "BEIT Multi-Headed Attention": "Self Attention"
        },
        {
          "Cross-Modal Attention": "Enhanced Text Features",
          "BEIT Multi-Headed Attention": "Attended"
        },
        {
          "Cross-Modal Attention": "Text-to-Video Attention",
          "BEIT Multi-Headed Attention": "Features"
        },
        {
          "Cross-Modal Attention": "",
          "BEIT Multi-Headed Attention": ""
        },
        {
          "Cross-Modal Attention": "Text-to-Audio Attention",
          "BEIT Multi-Headed Attention": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "P 2 ×dv\n(2)\nZ0 = XpWp + bp,\nZ0 ∈ R HW"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "are learnable\nwhere Wp ∈ R(P 2·3)×dv\nand bp ∈ Rdv"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "parameters."
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "3.2.2. Shifted Window Multi-Head Self-Attention (SW-"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "MSA)"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "To model\nlong-range dependencies efficiently, attention is"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "applied within local windows that shift across layers to cap-"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "ture cross-window interactions:"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "(cid:18) QK T"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "√\nV\nAttention(Q, K, V ) = softmax\n(3)"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "dk"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "where Q, K, V\n= Zl−1WQ, Zl−1WK, Zl−1WV , with"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "dk = dv/h and h as the number of attention heads."
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "3.2.3. Visual Feature Output"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "After processing through L transformer layers with hierar-"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "chical merging, we obtain the final visual feature sequence:"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "(4)\nFV = SwinT(V1, V2, ..., VT ) ∈ RT ×dv"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "where FV represents the sequence of visual features with"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "dimension dv = 1024."
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "3.3. Audio Features"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "For\naudio processing, we utilize HuBERT [10],\na\nself-"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "supervised speech representation learning model with su-"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "perior performance in capturing acoustic characteristics rel-"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "evant to emotion recognition."
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "3.3.1. Feature Extraction"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "Given the\naudio signal A(t)\ncorresponding to the video"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "frames, we first compute log-mel spectrogram features:"
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": ""
        },
        {
          "bidirectional self-attention, and predicts emotions using a polar coordinate system.": "(5)\nXA = MelFilterBank(STFT(A(t))) ∈ RT ′×F ,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "tional pathways, where AV →A, AV →T , AA→T , AT →V ,"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "The token embeddings are processed through multiple lay-",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "cross-attention mechanisms\nand AT →A are obtained via"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "ers of self-attention:",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "using modality-specific query, key, and value projections"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "∈\nand F ′\nof FV , FA,"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "(9)\nMHA(Q, K, V ) = Concat(head1, . . . , headh)WO,",
          "Similarly, we compute attention for the other five direc-": "T , with dimensions AA→V , AT →V"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "RT ×dv , AV →A, AT →A ∈ RT ×da , and AV →T , AA→T ∈"
        },
        {
          "3.4.2. Multi-Head Self-Attention": ", KW K\n, V W V\n),\nheadi = Attention(QW Q",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "i\ni",
          "Similarly, we compute attention for the other five direc-": "RT ×dt."
        },
        {
          "3.4.2. Multi-Head Self-Attention": "(10)",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "3.6. BEiT-based Encoder Refinement"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "i =\nwhere\neach attention head computes\nthe\nabove\nfor",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "1, . . . , h.",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "Following cross-modal attention, we apply a two-step re-"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "finement process:\n(1) Bidirectional Self-Attention Refine-"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "3.4.3. Text Feature Output",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "ment, where\nintra-modal dependencies\nare\nstrengthened,"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "The processed textual features are obtained as:",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "and\n(2) BEiT-based Encoder Refinement, which further"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "contextualizes and integrates multi-modal features. To en-"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "(11)\nFT = RoBERTa(t1, t2, ..., tS) ∈ RS×dt",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "hance\nintra-modal\nrelationships, we\napply\nself-attention"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "",
          "Similarly, we compute attention for the other five direc-": "within each modality:"
        },
        {
          "3.4.2. Multi-Head Self-Attention": "represents the sequence of text features with\nwhere FT",
          "Similarly, we compute attention for the other five direc-": ""
        },
        {
          "3.4.2. Multi-Head Self-Attention": "dimension dt = 768.",
          "Similarly, we compute attention for the other five direc-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "dt,t′ = tanh(Wdht + W′\ndht′ + bd),"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "αt,t′ = vT\nd dt,t′,"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "at,t′ = softmax(αt,t′),"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "T(cid:88) t\nlt =\nat,t′ht′."
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "′=1"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "where:"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "• ht and ht′ are hidden states at positions t and t′,"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "• Wd, W′\nd, bd, and vd are learnable parameters,"
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": ""
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "•\nlt is the refined representation at position t."
        },
        {
          "encoders [2]. The attention mechanism is formulated as:": "The final multi-modal feature representation is obtained"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: compares MAVEN’s performance against the",
      "data": [
        {
          "Text only\n0.0019\n0.0006\n0.0013": ""
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "The contributions of each modality are as follows:"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "• Visual Modality:\nAchieves\nthe highest\naverage CCC"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "(0.1754), excelling in arousal detection (0.2459) by cap-"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "turing subtle facial cues like widened eyes or\nfurrowed"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "brows, critical for emotion recognition in conversational"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "videos."
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "• Audio Modality: Yields a lower average CCC (0.0299),"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "with\nbetter\nvalence\nperformance\n(0.1283)\nthrough"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "prosody and pitch, but struggles in noisy in-the-wild set-"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "tings, limiting its reliability."
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "• Text Modality:\nProvides minimal\nimpact\n(CCC Avg:"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": ""
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "0.0013), offering contextual cues to disambiguate emo-"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": ""
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "tions like sarcasm,\nthough sparse emotional markers in"
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "transcripts constrain its contribution."
        },
        {
          "Text only\n0.0019\n0.0006\n0.0013": "These results highlight\nthe necessity of multi-modal\nin-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: compares MAVEN’s performance against the",
      "data": [
        {
          "work to better capture the dynamics of emotion evolution.": ""
        },
        {
          "work to better capture the dynamics of emotion evolution.": "References"
        },
        {
          "work to better capture the dynamics of emotion evolution.": ""
        },
        {
          "work to better capture the dynamics of emotion evolution.": "[1] Abhinav Anthiyur Aravindan,\nSriram Kalyan Chappidi,"
        },
        {
          "work to better capture the dynamics of emotion evolution.": ""
        },
        {
          "work to better capture the dynamics of emotion evolution.": "Anirudh Thumma,\nand Rohini Palanisamy.\nPrediction of"
        },
        {
          "work to better capture the dynamics of emotion evolution.": ""
        },
        {
          "work to better capture the dynamics of emotion evolution.": "arousal\nand valence\nstate\nfrom electrodermal\nactivity us-"
        },
        {
          "work to better capture the dynamics of emotion evolution.": "Current Directions\nin\ning wavelet based resnet50 model."
        },
        {
          "work to better capture the dynamics of emotion evolution.": ""
        },
        {
          "work to better capture the dynamics of emotion evolution.": "Biomedical Engineering, 9(1):555–558, 2023."
        },
        {
          "work to better capture the dynamics of emotion evolution.": "[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:"
        },
        {
          "work to better capture the dynamics of emotion evolution.": "arXiv preprint\nBert pre-training of\nimage\ntransformers."
        },
        {
          "work to better capture the dynamics of emotion evolution.": "arXiv:2106.08254, 2021. 7"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Zhang.\nAccurate eeg-based emotion recognition on com-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "2018."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "bined features using deep convolutional neural networks.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[16] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Ieee Access, 7:44317–44328, 2019.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "action unit\nrecognition: Aff-wild2, multi-task learning and"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[4] Weidong Chen, Xiaofen Xing, Peihao Chen, and Xiangmin",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "arcface. arXiv preprint arXiv:1910.04855, 2019. 2"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Xu. Vesper: A compact and effective pretrained model for",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[17] Dimitrios Kollias and Stefanos Zafeiriou. Exploiting multi-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "IEEE Transactions on Affective\nspeech emotion recognition.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "cnn features in cnn-rnn based dimensional emotion recogni-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Computing, 2024. 1, 3",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "IEEE Transactions on\ntion on the omg in-the-wild dataset."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[5] Yucel Cimtay,\nErhan Ekmekcioglu,\nand\nSeyma Caglar-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Affective Computing, 12(3):595–606, 2020."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Ozhan. Cross-subject multimodal emotion recognition based",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[18] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "on hybrid fusion.\nIEEE Access, 8:168865–168878, 2020. 1,",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "2, 3",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "unified framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[6] Denis Dresvyanskiy, Maxim Markitantov, Jiawei Yu, Hey-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "1, 2"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "sem Kaya, and Alexey Karpov. Multi-modal arousal and va-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[19] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "lence estimation under noisy conditions.\nIn Proceedings of",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "the IEEE/CVF Conference on Computer Vision and Pattern",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Recognition, pages 4773–4783, 2024. 2, 4",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "puter Vision, pages 3652–3660, 2021. 2"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[7] Cunhang Fan, Heng Xie, Jianhua Tao, Yongwei Li, Guanx-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[20] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "iong Pei, Taihao Li, and Zhao Lv.\nIcaps-reslstm:\nImproved",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "capsule network and residual\nlstm for eeg emotion recogni-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "tion. Biomedical Signal Processing and Control, 87:105422,",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "arXiv:1910.11111, 2019."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "2024. 3",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[8] Tom Gotsman, Neophytos Polydorou, and Abbas Edalat. Va-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "lence/arousal estimation of occluded faces from vr headsets.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "In 2021 IEEE Third International Conference on Cognitive",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Machine Intelligence (CogMI), pages 96–105.\nIEEE, 2021.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "1",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "127(6):907–929, 2019. 1, 3"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[9]\nSebastian Handrich, Laslo Dinges, Frerk Saxen, Ayoub Al-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[22] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Hamadi, and Sven Wachmuth.\nSimultaneous prediction of",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Zafeiriou.\nAnalysing affective behavior\nin the first abaw"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "valence/arousal and emotion categories in real-time. In 2019",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "2020 competition.\nIn 2020 15th IEEE International Confer-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "IEEE International Conference on Signal and Image Pro-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "ence on Automatic Face and Gesture Recognition (FG 2020),"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "cessing Applications (ICSIPA), pages 176–180. IEEE, 2019.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "pages 637–643. IEEE, 2020."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[10] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[23] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Mohamed.\nHubert:\nSelf-supervised speech representation",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "IEEE/ACM\nlearning by masked prediction of hidden units.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "arXiv:2105.03790, 2021."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "transactions on audio, speech, and language processing, 29:",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[24] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "3451–3460, 2021. 5",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[11]\nStephen Khor Wen Hwooi, Alice Othmani,\nand Aznul",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "mation, expression recognition, action unit detection & emo-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Qalid Md Sabri.\nDeep learning-based approach for con-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "tional reaction intensity estimation challenges.\nIn Proceed-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "tinuous affect prediction from facial expression images\nin",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "ings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "valence-arousal\nspace.\nIEEE Access,\n10:96053–96065,",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Pattern Recognition, pages 5889–5898, 2023. 2, 4"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "2022.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[25] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[12] Mustaqeem Khan, Wail Gueaieb, Abdulmotaleb El Saddik,",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Zafeiriou. Distribution matching for multi-task learning of"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "and Soonil Kwon. Mser: Multimodal speech emotion recog-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "classification tasks:\na large-scale study on faces & beyond."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "nition using cross-attention with deep fusion. Expert Systems",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "the AAAI Conference on Artificial Intelli-\nIn Proceedings of"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "with Applications, 245:122946, 2024. 1, 3",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "gence, pages 2813–2821, 2024."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[13] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[26] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "fanos Zafeiriou,\nIrene Kotsia, Alice Baird, Chris Gagne,"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Chunchang Shao, and Guanyu Hu. The 6th affective behav-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "ference on Computer Vision and Pattern Recognition, pages",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "ior analysis in-the-wild (abaw) competition.\nIn Proceedings"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "2328–2336, 2022. 2, 4",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "of\nthe IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[14] Dimitrios Kollias. Multi-label compound expression recog-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "tern Recognition, pages 4587–4598, 2024. 2, 3, 4"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "nition:\nC-expr database & network.\nIn Proceedings of",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "[27] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "the IEEE/CVF Conference on Computer Vision and Pattern",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "Dhall, Shreya Ghosh, Chunchang Shao,\nand Guanyu Hu."
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "Recognition, pages 5589–5598, 2023.",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "7th abaw competition: Multi-task learning and compound"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "[15] Dimitrios Kollias and Stefanos Zafeiriou. A multi-task learn-",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "expression recognition.\narXiv preprint arXiv:2407.03835,"
        },
        {
          "[3]\nJX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang, and YN": "ing & generation framework: Valence-arousal, action units",
          "& primary expressions.\narXiv preprint arXiv:1811.07771,": "2024. 2"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Zafeiriou,\nI Kotsia, Eric Granger, Marco Pedersoli, SL Ba-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "pressions, valence, arousal and action units for mobile de-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "con, Alice Baird, C Gagne, et al. Advancements in affective",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "vices. arXiv preprint arXiv:2203.13436, 2022. 1"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "and behavior analysis: The 8th abaw workshop and compe-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[40] Andrey V Savchenko.\nEmotieffnets\nfor\nfacial processing"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "tition. 2025. 4, 7, 8",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "in video-based valence-arousal prediction, expression clas-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[29] Xiaolong Liu, Lei Sun, Wenqiang Jiang, Fengyuan Zhang,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "of\nsification\nand\naction\nunit\ndetection.\nIn Proceedings"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Yuanyuan Deng, Zhaopei Huang, Liyu Meng, Yuchen Liu,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "the IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "and Chuanhe Liu. Evaef: Ensemble valence-arousal estima-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Recognition, pages 5716–5724, 2023. 1"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "tion framework in the wild. In Proceedings of the IEEE/CVF",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[41] Andrey V Savchenko. Hsemotion team at the 6th abaw com-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Conference on Computer Vision and Pattern Recognition,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "petition: Facial expressions, valence-arousal and emotion in-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "pages 5863–5871, 2023. 2, 3",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "tensity prediction. arXiv preprint arXiv:2403.11590, 2024."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[30] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[42] Andrey V Savchenko.\nHsemotion team at\nabaw-8 com-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "petition:\nAudiovisual\nambivalence/hesitancy,\nemotional"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "moyer, and Veselin Stoyanov. Roberta: A robustly optimized",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "arXiv\nmimicry intensity and facial expression recognition."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "bert pretraining approach. arXiv preprint arXiv:1907.11692,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "preprint arXiv:2503.10399, 2025."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "2019. 5",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": ""
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[43] Arman Savran, Ruben Gur, and Ragini Verma. Automatic"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "detection of emotion valence on faces using consumer depth"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Zhang, Stephen Lin, and Baining Guo.\nSwin transformer:",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "the IEEE International Confer-\ncameras.\nIn Proceedings of"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Hierarchical vision transformer using shifted windows.\nIn",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "ence on Computer Vision Workshops, pages 75–82, 2013. 4"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Proceedings of\nthe IEEE/CVF international conference on",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[44] Huihui Song, Qingshan Liu, Guojie Wang, Renlong Hang,"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "computer vision, pages 10012–10022, 2021. 4",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "and Bo Huang.\nSpatiotemporal\nsatellite image fusion us-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[32] Liyu Meng, Yuchen Liu, Xiaolong Liu, Zhaopei Huang,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "IEEE Journal of\ning deep convolutional neural networks."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Yuan Cheng, Meng Wang, Chuanhe Liu, and Qin Jin. Multi-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Selected Topics in Applied Earth Observations and Remote"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "arXiv\nmodal\nemotion estimation for\nin-the-wild videos.",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Sensing, 11(3):821–829, 2018. 3"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "preprint arXiv:2203.13032, 2022. 1",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[45] Rajkumar Theagarajan, Bir Bhanu, and Albert Cruz. Deep-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[33] Liyu Meng, Yuchen Liu, Xiaolong Liu, Zhaopei Huang,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "driver: Automated system for measuring valence and arousal"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Wenqiang Jiang, Tenggan Zhang, Chuanhe Liu,\nand Qin",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "in car driver videos.\nIn 2018 24th International Conference"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Jin.\nValence and arousal estimation based on multimodal",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "on Pattern Recognition (ICPR), pages 2546–2551.\nIEEE,"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "temporal-aware features for videos in the wild.\nIn Proceed-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "2018."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "ings of\nthe IEEE/CVF Conference on Computer Vision and",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[46] Gyanendra K Verma and Uma Shanker Tiwary. Affect repre-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Pattern Recognition, pages 2345–2352, 2022. 2",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "sentation and recognition in 3d continuous valence–arousal–"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[34] Anna Mitenkova, Jean Kossaifi, Yannis Panagakis, and Maja",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "dominance space. Multimedia Tools and Applications, 76:"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Pantic. Valence and arousal estimation in-the-wild with ten-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "2159–2183, 2017. 1, 2"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "sor methods.\nIn 2019 14th IEEE International Conference",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[47]\nJie Wei, Guanyu Hu, Xinyu Yang, Anh Tuan Luu,\nand"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "on Automatic Face & Gesture Recognition (FG 2019), pages",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Yizhuo Dong. Learning facial expression and body gesture"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "1–7. IEEE, 2019. 2, 4",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Expert\nvisual\ninformation for video emotion recognition."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[35] Wenxuan Mou, Oya Celiktutan, and Hatice Gunes. Group-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Systems with Applications, 237:121419, 2024."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "level arousal and valence recognition in static images: Face,",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[48] Hong-Xia Xie, I Li, Ling Lo, Hong-Han Shuai, Wen-Huang"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "body and context.\nIn 2015 11th IEEE International Confer-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Cheng, et al. Technical report for valence-arousal estimation"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "ence and Workshops on Automatic Face and Gesture Recog-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "in abaw2 challenge. arXiv preprint arXiv:2107.03891, 2021."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "nition (FG), pages 1–6. IEEE, 2015.",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": ""
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[49] Miku Yanagimoto and Chika Sugimoto. Recognition of per-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[36]\nJo˜ao Ribeiro Pinto, Tiago Gonc¸alves, Carolina Pinto, Lu´ıs",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "sisting emotional valence from eeg using convolutional neu-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Sanhudo,\nJoaquim Fonseca, Filipe Gonc¸alves, Pedro Car-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "ral networks.\nIn 2016 IEEE 9th International Workshop"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "valho, and Jaime S Cardoso. Audiovisual classification of",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "on Computational\nIntelligence and Applications\n(IWCIA),"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "group emotion valence using activity recognition networks.",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "pages 27–32. IEEE, 2016."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "In 2020 IEEE 4th International Conference on Image Pro-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[50] Hao-Chun Yang and Chi-Chun Lee. An attribute-invariant"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "cessing, Applications and Systems (IPAS), pages 114–119.",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "variational\nlearning for emotion recognition using physiol-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "IEEE, 2020.",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "ogy.\nIn ICASSP 2019-2019 IEEE international conference"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "[37] R Gnana Praveen, Wheidima Carneiro de Melo, Nasib Ul-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "on acoustics, speech and signal processing (ICASSP), pages"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "lah, Haseeb Aslam, Osama Zeeshan, Th´eo Denorme, Marco",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "1184–1188. IEEE, 2019."
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Pedersoli, Alessandro L Koerich, Simon Bacon, Patrick Car-",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "[51]\nJun Yu, Gongpeng Zhao, Yongqi Wang,\nZhihong Wei,"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "dinal, et al. A joint cross-attention model\nfor audio-visual",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Zerui Zhang, Zhongpeng Cai, Guochen Xie,\nJichao Zhu,"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "fusion in dimensional emotion recognition.\nIn Proceedings",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Wangyuan Zhu, Shuoping Yang, et al.\nImproving valence-"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "of the IEEE/CVF conference on computer vision and pattern",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "arousal estimation with spatiotemporal relationship learning"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "recognition, pages 2486–2495, 2022. 2",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "the IEEE/CVF\nand multimodal\nfusion.\nIn Proceedings of"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "Journal of\n[38]\nJames A Russell. A circumplex model of affect.",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "[28] Dimitrios Kollias,\nPanagiotis\nTzirakis,\nAS\nCowen,\nS": "personality and social psychology, 39(6):1161, 1980. 2",
          "[39] Andrey V Savchenko.\nFrame-level prediction of\nfacial ex-": "pages 7878–7885, 2024. 2"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Athanasios Papaioannou, Guoying Zhao, and Irene Kotsia."
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Aff-wild: valence and arousal’in-the-wild’challenge. In Pro-"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "ceedings of the IEEE conference on computer vision and pat-"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "tern recognition workshops, pages 34–41, 2017. 1, 4"
        },
        {
          "[52]": "[53]",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Shiqing Zhang, Yijiao Yang, Chen Chen, Xingnan Zhang,"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Qingming Leng, and Xiaoming Zhao. Deep learning-based"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "multimodal emotion recognition from audio, visual, and text"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "modalities: A systematic review of recent advancements and"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "future prospects.\nExpert Systems with Applications, 237:"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "121692, 2024. 1"
        },
        {
          "[52]": "[54] Ziyang Zhang, Liuwei An, Zishun Cui, Ao Xu, Tengteng",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": ""
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Dong, Yueqi Jiang, Jingyi Shi, Xin Liu, Xiao Sun, and Meng"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Wang.\nAbaw5 challenge: A facial affect\nrecognition ap-"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "proach utilizing transformer encoder and audiovisual fusion."
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "the IEEE/CVF Conference on Computer\nIn Proceedings of"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "Vision and Pattern Recognition, pages 5725–5734, 2023. 2,"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "3, 4"
        },
        {
          "[52]": "[55] Weiwei Zhou, Chenkun Ling,",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "and Zefeng Cai.\nEmotion"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "recognition with clip and sequential learning. arXiv preprint"
        },
        {
          "[52]": "",
          "Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,": "arXiv:2503.09929, 2025. 3"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Prediction of arousal and valence state from electrodermal activity using wavelet based resnet50 model",
      "authors": [
        "Abhinav Anthiyur Aravindan",
        "Sriram Kalyan Chappidi",
        "Anirudh Thumma",
        "Rohini Palanisamy"
      ],
      "year": "2023",
      "venue": "Current Directions in Biomedical Engineering"
    },
    {
      "citation_id": "2",
      "title": "Bert pre-training of image transformers",
      "authors": [
        "Hangbo Bao",
        "Li Dong",
        "Songhao Piao",
        "Furu Wei",
        "Beit"
      ],
      "year": "2021",
      "venue": "Bert pre-training of image transformers",
      "arxiv": "arXiv:2106.08254"
    },
    {
      "citation_id": "3",
      "title": "Accurate eeg-based emotion recognition on combined features using deep convolutional neural networks",
      "authors": [
        "Jx Chen",
        "Zhang",
        "Y Mao",
        "D Huang",
        "Jiang",
        "Zhang"
      ],
      "year": "2019",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "4",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Peihao Chen",
        "Xiangmin Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Cross-subject multimodal emotion recognition based on hybrid fusion",
      "authors": [
        "Yucel Cimtay",
        "Erhan Ekmekcioglu",
        "Seyma Caglar-Ozhan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Multi-modal arousal and valence estimation under noisy conditions",
      "authors": [
        "Denis Dresvyanskiy",
        "Maxim Markitantov",
        "Jiawei Yu",
        "Heysem Kaya",
        "Alexey Karpov"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Icaps-reslstm: Improved capsule network and residual lstm for eeg emotion recognition",
      "authors": [
        "Cunhang Fan",
        "Heng Xie",
        "Jianhua Tao",
        "Yongwei Li",
        "Guanxiong Pei",
        "Taihao Li",
        "Zhao Lv"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "8",
      "title": "Valence/arousal estimation of occluded faces from vr headsets",
      "authors": [
        "Tom Gotsman",
        "Neophytos Polydorou",
        "Abbas Edalat"
      ],
      "year": "2021",
      "venue": "2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI)"
    },
    {
      "citation_id": "9",
      "title": "Simultaneous prediction of valence/arousal and emotion categories in real-time",
      "authors": [
        "Sebastian Handrich",
        "Laslo Dinges",
        "Frerk Saxen",
        "Ayoub Al-Hamadi",
        "Sven Wachmuth"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)"
    },
    {
      "citation_id": "10",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "11",
      "title": "Deep learning-based approach for continuous affect prediction from facial expression images in valence-arousal space",
      "authors": [
        "Stephen Khor",
        "Wen Hwooi",
        "Alice Othmani",
        "Aznul Qalid",
        "Md Sabri"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Multimodal speech emotion recognition using cross-attention with deep fusion",
      "authors": [
        "Mustaqeem Khan",
        "Wail Gueaieb",
        "Abdulmotaleb Saddik",
        "Soonil Kwon",
        "Mser"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "13",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "16",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "17",
      "title": "Exploiting multicnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "19",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "21",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "23",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "24",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "28",
      "title": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "S Cowen",
        "I Zafeiriou",
        "Eric Kotsia",
        "Marco Granger",
        "Pedersoli",
        "Alice Bacon",
        "C Baird",
        "Gagne"
      ],
      "year": "2008",
      "venue": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition"
    },
    {
      "citation_id": "29",
      "title": "Evaef: Ensemble valence-arousal estimation framework in the wild",
      "authors": [
        "Xiaolong Liu",
        "Lei Sun",
        "Wenqiang Jiang",
        "Fengyuan Zhang",
        "Yuanyuan Deng",
        "Zhaopei Huang",
        "Liyu Meng",
        "Yuchen Liu",
        "Chuanhe Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "31",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "32",
      "title": "Multimodal emotion estimation for in-the-wild videos",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Yuan Cheng",
        "Meng Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "Multimodal emotion estimation for in-the-wild videos",
      "arxiv": "arXiv:2203.13032"
    },
    {
      "citation_id": "33",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "Anna Mitenkova",
        "Jean Kossaifi",
        "Yannis Panagakis",
        "Maja Pantic"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "35",
      "title": "Grouplevel arousal and valence recognition in static images: Face, body and context",
      "authors": [
        "Wenxuan Mou",
        "Oya Celiktutan",
        "Hatice Gunes"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "36",
      "title": "Audiovisual classification of group emotion valence using activity recognition networks",
      "authors": [
        "João Ribeiro Pinto",
        "Tiago Gonc ¸alves",
        "Carolina Pinto",
        "Luís Sanhudo",
        "Joaquim Fonseca",
        "Filipe Gonc ¸alves",
        "Pedro Carvalho",
        "Jaime Cardoso"
      ],
      "year": "2020",
      "venue": "2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS)"
    },
    {
      "citation_id": "37",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "Wheidima Gnana Praveen",
        "Nasib Carneiro De Melo",
        "Haseeb Ullah",
        "Osama Aslam",
        "Théo Zeeshan",
        "Marco Denorme",
        "Alessandro Pedersoli",
        "Simon Koerich",
        "Patrick Bacon",
        "Cardinal"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "38",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "39",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "arxiv": "arXiv:2203.13436"
    },
    {
      "citation_id": "40",
      "title": "Emotieffnets for facial processing in video-based valence-arousal prediction, expression classification and action unit detection",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Hsemotion team at the 6th abaw competition: Facial expressions, valence-arousal and emotion intensity prediction",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2024",
      "venue": "Hsemotion team at the 6th abaw competition: Facial expressions, valence-arousal and emotion intensity prediction",
      "arxiv": "arXiv:2403.11590"
    },
    {
      "citation_id": "42",
      "title": "Hsemotion team at abaw-8 competition: Audiovisual ambivalence/hesitancy, emotional mimicry intensity and facial expression recognition",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2025",
      "venue": "Hsemotion team at abaw-8 competition: Audiovisual ambivalence/hesitancy, emotional mimicry intensity and facial expression recognition",
      "arxiv": "arXiv:2503.10399"
    },
    {
      "citation_id": "43",
      "title": "Automatic detection of emotion valence on faces using consumer depth cameras",
      "authors": [
        "Arman Savran",
        "Ruben Gur",
        "Ragini Verma"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "44",
      "title": "Spatiotemporal satellite image fusion using deep convolutional neural networks",
      "authors": [
        "Huihui Song",
        "Qingshan Liu",
        "Guojie Wang",
        "Renlong Hang",
        "Bo Huang"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing"
    },
    {
      "citation_id": "45",
      "title": "Deepdriver: Automated system for measuring valence and arousal in car driver videos",
      "authors": [
        "Rajkumar Theagarajan",
        "Bir Bhanu",
        "Albert Cruz"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "46",
      "title": "Affect representation and recognition in 3d continuous valence-arousaldominance space",
      "authors": [
        "K Gyanendra",
        "Uma Verma",
        "Tiwary Shanker"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "47",
      "title": "Learning facial expression and body gesture visual information for video emotion recognition",
      "authors": [
        "Jie Wei",
        "Guanyu Hu",
        "Xinyu Yang",
        "Anh Luu",
        "Yizhuo Dong"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "48",
      "title": "Technical report for valence-arousal estimation in abaw2 challenge",
      "authors": [
        "Hong-Xia Xie",
        "I Li",
        "Ling Lo",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "year": "2021",
      "venue": "Technical report for valence-arousal estimation in abaw2 challenge",
      "arxiv": "arXiv:2107.03891"
    },
    {
      "citation_id": "49",
      "title": "Recognition of persisting emotional valence from eeg using convolutional neural networks",
      "authors": [
        "Miku Yanagimoto",
        "Chika Sugimoto"
      ],
      "year": "2016",
      "venue": "2016 IEEE 9th International Workshop on Computational Intelligence and Applications (IWCIA)"
    },
    {
      "citation_id": "50",
      "title": "An attribute-invariant variational learning for emotion recognition using physiology",
      "authors": [
        "Chun Hao",
        "Chi-Chun Yang",
        "Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "51",
      "title": "Improving valencearousal estimation with spatiotemporal relationship learning and multimodal fusion",
      "authors": [
        "Jun Yu",
        "Gongpeng Zhao",
        "Yongqi Wang",
        "Zhihong Wei",
        "Zerui Zhang",
        "Zhongpeng Cai",
        "Guochen Xie",
        "Jichao Zhu",
        "Wangyuan Zhu",
        "Shuoping Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Aff-wild: valence and arousal'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "53",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "Shiqing Zhang",
        "Yijiao Yang",
        "Chen Chen",
        "Xingnan Zhang",
        "Qingming Leng",
        "Xiaoming Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "54",
      "title": "Abaw5 challenge: A facial affect recognition approach utilizing transformer encoder and audiovisual fusion",
      "authors": [
        "Ziyang Zhang",
        "Liuwei An",
        "Zishun Cui",
        "Ao Xu",
        "Tengteng Dong",
        "Yueqi Jiang",
        "Jingyi Shi",
        "Xin Liu",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition with clip and sequential learning",
      "authors": [
        "Weiwei Zhou",
        "Chenkun Ling",
        "Zefeng Cai"
      ],
      "year": "2025",
      "venue": "Emotion recognition with clip and sequential learning",
      "arxiv": "arXiv:2503.09929"
    }
  ]
}