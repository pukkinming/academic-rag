{
  "paper_id": "2406.07753v1",
  "title": "The Muse 2024 Multimodal Sentiment Analysis Challenge: Social Perception And Humor Recognition",
  "published": "2024-06-11T22:26:20Z",
  "authors": [
    "Shahin Amiriparian",
    "Lukas Christ",
    "Alexander Kathan",
    "Maurice Gerczuk",
    "Niklas M√ºller",
    "Steffen Klug",
    "Lukas Stappen",
    "Andreas K√∂nig",
    "Erik Cambria",
    "Bj√∂rn Schuller",
    "Simone Eulitz"
  ],
  "keywords": [
    "Multimodal Sentiment Analysis",
    "Affective Computing",
    "Social Perception",
    "Humor Detection",
    "Multimodal Fusion",
    "Workshop",
    "Challenge",
    "Benchmark"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The Multimodal Sentiment Analysis Challenge (MuSe) 2024 addresses two contemporary multimodal affect and sentiment analysis problems: In the Social Perception Sub-Challenge (M S P ), participants will predict 16 different social attributes of individuals such as assertiveness, dominance, likability, and sincerity based on the provided audio-visual data. The Cross-Cultural Humor Detection Sub-Challenge (M S H ) dataset expands upon the Passau Spontaneous Football Coach Humor (P SFCH) dataset, focusing on the detection of spontaneous humor in a cross-lingual and cross-cultural setting. The main objective of MuSe 2024 is to unite a broad audience from various research domains, including multimodal sentiment analysis, audio-visual affective computing, continuous signal processing, and natural language processing. By fostering collaboration and exchange among experts in these fields, the MuSe 2024 endeavors to advance the understanding and application of sentiment analysis and affective computing across multiple modalities. This baseline paper provides details on each subchallenge and its corresponding dataset, extracted features from",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In its 5th edition, the Multimodal Sentiment Analysis Challenge (MuSe) proposes two tasks, namely audio-visual analysis of perceived characteristics of individuals and cross-cultural humor detection. Each respective sub-challenge employs a distinct dataset.\n\nIn the first sub-challenge, M S P , participants are tasked with training their machine learning models for recognition of perceived characteristics of individuals from video interviews. Audio-visual social perception analysis explores social traits that are important for how we are being perceived by other people  [1] . The perception others have of us can have a significant impact on our relationships with them as well as on our own professional future  [3] . At the same time, social perception is a complex phenomenon for which different theories have been brought forward. The Dual Perspective Model (DPM)  [2]  is based on the dimensions of agency (i. e., traits related to goal-achievement such as competence or dominance) and communality (i. e., traits referring to social relations such as friendliness or warmth)  [11] . The former is stereotypically associated with masculine gender roles, while the latter is traditionally attributed to femininity  [25] . In this context, the work by Eulitz and Gazdag  [27]  underscores the significance of understanding how perceived characteristics, particularly agentic traits, influence professional success, including the performance of CEOs in the stock market. Such insights are invaluable for organizations aiming to thrive in dynamic and competitive environments. However, extracting these insights requires advanced tools capable of capturing and analyzing nuanced social signals. Traditional methods of assessing social perception often rely on structured questionnaires or observational techniques to gauge individuals' interpretations and responses to social cues and interactions. In contrast, audio-visual machine learning systems offer scalability, enabling researchers to analyze large datasets efficiently. Moreover, these systems can uncover patterns and correlations that may not be immediately apparent to human observers, leading to deeper insights into the factors influencing social perception. This is where audio-visual machine learning systems play a pivotal role and can offer a holistic approach to understanding social perception by leveraging both auditory and visual cues. They can capture subtle nuances in facial expressions  [16, 38, 59] , body language  [15] , tone of voice  [59] , and verbal content  [22, 53] , providing a rich set of features to analyze. In the context of the Social Perception Sub-Challenge, the LMU Munich Executive Leadership Perception (LMU-ELP) dataset (cf. Section 2.1) presents a unique opportunity to explore the intersection of audio-visual data and social perception.\n\nIn the second task, Cross-Cultural Humor Detection Sub-Challenge (M S H\n\n), participants will train their models to detect humor within German football press conference recordings.\n\nHumor is a ubiquitous yet puzzling phenomenon in human communication. While it is strongly connected with an intention to amuse one's audience  [31] , humor has been shown to potentially elicit a wide range of both positive and negative effects  [17] . Throughout the last three decades, researchers in Affective Computing and related fields have addressed problems pertaining to computational humor, in particular, automatic humor detection  [54, 60, 68] . Spurred by the advancements in multimodal sentiment analysis, considerable attention has been paid to multimodal approaches to humor detection recently  [13, 35] . Such multimodal methods are particularly promising, as humor in personal interactions is an inherently multimodal phenomenon, expressed not just by means of what is said, but also via, e. g., gestures and facial expressions. A plethora of datasets dedicated to multimodal humor recognition exists  [36, 46, 64] . However, they are typically built from recordings of staged contexts, e. g., TV shows or TED talks, thus potentially missing out on spontaneous, in-the-wild aspects of humorous communication. Furthermore, several such datasets utilize audience laughter as a proxy label for humor, thus taking the risk of reducing the complex concept of humorous communication to the mere delivery of (scripted) punchlines. The P SFCH dataset  [21]  seeks to mitigate these issues by providing videos from a semistaged context, namely press conferences, that have been labeled manually for humor.\n\nM S H adds another layer of complexity to the humor recognition task by introducing a cross-cultural scenario. More specifically, the training data consists of German recordings, while the test data is composed of English videos. Whereas empirical studies have been conducted on cross-cultural similarities and differences in humor  [39, 55] , this problem has not received much attention from the machine learning domain. To the best of our knowledge, last year's edition of M S H was the first to introduce this task to the community, giving rise to a range of different systems proposed by the challenge's participants  [33, 42, 65, 67, 70] . This year's edition of M S H maintains the same data and data partitions as last year's  [20] . The training set comprises recordings from German football press conferences, while the unseen test set exclusively features press conferences conducted in English, thus setting the stage for a cross-cultural, cross-lingual evaluation scenario. For the training partition, we utilize the German Passau Spontaneous Football Coach Humor (P SFCH) (P SFCH) dataset  [21] , previously featured in the 2022 and 2023 editions of MuSe  [19, 33, 40, 65, 66, 70] . For the unseen test partition, we expanded the P SFCH dataset by incorporating press conference recordings delivered by seven distinct coaches from the English Premier League spanning from September 2016 to September 2020. Both the training and test partitions exclusively contain recordings where the respective coach is speaking, contributing to an overall duration exceeding 17 hours. Although the original videos are labeled according to the Humor Style Questionnaire (HSQ) framework introduced by Martin et al.  [44] , the focus in this year's M S H challenge is on binary prediction, specifically detecting the presence or absence of humor.\n\nThe sub-challenges presented in MuSe 2024 are designed to captivate a broad audience, drawing the interest of researchers spanning numerous domains, including multimodal sentiment analysis, affective computing, natural language processing, and signal processing. MuSe 2024 provides participants with an ideal platform to apply their expertise by leveraging multimodal self-supervised and representation learning techniques, as well as harnessing the power of generative AI and Large Language Models (LLMs).\n\nEach sub-challenge is characterized by its unique datasets and prediction objectives, providing participants with an opportunity In Section 2, we provide a comprehensive overview of the outlined sub-challenges, detailing their corresponding datasets and the challenge protocol. Following this, Section 3 elaborates on our pre-processing and feature extraction pipeline, along with the experimental setup utilized to compute baseline results for each subchallenge. Subsequently, the results are showcased in Section 4, leading to the conclusion of the paper in Section 5.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Two Sub-Challenges",
      "text": "In this section, we describe each sub-challenge and dataset in detail and provide a description of the challenge protocol.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Social Perception Sub-Challenge",
      "text": "For the first task, Social Perception Sub-Challenge (M S P ), we introduce the novel LMU-ELP dataset, which consists of audiovisual recordings of US executives, specifically the highest-ranking executives of listed firms -chief executive officers (CEOs). In our dataset, these CEOs present their firms to potential investors before taking their firms public, i. e., selling shares in the stock market for the first time. The goal of this challenge is to predict the agency and communiality of each individual on a 16-dimensional Likert scale ranging from 1 to 7. The target labels have been selected based on an established measure in social psychology (Bem Sex-Role Inventory scale  [12] ) to assess perceptions of agency and commonality, the basic dimension of the Dual Perspective Model  [2] , including aggressiveness, arrogance, assertiveness, confidence, dominance, independence, leadership qualities, and risk-taking propensity (pertaining to agency), as well as attributes like collaboration, enthusiasm, friendliness, good-naturedness, kindness, likability, sincerity, and warmth (associated with communality). Using Amazon Mechanical Turk (MTurk), 4304 annotators have labeled all dimensions of our data, comprising 177 CEOs. The dataset stands out for its comprehensive coverage of 16 distinguished labels of each individual, offering new perspectives in multimodal sensing of social signals. To the best of our knowledge, LMU-ELP is the first multimodal dataset that provides detailed insights into the nuanced dimensions of gender-associated attributes. M S P participants are encouraged to explore multimodal machine learning methods to automatically recognize CEOs' perceived characteristics. Each team will submit their 16 predictions. The evaluation metric is Pearson's correlation coefficient ( ), and the mean of all 16 values is set as the challenge's evaluation criterion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Cross-Cultural Humor Sub-Challenge",
      "text": "The training partition provided in M S H comprises videos of 10 football coaches from the German Bundesliga, all of them native German speakers. The test set, in contrast, features 6 English Premier League coaches from 6 different countries (Argentine, England, France, Germany, Portugal, Spain), only one of them being a native speaker. All subjects are male and aged between 30 and 53 years (training partition) and 47 to 57 years (test), respectively. 7 of the 10 German coaches are utilized as the training set, whereas the remaining 3 are used as the development set. Table  1  provides further statistics of the dataset.\n\nThe videos are cut to only include segments in which the coaches are speaking. In addition to the recordings, manual transcriptions with corresponding timestamps are provided.\n\nThe videos in P SFCH are originally labeled according to the HSQ, a two-dimensional model of humor proposed by Martin et al.  [44] . From these annotations, the gold standard is created as elaborated in  [21] , such that each data point corresponds to a 2 s frame with a binary label. Overall, humorous segments make up 4.38 % of the training segments, 2.81 % of the development segments, and 6.17 % of the segments used for the test.\n\nSame as in previous editions of M S H , AUC serves as the evaluation metric.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Challenge Protocol",
      "text": "To join the challenge, participants must be affiliated with an academic institution and complete the EULA found on the MuSe 2024 homepage 1  homepage. The organizers will not compete in any subchallenge. During the contest, participants upload their predictions for test labels on the CodaBench platform. Each sub-challenge allows up to 5 predictions, where the best among them determines the rank on the leaderboard. All teams are encouraged to submit a paper detailing their experiments and results. Winning a subchallenge requires an accepted paper. All papers are subject to a double-blind peer-review process.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Approaches",
      "text": "To support participants in time-efficient model development and reproducing challenge baselines, we provide a set of expert-designed (explainable) and Transformers'-based features. For the M S P task, we provide 6 feature sets, including three vision-based and three audio-based features. Additionally, for the M S H subchallenge, we provide an extra feature set tailored for the text modality, totaling 7 feature sets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pre-Processing",
      "text": "We split each dataset into three partitions for training, development, and testing, ensuring that the distribution of target labels and the overall length of recordings are balanced across each partition. Moreover, we maintained speaker independence throughout all partitions.\n\nFor the LMU-ELP recordings (each CEO recording has a fixed length of 30 seconds), we have removed data points in which the faces of multiple persons are presented, ensuring that each recording solely contains the CEO for which the labeling has been conducted. Furthermore, we have eliminated video recordings with fewer than 5 face frames available. For the audio modality of LMU-ELP dataset, we removed the background noise using the free online tool Vocal Remover 2 .\n\nFrom the test partition of the P SFCH dataset, we have manually removed video clips if the coaches did not speak English. Additionally, we discarded clips with poor audio quality.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio",
      "text": "Prior to extracting audio features, we normalize all audio files to -3 dB and convert them to mono format, with a sampling rate of 16 kHz and a bit depth of 16. Subsequently, we employ the S MILE toolkit  [29]  to compute handcrafted features. Additionally, we generate high-dimensional audio representations using both D S  [6]  and a modified version of W 2V 2.0  [9] . Both systems have demonstrated their efficacy in audio-based Speech Emotion Recognition (SER) and sentiment analysis tasks  [5, 8, 14, 30] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "G Maps. We Utilize The",
      "text": "SMILE toolkit  [29]  to extract 88 dimensional extended Geneva Minimalistic Acoustic Parameter Set ( G MAPS) features  [28] , which have shown to be robust for sentiment analysis and SER tasks  [10, 50, 62, 66] . We employ the standard configuration for each sub-challenge and extract features using a window size of 2000 ms and a hop size of 500 ms.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D S",
      "text": ". Utilizing D S  [6] , we harness (more conventional) deep Convolutional Neural Network (CNN)based representations from audio data. Our approach involves the initial generation of Mel-spectrograms for each audio file, employing a window size of 1000 ms and a hop size of 500 ms, with a configuration of 128 Mels and utilizing the viridis color mapping. These spectrogram representations are then fed into D N 121. We extract a 1024-dimensional feature vector from the output of the last pooling layer. The effectiveness of D S has been validated across various speech and audio recognition tasks  [5, 7, [48] [49] [50] .\n\n3.2.3 W 2V 2.0 . In recent years, there has been a major interest in self-supervised pretrained Transformer models in computer audition  [43] . An exemplary foundation model is W 2V 2.0  [9] , which has found widespread application in SER tasks  [47, 51] . Given that all sub-challenges are affect-related, we opt for a large version of W 2V 2.0 fine-tuned on the MSP-Podcast dataset specifically 2 https://vocalremover.org/ for emotion recognition  [63]   3  . We extract deep features from an audio signal by averaging its representations in the final layer of this model, yielding 1024-dimensional embeddings.\n\nFor M S H , we extract 2 Hz features by sliding a 3000 ms window over each audio file, with a step size of 500 ms, and for M S P , a 2000 ms window size (with a hop size of 500 ms) is applied. In the previous edition of MuSe, participants frequently employed W 2V 2.0 features, demonstrating their efficacy in capturing nuanced audio characteristics for the affective computing tasks  [33, 41, 42, 65, 70] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Video",
      "text": "To compute the visual modality baseline, we solely extract representations from subjects' faces. In order to do so, we utilize Multitask Cascaded Convolutional Networks (MTCNN) to isolate faces from video recordings in both datasets. Subsequently, we obtain Facial Action Units (FAUs), F N 512 , and V T FER representations from each face image.\n\nThese features already proved to be useful in affect and sentiment analysis tasks. For example, Li et al.  [42]  used FAU, F N 512 , and ViT features, among others, and integrated them into a multimodal feature encoder module to perform binary humor recognition. Similarly, recent works (e. g.,  [49, 65] ) conducted experiments applying these features, achieving an overall best result when fusing them with other features from the audio and text modality. Another study by Yi et al.  [69]  comes to similar conclusions, presenting a multimodal transformer fusion approach in which they used FAU and ViT features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mtcnn.",
      "text": "We utilize the MTCNN face detection model  [71]  to extract images of the subjects' faces.\n\nIn numerous videos from both datasets, multiple individuals are often visible, yet only the CEOs in the M S P dataset or the coaches in the M S H dataset are relevant. Initially, we aim to automatically filter out the CEO's or coach's face in each video through clustering of face embeddings. Subsequently, we manually refine the obtained face sets, retaining only the one corresponding to the CEO or coach. After the extraction of face images, we employ P F , F N 512 , and V T FER for feature extraction.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Facial Action Units (Fau)",
      "text": ". FAUs  [26]  offer a transparent method for encoding facial expressions by linking them to the activation of specific facial muscles. Given the rich emotional information conveyed by facial expressions, FAUs have received substantial interest in the sentiment analysis and affective computing community  [72] . We employ the pyfeat library  4  to automatically estimate the activation levels of 20 distinct FAUs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "F N 512",
      "text": ". We employ the F N 512 model  [57] , which is specially trained for face recognition. Specifically, we utilize the implementation provided in the deepface library  [58] , yielding a 512-dimensional embedding for each face image.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vision Transformer (V T Fer",
      "text": "). Additionally, we leverage a finetuned variant of ViT  [24]  provided by  [18]   5  , adapted to emotion recognition on the FER2013 dataset  [32] . We choose this approach since, contrary to the original ViT model, it is explicitly trained on pictures of faces rather than a wide variety of domains. The last hidden state of the special [CLS] token is utilized as the representation of a face image.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Text: Transformers",
      "text": "This modality is exclusive to the M S H sub-challenge. Given that M S H involves training and development data in German but a test set in English, we utilize the multilingual version of BERT  [23]    6  , pretrained on Wikipedia entries across 104 languages, including German and English. This model has demonstrated effective generalization across different languages  [52] . Specifically, we compute sentence representations by extracting the encoding of the CLS token from the model's final layer (768-dimensional), which represents the entire input sentence.\n\nWhile BERT embeddings have demonstrated to be suitable for the task of humor recognition  [33, 42, 67] , they can only be regarded as a simple baseline. The advent of larger LLMs such as LLaMa  [61]  or Mistral  [37]  opens up new avenues for humor recognition based on the textual modality. We would like to strongly encourage participants to explore the aptitude of (multilingual) LLMs for M S H , as the text modality proves to be promising for the problem at hand (cf. Section 4.2).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Alignment",
      "text": "The P SFCH dataset comprises audio, video, and transcripts. To facilitate the development of multimodal models leveraging these features, we align the various modalities with each other and with the labeling scheme specific to the task.\n\nBoth audio and face-based features are extracted at a rate of 2 Hz, using sliding windows with a step size of 500 ms for audio, and sampling faces at a 2 Hz rate for the video modality. Deriving sentencewise timestamps from manual transcripts is done in three steps: firstly, the Montreal Forced Aligner (MFA)  [45]  toolkit is utilized to generate word-level timestamps. Next, punctuation is automatically added to the transcripts using the deepmultilingualpunctuation tool  [34] . Finally, the transcripts are segmented into sentences using PySBD  [56] , allowing for the inference of sentence-wise timestamps from the word-level timestamps. Subsequently, 2 Hz textual features are computed by averaging the embeddings of sentences overlapping with the respective 500 ms windows.\n\nAs the labels in P SFCH correspond to windows of size 2 s, the alignment of features for M S H with annotations isn't direct but can be facilitated.\n\nIn M S P , the labels pertain to entire videos, eliminating the need for alignment with the labels.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baseline Training",
      "text": "We provide participants with a simple baseline system based on a GRU-RNN. Our model encodes sequential data via a stack of GRU layers. The final hidden representation of the last of these layers is taken as the embedding of the entire sequence and fed into two feed-forward layers for classification. For both tasks, hyperparameters, including the number of GRU layers, learning rate, and GRU representation size, are optimized. To obtain a set of unimodal baselines, the model is optimized and trained for each feature as described in Section 3. For a simple multimodal baseline, we employ a weighted late fusion approach, averaging the bestperforming audio-based model's predictions with those of the bestperforming video-based model. The predictions are weighted by the performance of the respective model on the development set. Simulating challenge conditions, we conduct all experiments with 5 fixed seeds for both sub-challenges. We provide the baseline code, checkpoints, and hyperparameter configurations in the accompanying GitHub repository 7  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "M S P",
      "text": ". For each of the 16 prediction targets in M S P , a separate model is trained. We observe this approach to be more promising than a multi-label prediction setup, i. e., one model predicting all 16 labels. Consequently, the late fusion approach also fuses the best video and audio models per target label. Nonetheless, we would like to encourage participants to explore combinations of prediction targets. In order to keep the number of experiments in a reasonable range, we optimize the hyperparameters on one target only and employ the configuration thus found for all 16 labels. As M S P is a regression task, we choose Mean Squared Error (MSE) for the loss function.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "M S H",
      "text": ". In the P SFCH dataset, each label pertains to a 2 s frame. As the features are extracted in 500 ms intervals, data points in the M S H experiments are sequences with a length of 4 at most. Binary Cross-Entropy (BCE) is used as the loss function.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Results",
      "text": "We implement the training of GRUs as outlined in the preceding section. In the following, we introduce and analyze the baseline results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "M S P",
      "text": "For the M S P sub-challenge, we first present the mean Pearson's correlation over all 16 target dimensions for each feature in Table  3 . Averaged across all targets, V T FER leads to the best overall single-modality results at = .3679 on development and = .2577 on test. Looking only at the audio modality, G MAPS outperforms both evaluated deep features -D S and W 2V 2.0 -at .2561 and .1442. Overall, however, results on development and test partition often diverge substantially, e. g., D S drops from .2075 to .0113. The weighted late fusion approach, which chooses the best model for each target separately, leads to greatly increased correlations on both development ( = .5579) and test set ( = .3573), suggesting that no singular feature works best for every target label.\n\nTo further investigate how the configurations fare at recognizing each of the 16 attributes, we report the best development and accompanying test set results for the individual target attributes in Table  2 . We further make use of the \"Big Two\" dimensions Agency and Communion  [11]  to split the target labels into two groups. From the agentive subgroup, aggressive can be recognized quite well by models trained on any of the evaluated features, with V T FER achieving the best results on both development and test results and further showing solid generalization capabilities. However, for confident, the best strong development performance of V T FER does not transfer to the test set, dropping from .7373 to a mere .0783 Pearson's correlation. In the communal subgroup, goodnatured and kind are detected quite reliably with good generalization behavior from models trained on visual features. Audio models trained on deep features (D S and W 2V 2.0 ), on the other hand, see performance drop to chance level when moving from development to test. . All models achieve above-chance (0.5 AUC) results, regardless of modality and employed feature representations. In this year's rendition of the sub-challenge, the introduction of V T FER features helped the visual modality catch up to the best audio results found again with W 2V 2.0 embeddings. GRUs trained on these features reach .7995 (V T FER ) and .8042 (W 2V 2.0 ) on the test partition. However, it can be observed that V T FER features do not generalize as well to unseen data, with performance dropping from .8932 to .7995 between development and test partitions. As last year, these generalization deficits extend across all visual features while the audio modality fares better, with W 2V 2.0 and G MAPS only slightly dropping in AUC and D S matching development performance on the test set. The efficacy of exploiting linguistic information is further demonstrated by the purely textual BERT features, which come third after W 2V 2.0 and V T FER , outperforming all other single modality models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "M S H",
      "text": "Our late fusion approach consistently improves over the respective unimodal baselines. The performance gains achieved in the visual modality with V T FER features further lead to fusion settings, which include video, eclipsing those that rely on only audio and text. The overall highest AUCs of .9251 .8682 on development and test sets, respectively, are achieved by fusing all three modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "We introduced MuSe 2024 -the 5th Multimodal Sentiment Analysis challenge, comprising two sub-challenges, M S P and M S H\n\n. For the M S P sub-challenge, the novel LMU-ELP dataset has been introduced and is made available, which consists of interview recordings of CEOs who present their firms to potential investors before taking their firms public. Participants were tasked to predict 16 different attributes of CEOs, e. g., confidence, or sincerity on a Likert scale from 1 to 7.\n\nThe M S H sub-challenge is a relaunch of the same task from the 2023 edition of MuSe  [4, 20] . It uses an extended version of the P SFCH dataset  [21] . Participants are tasked to detect spontaneous humor in press conferences across cultures and languages, training their models on recordings of German-speaking trainers and testing on English data.\n\nWe employed publicly accessible codes and repositories to extract audio, visual, and textual features. Additionally, we trained simple GRU models on the acquired representations to establish the official challenge baselines. These baselines represent the models' performance on the test partitions of each sub-challenge, outlined as follows: a mean value of .3573 for M S P , and an AUC value of .8682 for M S H\n\n. Both baseline results were achieved via late fusion of all modalities in each respective sub-challenge.\n\nBy sharing our code, datasets, and features publicly, we aim to facilitate broader participation and engagement from the research community. This open approach promotes transparency and encourages researchers to build upon our work, accelerating progress in multimodal data processing. As we continue to refine our baseline systems and explore new methodologies, we anticipate further advancements in our understanding of human behavior and communication across different modalities. Through ongoing collaboration and experimentation within the MuSe 2024 framework, we hope to drive innovation and ultimately contribute to the development of more effective multimodal machine learning systems for behavioral modeling and affective analysis.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Acknowledgments",
      "text": "This project has received funding from the Deutsche Forschungsgemeinschaft (DFG) under grant agreement No. 461420398, and the DFG's Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS). Shahin Amiriparian and Bj√∂r W. Schuller are also affiliated with the Munich Center for Machine Learning (MCML), Germany. Bj√∂r W. Schuller is further affiliated with the Chair of Embedded Intelligence for Healthcare and Wellbeing (EIHW), University of Augsburg, Germany, Chair of Health Informatics (CHI), Klinikum rechts der Isar (MRI), Technical University of Munich, Germany, and the Munich Data Science Institute (MDSI), Germany.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Group on Language, Audio, & Music": "Imperial College London",
          "Institute of Strategic Management": "LMU Munich"
        },
        {
          "Group on Language, Audio, & Music": "London, United Kingdom",
          "Institute of Strategic Management": "Munich, Germany"
        },
        {
          "Group on Language, Audio, & Music": "ABSTRACT",
          "Institute of Strategic Management": "each data modality, and discusses challenge baselines. For our base-"
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "line system, we make use of a range of Transformers and expert-"
        },
        {
          "Group on Language, Audio, & Music": "The Multimodal Sentiment Analysis Challenge (MuSe) 2024 ad-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "designed features and train Gated Recurrent Unit (GRU)-Recurrent"
        },
        {
          "Group on Language, Audio, & Music": "dresses two contemporary multimodal aÔ¨Äect and sentiment analy-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "Neural Network (RNN) models on them, resulting in a competi-"
        },
        {
          "Group on Language, Audio, & Music": "sis problems: In the Social Perception Sub-Challenge (MuSe-Perception),",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "tive baseline system. On the unseen test datasets of the respective"
        },
        {
          "Group on Language, Audio, & Music": "participants will predict 16 diÔ¨Äerent social attributes of individuals",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "sub-challenges,\nit achieves a mean Pearson‚Äôs Correlation CoeÔ¨É-"
        },
        {
          "Group on Language, Audio, & Music": "such as assertiveness, dominance, likability, and sincerity based on",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "cient (ùúå) of 0.3573 for MuSe-Perception and an Area Under the"
        },
        {
          "Group on Language, Audio, & Music": "the provided audio-visual data. The Cross-Cultural Humor Detec-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "Curve (AUC) value of 0.8682 for MuSe-Humor."
        },
        {
          "Group on Language, Audio, & Music": "tion Sub-Challenge (MuSe-Humor) dataset expands upon the Pas-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "sau Spontaneous Football Coach Humor (Passau-SFCH) dataset,",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "focusing on the detection of spontaneous humor in a cross-lingual",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "CCS CONCEPTS"
        },
        {
          "Group on Language, Audio, & Music": "and cross-cultural setting. The main objective of MuSe 2024 is to",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "unite a broad audience from various research domains,\nincluding",
          "Institute of Strategic Management": "‚Ä¢ Computing methodologies ‚Üí Computer vision; Natural"
        },
        {
          "Group on Language, Audio, & Music": "multimodal sentiment analysis, audio-visual aÔ¨Äective computing,",
          "Institute of Strategic Management": "language processing; Neural networks; Machine learning."
        },
        {
          "Group on Language, Audio, & Music": "continuous signal processing, and natural language processing. By",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "KEYWORDS"
        },
        {
          "Group on Language, Audio, & Music": "fostering collaboration and exchange among experts in these Ô¨Åelds,",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "Multimodal Sentiment Analysis; AÔ¨Äective Computing; Social Per-"
        },
        {
          "Group on Language, Audio, & Music": "the MuSe 2024 endeavors to advance the understanding and appli-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "ception; Humor Detection; Multimodal Fusion; Workshop; Chal-"
        },
        {
          "Group on Language, Audio, & Music": "cation of sentiment analysis and aÔ¨Äective computing across mul-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "lenge; Benchmark"
        },
        {
          "Group on Language, Audio, & Music": "tiple modalities. This baseline paper provides details on each sub-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "challenge and its corresponding dataset, extracted features from",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "Permission to make digital or hard copies of all or part of this work for personal or",
          "Institute of Strategic Management": "ACM Reference Format:"
        },
        {
          "Group on Language, Audio, & Music": "classroom use is granted without fee provided that copies are not made or distributed",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "Shahin Amiriparian, Lukas Christ, Alexander Kathan, Maurice Gerczuk,"
        },
        {
          "Group on Language, Audio, & Music": "for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "Niklas M√ºller, SteÔ¨Äen Klug, Lukas Stappen, Andreas K√∂nig, Erik Cambria,"
        },
        {
          "Group on Language, Audio, & Music": "tion on the Ô¨Årst page. Copyrights for components of this work owned by others than",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "the author(s) must be honored. Abstracting with credit is permitted. To copy other-",
          "Institute of Strategic Management": "Bj√∂rn W. Schuller, and Simone Eulitz. 2024.\nThe MuSe 2024 Multimodal"
        },
        {
          "Group on Language, Audio, & Music": "wise, or republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc",
          "Institute of Strategic Management": "Sentiment Analysis Challenge: Social Perception and Humor Recognition."
        },
        {
          "Group on Language, Audio, & Music": "permission and/or a fee. Request permissions from permissions@acm.org.",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "In Proceedings of the 5th Multimodal Sentiment Analysis Challenge and Work-"
        },
        {
          "Group on Language, Audio, & Music": "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "",
          "Institute of Strategic Management": "shop: Social Signal QuantiÔ¨Åcation and Humor Recognition (MuSe ‚Äô24), Oc-"
        },
        {
          "Group on Language, Audio, & Music": "¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
          "Institute of Strategic Management": ""
        },
        {
          "Group on Language, Audio, & Music": "ACM ISBN xxx-x-xxxx-xxxx-x/xx/xx. . . $15.00",
          "Institute of Strategic Management": "tober 28, 2024, Melbourne, Australia. ACM, New York, NY, USA, 10 pages."
        },
        {
          "Group on Language, Audio, & Music": "https://doi.org/10.1145/xxxxxxx.xxxxxxx",
          "Institute of Strategic Management": "https://doi.org/10.1145/xxxxxxx.xxxxxxx"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "1\nINTRODUCTION",
          "Shahin Amiriparian et al.": "by the advancements in multimodal sentiment analysis, consider-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "able attention has been paid to multimodal approaches to humor"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In its 5th edition, the Multimodal Sentiment Analysis Challenge",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "detection recently [13, 35]. Such multimodal methods are partic-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "(MuSe) proposes two tasks, namely audio-visual analysis of per-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ularly promising, as humor in personal\ninteractions is an inher-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ceived characteristics of individuals and cross-cultural humor de-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ently multimodal phenomenon, expressed not\njust by means of"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tection. Each respective sub-challenge employs a distinct dataset.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "what is said, but also via, e. g., gestures and facial expressions. A"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In the Ô¨Årst sub-challenge, MuSe-Perception, participants",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "plethora of datasets dedicated to multimodal humor recognition ex-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "are tasked with training their machine learning models for recog-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ists [36, 46, 64]. However, they are typically built from recordings"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "nition of perceived characteristics of\nindividuals\nfrom video in-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "of staged contexts, e. g., TV shows or TED talks, thus potentially"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "terviews. Audio-visual social perception analysis explores social",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "missing out on spontaneous, in-the-wild aspects of humorous com-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "traits that are important for how we are being perceived by other",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "munication. Furthermore,\nseveral\nsuch datasets utilize audience"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "people [1]. The perception others have of us can have a signiÔ¨Åcant",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "laughter as a proxy label for humor, thus taking the risk of reduc-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "impact on our relationships with them as well as on our own pro-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ing the complex concept of humorous communication to the mere"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "fessional\nfuture [3]. At the same time, social perception is a com-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "delivery of\n(scripted) punchlines. The Passau-SFCH dataset [21]"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "plex phenomenon for which diÔ¨Äerent theories have been brought",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "seeks to mitigate these issues by providing videos from a semi-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "forward. The Dual Perspective Model\n(DPM)\n[2] is based on the",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "staged context, namely press conferences, that have been labeled"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "dimensions of agency (i. e., traits related to goal-achievement such",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "manually for humor."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "as competence or dominance) and communality (i. e., traits refer-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "MuSe-Humor adds another layer of complexity to the humor"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ring to social relations such as friendliness or warmth) [11]. The",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "recognition task by introducing a cross-cultural\nscenario. More"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "former is stereotypically associated with masculine gender roles,",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "speciÔ¨Åcally, the training data consists of German recordings, while"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "while the latter\nis\ntraditionally attributed to femininity [25].\nIn",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "the test data is composed of English videos. Whereas empirical"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "this context, the work by Eulitz and Gazdag [27] underscores the",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "studies have been conducted on cross-cultural similarities and dif-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "signiÔ¨Åcance of understanding how perceived characteristics, par-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ferences in humor [39, 55],\nthis problem has not received much"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ticularly agentic traits,\ninÔ¨Çuence professional\nsuccess,\nincluding",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "attention from the machine learning domain. To the best of our"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the performance of CEOs in the stock market. Such insights are",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "knowledge, last year‚Äôs edition of MuSe-Humor was the Ô¨Årst to in-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "invaluable for organizations aiming to thrive in dynamic and com-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "troduce this task to the community, giving rise to a range of dif-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "petitive environments. However, extracting these insights requires",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ferent systems proposed by the challenge‚Äôs participants [33, 42, 65,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "advanced tools capable of capturing and analyzing nuanced social",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "67, 70]."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "signals. Traditional methods of assessing social perception often",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "This year‚Äôs edition of MuSe-Humor maintains the same data"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "rely on structured questionnaires or observational\ntechniques to",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "and data partitions as last year‚Äôs [20]. The training set comprises"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "gauge individuals‚Äô interpretations and responses to social cues and",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "recordings from German football press conferences, while the un-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "interactions.\nIn contrast, audio-visual machine learning systems",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "seen test set exclusively features press conferences conducted in"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "oÔ¨Äer scalability, enabling researchers to analyze large datasets ef-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "English,\nthus setting the stage for a cross-cultural, cross-lingual"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Ô¨Åciently. Moreover, these systems can uncover patterns and corre-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "evaluation scenario. For the training partition, we utilize the Ger-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "lations that may not be immediately apparent to human observers,",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "man Passau Spontaneous Football Coach Humor (Passau-SFCH)"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "leading to deeper insights into the factors inÔ¨Çuencing social per-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "(Passau-SFCH) dataset [21], previously featured in the 2022 and"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ception. This is where audio-visual machine learning systems play",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "2023 editions of MuSe [19, 33, 40, 65, 66, 70]. For the unseen test"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "a pivotal role and can oÔ¨Äer a holistic approach to understanding so-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "partition, we expanded the Passau-SFCH dataset by incorporating"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "cial perception by leveraging both auditory and visual cues. They",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "press conference recordings delivered by seven distinct coaches"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "can capture subtle nuances in facial expressions [16, 38, 59], body",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "from the English Premier League spanning from September 2016"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "language [15], tone of voice [59], and verbal content [22, 53], pro-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "to September 2020. Both the training and test partitions exclusively"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "viding a rich set of features to analyze. In the context of the Social",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "contain recordings where the respective coach is speaking, con-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Perception Sub-Challenge, the LMU Munich Executive Leadership",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "tributing to an overall duration exceeding 17 hours. Although the"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Perception (LMU-ELP) dataset\n(cf. Section 2.1) presents a unique",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "original videos are labeled according to the Humor Style Question-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "opportunity to explore the intersection of audio-visual data and",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "naire (HSQ)\nframework introduced by Martin et al.\n[44],\nthe fo-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "social perception.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "cus in this year‚Äôs MuSe-Humor challenge is on binary prediction,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In the second task, Cross-Cultural Humor Detection Sub-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "speciÔ¨Åcally detecting the presence or absence of humor."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Challenge (MuSe-Humor), participants will train their models",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "The sub-challenges presented in MuSe 2024 are designed to cap-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "to detect humor within German football press conference record-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "tivate a broad audience, drawing the interest of researchers span-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ings.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ning numerous domains, including multimodal sentiment analysis,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Humor is a ubiquitous yet puzzling phenomenon in human com-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "aÔ¨Äective computing, natural\nlanguage processing, and signal pro-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "munication. While it\nis strongly connected with an intention to",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "cessing. MuSe 2024 provides participants with an ideal platform"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "amuse one‚Äôs audience [31], humor has been shown to potentially",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "to apply their expertise by leveraging multimodal self-supervised"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "elicit a wide range of both positive and negative eÔ¨Äects [17]. Through-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "and representation learning techniques, as well as harnessing the"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "out the last three decades, researchers in AÔ¨Äective Computing and",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "power of generative AI and Large Language Models (LLMs)."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "related Ô¨Åelds have addressed problems pertaining to computational",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "Each sub-challenge is characterized by its unique datasets and"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "humor, in particular, automatic humor detection [54, 60, 68]. Spurred",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "prediction objectives, providing participants with an opportunity"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Statistics for eachsub-challenge dataset.Included itscomprehensivecoverageof16distinguishedlabelsofeachindi-",
      "data": [
        {
          "MuSe 2024: Baseline Paper": "Table 1: Statistics for each sub-challenge dataset. Included",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "its comprehensive coverage of 16 distinguished labels of each indi-"
        },
        {
          "MuSe 2024: Baseline Paper": "are the number of unique subjects (#), and the video dura-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "vidual, oÔ¨Äering new perspectives in multimodal sensing of social"
        },
        {
          "MuSe 2024: Baseline Paper": "tions formatted as h:mm:ss, and the total number of sub-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "signals. To the best of our knowledge, LMU-ELP is the Ô¨Årst multi-"
        },
        {
          "MuSe 2024: Baseline Paper": "jects along with the overall duration of all recordings in each",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "modal dataset that provides detailed insights into the nuanced di-"
        },
        {
          "MuSe 2024: Baseline Paper": "dataset.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "mensions of gender-associated attributes. MuSe-Perception par-"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ticipants are encouraged to explore multimodal machine learning"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "methods to automatically recognize CEOs‚Äô perceived characteris-"
        },
        {
          "MuSe 2024: Baseline Paper": "MuSe-Perception\nMuSe-Humor",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tics. Each team will submit\ntheir 16 predictions. The evaluation"
        },
        {
          "MuSe 2024: Baseline Paper": "Partition\n#\nDuration\n#\nDuration",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "metric is Pearson‚Äôs correlation coeÔ¨Écient (ùúå), and the mean of all"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "16 ùúå values is set as the challenge‚Äôs evaluation criterion."
        },
        {
          "MuSe 2024: Baseline Paper": "Train\n59\n0 :30 :13\n7\n7 :44 :49",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "Development\n58\n0 :29 :12\n3\n3 :06 :48",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2.2\nThe Cross-Cultural Humor Sub-Challenge"
        },
        {
          "MuSe 2024: Baseline Paper": "Test\n60\n0 :30 :10\n6\n6 :35 :16",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "The training partition provided in MuSe-Humor comprises videos"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "of 10 football coaches from the German Bundesliga, all of them na-"
        },
        {
          "MuSe 2024: Baseline Paper": "√ç\n177\n1 :29 :35\n16\n17 :26 :53",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tive German speakers. The test set,\nin contrast, features 6 English"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Premier League coaches from 6 diÔ¨Äerent countries (Argentine, Eng-"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "land, France, Germany, Portugal, Spain), only one of them being a"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "native speaker. All subjects are male and aged between 30 and 53"
        },
        {
          "MuSe 2024: Baseline Paper": "to delve into speciÔ¨Åc problem domains while simultaneously con-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "years (training partition) and 47 to 57 years (test), respectively. 7"
        },
        {
          "MuSe 2024: Baseline Paper": "tributing to broader research endeavors. By serving as a central",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "of the 10 German coaches are utilized as the training set, whereas"
        },
        {
          "MuSe 2024: Baseline Paper": "hub for the comparison of methodologies across tasks, MuSe fa-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the remaining 3 are used as the development set. Table 1 provides"
        },
        {
          "MuSe 2024: Baseline Paper": "cilitates the discovery of novel\ninsights into the eÔ¨Äectiveness of",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "further statistics of the dataset."
        },
        {
          "MuSe 2024: Baseline Paper": "various approaches, modalities, and features within the Ô¨Åeld of af-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "The videos are cut to only include segments in which the coaches"
        },
        {
          "MuSe 2024: Baseline Paper": "fective computing.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "are speaking. In addition to the recordings, manual transcriptions"
        },
        {
          "MuSe 2024: Baseline Paper": "In Section 2, we provide a comprehensive overview of the out-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "with corresponding timestamps are provided."
        },
        {
          "MuSe 2024: Baseline Paper": "lined sub-challenges, detailing their corresponding datasets and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "The videos in Passau-SFCH are originally labeled according to"
        },
        {
          "MuSe 2024: Baseline Paper": "the challenge protocol. Following this, Section 3 elaborates on our",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the HSQ, a two-dimensional model of humor proposed by Martin"
        },
        {
          "MuSe 2024: Baseline Paper": "pre-processing and feature extraction pipeline, along with the ex-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "et al. [44]. From these annotations,\nthe gold standard is created"
        },
        {
          "MuSe 2024: Baseline Paper": "perimental setup utilized to compute baseline results for each sub-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "as elaborated in [21], such that each data point corresponds to a"
        },
        {
          "MuSe 2024: Baseline Paper": "challenge. Subsequently,\nthe results are showcased in Section 4,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2 s frame with a binary label. Overall, humorous segments make"
        },
        {
          "MuSe 2024: Baseline Paper": "leading to the conclusion of the paper in Section 5.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "up 4.38 % of the training segments, 2.81 % of the development seg-"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ments, and 6.17 % of the segments used for the test."
        },
        {
          "MuSe 2024: Baseline Paper": "2\nTHE TWO SUB-CHALLENGES",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Same as in previous editions of MuSe-Humor, AUC serves as"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the evaluation metric."
        },
        {
          "MuSe 2024: Baseline Paper": "In this section, we describe each sub-challenge and dataset in detail",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "and provide a description of the challenge protocol.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2.3\nChallenge Protocol"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "To join the challenge, participants must be aÔ¨Éliated with an aca-"
        },
        {
          "MuSe 2024: Baseline Paper": "2.1\nThe Social Perception Sub-Challenge",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "demic institution and complete the EULA found on the MuSe 2024"
        },
        {
          "MuSe 2024: Baseline Paper": "For the Ô¨Årst task, Social Perception Sub-Challenge (MuSe-Perception),",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "homepage1 homepage. The organizers will not compete in any sub-"
        },
        {
          "MuSe 2024: Baseline Paper": "we introduce the novel LMU-ELP dataset, which consists of audio-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "challenge. During the contest, participants upload their predictions"
        },
        {
          "MuSe 2024: Baseline Paper": "visual recordings of US executives, speciÔ¨Åcally the highest-ranking",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "for test labels on the CodaBench platform. Each sub-challenge al-"
        },
        {
          "MuSe 2024: Baseline Paper": "executives of listed Ô¨Årms - chief executive oÔ¨Écers (CEOs).\nIn our",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "lows up to 5 predictions, where the best among them determines"
        },
        {
          "MuSe 2024: Baseline Paper": "dataset, these CEOs present\ntheir Ô¨Årms to potential\ninvestors be-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the rank on the leaderboard. All teams are encouraged to submit"
        },
        {
          "MuSe 2024: Baseline Paper": "fore taking their Ô¨Årms public, i. e., selling shares in the stock mar-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "a paper detailing their experiments and results. Winning a sub-"
        },
        {
          "MuSe 2024: Baseline Paper": "ket\nfor the Ô¨Årst time. The goal of\nthis challenge is to predict\nthe",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "challenge requires an accepted paper. All papers are subject to a"
        },
        {
          "MuSe 2024: Baseline Paper": "agency and communiality of each individual on a 16-dimensional",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "double-blind peer-review process."
        },
        {
          "MuSe 2024: Baseline Paper": "Likert scale ranging from 1 to 7. The target\nlabels have been se-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "lected based on an established measure in social psychology (Bem",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3\nBASELINE APPROACHES"
        },
        {
          "MuSe 2024: Baseline Paper": "Sex-Role Inventory scale [12]) to assess perceptions of agency and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "To support participants in time-eÔ¨Écient model development and"
        },
        {
          "MuSe 2024: Baseline Paper": "commonality, the basic dimension of the Dual Perspective Model [2],",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "reproducing challenge baselines, we provide a set of expert-designed"
        },
        {
          "MuSe 2024: Baseline Paper": "including aggressiveness, arrogance, assertiveness, conÔ¨Ådence, dom-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "(explainable) and Transformers‚Äô-based features. For the MuSe-Perception"
        },
        {
          "MuSe 2024: Baseline Paper": "inance,\nindependence,\nleadership qualities, and risk-taking propen-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "task, we provide 6 feature sets,\nincluding three vision-based and"
        },
        {
          "MuSe 2024: Baseline Paper": "sity (pertaining to agency), as well as attributes like collaboration,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "three audio-based features. Additionally, for the MuSe-Humor sub-"
        },
        {
          "MuSe 2024: Baseline Paper": "enthusiasm, friendliness, good-naturedness, kindness,\nlikability, sin-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "challenge, we provide an extra feature set tailored for the text modal-"
        },
        {
          "MuSe 2024: Baseline Paper": "cerity, and warmth (associated with communality). Using Amazon",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ity, totaling 7 feature sets."
        },
        {
          "MuSe 2024: Baseline Paper": "Mechanical Turk (MTurk), 4304 annotators have labeled all dimen-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "sions of our data, comprising 177 CEOs. The dataset stands out for",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "1https://www.muse-challenge.org/challenge/participate"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3.1\nPre-processing",
          "Shahin Amiriparian et al.": "for emotion recognition [63]3. We extract deep features from an au-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "dio signal by averaging its representations in the Ô¨Ånal layer of this"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "We split each dataset\ninto three partitions for training, develop-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "model, yielding 1024-dimensional embeddings."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ment, and testing, ensuring that\nthe distribution of\ntarget\nlabels",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "For MuSe-Humor, we extract 2 Hz features by sliding a 3000 ms"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and the overall length of recordings are balanced across each parti-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "window over each audio Ô¨Åle, with a step size of 500 ms, and for"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tion. Moreover, we maintained speaker independence throughout",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "MuSe-Perception, a 2000 ms window size (with a hop size of 500 ms)"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "all partitions.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "is applied. In the previous edition of MuSe, participants frequently"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "For the LMU-ELP recordings (each CEO recording has a Ô¨Åxed",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "employed Wav2Vec2.0\nfeatures, demonstrating their eÔ¨Écacy in"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "length of 30 seconds), we have removed data points in which the",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "capturing nuanced audio characteristics for the aÔ¨Äective comput-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "faces of multiple persons are presented, ensuring that each record-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ing tasks [33, 41, 42, 65, 70]."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ing solely contains the CEO for which the labeling has been con-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ducted. Furthermore, we have eliminated video recordings with",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "fewer than 5 face frames available. For the audio modality of LMU-ELP",
          "Shahin Amiriparian et al.": "3.3\nVideo"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "dataset, we removed the background noise using the free online",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "To compute the visual modality baseline, we solely extract repre-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tool Vocal Remover2.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "sentations from subjects‚Äô faces. In order to do so, we utilize Multi-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "From the test partition of\nthe Passau-SFCH dataset, we have",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "task Cascaded Convolutional Networks (MTCNN) to isolate faces"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "manually removed video clips if the coaches did not speak English.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "from video recordings in both datasets. Subsequently, we obtain Fa-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Additionally, we discarded clips with poor audio quality.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "cial Action Units (FAUs), FaceNet512 , and ViT-FER representations"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "from each face image."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3.2\nAudio",
          "Shahin Amiriparian et al.": "These features already proved to be useful\nin aÔ¨Äect and senti-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ment analysis tasks. For example, Li et al. [42] used FAU, FaceNet512 ,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Prior to extracting audio features, we normalize all audio Ô¨Åles to",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "and ViT features, among others, and integrated them into a multi-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "‚àí3 dB and convert them to mono format, with a sampling rate of",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "modal feature encoder module to perform binary humor recogni-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "16 kHz and a bit depth of 16. Subsequently, we employ the openS-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "tion. Similarly, recent works (e. g., [49, 65]) conducted experiments"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "MILE toolkit [29] to compute handcrafted features. Additionally,",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "applying these features, achieving an overall best result when fus-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "we generate high-dimensional audio representations using both",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ing them with other features from the audio and text modality. An-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "DeepSpectrum [6] and a modiÔ¨Åed version of Wav2Vec2.0\n[9].",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "other study by Yi et al. [69] comes to similar conclusions, present-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Both systems have demonstrated their eÔ¨Écacy in audio-based Speech",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "ing a multimodal transformer fusion approach in which they used"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Emotion Recognition (SER) and sentiment analysis tasks [5, 8, 14,",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "FAU and ViT features."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "30].",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "3.3.1"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3.2.1\neGeMAPS. We utilize the openSMILE toolkit [29] to extract",
          "Shahin Amiriparian et al.": "MTCNN. We utilize the MTCNN face detection model [71]"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "88 dimensional extended Geneva Minimalistic Acoustic Parameter",
          "Shahin Amiriparian et al.": "to extract images of the subjects‚Äô faces."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Set (eGeMAPS) features [28], which have shown to be robust for",
          "Shahin Amiriparian et al.": "In numerous videos from both datasets, multiple individuals are"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "sentiment analysis and SER tasks [10, 50, 62, 66]. We employ the",
          "Shahin Amiriparian et al.": "often visible, yet only the CEOs in the MuSe-Perception dataset"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "standard conÔ¨Åguration for each sub-challenge and extract features",
          "Shahin Amiriparian et al.": "or the coaches in the MuSe-Humor dataset are relevant.\nInitially,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "using a window size of 2000 ms and a hop size of 500 ms.",
          "Shahin Amiriparian et al.": "we aim to automatically Ô¨Ålter out\nthe CEO‚Äôs or coach‚Äôs face in"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "each video through clustering of face embeddings. Subsequently,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "DeepSpectrum . Utilizing DeepSpectrum [6], we harness",
          "Shahin Amiriparian et al.": "we manually reÔ¨Åne the obtained face sets, retaining only the one"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "(more conventional) deep Convolutional Neural Network (CNN)-",
          "Shahin Amiriparian et al.": "corresponding to the CEO or coach. After the extraction of face im-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "based representations from audio data. Our approach involves the",
          "Shahin Amiriparian et al.": "ages, we employ Py-Feat , FaceNet512 , and ViT-FER for feature"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "initial generation of Mel-spectrograms for each audio Ô¨Åle, employ-",
          "Shahin Amiriparian et al.": "extraction."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ing a window size of 1000 ms and a hop size of 500 ms, with a",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "3.3.2"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "conÔ¨Åguration of 128 Mels and utilizing the viridis color mapping.",
          "Shahin Amiriparian et al.": "Facial Action Units (FAU). FAUs [26] oÔ¨Äer a transparent method"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "These spectrogram representations are then fed into DenseNet121.",
          "Shahin Amiriparian et al.": "for encoding facial expressions by linking them to the activation"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "We extract a 1024-dimensional\nfeature vector from the output of",
          "Shahin Amiriparian et al.": "of speciÔ¨Åc facial muscles. Given the rich emotional\ninformation"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the last pooling layer. The eÔ¨Äectiveness of DeepSpectrum has",
          "Shahin Amiriparian et al.": "conveyed by facial expressions, FAUs have received substantial in-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "been validated across various speech and audio recognition tasks",
          "Shahin Amiriparian et al.": "terest in the sentiment analysis and aÔ¨Äective computing commu-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[5, 7, 48‚Äì50].",
          "Shahin Amiriparian et al.": "nity [72]. We employ the pyfeat library4 to automatically estimate"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "the activation levels of 20 distinct FAUs."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3.2.3\nWav2Vec2.0 .",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In recent years, there has been a major inter-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "est in self-supervised pretrained Transformer models in computer",
          "Shahin Amiriparian et al.": "3.3.3\nFaceNet512 . We employ the FaceNet512 model [57], which"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "audition [43]. An exemplary foundation model is Wav2Vec2.0 [9],",
          "Shahin Amiriparian et al.": "is specially trained for face recognition. SpeciÔ¨Åcally, we utilize the"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "which has found widespread application in SER tasks [47, 51]. Given",
          "Shahin Amiriparian et al.": "implementation provided in the deepface library [58], yielding a"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "that all sub-challenges are aÔ¨Äect-related, we opt for a large version",
          "Shahin Amiriparian et al.": "512-dimensional embedding for each face image."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "of Wav2Vec2.0 Ô¨Åne-tuned on the MSP-Podcast dataset speciÔ¨Åcally",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "3https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2https://vocalremover.org/",
          "Shahin Amiriparian et al.": "4https://py-feat.org"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe 2024: Baseline Paper": "3.3.4\nVision Transformer (ViT-FER ). Additionally, we leverage a",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "layers is taken as the embedding of\nthe entire sequence and fed"
        },
        {
          "MuSe 2024: Baseline Paper": "Ô¨Ånetuned variant of ViT [24] provided by [18]5, adapted to emo-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "into two feed-forward layers for classiÔ¨Åcation. For both tasks, hy-"
        },
        {
          "MuSe 2024: Baseline Paper": "tion recognition on the FER2013 dataset [32]. We choose this ap-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "perparameters, including the number of GRU layers, learning rate,"
        },
        {
          "MuSe 2024: Baseline Paper": "proach since, contrary to the original ViT model,\nit\nis explicitly",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and GRU representation size, are optimized. To obtain a set of uni-"
        },
        {
          "MuSe 2024: Baseline Paper": "trained on pictures of faces rather than a wide variety of domains.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "modal baselines, the model\nis optimized and trained for each fea-"
        },
        {
          "MuSe 2024: Baseline Paper": "The last hidden state of the special [CLS] token is utilized as the",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ture as described in Section 3. For a simple multimodal baseline,"
        },
        {
          "MuSe 2024: Baseline Paper": "representation of a face image.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "we employ a weighted late fusion approach, averaging the best-"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "performing audio-based model‚Äôs predictions with those of the best-"
        },
        {
          "MuSe 2024: Baseline Paper": "3.4\nText: Transformers",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "performing video-based model. The predictions are weighted by"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the performance of the respective model on the development set."
        },
        {
          "MuSe 2024: Baseline Paper": "This modality is exclusive to the MuSe-Humor sub-challenge. Given",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Simulating challenge conditions, we conduct all experiments with"
        },
        {
          "MuSe 2024: Baseline Paper": "that MuSe-Humor involves training and development data in Ger-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "5 Ô¨Åxed seeds for both sub-challenges. We provide the baseline code,"
        },
        {
          "MuSe 2024: Baseline Paper": "man but a test set in English, we utilize the multilingual version of",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "checkpoints, and hyperparameter conÔ¨Ågurations in the accompa-"
        },
        {
          "MuSe 2024: Baseline Paper": "BERT [23]6, pretrained on Wikipedia entries across 104 languages,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "nying GitHub repository7."
        },
        {
          "MuSe 2024: Baseline Paper": "including German and English. This model has demonstrated ef-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "fective generalization across diÔ¨Äerent languages [52]. SpeciÔ¨Åcally,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3.6.1\nMuSe-Perception. For each of the 16 prediction targets in"
        },
        {
          "MuSe 2024: Baseline Paper": "we compute sentence representations by extracting the encoding",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "MuSe-Perception, a separate model is trained. We observe this ap-"
        },
        {
          "MuSe 2024: Baseline Paper": "of\nthe CLS token from the model‚Äôs Ô¨Ånal\nlayer (768-dimensional),",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "proach to be more promising than a multi-label prediction setup,"
        },
        {
          "MuSe 2024: Baseline Paper": "which represents the entire input sentence.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "i. e., one model predicting all 16 labels. Consequently, the late fu-"
        },
        {
          "MuSe 2024: Baseline Paper": "While BERT embeddings have demonstrated to be suitable for",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "sion approach also fuses the best video and audio models per tar-"
        },
        {
          "MuSe 2024: Baseline Paper": "the task of humor recognition [33, 42, 67],\nthey can only be re-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "get label. Nonetheless, we would like to encourage participants to"
        },
        {
          "MuSe 2024: Baseline Paper": "garded as a simple baseline. The advent of\nlarger LLMs such as",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "explore combinations of prediction targets.\nIn order to keep the"
        },
        {
          "MuSe 2024: Baseline Paper": "LLaMa [61] or Mistral [37] opens up new avenues for humor recog-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "number of experiments in a reasonable range, we optimize the hy-"
        },
        {
          "MuSe 2024: Baseline Paper": "nition based on the textual modality. We would like to strongly en-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "perparameters on one target only and employ the conÔ¨Åguration"
        },
        {
          "MuSe 2024: Baseline Paper": "courage participants to explore the aptitude of (multilingual) LLMs",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "thus found for all 16 labels. As MuSe-Perception is a regression"
        },
        {
          "MuSe 2024: Baseline Paper": "for MuSe-Humor, as the text modality proves to be promising for",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "task, we choose Mean Squared Error (MSE) for the loss function."
        },
        {
          "MuSe 2024: Baseline Paper": "the problem at hand (cf. Section 4.2).",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "3.6.2\nMuSe-Humor.\nIn the Passau-SFCH dataset, each label per-"
        },
        {
          "MuSe 2024: Baseline Paper": "3.5\nAlignment",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tains to a 2 s frame. As the features are extracted in 500 ms intervals,"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "data points in the MuSe-Humor experiments are sequences with"
        },
        {
          "MuSe 2024: Baseline Paper": "The Passau-SFCH dataset comprises audio, video, and transcripts.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "a length of 4 at most. Binary Cross-Entropy (BCE)\nis used as the"
        },
        {
          "MuSe 2024: Baseline Paper": "To facilitate the development of multimodal models leveraging these",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "loss function."
        },
        {
          "MuSe 2024: Baseline Paper": "features, we align the various modalities with each other and with",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "the labeling scheme speciÔ¨Åc to the task.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "4\nBASELINE RESULTS"
        },
        {
          "MuSe 2024: Baseline Paper": "Both audio and face-based features are extracted at a rate of 2 Hz,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "using sliding windows with a step size of 500 ms for audio, and sam-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "We implement\nthe training of GRUs as outlined in the preceding"
        },
        {
          "MuSe 2024: Baseline Paper": "pling faces at a 2 Hz rate for the video modality. Deriving sentence-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "section.\nIn the following, we introduce and analyze the baseline"
        },
        {
          "MuSe 2024: Baseline Paper": "wise timestamps from manual\ntranscripts is done in three steps:",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "results."
        },
        {
          "MuSe 2024: Baseline Paper": "Ô¨Årstly, the Montreal Forced Aligner (MFA) [45] toolkit is utilized",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "MuSe-Perception\n4.1"
        },
        {
          "MuSe 2024: Baseline Paper": "to generate word-level timestamps. Next, punctuation is automati-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "cally added to the transcripts using the deepmultilingualpunctuation",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "For the MuSe-Perception sub-challenge, we Ô¨Årst present the mean"
        },
        {
          "MuSe 2024: Baseline Paper": "tool [34]. Finally, the transcripts are segmented into sentences us-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Pearson‚Äôs correlation over all 16 target dimensions for each feature"
        },
        {
          "MuSe 2024: Baseline Paper": "ing PySBD [56], allowing for the inference of sentence-wise times-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "in Table 3. Averaged across all\ntargets, ViT-FER leads to the best"
        },
        {
          "MuSe 2024: Baseline Paper": "tamps from the word-level timestamps. Subsequently, 2 Hz textual",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "overall single-modality results at ùúå = .3679 on development and"
        },
        {
          "MuSe 2024: Baseline Paper": "features are computed by averaging the embeddings of sentences",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ùúå = .2577 on test. Looking only at the audio modality, eGeMAPS"
        },
        {
          "MuSe 2024: Baseline Paper": "overlapping with the respective 500 ms windows.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "outperforms both evaluated deep features ‚Äì DeepSpectrum and"
        },
        {
          "MuSe 2024: Baseline Paper": "As the labels in Passau-SFCH correspond to windows of size 2 s,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Wav2Vec2.0 ‚Äì at .2561 and .1442. Overall, however, results on de-"
        },
        {
          "MuSe 2024: Baseline Paper": "the alignment of features for MuSe-Humor with annotations isn‚Äôt",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "velopment and test partition often diverge substantially, e. g., Deep-"
        },
        {
          "MuSe 2024: Baseline Paper": "direct but can be facilitated.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Spectrum drops from .2075 to .0113. The weighted late fusion ap-"
        },
        {
          "MuSe 2024: Baseline Paper": "In MuSe-Perception, the labels pertain to entire videos, elimi-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "proach, which chooses the best model\nfor each target separately,"
        },
        {
          "MuSe 2024: Baseline Paper": "nating the need for alignment with the labels.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "leads to greatly increased correlations on both development (ùúå ="
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ".5579) and test set (ùúå = .3573), suggesting that no singular feature"
        },
        {
          "MuSe 2024: Baseline Paper": "3.6\nBaseline Training",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "works best for every target label."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "To further investigate how the conÔ¨Ågurations fare at recogniz-"
        },
        {
          "MuSe 2024: Baseline Paper": "We provide participants with a simple baseline system based on",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ing each of the 16 attributes, we report the best development and"
        },
        {
          "MuSe 2024: Baseline Paper": "a GRU-RNN. Our model encodes\nsequential data via a stack of",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "GRU layers. The Ô¨Ånal hidden representation of\nthe last of\nthese",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "accompanying test set results for the individual\ntarget attributes"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "in Table 2. We further make use of the ‚ÄúBig Two‚Äù dimensions Agency"
        },
        {
          "MuSe 2024: Baseline Paper": "5https://huggingface.co/trpakov/vit-face-expression",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "6https://huggingface.co/bert-base-multilingual-cased",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "7https://github.com/amirip/MuSe-2024"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and Communion [11]\nto split\nthe target\nlabels\ninto two groups.",
          "Shahin Amiriparian et al.": "We employed publicly accessible codes and repositories to ex-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "From the agentive subgroup, aggressive can be recognized quite",
          "Shahin Amiriparian et al.": "tract audio, visual, and textual\nfeatures. Additionally, we trained"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "well by models trained on any of the evaluated features, with ViT-",
          "Shahin Amiriparian et al.": "simple GRU models on the acquired representations to establish"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "FER achieving the best results on both development and test re-",
          "Shahin Amiriparian et al.": "the oÔ¨Écial challenge baselines. These baselines represent the mod-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "sults and further showing solid generalization capabilities. How-",
          "Shahin Amiriparian et al.": "els‚Äô performance on the test partitions of each sub-challenge, out-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ever,\nfor conÔ¨Ådent,\nthe best strong development performance of",
          "Shahin Amiriparian et al.": "lined as follows: a mean ùúå value of .3573 for MuSe-Perception,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ViT-FER does not transfer to the test set, dropping from .7373 to a",
          "Shahin Amiriparian et al.": "and an AUC value of\n.8682 for MuSe-Humor. Both baseline re-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "mere .0783 Pearson‚Äôs correlation. In the communal subgroup, good-",
          "Shahin Amiriparian et al.": "sults were achieved via late fusion of all modalities in each respec-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "natured and kind are detected quite reliably with good generaliza-",
          "Shahin Amiriparian et al.": "tive sub-challenge."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tion behavior from models trained on visual features. Audio mod-",
          "Shahin Amiriparian et al.": "By sharing our code, datasets, and features publicly, we aim to"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "els trained on deep features (DeepSpectrum and Wav2Vec2.0 ), on",
          "Shahin Amiriparian et al.": "facilitate broader participation and engagement from the research"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "the other hand, see performance drop to chance level when moving",
          "Shahin Amiriparian et al.": "community. This open approach promotes transparency and en-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "from development to test.",
          "Shahin Amiriparian et al.": "courages researchers to build upon our work, accelerating progress"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "in multimodal data processing. As we continue to reÔ¨Åne our base-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "line systems and explore new methodologies, we anticipate further"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "MuSe-Humor\n4.2",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "advancements in our understanding of human behavior and com-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Table 4 reports the baselines for MuSe-Humor.",
          "Shahin Amiriparian et al.": "munication across diÔ¨Äerent modalities. Through ongoing collabo-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "All models achieve above-chance (0.5 AUC) results, regardless",
          "Shahin Amiriparian et al.": "ration and experimentation within the MuSe 2024 framework, we"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "of modality and employed feature representations.\nIn this year‚Äôs",
          "Shahin Amiriparian et al.": "hope to drive innovation and ultimately contribute to the develop-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "rendition of the sub-challenge, the introduction of ViT-FER features",
          "Shahin Amiriparian et al.": "ment of more eÔ¨Äective multimodal machine learning systems for"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "helped the visual modality catch up to the best audio results found",
          "Shahin Amiriparian et al.": "behavioral modeling and aÔ¨Äective analysis."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "again with Wav2Vec2.0 embeddings. GRUs trained on these fea-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tures reach .7995 (ViT-FER ) and .8042 (Wav2Vec2.0 ) on the test",
          "Shahin Amiriparian et al.": "6\nACKNOWLEDGMENTS"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "partition. However,\nit can be observed that ViT-FER features do",
          "Shahin Amiriparian et al.": "This project has received funding from the Deutsche Forschungs-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "not generalize as well to unseen data, with performance dropping",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "gemeinschaft (DFG) under grant agreement No. 461420398, and the"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "from .8932 to .7995 between development and test partitions. As",
          "Shahin Amiriparian et al.": "DFG‚Äôs Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS)."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "last year, these generalization deÔ¨Åcits extend across all visual fea-",
          "Shahin Amiriparian et al.": "Shahin Amiriparian and Bj√∂r W. Schuller are also aÔ¨Éliated with"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tures while the audio modality fares better, with Wav2Vec2.0 and",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "the Munich Center for Machine Learning (MCML), Germany. Bj√∂r"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "eGeMAPS only slightly dropping in AUC and DeepSpectrum matching",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "W. Schuller is further aÔ¨Éliated with the Chair of Embedded Intel-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "development performance on the test set. The eÔ¨Écacy of exploiting",
          "Shahin Amiriparian et al.": "ligence for Healthcare and Wellbeing (EIHW), University of Augs-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "linguistic information is further demonstrated by the purely tex-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "burg, Germany, Chair of Health Informatics (CHI), Klinikum rechts"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tual BERT features, which come third after Wav2Vec2.0 and ViT-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "",
          "Shahin Amiriparian et al.": "der Isar (MRI), Technical University of Munich, Germany, and the"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "FER , outperforming all other single modality models.",
          "Shahin Amiriparian et al.": "Munich Data Science Institute (MDSI), Germany."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Our late fusion approach consistently improves over the respec-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tive unimodal baselines. The performance gains achieved in the",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "visual modality with ViT-FER features further lead to fusion set-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tings, which include video, eclipsing those that rely on only audio",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and text. The overall highest AUCs of .9251 .8682 on development",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and test sets, respectively, are achieved by fusing all three modali-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ties.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "5\nCONCLUSIONS",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "We introduced MuSe 2024 ‚Äì the 5th Multimodal Sentiment Analy-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "sis challenge, comprising two sub-challenges, MuSe-Perception",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and MuSe-Humor. For the MuSe-Perception sub-challenge, the",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "novel LMU-ELP dataset has been introduced and is made available,",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "which consists of interview recordings of CEOs who present their",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Ô¨Årms to potential investors before taking their Ô¨Årms public. Partic-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ipants were tasked to predict 16 diÔ¨Äerent attributes of CEOs, e. g.,",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "conÔ¨Ådence, or sincerity on a Likert scale from 1 to 7.",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "The MuSe-Humor sub-challenge is a relaunch of the same task",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "from the 2023 edition of MuSe [4, 20]. It uses an extended version",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "of the Passau-SFCH dataset [21]. Participants are tasked to detect",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "spontaneous humor in press conferences across cultures and lan-",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "guages,\ntraining their models on recordings of German-speaking",
          "Shahin Amiriparian et al.": ""
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "trainers and testing on English data.",
          "Shahin Amiriparian et al.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluation Metric: [ùúå ‚Üë]": ""
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": "Wav2Vec2.0"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ""
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": "Test"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ""
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".3266"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".4808"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".1277"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".0618"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".1293"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".3514"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".2074"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".2826"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ""
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": "-.1357"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".0705"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".1116"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".1209"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".0382"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".1423"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".2541"
        },
        {
          "Evaluation Metric: [ùúå ‚Üë]": ".1732"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: MuSe-Perception baseline results. Eachline refers to experimentsconducted with 5 fixed seedsand reports the",
      "data": [
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": ""
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "Features"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "Audio"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "eGeMAPS"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": ""
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "DeepSpectrum"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": ""
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "Wav2Vec2.0"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": ""
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "Video"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "FAU"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "ViT-FER"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": ""
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "FaceNet512"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": ""
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "Late Fusion"
        },
        {
          "best Pearson correlation among them, together with the mean Pearson correlations and their standard deviations across the": "Audio + Video"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: MuSe-Perception baseline results. Eachline refers to experimentsconducted with 5 fixed seedsand reports the",
      "data": [
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Features"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Audio"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "eGeMAPS"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "DeepSpectrum"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Wav2Vec2.0"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Video"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "FAU"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "ViT-FER"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "FaceNet512"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Text"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "BERT"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": ""
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Late Fusion"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Audio + Text"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Audio + Video"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Text + Video"
        },
        {
          "Table 4: MuSe-Humor baseline results. Each line refers to experiments conducted with 5 Ô¨Åxed seeds and reports the best AUC-": "Audio + Text + Video"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe 2024: Baseline Paper": "REFERENCES",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "New York, NY, USA, 23‚Äì28.\nhttps://doi.org/10.1145/3551876.3554805"
        },
        {
          "MuSe 2024: Baseline Paper": "[1] Andrea E Abele, Naomi Ellemers, Susan T Fiske, Alex Koch, and Vincent Yzer-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[20]\nLukas Christ, Shahin Amiriparian, Alice Baird, Alexander Kathan, Niklas M√ºller,"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "SteÔ¨Äen Klug, Chris Gagne,\nPanagiotis Tzirakis, Lukas\nStappen, Eva-Maria"
        },
        {
          "MuSe 2024: Baseline Paper": "byt. 2021.\nNavigating the social world: Toward an integrated framework for",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "evaluating self, individuals, and groups. Psychological Review 128, 2 (2021), 290.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Me√üner, et al. 2023.\nThe muse 2023 multimodal sentiment analysis challenge:"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Mimicked emotions, cross-cultural humour, and personalisation. In Proceedings"
        },
        {
          "MuSe 2024: Baseline Paper": "[2] Andrea E. Abele and Bogdan Wojciszke. 2014. Chapter Four - Communal and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked"
        },
        {
          "MuSe 2024: Baseline Paper": "Agentic Content in Social Cognition: A Dual Perspective Model.\nIn Advances in",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Emotions, Humour and Personalisation. 1‚Äì10."
        },
        {
          "MuSe 2024: Baseline Paper": "Experimental Social Psychology, James M. Olson and Mark P. Zanna (Eds.). Vol. 50.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[21]\nLukas Christ, Shahin Amiriparian, Alexander Kathan, Niklas M√ºller, Andreas"
        },
        {
          "MuSe 2024: Baseline Paper": "Academic Press, 195‚Äì255.\nhttps://doi.org/10.1016/B978-0-12-800284-1.00004-7",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "[3] Nalini Ambady and John Joseph Skowronski. 2008.\nFirst impressions. Guilford",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "K√∂nig,\nand Bj√∂rn W Schuller.\n2023.\nTowards Multimodal\nPrediction\nof"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Spontaneous Humour: A Novel Dataset\nand\nFirst Results.\narXiv\npreprint"
        },
        {
          "MuSe 2024: Baseline Paper": "Press.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "[4]\nShahin Amiriparian, Lukas Christ, Andreas K√∂nig, Eva-Maria Messner, Alan",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "arXiv:2209.14272 (2023)."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[22] Andrew Cutler and David M Condon. 2022. Deep lexical hypothesis:\nIdentify-"
        },
        {
          "MuSe 2024: Baseline Paper": "Cowen, Erik Cambria, and Bj√∂rn W. Schuller. 2023. MuSe 2023 Challenge: Mul-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ing personality structure in natural\nlanguage.\nJournal of Personality and Social"
        },
        {
          "MuSe 2024: Baseline Paper": "timodal Prediction of Mimicked Emotions, Cross-Cultural Humour, and Person-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "alised Recognition of AÔ¨Äects. In Proceedings of the 31st ACM International Con-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Psychology (2022)."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[23]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:"
        },
        {
          "MuSe 2024: Baseline Paper": "ference on Multimedia (MM‚Äô23), October 29-November 2, 2023, Ottawa, Canada.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "Association for Computing Machinery, Ottawa, Canada.\nto appear.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Pre-training of Deep Bidirectional Transformers for Language Understanding."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In Proceedings of the 2019 Conference of the North American Chapter of the Asso-"
        },
        {
          "MuSe 2024: Baseline Paper": "[5]\nShahin Amiriparian, Nicholas Cummins, Sandra Ottl, Maurice Gerczuk, and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "Bj√∂rn Schuller. 2017. Sentiment Analysis Using Image-based Deep Spectrum Fea-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ciation for Computational Linguistics: Human Language Technologies. 4171‚Äì4186."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,"
        },
        {
          "MuSe 2024: Baseline Paper": "tures. In Proceedings 2nd International Workshop on Automatic Sentiment Analy-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Xiaohua\nZhai,\nThomas\nUnterthiner, Mostafa\nDehghani, Matthias Min-"
        },
        {
          "MuSe 2024: Baseline Paper": "sis in the Wild (WASA 2017) held in conjunction with the 7th biannual Conference",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "on AÔ¨Äective Computing and Intelligent Interaction (ACII 2017). AAAC, IEEE, San",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "derer, Georg Heigold,\nSylvain Gelly,\nJakob Uszkoreit,\nand Neil Houlsby."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2021.\nAn Image\nis Worth\n16x16 Words: Transformers\nfor\nImage Recog-"
        },
        {
          "MuSe 2024: Baseline Paper": "Antonio, TX, 26‚Äì29.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "[6]\nShahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Nicholas Cummins, Michael",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "nition\nat\nScale.\nIn\nInternational\nConference\non\nLearning\nRepresentations."
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "https://openreview.net/forum?id=YicbFdNTTy"
        },
        {
          "MuSe 2024: Baseline Paper": "Freitag, Sergey Pugachevskiy, and Bj√∂rn Schuller. 2017. Snore Sound ClassiÔ¨Åca-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[25] Alice H Eagly and Steven J Karau. 2002.\nRole congruity theory of prejudice"
        },
        {
          "MuSe 2024: Baseline Paper": "tion Using Image-based Deep Spectrum Features. In Proceedings INTERSPEECH",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": ""
        },
        {
          "MuSe 2024: Baseline Paper": "2017, 18th Annual Conference of the International Speech Communication Associ-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "toward female leaders. Psychological review 109, 3 (2002), 573."
        },
        {
          "MuSe 2024: Baseline Paper": "ation. ISCA, ISCA, Stockholm, Sweden, 3512‚Äì3516.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[26]\nPaul Ekman and Wallace V Friesen. 1978. Facial action coding system. Environ-"
        },
        {
          "MuSe 2024: Baseline Paper": "[7]\nShahin Amiriparian, Maurice Gerczuk, Lukas Stappen, Alice Baird, Lukas Koebe,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "mental Psychology & Nonverbal Behavior (1978)."
        },
        {
          "MuSe 2024: Baseline Paper": "Sandra Ottl, and Bj√∂rn Schuller. 2020. Towards Cross-Modal Pre-Training and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[27]\nSimone Maria Eulitz and Brooke A. Gazdag. 2021.\nBeyond Biology ‚Äì The"
        },
        {
          "MuSe 2024: Baseline Paper": "Learning Tempo-Spatial Characteristics for Audio Recognition with Convolu-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "impact\nof\nPerceptions CEO Social Gender\non\nInvestor\nReactions During"
        },
        {
          "MuSe 2024: Baseline Paper": "tional and Recurrent Neural Networks. EURASIP Journal on Audio, Speech, and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "an IPO.\nAcademy of Management\nProceedings\n2021,\n1\n(Aug.\n2021),\n12379."
        },
        {
          "MuSe 2024: Baseline Paper": "Music Processing 2020, 19 (2020), 1‚Äì11.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "https://doi.org/10.5465/AMBPP.2021.12379abstract Publisher: Academy of Man-"
        },
        {
          "MuSe 2024: Baseline Paper": "[8]\nShahin Amiriparian, Tobias H√ºbner, Vincent Karas, Maurice Gerczuk, San-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "agement."
        },
        {
          "MuSe 2024: Baseline Paper": "dra Ottl, and Bj√∂rn W. Schuller. 2022.\nDeepSpectrumLite: A Power-EÔ¨Écient",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[28]\nFlorian Eyben, Klaus R Scherer, Bj√∂rn W Schuller, Johan Sundberg, Elisabeth An-"
        },
        {
          "MuSe 2024: Baseline Paper": "Transfer Learning Framework for Embedded Speech and Audio Processing",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "dr√©, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka, Shrikanth S"
        },
        {
          "MuSe 2024: Baseline Paper": "From Decentralized Data.\nFrontiers in ArtiÔ¨Åcial\nIntelligence 5 (2022), 10 pages.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Narayanan,\net\nal.\n2015.\nThe Geneva minimalistic\nacoustic parameter\nset"
        },
        {
          "MuSe 2024: Baseline Paper": "https://doi.org/10.3389/frai.2022.856232",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "(GeMAPS) for voice research and aÔ¨Äective computing.\nIEEE Transactions on Af-"
        },
        {
          "MuSe 2024: Baseline Paper": "[9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "fective Computing 7, 2 (2015), 190‚Äì202."
        },
        {
          "MuSe 2024: Baseline Paper": "wav2vec 2.0: A framework for self-supervised learning of speech representa-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[29]\nFlorian Eyben, Martin W√∂llmer, and Bj√∂rn Schuller. 2010. Opensmile: the mu-"
        },
        {
          "MuSe 2024: Baseline Paper": "tions. Advances in neural information processing systems 33 (2020), 12449‚Äì12460.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "nich versatile and fast open-source audio feature extractor. In Proceedings of the"
        },
        {
          "MuSe 2024: Baseline Paper": "[10] Alice Baird, Shahin Amiriparian, and Bj√∂rn Schuller. 2019. Can deep generative",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "18th ACM International Conference on Multimedia. Association for Computing"
        },
        {
          "MuSe 2024: Baseline Paper": "audio be emotional? Towards an approach for personalised emotional audio gen-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Machinery, Firenze, Italy, 1459‚Äì1462."
        },
        {
          "MuSe 2024: Baseline Paper": "eration. In 2019 IEEE 21st International Workshop on Multimedia Signal Processing",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[30] Maurice Gerczuk, Shahin Amiriparian, Sandra Ottl, and Bj√∂rn Schuller. 2022."
        },
        {
          "MuSe 2024: Baseline Paper": "(MMSP). IEEE, IEEE, Kuala Lumpur, Malaysia, 1‚Äì5.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion"
        },
        {
          "MuSe 2024: Baseline Paper": "[11] David Bakan. 1966. The duality of human existence: An essay on psychology and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Recognition.\nIEEE Transactions on AÔ¨Äective Computing 13 (2022)."
        },
        {
          "MuSe 2024: Baseline Paper": "religion. Rand Mcnally, Oxford, England. Pages: 242.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[31]\nPanagiotis Gkorezis, Eugenia\nPetridou,\nand\nPanteleimon Xanthiakos.\n2014."
        },
        {
          "MuSe 2024: Baseline Paper": "[12]\nSandra L Bem. 1981. A manual for the Bem sex role inventory. California: Mind",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Leader positive humor and organizational cynicism: LMX as a mediator. Lead-"
        },
        {
          "MuSe 2024: Baseline Paper": "Garden (1981).",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ership & Organization Development Journal 35 (2014), 305 ‚Äì 315."
        },
        {
          "MuSe 2024: Baseline Paper": "[13] Dario Bertero and Pascale Fung. 2016. Deep learning of audio and language fea-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[32]\nIan J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi"
        },
        {
          "MuSe 2024: Baseline Paper": "tures for humor prediction. In Proceedings of the Tenth International Conference",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun"
        },
        {
          "MuSe 2024: Baseline Paper": "on Language Resources and Evaluation (LREC‚Äô16). 496‚Äì501.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Lee, et al. 2013. Challenges in representation learning: A report on three machine"
        },
        {
          "MuSe 2024: Baseline Paper": "[14] Bj√∂rn W. Schuller and Anton Batliner and Christian Bergler and Cecilia Mas-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "learning contests. In Neural Information Processing: 20th International Conference,"
        },
        {
          "MuSe 2024: Baseline Paper": "colo and Jing Han and Iulia Lefter and Heysem Kaya and Shahin Amiriparian",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part III 20. Springer,"
        },
        {
          "MuSe 2024: Baseline Paper": "and Alice Baird and Lukas Stappen and Sandra Ottl and Maurice Gerczuk and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "117‚Äì124."
        },
        {
          "MuSe 2024: Baseline Paper": "Panaguiotis Tzirakis and Chlo√´ Brown and Jagmohan Chauhan and Andreas",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[33] Tam√°s Gr√≥sz, Anja Virkkunen, Dejan Porjazovski, and Mikko Kurimo. 2023. Dis-"
        },
        {
          "MuSe 2024: Baseline Paper": "Grammenos and Apinan Hasthanasombat and Dimitris Spathis and Tong Xia",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "covering Relevant Sub-spaces of BERT, Wav2Vec 2.0, ELECTRA and ViT Embed-"
        },
        {
          "MuSe 2024: Baseline Paper": "and Pietro Cicuta and Leon J. M. Rothkrantz and Joeri Zwerts and Jelle Treep",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "dings for Humor and Mimicked Emotion Recognition with Integrated Gradients."
        },
        {
          "MuSe 2024: Baseline Paper": "and Casper Kaandorp. 2021. The INTERSPEECH 2021 Computational Paralin-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Work-"
        },
        {
          "MuSe 2024: Baseline Paper": "guistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "shop: Mimicked Emotions, Humour and Personalisation. 27‚Äì34."
        },
        {
          "MuSe 2024: Baseline Paper": "In Proceedings INTERSPEECH 2021, 22nd Annual Conference of the International",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[34] Oliver Guhr, Anne-Kathrin Schumann, Frank Bahrmann, and Hans\nJoachim"
        },
        {
          "MuSe 2024: Baseline Paper": "Speech Communication Association. ISCA, ISCA, Brno, Czechia, 431‚Äì435.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "B√∂hme. 2021. FullStop: Multilingual Deep Models for Punctuation Prediction. In"
        },
        {
          "MuSe 2024: Baseline Paper": "[15]\nSimon M Breil, Sarah Osterholz, SteÔ¨Äen Nestler, and Mitja D Back. 2021.\n13",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Proceedings of the Swiss Text Analytics Conference 2021. CEUR Workshop Proceed-"
        },
        {
          "MuSe 2024: Baseline Paper": "contributions of nonverbal cues to the accurate judgment of personality traits.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ings, Winterthur, Switzerland.\nhttp://ceur-ws.org/Vol-2957/sepp_paper4.pdf"
        },
        {
          "MuSe 2024: Baseline Paper": "The Oxford handbook of accurate personality judgment\n(2021), 195‚Äì218.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[35] Md Kamrul Hasan, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea,"
        },
        {
          "MuSe 2024: Baseline Paper": "[16] Andrew J Calder, Michael Ewbank, and Luca Passamonti. 2011. Personality in-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Louis-Philippe Morency, and Ehsan Hoque. 2021. Humor knowledge enriched"
        },
        {
          "MuSe 2024: Baseline Paper": "Ô¨Çuences the neural responses to viewing facial expressions of emotion.\nPhilo-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "transformer for understanding multimodal humor.\nIn Proceedings of\nthe AAAI"
        },
        {
          "MuSe 2024: Baseline Paper": "sophical Transactions of the Royal Society B: Biological Sciences 366, 1571 (2011),",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Conference on ArtiÔ¨Åcial Intelligence, Vol. 35. 12972‚Äì12980."
        },
        {
          "MuSe 2024: Baseline Paper": "1684‚Äì1701.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[36] Md Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong,"
        },
        {
          "MuSe 2024: Baseline Paper": "[17] Arnie Cann, Amanda J Watson, and Elisabeth A Bridgewater. 2014. Assessing",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed (Ehsan) Hoque."
        },
        {
          "MuSe 2024: Baseline Paper": "humor at work: The humor climate questionnaire. Humor 27, 2 (2014), 307‚Äì323.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2019. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor."
        },
        {
          "MuSe 2024: Baseline Paper": "[18] Aayushi Chaudhari, Chintan Bhatt, Achyut Krishna, and Pier Luigi Mazzeo.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language"
        },
        {
          "MuSe 2024: Baseline Paper": "2022.\nViTFER:\nfacial emotion recognition with vision transformers.\nApplied",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Processing and the 9th International Joint Conference on Natural Language Process-"
        },
        {
          "MuSe 2024: Baseline Paper": "System Innovation 5, 4 (2022), 80.",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong,"
        },
        {
          "MuSe 2024: Baseline Paper": "[19] Chengxin Chen and Pengyuan Zhang. 2022.\nIntegrating Cross-Modal Interac-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "China, 2046‚Äì2056.\nhttps://doi.org/10.18653/v1/D19-1211"
        },
        {
          "MuSe 2024: Baseline Paper": "tions via Latent Representation Shift for Multi-Modal Humor Detection. In Pro-",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[37] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-"
        },
        {
          "MuSe 2024: Baseline Paper": "ceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,"
        },
        {
          "MuSe 2024: Baseline Paper": "Challenge (Lisboa, Portugal) (MuSe‚Äô 22). Association for Computing Machinery,",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Guillaume Lample, Lucile Saulnier, et al. 2023.\nMistral 7B.\narXiv preprint"
        },
        {
          "MuSe 2024: Baseline Paper": "",
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "arXiv:2310.06825 (2023)."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[38] Alexander Kachur, Evgeny Osin, Denis Davydov, Konstantin Shutilov,\nand",
          "Shahin Amiriparian et al.": "comparison of smiling behavior in humorous sequences in american english and"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Alexey Novokshonov. 2020. Assessing the Big Five personality traits using real-",
          "Shahin Amiriparian et al.": "french interactions.\nIntercultural Pragmatics 15, 4 (2018), 563‚Äì591."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "life static facial images. ScientiÔ¨Åc Reports 10, 1 (2020), 8487.",
          "Shahin Amiriparian et al.": "[56] Nipun Sadvilkar and Mark Neumann. 2020. PySBD: Pragmatic Sentence Bound-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[39] Anna Ladilova and Ulrike Schr√∂der. 2022. Humor in intercultural interaction: A",
          "Shahin Amiriparian et al.": "ary Disambiguation.\nIn Proceedings of Second Workshop for NLP Open Source"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "source for misunderstanding or a common ground builder? A multimodal anal-",
          "Shahin Amiriparian et al.": "Software (NLP-OSS). Association for Computational Linguistics, Online, 110‚Äì114."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ysis.\nIntercultural Pragmatics 19, 1 (2022), 71‚Äì101.",
          "Shahin Amiriparian et al.": "https://www.aclweb.org/anthology/2020.nlposs-1.15"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[40]\nJia Li, Ziyang Zhang, Junjie Lang, Yueqi Jiang, Liuwei An, Peng Zou, Yangyang",
          "Shahin Amiriparian et al.": "[57]\nFlorian SchroÔ¨Ä, Dmitry Kalenichenko,\nand\nJames Philbin.\n2015.\nFaceNet:"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Xu, Sheng Gao, Jie Lin, Chunxiao Fan, Xiao Sun, and Meng Wang. 2022. Hybrid",
          "Shahin Amiriparian et al.": "A uniÔ¨Åed\nembedding\nfor\nface\nrecognition\nand\nclustering.\nIn\n2015\nIEEE"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Multimodal Feature Extraction, Mining and Fusion for Sentiment Analysis.\nIn",
          "Shahin Amiriparian et al.": "Conference\non\nComputer\nVision\nand\nPattern\nRecognition\n(CVPR).\nIEEE."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop",
          "Shahin Amiriparian et al.": "https://doi.org/10.1109/cvpr.2015.7298682"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and Challenge (Lisboa, Portugal) (MuSe‚Äô 22). Association for Computing Machin-",
          "Shahin Amiriparian et al.": "[58]\nSeÔ¨Åk\nIlkin\nSerengil\nand\nAlper\nOzpinar.\n2020.\nLightFace:\nA\nHy-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ery, New York, NY, USA, 81‚Äì88.\nhttps://doi.org/10.1145/3551876.3554809",
          "Shahin Amiriparian et al.": "brid\nDeep\nFace\nRecognition\nFramework.\nIn\n2020\nInnovations\nin\nIn-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[41] Qi Li, Shulei Tang, Feixiang Zhang, Ruotong Wang, Yangyang Xu, Zhuoer Zhao,",
          "Shahin Amiriparian et al.": "telligent\nSystems\nand\nApplications\nConference\n(ASYU).\nIEEE,\n23‚Äì27."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Xiao Sun, and Meng Wang. 2023. Temporal-aware Multimodal Feature Fusion",
          "Shahin Amiriparian et al.": "https://doi.org/10.1109/ASYU50717.2020.9259802"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "for Sentiment Analysis. In Proceedings of the 4th on Multimodal Sentiment Anal-",
          "Shahin Amiriparian et al.": "[59]\nSinan Sonlu, Uƒüur G√ºd√ºkbay, and Funda Durupinar. 2021.\nA conversational"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation.",
          "Shahin Amiriparian et al.": "agent framework with multi-modal personality expression. ACM Transactions"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "99‚Äì105.",
          "Shahin Amiriparian et al.": "on Graphics (TOG) 40, 1 (2021), 1‚Äì16."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[42] Qi Li, Yangyang Xu, Zhuoer Zhao, Shulei Tang, Feixiang Zhang, Ruotong Wang,",
          "Shahin Amiriparian et al.": "[60]\nJulia M Taylor and Lawrence J Mazlack. 2004.\nComputationally recognizing"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Xiao Sun, and Meng Wang. 2023.\nJTMA: Joint Multimodal Feature Fusion and",
          "Shahin Amiriparian et al.": "wordplay in jokes. In Proceedings of the Annual Meeting of the Cognitive Science"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Temporal Multi-head Attention for Humor Detection.\nIn Proceedings of the 4th",
          "Shahin Amiriparian et al.": "Society, Vol. 26."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions,",
          "Shahin Amiriparian et al.": "[61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Humour and Personalisation. 59‚Äì65.",
          "Shahin Amiriparian et al.": "Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[43]\nShuo\nLiu, Adria Mallol-Ragolta,\nEmilia\nParada-Cabaleiro, Kun Qian, Xin",
          "Shahin Amiriparian et al.": "Azhar, et al. 2023. Llama: Open and eÔ¨Écient foundation language models. arXiv"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Jing,\nAlexander\nKathan,\nBin Hu,\nand\nBj√∂rn W.\nSchuller.\n2022.\nAu-",
          "Shahin Amiriparian et al.": "preprint arXiv:2302.13971 (2023)."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "dio\nself-supervised\nlearning: A survey.\nPatterns\n3,\n12\n(2022),\n100616.",
          "Shahin Amiriparian et al.": "[62] Bogdan Vlasenko, RaviShankar Prasad, and Mathew Magimai.-Doss. 2021.\nFu-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "https://doi.org/10.1016/j.patter.2022.100616",
          "Shahin Amiriparian et al.": "sion of Acoustic and Linguistic Information using Supervised Autoencoder for"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[44] Rod A Martin, Patricia Puhlik-Doris, Gwen Larsen, Jeanette Gray, and Kelly Weir.",
          "Shahin Amiriparian et al.": "Improved Emotion Recognition.\nIn Proceedings of the 2nd on Multimodal Senti-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "2003. Individual diÔ¨Äerences in uses of humor and their relation to psychological",
          "Shahin Amiriparian et al.": "ment Analysis Challenge. Association for Computing Machinery, New York, NY,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "well-being: Development of the Humor Styles Questionnaire. Journal of research",
          "Shahin Amiriparian et al.": "USA, 51‚Äì59."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "in personality 37, 1 (2003), 48‚Äì75.",
          "Shahin Amiriparian et al.": "[63]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M.\nSchmitt,\nF. Burkhardt,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[45] Michael McAuliÔ¨Äe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Mor-",
          "Shahin Amiriparian et al.": "F.\nEyben,\nand B. W.\nSchuller.\n2023.\nDawn\nof\nthe Transformer\nEra\nin"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "gan Sonderegger. 2017. Montreal Forced Aligner: Trainable Text-Speech Align-",
          "Shahin Amiriparian et al.": "Speech\nEmotion\nRecognition:\nClosing\nthe\nValence\nGap.\nIEEE\nTrans-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ment Using Kaldi..\nIn Proceedings of\nINTERSPEECH, Vol. 2017.\nInternational",
          "Shahin Amiriparian et al.": "actions\non\nPattern\nAnalysis\n&\nMachine\nIntelligence\n01\n(2023),\n1‚Äì13."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Speech Communication Association (ISCA), Stockholm, Sweden, 498‚Äì502.",
          "Shahin Amiriparian et al.": "https://doi.org/10.1109/TPAMI.2023.3263585"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[46] Anirudh Mittal, Pranav Jeevan, Prerak Gandhi, Diptesh Kanojia, and Pushpak",
          "Shahin Amiriparian et al.": "[64]\nJiaming Wu, Hongfei Lin, Liang Yang, and Bo Xu. 2021. MUMOR: A Multimodal"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Bhattacharyya. 2021.\n\" So You Think You‚Äôre Funny?\": Rating the Humour Quo-",
          "Shahin Amiriparian et al.": "Dataset\nfor Humor Detection in Conversations.\nIn CCF International Confer-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tient in Standup Comedy.\narXiv:arXiv preprint arXiv:2110.12765",
          "Shahin Amiriparian et al.": "ence on Natural Language Processing and Chinese Computing. Springer, Springer,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[47]\nEdmilson Morais, Ron Hoory, Weizhong Zhu, Itai Gat, Matheus Damasceno, and",
          "Shahin Amiriparian et al.": "Qingdao, China, 619‚Äì627."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Hagai Aronowitz. 2022.\nSpeech emotion recognition using self-supervised fea-",
          "Shahin Amiriparian et al.": "[65] Heng Xie, Jizhou Cui, Yuhang Cao, Junjie Chen, Jianhua Tao, Cunhang Fan, Xue-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tures. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and",
          "Shahin Amiriparian et al.": "fei Liu, Zhengqi Wen, Heng Lu, Yuguang Yang, et al. 2023. Multimodal Cross-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Signal Processing (ICASSP). IEEE, 6922‚Äì6926.",
          "Shahin Amiriparian et al.": "Lingual Features and Weight Fusion for Cross-Cultural Humor Detection.\nIn"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[48]\nSandra Ottl, Shahin Amiriparian, Maurice Gerczuk, Vincent Karas, and Bj√∂rn",
          "Shahin Amiriparian et al.": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop:"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Schuller. 2020. Group-level Speech Emotion Recognition Utilising Deep Spec-",
          "Shahin Amiriparian et al.": "Mimicked Emotions, Humour and Personalisation. 51‚Äì57."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "trum Features. In Proceedings of\nthe 8th ICMI 2020 EmotiW ‚Äì Emotion Recogni-",
          "Shahin Amiriparian et al.": "[66] Haojie Xu, Weifeng Liu,\nJiangwei Liu, Mingzheng Li, Yu Feng, Yasi Peng,"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tion In The Wild Challenge (EmotiW 2020), 22nd ACM International Conference",
          "Shahin Amiriparian et al.": "Yunwei\nShi, Xiao\nSun,\nand Meng Wang.\n2022.\nHybrid Multimodal\nFu-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "on Multimodal\nInteraction (ICMI 2020). ACM, ACM, Utrecht, The Netherlands,",
          "Shahin Amiriparian et al.": "sion for Humor Detection.\nIn Proceedings\nof\nthe 3rd International\non Multi-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "821‚Äì826.",
          "Shahin Amiriparian et al.": "modal Sentiment Analysis Workshop and Challenge\n(Lisboa, Portugal)\n(MuSe‚Äô"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[49] Ho-Min Park, Ganghyun Kim, Arnout Van Messem, and Wesley De Neve. 2023.",
          "Shahin Amiriparian et al.": "22). Association\nfor Computing Machinery, New York, NY, USA,\n15‚Äì21."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "MuSe-Personalization 2023: Feature Engineering, Hyperparameter Optimiza-",
          "Shahin Amiriparian et al.": "https://doi.org/10.1145/3551876.3554802"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "tion, and Transformer-Encoder Re-discovery. In Proceedings of the 4th on Multi-",
          "Shahin Amiriparian et al.": "[67] Mingyu Xu, Shun Chen, Zheng Lian, and Bin Liu. 2023. Humor Detection System"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "modal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour",
          "Shahin Amiriparian et al.": "for MuSE 2023: Contextual Modeling, Pesudo Labelling, and Post-smoothing. In"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "and Personalisation. 89‚Äì97.",
          "Shahin Amiriparian et al.": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop:"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[50] Ho-min Park, Ilho Yun, Ajit Kumar, Ankit Kumar Singh, Bong Jun Choi, Dhanan-",
          "Shahin Amiriparian et al.": "Mimicked Emotions, Humour and Personalisation. 35‚Äì41."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "jay Singh, and Wesley De Neve. 2022. Towards Multimodal Prediction of Time-",
          "Shahin Amiriparian et al.": "[68] Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. 2015. Humor recognition"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "continuous Emotion using Pose Feature Engineering and a Transformer Encoder.",
          "Shahin Amiriparian et al.": "and humor anchor extraction. In Proceedings of the 2015 conference on empirical"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "In Proceedings of the 3rd International on Multimodal Sentiment Analysis Work-",
          "Shahin Amiriparian et al.": "methods in natural language processing. Association for Computational Linguis-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "shop and Challenge. 47‚Äì54.",
          "Shahin Amiriparian et al.": "tics, Lisbon, Portugal, 2367‚Äì2376."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[51]\nLeonardo Pepino, Pablo Riera, and Luciana Ferrer. 2021. Emotion Recognition",
          "Shahin Amiriparian et al.": "[69] Guofeng Yi, Yuguang Yang, Yu Pan, Yuhang Cao, Jixun Yao, Xiang Lv, Cunhang"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "from Speech Using wav2vec 2.0 Embeddings.\nIn Proc.\nInterspeech 2021.\nISCA,",
          "Shahin Amiriparian et al.": "Fan, Zhao Lv, Jianhua Tao, Shan Liang, et al. 2023. Exploring the Power of Cross-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ISCA, Brno, Czechia, 3400‚Äì3404. https://doi.org/10.21437/Interspeech.2021-703",
          "Shahin Amiriparian et al.": "Contextual Large Language Model in Mimic Emotion Prediction. In Proceedings"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[52] Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How Multilingual is Multi-",
          "Shahin Amiriparian et al.": "of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "lingual BERT?.\nIn Proceedings of the 57th Annual Meeting of the Association for",
          "Shahin Amiriparian et al.": "Emotions, Humour and Personalisation. 19‚Äì26."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Computational Linguistics. Association for Computational Linguistics, Florence,",
          "Shahin Amiriparian et al.": "[70]\nJun Yu, Wangyuan Zhu, Jichao Zhu, Xiaxin Shen, Jianqing Sun, and Jiaen Liang."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Italy, 4996‚Äì5001.\nhttps://doi.org/10.18653/v1/P19-1493",
          "Shahin Amiriparian et al.": "2023. MMT-GD: Multi-Modal Transformer with Graph Distillation for Cross-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[53] Tejas Pradhan, Rashi Bhansali, Dimple Chandnani, and Aditya Pangaonkar. 2020.",
          "Shahin Amiriparian et al.": "Cultural Humor Detection.\nIn Proceedings of\nthe 4th on Multimodal Sentiment"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Analysis of personality traits using natural language processing and deep learn-",
          "Shahin Amiriparian et al.": "Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisa-"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "ing. In 2020 Second International Conference on Inventive Research in Computing",
          "Shahin Amiriparian et al.": "tion. 43‚Äì49."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Applications (ICIRCA). IEEE, 457‚Äì461.",
          "Shahin Amiriparian et al.": "[71] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016.\nJoint Face"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[54]\nShraman Pramanick, Aniket Roy, and Vishal M Patel. 2022. Multimodal Learning",
          "Shahin Amiriparian et al.": "Detection and Alignment Using Multitask Cascaded Convolutional Networks."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "using Optimal Transport for Sarcasm and Humor Detection. In Proceedings of the",
          "Shahin Amiriparian et al.": "IEEE Signal Processing Letters 23 (04 2016)."
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "IEEE/CVF Winter Conference on Applications of Computer Vision. 3930‚Äì3940.",
          "Shahin Amiriparian et al.": "[72] Ruicong Zhi, Mengyi Liu, and Dezheng Zhang. 2020. A comprehensive survey"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "[55] B√©atrice Priego-Valverde, Brigitte Bigi, Salvatore Attardo, Lucy Pickering, and",
          "Shahin Amiriparian et al.": "on automatic facial action unit analysis. The Visual Computer 36 (2020), 1067‚Äì"
        },
        {
          "MuSe ‚Äô24, October 28, 2024, Melbourne, Australia": "Elisa Gironzetti. 2018.\nIs smiling during humor so obvious? a cross-cultural",
          "Shahin Amiriparian et al.": "1093."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Navigating the social world: Toward an integrated framework for evaluating self, individuals, and groups",
      "authors": [
        "Andrea Abele",
        "Naomi Ellemers",
        "Susan Fiske",
        "Alex Koch",
        "Vincent Yzerbyt"
      ],
      "year": "2021",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "2",
      "title": "Chapter Four -Communal and Agentic Content in Social Cognition: A Dual Perspective Model",
      "authors": [
        "Andrea Abele",
        "Bogdan Wojciszke"
      ],
      "year": "2014",
      "venue": "Advances in Experimental Social Psychology",
      "doi": "10.1016/B978-0-12-800284-1.00004-7"
    },
    {
      "citation_id": "3",
      "title": "First impressions",
      "authors": [
        "Nalini Ambady",
        "John Joseph"
      ],
      "year": "2008",
      "venue": "First impressions"
    },
    {
      "citation_id": "4",
      "title": "MuSe 2023 Challenge: Multimodal Prediction of Mimicked Emotions, Cross-Cultural Humour, and Personalised Recognition of Affects",
      "authors": [
        "Shahin Amiriparian",
        "Lukas Christ",
        "Andreas K√∂nig",
        "Eva-Maria Messner",
        "Alan Cowen",
        "Erik Cambria",
        "Bj√∂rn Schuller"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia (MM'23)"
    },
    {
      "citation_id": "5",
      "title": "Sentiment Analysis Using Image-based Deep Spectrum Features",
      "authors": [
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Sandra Ottl",
        "Maurice Gerczuk",
        "Bj√∂rn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings 2nd International Workshop on Automatic Sentiment Analysis in the Wild (WASA 2017) held in conjunction with the 7th biannual Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "6",
      "title": "Snore Sound Classification Using Image-based Deep Spectrum Features",
      "authors": [
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Sandra Ottl",
        "Nicholas Cummins",
        "Michael Freitag",
        "Sergey Pugachevskiy",
        "Bj√∂rn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings INTERSPEECH 2017, 18th Annual Conference of the International Speech Communication Association. ISCA, ISCA"
    },
    {
      "citation_id": "7",
      "title": "Towards Cross-Modal Pre-Training and Learning Tempo-Spatial Characteristics for Audio Recognition with Convolutional and Recurrent Neural Networks",
      "authors": [
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Koebe",
        "Sandra Ottl",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "8",
      "title": "DeepSpectrumLite: A Power-Efficient Transfer Learning Framework for Embedded Speech and Audio Processing From Decentralized Data",
      "authors": [
        "Shahin Amiriparian",
        "Tobias H√ºbner",
        "Vincent Karas",
        "Maurice Gerczuk",
        "Sandra Ottl",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "Frontiers in Artificial Intelligence",
      "doi": "10.3389/frai.2022.856232"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Can deep generative audio be emotional? Towards an approach for personalised emotional audio generation",
      "authors": [
        "Alice Baird",
        "Shahin Amiriparian",
        "Bj√∂rn Schuller"
      ],
      "year": "2019",
      "venue": "2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "11",
      "title": "The duality of human existence: An essay on psychology and religion",
      "authors": [
        "David Bakan"
      ],
      "year": "1966",
      "venue": "The duality of human existence: An essay on psychology and religion"
    },
    {
      "citation_id": "12",
      "title": "A manual for the Bem sex role inventory",
      "authors": [
        "L Sandra",
        "Bem"
      ],
      "year": "1981",
      "venue": "California: Mind Garden"
    },
    {
      "citation_id": "13",
      "title": "Deep learning of audio and language features for humor prediction",
      "authors": [
        "Dario Bertero",
        "Pascale Fung"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16"
    },
    {
      "citation_id": "14",
      "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates",
      "authors": [
        "Bj√∂rn Schuller",
        "Anton Batliner",
        "Christian Bergler",
        "Cecilia Mascolo",
        "Jing Han",
        "Iulia Lefter",
        "Heysem Kaya",
        "Shahin Amiriparian",
        "Alice Baird",
        "Lukas Stappen",
        "Sandra Ottl",
        "Maurice Gerczuk",
        "Panaguiotis Tzirakis",
        "Chlo√´ Brown",
        "Jagmohan Chauhan",
        "Andreas Grammenos",
        "Apinan Hasthanasombat",
        "Dimitris Spathis",
        "Tong Xia",
        "Pietro Cicuta",
        "Leon Rothkrantz",
        "Joeri Zwerts",
        "Jelle Treep",
        "Casper Kaandorp"
      ],
      "year": "2021",
      "venue": "Proceedings INTERSPEECH 2021, 22nd Annual Conference of the International Speech Communication Association. ISCA, ISCA"
    },
    {
      "citation_id": "15",
      "title": "13 contributions of nonverbal cues to the accurate judgment of personality traits. The Oxford handbook of accurate personality judgment",
      "authors": [
        "Sarah Simon M Breil",
        "Steffen Osterholz",
        "Mitja D Nestler",
        "Back"
      ],
      "year": "2021",
      "venue": "13 contributions of nonverbal cues to the accurate judgment of personality traits. The Oxford handbook of accurate personality judgment"
    },
    {
      "citation_id": "16",
      "title": "Personality influences the neural responses to viewing facial expressions of emotion",
      "authors": [
        "J Andrew",
        "Michael Calder",
        "Luca Ewbank",
        "Passamonti"
      ],
      "year": "2011",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "17",
      "title": "Assessing humor at work: The humor climate questionnaire",
      "authors": [
        "Arnie Cann",
        "Amanda Watson",
        "Elisabeth Bridgewater"
      ],
      "year": "2014",
      "venue": "Humor"
    },
    {
      "citation_id": "18",
      "title": "ViTFER: facial emotion recognition with vision transformers",
      "authors": [
        "Aayushi Chaudhari",
        "Chintan Bhatt",
        "Achyut Krishna",
        "Pier Mazzeo"
      ],
      "year": "2022",
      "venue": "Applied System Innovation"
    },
    {
      "citation_id": "19",
      "title": "Integrating Cross-Modal Interactions via Latent Representation Shift for Multi-Modal Humor Detection",
      "authors": [
        "Chengxin Chen",
        "Pengyuan Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge",
      "doi": "10.1145/3551876.3554805"
    },
    {
      "citation_id": "20",
      "title": "The muse 2023 multimodal sentiment analysis challenge: Mimicked emotions, cross-cultural humour, and personalisation",
      "authors": [
        "Lukas Christ",
        "Shahin Amiriparian",
        "Alice Baird",
        "Alexander Kathan",
        "Niklas M√ºller",
        "Steffen Klug",
        "Chris Gagne",
        "Panagiotis Tzirakis",
        "Lukas Stappen",
        "Eva-Maria Me√üner"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "21",
      "title": "Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results",
      "authors": [
        "Lukas Christ",
        "Shahin Amiriparian",
        "Alexander Kathan",
        "Niklas M√ºller",
        "Andreas K√∂nig",
        "Bj√∂rn Schuller"
      ],
      "year": "2023",
      "venue": "Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results",
      "arxiv": "arXiv:2209.14272"
    },
    {
      "citation_id": "22",
      "title": "Deep lexical hypothesis: Identifying personality structure in natural language",
      "authors": [
        "Andrew Cutler",
        "David Condon"
      ],
      "year": "2022",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "23",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "24",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "25",
      "title": "Role congruity theory of prejudice toward female leaders",
      "authors": [
        "H Alice",
        "Steven Eagly",
        "Karau"
      ],
      "year": "2002",
      "venue": "Psychological review"
    },
    {
      "citation_id": "26",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "27",
      "title": "Beyond Biology -The impact of Perceptions CEO Social Gender on Investor Reactions During an IPO",
      "authors": [
        "Maria Simone",
        "Brooke Eulitz",
        "Gazdag"
      ],
      "year": "2021",
      "venue": "Academy of Management Proceedings",
      "doi": "10.5465/AMBPP.2021.12379abstract"
    },
    {
      "citation_id": "28",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Bj√∂rn Schuller",
        "Johan Sundberg",
        "Elisabeth Andr√©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition",
      "authors": [
        "Maurice Gerczuk",
        "Shahin Amiriparian",
        "Sandra Ottl",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Leader positive humor and organizational cynicism: LMX as a mediator. Leadership & Organization",
      "authors": [
        "Panagiotis Gkorezis",
        "Eugenia Petridou",
        "Panteleimon Xanthiakos"
      ],
      "year": "2014",
      "venue": "Development Journal"
    },
    {
      "citation_id": "32",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "33",
      "title": "Discovering Relevant Sub-spaces of BERT, Wav2Vec 2.0, ELECTRA and ViT Embeddings for Humor and Mimicked Emotion Recognition with Integrated Gradients",
      "authors": [
        "Tam√°s Gr√≥sz",
        "Anja Virkkunen",
        "Dejan Porjazovski",
        "Mikko Kurimo"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "34",
      "title": "FullStop: Multilingual Deep Models for Punctuation Prediction",
      "authors": [
        "Oliver Guhr",
        "Anne-Kathrin Schumann",
        "Frank Bahrmann",
        "Hans B√∂hme"
      ],
      "year": "2021",
      "venue": "Proceedings of the Swiss Text Analytics Conference 2021. CEUR Workshop Proceedings"
    },
    {
      "citation_id": "35",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amirali Bagher Zadeh",
        "Jianyuan Zhong",
        "Md Iftekhar Tanveer",
        "Louis-Philippe Morency",
        "Mohammed (ehsan) Hoque"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1211"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "",
      "arxiv": "arXiv:2310.06825"
    },
    {
      "citation_id": "38",
      "title": "Assessing the Big Five personality traits using reallife static facial images",
      "authors": [
        "Alexander Kachur",
        "Evgeny Osin",
        "Denis Davydov",
        "Konstantin Shutilov",
        "Alexey Novokshonov"
      ],
      "year": "2020",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "39",
      "title": "Humor in intercultural interaction: A source for misunderstanding or a common ground builder? A multimodal analysis",
      "authors": [
        "Anna Ladilova",
        "Ulrike Schr√∂der"
      ],
      "year": "2022",
      "venue": "Intercultural Pragmatics"
    },
    {
      "citation_id": "40",
      "title": "Hybrid Multimodal Feature Extraction, Mining and Fusion for Sentiment Analysis",
      "authors": [
        "Jia Li",
        "Ziyang Zhang",
        "Junjie Lang",
        "Yueqi Jiang",
        "Liuwei An",
        "Peng Zou",
        "Yangyang Xu",
        "Sheng Gao",
        "Jie Lin",
        "Chunxiao Fan",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge",
      "doi": "10.1145/3551876.3554809"
    },
    {
      "citation_id": "41",
      "title": "Temporal-aware Multimodal Feature Fusion for Sentiment Analysis",
      "authors": [
        "Qi Li",
        "Shulei Tang",
        "Feixiang Zhang",
        "Ruotong Wang",
        "Yangyang Xu",
        "Zhuoer Zhao",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "42",
      "title": "JTMA: Joint Multimodal Feature Fusion and Temporal Multi-head Attention for Humor Detection",
      "authors": [
        "Qi Li",
        "Yangyang Xu",
        "Zhuoer Zhao",
        "Shulei Tang",
        "Feixiang Zhang",
        "Ruotong Wang",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "43",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "Shuo Liu",
        "Adria Mallol-Ragolta",
        "Emilia Parada-Cabaleiro",
        "Kun Qian",
        "Xin Jing",
        "Alexander Kathan",
        "Bin Hu",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "Patterns",
      "doi": "10.1016/j.patter.2022.100616"
    },
    {
      "citation_id": "44",
      "title": "Individual differences in uses of humor and their relation to psychological well-being: Development of the Humor Styles Questionnaire",
      "authors": [
        "Rod Martin",
        "Patricia Puhlik-Doris",
        "Gwen Larsen",
        "Jeanette Gray",
        "Kelly Weir"
      ],
      "year": "2003",
      "venue": "Journal of research in personality"
    },
    {
      "citation_id": "45",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "authors": [
        "Michael Mcauliffe",
        "Michaela Socolof",
        "Sarah Mihuc",
        "Michael Wagner",
        "Morgan Sonderegger"
      ],
      "year": "2017",
      "venue": "International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "46",
      "title": "Diptesh Kanojia, and Pushpak Bhattacharyya. 2021",
      "authors": [
        "Anirudh Mittal",
        "Pranav Jeevan",
        "Prerak Gandhi"
      ],
      "venue": "So You Think You're Funny?\": Rating the Humour Quotient in Standup Comedy",
      "arxiv": "arXiv:arXivpreprintarXiv:2110.12765"
    },
    {
      "citation_id": "47",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Group-level Speech Emotion Recognition Utilising Deep Spectrum Features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th ICMI 2020 EmotiW -Emotion Recognition In The Wild Challenge"
    },
    {
      "citation_id": "49",
      "title": "MuSe-Personalization 2023: Feature Engineering, Hyperparameter Optimization, and Transformer-Encoder Re-discovery",
      "authors": [
        "Ho-Min",
        "Ganghyun Park",
        "Arnout Kim",
        "Wesley Van Messem",
        "Neve De"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "50",
      "title": "Towards Multimodal Prediction of Timecontinuous Emotion using Pose Feature Engineering and a Transformer Encoder",
      "authors": [
        "Ilho Ho-Min Park",
        "Ajit Yun",
        "Ankit Kumar",
        "Bong Singh",
        "Dhananjay Choi",
        "Wesley Singh",
        "Neve De"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "51",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021. ISCA, ISCA",
      "doi": "10.21437/Interspeech.2021-703"
    },
    {
      "citation_id": "52",
      "title": "How Multilingual is Multilingual BERT?",
      "authors": [
        "Telmo Pires",
        "Eva Schlinger",
        "Dan Garrette"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1493"
    },
    {
      "citation_id": "53",
      "title": "Analysis of personality traits using natural language processing and deep learning",
      "authors": [
        "Tejas Pradhan",
        "Rashi Bhansali",
        "Dimple Chandnani",
        "Aditya Pangaonkar"
      ],
      "year": "2020",
      "venue": "2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)"
    },
    {
      "citation_id": "54",
      "title": "Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection",
      "authors": [
        "Shraman Pramanick",
        "Aniket Roy",
        "M Vishal",
        "Patel"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "55",
      "title": "Is smiling during humor so obvious? a cross-cultural comparison of smiling behavior in humorous sequences in american english and french interactions",
      "authors": [
        "B√©atrice Priego-Valverde",
        "Brigitte Bigi",
        "Salvatore Attardo",
        "Lucy Pickering",
        "Elisa Gironzetti"
      ],
      "year": "2018",
      "venue": "Intercultural Pragmatics"
    },
    {
      "citation_id": "56",
      "title": "PySBD: Pragmatic Sentence Boundary Disambiguation",
      "authors": [
        "Nipun Sadvilkar",
        "Mark Neumann"
      ],
      "year": "2020",
      "venue": "Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)"
    },
    {
      "citation_id": "57",
      "title": "FaceNet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2015.7298682"
    },
    {
      "citation_id": "58",
      "title": "LightFace: A Hybrid Deep Face Recognition Framework",
      "authors": [
        "Sefik Ilkin",
        "Alper Ozpinar"
      ],
      "year": "2020",
      "venue": "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)",
      "doi": "10.1109/ASYU50717.2020.9259802"
    },
    {
      "citation_id": "59",
      "title": "A conversational agent framework with multi-modal personality expression",
      "authors": [
        "Sinan Sonlu",
        "Uƒüur G√ºd√ºkbay",
        "Funda Durupinar"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "60",
      "title": "Computationally recognizing wordplay in jokes",
      "authors": [
        "M Julia",
        "Lawrence Taylor",
        "Mazlack"
      ],
      "year": "2004",
      "venue": "Proceedings of the Annual Meeting of the Cognitive Science Society"
    },
    {
      "citation_id": "61",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timoth√©e Lacroix",
        "Baptiste Rozi√®re",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "2023",
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "62",
      "title": "Fusion of Acoustic and Linguistic Information using Supervised Autoencoder for Improved Emotion Recognition",
      "authors": [
        "Bogdan Vlasenko",
        "Ravishankar Prasad",
        "Mathew Magimai",
        ". Doss"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "63",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3263585"
    },
    {
      "citation_id": "64",
      "title": "MUMOR: A Multimodal Dataset for Humor Detection in Conversations",
      "authors": [
        "Jiaming Wu",
        "Hongfei Lin",
        "Liang Yang",
        "Bo Xu"
      ],
      "year": "2021",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "65",
      "title": "Multimodal Cross-Lingual Features and Weight Fusion for Cross-Cultural Humor Detection",
      "authors": [
        "Heng Xie",
        "Jizhou Cui",
        "Yuhang Cao",
        "Junjie Chen",
        "Jianhua Tao",
        "Cunhang Fan",
        "Xuefei Liu",
        "Zhengqi Wen",
        "Heng Lu",
        "Yuguang Yang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "66",
      "title": "Hybrid Multimodal Fusion for Humor Detection",
      "authors": [
        "Haojie Xu",
        "Weifeng Liu",
        "Jiangwei Liu",
        "Mingzheng Li",
        "Yu Feng",
        "Yasi Peng",
        "Yunwei Shi",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge",
      "doi": "10.1145/3551876.3554802"
    },
    {
      "citation_id": "67",
      "title": "Humor Detection System for MuSE 2023: Contextual Modeling, Pesudo Labelling, and Post-smoothing",
      "authors": [
        "Mingyu Xu",
        "Shun Chen",
        "Zheng Lian",
        "Bin Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "68",
      "title": "Humor recognition and humor anchor extraction",
      "authors": [
        "Diyi Yang",
        "Alon Lavie",
        "Chris Dyer",
        "Eduard Hovy"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing. Association for Computational Linguistics"
    },
    {
      "citation_id": "69",
      "title": "Exploring the Power of Cross-Contextual Large Language Model in Mimic Emotion Prediction",
      "authors": [
        "Guofeng Yi",
        "Yuguang Yang",
        "Yu Pan",
        "Yuhang Cao",
        "Jixun Yao",
        "Xiang Lv",
        "Cunhang Fan",
        "Zhao Lv",
        "Jianhua Tao",
        "Shan Liang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "70",
      "title": "MMT-GD: Multi-Modal Transformer with Graph Distillation for Cross-Cultural Humor Detection",
      "authors": [
        "Jun Yu",
        "Wangyuan Zhu",
        "Jichao Zhu",
        "Xiaxin Shen",
        "Jianqing Sun",
        "Jiaen Liang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "71",
      "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "72",
      "title": "A comprehensive survey on automatic facial action unit analysis",
      "authors": [
        "Ruicong Zhi",
        "Mengyi Liu",
        "Dezheng Zhang"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    }
  ]
}