{
  "paper_id": "2407.04966v1",
  "title": "A Layer-Anchoring Strategy For Enhancing Cross-Lingual Speech Emotion Recognition",
  "published": "2024-07-06T05:56:55Z",
  "authors": [
    "Shreya G. Upadhyay",
    "Carlos Busso",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech emotion recognition",
    "large pretrained models",
    "cross-lingual"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-lingual speech emotion recognition (SER) is important for a wide range of everyday applications. While recent SER research relies heavily on large pretrained models for emotion training, existing studies often concentrate solely on the final transformer layer of these models. However, given the taskspecific nature and hierarchical architecture of these models, each transformer layer encapsulates different levels of information. Leveraging this hierarchical structure, our study focuses on the information embedded across different layers. Through an examination of layer feature similarity across different languages, we propose a novel strategy called a layer-anchoring mechanism to facilitate emotion transfer in cross-lingual SER tasks. Our approach is evaluated using two distinct language affective corpora (MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on the BIIC-podcast corpus. The analysis uncovers interesting insights into the behavior of popular pretrained models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, large pretrained models like Wav2Vec 2.0  [1] , WavLM  [2] , Whisper  [3] , and Hubert  [4]  have gained popularity, offering versatile capabilities across diverse applications. Trained on extensive datasets, these models serve as potent resources for tasks beyond their original domains. A notable trend involves fine-tuning these pretrained models for specific tasks such as SER  [5] [6] [7] [8] , phonetics  [9, 10] , speaker or language change detection  [11] , and speaker identification  [12, 13] . These models also serve as feature extractors, providing a rich source of abstract representations useful across different tasks.\n\nThese transformer-based models exhibit a hierarchical architecture and within this hierarchy, diverse levels of information are embedded in the transformer layers  [14] . The layer information varies based on the task specificity. For example, in models trained for automatic speech recognition (ASR), initial layers capture fundamental acoustic details, while later layers encapsulate more complex lexical information. When employing these layer embeddings for different tasks, the ability to selectively choose or assign weights to layers that are more relevant can enhance the learning. Numerous studies not only use the final transformer layer but also utilize the information within other layers of the pretrained models  [10, 15, 16] . They aim to utilize this valuable information in a weighted or averaged manner, aligning features more effectively with specific task requirements. For instance, in speaker verification tasks  [15] , various studies analyze different feature extraction methodologies developed upon pretrained models. Additionally, they explore regularization techniques and learning rate scheduling to stabilize the fine-tuning process, resulting in notable performance enhancements. Another research endeavor  [16]  emphasizes that speech representations derived from specific neural models like Transformers exhibit closer alignment with human perception, particularly regarding phonetic transcriptions. Moreover, English et al.  [10]  highlights the Transformer architectures' capacity to effectively capture significant phonetic nuances.\n\nSpeech representations derived from specific neural models, such as different transformer layers in pretrained models exhibit enhanced efficiency in recognizing phoneme  [17] . These pretrained model's transformer architectures are adept at capturing substantial levels of phonetic information across various layers  [10, 17] . In the domain of cross-lingual SER, where phonetic alignment between diverse language corpora is advantageous  [5] , certain layers within large pretrained models beyond the final layer may hold greater importance  [14] . These layers which directly encode acoustic cues and phonetic characteristics can form the fundamentals of tasks related to emotion recognition. Recognizing the potential efficiency gains in crosslingual SER models, this paper introduces a novel approach known as Layer-anchoring. This method strategically aligns layers based on their similarity across the two language corpora. By prioritizing and aligning layers that exhibit greater commonality between the features of both corpora, the model can enhance its performance. This novel strategy acknowledges the task-specific nature of SER and leverages the hierarchical structure of pretrained models to optimize layer representation utilization in a nuanced and contextually relevant manner.\n\nGiven that WavLM  [2]  has currently secured the top position on the SUPERB benchmark  [18]  (retrieved on  March 10, 2024) , our experimentation will focus on utilizing WavLM and analyzing its performance with our proposed layer-anchoring algorithm. To address cross-lingual scenarios, we also use the multilingual pretrained model, Whisper  [3]  a widely adopted in recent studies  [19, 20] , to compare its performance and insights with those of the monolingual model (WavLM). This study employs two distinct language corpora: the MSP-Podcast (American English)  [21]  and the BIIC-Podcast (Taiwanese Mandarin)  [22]  corpora. First, we analyze layer similarities within the pretrained model's encoded features across both corpora. This analysis reveals layers exhibiting better commonality between the corpora than the final layer. Building upon this insight, we implement the layer anchoring mechanism (LAM) to develop a cross-lingual SER model. Our proposed model, referred to as a layer-anchoring mechanism with a group of layers (LAM-GL), outperforms alternative approaches achieving 60.21% unweighted average recall (UAR) with WavLM features and 59.65% with Whisper encoded features over the BIICpodcast corpus.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Layer Similarity Analysis",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Naturalistic Corpora",
      "text": "The MSP-Podcast (MSP-P)  [21]  corpus contains 166 hours of emotional American English speech (v1.10), sourced from audio-sharing websites. This resource is valuable for SER research due to its extensive size and emotionally balanced dialogues from various individuals. It includes annotations for primary emotions, secondary emotions, and emotional attributes.\n\nIn this study, this corpus is used primarily as the source corpus and focuses only on four primary emotion categories (Neutral, Happiness, Anger, and Sadness), comprising a total of 49,018 samples with predefined train-validation-test splits. The phonetic information is already included in the MSP-P corpus.\n\nThe BIIC-Podcast (BIIC-P)  [22]  corpus is a SER database (v1.0) in Taiwanese Mandarin. It contains 157 hours of speech samples from podcasts and follows a data collection methodology similar to the MSP-P corpus. The annotations cover primary and secondary emotional categories, as well as three emotional attributes. Here, the BIIC-P corpus is used as the target corpus. For this study, we employ 22,799 samples focusing on four primary emotion categories with the predefined trainvalidation-test splits given by the database provider. For the BIIC-P corpus phonetic knowledge, we employ the same phone aligner as shown in our previous work  [5] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Layer Similarities",
      "text": "Previous research using large pretrained models on different tasks reveals that each layer contributes different levels of information during task learning  [10, 17] . Building on these findings, we aim to examine the degree of layer-similarity encoded within these model's layer representations across two distinct language corpora (the MSP-P and the BIIC-P). To explore this comparison, we employ two off-the-shelf models: WavLM and Whisper. We analyze the layer-similarity at both utterance and phonetic levels. We include phonetic-level analysis because our prior study  [5]  has shown that cross-lingual contexts may reveal shared phonetic commonalities. Figure  1  visually illustrates the cosine similarity between layer representations of the BIIC-P and MSP-P corpora across four primary emotions, including both the utterance-level and phonetic-level. Here, we only use training samples, excluding the test samples.\n\nUtterance-Level Layer Similarity: To assess the presence of similarities across layers in the two corpora for different emotions over the whole utterance, we extract all-layer feature representations from the considered pretrained models (WavLM and Whisper) for both the MSP-P and the BIIC-P corpora using entire utterances. Subsequently, we compute the layersimilarity using the cosine similarity metric between the layer representations of the MSP-P and the BIIC-P corpora across each emotional category as presented in Figure  1 . Upon examining plots depicted in Figure  1 , we observe differing layer similarity behaviors between the WavLM and Whisper models' feature embeddings. In WavLM, later layers, such as layer 11 show higher similarity, while the Whisper model's initial layers (1 to 5) exhibit greater similarity. This trend is constant across different emotions. WavLM shares a similar phenomenon with wav2vec2.0  [14] , suggesting that high-level features may be more general and potentially lead to higher similarity. However, this assumption contrasts with the Whisper model, possibly due to distinct training methodologies. These findings prompt further exploration into phonetic-level layer similarity to determine any distinct observations compared to utterance-level analysis.\n\nPhonetic-Level Layer Similarity: For phonetic-level layer similarity analysis, our specific focus lies on vowels which according to literature possess a higher capacity to convey emotion and are prevalent across different languages. We consider six common vowels across the MSP-P and BIIC-P corpora:\n\n[A/a, E, @, i, O, u]. From Figure  1 , a clear pattern emerges in the similarity of layer features at the vowel level between the WavLM and Whisper models, aligning with the observations made at the utterance-level. Specifically, the WavLM model displays a trend of increasing layer similarity in its later layers for all vowel phones, contrasting starkly with the Whisper model, where greater similarity is observed in the initial layers.\n\nInvestigating inter-vowel segment similarities within [A/a, E, @, i, O, u], Figure  1  reveals variations across layers for different emotions. Nonetheless, an overall high degree of similarity is observed between the corpora at the corresponding layers.\n\nThe above analyses highlight that due to task specificity and the hierarchical nature of models, in self-supervised learning models like WavLM, later layers encapsulate more abstract patterns and language-specific phonetic nuances as the model learns to predict future speech tokens. Conversely, Whisper be- [2, 6, 9] Random-Layers (RL2)  [1, 5, 12]  [1, 5, 12] Random-Layers (RL3)  [3, 7, 11]  [  3, 7, 11]  ing weakly supervised, the early layers may capture more basic acoustic features as they primarily rely on the input data with explicit labels so we observe greater layer-similarity towards the initial layers. This observation indicates that similar layer selection relies not only on task specifics but also on the model's training methodology. The dissimilarity of the final layer could stem from its alignment with the pre-training objective, which prioritizes tasks other than SER. Thus, while effective for its original purposes, it might not optimize cross-language speech emotion recognition (CL-SER).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Unified Layer Selection",
      "text": "As per our hypothesis, aligning layer features exhibiting high layer-similarities across different language corpora and imposing constraints on those layers can enhance the effectiveness of emotion transfer in CL-SER tasks. To anchor on more similar layers, we select specific sets of layers based on the findings of our previous analysis in Section 2.2. Table  1  outlines the selected layers under different settings. Drawing from our previous experience, we have observed that a group of anchors tends to outperform individual ones. Therefore, the table includes the group-layers (GL), representing clusters of highly similar layers. Additionally, we identify the best-layer (BL) and worstlayers (WL), along with three sets of random-layers (RL1, RL2, RL3). Except for BL, we select the top three layers for all cases concerning training CL-SER in the subsequent section.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Layer Anchored Cross-Lingual Ser",
      "text": "In all our experiments, we consider the MSP-P (source) and BIIC-P (target) corpora as benchmarks to test our idea. The WavLM and Whisper embedding feature vectors are used as the pretrained layer representations. For the CL-SER architecture, we use the transformer with 4-fully connected layer architecture, Following the same model presented in our previous work  [5]  with an attention-weighted layer feature pooling concept as our backbone SER architecture. We employ the Adam optimizer with a learning rate of 0.0001 and a decay factor of 0.001, and back-propagation is done with the cross-entropy loss function. The network undergoes a maximum of 70 epochs and a batch size of 64 with early stopping. To evaluate model performances, we use the UAR metric. Since both utterance-level and phonetic-level similarities yield similar insights from Section 2, we integrate LAM over the entire utterance. Our investigations detailed in Section 2 offer initial insights suggesting that specific layers may exhibit more similarity and can enhance emotion modulation across both corpora. Motivated by these findings, we devise a layer anchoring mechanism aim at incorporating the layer-alignment constraint in the CL-SER modeling (Figure  2 ). Our proposed unsupervised CL-SER comprises two branches: (1) a conventional emotion classification branch tasked with classifying emotions, and (2) a layer anchoring mechanism (LAM) branch that identifies the layers in the transformer that increase the similarities across languages at the phonetic level. Equation 1 outlines the LAM loss.\n\nWhere\n\ntar denote the feature representations of layer i from the source and target corpora, respectively. Cov(•) represents the covariance matrix and ∥.∥ F denotes the Frobenius norm. Let n be the number of predefined layers for anchoring. The LossCORAL is the Correlation Alignment Loss (CORAL)  [23]  between the source and target representations for these layers.\n\nMore specifically, in the LAM scenario where the CORAL loss is used, the source and target features refers to the similar layer representations from the MSP-P corpus and the BIIC-P corpus, respectively. This is to align the distributions of features between these two layer-similar representations by minimizing the difference in their second-order statistics. The mathematical formulation for the attentional weighted average estimation is defined by Equation  2 ,\n\nwhere Lavg denotes the attentional weighted average of the feature representations from all layers, and αi denotes the attention weight assigned to the layer i. Here 12 i αi = 1 and αi ≥ 0 for all i. The weight αi can be computed using Equation  3 .\n\nWhere ai represents the attention score for layer i. The attention scores are the learnable parameter.\n\nThe overall loss for the first branch is the sum of the crossentropy loss for the classic SER task and the CORAL loss for the layer anchoring mechanism. The complete loss is calculated using Equation  4 ,\n\nwhere LER and LCORAL are the losses for the emotion classification and domain adaptation tasks. γ is the regularization parameter which is set to a constant value (γ = 0.5).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Results And Analyses",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance Comparison",
      "text": "Table  2  shows the performance table, which includes results of our proposed idea LAM-GL, the baseline models, and the ablations results with WavLM and Whisper model's encoded layer representations. We assess three baseline methods: ensemble learning  [24] , which combines predictions from diverse models to enhance recognition accuracy in cross-lingual scenarios (Ensemble); few-shot learning  [25] , adapting models to target domains with limited labeled data (Few-shot); and our previously proposed Phonetic-constraint based anchoring (PA) method  [5] , used for learning in a common phonetic space for SER. Compared with the baselines in Table  2  for the MSP-P→BIIC-P task, our layer anchoring approach (LAM-GL) yields superior performance. Specifically, compared to models using only the last layer (the 12th layer), LAM-GL achieves a UAR of 60.21%, surpassing Ensemble  [24]  at 52.18%, Few-shot  [25]  at 53.62%, and PC  [5]  at 58.14% with WavLM features. This enhanced performance is also evident with Whisper features. Additionally, drawing on prior work employing layer information for tasks like speaker identification and phone recognition  [15] , we integrate these methods with PC, which outperforms Ensemble and Few-shot, denoted as PC-Avg  [5, 15]  and PC-Atn  [5, 15] . Comparing these models with LAM-GL reveals further performance improvements, with WavLM features achieving 2.15% and 1.38% UAR for PC-Avg and PC-Atn models, respectively. The same patterns are observed with Whisper features, suggesting that our LAM-GL model, which aligns more similar layers across various corpus features, provides improved utility for CL-SER.\n\nTo validate our LAM-GA method, we extended our investigation beyond the selected layers, exploring whether aligning any random layer of the two corpora or all layers is acceptable or if precise selection is necessary. We train LAM-GA models with diverse configurations: utilizing all layers (LAM-AL), the best layer (LAM-BL), the worst layer (LAM-WL), and random selections (LAM-RL1, LAM-RL2, LAM-RL3). Table  1  presents the selected layers for these analyses. The MSP-P→BIIC-P task results from Table  2  indicate that anchoring all layer models (LAM-AL) does not enhance performance over LAM-GL, yielding 58.54% for WavLM and 57.97% for Whisper UAR. Similarly, using the best layer (LAM-BL), which achieves the UAR of 59.16% for WavLM and 58.11% for Whisper, is not better than the LAM-GL strategy. Furthermore, the performance of the worst layer (LAM-WL) suggests that aligning dissimilar layers can adversely affect model performance, with WavLM features scoring 58.01% and Whisper features 57.72%. Despite generating three random sets for both WavLM and Whisper model features, results obtained for LAM-RL1, LAM-RL2, and LAM-RL3 did not outperform our LAM-GA As a validation of our concept, we incorporate cross-lingual SER assessments utilizing the BIIC-P corpus as the source and the MSP-P corpus as the target. The results are presented in Table 2. In the BIIC-P→MSP-P task, our proposed model LAM-GL demonstrates superior performance compared to other models listed in Table  2 , achieving 56.68% with WavLM features and 56.37% with Whisper features. The overall analysis of Table 2 for the BIIC-P→MSP-P task confirms a similar trend to the one observed in the MSP-P→BIIC-P task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cl-Ser With Different Layer Selection Strategy",
      "text": "In this investigation, we explore the performance of LAM-GL compared to LAM across various phoneme groups (vowelbased (Vowl), consonant-based (Cons)), as well as an utterancebased approach (Uttr) over specific emotion detection task. We segment utterances based on different phoneme groups (Vowl, Cons), selecting layers to train our LAM-GA model. Results in Table  3  for the MSP-P→BIIC-P task across four primary emotions reveal that while the selected layers remain relatively consistent across the Uttr, Vowl, and Cons strategies, notable performance differences emerge across different emotions. Particularly, the Vowl approach demonstrates superior performance for emotions such as Happiness and Anger, achieving 74.21% UAR and 75.55% UAR, respectively, with WavLM features. A similar trend is observed with the Whisper model. This significant finding suggests that vowels exhibit a higher level of commonality over the two different language corpora features, potentially facilitating more efficient emotion transfer compared to considering the entire utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This study introduces a novel approach that aims at reducing layer feature disparities between different language corpora through a layer anchoring strategy. By capitalizing on pretrained models and aligning similar layer features from the source language to those of the target language, we illustrate the efficacy of our method in harmonizing phonetic characteristics while mitigating discrepancies. Our experimentation and evaluation reveal that our layer-anchoring strategy (LAM-GA) achieves the best UAR of 60.21% by effectively facilitating emotion transfer in cross-lingual SER. Additionally, we uncover intriguing insights indicating that the selection of layers is not uniform across all pretrained models but varies depending on the task and the model's training methodology. Our future work will delve deeper into the observed differences in specific emotion recognition using the LAM-GA method under various strategies, even when the layer disparities are minimal. Also, we will explore enhanced algorithms to accommodate multiple languages in the SER training.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Phonetic similarities across feature layer representations in MSP-P and BIIC-P corpora for WavLM and Whisper models,",
      "page": 2
    },
    {
      "caption": "Figure 1: visually illustrates the",
      "page": 2
    },
    {
      "caption": "Figure 1: . Upon ex-",
      "page": 2
    },
    {
      "caption": "Figure 1: , we observe differing layer",
      "page": 2
    },
    {
      "caption": "Figure 1: , a clear pattern emerges in",
      "page": 2
    },
    {
      "caption": "Figure 1: reveals variations across layers for different",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Our proposed unsupervised CL-SER",
      "page": 3
    },
    {
      "caption": "Figure 2: Proposed contrastive learning approach using layer",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The proposed model performance (in UAR) for each",
      "data": [
        {
          "Uttr": "Vowl",
          "WavLM\nWhisper": "WavLM\nWhisper",
          "[8,9,11]\n[1,2,3]": "[8,9,11]\n[1,3,5]",
          "75.13\n72.88\n74.33\n69.57\n75.70\n72.63\n73.80\n69.81": "74.21\n75.55\n75.98\n69.92\n75.02\n74.19\n74.83\n70.46"
        },
        {
          "Uttr": "Cons",
          "WavLM\nWhisper": "WavLM\nWhisper",
          "[8,9,11]\n[1,2,3]": "[9,10,11]\n[3,5,6]",
          "75.13\n72.88\n74.33\n69.57\n75.70\n72.63\n73.80\n69.81": "74.19\n73.73\n74.92\n68.63\n73.34\n73.97\n73.30\n69.95"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: The proposed model performance (in UAR) for each",
      "data": [
        {
          "TopLayer": "w/Layer",
          "CC\nEnsemble [24]\nFew-shot [25]\nPC [5]": "PC-Avg [5, 15]\nPC-Atn [5, 15]",
          "52.01**\n51.87 **\n52.18**\n52.03**\n53.62**\n52.74**\n58.14**\n57.83*": "58.06**\n58.01**\n58.83*\n58.92*",
          "48.39 **\n49.01 **\n51.75 **\n51.98 **\n50.59 **\n51.75 **\n55.35 *\n54.93 *": "55.24 *\n54.32 *\n55.64 *\n56.10 *"
        },
        {
          "TopLayer": "",
          "CC\nEnsemble [24]\nFew-shot [25]\nPC [5]": "LAM-GL",
          "52.01**\n51.87 **\n52.18**\n52.03**\n53.62**\n52.74**\n58.14**\n57.83*": "60.21\n59.65",
          "48.39 **\n49.01 **\n51.75 **\n51.98 **\n50.59 **\n51.75 **\n55.35 *\n54.93 *": "56.68\n56.37"
        },
        {
          "TopLayer": "",
          "CC\nEnsemble [24]\nFew-shot [25]\nPC [5]": "LAM-AL\nLAM-BL\nLAM-WL\nLAM-RL1\nLAM-RL2\nLAM-RL3",
          "52.01**\n51.87 **\n52.18**\n52.03**\n53.62**\n52.74**\n58.14**\n57.83*": "58.54\n57.97\n59.16\n58.11\n58.01\n57.72\n58.94\n56.24\n58.55\n57.39\n57.23\n57.84",
          "48.39 **\n49.01 **\n51.75 **\n51.98 **\n50.59 **\n51.75 **\n55.35 *\n54.93 *": "55.39\n54.91\n55.75\n54.21\n54.64\n53.77\n54.93\n54.29\n53.85\n53.38\n54.20\n54.43"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "U Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17"
    },
    {
      "citation_id": "2",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Phonetic anchor-based transfer learning to facilitate unsupervised crosslingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "B.-H Su",
        "W.-C Lin",
        "W.-S Chien",
        "Y.-T Wu",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "8",
      "title": "Peft-ser: On the use of parameter efficient transfer learning approaches for speech emotion recognition using pre-trained speech models",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "9",
      "title": "Wav2vec behind the scenes: How end2end models learn phonetics",
      "authors": [
        "T Tom Dieck",
        "P.-A Pérez-Toro",
        "T Arias-Vergara",
        "E Nöth",
        "P Klumpp"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "10",
      "title": "Domaininformed probing of wav2vec 2.0 embeddings for phonetic features",
      "authors": [
        "P English",
        "J Kelleher",
        "J Carson-Berndsen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics"
    },
    {
      "citation_id": "11",
      "title": "Speaker and language change detection using wav2vec2 and whisper",
      "authors": [
        "T Berns",
        "N Vaessen",
        "D Van Leeuwen"
      ],
      "year": "2023",
      "venue": "Speaker and language change detection using wav2vec2 and whisper",
      "arxiv": "arXiv:2302.09381"
    },
    {
      "citation_id": "12",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "13",
      "title": "Exploring wav2vec 2.0 on speaker verification and language identification",
      "authors": [
        "Z Fan",
        "M Li",
        "S Zhou",
        "B Xu"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 on speaker verification and language identification"
    },
    {
      "citation_id": "14",
      "title": "Exploration of a selfsupervised speech model: A study on emotional corpora",
      "authors": [
        "Y Li",
        "Y Mohamied",
        "P Bell",
        "C Lai"
      ],
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "15",
      "title": "An attention-based backend allowing efficient finetuning of transformer models for speaker verification",
      "authors": [
        "J Peng",
        "O Plchot",
        "T Stafylakis",
        "L Mošner",
        "L Burget",
        "J Černockỳ"
      ],
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Practices and pitfalls in inferring neural representations",
      "authors": [
        "V Popov",
        "M Ostarek",
        "C Tenison"
      ],
      "year": "2018",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "17",
      "title": "Neural representations for modeling variation in speech",
      "authors": [
        "M Bartelds",
        "W Vries",
        "F Sanal",
        "C Richter",
        "M Liberman",
        "M Wieling"
      ],
      "year": "2022",
      "venue": "Journal of Phonetics"
    },
    {
      "citation_id": "18",
      "title": "Ml-superb: Multilingual speech universal performance benchmark",
      "authors": [
        "J Shi",
        "D Berrebbi",
        "W Chen",
        "H Chung",
        "E Hu",
        "W Huang",
        "X Chang",
        "S Li",
        "A Mohamed",
        "H Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "19",
      "title": "Novel speech recognition systems applied to forensics within child exploitation: Wav2vec2. 0 vs. whisper",
      "authors": [
        "J Vásquez-Correa",
        "A Álvarez Muniain"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "20",
      "title": "A study on incorporating whisper for robust speech assessment",
      "authors": [
        "R Zezario",
        "Y.-W Chen",
        "S.-W Fu",
        "Y Tsao",
        "H.-M Wang",
        "C.-S Fuh"
      ],
      "year": "2023",
      "venue": "A study on incorporating whisper for robust speech assessment",
      "arxiv": "arXiv:2309.12766"
    },
    {
      "citation_id": "21",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "S Upadhyay",
        "W.-S Chien",
        "B.-H Su",
        "L Goncalves",
        "Y.-T Wu",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "2023 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "23",
      "title": "Correlation alignment for unsupervised domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2017",
      "venue": "Correlation alignment for unsupervised domain adaptation"
    },
    {
      "citation_id": "24",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "25",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    }
  ]
}