{
  "paper_id": "2508.04230v1",
  "title": "Towards Interpretable Emotion Recognition: Identifying Key Features With Machine Learning",
  "published": "2025-08-06T09:09:25Z",
  "authors": [
    "Yacouba Kaloga",
    "Ina Kodrasi"
  ],
  "keywords": [
    "Important Features",
    "Emotion Recognition",
    "Interpretability"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Unsupervised methods, such as wav2vec2 and HuBERT, have achieved state-of-the-art performance in audio tasks, leading to a shift away from research on interpretable features. However, the lack of interpretability in these methods limits their applicability in critical domains like medicine, where understanding feature relevance is crucial. To better understand the features of unsupervised models, it remains critical to identify the interpretable features relevant to a given task. In this work, we focus on emotion recognition and use machine learning algorithms to identify and generalize the most important interpretable features for this task. While previous studies have explored feature relevance in emotion recognition, they are often constrained by narrow contexts and present inconsistent findings. Our approach aims to overcome these limitations, providing a broader and more robust framework for identifying the most important interpretable features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Effective communication relies on the accurate mutual perception of emotions between interlocutors. A significant portion of information exchanged in a conversation is conveyed through emotional cues-visually, as well as through voice, intonation, and other auditory features. When emotional perception is impaired, as in certain pathologies  [1] , communication becomes particularly challenging, underscoring the importance of preserving these cues in auditory interactions. At the same time, a growing share of our audio interactions now occurs through electronic systems, including remote meetings, phone calls, and hearing aids. While these systems process sound to optimize factors such as signal quality, signal intelligibility, energy efficiency, or latency, the preservation of emotional cues is rarely considered. Evaluating the impact of various processing schemes on emotional cues through human subjective testing is impractical, due to high costs and limited scalability. Automated systems that use machine learning models for speech emotion recognition (SER) across different sound processing schemes can provide a viable alternative. Previous studies have shown similarities between human and machine emotion perception, with machines typically relying on handcrafted acoustic features to identify key emotional cues  [2, 3] . The impact of various processing schemes (or perturbations) on emotion perception can then be quantified either by analyzing how the handcrafted acoustic features are affected  [4, 5]  or by measuring performance degradation in models that depend on these features  [4, 6] . Such approaches, however, typically rely on a narrow range of acoustic features and limited number of datasets, lacking generalizability and highlighting the need for a broader set of emotionally relevant acoustic features that can be applied across various settings.\n\nOur work aims to identify relevant acoustic features for SER using various automatic classifiers on various datasets. More specifically, we employ six classification models, a significantly higher number compared to what is typically seen in the SER literature. Additionally, we arXiv:2508.04230v1 [eess.AS] 6 Aug 2025 use six different datasets in four different languages, with models trained multiple times using different splits of the same dataset. Utilizing multiple models (trained multiple times on different splits of the data) and datasets provides several advantages. First, extracting the relevant acoustic features across multiple models (resp. datasets) increases robustness. If a particular feature is consistently important across different models (resp. datasets), it indicates its reliability and reduces the influence of individual model (resp. dataset) characteristics. Additionally, using multiple models helps mitigate the influence of underperforming models whose feature importances are unreliable due to poor performance, as well as models that overperform by relying on features highly specific to a particular dataset. Secondly, testing feature importance algorithms on multiple models and different datasets enables us to gauge the generalizability of feature rankings across different learning paradigms and datasets. Finally, running the analysis multiple times can help mitigate the challenges posed by correlated features. Specifically, our contributions are as follows:\n\n• We derive the most relevant acoustic features for SER across different classification models and datasets.\n\n• We propose an approach to combine the relevance of features from each individual model and dataset, resulting in a single, robust, and generalizable list of key acoustic features.\n\n• We experimentally demonstrate the advantages of our approach for establishing key acoustic features, showing that it outperforms using a single classifier or a single dataset in terms of robustness and generalizability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Research on speech emotion perception has primarily explored key acoustic features and their impact on human perception. In this section, we review these studies and their limitations. Further, we examine how machine learning can offer more general findings by comparing its performance to human perception. Finally, we discuss why current studies leveraging this connection remain limited.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Acoustic Cues And Human Emotion Perception",
      "text": "Numerous studies have established a strong connection between specific acoustic features and human perception of emotions. Pitch variations play a crucial role, with studies showing that higher mean pitch values are associated with high-arousal emotions such as anger, fear, and excitement, while lower values are linked to lowarousal emotions like boredom  [7] [8] [9] [10] [11] . Temporal aspects, such as speech rate and rhythm, and spectral measures, including formants and spectral flux, are also critical in conveying emotions  [2, 12] . Additional acoustic cues, such as prosody contour, intensity, and energyrelated features, further enrich the emotional expressiveness of speech  [13, 14] . Importantly, these acoustic features do not function in isolation but interact in complex ways to shape emotional perception. Despite these insights, existing studies face limitations. Most experiments rely on small sample sizes due to the high cost and time required for participant-based evaluations. This leads to findings that are highly context-dependent and difficult to generalize  [9] . Additionally, some studies report contradictory results, likely due to differences in methodology, dataset composition, and speaker variability (for example,  [15]  has inconsistent findings on sadness with respect to  [16] [17] [18] [19] ). Given these constraints, machine learning offers a promising avenue for studying emotional perception, as it allows for the analysis of large-scale datasets and the identification of underlying patterns that may be difficult to discern from human-limited studies.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Machine-Based Ser And Human Emotion Perception",
      "text": "Recent studies have directly compared machine learning models to human emotion perception. For instance,  [4]  evaluated classification models (i.e., multilayer perceptron (MLP) and support vector machine (SVM)) against human perception under both clean and noisy conditions. Their study introduced emotion distractors, i.e., emotions present in the test set but unseen during training, to ensure that models were performing true recognition rather than simple discrimination. Their results showed striking similarities between human and machine perception, including comparable overall SER performance, similar relative accuracy across different emotions, consistent performance degradation in noisy environments, and similar confusion matrices, particularly for well-defined emotions like anger and sadness. These findings align with earlier literature  [3, 6] , which also reported strong similarities between human and machine SER. Additionally, regression models predicting valence/arousal positioning have shown that human-based emotion placement can effectively pre-dict machine performance on the same task  [2] . This suggests that both humans and machine learning models rely on similar acoustic cues when interpreting emotions  [3] [4] [5] 20] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Acoustic Cues And Ser",
      "text": "To the best of our knowledge, a limited number of studies have quantitatively explored the most important acoustic cues for SER in machine learning-based models. Such work is carried out using mainly regression and fixed effect regression  [2, 5, 15] . Since features importance results may typically depend on the classifier/regressor use, the generalizability of their conclusions is limited. Additionally, the number of features considered is relatively low, and some key features may be overlooked in these analyses. The study context is often limited to a small number of speakers, a few datasets, and a restricted linguistic environment. In  [20] , it is highlighted that models perform worse than humans in cross-dataset settings, emphasizing the lack of generalizability in these constrained setups. Furthermore, due to individual authors' choices-such as using different emotions or acoustic cues-aggregating findings to draw broader conclusions remains challenging. Given these limitations, our objective is to derive the most important acoustic cues for SER that generalize well across multiple representative datasets and models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "We consider D datasets, each consisting of n d utterances x i , i = 1, . . . , n d . Each utterance x i is associated with an emotion label y i , such as e.g., happiness, anger, sadness, etc. The SER task is a supervised task, where we train a model to learn to predict the correct emotion label y of an input utterance x. In this section, we propose an approach for identifying the most relevant acoustic features for SER using various datasets and various models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Set",
      "text": "Each utterance x i is represented by a fixed-size feature\n\nTo construct this feature list, we use the ComParE 2016 feature set  [21] , a large collection of features (Q = 6373) not specifically designed for SER but known to capture a broad range of acoustic information. This set is based on extracting 65 low-level descriptors (LLDs) such as e.g., pitch, spectral energy, and auditory spectrum, along with an additional 65 LLDs derived as their temporal differences. Since LLDs are time-varying and utterances are of different lengths, a wide range of functional descriptors are then applied to these LLDs (such as various distribution moments, local extrema, and different types of quantiles) in order to construct the fixed-size feature list.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deriving Key Acoustic Features",
      "text": "To identify the most important acoustic features for SER, we use classification models that inherently provide feature importance scores, quantifying each feature's contribution to the models' decision. To reliably leverage these importance scores to derive the most important acoustic features, achieving strong classification performance is essential. Additionally, special consideration must be given to correlated features. In many feature importance models, a single variable may be assigned high importance while its correlated counterparts, despite being equally relevant, may be overlooked. Conversely, importance may be spread across a cluster of correlated variables, creating the misleading impression that individual features lack significance. Addressing this challenge is a key aspect of feature importance analysis. tm This is also why our approach, which involves multiple models, datasets, and experiment repetitions, is relevant. If two features convey the same information, they should exhibit similar average or median importance across all these experimental conditions.\n\nTo identify the most important acoustic features for SER, we use M classification models and K datasets. While M = 4 and K = 6 are used in Section 5.2, the proposed approach is applicable to any number of models M and number of datasets K. Every time a model m is trained on a dataset d, with m = 1, . . . , M and d = 1, . . . , D, the outcome is the classification performance p m d and the importance score of each feature s m q,d . Preliminary experiments have shown that aggregating all feature importance scores in one step, such as e.g., using s q = 1 DM s m q,d , is inefficient. Specifically, the resulting selection of features is not as effective as using the selection of features from a single model with respect to the evaluation metrics introduced in Section 4.5. Instead, we propose to find the key acoustic features for SER by proceeding as following.\n\nFirst, we aggregate the normalized feature importance scores across all considered models for each individual dataset. The aggregated importance scores s q,d are com-puted as\n\n.\n\n(1)\n\nThen, for each dataset d, we construct a new feature list f order d by ordering the features in f in decreasing order of importance according to the scores s q,d . Each model is then retrained using subsets of the top-ranked features, varying from 0.5% to 20% in increments of 0.5%. This process is stopped as soon as the model achieves a performance that matches or exceeds the original performance p m d obtained using all features. Let pt(m) denote the minimum percentage of top-ranked features required for model m to reach at least par performance. We then define the multiset M (allowing repeated elements) of all features that were used to reach this threshold\n\n(2) Since the features correspond to statistics of LLDs, we identify the most important acoustic cues for SER by counting the occurrences of each LLD statistic in the multiset M , and ranking them based on frequency.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings",
      "text": "In this section, we present the SER datasets, classification models, and feature importance algorithms employed. Finally, we conclude this section with a detailed description of model training and evaluation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Emotions",
      "text": "To derive a generalizable list of important acoustic cues, we employ six datasets commonly used in the speech emotion recognition (SER) literature, i.e., CaFE  [22] , Emovo  [23] , EmoDB  [24] , DEMoS  [25] , Tess  [26] , and Gemep-5emo  [27] . As summarized in Table  1 , these datasets span four languages-Italian, German, French, and English. According to the emotion mapping shown in Table  2 , we consider utterances labeled as neutral, as well as those representing six core emotions, i.e., happiness, (hot) anger, fear, sadness, surprise, and disgust.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance Evaluation",
      "text": "The standard evaluation metric used in the SER literature is the Unweighted Average Recall (UAR), which is derived from the individual recalls of each emotion. A model's recall for a specific emotion is calculated by dividing the number of correctly identified instances of that emotion by the total number of instances of that emotion in the dataset. This metric is preferred over the conventional accuracy metric since it focuses on the model's capability to recognize a particular emotion without emphasizing discrimination among other emotions. The UAR is simply the arithmetic mean of the recalls across all emotions 1  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Models",
      "text": "The classifiers used to compute the most important acoustic features for SER include the Linear Support Vector Machine (L-SVM)  [28] , Logistic Regression (LGRG)  [29] , Random Forest Fourier (RFC)  [30] , and Light Gradient Boosting Machine (LGBM)  [31]  classifiers. These classifiers were chosen for their inherent ability to determine feature importance. Findings are then validated using two additional classifiers, i.e., the Radial Basis Function SVM (RBF-SVM)  [28]  and Multilayer Peceptron (MLP)  [32]  (which do not offer a straightforward method for determining feature importance). These classifiers were selected because they exhibit similarities to human perception in SER  [4] . Each classification model has a set of hyperparameters, which we optimize using grid search focused on key parameters, balancing training efficiency with strong performance (cf. Section 4.4).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Selection Of Hyperparameters",
      "text": "In this section, we outline the process of selecting optimal hyperparameters for each considered model. The selection is carried out in three key steps, i.e.,:\n\n1. Split datasets. To ensure an unbiased evaluation with respect to speaker and sex specificity, we carefully split each dataset into training and test sets based on two key principles. First, all utterances from a given speaker are assigned exclusively to either the training or the test set, preventing any speaker from appearing in both. Second, we strive for the best possible male-female balance among speakers in the test set. Following these principles, the splitting strategy is as follows: for datasets with more than 12 speakers, 20% of the speakers are assigned to the test set. For datasets with fewer than 12 speakers, two speakers are assigned to the test set. Exceptions are made for Emovo (with six Table  1 : Summary of the used emotion recognition datasets and their characteristics. Neutral utterances denote utterances with an emotionally neutral meaning, chunks refer to segments extracted (based on syntax or prosody of the speaker) from sentences, whereas non-sense utterances are plausible sequences of language sounds without any meaning. The Male (F) column summarizes the number of male (female) speakers in each dataset. French C. denotes French Canadian. The Utterances column presents the total number of utterances in each dataset (#), the number of distinct sentences uttered in various emotions when applicable (# Sentences), and the mean (µ) and standard deviation (σ) of the duration of utterances (in seconds) in each dataset. speakers in total) and Tess (with two speakers in total), where only one speaker is used for testing.\n\n2. Validation. To select the optimal hyperparameters, we use stratified 5-fold cross-validation. For each hyperparameter configuration p, the model is trained on four folds and evaluated on the remaining fold. This process is repeated five times, ensuring that each fold serves as the validation set once. The results from these five runs are then averaged, and the configuration yielding the highest average performance is selected as the optimal hyperparameter configuration p * . This cross-validation strategy enables robust assessment of different hyperpa-rameter combinations and ensures effective selection for each model. For the larger DEMoS dataset, due to its size, a single evaluation is performed using one fold as a dedicated validation set.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "3.",
      "text": "Test. Finally, we train each model on the entire training set using the hyperparameters p * that were determined in step 2. Then, we evaluate this trained model on the test set to assess its performance on unseen data.\n\nTo further reduce bias, this procedure is repeated 3 times for each dataset (i.e., using 3 different splits of the data into training and test sets), and the average of these three runs corresponds to the model's performance on that particular dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Features Importance Evaluation",
      "text": "In order to validate the procedure above in the experimental part, we need to be able to determine which feature importance list is better. To do so, we rank the features in order of importance for both lists. We then retrain our model using a growing percentage of features in their importance order. The feature selection that achieves the same performance as using all features with a smaller percentage of features will be considered better.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we present the results of the experiments described previously, followed by an analysis of the most important features. Note that when referring to the selection of important features, we mean selecting those with the highest importance scores from a ranked list of feature importance coefficients.\n\n5.1 Performance of all models across for all datasets using the complete feature set Table  3  summarizes the unweighted average recall (UAR) achieved by each model on the considered datasets, using the training and validation procedure described in Section 4.4.\n\nFrom a dataset-level perspective, it is evident that classification performance is significantly influenced by the specific dataset. Notably, the number of speakers appears to affect the variance observed across the three runs (each involving different speaker combinations in the test set). Datasets with fewer speakers generally show higher variance (with the Tess dataset being a notable exception). This increased variability may stem from the limited number of speakers, which restricts the model's ability to generalize and leads to overfitting on speaker-specific traits present in the training data. This observation highlights the importance of using a diverse and extensive set of datasets, as done in this study. Beyond dataset size, no clear correlations are observed between classification performance and other dataset characteristics (as listed in Table 1), such as the speakers' native language, the spoken language, or the nature of the utterances.\n\nFrom a model-level perspective, the RBF-SVM, L-SVM, MLP, and LGRG models generally yield the best overall performance. However, on the EMOVO dataset, the RFC and LGBM models considerably outperform these models. Whether this is due to overfitting, where certain models capture dataset-specific cues, or simply because some models are better suited to particular datasets, this observation underscores the value of our multi-model approach. Its key strength lies in mitigating the limitations or over-specialization that any single model might exhibit.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Selection: Multiple Vs One Model",
      "text": "To illustrate the advantage of our proposed approach for aggregating feature importance scores across models, in this section we compare the performance when models are trained under two conditions: (i) using top-ranked features derived from the aggregated importance scores across all models, as described in Section 3.2, and (ii) using topranked features based solely on each individual model's importance scores. These performances are compared to the baseline performance when all features are used. As outlined in Section 4.4, each experiment is repeated three times. Figure  1  shows the difference from the baseline performance for each dataset, averaged across all considered models 2 using various thresholds of top-ranked features based on either the aggregated feature importance scores or the individual feature importance scores. The presented results demonstrate the advantage of using aggregated feature importance scores across multiple models. Specifically, even with just the top-ranked 0.5% features, performance is higher for each dataset when the features are selected based on aggregated importance scores, Figure  1 : Average UAR difference across all models for each dataset when retraining models using top-ranked features at various thresholds, compared to baseline performance (i.e., when models are trained using all features). Left: Top-ranked features selected based on each model's own feature importance scores. Right: Topranked features selected using the proposed aggregated feature importance scores across all models.\n\nas compared to using individual model rankings. Furthermore, when selecting the top-ranked 6% of features using aggregated importance scores, baseline performance is achieved for all datasets. In contrast, when selecting the top-ranked 6% of features using individual importance scores for each model, baseline performance is achieved only for three datasets.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Most Important Acoustic Cues For Ser",
      "text": "The results in Section 5.2 show that using 6% of topranked features through aggregated feature importance scores yields the same or better performance than using the complete feature set for all datasets (cf. Figure  1 ). Retaining these 6% of top-ranked features, we obtain a multiset of 2, 282 features, among which 1, 687 are distinct. This considerable number of distinct features underscores the complexity of the emotion recognition task, where information lies on a diverse and extensive set of features. Since each feature stems from an LLD, we determine the importance of an LLD by counting the number of times features derived from it appear in the top-ranked features (with ∆LLD features also attributed to the specific LLD it is extracted from). Figure  2  presents the normalized occurrence distribution of these LLDs. It can be observed that a small number of LLDs, such as F0final and audspec lengthL1norm, are highly important. While the entire list of LLDs is significant, as each LLD represents an important acoustic cue for SER, the distribution shows a sharp decline, followed by a gradual, steady slope. The  Although this cutoff is used for illustrative purposes and does not have intrinsic significance, it highlights the top 16 LLDs that are particularly important. In the remainder of this section, we provide insights into these top 16 LLDs, which are crucial for SER, as determined using our proposed approach.\n\nF0Final. The fundamental frequency, commonly referred to as pitch, stands out as the LLD on which models heavily rely for effective emotion recognition. This observation aligns with existing literature on emotion recognition (see Section 2.3), where the significance of pitch is widely acknowledged.\n\nAudSpec. The auditory spectrogram is a transformation of the linear-frequency spectrogram that takes into account the non-linear frequency resolution of human hearing. Various characteristics of the auditory spectrogram as captured by the LLDs audspec lengthL1norm, audSpec Rfilt[0], and audSpec Rfilt  [1] , appear to be important acoustic cues for SER. The LLD audspec lengthL1norm is the magnitude of the l 1 norm of the auditory spectrum, broadly corresponding to the perceived loudness. The LLDs audSpec Rfilt[0] and aud-Spec Rfilt  [1]  are the first coefficients of the Rasta transformation used to make to make auditory spectrograms more resilient to noise, adverse conditions and other factors that can affect speech perception and analysis.\n\nVoicingFinalUnclipped. This LLD represents the probability that F0Final is voiced. Emotional states can significantly affect phonation and voice intensity, both of which are key factors in vocal sound production. Since these factors directly influence whether F0Final is voiced, this variable is expected to vary with the speaker's emotional involvement.\n\nMFCCs.\n\nMel-Frequency Cepstral Coefficients (MFCCs) are derived from the cosine transform applied to lagirlogarithmic Mel scale representation of the input signal spectrum. These coefficients offer a compact representation of the overall spectral contour and are robust to variations in recording conditions and speaker characteristics. The lower MFCC coefficients, which correspond to formants, tend to be more important for SER than the higher MFCC coefficients, which capture finer spectral details and rapid variations in the spectrum.\n\nLogHNR. The logarithm of the harmonic-to-noise ratio indicates voicing characteristics, and is hence, also relevant for SER.\n\nPCM. Apart from the aforementioned features, the last category of important features includes Pulse Code Modulation (PCM) features related to variation and energy derived from the signal or its spectrum. Even without incorporating human auditory specificity, these features are relevant for the SER task. More specifically:\n\n• The pcm RMSenergy measure is closely associated with loudness, which is an important acoustic cue for SER.\n\n• Emotions have significant effects on spectral energy distribution. For instance, emotions like sadness are often associated with low harmonic energy above 1 kHz, whereas emotions like anger, happiness, or fear, tend to manifest as high harmonic energy within this frequency range  [13] . Hence, it's unsurprising to find several features linked to such spectral characteristics to be important for SER. These include the pcm fftmag spectralRollOff25.0, which designates the frequency demarcating 25% of the signal's energy, and the pcm fftmag spectralRollOff50.0, which designates the frequency demarcating 50% of the signal's energy.\n\n• The (pcm zcr) measure, which is the zero crossing rate quantifing the number of sign alternations in a signal, also ranks among the crucial features. Indeed, emotion-related changes in vocal expression, intensity, and timbre might affect the temporal variations captured by the zero crossing rate.\n\nThese are the characteristics emphasized by our study. The exact computation of these LLDs can be found in  [33] . The next phase of this project will involve developing a method to explicitly or implicitly preserve some of these features determine the effectiveness of such an approach.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented a comprehensive analysis employing multiple models and datasets to identify the most significant and robust features for the SER task. To the best of our knowledge, this is the first study to utilize such a diverse array of datasets and models to establish feature importance in the SER literature. Despite the complexities inherent in SER, we have identified several key features that are highly relevant in this context. In the future, we will extend our findings to specific emotions by repeating the experiments using recall for each emotion as the evaluation metric. Additionally, we will investigate whether the degradation of these features under adverse conditions such as background noise, channel variations, or speaker diversity, correlates with the performance drop of automatic models and human perception of emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Acknowledgments",
      "text": "This work was partially supported by Sonova AG, Stäfa, Switzerland.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the difference from the baseline",
      "page": 6
    },
    {
      "caption": "Figure 1: Average UAR difference across all models for each dataset when retraining models using top-ranked",
      "page": 7
    },
    {
      "caption": "Figure 2: presents the normalized oc-",
      "page": 7
    },
    {
      "caption": "Figure 2: Normalized occurrence of LLDs in the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "48.6±4.0 50.6±2.8 40.1±1.6 44.6±4.0 49.8±3.8 49.6±2.6\n78.8±2.4 80.3±1.5 70.8±0.9 75.3±0.1 82.7±2.3 83.9±1.8\n74.2±0.1 74.9±0.8 51.6±0.6 63.8±0.8 65.9±0.3 66.0±0.3\n43.2±5.8 36.7±5.0 43.5±1.7 44.2±3.3 36.1±5.1 35.7±4.7\n83.3±8.2 80.0±14.1 80.0±12.2 76.7±8.2 80.0±7.1 76.7±4.1\n58.9±0.0 61.7±0.9 57.0±0.4 57.6±0.4 69.9±0.0 69.6±0.0": "64.5±15.2 64.0±16.2 57.2±14.2 60.4±13.0 64.1±16.5 63.6±16.3"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in autism spectrum disorder across age groups: A cross-sectional investigation of various visual and auditory communicative domains",
      "authors": [
        "F Leung",
        "V Stojanovik",
        "M Micai",
        "C Jiang",
        "F Liu"
      ],
      "year": "2023",
      "venue": "Autism Research"
    },
    {
      "citation_id": "3",
      "title": "In the ear of the beholder: How age shapes emotion processing in nonverbal vocalizations",
      "authors": [
        "C Lima",
        "T Alves",
        "S Scott",
        "S Castro"
      ],
      "year": "2014",
      "venue": "Emotion (Washington, D.C.)"
    },
    {
      "citation_id": "4",
      "title": "Psychoacoustic cues to emotion in speech prosody and music",
      "authors": [
        "E Coutinho",
        "N Dibben"
      ],
      "year": "2012",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "5",
      "title": "Perception and classification of emotions in nonsense speech: Humans versus machines",
      "authors": [
        "E Parada-Cabaleiro",
        "A Batliner",
        "M Schmitt",
        "M Schedl",
        "G Costantini",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "PLoS One"
    },
    {
      "citation_id": "6",
      "title": "Comparing cross-lingual automatic and human emotion recognition in background noise",
      "authors": [
        "J Koemans"
      ],
      "year": "2020",
      "venue": "Comparing cross-lingual automatic and human emotion recognition in background noise"
    },
    {
      "citation_id": "7",
      "title": "Identifying emotions in opera singing: Implications of adverse acoustic conditions",
      "authors": [
        "E Parada-Cabaleiro",
        "M Schmitt",
        "A Batliner",
        "S Hantke",
        "G Costantini",
        "K Scherer",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Identifying emotions in opera singing: Implications of adverse acoustic conditions"
    },
    {
      "citation_id": "8",
      "title": "Speech variability and emotion : production and perception",
      "authors": [
        "S Mozziconacci"
      ],
      "year": "1998",
      "venue": "Speech variability and emotion : production and perception"
    },
    {
      "citation_id": "9",
      "title": "Expressing degree of activation in synthetic speech",
      "authors": [
        "M Schroder"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on"
    },
    {
      "citation_id": "10",
      "title": "Vocal emotion recognition by normal-hearing listeners and cochlear implant users [published correction appears in trends amplif",
      "authors": [
        "X Luo",
        "Q Fu",
        "J Galvin"
      ],
      "year": "2007",
      "venue": "Trends Amplif"
    },
    {
      "citation_id": "11",
      "title": "Acoustical correlates of affective prosody",
      "authors": [
        "K Hammerschmidt",
        "U Jürgens"
      ],
      "year": "2007",
      "venue": "J. Voice"
    },
    {
      "citation_id": "12",
      "title": "Intonation and emotion: influence of pitch levels and contour type on creating emotions",
      "authors": [
        "E Rodero"
      ],
      "year": "2011",
      "venue": "J. Voice"
    },
    {
      "citation_id": "13",
      "title": "Expression of emotion and attitude through temporal speech variations",
      "authors": [
        "S Mozziconacci",
        "D Hermes"
      ],
      "year": "2001",
      "venue": "Expression of emotion and attitude through temporal speech variations"
    },
    {
      "citation_id": "14",
      "title": "Influence on spectral energy distribution of emotional expression",
      "authors": [
        "M Guzman",
        "S Correa",
        "D Muñoz",
        "R Mayerhoff"
      ],
      "year": "2013",
      "venue": "Journal of Voice"
    },
    {
      "citation_id": "15",
      "title": "Emotion Perception and Recognition from Speech",
      "authors": [
        "C.-H Wu",
        "J.-F Yeh",
        "Z.-J Chuang"
      ],
      "year": "2009",
      "venue": "Emotion Perception and Recognition from Speech"
    },
    {
      "citation_id": "16",
      "title": "The effect of noise on emotion perception in an unknown language",
      "authors": [
        "O Scharenborg",
        "S Kakouros",
        "J Koemans"
      ],
      "year": "2018",
      "venue": "The effect of noise on emotion perception in an unknown language"
    },
    {
      "citation_id": "17",
      "title": "The perception of emotions in noisified nonsense speech",
      "authors": [
        "E Parada-Cabaleiro",
        "A Baird",
        "A Batliner",
        "S Hantke",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "The perception of emotions in noisified nonsense speech"
    },
    {
      "citation_id": "18",
      "title": "Decoding speech prosody in five languages",
      "authors": [
        "W Thompson",
        "L.-L Balkwill"
      ],
      "year": "2006",
      "venue": "Semiotica"
    },
    {
      "citation_id": "19",
      "title": "Spectral moment features augmented by low order cepstral coefficients for robust asr",
      "authors": [
        "P Tsiakoulis",
        "A Potamianos",
        "D Dimitriadis"
      ],
      "year": "2010",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "20",
      "title": "Recognizing emotions in a foreign language",
      "authors": [
        "M Pell",
        "L Monetta",
        "S Paulmann",
        "S Kotz"
      ],
      "year": "2009",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "21",
      "title": "A preliminary study of cross-lingual emotion recognition from speech: Automatic classification versus human perception",
      "authors": [
        "J Jeon",
        "D Le",
        "R Xia",
        "Y Liu"
      ],
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "22",
      "title": "openSMILE -the munich versatile and fast Open-Source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. ACM Multimedia (MM)"
    },
    {
      "citation_id": "23",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "A canadian french emotional speech dataset"
    },
    {
      "citation_id": "24",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "25",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "26",
      "title": "DEMoS: an Italian emotional speech corpus. Elicitation methods, machine learning, and perception",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "DEMoS: an Italian emotional speech corpus. Elicitation methods, machine learning, and perception"
    },
    {
      "citation_id": "27",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)"
    },
    {
      "citation_id": "28",
      "title": "Introducing the geneva multimodal expression corpus for experimental research on emotion perception",
      "authors": [
        "T Bänziger",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "29",
      "title": "The nature of statistical learning theory",
      "authors": [
        "V Vapnik"
      ],
      "year": "2000",
      "venue": "The nature of statistical learning theory"
    },
    {
      "citation_id": "30",
      "title": "Applied logistic regression",
      "authors": [
        "D Hosmer",
        "S Lemeshow"
      ],
      "year": "2000",
      "venue": "Applied logistic regression"
    },
    {
      "citation_id": "31",
      "title": "Random features for largescale kernel machines",
      "authors": [
        "A Rahimi",
        "B Recht"
      ],
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Lightgbm: a highly efficient gradient boosting decision tree",
      "authors": [
        "G Ke",
        "Q Meng",
        "T Finley",
        "T Wang",
        "W Chen",
        "W Ma",
        "Q Ye",
        "T.-Y Liu"
      ],
      "year": "2017",
      "venue": "Proc. International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "Learning representations by back-propagating errors",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1986",
      "venue": "Nature"
    },
    {
      "citation_id": "34",
      "title": "Real-time speech and music classification by large audio feature space extraction",
      "authors": [
        "F Eyben"
      ],
      "year": "2015",
      "venue": "Real-time speech and music classification by large audio feature space extraction"
    }
  ]
}