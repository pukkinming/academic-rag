{
  "paper_id": "2309.10780v1",
  "title": "Towards Affective Computing That Works For Everyone",
  "published": "2023-09-19T17:31:29Z",
  "authors": [
    "Tessa Verhoef",
    "Eduard Fosch-Villaronga"
  ],
  "keywords": [
    "affective computing",
    "emotion recognition",
    "diversity",
    "discrimination",
    "bias",
    "fairness",
    "inclusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Missing diversity, equity, and inclusion elements in affective computing datasets directly affect the accuracy and fairness of emotion recognition algorithms across different groups. A literature review reveals how affective computing systems may work differently for different groups due to, for instance, mental health conditions impacting facial expressions and speech or agerelated changes in facial appearance and health. Our work analyzes existing affective computing datasets and highlights a disconcerting lack of diversity in current affective computing datasets regarding race, sex/gender, age, and (mental) health representation. By emphasizing the need for more inclusive sampling strategies and standardized documentation of demographic factors in datasets, this paper provides recommendations and calls for greater attention to inclusivity and consideration of societal consequences in affective computing research to promote ethical and accurate outcomes in this emerging field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Diversity and inclusion are critical aspects of the responsible development of artificial intelligence (AI) technologies, including affective computing. Affective computing, which focuses on recognizing, interpreting, and responding to human emotions, has the potential to revolutionize various domains, such as healthcare, education, and human-machine interaction  [1] . Capturing subjective states through technical means is challenging, though, and errors can occur, as seen with lie detectors not working adequately  [2]  or gender classifier systems misgendering users  [3] . If used for ulterior decision-making processes, such inferences could have disastrous consequences for people, the impacts of which may vary depending on the context of an application, i.e., flagging innocent people as potential criminals in border control  [4]  or detrimentally affecting vulnerable groups in mental health care  [5] .\n\nGiven that single, unimodal data streams seem not robust enough to recognize human emotions, a growing trend in affective computing is using multimodal data to achieve machine interpretation, prediction, and understanding of human affective processes  [6] ,  [7] . These modalities include recognizing facial expressions, vocal tones, body posture and gestures, and other physiological signals such as heart rate and skin Supported by the Global Transformations and Governance Challenges Initiative at Leiden University for project Gendering Algorithms conductance  [8] . Multimodal affective computing promises to measure user reactions to particular content better, create more interactive and engaging user experiences, and gain deeper insights into user behavior much more accurately than with unimodal systems  [9] .\n\nHowever, combining different information strains is not straightforward  [5] ,  [10]  and to what extent multimodal approaches will be able to solve existing problems in the field of affective computing remains an open question. Issues such as the existing disagreement on the nature and scientific understanding of emotions  [11] , or problems related to bias, discrimination, and injustice in affective computing  [1] ,  [12]  likely remain. For instance, although emotion recognition algorithms have gained significant attention, they may have different outcomes for different groups due to health conditions, age, and gender  [13] ,  [14] . Mental health conditions such as depression or schizophrenia can affect facial expressions and speech, making it difficult to identify emotions accurately  [15] . Non-neurotypical individuals may also have difficulty expressing emotions, while individuals with PTSD or phobias may display exaggerated or blunted emotional responses  [16] . Concerning age, children's facial expressions and speech may be less distinct than adults and older adults' facial appearance changes due to aging can make it harder for algorithms to detect emotions accurately  [17] .\n\nGiven the growing interest in using these techniques in sensitive contexts such as healthcare and education, we explore how multimodal affective computing impacts diversity, equity, and inclusion in this article. In particular, we review the various ways human traits influence emotional expression and discuss its consequences for the current state of diversity and inclusion in affective computing. We then analyze an extensive list of datasets commonly used in affective computing and highlight how diverse and inclusive they are by juxtaposing them with the different grounds for discrimination that the law provides (i.e., religion or belief, origin, sexual orientation, sex, skin color, race, civil status, disability or chronic illness, or age), focusing on those characteristics that may affect emotion recognition the most. We anticipate that systems trained on the datasets currently available and used most widely may not work equally well for everyone and will likely have racial biases, biases against users with (mental) disabilities, and age biases because they derive from limited samples that do not fully represent societal diversity. We conclude by highlighting that there is still a long way to go for the field of affective computing to combat biases and inequalities that are typically exacerbated by the lack of diversity in datasets, technical teams, and the community  [18] . Finally, we propose recommendations for improving diversity and inclusion in affective computing research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Human Traits Influencing Emotional",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Expression",
      "text": "Although physical and physiological markers to recognize emotions have gained significant attention in recent years, algorithms based on these markers may have different outcomes for different groups due to several factors that influence the recognition of emotions, such as age, health conditions and gender.\n\nEmotion recognition algorithms may have limitations in detecting emotions in different age groups  [19] ,  [20] . For instance, children's facial expressions and speech may be less distinct than adults'  [21]  and may not have a full range of emotions, making it difficult to identify their emotional state accurately  [22] . Furthermore, children may understand emotions differently than adults, and their expressions may not match their feelings, further complicating emotion recognition. One of the main challenges in emotion recognition algorithms for older adults is changes in facial appearance due to aging. As people age, their faces undergo various changes, such as the loss of muscle tone, wrinkle formation, and reduced eyebrow movements, making it harder for algorithms to detect emotions such as surprise or anger  [23] . Also, if people undergo plastic surgery, which tends to happen later in life, this may affect emotion recognition systems  [24] . In addition, health conditions that predominantly affect older individuals are known to affect emotional expressions. Alzheimer's Disease, for instance, is associated with impairments in the production of facial expressions and mood disorders  [25] , and Parkinson's Disease affects speech prosody and other communicative functions accompanied by an impact on mood  [26] .\n\nSome mental health conditions affect facial expressions and speech, making it difficult to identify emotions accurately. For example, individuals with depression or schizophrenia often have a flattened affect, meaning they display fewer emotional expressions that, although not always affecting their subjective experience, make it challenging to detect emotional states  [15] ,  [27] . Similarly, non-neurotypical individuals may have difficulty expressing emotions, making it difficult to identify emotions accurately using physical markers  [28] . Individuals with post-traumatic stress disorder (PTSD) or phobias may display exaggerated or blunted emotional responses, impacting emotion recognition  [16] .\n\nCultural differences may play a role as well  [29] , and emotion recognition algorithms may for example need different approaches for accurately detecting emotions in the deaf community  [30] . Differences in body posture and facial expressions may carry linguistic meaning in sign languages  [31]  and may therefore be less reliable for detecting emotions in sign language users as compared to the hearing community.\n\nRecent studies have also revealed that some systems based on facial features perform better in one gender than another, with generally lower accuracy for female faces  [32] . This is not unlike well-known biases in face recognition, where for instance, algorithms developed by major tech companies were significantly less accurate in recognizing darker-skinned individuals, particularly women, than lighter-skinned individuals  [33] . Several rationales may explain these disparities in accuracy and apparent errors in recognition performance. First, algorithms developed using non-diverse datasets may have gender biases in emotion recognition accuracy in underrepresented groups. Better emotion recognition was reported in female individuals, for instance, when females were overrepresented in the data  [20] . Gender-balanced data does not guarantee balanced performance though, since other factors play a role as well. E.g. female faces have been found to be on average more similar to each other than male faces  [32]  and controlling for specific features known to be more prevalent in one gender over others, such as beards for males or make-up for females, balances performance more  [32] .\n\nAffective computing researchers increasingly focus on the multimodal integration of multiple data sources as the solution for improving emotion recognition systems. However, the field of Machine Learning has identified problems that may arise when measurements from multiple datasets are combined, which may introduce increased \"structured missingness\"  [34] , referring to non-random patterns of missing data or underrepresentation. This problem is especially prevalent in data describing highly heterogeneous population characteristics, such as the expression of emotions. Besides problems in learning performance and prediction accuracy, structured missingness may perpetuate or exacerbate existing inequalities, especially when data from underrepresented groups is missing entirely.\n\nFor the field of affective computing, the extensive use of multimodal integration potentially involves combining datasets which are individually already relatively sparse and do not cover the range of genders, variety in mental health conditions, age groups, or other conditions that may have a direct effect on emotions. This can be very problematic and amplify biases that are already a major problem in single datasets. As an example, the most widely used facial expression dataset (Extended Cohn-Kanade (CK+)  [35]  has over 200 subjects in it, making it one of the most extensive datasets in terms of the number of subjects included for data collected in the lab, but it contains only two genders, not equally distributed (69% female), and a very skewed racial distribution with 81% euro-american, 13% afro-american, and 6% 'other.' The authors who released the original Cohn-Kanade dataset 23 years ago  [36]  already pointed out that many critical individual differences exist in facial expression features that vary with sex, age, and race. Moreover, they noted how various health conditions can affect facial expressions as well. They suggested including large samples of subjects with diverse backgrounds and health statuses to train emotion recognition systems that are robust to individual differences and work effectively for everyone. As we will show, the field has, unfortunately, yet to progress in this ideal direction.\n\nIf this probable inequality in access to functioning affective computing technology is not addressed, it's use in products and services will be problematic and potentially even illegal  [18] . Given that religion or belief, origin, sexual orientation, sex, skin color, race, civil status, disability or chronic illness, or age are grounds for discrimination, errors in this area could lead to bias and discrimination, which is prohibited by law.\n\nAlthough it is unsurprising that discriminatory outcomes may result from poor datasets that do not account for intersectional differences such as age, mental health, and gender, at this point, we do not know the magnitude of the problem, which is what we aim to unearth in this contribution.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Analysis Of Datasets Commonly Used In Affective Computing",
      "text": "To understand the magnitude of this issue, we analyze a diverse list of datasets commonly used in the field of affective computing. We base our selection on the most recent comprehensive review paper we could find  [8] , in which many datasets based on various signal modalities were listed.\n\nAn increasing number of datasets used in affective computing tasks contain (large amounts of) data collected from the web, such as written reviews on Amazon  [37] ,  [38]  or IMDB  [39]  for textual sentiment analysis, or images and movies for bodily gesture and facial expression recognition through Google image search or YouTube  [40] ,  [41] . Since such datasets did not involve the recruitment of test subjects in a lab, no demographic information is available about the people performing the recorded emotions, making it impossible to assess our diversity dimensions for these sources. We, therefore, base our analysis on datasets that were created in the lab. In addition, we are aware of the fact that affective computing has been used to develop systems and applications targeted at specific populations, such as emotion detection and regulation systems for autism spectrum disorder (ASD)  [42] , Virtual Reality (VR) Therapy to help individuals with mental health problems, such as anxiety disorders or post-traumatic stress disorder (PTSD)  [43] , Affective Tutoring Systems for special needs education  [44] , or emotion-sensing Chatbots for mental health support  [45] . Those systems are clearly meant to work for a specific target user group. However, here we focus on work published in general affect recognition aimed to be used ubiquitously by everyone, in general human-computer interaction, healthcare, social robotics, entertainment, advertising, automotive and education settings. In these application areas, it is essential that the technology is available and performs adequately for all potential users equally.\n\nConsidering all these criteria, we eventually included 26 datasets in our analyses with in total 1121 subjects. These datasets were released between 1998 and 2018 and span five different modalities: Speech, Face, Body, Physiological signals (e.g., EEG, GSR, temperature), and Multimodal (combinations of the first four). Table I lists all datasets with a short explanation of the type of collected data and a reference to the source paper, as well as analyzed demographic features, which are discussed in the results section.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "Table  I  shows an overview of all datasets and their analyzed characteristics. We list which modality the data was based on, the year the dataset was released, the number of subjects included, the mean age of the subjects if it was mentioned in the paper, the percentage of female subjects included if specified, and the racial diversity of the subjects if mentioned. We analyzed the papers for the complete list of different grounds for discrimination that the law provides, but sexual orientation, religion, and civil status were never mentioned; therefore, we will not discuss them further in this paper.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Race Or Cultural Background: An Incomplete Task",
      "text": "When we look at the inclusion of subjects with different ethnic or cultural backgrounds, most papers actually do not mention it at all. Some explicitly state that all participants have the same background (indicated in the table as 'single'), while others include more diverse groups (indicated as 'multi'). For the multi-background datasets, we counted how many ethnic groups were represented out of four categories, Asian, Black, Latino and White. Fig.  1  plots these numbers for each modality and shows that this has been taken seriously mainly in the datasets for facial expression recognition, while datasets in the other modalities lack diversity. Most papers that describe datasets with diverse subject backgrounds also mention the exact composition  [35] ,  [46] ,  [47] ,  [52] ,  [53] ,  [55] ,  [57] -  [59] ,  [61] ,  [63] . For the pie chart in Fig.  2  all racial composition data from these studies were gathered, and we can see that some groups, especially Black and Latino, are highly underrepresented. Similar findings were highlighted by  [20]  for datasets based on facial features and by  [71]  for audiovisual datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Sex And Gender Are Different, But Not Always Accounted For",
      "text": "When looking at the inclusion of different sexes, we see that most papers report the composition of their subject pool based on the percentage of male and female participants. Only one exception is observed (CASME II) where no information on this is mentioned. As also reported by  [71] , most datasets include equal or almost equal numbers of male and female subjects, besides a few exceptions where either female or male participants are outnumbered  [35] ,  [54] ,  [61] ,  [65]  or not present at all  [49] ,  [52] . That said, this binary distinction may not work for contemporary societies in which other communities are not represented (intersex, transgender). Moreover, gender is different from sex and plays a crucial role in shaping one's self expression  [3] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Prominent Young Age May Disregard Older Groups",
      "text": "For some, especially older dataset papers (N=7), no participants' age information was mentioned. Some papers reported the mean age of the subjects included in their datasets (N=14), sometimes including standard deviations (N=8), while others reported a range (N=14). In Table  1  we can see that the mean age of subjects is almost exclusively (with one exception) between 20 and 30 years of age across datasets. This is probably because many research groups use undergraduate or graduate students from their programs to participate in their studies. For the studies that reported an age range, Fig.  3  illustrates these for each dataset. We can see here that especially older age groups (50 and up) are highly underrepresented in the data. The one paper that includes a very large range (5-75,  [58] ), also reported percentages for different age groups. The very young and slightly older categories were still much less represented, with only 2.8% and 5.5%, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. (Mental) Health Or Disability: An Unfinished Agenda",
      "text": "The vast majority of papers introducing general-purpose affective computing datasets do not mention any inclusion of populations of subjects with varying (mental) health conditions. A few papers  [62] ,  [64] ,  [68]  mention explicitly that their participants were 'healthy', and these happen to all be datasets that include physiological data. One paper  [65]  was more specific and explicitly excluded participants with \"pregnancy, heavy smoking, mental disorders, chronic and cardiovascular diseases.\" Another study  [63]  reported that they selected the subjects using the Eysenck Personality Questionnaire (EPQ), which characterizes personality in terms of Extraversion/Introversion, Neuroticism/ Stability and Psychoticism/Socialisation. The researchers noticed that their technology was less able to pick up on the physiological Fig.  2 . Some groups are highly underrepresented Fig.  3 . Age ranges across datasets that mention age range signals of introverted people or those with unstable mood; therefore, such individuals were deliberately excluded from participation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Discussion",
      "text": "Our findings shed light on the current state of inclusivity in affective computing datasets. The results reveal gaps in the representation of diverse populations in these datasets, particularly regarding race, age, and (mental) health/disability.\n\nOne noteworthy observation is that race is often not mentioned in the papers. When it is, most datasets lack diversity in terms of cultural backgrounds or origins. This is particularly evident in datasets for modalities other than facial expression recognition. This distinction is important because people do not only differ based on how they look but culture can also influence how people express emotions, for example, by emphasizing eye or mouth usage to identify people  [29] . The underrepresentation of certain racial/ethnic groups, such as Black and Latino populations, is concerning and highlights the need for more inclusive sampling strategies to ensure these technologies work equally well across racial and cultural groups.\n\nThe age of participants is also an essential factor to consider in affective computing datasets. Mirroring findings for facial expressions datasets  [20]  and audiovisual datasets  [71] , the majority of datasets in our sample report a mean age between 20 and 30 years, indicating a bias towards younger age groups. This leads to an under-representation of older age groups, with limited data available for populations aged 50 and above. This is especially problematic since, as we reviewed, physical changes and age-related health conditions may significantly affect the expression of emotions in older populations.\n\nFurthermore, the inclusion of populations with varying (mental) health conditions is lacking in affective computing datasets. Many papers do not mention any specific inclusion or exclusion criteria related to (mental) health or disability, and some even explicitly mention that their participants were \"healthy.\" This lack of diversity in terms of mental health or disability status may limit the generalizability of affective computing technologies to populations with different mental health conditions and may perpetuate stigmas and biases related to mental health. We observed that especially for datasets based on physiological data, participants with health problems are sometimes explicitly excluded, which means that any multimodal dataset that includes physiological signals will have non-random missing values which can result in biased models that perform well for healthy individuals but poorly for those belonging to the underrepresented group. These findings highlight the need for researchers in the field of affective computing to be more mindful of inclusivity and actively consider the representation of diverse populations in their datasets to ensure that the technologies developed are more representative, equitable, and beneficial for all individuals.\n\nCareful data collection with subjects in the lab is timeconsuming and costly. It is, therefore, no surprise that recent datasets are often created by scraping data from the web. This has advantages, such as the large volume of data that can be collected in this way, which will also increase the inclusion of more diverse data sources. However, a disadvantage is that demographic information on the subjects in the dataset is not available, making it hard to measure and correct potential biases in the data. In this respect, emotion recognition algorithms also rely heavily on human annotations, which can be influenced by the annotators' demographic characteristics, and this can significantly impact the algorithms' accuracy  [71] .\n\nTo ensure consistent model performance for all target groups, sensitive applications such as emotion recognition must address representational bias in the data of both emotion expressors and annotators. Another way to potentially mitigate the adverse consequences of bias would be introducing a standardized (mandatory) way to document the inclusion of various relevant demographic factors in datasets. The recommendation proposed by  [34]  for machine learning in general, i.e., \"appropriate sensitivity to social processes that underlie data generation and contextual awareness of potential social, cultural and historical determinants of discriminatory patterns are crucial for effective bias mitigation. Thus, involving experts with domain knowledge and social scientific training is vital;\" applies to affective computing to a great extent. The mandatory Ethical Impact Statement in the papers presented at the Affective Computing and Intelligent Interaction (ACII) conference is a significant first step in this direction.\n\nSo far, we have focused mainly on the inclusion of diverse populations in datasets that are used for training affective computing systems, but it is equally crucial that systems are tested on diverse participants to make sure the recognition accuracy is generalizing and works equally well for diverse groups in society. Especially with the kinds of datasets that extensively use data downloaded from the internet, it is essential to assess potential biases by testing the technology on diverse users directly. Given the sometimes very sensitive application areas of affective computing, including the (mental) healthcare industry, it might not be excessive to apply similar guidelines surrounding diversity and inclusion used for clinical trials in medical sciences 1  to the testing of affecting computing technologies. It is good to remember that inferences based on subjective data could lead to disastrous consequences depending on the application context, where stakes could be extremely high. In other words, a recommender system that suggests a new song that you may like or a new movie to watch is not the same as a system that is meant to diagnose you with a particular disorder or disease  [18]  or a system that may be used in border control  [4] .\n\nIn conclusion, this study revealed that affective computing datasets generally lack diversity, with a limited representation of certain racial/ethnic groups and cultural backgrounds, sex and gender imbalances, skewed age demographics, and a total neglect of (mental) health/disability factors. This highlights the need for more inclusive sampling strategies and standardized documentation of demographic factors in datasets. Additionally, testing affective computing systems on diverse populations is crucial to ensure generalizability and accuracy. The sensitive nature of affective computing applications calls for guidelines similar to clinical trials in medical sciences. It is imperative to be mindful of the potential consequences of bias in subjective inferences, especially in such high-stakes contexts as those usually involved in affective computing.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This research addresses the ethical implications of inclusivity in affective computing datasets and its consequences for emotion recognition algorithms. This study underscores the need for greater diversity and representation in research samples by highlighting current datasets' limitations and potential biases in affective computing systems. The ethical implications of biased and inaccurate emotion recognition systems are significant, as they can impact vulnerable populations, including individuals with mental health conditions, children, older adults, and different genders. The potential consequences of bias in high-stakes contexts, such as decision-making and human-computer interactions, are also discussed, emphasizing the need for fair and accurate emotion recognition systems. The paper advocates for introducing inclusive sampling strategies, standardized documentation of demographic factors, and diversity and inclusion guidelines akin to those for clinical trials. The authors highlight that having more balanced datasets does not automatically lead to fair algorithms, and caution should be applied when considering their recommendations.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: plots these numbers for",
      "page": 3
    },
    {
      "caption": "Figure 2: all racial",
      "page": 3
    },
    {
      "caption": "Figure 1: Racial diversity (number of ethnic groups included), grouped by",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates",
      "page": 5
    },
    {
      "caption": "Figure 2: Some groups are highly underrepresented",
      "page": 5
    },
    {
      "caption": "Figure 3: Age ranges across datasets that mention age range",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality": "Speech",
          "Database": "Berlin \nDatabase \nof \nEmotional \nSpeech \n(Emo-DB)",
          "Explanation": "Audio  data  of  sentences  spoken  in  German  by  actors \ndisplaying various emotions.",
          "Year": "2005",
          "#": "10",
          "Age": "",
          "%Female": "50.00%",
          "Race": "",
          "Ref": "[46]"
        },
        {
          "Modality": "",
          "Database": "Belfast Induced Natural \nEmotion  (Belfast)",
          "Explanation": "Describes three sets of data, with slight variations. Here \nwe included the third and most diverse set.",
          "Year": "2012",
          "#": "60",
          "Age": "26.35",
          "%Female": "50.00%",
          "Race": "multi",
          "Ref": "[47]"
        },
        {
          "Modality": "Body",
          "Database": "FAce \nand \nBOdy \ndatabase  (FABO)",
          "Explanation": "Bimodal  database  with  simultaneous  video  footage  of \nboth facial expressions and body gesture.",
          "Year": "2006",
          "#": "23",
          "Age": "26.81",
          "%Female": "52.17%",
          "Race": "",
          "Ref": "[48]"
        },
        {
          "Modality": "",
          "Database": "THEATER Corpus",
          "Explanation": "Movie  clips  coded  with  affective  states  and  related  to \nbasic gestural form features.",
          "Year": "2009",
          "#": "2",
          "Age": "",
          "%Female": "0.00%",
          "Race": "",
          "Ref": "[49]"
        },
        {
          "Modality": "",
          "Database": "GEneva \nMultimodal \nEmotion \nPortrayals \n(GEMEP)",
          "Explanation": "Database of body postures and gestures collected from \nthe perspectives of both an interlocutor and an observer.",
          "Year": "2010",
          "#": "10",
          "Age": "",
          "%Female": "50.00%",
          "Race": "",
          "Ref": "[50]"
        },
        {
          "Modality": "",
          "Database": "EMILYA",
          "Explanation": "Body  gesture  data  captured  with  motion  capture  tech- \nnology of actors expressing 8 emotions in 7 actions.",
          "Year": "2014",
          "#": "11",
          "Age": "26",
          "%Female": "54.55%",
          "Race": "",
          "Ref": "[51]"
        },
        {
          "Modality": "Face",
          "Database": "JAFFE",
          "Explanation": "Database with images of 7 facial expressions.",
          "Year": "1998",
          "#": "10",
          "Age": "",
          "%Female": "100.00%",
          "Race": "single",
          "Ref": "[52]"
        },
        {
          "Modality": "",
          "Database": "Extended Cohn-Kanade \n(CK+)",
          "Explanation": "Facial expression images of subjects who were in- \nstructed to perform 7 facial expressions.",
          "Year": "2010",
          "#": "210",
          "Age": "",
          "%Female": "69.05%",
          "Race": "multi",
          "Ref": "[35]"
        },
        {
          "Modality": "",
          "Database": "MMI",
          "Explanation": "Instead of a single image, this dataset consists of onset- \napex-offset sequences.",
          "Year": "2010",
          "#": "25",
          "Age": "",
          "%Female": "48.00%",
          "Race": "multi",
          "Ref": "[53]"
        },
        {
          "Modality": "",
          "Database": "Oulu-CASIA NIR-VIS \n(Oulu-CASIA)",
          "Explanation": "Includes 2,880 image sequences captured with illumina- \ntion invariant techniques, such as near-infrared (NIR).",
          "Year": "2011",
          "#": "80",
          "Age": "",
          "%Female": "26.25%",
          "Race": "multi",
          "Ref": "[54]"
        },
        {
          "Modality": "",
          "Database": "BU 3D Facial Expres- \nsion  (BU-3DFE)",
          "Explanation": "Contains  606  facial  expression  images  in  3D  captured \nfrom people with one of six facial expressions.",
          "Year": "2006",
          "#": "100",
          "Age": "",
          "%Female": "60.00%",
          "Race": "multi",
          "Ref": "[55]"
        },
        {
          "Modality": "",
          "Database": "BU 4D Facial Expres- \nsion  (BU-4DFE)",
          "Explanation": "Contains sequences of 3D images of facial expressions, \nadding an extra dimension.",
          "Year": "2008",
          "#": "41",
          "Age": "",
          "%Female": "56.10%",
          "Race": "multi",
          "Ref": "[56]"
        },
        {
          "Modality": "",
          "Database": "BP4D-Spontaneous",
          "Explanation": "Well-annotated  3D  video  database  consisting  of  spon- \ntaneous facial expressions.",
          "Year": "2014",
          "#": "41",
          "Age": "23.5",
          "%Female": "56.10%",
          "Race": "multi",
          "Ref": "[57]"
        },
        {
          "Modality": "",
          "Database": "4DFAB",
          "Explanation": "Dynamic 3D faces captured over a five-year period.",
          "Year": "2018",
          "#": "180",
          "Age": "29.21",
          "%Female": "33.33%",
          "Race": "multi",
          "Ref": "[58]"
        },
        {
          "Modality": "",
          "Database": "Spontaneous \nMicro- \nexpression  (SMIC)",
          "Explanation": "Contains 164 micro-expression video clips.",
          "Year": "2013",
          "#": "20",
          "Age": "",
          "%Female": "30.00%",
          "Race": "multi",
          "Ref": "[59]"
        },
        {
          "Modality": "",
          "Database": "CASME  II",
          "Explanation": "Micro-expression videos.",
          "Year": "2014",
          "#": "35",
          "Age": "22.03",
          "%Female": "",
          "Race": "",
          "Ref": "[60]"
        },
        {
          "Modality": "",
          "Database": "Spontaneous \nMicro- \nFacial \nMovement \n(SAMM)",
          "Explanation": "Another  micro-expression  video  dataset  with  high  res- \nolution.",
          "Year": "2018",
          "#": "32",
          "Age": "33.24",
          "%Female": "50.00%",
          "Race": "multi",
          "Ref": "[61]"
        },
        {
          "Modality": "Physiology",
          "Database": "DEAP",
          "Explanation": "Combines  physiological  signals  such  as  brain  waves, \nskin conductance and body temperature.",
          "Year": "2012",
          "#": "32",
          "Age": "26.9",
          "%Female": "50.00%",
          "Race": "",
          "Ref": "[62]"
        },
        {
          "Modality": "",
          "Database": "SEED",
          "Explanation": "Contains  brain  wave  data  from  subjects  who  were \nexposed to movie clips.",
          "Year": "2015",
          "#": "15",
          "Age": "23.27",
          "%Female": "53.33%",
          "Race": "single",
          "Ref": "[63]"
        },
        {
          "Modality": "",
          "Database": "AMIGOS",
          "Explanation": "Brainwaves, heartbeat and skin conductance were \nrecorded using wearable sensors.",
          "Year": "2017",
          "#": "40",
          "Age": "28.3",
          "%Female": "32.50%",
          "Race": "",
          "Ref": "[64]"
        },
        {
          "Modality": "",
          "Database": "Wearable \nStress \nand \nAffect  Detection \n(WE- \nSAD)",
          "Explanation": "Combines data measured from multiple sensor modali- \nties: blood volume pulse, heartbeat, electrodermal activ- \nity, electromyogram, respiration, body temperature, and \nthreeaxis acceleration.",
          "Year": "2018",
          "#": "15",
          "Age": "27.5",
          "%Female": "20.00%",
          "Race": "",
          "Ref": "[65]"
        },
        {
          "Modality": "Multimodal",
          "Database": "Interactive \nEmotional \nDyadic Motion Capture \n(IEMOCAP)",
          "Explanation": "Combines \nspeech,  body  and \nfacial \nsignals \nthrough \nmarkers  on  the  face,  head,  and  hands,  while  actors \nwere recorded during scripted and spontaneous spoken \nscenarios.",
          "Year": "2008",
          "#": "10",
          "Age": "",
          "%Female": "50.00%",
          "Race": "",
          "Ref": "[66]"
        },
        {
          "Modality": "",
          "Database": "CreativeIT",
          "Explanation": "Contains  detailed  full-body  motion,  visual-audio  and \ntext  description  data  collected  from  actors  during  af- \nfective dyadic interactions.",
          "Year": "2011",
          "#": "16",
          "Age": "",
          "%Female": "56.25%",
          "Race": "",
          "Ref": "[67]"
        },
        {
          "Modality": "",
          "Database": "MAHNOB-HCI",
          "Explanation": "Combines  video  with  physiological  data  using  6  video \ncameras, a head-worn microphone, an eye gaze tracker, \nand physiological sensors.",
          "Year": "2012",
          "#": "27",
          "Age": "26.06",
          "%Female": "59.26%",
          "Race": "",
          "Ref": "[68]"
        },
        {
          "Modality": "",
          "Database": "RECOLA",
          "Explanation": "Combines  audio  and  video  with  physiological  data \nrecorded  during  spontaneous  interactions  between  sub- \njects.",
          "Year": "2013",
          "#": "46",
          "Age": "22",
          "%Female": "58.70%",
          "Race": "",
          "Ref": "[69]"
        },
        {
          "Modality": "",
          "Database": "DECAF",
          "Explanation": "Combines near-infra-red (NIR) facial videos with brain \nwaves and various other physiological signals.",
          "Year": "2015",
          "#": "30",
          "Age": "27.3",
          "%Female": "46.67%",
          "Race": "",
          "Ref": "[70]"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The emergence of \"truth machines\"?: Artificial intelligence approaches to lie detection",
      "authors": [
        "J Oravec"
      ],
      "year": "2022",
      "venue": "Ethics and Information Technology"
    },
    {
      "citation_id": "3",
      "title": "Gendering algorithms in social media",
      "authors": [
        "E Fosch-Villaronga",
        "A Poulsen",
        "R Søraa",
        "B Custers"
      ],
      "year": "2021",
      "venue": "ACM SIGKDD Explorations Newsletter"
    },
    {
      "citation_id": "4",
      "title": "The politics of deceptive borders: 'biomarkers of deceit' and the case of iBorderCtrl",
      "authors": [
        "J Monedero",
        "L Dencik"
      ],
      "year": "2022",
      "venue": "Publisher: Routledge eprint",
      "doi": "10.1080/1369118X.2020.1792530"
    },
    {
      "citation_id": "5",
      "title": "Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A case study of automated video interviews",
      "authors": [
        "B Booth",
        "L Hickman",
        "S Subburaj",
        "L Tay",
        "S Woo",
        "S D'mello"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "6",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Affective Computing for Large-scale Heterogeneous Multimedia Data: A Survey",
      "authors": [
        "S Zhao",
        "S Wang",
        "M Soleymani",
        "D Joshi",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications"
    },
    {
      "citation_id": "8",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Multimodal Affective Computing to Enhance the User Experience of Educational Software Applications",
      "authors": [
        "J Garcia-Garcia",
        "V Penichet",
        "M Lozano",
        "J Garrido",
        "-C Law"
      ],
      "venue": "Mobile Information Systems"
    },
    {
      "citation_id": "10",
      "title": "Unintended Consequences of Biased Robotic and Artificial Intelligence Systems [Ethical, Legal, and Societal Issues]",
      "authors": [
        "L Righetti",
        "R Madhavan",
        "R Chatila"
      ],
      "year": "2019",
      "venue": "IEEE Robotics & Automation Magazine"
    },
    {
      "citation_id": "11",
      "title": "Perspectives from affective science on understanding the nature of emotion",
      "authors": [
        "E Fox"
      ],
      "year": "2018",
      "venue": "Brain and neuroscience advances"
    },
    {
      "citation_id": "12",
      "title": "Gender biases in the training methods of affective computing: Redesign and validation of the Self-Assessment Manikin in measuring emotions via audiovisual clips",
      "authors": [
        "C Sainz-De Baranda Andujar",
        "L Guti E ´rrez-Martín",
        "J Miranda-Calero",
        "M Blanco-Ruiz",
        "C Lo ´pez-Ongil"
      ],
      "year": "2022",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "13",
      "title": "Is gender encoded in the smile? A computational framework for the analysis of the smile driven dynamic face for gender recognition",
      "authors": [
        "H Ugail",
        "A Al-Dahoud"
      ],
      "year": "2018",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "14",
      "title": "Effect of subject's age and gender on face recognition results",
      "authors": [
        "S Wu",
        "D Wang"
      ],
      "year": "2019",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "15",
      "title": "Memory in health and in schizophrenia",
      "authors": [
        "R Gur",
        "R Gur"
      ],
      "year": "2013",
      "venue": "Dialogues in Clinical Neuroscience",
      "doi": "10.31887/DCNS.2013.15.4/rgur"
    },
    {
      "citation_id": "16",
      "title": "Post-traumatic stress disorder",
      "authors": [
        "R Yehuda",
        "C Hoge",
        "A Mcfarlane",
        "E Vermetten",
        "R Lanius",
        "C Nievergelt",
        "S Hobfoll",
        "K Koenen",
        "T Neylan",
        "S Hyman"
      ],
      "year": "2015",
      "venue": "Nature Reviews Disease Primers"
    },
    {
      "citation_id": "17",
      "title": "Age Differences in Emotion Recognition Skills and the Visual Scanning of Emotion Faces",
      "authors": [
        "S Sullivan",
        "T Ruffman",
        "S Hutton"
      ],
      "year": "2007",
      "venue": "The Journals of Gerontology: Series B"
    },
    {
      "citation_id": "18",
      "title": "Diversity and Inclusion in Artificial Intelligence",
      "authors": [
        "E Fosch-Villaronga",
        "A Poulsen"
      ],
      "year": "2022",
      "venue": "Law and Artificial Intelligence: Regulating AI and Applying AI in Legal Practice"
    },
    {
      "citation_id": "19",
      "title": "Verbosity and emotion recognition in older adults",
      "authors": [
        "T Ruffman",
        "J Murray",
        "J Halberstadt",
        "M Taumoepeau"
      ],
      "year": "2010",
      "venue": "Psychology and Aging"
    },
    {
      "citation_id": "20",
      "title": "Female, white, 27? bias evaluation on data and algorithms for affect recognition in faces",
      "authors": [
        "J Pahl",
        "I Rieger",
        "T Wittenberg",
        "U Schmid"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "21",
      "title": "Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems",
      "authors": [
        "A Howard",
        "C Zhang",
        "E Horvitz"
      ],
      "year": "2017",
      "venue": "2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)"
    },
    {
      "citation_id": "22",
      "title": "Emotion Knowledge, Theory of Mind, and Language in Young Children: Testing a Comprehensive Conceptual Model",
      "authors": [
        "E Conte",
        "V Ornaghi",
        "I Grazzani",
        "A Pepe",
        "V Cavioni"
      ],
      "year": "2019",
      "venue": "Emotion Knowledge, Theory of Mind, and Language in Young Children: Testing a Comprehensive Conceptual Model"
    },
    {
      "citation_id": "23",
      "title": "Expressive Virtual Human: Impact of expressive wrinkles and pupillary size on emotion recognition",
      "authors": [
        "A.-S Milcent",
        "E Geslin",
        "A Kadri",
        "S Richir"
      ],
      "year": "2019",
      "venue": "Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, IVA '19"
    },
    {
      "citation_id": "24",
      "title": "Plastic Surgery: A New Dimension to Face Recognition",
      "authors": [
        "R Singh",
        "M Vatsa",
        "H Bhatt",
        "S Bharadwaj",
        "A Noore",
        "S Nooreyezdan"
      ],
      "year": "2010",
      "venue": "Conference Name: IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "25",
      "title": "Emotional and Neuropsychiatric Disorders Associated with Alzheimer's Disease",
      "authors": [
        "K Heilman",
        "S Nadeau"
      ],
      "year": "2022",
      "venue": "Neurotherapeutics"
    },
    {
      "citation_id": "26",
      "title": "Communication changes in parkinson's disease",
      "authors": [
        "N Miller"
      ],
      "year": "2017",
      "venue": "Practical Neurology"
    },
    {
      "citation_id": "27",
      "title": "Emotional face processing and flat affect in schizophrenia: functional and structural neural correlates",
      "authors": [
        "M Lepage",
        "K Sergerie",
        "A Benoit",
        "Y Czechowska",
        "E Dickie",
        "J Armony"
      ],
      "year": "2011",
      "venue": "Psychological Medicine"
    },
    {
      "citation_id": "28",
      "title": "Facial emotion recognition in child psychiatry: A systematic review",
      "authors": [
        "L Collin",
        "J Bindra",
        "M Raju",
        "C Gillberg",
        "H Minnis"
      ],
      "year": "2013",
      "venue": "Research in Developmental Disabilities"
    },
    {
      "citation_id": "29",
      "title": "Putting culture under the 'spotlight'reveals universal information use for face recognition",
      "authors": [
        "R Caldara",
        "X Zhou",
        "S Miellet"
      ],
      "year": "2010",
      "venue": "PLoS One"
    },
    {
      "citation_id": "30",
      "title": "Facial emotion recognition of deaf and hard-of-hearing students for engagement detection using deep learning",
      "authors": [
        "I Lasri",
        "A Riadsolh",
        "M Elbelkacemi"
      ],
      "year": "2023",
      "venue": "Education and Information Technologies"
    },
    {
      "citation_id": "31",
      "title": "Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective",
      "authors": [
        "D Bragg",
        "O Koller",
        "M Bellard",
        "L Berke",
        "P Boudreault",
        "A Braffort",
        "N Caselli",
        "M Huenerfauth",
        "H Kacorri",
        "T Verhoef",
        "C Vogler",
        "M Morris"
      ],
      "year": "2019",
      "venue": "Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS '19"
    },
    {
      "citation_id": "32",
      "title": "Gendered Differences in Face Recognition Accuracy Explained by Hairstyles, Makeup, and Facial Morphology",
      "authors": [
        "V Albiero",
        "K Zhang",
        "M King",
        "K Bowyer"
      ],
      "year": "2022",
      "venue": "Conference Name: IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "33",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "34",
      "title": "Learning from data with structured missingness",
      "authors": [
        "R Mitra",
        "S Mcgough",
        "T Chakraborti",
        "C Holmes",
        "R Copping",
        "N Hagenbuch",
        "S Biedermann",
        "J Noonan",
        "B Lehmann",
        "A Shenvi",
        "X Doan",
        "D Leslie",
        "G Bianconi",
        "R Sanchez-Garcia",
        "A Davies",
        "M Mackintosh",
        "E.-R Andrinopoulou",
        "A Basiri",
        "C Harbron",
        "B Macarthur"
      ],
      "year": "2023",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops"
    },
    {
      "citation_id": "36",
      "title": "Comprehensive Database for Facial Expression Analysis",
      "authors": [
        "T Kanade",
        "Y Tian",
        "J Cohn"
      ],
      "year": "2000",
      "venue": "Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition 2000, FG '00"
    },
    {
      "citation_id": "37",
      "title": "Biographies, Bollywood, Boomboxes and Blenders: Domain Adaptation for Sentiment Classification",
      "authors": [
        "J Blitzer",
        "M Dredze",
        "F Pereira"
      ],
      "year": "2007",
      "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Confidence-weighted linear classification",
      "authors": [
        "M Dredze",
        "K Crammer",
        "F Pereira"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning, ICML '08"
    },
    {
      "citation_id": "39",
      "title": "Learning Word Vectors for Sentiment Analysis",
      "authors": [
        "A Maas",
        "R Daly",
        "P Pham",
        "D Huang",
        "A Ng",
        "C Potts"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "40",
      "title": "EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "Conference Name: IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Affective computing, emotional development, and autism",
      "authors": [
        "D Messinger",
        "L Duvivier",
        "Z Warren",
        "M Mahoor",
        "J Baker",
        "A Warlaumont",
        "P Ruvolo"
      ],
      "year": "2015",
      "venue": "The Oxford handbook of affective computing, Oxford library of psychology"
    },
    {
      "citation_id": "43",
      "title": "Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Mar ´ın-Morales",
        "J Higuera-Trujillo",
        "A Greco",
        "J Guixeres",
        "C Llinares",
        "E Scilingo",
        "M Alcan",
        "G Valenza"
      ],
      "year": "2018",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "44",
      "title": "The Effect of Motivational Learning Companions on Low Achieving Students and Students with Disabilities",
      "authors": [
        "B Woolf",
        "I Arroyo",
        "K Muldner",
        "W Burleson",
        "D Cooper",
        "R Dolan",
        "R Christopherson"
      ],
      "year": "2010",
      "venue": "Intelligent Tutoring Systems"
    },
    {
      "citation_id": "45",
      "title": "EMMA: An Emotion-Aware Wellbeing Chatbot",
      "authors": [
        "A Ghandeharioun",
        "D Mcduff",
        "M Czerwinski",
        "K Rowan"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "46",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Journal Abbreviation: 9th European Conference on Speech Communication and Technology Pages: 1520 Publication Title: 9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "47",
      "title": "The Belfast Induced Natural Emotion Database",
      "authors": [
        "I Sneddon",
        "M Mcrorie",
        "G Mckeown",
        "J Hanratty"
      ],
      "year": "2012",
      "venue": "Conference Name: IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "A Bimodal Face and Body Gesture Database for Automatic Analysis of Human Nonverbal Affective Behavior",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2006",
      "venue": "18th International Conference on Pattern Recognition (ICPR'06)"
    },
    {
      "citation_id": "49",
      "title": "Gesture and emotion: Can basic gestural form features discriminate emotions?",
      "authors": [
        "M Kipp",
        "J.-C Martin"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "50",
      "title": "Introducing the Geneva Multimodal expression corpus for experimental research on emotion perception",
      "authors": [
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "51",
      "title": "Emilya: Emotional body expression in daily actions database",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "52",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "53",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC"
    },
    {
      "citation_id": "54",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietik A ¨inen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "55",
      "title": "A 3D facial expression database for facial behavior research",
      "authors": [
        "L Yin",
        "X Wei",
        "Y Sun",
        "J Wang",
        "M Rosato"
      ],
      "year": "2006",
      "venue": "7th International Conference on Automatic Face and Gesture Recognition (FGR06)"
    },
    {
      "citation_id": "56",
      "title": "A high-resolution 3d dynamic facial expression database",
      "authors": [
        "L Yin",
        "X Chen",
        "Y Sun",
        "T Worm",
        "M Reale"
      ],
      "year": "2008",
      "venue": "2008 8th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "57",
      "title": "BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "58",
      "title": "4dfab: A large scale 4d database for facial expression analysis and biometric applications",
      "authors": [
        "S Cheng",
        "I Kotsia",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "A Spontaneous Micro-expression Database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietik A ¨inen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "60",
      "title": "CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "61",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "64",
      "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2021",
      "venue": "Conference Name: IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI '18"
    },
    {
      "citation_id": "66",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "67",
      "title": "The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations",
      "authors": [
        "A Metallinou",
        "Z Yang",
        "C -C. Lee",
        "C Busso",
        "S Carnicke",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "68",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Conference Name: IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "70",
      "title": "DECAF: MEG-Based Multimodal Database for Decoding Affective Physiological Responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "Conference Name: IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Representational bias in expression and annotation of emotions in audiovisual databases",
      "authors": [
        "W Saakyan",
        "O Hakobyan",
        "H Drimalla"
      ],
      "year": "2021",
      "venue": "CAIP 2021: Proceedings of the 1st International Conference on AI for People: Towards Sustainable AI"
    }
  ]
}