{
  "paper_id": "2503.21480v2",
  "title": "Omnivox: Zero-Shot Emotion Recognition With Omni-Llms",
  "published": "2025-03-27T13:12:49Z",
  "authors": [
    "John Murzaku",
    "Owen Rambow"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The use of omni-LLMs (large language models that accept any modality as input), particularly for multimodal cognitive state tasks involving speech, is understudied. We present OmniVox, the first systematic evaluation of four omni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely used multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot omni-LLMs outperform or are competitive with fine-tuned audio models. Alongside our audio-only evaluation, we also evaluate omni-LLMs on text only and text and audio. We present acoustic prompting, an audio-specific prompting strategy for omni-LLMs which focuses on acoustic feature analysis, conversation context analysis, and step-by-step reasoning. We compare our acoustic prompting to minimal prompting and full chain-of-thought prompting techniques. We perform a context window analysis on IEMOCAP and MELD, and find that using context helps, especially on IEMOCAP. We conclude with an error analysis on the generated acoustic reasoning outputs from the omni-LLMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The Emotion Recognition in Conversation (ERC) task, a subset of broader cognitive state modelling tasks, has received a significant amount of attention from both the NLP and speech processing communities. Many corpora have been created, involving speech, text transcripts, and videos as modalities, and these corpora have been used to explore unimodal or multimodal architectures. The previous works often report results, usually fine-tuned and tested on one modality (mainly text or speech); however, these studies do not have a unifying approach. In other words, is there a task-agnostic, modality-agnostic, generalized model that can perform well on the ERC task in a zero-shot manner? Recently, there has been an emergence of omni-LLMs such as Gemini  (Team et al., 2024; Google, 2024) ,  GPT-4o (OpenAI, 2024) , and Phi-4-Multimodal  (Abouelenin et al., 2025) , which are models that accept any modality as input, and either output a single modality or combination of modalities. On March 26th, 2025, Qwen-2.5-Omni, a new omni-LLM was released  (Team, 2025) , which we do not evaluate on due to its recent release date. This model is the successor to Qwen-Audio  (Chu et al., 2023)  and Qwen-2-Audio  (Chu et al., 2024) , whose multi-task pre-training included emotion recognition data.\n\nOur study aims to discover the emergent audio capabilities of omni-LLMs for ERC. While omni-LLMs inherently accept multiple modalities  (text, speech, image)  as input, we prioritize audio-to-text inference, which specifically translates raw audio inputs directly into text emotion labels. We emphasize that this is a previously unexplored direction. We also present results on additional modalities, including text and combined text-speech inputs as complementary analyses to further enrich our findings.\n\nOur main contributions are summarized as follows:\n\n• To the best of our knowledge, we are the first to perform a systematic evaluation on four omni-LLMs for zero-shot emotion recognition from audio only. Compared to previous audio-only baselines, we find that our zero-shot method outperforms state-of-the-art fine-tuned audio model baselines (up to 7.6% improvement for IEMOCAP, 4.7% improvement for MELD), or are very competitive compared to baselines (2.9% lower for IEMOCAP and 0.7% lower for MELD). We also evaluate on text only and both text and speech as inputs.\n\n• We present results on how to prompt omni-LLMs with audio input for the emotion task with three different zero-shot prompting strategies: (i) minimal, where we instruct the model to only predict an emotion; (ii) Acoustic, where we instruct the model to perform an acoustic analysis then predict the emotion; (iii) chainof-thought (CoT), where we instruct the model to perform an acoustic analysis, perform a step-by-step reasoning, and then output the label.\n\n• We present results on how many turns of audio context helps. We specifically report results on no context, and various contexts, up to 12 turns. Our results serve as a preliminary analysis into how many turns of dialogues omni-LLMs can track.\n\n• We perform a comprehensive error analysis on where audio models fail for emotion, particularly focusing on acoustic reasoning. We conclude with insights for future emotion tasks, and present a general discussion on how to best use omni-LLMs.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Fine-tuned Models for ERC Many papers have focused on fine-tuned multimodal (text, audio, vision) models for ERC. Most recently, there has been multiple papers  (Shou et al., 2024; Zhao et al., 2025)  achieving state-of-the-art (SOTA) results on IEMOCAP and MELD using multimodal state space models, specifically the Mamba architecture  (Gu & Dao, 2023) .\n\nSimilarly, with the exploding popularity of large language models (LLMs), previous papers have fine-tuned LLMs,  (Lei et al., 2023; Wu et al., 2024)  used LLM supervised pretraining  (Dutta & Ganapathy, 2025) , all yielding SOTA results when the papers released. Finally, the majority of recent ERC work optimizes for fusion network architectures: teacher-student fusion networks  (Yun et al., 2024) , graph neural network (GNN) based architectures  (Meng et al., 2024) , feature alignment fusion networks  (Wang et al., 2025) , multimodal transformer fusion networks  (Cheng et al., 2024; Sasu et al., 2025) , and early/late fusion with TTS generated emotional speech  (Soubki et al., 2025) .\n\nZero-shot ERC Conversely, there has not been as much attention for the zero-shot ERC task; for the work that exists, the focus is only on the text modality. Most recently,  Wu et al. (2024)  perform zero-shot experiments on IEMOCAP and MELD with Claude-3.5 Sonnet  (Anthropic, 2025) . However, this is on the text-only modality, with descriptions of speech features in natural language.  Lei et al. (2023)  similarly performed zero-shot experiments with  GPT-3.5 (OpenAI, 2022) , but only on text transcripts of IEMOCAP and MELD.\n\nOur work distinguishes itself from previous research in two salient ways: first, we use a multimodal framework with a unifying omni-LLM, using text, audio, or both text and audio (with particular emphasis placed on audio). Second, and most critically, our entire approach is zero-shot, with a single modality-agnostic omni-LLM. This zero-shot multimodal paradigm positions our approach uniquely within the ERC literature: we are the first to explore the zero-shot ERC task with audio only, and we are the first to offer insights and a framework for working with omni-LLMs.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We use two widely used multimodal emotion recognition corpora: IEMOCAP We use the IEMOCAP (Interactive Emotional Dyadic Motion Capture) corpus  (Busso et al., 2008) , which consists of nine emotion labels: neutral, anger, frustration, happiness, excitement, sadness, fear, surprise, and other. The corpus consists of dyadic conversations among paid actors and is annotated on the utterance level. Regarding modalities, the corpus includes Figure  1 : The proposed OmniVox framework. We perform zero-shot emotion recognition from audio inputs enhanced by text instructions, and optional contextual information or transcripts. We then generate a context analysis, acoustic feature interpretation, and a final chain-of-thought reasoning, ultimately predicting a specific emotion label (e.g., sad in this example).\n\nvideo, speech, motion capture of face, and text transcriptions; we only use the speech and the text transcriptions. Regarding labels, we collapse the label set to six labels (anger, happiness, excitement, sadness, frustration, and neutral), following the most recent work using IEMOCAP  (Gong et al., 2024; Wu et al., 2024) .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Meld",
      "text": "We also use the MELD (Multimodal EmotionLines Dataset) corpus  (Poria et al., 2018) , which contains naturally occurring dialogues from the TV series Friends. The corpus is annotated with seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and surprise. Similar to IEMOCAP, MELD also includes speech, video, and text transcriptions as modalities, and we again only use the speech and the text transcriptions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models",
      "text": "We provide a brief overview of the omni-LLM models that we use in this paper. A summary of the supported input and output modalities of each model is shown in Appendix Table  6 .\n\nGemini 2.0 We use the Gemini-2.0-Flash (henceforth Gemini) and the Gemini-2.0-Flash-Lite (henceforth Gemini-Lite) models through the Google Gemini API  (Google, 2024) . The Gemini-2.0 series of models are optimized for multimodal understanding across many tasks spanning text, audio, video, and image, outperforming the Gemini-1.5 models  Team et al. (2024) . We also perform experiments on Gemini-Lite, which provides faster runtimes and cost efficiency. Table  6  shows the modalities of both models: both Gemini and Gemini-Lite accept any modality as input, but Gemini outputs any modality (with future API releases allowing audio), while Gemini-Lite only allows text output.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gpt-4O (Audio)",
      "text": "We perform experiments on the audio checkpoint of  GPT-4o (OpenAI, 2024) , specifically Gpt-4o-Audio-preview. While GPT-4o is reported as an omni model, the audio modality has a distinct API endpoint (that is, we cannot use the standard GPT-4o endpoint). Table  6  shows the allowed input modalities: GPT-4o accepts audio and/or text (with mandatory audio input), and can output either text, audio, or text and audio.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Phi-4-Multimodal",
      "text": "We use the newly released Phi-4-Multimodal-Instruct model  (Abouelenin et al., 2025) . The model consists of 5.6B parameters, with Phi-4-Mini-Instruct as the language model, and separate vision and speech encoders. Table  6  shows the allowed input modalities: Phi-4-Multimodal-Instruct accepts audio, text, or image (or any combination of the three), and only outputs text.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Setup",
      "text": "We describe our task setup shown in Figure  1 .\n\nStep 1: Input. We begin by building our inputs to the omni-LLM. We begin with our audio file as input, followed by the text instruction (we describe the text instruction in the following subsection). Optionally, we include audio and/or text context (up to 12 previous turns of dialogue), and an optional text transcript.\n\nStep 2: Prompt. We prompt the omni-LLM with the input from Step 1. For our main prompt, the omni-LLM then generates a context analysis of the conversation (if context is provided), an acoustic analysis detailing features such as pitch, pace, and volume, and a step-by-step reasoning. We discuss further prompts and strategies in Section 3.4.\n\nStep 3: Output. The final output consists of one of the corpus specific emotion labels. We note that this final label comes after the analyses (and is therefore outputted at the end of the one single LLM call in Step 2).\n\nIn all of our main results (Section 4.1), we use a chain-of-thought (CoT) prompting approach, with the OmniVox framework mentioned above. Further, for most of our experiments, we follow a conversation context window of three turns (c=3), following  Wu et al. (2024) , who used the last three turns of context with text acoustic features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prompting Strategies",
      "text": "We provide a brief overview of the three prompting strategies we use. We include the full prompts in Appendix E. We remind the reader that our prompts are system instructions given to the omni-LLMs, and the omni-LLMs primarily take in an audio utterance as input (and optionally: audio context, text context, or a text transcript of the utterance).\n\nMinimal Our minimal prompt aims to classify emotion in the simplest way possible. To that end, our prompt consists of a simple instruction asking to classify the emotion in the audio and a list of which labels to use.\n\nAcoustic Our Acoustic prompt adds an initial reasoning step to our model. Specifically, we ask the model to first provide an acoustic analysis of the audio, then predict the final emotion label.\n\nChain-of-Thought (CoT) Our CoT prompt uses chain-of-thought prompting  (Wei et al., 2022) , explicitly instructing the model to reason step-by-step. We first include the instruction to perform an acoustic analysis from our Acoustic prompt, and then add an instruction to perform a final CoT reasoning step.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "We perform a weighted-F1 (W-F1) evaluation for both the test sets of IEMOCAP and MELD following the most recent works  (Wu et al., 2024; Yun et al., 2024; Shou et al., 2024) . We note that MELD contains an imbalanced label distribution, with a majority label of neutral (about 48.1% of the test set). We include an expanded confusion analysis in Section 5.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "We split our experiments into four subsections: Section 4.1 begins with results on each of the modalities (audio, text, text+audio). Section 4.2 focuses on prompting strategies for omni-LLMs, exclusively using the audio modality. Section 4.3 performs an analysis on how many turns of context help for audio-only experiments. Finally, Section 4.4 compares OmniVox's results on all modalities to previously reported results.   2024 )). Highlighted cells highlight the best performing configuration for each model.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Results",
      "text": "We describe the multimodal results per corpus. We use the chain-of-thought prompt mentioned in Section 3.4. Our results are shown in Table  1 . We also show examples of our LLM outputs from IEMOCAP and MELD in Appendix C.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iemocap.",
      "text": "Audio-only, with context (c=3) results beat all other modalities for IEMOCAP, for three models. Phi-4 is an outlier, consistently performing worse than the other models under all conditions, and showing a different pattern for its best result.\n\nAudio GPT-4o-audio with context (c=3) achieves the highest W-F1 (51.8) over all conditions and models, outperforming Gemini Lite (51.5) and Gemini (49.9). In all cases for the audio modality besides Phi-4, we find context helps. However, with no-context conditions (c=0), performance for GPT-4o-audio and Gemini is comparable (45.8 and 45.6, respectively).\n\nText Both Gemini and Gemini Lite show large gains with context added compared to no context (increase of 5.0% for Gemini, and 5.5% for Gemini-Lite) . However, our text-only results still are below the audio results.\n\nText+Audio GPT-4o-audio performs best with context (c=3), achieving a W-F1 score of 47.6, clearly benefiting from context integration. However, it performs worse than the audio-only modality than some models with no context (c=0), and all models with context (c=3).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Meld.",
      "text": "For MELD, unlike IEMOCAP, the text modality did increase the performance, with Gemini achieving the highest overall W-F1 score on Text only, and Gemini-Lite and Gpt-4o-Audio performing best on Text+Audio, all with context (c=3). Again, Phi-4 is an outlier, consistently performing worse than the other models, and again showing a different pattern for its best result.\n\nAudio Unlike IEMOCAP, GPT-4o-audio achieves the highest W-F1 (51.3) in the no-context condition (c=0), substantially outperforming both Gemini (43.7) and Gemini Lite (44.2). Interestingly, when context is added (c=3), GPT-4o-audio's performance decreases (to 47.7) while Gemini improves (to 48.9).\n\nText Gemini achieves the highest overall W-F1 score (62.8) with context (c=3), showing a modest improvement (1.6%) over the no-context condition. Gemini consistently outperforms Gemini Lite in both conditions.\n\nText+Audio Gemini-Lite performs better than text-only with context (1.4% increase) and audio-only with context (14% increase) and Gpt-4o-Audio performs better than audio-only by a large margin (14% increase).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Prompting Strategies",
      "text": "Table  1  emphasizes that the Gold Feats. column, which uses the text descriptions of gold acoustic features from  Wu et al. (2024) , achieves better or competitive results compared to audio-only. We aim to investigate the following question: does explicitly prompting for acoustic features help omni-LLMs for the audio-only modality? We present results for three different prompting strategies Minimal, Acoustic, and Chain-of-Thought (CoT) in Table  2 .\n\nIEMOCAP We find that explicitly prompting for acoustic analyses consistently enhanced model performance compared to minimal prompts. Specifically, Gemini achieved notable improvements from 45.0% to 48.1% (without context) and from 51.7% to 51.9% (with context). Similarly, GPT-4o-Audio improved its weighted F1 scores from 47.0% to 48.4% (without context) and 47.5% to 50.5% (with context). Phi-4-Multimodal yielded the largest performance gain gains, particularly when context was included, improving from 24.4% (minimal) to 42.6% (acoustic). Our CoT prompting method further enhanced Gemini-Lite and GPT-4o-Audio performances when context was provided, reaching 51.5% and 51.8% respectively.\n\nMELD Similar to our results for IEMOCAP, prompting for acoustic analyses also led to consistent improvements across models, especially without context. Gemini's weighted F1 score increased markedly from 35.8% to 42.5%, and GPT-4o-Audio yielded substantial gains from 31.7% to 50.5% in acoustic conditions. Our CoT prompting further improved performance, with GPT-4o-Audio achieving 51.3% without context and Gemini attaining 48.9% when context was included. Phi-4 displayed consistent improvement across prompt types, moving from 21.8% (minimal) to 36.3% (CoT) without context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Summary",
      "text": "Our results suggest that acoustic information contributes to emotion recognition accuracy across both IEMOCAP and MELD. Further, CoT generally enhances performance, especially when contextual information is provided.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Context Window",
      "text": "We aim to answer the following question: how many turns of audio context help? In other words, is there an optimal amount of turns for omni-LLMs to understand the emotion and contextual dynamics across a conversation, purely from audio? Table  3  shows our per-corpus results, using the CoT style prompt. We use context windows from 0 -5, 12: our maximum window choice mirror  Wu et al. (2024) , who use context windows of 12 turns.\n\nIEMOCAP Our results show that incorporating audio context enhances the emotional recognition performance for IEMOCAP across all evaluated models. Gemini improves from our c=0 baseline of 45.9% to 53.1% with a context window of 4 utterances. Gemini-Lite and Gpt-4o-Audio show similar trends, achieving peak performances of 54.1% and 55.9% at context windows of 12, respectively. MELD The effectiveness of audio context varies between models. While Gemini and Gemini-Lite exhibit clear performance improvements when context is introduced-reaching peak performance at 48.8% and 48.6% respectively, Gpt-4o-Audio shows a decline from its baseline of 51.3% when context is introduced. Phi-4-multimodal shows only marginal and inconsistent improvements across different context sizes. We hypothesize that this could be due to omni-LLMs failing with more complex audio: MELD has multi-party TV show dialogues with background noise, audience laughter, and varying conditions, whereas IEMOCAP has better quality, dyadic audio recordings in a lab setting. We quantify this using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs (14.35dB for MELD vs 15.86dB for IEMOCAP), however, MELD shows 56% higher standard deviation (8.82dB vs 5.64dB).   (Lei et al., 2023; Wu et al., 2024; Shou et al., 2024; Yun et al., 2024) . All values shown are W-F1 scores. Best results in each category are bolded. Fine-tuned results worse than our zero-shot OmniVox are in red.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison To Previous Work",
      "text": "We conclude by comparing our top performing approaches (GPT-4o-audio, c=12 for IEMO-CAP, and GPT-4o-audio, c=0 for MELD) to four recent works, which evaluated on audio, text (or, speech features as text), and text and audio. Our results are shown in Table  4 .\n\nFor both IEMOCAP and MELD, our approach achieves competitive audio-only results (55.9% for IEMOCAP, 51.3% for MELD), surpassing several fine-tuned models (shown in red), although falling behind the highest reported audio result from  Shou et al. (2024) .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis",
      "text": "We perform an error analysis for the audio only modality on each corpus using the top performing model configuration (GPT-4o-audio with c=12 for IEMOCAP, GPT-4o-Audio, c=0 for MELD). We split our error analysis into three motivating research questions:\n\nRQ1: Are there specific emotion pairs that omni-LLMs frequently confuse? If yes, are they aligned with confusion patterns from previous work?",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rq2:",
      "text": "In Section 4, we saw that explicitly prompting for acoustic descriptions help. To what extent do the LLM generated acoustic descriptions match the ground truth acoustic descriptions?\n\nRQ3: Combining RQ1 and RQ2, do the emotion pairs that the omni-LLM most frequently misclassified also systematically exhibit discrepancies in their acoustic feature descriptions (e.g., LLM predicts \"high volume\", but ground truth is \"low volume\")?",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rq1: Confusion Analysis",
      "text": "IEMOCAP The most notable confusion occurred when angry was misclassified as frustrated (61% error rate). Other significant misclassifications included cases where the omni-LLM defaulted to neutral: frustrated predicted as neutral (23% error rate), excited predicted as neutral (22% error rate), sad predicted as neutral (25% error rate). Our findings are consistent with previous works  (Hu et al., 2023; Wu et al., 2024; Yun et al., 2024) , although we note we have a higher error rate on angry misclassified as frustrated.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Meld",
      "text": "The most significant misclassifications in MELD occurred when sadness was misclassified as neutral, with a 64% error rate. Other notable confusions also are primarily due to the omni-LLM frequently defaulting predictions to neutral: joy predicted as neutral (42% error rate), anger predicted as neutral (41% error rate), surprise predicted as neutral (44% error rate). Our findings again align with previous research  (Hu et al., 2023; Yun et al., 2024) , where both noticed neutral defaulting for the MELD corpus.\n\nSummary Our confusion analysis reveals consistent patterns among both IEMOCAP and MELD, namely the neutral defaulting. However, across both corpora, we closely align with confusion patterns observed in fine-tuned models.\n\nomni-LLMs not only rival, but even sometimes surpass fine-tuned audio models on the emotion corpora of IEMOCAP and MELD. We present multiple prompting strategies and find that adding acoustic analysis helps omni-LLMs. We find that for IEMOCAP, adding context helps, whereas for MELD, the results are not as consistent, particularly for GPT-4o-Audio. We conclude with an error analysis, particularly focusing on the generated acoustic descriptions. We find that omni-LLMs do not perfectly match with ground truth descriptions, and furthermore, confusion patterns are closely tied to acoustic analysis mismatches.\n\nExample 1: IEMOCAP. Gemini-2.0. With Context",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conversation :",
      "text": "S p e a k e r 0 : A l l t h e t ime , e v e r y day , e v e r y day . They h a v e e-m a i l o v e r t h e r e and s t u f f l i k e t h a t , r i g h t ? I can s e n d you p i c t u r e s .\n\nS p e a k e r 1 : \" Yeah , a s much a s I can . \"\n\nS p e a k e r 0 : \" Yeah . \" He m e n t i o n s c a l l i n g someone , f i g u r i n g s o m e t h i n g out , and s e n d i n g p i c t u r e s v i a e m a i l .\n\nA c o u s t i c A n a l y s i s : A l l r i g h t , j u s t l i s t e n .\n\nA c o u s t i c A n a l y s i s : The t o n e o f t h e s p e a k e r i s f i r m and commanding . The p i t c h i s r e l a t i v e l y s t e a d y , w i t h o u t s i g n i f i c a n t f l u c t u a t i o n s . The rhythm i s c o n t r o l l e d , and t h e i n t e n s i t y i s moderate , s u g g e s t i n g a l e v e l o f a s s e r t i v e n e s s . S p e a k e r 1 : No , j u s t k n o w i n g you ' r e gonna be t h e r e i s enough .\n\nS p e a k e r 0 : A l r i g h t , w e l l , be good , I l o v e you .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Minimal Prompt",
      "text": "You ' l l h e a r s e v e r a l a u d i o c l i p s from a c o n v e r s a t i o n .",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition",
      "page": 3
    },
    {
      "caption": "Figure 1: Step 1: Input. We begin by building our inputs to the omni-LLM. We begin with our",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs": "John Murzaku1, 3, Owen Rambow2, 3"
        },
        {
          "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs": "1Department of Computer Science"
        },
        {
          "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs": "2Department of Linguistics"
        },
        {
          "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs": "3Institute for Advanced Computational Science"
        },
        {
          "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs": "Stony Brook University"
        },
        {
          "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs": "jmurzaku@cs.stonybrook.edu"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "The use of omni-LLMs (large language models that accept any modality as"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "is understudied. We present OmniVox, the first systematic evaluation of"
        },
        {
          "Abstract": "four omni-LLMs on the zero-shot emotion recognition task. We evaluate"
        },
        {
          "Abstract": "on two widely used multimodal emotion benchmarks:"
        },
        {
          "Abstract": "MELD, and find zero-shot omni-LLMs outperform or are competitive with"
        },
        {
          "Abstract": "fine-tuned audio models. Alongside our audio-only evaluation, we also"
        },
        {
          "Abstract": "evaluate omni-LLMs on text only and text and audio. We present acoustic"
        },
        {
          "Abstract": "prompting, an audio-specific prompting strategy for omni-LLMs which"
        },
        {
          "Abstract": "focuses on acoustic feature analysis, conversation context analysis, and"
        },
        {
          "Abstract": "step-by-step reasoning. We compare our acoustic prompting to minimal"
        },
        {
          "Abstract": "prompting and full chain-of-thought prompting techniques. We perform"
        },
        {
          "Abstract": "a context window analysis on IEMOCAP and MELD, and find that using"
        },
        {
          "Abstract": "context helps, especially on IEMOCAP. We conclude with an error analysis"
        },
        {
          "Abstract": "on the generated acoustic reasoning outputs from the omni-LLMs."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "",
          "Related Work": "Fine-tuned Models for ERC Many papers have focused on fine-tuned multimodal (text,"
        },
        {
          "2": "",
          "Related Work": "audio, vision) models for ERC. Most recently, there has been multiple papers (Shou et al.,"
        },
        {
          "2": "",
          "Related Work": "2024; Zhao et al., 2025) achieving state-of-the-art (SOTA) results on IEMOCAP and MELD"
        },
        {
          "2": "",
          "Related Work": "using multimodal state space models, specifically the Mamba architecture (Gu & Dao, 2023)."
        },
        {
          "2": "",
          "Related Work": "Similarly, with the exploding popularity of large language models (LLMs), previous papers"
        },
        {
          "2": "",
          "Related Work": "have fine-tuned LLMs, (Lei et al., 2023; Wu et al., 2024) used LLM supervised pretraining"
        },
        {
          "2": "",
          "Related Work": "(Dutta & Ganapathy, 2025), all yielding SOTA results when the papers released. Finally, the"
        },
        {
          "2": "",
          "Related Work": "majority of recent ERC work optimizes for fusion network architectures: teacher-student"
        },
        {
          "2": "",
          "Related Work": "fusion networks (Yun et al., 2024), graph neural network (GNN) based architectures (Meng"
        },
        {
          "2": "",
          "Related Work": "et al., 2024), feature alignment fusion networks (Wang et al., 2025), multimodal transformer"
        },
        {
          "2": "",
          "Related Work": "fusion networks (Cheng et al., 2024; Sasu et al., 2025), and early/late fusion with TTS"
        },
        {
          "2": "generated emotional speech (Soubki et al., 2025).",
          "Related Work": ""
        },
        {
          "2": "",
          "Related Work": "Zero-shot ERC Conversely, there has not been as much attention for the zero-shot ERC"
        },
        {
          "2": "",
          "Related Work": "task; for the work that exists, the focus is only on the text modality. Most recently, Wu et al."
        },
        {
          "2": "",
          "Related Work": "(2024) perform zero-shot experiments on IEMOCAP and MELD with Claude-3.5 Sonnet"
        },
        {
          "2": "",
          "Related Work": "(Anthropic, 2025). However, this is on the text-only modality, with descriptions of speech"
        },
        {
          "2": "",
          "Related Work": "features in natural language. Lei et al. (2023) similarly performed zero-shot experiments"
        },
        {
          "2": "with GPT-3.5 (OpenAI, 2022), but only on text transcripts of IEMOCAP and MELD.",
          "Related Work": ""
        },
        {
          "2": "",
          "Related Work": "Our work distinguishes itself from previous research in two salient ways: first, we use"
        },
        {
          "2": "",
          "Related Work": "a multimodal framework with a unifying omni-LLM, using text, audio, or both text and"
        },
        {
          "2": "",
          "Related Work": "audio (with particular emphasis placed on audio). Second, and most critically, our entire ap-"
        },
        {
          "2": "",
          "Related Work": "proach is zero-shot, with a single modality-agnostic omni-LLM. This zero-shot multimodal"
        },
        {
          "2": "",
          "Related Work": "paradigm positions our approach uniquely within the ERC literature: we are the first to"
        },
        {
          "2": "",
          "Related Work": "explore the zero-shot ERC task with audio only, and we are the first to offer insights and a"
        },
        {
          "2": "framework for working with omni-LLMs.",
          "Related Work": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "from audio inputs enhanced by text instructions, and optional contextual information or"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "transcripts. We then generate a context analysis, acoustic feature interpretation, and a final"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "chain-of-thought reasoning, ultimately predicting a specific emotion label (e.g., sad in this"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": ""
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "face, and text"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "and the text transcriptions. Regarding labels, we collapse the label set to six labels (anger,"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "happiness, excitement, sadness, frustration, and neutral), following the most recent work"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "using IEMOCAP (Gong et al., 2024; Wu et al., 2024)."
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "MELD We also use the MELD (Multimodal EmotionLines Dataset) corpus (Poria et al.,"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "2018), which contains naturally occurring dialogues from the TV series Friends. The corpus"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "is annotated with seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "surprise. Similar to IEMOCAP, MELD also includes speech, video, and text transcriptions"
        },
        {
          "Figure 1: The proposed OmniVox framework. We perform zero-shot emotion recognition": "as modalities, and we again only use the speech and the text transcriptions."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3\nTask Setup": "We describe our task setup shown in Figure 1."
        },
        {
          "3.3\nTask Setup": "Step 1:\nInput. We begin by building our inputs to the omni-LLM. We begin with our"
        },
        {
          "3.3\nTask Setup": "audio file as input, followed by the text instruction (we describe the text instruction in the"
        },
        {
          "3.3\nTask Setup": "following subsection). Optionally, we include audio and/or text context (up to 12 previous"
        },
        {
          "3.3\nTask Setup": "turns of dialogue), and an optional text transcript."
        },
        {
          "3.3\nTask Setup": "Step 2: Prompt. We prompt the omni-LLM with the input from Step 1. For our main prompt,"
        },
        {
          "3.3\nTask Setup": "the omni-LLM then generates a context analysis of the conversation (if context is provided),"
        },
        {
          "3.3\nTask Setup": "an acoustic analysis detailing features such as pitch, pace, and volume, and a step-by-step"
        },
        {
          "3.3\nTask Setup": "reasoning. We discuss further prompts and strategies in Section 3.4."
        },
        {
          "3.3\nTask Setup": "Step 3: Output. The final output consists of one of the corpus specific emotion labels. We"
        },
        {
          "3.3\nTask Setup": "note that this final label comes after the analyses (and is therefore outputted at the end of"
        },
        {
          "3.3\nTask Setup": "the one single LLM call in Step 2)."
        },
        {
          "3.3\nTask Setup": "In all of our main results (Section 4.1), we use a chain-of-thought (CoT) prompting approach,"
        },
        {
          "3.3\nTask Setup": "with the OmniVox framework mentioned above. Further, for most of our experiments, we"
        },
        {
          "3.3\nTask Setup": "follow a conversation context window of three turns (c=3), following Wu et al. (2024), who"
        },
        {
          "3.3\nTask Setup": "used the last three turns of context with text acoustic features."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Model": "",
          "Audio": "c=3",
          "Text": "c=3",
          "Text + Audio": "c=0",
          "Gold Feats.": ""
        },
        {
          "Dataset": "",
          "Model": "Gemini",
          "Audio": "49.9",
          "Text": "44.2",
          "Text + Audio": "45.9",
          "Gold Feats.": "52.7"
        },
        {
          "Dataset": "",
          "Model": "Gemini Lite",
          "Audio": "51.5",
          "Text": "42.5",
          "Text + Audio": "42.1",
          "Gold Feats.": "49.9"
        },
        {
          "Dataset": "IEMOCAP",
          "Model": "",
          "Audio": "",
          "Text": "",
          "Text + Audio": "",
          "Gold Feats.": ""
        },
        {
          "Dataset": "",
          "Model": "Gpt-4o-Audio",
          "Audio": "51.8",
          "Text": "–",
          "Text + Audio": "44.7",
          "Gold Feats.": "–"
        },
        {
          "Dataset": "",
          "Model": "Phi-4-Multimodal",
          "Audio": "37.6",
          "Text": "–",
          "Text + Audio": "37.9",
          "Gold Feats.": "–"
        },
        {
          "Dataset": "",
          "Model": "Gemini",
          "Audio": "48.9",
          "Text": "62.8",
          "Text + Audio": "58.5",
          "Gold Feats.": "54.1"
        },
        {
          "Dataset": "",
          "Model": "Gemini Lite",
          "Audio": "47.8",
          "Text": "60.4",
          "Text + Audio": "57.6",
          "Gold Feats.": "42.5"
        },
        {
          "Dataset": "MELD",
          "Model": "",
          "Audio": "",
          "Text": "",
          "Text + Audio": "",
          "Gold Feats.": ""
        },
        {
          "Dataset": "",
          "Model": "Gpt-4o-Audio",
          "Audio": "47.7",
          "Text": "–",
          "Text + Audio": "61.0",
          "Gold Feats.": "–"
        },
        {
          "Dataset": "",
          "Model": "Phi-4-Multimodal",
          "Audio": "35.3",
          "Text": "–",
          "Text + Audio": "43.4",
          "Gold Feats.": "–"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: shows our",
      "data": [
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "48.1",
          "MELD": "42.5"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "51.9",
          "MELD": "46.2"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "43.5",
          "MELD": "48.2"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "44.6",
          "MELD": "45.9"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "48.4",
          "MELD": "50.5"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "50.5",
          "MELD": "45.7"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "29.2",
          "MELD": "31.5"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "42.6",
          "MELD": "34.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Model": "",
          "Context Window Size (c)": "2"
        },
        {
          "Dataset": "",
          "Model": "Gemini",
          "Context Window Size (c)": "48.5"
        },
        {
          "Dataset": "",
          "Model": "Gemini-Lite",
          "Context Window Size (c)": "47.4"
        },
        {
          "Dataset": "IEMOCAP",
          "Model": "",
          "Context Window Size (c)": ""
        },
        {
          "Dataset": "",
          "Model": "Gpt-4o-Audio",
          "Context Window Size (c)": "51.1"
        },
        {
          "Dataset": "",
          "Model": "phi-4-multimodal",
          "Context Window Size (c)": "35.8"
        },
        {
          "Dataset": "",
          "Model": "Gemini",
          "Context Window Size (c)": "48.0"
        },
        {
          "Dataset": "",
          "Model": "Gemini-Lite",
          "Context Window Size (c)": "47.9"
        },
        {
          "Dataset": "MELD",
          "Model": "",
          "Context Window Size (c)": ""
        },
        {
          "Dataset": "",
          "Model": "Gpt-4o-Audio",
          "Context Window Size (c)": "48.2"
        },
        {
          "Dataset": "",
          "Model": "phi-4-multimodal",
          "Context Window Size (c)": "35.9"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Gemini-Lite": "",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "Gpt-4o-Audio",
          "43.6": "51.3",
          "48.6": "48.3",
          "47.9": "48.2",
          "47.7": "47.4",
          "47.5": "47.8",
          "48.3": "48.0",
          "48.0": "47.1"
        },
        {
          "Gemini-Lite": "phi-4-multimodal",
          "43.6": "36.3",
          "48.6": "37.2",
          "47.9": "35.9",
          "47.7": "35.3",
          "47.5": "36.9",
          "48.3": "36.4",
          "48.0": "37.0"
        },
        {
          "Gemini-Lite": "Table 3: Effect of context window size on audio-only performance (W-F1 scores). Baseline",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "the no context baseline, and bold indicates the best performance in each column.",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "Gpt-4o-Audio show similar trends, achieving peak performances of 54.1% and 55.9% at",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "MELD The effectiveness of audio context varies between models. While Gemini and",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "Gemini-Lite exhibit clear performance improvements when context is introduced—reaching",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "peak performance at 48.8% and 48.6% respectively, Gpt-4o-Audio shows a decline from",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "its baseline of 51.3% when context is introduced. Phi-4-multimodal shows only marginal",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "and inconsistent improvements across different context sizes. We hypothesize that this",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "could be due to omni-LLMs failing with more complex audio: MELD has multi-party TV",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "show dialogues with background noise, audience laughter, and varying conditions, whereas",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "IEMOCAP has better quality, dyadic audio recordings in a lab setting. We quantify this",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        },
        {
          "Gemini-Lite": "",
          "43.6": "",
          "48.6": "",
          "47.9": "",
          "47.7": "",
          "47.5": "",
          "48.3": "",
          "48.0": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": ""
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": ""
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Method"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Lei et al. (2023)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Shou et al. (2024)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Wu et al. (2024)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Yun et al. (2024)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "OmniVox (Ours)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Lei et al. (2023)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Shou et al. (2024)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Wu et al. (2024)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "Yun et al. (2024)"
        },
        {
          "using the signal-to-noise ratio (SNR) and find that MELD and IEMOCAP have similar SNRs": "OmniVox (Ours)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "Vol.\nPitch\nRate Gold → Pred.",
          "MELD": "Vol.\nPitch\nRate"
        },
        {
          "IEMOCAP": "Joy → Neu.",
          "MELD": ""
        },
        {
          "IEMOCAP": "50.0% 51.9% 51.0% Higher",
          "MELD": "12.3% 22.1% 0.0%"
        },
        {
          "IEMOCAP": "9.6%\n2.9%\n5.8% Lower",
          "MELD": "30.3% 16.0% 7.7%"
        },
        {
          "IEMOCAP": "40.4% 44.2% 43.3% Same",
          "MELD": "57.4% 62.0% 92.3%"
        },
        {
          "IEMOCAP": "Sad → Neu.",
          "MELD": ""
        },
        {
          "IEMOCAP": "4.5%\n0.0%\n2.3% Higher",
          "MELD": "19.8% 20.5% 1.1%"
        },
        {
          "IEMOCAP": "42.0% 33.0% 53.4% Lower",
          "MELD": "36.4% 16.7% 16.8%"
        },
        {
          "IEMOCAP": "53.4% 67.0% 44.3% Same",
          "MELD": "43.8% 62.9% 82.1%"
        },
        {
          "IEMOCAP": "Ang. → Neu.",
          "MELD": ""
        },
        {
          "IEMOCAP": "42.3% 69.2% 71.2% Higher",
          "MELD": "19.4% 12.9% 3.1%"
        },
        {
          "IEMOCAP": "1.9%\n0.0%\n0.0% Lower",
          "MELD": "29.5% 24.2% 6.2%"
        },
        {
          "IEMOCAP": "53.8% 30.8% 28.8% Same",
          "MELD": "51.2% 62.9% 90.6%"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5.2": "We begin by defining our setup and motivation. Wu et al. (2024) extracted raw audio features",
          "RQ2: Acoustic Descriptions": ""
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "from the speech files, namely volume and volume variation, pitch and pitch variation, and"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "speaking rate. From these features, the authors binned by threshold and converted these"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "into natural language descriptions (e.g., if threshold is a certain value, the volume is low)."
        },
        {
          "5.2": "We compare our omni-LLM generated features in the “Acoustic Analysis” section of our",
          "RQ2: Acoustic Descriptions": ""
        },
        {
          "5.2": "prompt to these reference features, evaluating on F1.",
          "RQ2: Acoustic Descriptions": ""
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "IEMOCAP Our IEMOCAP results show that average acoustic features are more accurately"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "predicted than their variations, with average volume achieving the highest F1 score (0.68),"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "followed by average pitch (0.62) and speaking rate (0.60). The model, however, struggles"
        },
        {
          "5.2": "with capturing acoustic variations with substantially lower F1 scores for volume variation",
          "RQ2: Acoustic Descriptions": ""
        },
        {
          "5.2": "(0.33) and pitch variation (0.27).",
          "RQ2: Acoustic Descriptions": ""
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "MELD Our MELD results achieve the highest F1 score for pitch(0.69), followed by average"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "volume (0.63) and speaking rate (0.62), showing relatively strong prediction of baseline"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "acoustic features. However, the model performs substantially worse on capturing acoustic"
        },
        {
          "5.2": "variations, with both volume and pitch variations having an identical F1 of 0.26.",
          "RQ2: Acoustic Descriptions": ""
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "Summary Our findings suggest that while omni-LLMs can reasonably predict baseline"
        },
        {
          "5.2": "",
          "RQ2: Acoustic Descriptions": "acoustic levels from audio alone, they have difficulty capturing the dynamic fluctuations."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach,"
        },
        {
          "References": "Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al."
        },
        {
          "References": "Phi-4-mini technical report: Compact yet powerful multimodal\nlanguage models via"
        },
        {
          "References": "mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025."
        },
        {
          "References": "Anthropic.\nClaude 3.5 sonnet.\nhttps://www.anthropic.com/news/claude-3-5-sonnet,"
        },
        {
          "References": "2025."
        },
        {
          "References": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim,"
        },
        {
          "References": "Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan.\nIemocap:\nInteractive"
        },
        {
          "References": "emotional dyadic motion capture database. Language resources and evaluation, 42:335–359,"
        },
        {
          "References": "2008."
        },
        {
          "References": "Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang"
        },
        {
          "References": "Peng, and Alexander Hauptmann. Emotion-llama: Multimodal emotion recognition and"
        },
        {
          "References": "reasoning with instruction tuning. Advances in Neural Information Processing Systems, 37:"
        },
        {
          "References": "110805–110853, 2024."
        },
        {
          "References": "Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou,"
        },
        {
          "References": "and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified"
        },
        {
          "References": "large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023."
        },
        {
          "References": "Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun"
        },
        {
          "References": "arXiv preprint\nLv,\nJinzheng He,\nJunyang Lin, et al.\nQwen2-audio technical report."
        },
        {
          "References": "arXiv:2407.10759, 2024."
        },
        {
          "References": "Soumya Dutta and Sriram Ganapathy. Llm supervised pre-training for multimodal emotion"
        },
        {
          "References": "recognition in conversations. arXiv preprint arXiv:2501.11468, 2025."
        },
        {
          "References": "Ziwei Gong, Muyin Yao, Xinyi Hu, Xiaoning Zhu, and Julia Hirschberg. A mapping"
        },
        {
          "References": "on current classifying categories of emotions used in multimodal models for emotion"
        },
        {
          "References": "recognition. In Sophie Henning and Manfred Stede (eds.), Proceedings of The 18th Linguistic"
        },
        {
          "References": "Annotation Workshop (LAW-XVIII), pp. 19–28, St. Julians, Malta, March 2024. Association"
        },
        {
          "References": "for Computational Linguistics. URL https://aclanthology.org/2024.law-1.3/."
        },
        {
          "References": "https://blog.google/technology/google-deepmind/\nGoogle.\nGemini\n2.0."
        },
        {
          "References": "google-gemini-ai-update-december-2024/, December 2024."
        },
        {
          "References": "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces."
        },
        {
          "References": "arXiv preprint arXiv:2312.00752, 2023."
        },
        {
          "References": "Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, and Songlin Hu.\nSupervised adversarial"
        },
        {
          "References": "contrastive learning for emotion recognition in conversations.\nIn Anna Rogers, Jordan"
        },
        {
          "References": "Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the"
        },
        {
          "References": "Association for Computational Linguistics (Volume 1: Long Papers), pp. 10835–10852, Toronto,"
        },
        {
          "References": "Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023."
        },
        {
          "References": "acl-long.606. URL https://aclanthology.org/2023.acl-long.606/."
        },
        {
          "References": "Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Runqi Qiao, and Sirui Wang."
        },
        {
          "References": "Instructerc: Reforming emotion recognition in conversation with multi-task retrieval-"
        },
        {
          "References": "augmented large language models. arXiv preprint arXiv:2309.11911, 2023."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria,"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "conversations. arXiv preprint arXiv:1810.02508, 2018."
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, and"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "Natalie Schluter. Akan cinematic emotions (ace): A multimodal multi-party dataset for"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "emotion recognition in movie dialogues. arXiv preprint arXiv:2502.10973, 2025."
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "Yuntao Shou, Tao Meng, Fuchen Zhang, Nan Yin, and Keqin Li. Revisiting multi-modal"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "emotion learning with broad state space models and probability-guidance fusion. arXiv"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "preprint arXiv:2404.17858, 2024."
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "Adil Soubki, John Murzaku, Peter Zeng, and Owen Rambow. Synthetic audio helps for"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "cognitive state tasks. arXiv preprint arXiv:2502.06922, 2025."
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett"
        },
        {
          "OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024.": "Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multi-"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A\nLimitations & Future Work": "OmniVox presents the first systematic use and evaluation advancement in zero-shot ERC"
        },
        {
          "A\nLimitations & Future Work": "using omni-LLMs. However, we acknowledge some limitations of our work."
        },
        {
          "A\nLimitations & Future Work": "Reliance on Omni-LLMs Our approach relies on pre-trained omni-LLMs, particularly three"
        },
        {
          "A\nLimitations & Future Work": "closed omni-LLMs (GPT-4o-audio, Gemini-2.0-Flash, Gemini-2.0-Flash-Lite). We attempt to"
        },
        {
          "A\nLimitations & Future Work": "address this by making our experiments as reproducible as possible and evaluating on one"
        },
        {
          "A\nLimitations & Future Work": "open omni-LLM (Phi-4-Multimodal-Instruct)."
        },
        {
          "A\nLimitations & Future Work": "Performance Metrics The zero-shot nature of OmniVox, although in some instances outper-"
        },
        {
          "A\nLimitations & Future Work": "forming fine-tuned audio-only models, falls short on other modalities (text, text+audio). We"
        },
        {
          "A\nLimitations & Future Work": "emphasize that our paper primarily focuses on the audio modality, and that omni-LLMs may"
        },
        {
          "A\nLimitations & Future Work": "not perform as well as text-only LLMs. We do intend to further explore this performance"
        },
        {
          "A\nLimitations & Future Work": "gap in future work."
        },
        {
          "A\nLimitations & Future Work": "Reliance on Two Corpora We do note that our work relies on only two corpora: IEMOCAP"
        },
        {
          "A\nLimitations & Future Work": "and MELD. While these two corpora are widely used and benchmarked by the many"
        },
        {
          "A\nLimitations & Future Work": "previous works, they may not generalize to real-world emotion detection and classification."
        },
        {
          "A\nLimitations & Future Work": "Future Work We hypothesize that our work may transfer to other cognitive state tasks (e.g."
        },
        {
          "A\nLimitations & Future Work": "sentiment, belief,\nintent). This paper provides a framework and initial exploration into"
        },
        {
          "A\nLimitations & Future Work": "one of many cognitive state tasks (emotion). In future work, we will use the insights and"
        },
        {
          "A\nLimitations & Future Work": "discoveries from this work to apply to other cognitive state tasks."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "CONVERSATION :"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "S p e a k e r\n0 :\nA l l\nt h e\ntime ,\ne v e r y\nday ,"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "l i k e\nt h a t\n,\nr i g h t ?\nI\ncan\ns e n d\nyou"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "S p e a k e r\n1 :\n” Yeah ,\na s much\na s\nI\ncan . ”"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "S p e a k e r\n0 :\n” Yeah . ”"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "T a r g e t\nU t t e r a n c e :\nI\nl o v e\nyou"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "LLM OUTPUT:"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "C o n v e r s a t i o n a l\nC o n t e x t :\nThe\ns p e a k e r\ni s"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "He m e n t i o n s\nc a l l i n g\nsomeone ,\nf i g u r i n g"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "e m a i l\n."
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "A c o u s t i c\nA n a l y s i s :\nThe\nt a r g e t\na u d i o\nh a s"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "s u b d u e d\nand\nq u i e t .\nThere ' s\na\ns l i g h t"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "R e a s o n i n g :\nThe\nl ow\np i t c h ,\ns l o w\npace ,\nand"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "The\nt r a i l i n g\no f f\na t\nt h e\nend\nf u r t h e r"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "r e a s s u r a n c e ,\nt h e\na c o u s t i c\nf e a t u r e s"
        },
        {
          "Example 1: IEMOCAP. Gemini-2.0. With Context": "L a b e l\n:\ns a d"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "summarizes them.": ""
        },
        {
          "summarizes them.": "Model"
        },
        {
          "summarizes them.": ""
        },
        {
          "summarizes them.": "Gemini-2.0-Flash"
        },
        {
          "summarizes them.": ""
        },
        {
          "summarizes them.": "Gemini-2.0-Flash-Lite"
        },
        {
          "summarizes them.": ""
        },
        {
          "summarizes them.": "Phi-4-Multimodal-Instruct"
        },
        {
          "summarizes them.": ""
        },
        {
          "summarizes them.": "GPT-4o-Audio"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "P l e a s e": "",
          "l i s t e n": "s o l e l y"
        },
        {
          "P l e a s e": "A f t e r",
          "l i s t e n": "l i s t e n i n g"
        },
        {
          "P l e a s e": "− a n g e r",
          "l i s t e n": ""
        },
        {
          "P l e a s e": "− j o y",
          "l i s t e n": ""
        },
        {
          "P l e a s e": "− s a d n e s s",
          "l i s t e n": ""
        },
        {
          "P l e a s e": "− s u r p r i s e",
          "l i s t e n": ""
        },
        {
          "P l e a s e": "− f e a r",
          "l i s t e n": ""
        },
        {
          "P l e a s e": "− d i s g u s t",
          "l i s t e n": ""
        },
        {
          "P l e a s e": "− n e u t r a l",
          "l i s t e n": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "You '": "The",
          "l\nl\nh e a r": "f i r s t"
        },
        {
          "You '": "",
          "l\nl\nh e a r": "TARGET'"
        },
        {
          "You '": "",
          "l\nl\nh e a r": "p i t c h ,"
        },
        {
          "You '": "− a n g e r",
          "l\nl\nh e a r": ""
        },
        {
          "You '": "− j o y",
          "l\nl\nh e a r": ""
        },
        {
          "You '": "− s a d n e s s",
          "l\nl\nh e a r": ""
        },
        {
          "You '": "− s u r p r i s e",
          "l\nl\nh e a r": ""
        },
        {
          "You '": "− f e a r",
          "l\nl\nh e a r": ""
        },
        {
          "You '": "− d i s g u s t",
          "l\nl\nh e a r": ""
        },
        {
          "You '": "− n e u t r a l",
          "l\nl\nh e a r": ""
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras",
      "authors": [
        "Abdelrahman Abouelenin",
        "Atabak Ashfaq",
        "Adam Atkinson",
        "Hany Awadalla",
        "Nguyen Bach",
        "Jianmin Bao",
        "Alon Benhaim",
        "Martin Cai",
        "Vishrav Chaudhary",
        "Congcong Chen"
      ],
      "year": "2025",
      "venue": "Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras",
      "arxiv": "arXiv:2503.01743"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Anthropic"
      ],
      "year": "2025",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "6",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "7",
      "title": "Llm supervised pre-training for multimodal emotion recognition in conversations",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2025",
      "venue": "Llm supervised pre-training for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2501.11468"
    },
    {
      "citation_id": "8",
      "title": "A mapping on current classifying categories of emotions used in multimodal models for emotion recognition",
      "authors": [
        "Ziwei Gong",
        "Muyin Yao",
        "Xinyi Hu",
        "Xiaoning Zhu",
        "Julia Hirschberg"
      ],
      "year": "2024",
      "venue": "Proceedings of The 18th Linguistic Annotation Workshop (LAW-XVIII)"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Google",
        "Gemini"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao",
        "Mamba"
      ],
      "year": "2023",
      "venue": "Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "11",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.606"
    },
    {
      "citation_id": "12",
      "title": "Instructerc: Reforming emotion recognition in conversation with multi-task retrievalaugmented large language models",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Runqi Qiao",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with multi-task retrievalaugmented large language models",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "13",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "14",
      "title": "OpenAI. Gpt-4o",
      "year": "2022",
      "venue": "OpenAI. Gpt-4o"
    },
    {
      "citation_id": "15",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "16",
      "title": "Akan cinematic emotions (ace): A multimodal multi-party dataset for emotion recognition in movie dialogues",
      "authors": [
        "David Sasu",
        "Zehui Wu",
        "Ziwei Gong",
        "Run Chen",
        "Pengyuan Shi",
        "Lin Ai",
        "Julia Hirschberg",
        "Natalie Schluter"
      ],
      "year": "2025",
      "venue": "Akan cinematic emotions (ace): A multimodal multi-party dataset for emotion recognition in movie dialogues",
      "arxiv": "arXiv:2502.10973"
    },
    {
      "citation_id": "17",
      "title": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "18",
      "title": "Synthetic audio helps for cognitive state tasks",
      "authors": [
        "Adil Soubki",
        "John Murzaku",
        "Peter Zeng",
        "Owen Rambow"
      ],
      "year": "2025",
      "venue": "Synthetic audio helps for cognitive state tasks",
      "arxiv": "arXiv:2502.06922"
    },
    {
      "citation_id": "19",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "Gemini Team",
        "Petko Georgiev",
        "Ian Ving",
        "Ryan Lei",
        "Libin Burnell",
        "Anmol Bai",
        "Garrett Gulati",
        "Damien Tanzer",
        "Zhufeng Vincent",
        "Shibo Pan",
        "Wang"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "20",
      "title": "Qwen2.5-omni technical report",
      "authors": [
        "Qwen Team"
      ],
      "year": "2025",
      "venue": "Qwen2.5-omni technical report"
    },
    {
      "citation_id": "21",
      "title": "Enhancing multimodal emotion recognition through multi-granularity cross-modal alignment",
      "authors": [
        "Xuechen Wang",
        "Shiwan Zhao",
        "Haoqin Sun",
        "Hui Wang",
        "Jiaming Zhou",
        "Yong Qin"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "authors": [
        "Zehui Wu",
        "Ziwei Gong",
        "Lin Ai",
        "Pengyuan Shi",
        "Kaan Donbekci",
        "Julia Hirschberg"
      ],
      "year": "2024",
      "venue": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "arxiv": "arXiv:2407.21315"
    },
    {
      "citation_id": "24",
      "title": "TelME: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "Taeyang Yun",
        "Hyunkuk Lim",
        "Jeonghwan Lee",
        "Min Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2024.naacl-long.5"
    },
    {
      "citation_id": "25",
      "title": "Temporal-frequency state space duality: An efficient paradigm for speech emotion recognition",
      "authors": [
        "Jiaqi Zhao",
        "Fei Wang",
        "Kun Li",
        "Yanyan Wei",
        "Shengeng Tang",
        "Shu Zhao",
        "Xiao Sun"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}