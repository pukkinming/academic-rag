{
  "paper_id": "2306.14517v1",
  "title": "Cross-Lingual Cross-Age Group Adaptation For Low-Resource Elderly Speech Emotion Recognition",
  "published": "2023-06-26T08:48:08Z",
  "authors": [
    "Samuel Cahyawijaya",
    "Holy Lovenia",
    "Willy Chung",
    "Rita Frieske",
    "Zihan Liu",
    "Pascale Fung"
  ],
  "keywords": [
    "speech emotion recognition",
    "cross-lingual adaptation",
    "cross-age adaptation",
    "elderly",
    "low-resource language"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition plays a crucial role in humancomputer interactions. However, most speech emotion recognition research is biased toward English-speaking adults, which hinders its applicability to other demographic groups in different languages and age groups. In this work, we analyze the transferability of emotion recognition across three different languages-English, Mandarin Chinese, and Cantonese; and 2 different age groups-adults and the elderly. To conduct the experiment, we develop an English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion, and a Cantonese speech emotion dataset, YueMotion. This study concludes that different language and age groups require specific speech features, thus making cross-lingual inference an unsuitable method. However, cross-group data augmentation is still beneficial to regularize the model, with linguistic distance being a significant influence on cross-lingual transferability. We release publicly release our code at https://github.com/ HLTCHKUST/elderly_ser.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human emotion is a crucial step towards better human-computer interaction  [1, 2, 3, 4] . Most studies on speech emotion recognition are centered on young-adult people, mainly originating from English-speaking countries  [5, 6, 7, 8] . This demographic bias causes existing commercial emotion recognition systems to inaccurately perceived emotion in the elderly  [9] . Despite the fast-growing elderly demographic in many countries  [10] , only a few studies with a fairly limited amount of data work on emotion recognition for elderly  [11, 12, 13] , especially from non-English-speaking countries  [14] .\n\nTo cope with the limited resource problem in the elderly emotion recognition, in this work, we study the prospect of transferring emotion recognition ability over various age groups and languages through the utilization of multilingual pre-trained speech models, e.g., Wav2Vec 2.0  [15] . Specifically, we aim to understand the transferability of emotion recognition ability using only speech modality between two languages and two age groups, i.e, English-speaking adults, English-speaking elderly, Mandarin-speaking adults, and Mandarin-speaking elderly. To do this, we develop BiMotion, a bi-lingual bi-age-group speech emotion recognition benchmark that covers 6 adult and elderly speech emotion recognition datasets from English and Mandarin Chinese. Additionally, we analyze the effect of language distance on the transferability of emotion recognition * Equal contribution. We analyze the transferability of emotion recognition ability in three ways, i.e., 1) cross-group inference using only the source group data for training to better understand the features distinction between each group, 2) cross-group data augmentation to better understand the transferability across different groups, and 3) feature-space projection to better understand the effect of transferability across different language distance. Through a series of experiments, we conclude that: • Very distinct speech features are needed for recognizing emotion across different language and age groups, which makes cross-lingual inference an ill-suited method for transferring speech emotion recognition ability. • Despite the different important speech features across groups, data augmentation across different groups is still beneficial as it helps the model to better regularize yielding higher evaluation performance. • Language distance plays a crucial role in cross-lingual transferability, as more similar languages tend to share more similar speech characteristics, thus allowing better generalization to the target language.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Various efforts have explored emotion recognition solutions for around two decades ago through different modalities  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25] . Most studies focus on methods that can estimate the subject emotion effectively through a specific modality while some others focus on combining multiple modalities to better estimate the subject emotion. The transferability of emotion recognition across different cultures and language groups has also been previously studied  [26, 27, 28]  showing that there are significant differences across languages and cultures. Nevertheless, it is only evaluated on a small-scale model that owns limited world knowledge. In this work, we extend this analysis to pre-trained speech models and further explore the transferability to cover an extremely low-resource group, i.e., the elderly in the low-resource language group.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Pre-Trained Speech Model",
      "text": "Large pre-trained speech models achieve state-of-the-art performance for various speech tasks in recent years  [15, 29, 30] . These models have been trained in an unsupervised manner on large-scale multilingual speech corpora. The representation of these models has been shown to be effective, achieving state-ofthe-art performance in various downstream tasks including automatic speech recognition, speaker verification, language identification, and emotion recognition  [31, 32, 33, 34, 35] . In this work, we adopt the XLSR Wav2Vec 2.0 model  [15, 30] , which has been pre-trained on large-scale multilingual speech corpora. The multilingual representation learned by the model will be crucial for evaluating the transferability of emotion recognition ability across different languages. We construct our benchmark from a collection of six publicly available datasets, i.e., CREMA-D  [11] , CSED  [14] , ElderReact  [12] , ESD  [36] , IEMOCAP  [5] , and TESS  [13] . For datasets with an official split, i.e., ESD and ElderReact, we use the provided dataset splits. For the other datasets, we generate a new dataset split since there is no official split provided. The statistics of the datasets in BiMotion are shown in Table  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Benchmark And Dataset",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Yuemotion Dataset",
      "text": "To strengthen our transferability analysis, we further introduce a new speech emotion recognition dataset covering adults in Cantonese named YueMotion. The utterances in YueMotion are recorded by 11 speakers (4 males and 7 females) each with a",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "We employ two methods to evaluate the transferability of emotion recognition ability, i.e., cross-group data augmentation and cross-group inference.  1  The detail of each method is explained in the following subsection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Group Inference",
      "text": "We define a set of language L={l1, l2, . . . , ln} and a set of age group A={a1, a2, . . . , am}. We define training and evaluation data from a language li and age group aj as X trn l i ,a j and X tst l i ,a j , respectively. To understand the transferability from the source group to the target group, we explore a method called cross-group inference. Following prior works on zero-shot accent/language adaptation  [37, 38, 39] , we explore cross-lingual and cross-age inference settings in our experiment. Specifically, given a training dataset X trn l i ,a j , we select three out-of-group test datasets: 1) datasets that are not in li language (X tst ¬l i ,a j ), 2) datasets that are not in aj age-group (X tst l i ,¬a j ), and 3) datasets that are not in li language and aj age-group X tst ¬l i ,¬a j . We conduct cross-group inference by training the models on a specific group training set and evaluating them on the three out-of-group test sets. With this method, we can analyze the effect of crossgroup information on a specific language and age group.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Group Data Augmentation",
      "text": "Prior works  [40, 41]  have shown that cross-domain and crosslingual data augmentation method effectively improves the model performance in textual and speech modalities, especially on low-resource languages and domains. We adopt the data augmentation technique to the speech emotion recognition task by utilizing cross-lingual and cross-age data augmentation. Our cross-group data augmentation injects a training dataset X trn l i ,a j from a language group li and an age group aj with data from other language and age groups producing a new augmented dataset. Given X trn l i ,a j , there are three different data augmentation settings possible, i.e., cross-lingual data augmentation on the same age group resulting in X trn L,a j , cross-age data augmentation on the same language resulting in X trn l i ,A , and crosslingual cross-age data augmentation resulting in X trn L,A . We then fine-tune the model on the augmented dataset and evaluate it on X tst l i ,a j , the test set with a language li and age group aj. Through this method, we can analyze the influence of crossgroup augmentation on a specific language and age group.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature-Space Projection",
      "text": "Given a fine-tuned emotion recognition model θ, we extract the speech representation by taking the high-dimensional features before the last linear classification layer. By using θ, we extract the speech features from data points and perform matrix decomposition  [42, 43, 44, 45, 46, 47, 48]  into a 2-dimensional space with UMAP  [49]  for visualization. With this approach, we can approximate the effect of augmenting cross-group data points to the training coverage, which will determine the prediction quality of the model to the evaluation data in a specific language li and age-group aj.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Models",
      "text": "For our experiment, we utilize the XLSR-53 Wav2Vec 2.0  [15, 30]  model which is pre-trained in 53 languages including English, Mandarin, and Cantonese.  2  We compare the performance of the cross-group data augmentation and cross-group inference settings with a simple in-group training baseline where given a language li and an age-group aj, the model is fine-tuned only on X l i ,a j and evaluated on the test set of the corresponding language and age-group.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Settings",
      "text": "To evaluate the transferability of emotion recognition ability, we conduct an extensive analysis in the BiMotion benchmark. We explore two different cross-group transfer methods, i.e., cross-group data-augmentation ( §4.2) and cross-group inference ( §4.1). Specifically, we fine-tune the pre-trained XLSR-53 Wav2Vec 2.0 model using the in-group and the cross-group augmented datasets, i..e., X l i ,a j , Xa j , X l i , X all , and evaluate on all the test set covering both the in-group and the out-of-group test data, i.e, X test l i ,a j , X test ¬l i , X test ¬a j , and X test ¬l i ,¬a j . To further strengthen our analysis, we conduct the transferability experiment on the YueMotion dataset. We use the Cantonese elderly and Cantonese adults data on the YueMotion as the test set, and explore cross-group data augmentation and cross-group inference for those test datasets. To measure the effectiveness of the cross-group transferability on YueMotion, we utilize the training datasets from BiMotion and compare the cross-group transfer methods with the baseline trained only on the YueMotion training dataset. Hyperparameters We use the same hyperparameters in all experiments. For fine-tuning the model we use the following hyperparameter setting, i.e., AdamW optimizer  [50]  with a learning rate of 5e-5, a dropout rate of 0.1, a number of epochs of 20, and early stopping of 5 epochs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Some speech utterances may consist of multiple emotion labels, e.g., happiness and surprise, which makes emotion recognition a multilabel problem. By framing the problem as multilabel, the occurrence of each emotion becomes sparse, thus leading to an imbalanced label distribution. To consider the imbalance distribution, we use binary F1-score to compute per-label evaluation performance and take the weighted F1-score over different emotion labels, which is an average of the F1 scores for each class weighted by the number of samples in that class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transferability Via Cross-Group Inference",
      "text": "The results of our cross-group inference experiment are shown in Table  3 . Based on our experiments, cross-lingual, cross-age, and cross-age cross-lingual inferences are not beneficial for improving the performance of the target group compared to the baseline trained on the specific language and age group, instead, they hinder the performance by a huge margin. For instance, the best cross-age inference comes from English-Adults to English-Elderly, however, it still hampers the performance of the in-group English-Elderly baseline by ∼15% weighted F1score. While for cross-lingual inference, the results are much worse with more than ∼ 30% lower weighted F1-score than the corresponding in-group training baseline. This result suggests that there are some differences in speech features used to recognize emotion between different languages and different age groups, which makes transfer through cross-group inference not suitable for speech emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transferability Via Cross-Group Data Augmentation",
      "text": "Despite having different characteristics, cross-group transfer through data augmentation could provide richer and more diverse samples of emotion-induced speech and help to avoid overfitting problems, especially in a low-resource setting. Based on our cross-group data augmentation results in Table 4, the English-Elderly and English-Adults groups benefit the most from the cross-group data augmentation, gaining ∼12% and ∼5% weighted F1-score respectively. While for the Mandarin groups, the improvement is quite small. Mandarin-Elderly gains ∼3% weighted F1-score with data augmentation from Mandarin-Elderly, while the result for the Mandarin-    Adults remains the same, which is probably due to the limited data available from the Mandarin-Elderly group.\n\nFor cross-lingual data augmentation, the effect is apparent with the only improvement coming from the English-Adults group, while the performance in other groups decreases. We conjecture this is due to the huge linguistic differences between English and Mandarin  [27, 51] . Further analysis regarding the effect of language distance is discussed on §6.3. Furthermore, a combination of cross-lingual and cross-age data augmentation tends to improve the performance even higher. Specifically, the performance on English-Elderly and English-Adults increase by ∼13% and ∼17% weighted F1-score, respectively. The performance on Mandarin-Adults group improves marginally by ∼0.4% weighted F1-score. On the other hand, the performance of Mandarin-Elderly decreases by ∼5% weighted F1score compared to the in-group training baseline. This might be caused by distributional shift due to the large amount of augmentation from other groups with respect to the amount of data in the Mandarin-Elderly group.\n\nWe further analyze the cross-group behavior further through feature-space projection using the model trained on all BiMotion training data. As shown in Figure  3 , cross-age data augmentation improves the training coverage of the model resulting in a better generalization on unseen evaluation data, despite the speech feature differences among different age groups. This shows the potential of leveraging cross-age data augmentation for modeling low-resource groups such as the elderly.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Effect Of Language Distance On Transferability",
      "text": "We further analyze the effect of language distance on crosslingual data augmentation. Specifically, we explore the effect of cross-lingual data augmentation from Mandarin and English to Cantonese. To balance the amount of data between English and Mandarin, we only utilize the ESD dataset  [36]  which contains 15K training, 1K validation, and 1.5K utterances for each English and Mandarin. For the Cantonese dataset, we utilize the newly constructed speech emotion dataset, YueMotion. As shown in Table  5 , the performance on the Cantonese data increases when the model is augmented with Mandarin data and decreases when the model is augmented with English. This follows the linguistic similarity of languages; Mandarin and Cantonese are more similar as both come from the same language family, i.e., Sino-Tibetan, compared to English that comes from the Indo-European language family. Similar pattern is also observed in the Mandarin test data when the Cantonese-Mandarin trained model outperforms the Cantonese-Mandarin-English model by ∼3% weighted F1-score.\n\nWe also analyze the cross-lingual behavior further through feature-space projection with the Cantonese-Mandarin-English trained model. As shown in Figure  2 , the original Cantonese training data cannot cover all the test data. When Mandarin data is added, the training coverage improved which covers more test samples for each label. When English data is added, the training coverage is improved, but not without shifting the training distribution, e.g., the out-of-distribution on the sad label, causing the model to perform worse on the Cantonese evaluation data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We investigate the transferability of speech emotion recognition ability to achieve a better generalization for the elderly in lowresource languages. We construct an English-Mandarin speech emotion benchmark for adults and the elderly, namely BiMotion, from six publicly available datasets. We also construct, YueMotion, a low-resource speech emotion dataset for adults in Cantonese to analyze the effect of language distance in crosslingual data augmentation. Based on the experiments, we conclude: 1) significantly distinct speech features are necessary to recognize emotion across different language and age groups, 2) although the speech features may vary across different groups, cross-group data augmentation is still beneficial to better generalize the model, and 3) language distance substantially affects the effectiveness of cross-lingual transferability. Our analysis lays the groundwork for developing more effective speech emotion recognition models for low-resource groups, e.g., the elderly in low-resource languages.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Elderly speech emotion data in low-resource lan-",
      "page": 1
    },
    {
      "caption": "Figure 2: Cross-group data augmentation on the Cantonese-Adults data. Blue, red, olive regions represent the density plot of the",
      "page": 4
    },
    {
      "caption": "Figure 3: Cross-group data augmentation on the Mandarin-",
      "page": 4
    },
    {
      "caption": "Figure 3: , cross-age data",
      "page": 4
    },
    {
      "caption": "Figure 2: , the original Cantonese",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "recognition systems\nto inaccurately perceived emotion in the",
          "groups,\nand 3)\nfeature-space projection to better understand": "the effect of transferability across different\nlanguage distance."
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "elderly [9]. Despite the fast-growing elderly demographic in",
          "groups,\nand 3)\nfeature-space projection to better understand": "Through a series of experiments, we conclude that:"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "many countries [10], only a few studies with a fairly limited",
          "groups,\nand 3)\nfeature-space projection to better understand": "• Very distinct speech features are needed for recognizing emo-"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "amount of data work on emotion recognition for elderly [11,",
          "groups,\nand 3)\nfeature-space projection to better understand": "tion across different\nlanguage and age groups, which makes"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "12, 13], especially from non-English-speaking countries [14].",
          "groups,\nand 3)\nfeature-space projection to better understand": "cross-lingual\ninference an ill-suited method for\ntransferring"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "To cope with the limited resource problem in the elderly",
          "groups,\nand 3)\nfeature-space projection to better understand": "speech emotion recognition ability."
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "emotion recognition,\nin this work, we study the prospect of",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "• Despite the different important speech features across groups,"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "transferring emotion recognition ability over various age groups",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "data augmentation across different groups is still beneficial as"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "and languages through the utilization of multilingual pre-trained",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "it helps the model to better regularize yielding higher evalua-"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "speech models, e.g., Wav2Vec 2.0 [15]. Specifically, we aim to",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "tion performance."
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "understand the transferability of emotion recognition ability us-",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "• Language distance plays a crucial role in cross-lingual trans-"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "ing only speech modality between two languages and two age",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "ferability, as more similar languages tend to share more simi-"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "groups,\ni.e, English-speaking adults, English-speaking elderly,",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "lar speech characteristics, thus allowing better generalization"
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "Mandarin-speaking adults, and Mandarin-speaking elderly. To",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "",
          "groups,\nand 3)\nfeature-space projection to better understand": "to the target language."
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "do this, we develop BiMotion, a bi-lingual bi-age-group speech",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        },
        {
          "This demographic bias\ncauses\nexisting commercial\nemotion": "emotion recognition benchmark that covers 6 adult and elderly",
          "groups,\nand 3)\nfeature-space projection to better understand": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Speech emotion recognition plays a crucial role in human-"
        },
        {
          "Abstract": "computer interactions. However, most speech emotion recogni-"
        },
        {
          "Abstract": "tion research is biased toward English-speaking adults, which"
        },
        {
          "Abstract": "hinders\nits applicability to other demographic groups\nin dif-"
        },
        {
          "Abstract": "ferent\nlanguages and age groups.\nIn this work, we analyze"
        },
        {
          "Abstract": "the transferability of emotion recognition across\nthree differ-"
        },
        {
          "Abstract": "ent languages–English, Mandarin Chinese, and Cantonese; and"
        },
        {
          "Abstract": "2 different age groups–adults and the elderly. To conduct\nthe"
        },
        {
          "Abstract": "experiment, we develop an English-Mandarin speech emotion"
        },
        {
          "Abstract": "benchmark for adults and the elderly, BiMotion, and a Can-"
        },
        {
          "Abstract": "tonese speech emotion dataset, YueMotion.\nThis\nstudy con-"
        },
        {
          "Abstract": "cludes that different\nlanguage and age groups require specific"
        },
        {
          "Abstract": "speech features, thus making cross-lingual inference an unsuit-"
        },
        {
          "Abstract": "able method. However, cross-group data augmentation is still"
        },
        {
          "Abstract": "beneficial to regularize the model, with linguistic distance being"
        },
        {
          "Abstract": "a significant\ninfluence on cross-lingual\ntransferability. We re-"
        },
        {
          "Abstract": "lease publicly release our code at https://github.com/"
        },
        {
          "Abstract": "HLTCHKUST/elderly_ser."
        },
        {
          "Abstract": "Index Terms: speech emotion recognition, cross-lingual adap-"
        },
        {
          "Abstract": "tation, cross-age adaptation, elderly, low-resource language"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1.\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Understanding human emotion is a crucial\nstep towards bet-"
        },
        {
          "Abstract": "ter human-computer\ninteraction [1, 2, 3, 4]. Most studies on"
        },
        {
          "Abstract": "speech emotion recognition are centered on young-adult people,"
        },
        {
          "Abstract": "mainly originating from English-speaking countries [5, 6, 7, 8]."
        },
        {
          "Abstract": "This demographic bias\ncauses\nexisting commercial\nemotion"
        },
        {
          "Abstract": "recognition systems\nto inaccurately perceived emotion in the"
        },
        {
          "Abstract": "elderly [9]. Despite the fast-growing elderly demographic in"
        },
        {
          "Abstract": "many countries [10], only a few studies with a fairly limited"
        },
        {
          "Abstract": "amount of data work on emotion recognition for elderly [11,"
        },
        {
          "Abstract": "12, 13], especially from non-English-speaking countries [14]."
        },
        {
          "Abstract": "To cope with the limited resource problem in the elderly"
        },
        {
          "Abstract": "emotion recognition,\nin this work, we study the prospect of"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "transferring emotion recognition ability over various age groups"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "and languages through the utilization of multilingual pre-trained"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "speech models, e.g., Wav2Vec 2.0 [15]. Specifically, we aim to"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "understand the transferability of emotion recognition ability us-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ing only speech modality between two languages and two age"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "groups,\ni.e, English-speaking adults, English-speaking elderly,"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Mandarin-speaking adults, and Mandarin-speaking elderly. To"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "do this, we develop BiMotion, a bi-lingual bi-age-group speech"
        },
        {
          "Abstract": "emotion recognition benchmark that covers 6 adult and elderly"
        },
        {
          "Abstract": "speech emotion recognition datasets\nfrom English and Man-"
        },
        {
          "Abstract": "darin Chinese.\nAdditionally, we\nanalyze\nthe\neffect of\nlan-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "guage distance on the\ntransferability of\nemotion recognition"
        },
        {
          "Abstract": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "Dataset\nLanguage\nAge Group\n#Train\n#Valid\n#Test",
          "Table 2: Statistics of the YueMotion dataset.": "Age Group\nGender\n#Train\n#Valid\n#Test\n#All"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "English\nElderly\n150\n42\n300",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "CREMA-D",
          "Table 2: Statistics of the YueMotion dataset.": "Male\n120\n36\n84\n240"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "English\nAdults\n5000\n750\n1200",
          "Table 2: Statistics of the YueMotion dataset.": "Adults"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "Female\n210\n63\n147\n420"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "CSED\nMandarin\nElderly\n200\n52\n400",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "Total Samples\n330\n99\n231\n660"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "ElderReact\nEnglish\nElderly\n615\n355\n353",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "Mandarin\nAdults\n15000\n1000\n1500",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "ESD",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "English\nAdults\n15000\n1000\n1500",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "IEMOCAP\nEnglish\nAdults\n7500\n1039\n1500",
          "Table 2: Statistics of the YueMotion dataset.": "personal recording device. During the recording session, each"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "English\nElderly\n699\n200\n500",
          "Table 2: Statistics of the YueMotion dataset.": "speaker is asked to say out\nloud 10 pre-defined sentences used"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "TESS",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "English\nAdults\n700\n201\n500",
          "Table 2: Statistics of the YueMotion dataset.": "in daily conversations with 6 different emotions,\ni.e., happi-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "ness,\nsadness, neutral, disgust,\nfear, and anger.\nThe YueMo-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "tion dataset\nis used to further verify our hypothesis regarding"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "18, 19, 20, 21, 22, 23, 24, 25]. Most studies focus on methods",
          "Table 2: Statistics of the YueMotion dataset.": "the transferability of emotion recognition ability. The detailed"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "that can estimate the subject emotion effectively through a spe-",
          "Table 2: Statistics of the YueMotion dataset.": "statistics of YueMotion dataset are shown in Table 2."
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "cific modality while some others focus on combining multiple",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "modalities to better estimate the subject emotion.\nThe trans-",
          "Table 2: Statistics of the YueMotion dataset.": "4. Methodology"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "ferability of emotion recognition across different cultures and",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "We employ two methods to evaluate the transferability of emo-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "language groups has also been previously studied [26, 27, 28]",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "tion recognition ability, i.e., cross-group data augmentation and"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "showing that\nthere are significant differences across languages",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "and cultures. Nevertheless, it is only evaluated on a small-scale",
          "Table 2: Statistics of the YueMotion dataset.": "cross-group inference.1 The detail of each method is explained"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "model\nthat owns limited world knowledge.\nIn this work, we",
          "Table 2: Statistics of the YueMotion dataset.": "in the following subsection."
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "extend this analysis to pre-trained speech models and further",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "explore the transferability to cover an extremely low-resource",
          "Table 2: Statistics of the YueMotion dataset.": "4.1. Cross-Group Inference"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "group, i.e., the elderly in the low-resource language group.",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "We define a set of\n. . . ,\nlanguage L={l1,\nl2,\nln} and a set of"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "age group A={a1, a2,\n. . . , am}. We define training and eval-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "2.2. Pre-trained Speech Model",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "uation data from a language li and age group aj as X trn\nli,aj and"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "Large pre-trained speech models achieve state-of-the-art per-",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "X tst"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "li,aj , respectively. To understand the transferability from the"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "formance for various speech tasks in recent years [15, 29, 30].",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "source group to the target group, we explore a method called"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "These models have been trained in an unsupervised manner on",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "cross-group inference. Following prior works on zero-shot ac-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "large-scale multilingual speech corpora. The representation of",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "cent/language adaptation [37, 38, 39], we explore cross-lingual"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "these models has been shown to be effective, achieving state-of-",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "and cross-age inference settings in our experiment. Specifically,"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "the-art performance in various downstream tasks including au-",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "given a training dataset X trn"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "li,aj , we select three out-of-group test"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "tomatic speech recognition, speaker verification, language iden-",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "datasets:\n1) datasets that are not\nlanguage (X tst\nin li"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "¬li,aj ), 2)"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "tification, and emotion recognition [31, 32, 33, 34, 35].\nIn this",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "datasets that are not in aj age-group (X tst\nli,¬aj ), and 3) datasets"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "work, we adopt the XLSR Wav2Vec 2.0 model [15, 30], which",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "has been pre-trained on large-scale multilingual speech corpora.",
          "Table 2: Statistics of the YueMotion dataset.": "that are not in li language and aj age-group X tst\n¬li,¬aj . We con-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "The multilingual\nrepresentation learned by the model will be",
          "Table 2: Statistics of the YueMotion dataset.": "duct cross-group inference by training the models on a specific"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "crucial for evaluating the transferability of emotion recognition",
          "Table 2: Statistics of the YueMotion dataset.": "group training set and evaluating them on the three out-of-group"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "ability across different languages.",
          "Table 2: Statistics of the YueMotion dataset.": "test sets. With this method, we can analyze the effect of cross-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "group information on a specific language and age group."
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "3. Benchmark and Dataset",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "4.2. Cross-Group Data Augmentation"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "3.1. BiMotion Benchmark",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "Prior works [40, 41] have shown that cross-domain and cross-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "To test\nthe transferability of emotion recognition ability across",
          "Table 2: Statistics of the YueMotion dataset.": "lingual\ndata\naugmentation method\neffectively\nimproves\nthe"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "different\nlanguages\nand age groups, we develop a bilingual",
          "Table 2: Statistics of the YueMotion dataset.": "model performance in textual and speech modalities, especially"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "speech emotion recognition benchmark namely BiMotion,\nthat",
          "Table 2: Statistics of the YueMotion dataset.": "on low-resource languages and domains. We adopt the data aug-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "covers two languages,\ni.e., English and Mandarin, and two age",
          "Table 2: Statistics of the YueMotion dataset.": "mentation technique to the speech emotion recognition task by"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "groups,\ni.e., Elderly (age ≥60 y.o)\nand Adults\n(age 20-59).",
          "Table 2: Statistics of the YueMotion dataset.": "utilizing cross-lingual and cross-age data augmentation. Our"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "We construct our benchmark from a collection of six publicly",
          "Table 2: Statistics of the YueMotion dataset.": "cross-group data augmentation injects a training dataset X trn"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "li,aj"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "available datasets,\ni.e., CREMA-D [11], CSED [14], ElderRe-",
          "Table 2: Statistics of the YueMotion dataset.": "from a language group li and an age group aj with data from"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "act [12], ESD [36], IEMOCAP [5], and TESS [13]. For datasets",
          "Table 2: Statistics of the YueMotion dataset.": "other\nlanguage and age groups producing a new augmented"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "with an official split, i.e., ESD and ElderReact, we use the pro-",
          "Table 2: Statistics of the YueMotion dataset.": "dataset. Given X trn"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "li,aj ,"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "vided dataset splits. For the other datasets, we generate a new",
          "Table 2: Statistics of the YueMotion dataset.": "tation settings possible,\ni.e.,\ncross-lingual data augmentation"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "dataset split since there is no official split provided. The statis-",
          "Table 2: Statistics of the YueMotion dataset.": "on the same age group resulting in X trn"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "L,aj , cross-age data aug-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "tics of the datasets in BiMotion are shown in Table 1.",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "mentation on the same language resulting in X trn"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "li,A, and cross-"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "lingual cross-age data augmentation resulting in X trn\nL,A. We then"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "3.2. YueMotion Dataset",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "fine-tune the model on the augmented dataset and evaluate it"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "",
          "Table 2: Statistics of the YueMotion dataset.": "on X tst\nthe test\nset with a language li\nand age group aj."
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "To strengthen our transferability analysis, we further introduce a",
          "Table 2: Statistics of the YueMotion dataset.": "li,aj ,"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "new speech emotion recognition dataset covering adults in Can-",
          "Table 2: Statistics of the YueMotion dataset.": ""
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "tonese named YueMotion.\nThe utterances\nin YueMotion are",
          "Table 2: Statistics of the YueMotion dataset.": "1To enhance readability, we use colored underlines to mark the term"
        },
        {
          "Table 1: Statistics of datasets covered in BiMotion.": "recorded by 11 speakers (4 males and 7 females) each with a",
          "Table 2: Statistics of the YueMotion dataset.": "cross-group data augmentation and cross-group inference onwards."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: Evaluation results of cross-lingual and cross-age Table 4: Evaluation results of cross-lingual and cross-age",
      "data": [
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "adu denote English, Mandarin, elderly, and adults, respectively."
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Test Data"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Training"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Avg."
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "li=zho",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "li=eng\nli=eng\nli=zho\nli=zho"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Data"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "aj=adu",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "aj=eld\naj=adu\naj=eld\naj=adu"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "28.71",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Cross-all (X trn\n68.06\n77.24\n97.40\n73.73\n52.21\nL,A)"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "14.56",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Cross-age (X trn\n59.93\n66.96\n75.47\n97.00\n74.84\nli,A)"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "57.54",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Cross-lingual (X trn\n54.55\n74.13\n54.84\n95.82\n57.34\nL,aj )"
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": ""
        },
        {
          "cross-lingual inference. eng, zho, eld, and adu denote English,": "97.00",
          "cross-age cross-lingual data augmentation.\neng, zho, eld, and": "Baseline (X trn\n55.12\n70.28\n57.00\n97.00\n69.85\nli,aj )"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "methods in the application of speech emotion recognition,” in Ap-"
        },
        {
          "8. References": "[1] R. Beale and C. Peter, “The role of affect and emotion in HCI,” in",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "plication of Machine Learning.\nInTech, Feb. 2010."
        },
        {
          "8. References": "Affect and Emotion in Human-Computer Interaction.\nSpringer",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "Berlin Heidelberg, 2008, pp. 1–11.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[19] D. Bertero and P. Fung, “A first\nlook into a convolutional neural"
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "network for speech emotion detection,” in IEEE ICASSP, 2017."
        },
        {
          "8. References": "[2] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kol-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "lias, W. Fellenz, and J. Taylor, “Emotion recognition in human-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[20] H. Lovenia, D. P. Lestari, and R. Frieske, “What did i\njust hear?"
        },
        {
          "8. References": "computer interaction,” IEEE Signal Processing Magazine, vol. 18,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "detecting pornographic sounds in adult videos using neural net-"
        },
        {
          "8. References": "no. 1, pp. 32–80, 2001.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "the 17th International Audio Mostly\nworks,” in Proceedings of"
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Conference, 2022, pp. 92–95."
        },
        {
          "8. References": "[3] G.\nI. Winata, H. Lovenia, E. Ishii, F. B. Siddique, Y. Yang, and",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "P. Fung, “Nora: The well-being coach,” 2021.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[21]\nS. Latif, H. S. Ali, M. Usama, R. Rana, B. Schuller, and J. Qadir,"
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "“Ai-based emotion recognition: Promise, peril, and prescriptions"
        },
        {
          "8. References": "[4]\nE.\nIshii, G.\nI. Winata, S. Cahyawijaya, D. Lala, T. Kawahara,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "for prosocial path,” 2022."
        },
        {
          "8. References": "and\nP.\nFung,\n“ERICA: An\nempathetic\nandroid\ncompanion",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "the 22nd Annual\nfor\ncovid-19 quarantine,”\nin Proceedings of",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[22] W. Dai,\nS. Cahyawijaya, Y. Bang,\nand\nP.\nFung,\n“Weakly-"
        },
        {
          "8. References": "Meeting\nof\nthe\nSpecial\nInterest\nGroup\non\nDiscourse\nand",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "supervised multi-task learning for multimodal affect recognition,”"
        },
        {
          "8. References": "Dialogue.\nSingapore and Online: Association for Computational",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2021."
        },
        {
          "8. References": "Linguistics,\nJul.\n2021,\npp.\n257–260.\n[Online].\nAvailable:",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[23] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, “Multimodal end-"
        },
        {
          "8. References": "https://aclanthology.org/2021.sigdial-1.27",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "to-end sparse model for emotion recognition,” in Proceedings of"
        },
        {
          "8. References": "[5] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "the NAACL-HLT 2021.\nOnline: ACL, Jun. 2021, pp. 5305–5316."
        },
        {
          "8. References": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[24] N. Lubis, D. Lestari, A. Purwarianti, S. Sakti,\nand S. Naka-"
        },
        {
          "8. References": "interactive emotional dyadic motion capture database,” Language",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "mura, “Construction and analysis of indonesian emotional speech"
        },
        {
          "8. References": "Resources and Evaluation, vol. 42, no. 4, pp. 335–359, Nov. 2008.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2014\n17th Oriental Chapter\nof\nthe\nInternational\ncorpus,”\nin"
        },
        {
          "8. References": "[6]\nS. R. Livingstone and F. A. Russo,\n“The ryerson audio-visual",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Committee for the Co-ordination and Standardization of Speech"
        },
        {
          "8. References": "database of emotional speech and song (RAVDESS): A dynamic,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Databases and Assessment Techniques\n(COCOSDA).\nIEEE,"
        },
        {
          "8. References": "multimodal set of facial and vocal expressions in north american",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2014, pp. 1–5."
        },
        {
          "8. References": "english,” PLOS ONE, vol. 13, no. 5, May 2018.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[25] N. Lubis, S. Sakti, G. Neubig, T. Toda, A. Purwarianti,\nand"
        },
        {
          "8. References": "[7] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria,\nand L.-P.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "S. Nakamura,\n“Emotion and its\ntriggers\nin human spoken dia-"
        },
        {
          "8. References": "Morency, “Multi-attention recurrent network for human commu-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Situated Dialog in Speech-\nlogue:\nRecognition and analysis,”"
        },
        {
          "8. References": "nication comprehension,” in 32nd AAAI, 2018.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Based Human-Computer Interaction, pp. 103–110, 2016."
        },
        {
          "8. References": "[8]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[26]\nS. Hareli, K. Kafetsios, and U. Hess, “A cross-cultural study on"
        },
        {
          "8. References": "R. Mihalcea, “MELD: A multimodal multi-party dataset for emo-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "emotion expression and the learning of social norms,” Frontiers in"
        },
        {
          "8. References": "tion recognition in conversations,” in Proc. 57th ACL.\nACL, Jul.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Psychology, vol. 6, Oct. 2015."
        },
        {
          "8. References": "2019, pp. 527–536.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[27] N. Lim,\n“Cultural differences\nin emotion:\ndifferences\nin emo-"
        },
        {
          "8. References": "[9]\nE. Kim, D. Bryant, D. Srikanth, and A. Howard, “Age bias\nin",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "tional arousal\nlevel between the east and the west,” Integrative"
        },
        {
          "8. References": "emotion detection: An analysis of facial emotion recognition per-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Medicine Research, vol. 5, no. 2, pp. 105–109, Jun. 2016."
        },
        {
          "8. References": "formance on young, middle-aged, and older adults,” in Proc. 2021",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[28]\nI.\nIosifov,\nO.\nIosifova,\nO. Romanovskyi,\nV\n.\nSokolov,\nand"
        },
        {
          "8. References": "AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES ’21.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "I. Sukailo, “Transferability evaluation of speech emotion recogni-"
        },
        {
          "8. References": "New York, NY, USA: Association for Computing Machinery,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "tion between different\nlanguages,” in Advances in Computer Sci-"
        },
        {
          "8. References": "2021, p. 638–644.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "ence for Engineering and Education.\nSpringer International Pub-"
        },
        {
          "8. References": "[10] D. Rouzet, A. C. Sanchez, T. Renault, and O. Roehn, “Fiscal chal-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "lishing, 2022, pp. 413–426."
        },
        {
          "8. References": "lenges and inclusive growth in ageing societies,” no. 27, 2019.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[29] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-"
        },
        {
          "8. References": "[11] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "nov, and A. Mohamed, “Hubert: Self-supervised speech represen-"
        },
        {
          "8. References": "and R. Verma,\n“CREMA-d:\nCrowd-sourced emotional multi-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "tation learning by masked prediction of hidden units,” 2021."
        },
        {
          "8. References": "modal actors dataset,” IEEE Transactions on Affective Computing,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[30] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,"
        },
        {
          "8. References": "vol. 5, no. 4, pp. 377–390, Oct. 2014.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau,"
        },
        {
          "8. References": "[12] K. Ma, X. Wang, X. Yang, M. Zhang,\nJ. M. Girard, and L.-P.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "and M. Auli, “XLS-R: Self-supervised Cross-lingual Speech Rep-"
        },
        {
          "8. References": "Morency, “Elderreact: A multimodal dataset for recognizing emo-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "resentation Learning at Scale,” in Proc. Interspeech 2022, 2022."
        },
        {
          "8. References": "tional response in aging adults,” in 2019 International Conference",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[31]\nZ. Fan, M. Li, S. Zhou, and B. Xu, “Exploring wav2vec 2.0 on"
        },
        {
          "8. References": "on Multimodal Interaction, ser. ICMI ’19.\nNew York, NY, USA:",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Speaker Verification and Language Identification,” in Proc. Inter-"
        },
        {
          "8. References": "Association for Computing Machinery, 2019, p. 349–357.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "speech 2021, 2021, pp. 1509–1513."
        },
        {
          "8. References": "[13] M. K. Pichora-Fuller and K. Dupuis, “Toronto emotional speech",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[32]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from"
        },
        {
          "8. References": "set (tess),” 2020.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Interspeech\nSpeech Using wav2vec 2.0 Embeddings,”\nin Proc."
        },
        {
          "8. References": "[14] M. C. Lee, S. C. Yeh, S. Y. Chiu, and J. W. Chang, “A deep convo-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2021, 2021, pp. 3400–3404."
        },
        {
          "8. References": "lutional neural network based virtual elderly companion agent,” in",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[33] H. Lovenia, S. Cahyawijaya, G. Winata, P. Xu, Y. Xu, Z. Liu,"
        },
        {
          "8. References": "Proceedings of\nthe 8th ACM on Multimedia Systems Conference.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "R. Frieske, T. Yu, W. Dai, E. J. Barezi, Q. Chen, X. Ma, B. Shi,"
        },
        {
          "8. References": "ACM, Jun. 2017.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "and P. Fung, “ASCEND: A spontaneous Chinese-English dataset"
        },
        {
          "8. References": "[15] A. Baevski, H. Zhou, A. Mohamed,\nand M. Auli,\n“Wav2vec",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "13th\nfor\ncode-switching\nin multi-turn\nconversation,”\nin Proc."
        },
        {
          "8. References": "2.0: A framework for self-supervised learning of speech repre-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "LREC.\nELRA, Jun. 2022, pp. 7259–7268."
        },
        {
          "8. References": "the 34th International Conference\nsentations,” in Proceedings of",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[34]\nS. Cahyawijaya, H. Lovenia, A. F. Aji, G. I. Winata, B. Wilie,\n,"
        },
        {
          "8. References": "on Neural Information Processing Systems, ser. NIPS’20.\nRed",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "R. Mahendra, C. Wibisono, A. Romadhony, K. Vincentio, F. Koto,"
        },
        {
          "8. References": "Hook, NY, USA: Curran Associates Inc., 2020.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "J. Santoso, D. Moeljadi et al., “Nusacrowd: Open source initiative"
        },
        {
          "8. References": "[16]\nJ. Martinez-Miranda and A. Aldea, “Emotions in human and arti-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "for indonesian nlp resources,” 2022."
        },
        {
          "8. References": "ficial intelligence,” Computers in Human Behavior, vol. 21, no. 2,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[35] W. Dai, S. Cahyawijaya, T. Yu, E. J. Barezi, P. Xu, C. T. Yiu,"
        },
        {
          "8. References": "pp. 323–341, Mar. 2005.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "R. Frieske, H. Lovenia, G. Winata, Q. Chen, X. Ma, B. Shi, and"
        },
        {
          "8. References": "[17] D. Bertero and P. Fung, “Multimodal deep neural nets for detect-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "P. Fung, “CI-AVSR: A Cantonese audio-visual speech datasetfor"
        },
        {
          "8. References": "ing humor in tv sitcoms,” in 2016 IEEE SLT Workshop, 2016.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "in-car command recognition,” in Proc. 13th LREC.\nELRA, 2022."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "methods in the application of speech emotion recognition,” in Ap-"
        },
        {
          "8. References": "[1] R. Beale and C. Peter, “The role of affect and emotion in HCI,” in",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "plication of Machine Learning.\nInTech, Feb. 2010."
        },
        {
          "8. References": "Affect and Emotion in Human-Computer Interaction.\nSpringer",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "Berlin Heidelberg, 2008, pp. 1–11.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[19] D. Bertero and P. Fung, “A first\nlook into a convolutional neural"
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "network for speech emotion detection,” in IEEE ICASSP, 2017."
        },
        {
          "8. References": "[2] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kol-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "lias, W. Fellenz, and J. Taylor, “Emotion recognition in human-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[20] H. Lovenia, D. P. Lestari, and R. Frieske, “What did i\njust hear?"
        },
        {
          "8. References": "computer interaction,” IEEE Signal Processing Magazine, vol. 18,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "detecting pornographic sounds in adult videos using neural net-"
        },
        {
          "8. References": "no. 1, pp. 32–80, 2001.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "the 17th International Audio Mostly\nworks,” in Proceedings of"
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Conference, 2022, pp. 92–95."
        },
        {
          "8. References": "[3] G.\nI. Winata, H. Lovenia, E. Ishii, F. B. Siddique, Y. Yang, and",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "P. Fung, “Nora: The well-being coach,” 2021.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[21]\nS. Latif, H. S. Ali, M. Usama, R. Rana, B. Schuller, and J. Qadir,"
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "“Ai-based emotion recognition: Promise, peril, and prescriptions"
        },
        {
          "8. References": "[4]\nE.\nIshii, G.\nI. Winata, S. Cahyawijaya, D. Lala, T. Kawahara,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "for prosocial path,” 2022."
        },
        {
          "8. References": "and\nP.\nFung,\n“ERICA: An\nempathetic\nandroid\ncompanion",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "the 22nd Annual\nfor\ncovid-19 quarantine,”\nin Proceedings of",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[22] W. Dai,\nS. Cahyawijaya, Y. Bang,\nand\nP.\nFung,\n“Weakly-"
        },
        {
          "8. References": "Meeting\nof\nthe\nSpecial\nInterest\nGroup\non\nDiscourse\nand",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "supervised multi-task learning for multimodal affect recognition,”"
        },
        {
          "8. References": "Dialogue.\nSingapore and Online: Association for Computational",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2021."
        },
        {
          "8. References": "Linguistics,\nJul.\n2021,\npp.\n257–260.\n[Online].\nAvailable:",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[23] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, “Multimodal end-"
        },
        {
          "8. References": "https://aclanthology.org/2021.sigdial-1.27",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "to-end sparse model for emotion recognition,” in Proceedings of"
        },
        {
          "8. References": "[5] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "the NAACL-HLT 2021.\nOnline: ACL, Jun. 2021, pp. 5305–5316."
        },
        {
          "8. References": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[24] N. Lubis, D. Lestari, A. Purwarianti, S. Sakti,\nand S. Naka-"
        },
        {
          "8. References": "interactive emotional dyadic motion capture database,” Language",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "mura, “Construction and analysis of indonesian emotional speech"
        },
        {
          "8. References": "Resources and Evaluation, vol. 42, no. 4, pp. 335–359, Nov. 2008.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2014\n17th Oriental Chapter\nof\nthe\nInternational\ncorpus,”\nin"
        },
        {
          "8. References": "[6]\nS. R. Livingstone and F. A. Russo,\n“The ryerson audio-visual",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Committee for the Co-ordination and Standardization of Speech"
        },
        {
          "8. References": "database of emotional speech and song (RAVDESS): A dynamic,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Databases and Assessment Techniques\n(COCOSDA).\nIEEE,"
        },
        {
          "8. References": "multimodal set of facial and vocal expressions in north american",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2014, pp. 1–5."
        },
        {
          "8. References": "english,” PLOS ONE, vol. 13, no. 5, May 2018.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[25] N. Lubis, S. Sakti, G. Neubig, T. Toda, A. Purwarianti,\nand"
        },
        {
          "8. References": "[7] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria,\nand L.-P.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "S. Nakamura,\n“Emotion and its\ntriggers\nin human spoken dia-"
        },
        {
          "8. References": "Morency, “Multi-attention recurrent network for human commu-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Situated Dialog in Speech-\nlogue:\nRecognition and analysis,”"
        },
        {
          "8. References": "nication comprehension,” in 32nd AAAI, 2018.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Based Human-Computer Interaction, pp. 103–110, 2016."
        },
        {
          "8. References": "[8]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[26]\nS. Hareli, K. Kafetsios, and U. Hess, “A cross-cultural study on"
        },
        {
          "8. References": "R. Mihalcea, “MELD: A multimodal multi-party dataset for emo-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "emotion expression and the learning of social norms,” Frontiers in"
        },
        {
          "8. References": "tion recognition in conversations,” in Proc. 57th ACL.\nACL, Jul.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Psychology, vol. 6, Oct. 2015."
        },
        {
          "8. References": "2019, pp. 527–536.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[27] N. Lim,\n“Cultural differences\nin emotion:\ndifferences\nin emo-"
        },
        {
          "8. References": "[9]\nE. Kim, D. Bryant, D. Srikanth, and A. Howard, “Age bias\nin",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "tional arousal\nlevel between the east and the west,” Integrative"
        },
        {
          "8. References": "emotion detection: An analysis of facial emotion recognition per-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Medicine Research, vol. 5, no. 2, pp. 105–109, Jun. 2016."
        },
        {
          "8. References": "formance on young, middle-aged, and older adults,” in Proc. 2021",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[28]\nI.\nIosifov,\nO.\nIosifova,\nO. Romanovskyi,\nV\n.\nSokolov,\nand"
        },
        {
          "8. References": "AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES ’21.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "I. Sukailo, “Transferability evaluation of speech emotion recogni-"
        },
        {
          "8. References": "New York, NY, USA: Association for Computing Machinery,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "tion between different\nlanguages,” in Advances in Computer Sci-"
        },
        {
          "8. References": "2021, p. 638–644.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "ence for Engineering and Education.\nSpringer International Pub-"
        },
        {
          "8. References": "[10] D. Rouzet, A. C. Sanchez, T. Renault, and O. Roehn, “Fiscal chal-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "lishing, 2022, pp. 413–426."
        },
        {
          "8. References": "lenges and inclusive growth in ageing societies,” no. 27, 2019.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[29] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi-"
        },
        {
          "8. References": "[11] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "nov, and A. Mohamed, “Hubert: Self-supervised speech represen-"
        },
        {
          "8. References": "and R. Verma,\n“CREMA-d:\nCrowd-sourced emotional multi-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "tation learning by masked prediction of hidden units,” 2021."
        },
        {
          "8. References": "modal actors dataset,” IEEE Transactions on Affective Computing,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[30] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,"
        },
        {
          "8. References": "vol. 5, no. 4, pp. 377–390, Oct. 2014.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau,"
        },
        {
          "8. References": "[12] K. Ma, X. Wang, X. Yang, M. Zhang,\nJ. M. Girard, and L.-P.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "and M. Auli, “XLS-R: Self-supervised Cross-lingual Speech Rep-"
        },
        {
          "8. References": "Morency, “Elderreact: A multimodal dataset for recognizing emo-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "resentation Learning at Scale,” in Proc. Interspeech 2022, 2022."
        },
        {
          "8. References": "tional response in aging adults,” in 2019 International Conference",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[31]\nZ. Fan, M. Li, S. Zhou, and B. Xu, “Exploring wav2vec 2.0 on"
        },
        {
          "8. References": "on Multimodal Interaction, ser. ICMI ’19.\nNew York, NY, USA:",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Speaker Verification and Language Identification,” in Proc. Inter-"
        },
        {
          "8. References": "Association for Computing Machinery, 2019, p. 349–357.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "speech 2021, 2021, pp. 1509–1513."
        },
        {
          "8. References": "[13] M. K. Pichora-Fuller and K. Dupuis, “Toronto emotional speech",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[32]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from"
        },
        {
          "8. References": "set (tess),” 2020.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "Interspeech\nSpeech Using wav2vec 2.0 Embeddings,”\nin Proc."
        },
        {
          "8. References": "[14] M. C. Lee, S. C. Yeh, S. Y. Chiu, and J. W. Chang, “A deep convo-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "2021, 2021, pp. 3400–3404."
        },
        {
          "8. References": "lutional neural network based virtual elderly companion agent,” in",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[33] H. Lovenia, S. Cahyawijaya, G. Winata, P. Xu, Y. Xu, Z. Liu,"
        },
        {
          "8. References": "Proceedings of\nthe 8th ACM on Multimedia Systems Conference.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "R. Frieske, T. Yu, W. Dai, E. J. Barezi, Q. Chen, X. Ma, B. Shi,"
        },
        {
          "8. References": "ACM, Jun. 2017.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "and P. Fung, “ASCEND: A spontaneous Chinese-English dataset"
        },
        {
          "8. References": "[15] A. Baevski, H. Zhou, A. Mohamed,\nand M. Auli,\n“Wav2vec",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "13th\nfor\ncode-switching\nin multi-turn\nconversation,”\nin Proc."
        },
        {
          "8. References": "2.0: A framework for self-supervised learning of speech repre-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "LREC.\nELRA, Jun. 2022, pp. 7259–7268."
        },
        {
          "8. References": "the 34th International Conference\nsentations,” in Proceedings of",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[34]\nS. Cahyawijaya, H. Lovenia, A. F. Aji, G. I. Winata, B. Wilie,\n,"
        },
        {
          "8. References": "on Neural Information Processing Systems, ser. NIPS’20.\nRed",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "R. Mahendra, C. Wibisono, A. Romadhony, K. Vincentio, F. Koto,"
        },
        {
          "8. References": "Hook, NY, USA: Curran Associates Inc., 2020.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "J. Santoso, D. Moeljadi et al., “Nusacrowd: Open source initiative"
        },
        {
          "8. References": "[16]\nJ. Martinez-Miranda and A. Aldea, “Emotions in human and arti-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "for indonesian nlp resources,” 2022."
        },
        {
          "8. References": "ficial intelligence,” Computers in Human Behavior, vol. 21, no. 2,",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "[35] W. Dai, S. Cahyawijaya, T. Yu, E. J. Barezi, P. Xu, C. T. Yiu,"
        },
        {
          "8. References": "pp. 323–341, Mar. 2005.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": ""
        },
        {
          "8. References": "",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "R. Frieske, H. Lovenia, G. Winata, Q. Chen, X. Ma, B. Shi, and"
        },
        {
          "8. References": "[17] D. Bertero and P. Fung, “Multimodal deep neural nets for detect-",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "P. Fung, “CI-AVSR: A Cantonese audio-visual speech datasetfor"
        },
        {
          "8. References": "ing humor in tv sitcoms,” in 2016 IEEE SLT Workshop, 2016.",
          "[18]\nL. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, “Machine learning": "in-car command recognition,” in Proc. 13th LREC.\nELRA, 2022."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "version: Theory, databases and esd,” Speech Communication, vol."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "137, pp. 1–18, 2022."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[37] G.\nI. Winata,\nS. Cahyawijaya, Z. Liu, Z. Lin, A. Madotto,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "P. Xu, and P. Fung, “Learning Fast Adaptation on Cross-Accented"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Speech Recognition,” in Proc. Interspeech 2020, 2020."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[38]\nJ. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. John-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "son, “XTREME: A massively multilingual multi-task benchmark"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "for evaluating cross-lingual generalisation,” in Proc. 37th ICML,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "ser. Proceedings of Machine Learning Research, H. D.\nIII and"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "A. Singh, Eds., vol. 119.\nPMLR, 13–18 Jul 2020."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[39] G.\nI. Winata, A. F. Aji, S. Cahyawijaya, R. Mahendra, F. Koto,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "A. Romadhony, K. Kurniawan, D. Moeljadi, R. E. Prasojo,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "P. Fung, T. Baldwin, J. H. Lau, R. Sennrich, and S. Ruder, “Nusax:"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Multilingual parallel\nsentiment dataset\nfor 10 indonesian local"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "languages,” 2022."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[40]\nS.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, and S.-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "H. Lai, “Auggan: Cross domain adaptation with gan-based data"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "the European Conference on\naugmentation,”\nin Proceedings of"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Computer Vision (ECCV), September 2018."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[41]\nJ. Singh, B. McCann, N. S. Keskar, C. Xiong, and R. Socher,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "“Xlda: Cross-lingual data augmentation for natural\nlanguage in-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "ference and question answering,” 2019."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[42]\nI. Jolliffe, Principal Component Analysis.\nSpringer Verlag, 1986."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[43] A. Hyv¨arinen\nand E. Oja,\n“Independent\ncomponent\nanalysis:"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "The Official\nalgorithms\nand\napplications,” Neural Networks:"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Journal of\nthe\nInternational Neural Network Society,\nvol. 13,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "no. 4-5,\npp. 411–430,\nJun. 2000, PMID: 10946390.\n[Online]."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Available: http://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "pdf"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[44] N. Halko, P. G. Martinsson, and J. A. Tropp, “Finding structure"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "with\nrandomness:\nProbabilistic\nalgorithms\nfor\nconstructing"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "approximate matrix\ndecompositions,”\nSIAM Review,\nvol.\n53,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "no. 2, pp. 217–288, 2011. [Online]. Available: https://doi.org/10."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "1137/090771806"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[45] H. Lovenia, H. Tanaka, S. Sakti, A. Purwarianti, and S. Naka-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "mura, “Speech artifact\nremoval\nfrom eeg recordings of\nspoken"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "word production with tensor decomposition,” in ICASSP 2019 -"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "2019 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Signal Processing (ICASSP), 2019, pp. 1115–1119."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[46] G.\nI. Winata,\nS. Cahyawijaya, Z. Lin, Z. Liu,\nand P. Fung,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "“Lightweight and efficient end-to-end speech recognition using"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "low-rank transformer,”\nin ICASSP 2020 - 2020 IEEE Interna-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "(ICASSP), 2020, pp. 6144–6148."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[47]\nS. Cahyawijaya, G.\nI. Winata, H. Lovenia, B. Wilie, W. Dai,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "E. Ishii, and P. Fung, “Greenformer: Factorization toolkit for effi-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "cient deep neural networks,” 2021."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[48]\nS. Cahyawijaya,\n“Greenformers:\nImproving\ncomputation\nand"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "memory efficiency in transformer models via low-rank approxi-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "mation,” 2021."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[49]\nL. McInnes, J. Healy, N. Saul, and L. Grossberger, “Umap: Uni-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Journal of\nform manifold approximation and projection,” The"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Open Source Software, vol. 3, no. 29, p. 861, 2018."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[50]\nI. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "tion,” in ICLR, 2019."
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "[51] C. Fu, T. Dissanayake, K. Hosoda, T. Maekawa, and H. Ishiguro,"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "“Similarity of speech emotion in different\nlanguages revealed by"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "a neural network with attention,” in 2020 IEEE 14th International"
        },
        {
          "[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Emotional voice con-": "Conference on Semantic Computing (ICSC).\nIEEE, Feb. 2020."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The role of affect and emotion in HCI",
      "authors": [
        "R Beale",
        "C Peter"
      ],
      "year": "2008",
      "venue": "Affect and Emotion in Human-Computer Interaction"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "4",
      "title": "Nora: The well-being coach",
      "authors": [
        "G Winata",
        "H Lovenia",
        "E Ishii",
        "F Siddique",
        "Y Yang",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Nora: The well-being coach"
    },
    {
      "citation_id": "5",
      "title": "ERICA: An empathetic android companion for covid-19 quarantine",
      "authors": [
        "E Ishii",
        "G Winata",
        "S Cahyawijaya",
        "D Lala",
        "T Kawahara",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue. Singapore and Online"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "7",
      "title": "The ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "8",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Multi-attention recurrent network for human communication comprehension"
    },
    {
      "citation_id": "9",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. 57th ACL. ACL"
    },
    {
      "citation_id": "10",
      "title": "Age bias in emotion detection: An analysis of facial emotion recognition performance on young, middle-aged, and older adults",
      "authors": [
        "E Kim",
        "D Bryant",
        "D Srikanth",
        "A Howard"
      ],
      "year": "2021",
      "venue": "Proc. 2021 AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES '21"
    },
    {
      "citation_id": "11",
      "title": "Fiscal challenges and inclusive growth in ageing societies",
      "authors": [
        "D Rouzet",
        "A Sanchez",
        "T Renault",
        "O Roehn"
      ],
      "year": "2019",
      "venue": "Fiscal challenges and inclusive growth in ageing societies"
    },
    {
      "citation_id": "12",
      "title": "CREMA-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Elderreact: A multimodal dataset for recognizing emotional response in aging adults",
      "authors": [
        "K Ma",
        "X Wang",
        "X Yang",
        "M Zhang",
        "J Girard",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction, ser. ICMI '19"
    },
    {
      "citation_id": "14",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (tess)"
    },
    {
      "citation_id": "15",
      "title": "A deep convolutional neural network based virtual elderly companion agent",
      "authors": [
        "M Lee",
        "S Yeh",
        "S Chiu",
        "J Chang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 8th ACM on Multimedia Systems Conference"
    },
    {
      "citation_id": "16",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS'20"
    },
    {
      "citation_id": "17",
      "title": "Emotions in human and artificial intelligence",
      "authors": [
        "J Martinez-Miranda",
        "A Aldea"
      ],
      "year": "2005",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "18",
      "title": "Multimodal deep neural nets for detecting humor in tv sitcoms",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2016",
      "venue": "2016 IEEE SLT Workshop"
    },
    {
      "citation_id": "19",
      "title": "Machine learning methods in the application of speech emotion recognition",
      "authors": [
        "L Cen",
        "M Dong",
        "H Yu",
        "P Ch"
      ],
      "year": "2010",
      "venue": "Application of Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "A first look into a convolutional neural network for speech emotion detection",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "21",
      "title": "What did i just hear? detecting pornographic sounds in adult videos using neural networks",
      "authors": [
        "H Lovenia",
        "D Lestari",
        "R Frieske"
      ],
      "year": "2022",
      "venue": "Proceedings of the 17th International Audio Mostly Conference"
    },
    {
      "citation_id": "22",
      "title": "Ai-based emotion recognition: Promise, peril, and prescriptions for prosocial path",
      "authors": [
        "S Latif",
        "H Ali",
        "M Usama",
        "R Rana",
        "B Schuller",
        "J Qadir"
      ],
      "year": "2022",
      "venue": "Ai-based emotion recognition: Promise, peril, and prescriptions for prosocial path"
    },
    {
      "citation_id": "23",
      "title": "Weaklysupervised multi-task learning for multimodal affect recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Y Bang",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Weaklysupervised multi-task learning for multimodal affect recognition"
    },
    {
      "citation_id": "24",
      "title": "Multimodal endto-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Proceedings of the NAACL-HLT 2021. Online: ACL"
    },
    {
      "citation_id": "25",
      "title": "Construction and analysis of indonesian emotional speech corpus",
      "authors": [
        "N Lubis",
        "D Lestari",
        "A Purwarianti",
        "S Sakti",
        "S Nakamura"
      ],
      "venue": "the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)"
    },
    {
      "citation_id": "26",
      "title": "Emotion and its triggers in human spoken dialogue: Recognition and analysis",
      "authors": [
        "N Lubis",
        "S Sakti",
        "G Neubig",
        "T Toda",
        "A Purwarianti",
        "S Nakamura"
      ],
      "year": "2016",
      "venue": "Emotion and its triggers in human spoken dialogue: Recognition and analysis"
    },
    {
      "citation_id": "27",
      "title": "A cross-cultural study on emotion expression and the learning of social norms",
      "authors": [
        "S Hareli",
        "K Kafetsios",
        "U Hess"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "28",
      "title": "Cultural differences in emotion: differences in emotional arousal level between the east and the west",
      "authors": [
        "N Lim"
      ],
      "year": "2016",
      "venue": "Integrative Medicine Research"
    },
    {
      "citation_id": "29",
      "title": "Transferability evaluation of speech emotion recognition between different languages",
      "authors": [
        "I Iosifov",
        "O Iosifova",
        "O Romanovskyi",
        "V Sokolov",
        "I Sukailo"
      ],
      "year": "2022",
      "venue": "Advances in Computer Science for Engineering and Education"
    },
    {
      "citation_id": "30",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "31",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
      "authors": [
        "Z Fan",
        "M Li",
        "S Zhou",
        "B Xu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "33",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "ASCEND: A spontaneous Chinese-English dataset for code-switching in multi-turn conversation",
      "authors": [
        "H Lovenia",
        "S Cahyawijaya",
        "G Winata",
        "P Xu",
        "Y Xu",
        "Z Liu",
        "R Frieske",
        "T Yu",
        "W Dai",
        "E Barezi",
        "Q Chen",
        "X Ma",
        "B Shi",
        "P Fung"
      ],
      "year": "2022",
      "venue": "Proc. 13th LREC"
    },
    {
      "citation_id": "35",
      "title": "Nusacrowd: Open source initiative for indonesian nlp resources",
      "authors": [
        "S Cahyawijaya",
        "H Lovenia",
        "A Aji",
        "G Winata",
        "B Wilie",
        "R Mahendra",
        "C Wibisono",
        "A Romadhony",
        "K Vincentio",
        "F Koto",
        "J Santoso",
        "D Moeljadi"
      ],
      "year": "2022",
      "venue": "Nusacrowd: Open source initiative for indonesian nlp resources"
    },
    {
      "citation_id": "36",
      "title": "CI-AVSR: A Cantonese audio-visual speech datasetfor in-car command recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "T Yu",
        "E Barezi",
        "P Xu",
        "C Yiu",
        "R Frieske",
        "H Lovenia",
        "G Winata",
        "Q Chen",
        "X Ma",
        "B Shi",
        "P Fung"
      ],
      "year": "2022",
      "venue": "Proc. 13th LREC"
    },
    {
      "citation_id": "37",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "38",
      "title": "Learning Fast Adaptation on Cross-Accented Speech Recognition",
      "authors": [
        "G Winata",
        "S Cahyawijaya",
        "Z Liu",
        "Z Lin",
        "A Madotto",
        "P Xu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "39",
      "title": "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "authors": [
        "J Hu",
        "S Ruder",
        "A Siddhant",
        "G Neubig",
        "O Firat",
        "M Johnson"
      ],
      "year": "2020",
      "venue": "Proc. 37th ICML, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "40",
      "title": "Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages",
      "authors": [
        "G Winata",
        "A Aji",
        "S Cahyawijaya",
        "R Mahendra",
        "F Koto",
        "A Romadhony",
        "K Kurniawan",
        "D Moeljadi",
        "R Prasojo",
        "P Fung",
        "T Baldwin",
        "J Lau",
        "R Sennrich",
        "S Ruder"
      ],
      "year": "2022",
      "venue": "Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages"
    },
    {
      "citation_id": "41",
      "title": "Auggan: Cross domain adaptation with gan-based data augmentation",
      "authors": [
        "S.-W Huang",
        "C.-T Lin",
        "S.-P Chen",
        "Y.-Y Wu",
        "P.-H Hsu",
        "S.-H Lai"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "42",
      "title": "Xlda: Cross-lingual data augmentation for natural language inference and question answering",
      "authors": [
        "J Singh",
        "B Mccann",
        "N Keskar",
        "C Xiong",
        "R Socher"
      ],
      "year": "2019",
      "venue": "Xlda: Cross-lingual data augmentation for natural language inference and question answering"
    },
    {
      "citation_id": "43",
      "title": "Principal Component Analysis",
      "authors": [
        "I Jolliffe"
      ],
      "year": "1986",
      "venue": "Principal Component Analysis"
    },
    {
      "citation_id": "44",
      "title": "Independent component analysis: algorithms and applications",
      "authors": [
        "A Hyvärinen",
        "E Oja"
      ],
      "year": "2000",
      "venue": "Neural Networks: The Official Journal of the International Neural Network Society"
    },
    {
      "citation_id": "45",
      "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "authors": [
        "N Halko",
        "P Martinsson",
        "J Tropp"
      ],
      "year": "2011",
      "venue": "SIAM Review",
      "doi": "10.1137/090771806"
    },
    {
      "citation_id": "46",
      "title": "Speech artifact removal from eeg recordings of spoken word production with tensor decomposition",
      "authors": [
        "H Lovenia",
        "H Tanaka",
        "S Sakti",
        "A Purwarianti",
        "S Nakamura"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Lightweight and efficient end-to-end speech recognition using low-rank transformer",
      "authors": [
        "G Winata",
        "S Cahyawijaya",
        "Z Lin",
        "Z Liu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Greenformer: Factorization toolkit for efficient deep neural networks",
      "authors": [
        "S Cahyawijaya",
        "G Winata",
        "H Lovenia",
        "B Wilie",
        "W Dai",
        "E Ishii",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Greenformer: Factorization toolkit for efficient deep neural networks"
    },
    {
      "citation_id": "49",
      "title": "Greenformers: Improving computation and memory efficiency in transformer models via low-rank approximation",
      "authors": [
        "S Cahyawijaya"
      ],
      "year": "2021",
      "venue": "Greenformers: Improving computation and memory efficiency in transformer models via low-rank approximation"
    },
    {
      "citation_id": "50",
      "title": "Umap: Uniform manifold approximation and projection",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "N Saul",
        "L Grossberger"
      ],
      "year": "2018",
      "venue": "The Journal of Open Source Software"
    },
    {
      "citation_id": "51",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "52",
      "title": "Similarity of speech emotion in different languages revealed by a neural network with attention",
      "authors": [
        "C Fu",
        "T Dissanayake",
        "K Hosoda",
        "T Maekawa",
        "H Ishiguro"
      ],
      "year": "2020",
      "venue": "2020 IEEE 14th International Conference on Semantic Computing (ICSC)"
    }
  ]
}