{
  "paper_id": "2010.06200v1",
  "title": "End-To-End Triplet Loss Based Emotion Embedding System For Speech Emotion Recognition",
  "published": "2020-10-13T06:56:41Z",
  "authors": [
    "Puneet Kumar",
    "Sidharth Jain",
    "Balasubramanian Raman",
    "Partha Pratim Roy",
    "Masakazu Iwamura"
  ],
  "keywords": [
    "Affective Computing",
    "Deep Learning",
    "Emotion Recognition",
    "End-to-end Speech Processing",
    "Residual Neural Network",
    "Cosine Similarity"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, an end-to-end neural embedding system based on triplet loss and residual learning has been proposed for speech emotion recognition. The proposed system learns the embeddings from the emotional information of the speech utterances. The learned embeddings are used to recognize the emotions portrayed by given speech samples of various lengths. The proposed system implements Residual Neural Network architecture. It is trained using softmax pre-training and triplet loss function. The weights between the fully connected and embedding layers of the trained network are used to calculate the embedding values. The embedding representations of various emotions are mapped onto a hyperplane, and the angles among them are computed using the cosine similarity. These angles are utilized to classify a new speech sample into its appropriate emotion class. The proposed system has demonstrated 91.67% and 64.44% accuracy while recognizing emotions for RAVDESS and IEMOCAP dataset, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The need to develop efficient speech processing systems that are capable of recognizing various emotions from the speech is increasing at a fast rate  [1] . Such systems are useful for a wide range of applications such as robotics, security, service delivery, language translation, automated identification, intelligent toys, and lie detection  [2] . Speech is one of the important ways for a human to portray complex emotions. It can also be used as an efficient method of human-machine interaction  [3] . However, a major challenge in human-machine interaction is the correct detection of emotion from speech. It is natural for a human to recognize underlying emotions during their spoken interactions. However, it is difficult for machines to recognize complex emotions in natural speech.\n\nEmotional information included in a speech signal depends on several factors such as speaker, style, language, gender, accent, and sample-duration  [3] . The notions of various emotions are highly subjective. People interpret them differently depending upon their culture and environment. Likewise, labeling the speech data with suitable emotion during its preparation is also subjected to human variability. A potential approach to reduce human fluctuations in Speech Emotion Recognition (SER) is to develop an SER system that can recognize speech corresponding to various emotions without human intervention.\n\nSuch systems are called end-to-end systems, and as opposed to the conventional methods of emotional speech recognition, they do not require manual crafting of acoustic features. There is a need to develop an end-to-end SER system that can learn the emotional patterns in input speech data despite the aforementioned variations and bypassing the intermediate steps of speech processing  [4] .\n\nThe proposed model is based on Residual Neural Network (ResNet) architecture. The model is trained using softmax pretraining and triplet loss function. Then the embedding values are calculated from the weights between the fully connected and embedding layers. The embeddings are mapped onto a hyperplane, and cosine similarity values are calculated by measuring the cosine of the angles among the embedding representations of various emotions. Smaller the angle, higher will be the cosine similarity. The model is trained to minimize the angles among the representations of the speech of similar emotions and maximize the angles among the representations of the speech of different emotions. The computed angles are utilized to classify a new speech sample into its appropriate emotion class. The proposed approach has been validated for two emotional speech datasets -The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and The Interactive Emotional Dyadic Motion Capture (IEMOCAP). Recognition accuracies of 91.67% and 64.44% have been observed for RAVDESS and IEMOCAP dataset, respectively.\n\nThe major contributions of the paper are as follows. Firstly, a deep neural end-to-end SER system based on triplet loss and residual learning has been proposed. The proposed system is capable of learning emotion-related information from a labeled emotional speech dataset in the form of embeddings. Secondly, the embeddings learned by the proposed system are used to classify the speech samples of various lengths into appropriate emotion classes. Using the embeddings, the proposed system can estimate the emotions in unseen speech utterances.\n\nThe rest of the paper is organized as follows. Existing work in the context of speech emotion recognition has been surveyed in Section II. Section III elaborates on the proposed methodology. In Section IV, implementation details and experimental results have been discussed. Finally, Section V concludes the paper and highlights the scope for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In recent years, several SER approaches have been developed. Feature-based speech recognition systems attempt to extract characteristics from acoustic features such as fundamental frequencies, pitch, prosody, voice quality, Mel frequency cepstrum coefficient (MFCC), and linear prediction cepstrum coefficient (LPCC). They use these features to analyze the emotion patterns of speech samples  [2] . In this context, C. Lee et al.  [5]  used pitch, formants, and speech rate to differentiate positive emotions from negative emotions in speech signals.\n\nIn another work, J. Rong et al.  [6]  developed a data preprocessing technique to extract the most relevant acoustic features for emotion recognition. It has been observed that the features derived from high-key emotions such as happiness, anger, and interest show similar properties among themselves which are very different from low-key emotions such as sadness and despair. Hence, there is a need to derive an SER method that is independent of the polarity of the emotional features.\n\nHidden Markov model (HMM) based statistical methods and support vector machine (SVM) based classifiers have also been used for SER. For example, J. Lorenzo et al.  [7]  proposed an HMM-based method to detect and alter the emotional context of a speech sample while preserving the identity of the speaker. In another work, P. Shen et al.  [8]  trained an SVM based classifier to differentiate the emotions present in speech signals based on acoustic features. They used SVM to detect speech emotions accounting for gender-based variations. One of the major problems with using HMM-based SER models is that they are not always able to reliably estimate the parameters of global speech features  [2] .\n\nThe above-mentioned approaches to emotional speech recognition require manual crafting of acoustic features. Hence, it is challenging to come up with an end-to-end SER system using them  [9] . Neural network based models are capable of automatically extracting the features from the training data. In context of using them for SER, Stuhlsatz et al.  [10]  compared the performance of a neural network based Discriminant Analysis with SVM for the classification of emotional speech utterances. The neural network based classifier was observed to outperform the SVM based classifier for speech emotion detection. In another work, Mao et al.  [11]  used CNN to extract the features from speech spectrograms. Then they classified the features using a binary classifier. Their model outperformed classic machine learning models.\n\nDeep learning based systems have been used for other speech processing tasks as well. For instance, speaker recognition has been implemented by training speaker embeddings and differentiating the speakers based on them  [12] . Similarly, A. Jain et al.  [13]  implemented speaker independent accent embedding to differentiate multi accent speech. In the context of RNN based SER, N. Majumder et al.  [14]  implemented attention-based RNN to keep track of the identities of the speakers portraying specific emotions through conversations. In another work, S. Sahoo et al.  [15]  utilized a pre-trained deep convolutional neural network to predict the emotion classes of the audio segments. Deep neural networks along with residual learning  [16]  and triplet loss  [17]  are commonly used for facial expression recognition. They have been used in the area of speech processing also. For example, J. Kim et al.  [18]  implemented a deep residual network for speech emotion recognition. In another work, H. Bredin  [19]  used triplet loss along with LSTM to learn the embeddings for speech sequences. The embeddings were later used for speaker identification. As speech emotion recognition is a counterpart of facial emotion recognition, triplet loss and residual learning based techniques may prove to be useful for SER also.\n\nThe success of deep neural networks for various speech processing tasks advocates their suitability for SER. However, end-to-end SER using triplet loss and residual learning along with deep neural networks has not been explored to its full potential. With that as an inspiration, various state-of-the-art deep neural architectures have been implemented and the best performing one is implemented in the proposed work. The proposed system also overcomes the issues with existing SER approaches, i.e., need for manual crafting of acoustic features, bias towards the polarity of the emotional features in featurebased SER, and unreliability of statistical SER systems in estimating the parameters of the global speech features.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed System",
      "text": "This section describes an end-to-end approach to learn the embedding representations from emotional speech and use them for speech emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "Consider d-dimensional space R d where elements in R d are represented as {x 1 , x 2 , x 3 , .., x d } where x i is a d-tuple that denotes an embedding vector f (x) ∈ R d mapped from a set of speech utterances j y j . The projections of such embedding vectors are represented in a hyperplane where emotion similarity is measured using cosine similarity. The objectives of the proposed technique are to:\n\ni Learn the embeddings from input speech utterances, ii Visualize the embeddings projected in a hyperplane to analyze the learned emotion patterns, iii Use the learned embeddings to classify an unseen speech utterance into an appropriate emotion class. Fig.  1  explains the core hypothesis of the proposed work. Ideally, the emotion recognition should be independent of the length of the speech utterance. For example, the speech utterance S1 is longer than S2, but both incorporate the same emotion e1. Hence, the angle α 12 between them is expected to be smaller than the angles between the speech emotions of dissimilar emotions such as α 13 and α 14 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Methodology",
      "text": "Various phases of the proposed methodology are represented in Fig.  2  and discussed in the following sections.\n\n1) Phase I: Initialization and Pre-processing: The embeddings are initialized and data is pre-processed. a. Emotion Embedding Initialization: Emotion embedding is a technique to represent the emotional information of the speech in the form of vectors. It learns the emotional content of the speech and constructs the vector representations for it. The network weights are defined by a temporal average layer which takes frame-level activations and computes the layer activation h as follows:\n\nwhere :\n\nT : number of f rames in a speech utterance.\n\nt : time instance.\n\nx : embedding at given time instance t.\n\nh : activation of the layer.\n\n(\n\nEmotion embeddings are learned using the weights between the fully connected layer and the embedding layer after the training completes. Here, initial model weights are initialized using softmax pre-training, and the final weights are assigned to these embeddings in later phases.\n\nb. Audio Cache Generation & Pre-processing: The emotional speech dataset contains audio-clips with emotion labels. For each emotion label, the data is divided into a training set and a testing set. The cache is generated for both, which involves sampling the audio files and trimming the silence. After generating the audio cache, MFCC windows are randomly sampled from it. Since the model has been implemented for constant input shape, an appropriate value for the number of windows for the input signal had to be tuned. We took 10 windows from each input sample using random sampling of 10 continuous MFCC frames. For each window, 39 input values are present which correspond to 13 MFCC values, 13 first-derivative values, and 13 second-derivative values. If the number of windows for an input signal was less than 10, then zero padding was used to keep the input size fixed.\n\n2) Phase II: Embedding Training: A fully connected layer projects the utterance-level representations as embeddings. Emotion characteristics of the speech are learned by training embedding vectors for each emotion. The cross-entropy loss function is used to train the network along with triplet loss.\n\n• Cross-entropy Loss (Softmax Loss): When the output probability of a classification model is between 0 and 1, the cross-entropy loss function can be used to measure its performance. It produces stabler convergence for the data with noisy labels than other methods like mean absolute error loss, and categorical cross-entropy loss  [20] .\n\n• Triplet Loss: It is an optimization approach that compares a baseline input to a positive input and a negative input. It takes three speech samples and compares them in pairs. The distance between baseline input and positive input is minimized, and the distance between the baseline input and negative input is maximized  [17] .\n\nwhere :\n\nJ : triplet loss cost f unction.\n\nF : intermediate f unction.\n\nx e1 i : embeddings f or emotion 1.\n\nx e2 i : embeddings f or emotion 2.\n\nx (2)\n\nTriplet loss function, J is defined for three emotion embeddings: emotion 1, emotion 2, and emotion 3. Assuming that emotion 1 is similar to emotion 2, but it is dis-similar to emotion 3. Then the distance between e 1 and e 2 will be minimized, and that between e 1 and e 3 will be maximized. This is achieved by function F . Triplets of various emotion pairs are chosen at random, and all the speech utterances are covered. The cost function J computes the overall triplet loss.\n\nCosine Similarity: Triplet loss is internally optimized using cosine similarity. The cosine similarity has successfully been used to check the similarity among the texts of unequal lengths. With that inspiration, the proposed method has implemented it to check the similarity among speech samples of varying lengths. It calculates the cosine of the angle between the vectors projected in a multi-dimensional space. Smaller the angle becomes, higher will be the cosine similarity. Triplet loss checks the embeddings in pairs. It aims to maximize the cosine similarities for the pairs with the same emotions and minimize those with different emotions cos(x i , x j ) = x T i x j where :\n\nx i and x j : two emotion embeddings.\n\n(3)\n\nThe training process utilizes the above-mentioned concepts, i.e., softmax loss, triplet loss, and cosine similarity. It is carried out in the following two phases.\n\na. Softmax Pre-training: Softmax pre-training computes both softmax loss and triplet loss, and it trains the model for softmax loss. During pre-training, softmax pre-training is used to initialize the weights of the network. It maps the nonnormalized output of the network to a probability distribution over predicted output classes. Softmax pre-training has been found to help avoid getting stuck in a local minimum and producing stabler convergence along with triplet loss  [12] .\n\nb. Embedding Training with Triplet Loss: This is the full training phase. It computes and trains on both loss values, i.e., softmax loss and triplet loss. Before generating the embeddings, the model weights are L 2 normalized. Then the embedding vector is generated, and triplet loss is calculated and minimized. Although the triplet loss function is well-used for face emotion recognition, it has also found its applications in the area of speech recognition where it has been used to learn and represent the speaker embeddings from speech utterances  [12] . Softmax cross-entropy loss works well for a fixed number of classes. However, when there are a variable number of output classes then triplet loss can be used to learn good embeddings for each variation of each class  [17] .\n\nIt is observed that the softmax pre-training along with crossentropy loss resulted in better performance than triplet loss implementation along with cross-entropy loss. The combination of triplet loss implementation along with cross-entropy for the input pre-processed with softmax pre-training performed even better.\n\n3) Phase III: Emotion Inference: The steps to infer the emotion category for the unseen samples are as follows. a. Compile the test speech utterances in a new folder. b. Update the cache for the new folder. c. Generate new embeddings using the trained network. d. Check cosine similarity for the generated embeddings in comparision with already generated embeddings for the emotional classes in the training phase. e. Project various embeddings onto the R d hyperplane and measure the angles among them to determine their emotion category.\n\nThe model training focuses on learning the emotion embeddings using triplet loss and cosine similarity. The cosine similarity is used for the inference as well. It helped to learn a good representation of the emotional information in speech utterances and gave an idea about the quality of the embeddings produced in the testing phase.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "This section discusses the experimental implementation and analyses the results. 3) Network Architecture: The ablation study to choose the appropriate network architecture has been performed with a sliced RAVDESS dataset containing 1600 samples, with 200 samples for each emotion class. The dataset was divided into training and testing sets in 70% and 30% ratio. Fully Connected (FC) network, CNN, Residual Neural Network (ResNet), RNN, Long Short Term Memory (LSTM) based RNN, and Gated Recurrent Unit (GRU) based RNN have been evaluated. They have been analyzed to extract emotional features embedded in acoustic input, train utterance level emotion embeddings, and train the network using cosine similarity based triplet loss function. Their details have been presented in Table  I . Here 'x' represents the total number of layers. The analysis is performed for x = 6 to 15.  ResNet with 11 layers (and two residual blocks) performed best among these networks. It has been chosen as the suitable architecture for the proposed implementation. ResNet performs relation extraction using deep residual learning. A residual network contains skip connections among convolutional layers. It is known to perform better for large networks  [12] . Fig.  3  depicts a representative diagram of the implemented ResNet architecture. The network contains the following layers: Input Layer -to feed the input to the network in vector form; Conv1D Layers -to build ResNet block; BatchNorm Layer -to normalize the input and generate triplet loss; LeakyReLU Layer -consists of the activation function to define the output of a particular layer; Flatten Layer -to convert pooled feature map to a single column; Fully Connected Layer -the last layer of the network that takes input from the flatten layer and Softmax Layer -to generate cross-entropy loss. Dimensions of the layers have been shown along with them. The shape of the input is 900X390, where the first dimension 900 corresponds to the batch size, and the second dimension 390 corresponds to the feature values of an individual input speech sample.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Results And Discussion",
      "text": "The proposed system learns emotional embeddings from the model weights of the network trained on emotional speech datasets. The learned embeddings are used to recognize the speech samples of various emotions. The cosine similarity values are calculated by applying the Euclidean dot product function onto the embeddings represented in the hyperplane. Triplet loss function is used for model training. It aims to minimize the angles among the representations of the speech of similar emotions and maximize the angles among the representations of the speech of different emotions. The computed angles are utilized to classify a new speech sample into its appropriate emotion class. The following sections discuss the learned embeddings, the angles among their projections, and the emotion classification performance.\n\n1) Emotion Embeddings Visualization: The embedding plots for six important emotions, i.e., anger, neutral, happy, sad, fear, and surprise, have been drawn in Fig.  4 . These are the common emotion labels for RAVDESS and IEMOCAP dataset. Hence, they are selected for the embedding plots for comparative understanding. The experimentally obtained embeddings are visualized using t-distributed stochastic neighbor embedding (t-SNE) visualization method.\n\n2) Emotion Classification: The proposed approach has been validated for RAVDESS and IEMOCAP emotional speech dataset. The angles among the embeddings for RAVDESS and IEMOCAP are described in Table  II  and Table  III . The angles for a given speech utterance is checked with all the emotional classes. It is classified into the class with which it makes the least angle.\n\nThe proposed approach showed an emotion recognition accuracy of 91.67% for RAVDESS dataset and 64.44% for IEMOCAP dataset. Here, 'Un-weighted Accuracy' has been considered, which is defined as total correct predictions over total instances. As discussed in Tables  IV  and V , the proposed approach has demonstrated comparable performance to the benchmark results. It is to be noted that the performance for RAVDESS and IEMOCAP datasets has been compared for      it denotes the projections of the embeddings of the speech utterances in the hyperplane. Lesser deviation in the intra-class angles for RAVDESS corresponds to the closeness in their projections and more accurate SER predictions as compared to IEMOCAP. As observed in Fig.  4 , the emotion embeddings for RAVDESS dataset are more clearly defined than IEMOCAP  dataset. It is also noted that some overlap has been observed between the embeddings of 'sad' and 'fear', both of which are negative-valence emotions. On the other hand, both 'anger' and 'surprise' are emotions with high intensity. They also showed minor overlap between their embeddings. Another observation is that the network converged faster during the training when softmax pre-training was used along with triplet loss training.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusions And Future Work",
      "text": "In this paper, an end-to-end emotion embedding system has been proposed to learn the emotional patterns from speech samples in the form of an embedding matrix. It projects the mappings of speech information onto a hyperplane where triplet loss is used as a loss function to learn the similarities among various emotions based on cosine similarity. The emotion embedding matrix prepared has been used for speech emotion recognition and it demonstrated comparable recognition results to the benchmarks for two different datasets. The proposed system has classified the emotions for RAVDESS dataset with an accuracy of 91.67%, while 64.44% accuracy has been observed for IEMOCAP dataset.\n\nExperiments for various neural architectures to automatically learn the acoustic features were performed, and finally, ResNet was implemented, which demonstrated better performance in finding relational emotion patterns for speech samples. The current implementation requires checking of angles for each speech utterance with each emotion class. This process can be optimized with the aim of reducing training time, computational requirements, and model size In the future, it is also aimed to use the learned embeddings for other speech processing tasks such as emotional speech synthesis.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: explains the core hypothesis of the proposed work.",
      "page": 2
    },
    {
      "caption": "Figure 2: and discussed in the following sections.",
      "page": 2
    },
    {
      "caption": "Figure 1: Hyperplane with projections of speech utterances.",
      "page": 3
    },
    {
      "caption": "Figure 2: Representation of the proposed methodology.",
      "page": 3
    },
    {
      "caption": "Figure 3: Schematic description of proposed architecture.",
      "page": 5
    },
    {
      "caption": "Figure 3: depicts a representative diagram of",
      "page": 5
    },
    {
      "caption": "Figure 4: These are",
      "page": 5
    },
    {
      "caption": "Figure 5: presents the boxplots for the deviation values. Deviation",
      "page": 6
    },
    {
      "caption": "Figure 4: , the emotion embeddings for",
      "page": 6
    },
    {
      "caption": "Figure 4: t-SNE Visualization of Emotion Embeddings.",
      "page": 7
    },
    {
      "caption": "Figure 5: Boxplots of deviation in predicted angles.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "ResCNN\nLayer"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cross Entropy\nLoss": "Softmax Layer",
          "Column_2": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Automatic recognition of emotions from speech: a review of the literature and recommendations for practical realisation",
      "authors": [
        "T Vogt",
        "E André",
        "J Wagner"
      ],
      "year": "2008",
      "venue": "Automatic recognition of emotions from speech: a review of the literature and recommendations for practical realisation"
    },
    {
      "citation_id": "4",
      "title": "Automatic emotion analysis based on speech",
      "authors": [
        "I Chiriacescu"
      ],
      "year": "2009",
      "venue": "Automatic emotion analysis based on speech"
    },
    {
      "citation_id": "5",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on speech and audio processing"
    },
    {
      "citation_id": "6",
      "title": "Acoustic feature selection for automatic emotion recognition from speech",
      "authors": [
        "J Rong",
        "G Li",
        "Y.-P Chen"
      ],
      "year": "2009",
      "venue": "Information processing & management"
    },
    {
      "citation_id": "7",
      "title": "Emotion transplantation through adaptation in hmm-based speech synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "R Barra-Chicote",
        "R San-Segundo",
        "J Ferreiros",
        "J Yamagishi",
        "J Montero"
      ],
      "year": "2015",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "8",
      "title": "Cubic svm classifier based feature extraction and emotion detection from speech signals",
      "authors": [
        "U Jain",
        "K Nathani",
        "N Ruban",
        "A Raj",
        "Z Zhuang",
        "V Mahesh"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Sensor Networks and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "10",
      "title": "Deep neural networks for acoustic emotion recognition: raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "12",
      "title": "Deep Speaker: an end-to-end neural speaker embedding system",
      "authors": [
        "C Li",
        "X Ma",
        "B Jiang",
        "X Li",
        "X Zhang",
        "X Liu",
        "Y Cao",
        "A Kannan",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "Deep Speaker: an end-to-end neural speaker embedding system",
      "arxiv": "arXiv:1705.02304"
    },
    {
      "citation_id": "13",
      "title": "Improved accented speech recognition using accent embeddings and multi-task learning",
      "authors": [
        "A Jain",
        "M Upreti",
        "P Jyothi"
      ],
      "year": "2018",
      "venue": "Improved accented speech recognition using accent embeddings and multi-task learning"
    },
    {
      "citation_id": "14",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "15",
      "title": "A segment level approach to speech emotion recognition using transfer learning",
      "authors": [
        "S Sahoo",
        "P Kumar",
        "B Raman",
        "P Roy"
      ],
      "year": "2019",
      "venue": "Proceedings of the 5th Asian Conference on Pattern Recognition (ACPR)"
    },
    {
      "citation_id": "16",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "FaceNet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Deep temporal models using identity skip-connections for speech emotion recognition",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "TristouNet: triplet loss for speaker turn embedding",
      "authors": [
        "H Bredin"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "21",
      "title": "RAVDESS: The ryerson audio-visual database of emotional speech and song",
      "authors": [
        "S Livingstone",
        "K Peck",
        "F Russo"
      ],
      "year": "2012",
      "venue": "Annual meeting of the canadian society for brain, behaviour and cognitive science"
    },
    {
      "citation_id": "22",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "23",
      "title": "Emotion classification RAVDESS",
      "authors": [
        "M De Pinto"
      ],
      "year": "2019",
      "venue": "Emotion classification RAVDESS"
    },
    {
      "citation_id": "24",
      "title": "Stress detection through speech analysis",
      "authors": [
        "K Tomba",
        "J Dumoulin",
        "E Mugellini",
        "O Khaled",
        "S Hawila"
      ],
      "year": "2018",
      "venue": "International Joint Conference on e-Business and Telecommunications (ICETE)"
    },
    {
      "citation_id": "25",
      "title": "Recognizing emotion from singing and speaking using shared models",
      "authors": [
        "B Zhang",
        "G Essl",
        "E Provost"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "26",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "28",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Conference of the North American Chapter"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition using deep 1D & 2D CNN & LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}