{
  "paper_id": "2510.03060v1",
  "title": "Semantic Differentiation In Speech Emotion Recognition: Insights From Descriptive And Expressive Speech Roles",
  "published": "2025-10-03T14:42:35Z",
  "authors": [
    "Rongchen Guo",
    "Vincent Francoeur",
    "Isar Nejadgholi",
    "Sylvain Gagnon",
    "Miodrag Bolic"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is essential for improving human-computer interaction, yet its accuracy remains constrained by the complexity of emotional nuances in speech. In this study, we distinguish between descriptive semantics, which represents the contextual content of speech, and expressive semantics, which reflects the speaker's emotional state. After watching emotionally charged movie segments, we recorded audio clips of participants describing their experiences, along with the intended emotion tags for each clip, participants' self-rated emotional responses, and their valence/arousal scores. Through experiments, we show that descriptive semantics align with intended emotions, while expressive semantics correlate with evoked emotions. Our findings inform SER applications in human-AI interaction and pave the way for more context-aware AI systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The ability to accurately detect and interpret emotions in speech is vital for developing intelligent systems capable of natural and empathetic humancomputer interactions. Speech Emotion Recognition (SER) has gained significant traction in recent years, driven by applications ranging from virtual assistants to mental health monitoring  (Ley et al., 2019; Rumpa et al., 2015) . Despite these advancements, SER faces persistent challenges due to the complex and multi-dimensional nature of emotions, which often intertwine with contextual and speakerspecific factors.\n\nTraditional approaches to SER have largely focused on acoustic features, such as pitch, energy, and spectral properties, to infer emotional states  (Wu et al., 2011; Bitouk et al., 2010; Venkataramanan and Rajamohan, 2019; Likitha et al., 2017 ; Kwon * Equal contributions † Corresponding author  et al., 2003) . While effective to some extent, these methods often overlook the semantic content of speech, which can provide crucial contextual information. With the advances in natural language processing, it has become increasingly feasible to analyze the semantic aspects of speech for emotion recognition  (Tzirakis et al., 2021; Xu et al., 2021) . However, the interplay between semantic roles and emotional expression remains underexplored. Specifically, the distinction between intended emotions elicited by a stimulus and evoked emotions experienced by the speaker is rarely addressed, leaving a critical gap in the field.\n\nThis paper introduces a novel framework to address this gap by distinguishing two types of semantic roles in speech. We hypothesize that Descriptive semantics captures scenario-specific content, such as the narrative or context described in the speech. In contrast, Expressive semantics reflects the speaker's subjective emotional stance, shaped by their personal experiences and delivery style. In our framework, descriptive segments are expected to align with the intended emotion of the stimulus (the target emotion the video was designed to elicit), while expressive segments are expected to align with the evoked emotion (the participant's self-reported experience). This mapping allows us to distinguish stimulus-driven affect from speakerspecific affect, thereby addressing a critical gap in prior SER research that often assumes a single ground-truth label. This semantic distinction is particularly important in settings where it is essential to understand not only what happened -the contextual content of speech -but also how it was felt -the speaker's emotional state and tone. Such an understanding has practical implications for applications like emotion-aware AI systems, educational tools, and interactive entertainment, where both the content and emotional delivery of speech play key roles in creating engaging and effective human-computer interactions.\n\nTo validate our hypothesis, we collected a dataset comprising emotionally evocative movie clips to elicit a specific emotion. Participants watched the videos and provided ratings for the actual evoked emotions, alongside valence and arousal scores, creating a robust foundation for analysis. Our methodology to uncover the distinct relationships between semantic roles and intended versus evoked emotions involves three key steps: speech transcription with automatic speech recognition (ASR), semantic segmentation with LLMs, and emotion prediction with fine-tuned text classifiers/regressors. This work makes the following contributions:\n\n• First, we curated a SER dataset with 582 audio recordings spanning six emotion categories. Audio transcriptions are generated, and intended emotions, as well as evoked emotions, are measured in an experimental setup.\n\n• Second, we implemented an LLM-based semantic segmentation approach to separate the expressive and descriptive parts of speech and validated that through human evaluation.\n\n• Third, through experimentation, we show that descriptive semantics are more predictive of intended emotions, while expressive semantics are better aligned with evoked emotions.\n\nImportantly, our work goes beyond simply predicting emotion labels from participants' descriptions. By explicitly segmenting speech into descriptive and expressive roles, we quantify how different semantic functions relate to stimulus-intended versus self-experienced emotions. This role-based separation provides a principled way to reconcile discrepancies between intended and evoked affect and offers interpretable insights that are not available from standard text-only or audio-only models.\n\nOur findings have significant implications for designing more accurate and context-aware emotion recognition systems, with potential applications in virtual assistants, customer service, and mental health support. By bridging the gap between semantics and emotion, this research advances the state-of-the-art in SER and sets the stage for future exploration of semantic roles in emotional AI systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotions are complex psychological and physiological responses to salient events, involving bodily sensations, expressive behaviors, and cognitive evaluations  (Moors, 2024 (Moors, , 2009)) . Various linguistic features, including prosody, lexical choice, and sentence structure, play a role in the perception and expression of emotions  (Mohammad and Turney, 2010; Barrett et al., 2007; Keltner et al., 2019) . Speech emotion recognition (SER) models aim to detect emotional states from speech using acoustic, textual, or multimodal signals. With the advancement of LLMs and automatic speech recognitions (ASR), text-based emotion classification has seen improved accuracy  (Hama et al., 2024; Bekmanova et al., 2022; Bharti et al., 2022) . Acoustic-based emotion detectors have also progressed using acoustic feature extractors, such as openSMILE  (Eyben et al., 2010)  or audio embedding models, such as wav2vec  (Baevski et al., 2020)  and HuBERT  (Hsu et al., 2021) , which embed paralinguistic cues such as pitch, tempo, and energy into speech representations  (Ulgen et al., 2024; Chakhtouna et al., 2024; Zhao et al., 2024; Dutta and Ganapathy, 2024; Ghosh et al., 2016) . Multi-modal approaches, which combine speech, facial expressions, and physiological signals, have also become increasingly prominent in recent years  (Cheng et al., 2024; Khan et al., 2024; Morency and Baltrušaitis, 2017; Yoon et al., 2018; Niu et al., 2023) .\n\nEmotion elicitation via multimedia stimuli (e.g. short film clips) is a common technique in SER to induce targeted emotions (e.g., sadness, joy, fear)  (Li et al., 2021; Rumpa et al., 2015) . These movie-based emotion elicitation techniques have applications in various fields, including e-health monitoring and human-computer interaction  (Ley et al., 2019; Rumpa et al., 2015) . The stimuli are selected and validated through self-report and physiological measures  (Chen et al., 2021; Handayani et al., 2015; Soleymani et al., 2012) . While these methods control for the intended emotional target, they do not always account for the evoked emotion the speaker experiences and expresses. Prior work such as Siedlecka and Denson (2019) reviewed these paradigms in detail, but focused primarily on affect induction rather than the emotional content of participants' verbal responses. In our work, we analyze speech collected after stimulus exposure and study how intended and evoked emotions are reflected in participants' spoken descriptions. In doing so, we explore a novel distinction between semantic roles in language-namely, whether a speaker is being descriptive (e.g., summarizing the movie) or expressive (e.g., conveying their own reaction)-and how these roles align with different emotion types.\n\nMany SER datasets have been developed. In acted speech datasets, such as IEMOCAP  (Busso et al., 2008)  and  SAVEE (Jackson and Haq, 2014) , actors are recruited to read sentences or act in scenes that portray different emotions. In spontaneous speech datasets, such as MSP-Podcast (Lotfian and  Busso, 2017) ,  MSP-Conversation (Martinez-Lucas et al., 2020) ,  SAMAINE (McKeown et al., 2011) , and RECOLA  (Ringeval et al., 2013) , and elicited speech datasets, such as LSSED  (Fan et al., 2021) , BAUM-1  (Zhalehpour et al., 2016) , and eNTER-FACE  (Batliner et al., 2006) , audios are recorded in a freely speaking environment or with emotion elicitation methods. Speech is then annotated by a third party (perception-of-other). However, these datasets focus on one emotion label per speech and do not distinguish different types of emotions.\n\nTo this end, EMO-DB  (Burkhardt et al., 2005)  and IEMOCAP  (Busso et al., 2008)  analyzed emotional evocative sentences and perception-of-other in acted speech. Most similar to us, MuSE  (Jaiswal and Bara, 2020)  collects speech following emotional video stimuli and reports both self-reported and intended emotion annotations. While similar in structure, our work uniquely interprets the relationship between stimulus-intended and self-reported emotions through a semantic lens, enabling direct analysis of misalignment between the two emotion types.\n\nFurthermore, some recent studies in NLP have explored emotion elicitation and manipulation in conversational settings  (Gong et al., 2023; Ma et al., 2025; Qian et al., 2023; Meng et al., 2024) . While our study does not model conversational interactions, our semantic framework may offer insights into these settings by helping to identify when emotional influence is being attempted or received. For example, expressive speech segments may signal internal affective states, while descriptive segments may reflect contextual awareness or narrative framing. These distinctions could inform models of emotion transfer and regulation in human-computer dialogue.\n\nOur contribution lies in bridging the gap between stimulus-based emotion elicitation and the actual emotions conveyed by participants in speech. By segmenting utterances according to their semantic roles and analyzing how different roles align with either intended or evoked emotions, we propose a novel way to interpret emotional speech beyond traditional modality-based or label-based approaches.\n\nWhile prior SER studies have emphasized either acoustic or multimodal representations, our work suggests that semantic structure in language -accessible only through text -offers a distinct and interpretable signal for differentiating between types of emotion. Table  1 : Listing and information about the 12 movie clips used to elicit discrete emotions in the main study.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "The block diagram in Figure  1  summarizes our data collection, task definitions, and methodology, which we will elaborate on here and in the next section. Data collection was carried out in person at INSPIRE Laboratory of the School of Psychology at University of Ottawa. The experiment procedure was approved by the Research Ethics Board of University of Ottawa. The study included 97 student participants aged 18 to 27 (M = 19.9, SD = 2.5). The majority were women (81 women, 15 men, and 1 non-binary), and most participants were native English speakers (65 spoke English as their first language, 12 spoke French, and 20 spoke other languages). The sample was ethnically diverse, comprising 16 Asian, 20 Black/African, 7 Hispanic/Latino, 1 Indigenous, 15 Mixed/Multiple Ethnicities, 33 White/Caucasian, and 5 participants identifying as Other.\n\nOur study focused on the six basic emotions identified by  Ekman (1992)  as the target emotions in our experimental setup: sadness, fear, joy, dis-gust, surprise, and anger. Two movie clips for each emotion were sourced from film stimuli in the existing literature and validated in our pilot study. The twelve movie clips used in the study and their metainformation are listed in Table  1 . We trimmed clips to ensure optimal emotional salience and duration.\n\nTheir effectiveness was validated in a pilot study with 25 participants before the final data collection.\n\nIn the main study, participants watched six emotional video clips, one from each emotion category. To re-establish baseline levels of valence and arousal, the presentation of each emotional clip was preceded by a neutral video clip. To further mitigate potential carryover effects between conditions, a two-minute rest period was inserted between each neutral-emotional clip sequence, during which one of six still images was displayed on the computer screen. All video clips and still images were presented in random order to minimize potential sequence effects. The collected dataset consists of 97× 6 entries, with five elements: 1) Speech: a 30-second audio recording of the participant's verbal response to the following instruction:\n\n\"You are asked to verbally describe the scene during which you felt the strongest emotion in the last film clip and say how it made you feel.\" 2) Intended emotions: Each video is expected to provoke a certain emotion. 3) Evoked emotions: the intensities at which each of the emotions (sadness, fear, joy, disgust, surprise, anger) was felt, as rated by the participants on a 7-point Likert scale going from not at all to strongly. 4) Valence: the extent to which the overall feeling of the participant was positive or negative. 5) Arousal: the intensity of the overall feeling of the participant while watching the video. Valence and arousal were measured on a validated sliding scale where each extreme was illustrated by an emoticon.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Tasks",
      "text": "We define three tasks to examine the relationship between semantic types and emotion recognition.\n\nTo determine the most predictive semantic type for each task, we experimented with three different inputs: full transcriptions, descriptive semantic segments, and expressive semantic segments.\n\nTASK-1: Classification of Intended Emotion involves classifying the intended emotion associated with each video based on participants' speech.\n\nTASK-2: Classification of Evoked Emotion involves classifying participant-reported evoked emotions, which are subjective and may include multiple emotions simultaneously. While evoked emotions often include the intended emotion, individual differences can lead to variations. This task is framed as a multi-label classification problem, where each emotion (on a scale of 0 to 6) is binarized based on whether it is evoked or not.\n\nTASK-3: Regression of Valence and Arousal predicts participants' self-reported valence and arousal ratings, which provide a two-dimensional representation of emotional states.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methodology",
      "text": "As depicted in Figure  1 , our methodology consists of three sequential steps: speech recognition, semantic segmentation, and emotion prediction.\n\nStep-1: Automatic Speech Recognition -We used Whisper  (Radford et al., 2023) , an automatic speech recognition model, to transcribe the participants' speech data into text. Step-2: Semantic Segmentation -We used  GPT-4o (OpenAI, 2023)  to extract descriptive and expressive segments from the transcription obtained in step 1. The prompt is given in Table  3 . We set the sampling temperature to 0 to make the process more deterministic. Overlapping phrases were allowed when semantic roles intersected, ensuring comprehensive representation.\n\nStep-3: Emotion Prediction -The last step is to perform tasks described in Section 4 to study the relationship between semantic roles and emotion recognition. Each model is trained and evaluated on three input types: full transcriptions, descriptive segments, and expressive segments.\n\nAudio-Based Emotion Classification -In addition to text-based models, we also experimented with audio-based models trained directly on the speech recordings. These included a HuBERT model  (Hsu et al., 2021) , a Wav2Vec2 model  (Baevski et al., 2020) , and a baseline MFCC (mel-frequency cepstral features) classifier. The audio classifiers were evaluated on TASK-1 and TASK-2 using the full utterance audio as input. However, all speech-based models performed significantly worse than textbased classifiers. Since semantic role segmentation (i.e., distinguishing between descriptive and expressive segments) is inherently a linguistic task and not inferable from acoustic signals alone, we prioritized text-based methods for the core analyses of this paper moving forward.\n\n6 Experiments",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "For Step-1, automatic speech recognition, we used 'whisper-large-V3' 1  , a state-of-the-art system ASR model known for its robustness across diverse accents and noise conditions. We manually reviewed transcriptions in the development set, consisting of 33 × 6 audio transcriptions. Whisper achieved a disgust, fear, sadness, and surprise, indicated by the blue bars. Among these, \"disgust\" was the strongest emotion, receiving the highest score of 6.\n\nThe user will provide a paragraph describing their feelings towards a particular movie, delimited with \"'####\"'.\n\nYour task is to segment the paragraph into two parts according to the type of content: descriptive segments and expressive segments.\n\nDescriptive segments refer to elements or clauses that provide factual or narrative information about the movie content without explicitly reflecting personal emotions or opinions.\n\nExpressive segments refer to elements or clauses that convey personal feelings, attitudes, or opinions. These segments reflect individual reactions, emotions, and perceptions, or the intensity of these emotions.\n\nThe two parts (descriptive segments and expressive segments) can overlap, but all clauses of the given paragraph must be contained in at least one of the two parts.\n\nOutput your answer in the following format: <answer> <descriptive> [descriptive segments] </descriptive> <expressive> [expressive segments] </expressive> </answer> 4.13% word error rate, with errors mainly in unclear utterances at speech boundaries and between clauses.\n\nFor Step-2, we validated the effectiveness of GPT-4o segmentation again on the development set with 33 × 6 transcriptions. Two authors of this paper, one from the Computer Science department and the other from the Psychology department, were given the same instructions as the LLM and inde-pendently performed the same segmentation task. To calculate the agreement between human annotators and also between LLM and annotators, we computed cosine similarities of the segments, using sentence-transformer embeddings 2  . From Table 2, the average agreement between two human annotators (0.76) was comparable to human-LLM agreement (0.73 and 0.83). Most discrepancies arose from minor conjunctions to make sentences more complete. As a baseline, two random text segmentations would result in a similarity score of 0.63 -0.64. Overall, GPT-4o has an acceptable segmentation quality.\n\nFor Step-3, emotion prediction, we fine-tuned different classifiers/regressors, including BERT  (Devlin, 2018) , RoBERTa  (Liu, 2019) , and De-BERTa  (He et al., 2020) . Different text semantics identified in Step-2 are used as inputs to the models. For emotion classification (Tasks 1 and 2), we used the text embeddings from the models and applied a standard classification head with a softmax activation function to predict categorical emotions. For regression (Task 3), we modified the models by replacing the classification head with a fully connected layer that outputs a single continuous value, trained with mean squared error (MSE) loss to predict valence and arousal scores. This approach follows standard practice in adapting transformer encoders for regression tasks  (Xin et al., 2021; Taha, 2024; Orso and Xie, 2008) . Data are split on participants' level, with 1/3 of partici-How often do participants experience the intended emotion conveyed by the videos? 96.63% How frequently do participants feel emotions other than the intended one? 89.39% How often is the intended emotion rated as the highest by participants? 79.29% Chippendale's alpha coefficientbetween intended emotion and evoked emotions 0.1466\n\nTable  4 : Statistics of relationships between movie intended emotion tags and evoked emotions. Predicting the evoked emotions is a much more subjective task than predicting the intended emotion tag.\n\npants (33 participants) data used for training, 1/3 for validation, and the rest 1/3 for testing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparative Analysis Of Intended And Evoked Emotions",
      "text": "Table  4  shows the relationship between the intended and evoked emotions. While the participants experienced the intended emotion 96.63% of the time, they also reported other emotions 89.39% of the time. Surprisingly, more than 20% of the time, an emotion other than the intended one is experienced most. These results suggest that the experienced emotion is highly subjective and can deviate from the intended emotions.\n\nTo better quantify the subjectivity of Task-2, we calculate the Krippendorff's alpha coefficient  (Krippendorff, 2011 (Krippendorff, , 2018) )  between the intended emotion and evoked emotions. We treat the agreement between intended emotion and evoked emotion as the agreement between two annotators performing multi-label annotations. Each annotator labels 594 data points, since there are 99 × 6 speech. One annotator always label the intended emotion as true and other emotions as false. The other annotator labels the data with the participant's evoked emotion ratings in a multi-label fashion. Krippendorff's alpha coefficient is calculated as the inter-annotator agreement index on this multi-label annotation task with MASI distance  (Passonneau, 2006)  as the distance measurement between two sets of multi-label annotations. The low score of Krippendorff's alpha coefficient shows the high subjectivity of task T2 and the high variation of evoked emotions with respect to the intended emotion.\n\nFigure  2  gives examples of emotion ratings by three different participants in response to the six movie segments. Each row in the grid represents data from a different participant, while each column corresponds to one of the six movie segments. Within the bar charts, yellow bars indicate the intended emotion that the video clip aimed to elicit, while blue bars represent the emotions self-reported by the participants after watching the clips. The height of the bars reflects the intensity of the rated emotions on a numerical scale. These examples highlight variability in participants' emotional responses, often revealing discrepancies between the intended emotions and the emotions participants actually experienced.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Classification Of Intended Emotion",
      "text": "Aligned with our hypothesis, the classification results for TASK-1 demonstrate a clear advantage of using descriptive semantics as input for predicting the intended emotions associated with each movie segment. Table  5  shows the classification accuracy for both semantic types across three different classifiers. Across all models, descriptive semantics consistently yield significantly higher accuracy in predicting the intended emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classification Of Evoked Emotion",
      "text": "In TASK-2, we classified participant-reported evoked emotions, which are inherently subjective and may include multiple emotions simultaneously. Aligning with our hypothesis that expressive semantics better capture speaker-specific emotional experiences, results in sification accuracy compared to using descriptive semantics. We also observe that even with full semantics, TASK-2 achieves significantly lower F-scores than Task-1, as expected due to the subjectivity of this task.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion Of Audio-Based Classifications",
      "text": "To assess the role of acoustic features in emotion recognition, we trained several audio-only classifiers, including models based on HuBERT 3 , Wav2Vec2 4 , and MFCC features, for both TASK-1 and TASK-2. Across all models, we observed consistently poor performance, with classifiers frequently defaulting to one or two majority emotion classes. This suggests that prosodic and paralinguistic cues in our dataset were not strongly indicative of emotional content. One likely explanation is that participants generally delivered their responses in a steady and emotionally neutral tone, which limited the expressiveness of acoustic features.\n\nMoreover, unlike text-based inputs, speech signals do not easily lend themselves to semantic segmentation without speech recognition  (Wang et al., 2003;   3 facebook/hubert-base-ls960 4 facebook/wav2vec2-base  Ong and Herrera, 2005) . Audio-based classifiers cannot distinguish between descriptive and expressive segments in an obvious way, making it difficult to explore the semantic roles that are central to our research questions. While acoustic features are valuable in many speech emotion recognition tasks, in our study design where subjective emotional experience is linked to semantic framing, textual cues proved more informative and interpretable.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Regression Of Emotion Valence And Arousal:",
      "text": "Figure  3  shows the distributions of valence and arousal across different intended emotions, which exhibit high variability without clear patterns across emotions. Positive emotions, such as joy, correlates with higher valence, and negative emotions, such as fear, have lower valence and higher arousal. But there is no obvious clusters among the six emotions.\n\nThe results reported in  errors between descriptive and expressive semantics are statistically significant for valence under two of the three models and one model for arousal. The regression results are in line with the TASK-2 results and the statistical analysis partially supports the hypothesis that expressive semantics better capture subjective experience.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This study introduces a novel framework for Speech Emotion Recognition (SER) by distinguishing between semantic roles in speech. By leveraging LLMs' zero-shot capabilities in text segmentation, we tackle a previously difficult challenge. To our knowledge, this is the first work to segment speech into two semantic roles, expressive and descriptive content, to enable more fine-grained and nuanced emotion detection.\n\nOur findings reveal that descriptive semantics are more predictive of intended emotions, while expressive semantics are more closely aligned with evoked emotions and their valence and arousal dimensions. This differentiation can inform future research in emotion detection. In some contexts, it might be more useful to instruct users and guide them toward only one of these modes of expressing emotions. In other applications, it might be more suitable to leave it to the users to express their emo-tions in a mixture of expressive and descriptive modes. The LLMs can then be used to segment the speech and use the segments depending on the predictive goals. This approach enhances the development of more accurate and context-aware emotion recognition systems, with applications in mental health, virtual assistants, and customer service.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limitations",
      "text": "This study, while providing valuable insights into the segmentation of speech for emotion recognition, has limitations. First, the dataset used in this research is curated from emotionally evocative movie clips, which, although varied, may not fully represent the broad diversity of real-world speech interactions. The emotional expressions captured in these clips might not encompass the full spectrum of spontaneous and everyday speech, which could limit the generalizability of the findings.\n\nSecond, although we included baseline speechbased emotion classifiers, their performance was substantially lower than that of text-based models. This gap likely stems from the emotional neutrality of the participants' tone and the nature of the task. However, future work could explore whether jointly modeling text and acoustic features, perhaps guided by semantic segmentation, might uncover latent prosodic patterns aligned with specific se-mantic roles.\n\nThird, while the study distinguishes between descriptive and expressive semantics, it focuses primarily on self-reported emotional responses, which can be subjective and influenced by individual differences in emotional expression and perception. This subjectivity introduces variability in the emotional ratings, potentially affecting the accuracy and robustness of the regression models.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethics Statement",
      "text": "This research was approved by the Research Ethics Board of University of Ottawa. All participants provided their informed consent prior to participating in the study. Participants had the option to withdraw from the experiment at any time and for any reason, including emotional distress. Data collected during the study were handled securely and used exclusively for research purposes. All personal data was anonymized.\n\nIn Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis,  Mohammad (2022)  provides a structured ethical framework for developing and deploying Automatic Emotion Recognition (AER) systems, along 50 ethical considerations. He specifically emphasizes on the risks of privacy violations, reinforcing biases, and potential misuse in surveillance or manipulation. This Ethics Sheet serves as a guide for responsible AER development, and encourages researchers to question why they automate, whose interests are served, and how success is measured.\n\nRecognizing the ethical risks and potential misuse of SER technologies, we strongly caution against issues such as biases in emotion datasets, AI models enforcing rigid norms on emotional expression, and the exclusion of neurodiverse and marginalized groups. These concerns must be carefully addressed before deploying SER systems in realworld applications. We urge industries to adopt responsible, explainable, and inclusive AI development practices, ensuring that these technologies are fair, transparent, and beneficial to all users.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Data Collection and Algorithm Workflow: Participants watched six videos eliciting specific emotions",
      "page": 2
    },
    {
      "caption": "Figure 1: summarizes our",
      "page": 4
    },
    {
      "caption": "Figure 1: , our methodology consists",
      "page": 5
    },
    {
      "caption": "Figure 2: Examples of participants’ rated emotions. Each row represents a participant who watched six movie",
      "page": 6
    },
    {
      "caption": "Figure 2: gives examples of emotion ratings by",
      "page": 7
    },
    {
      "caption": "Figure 3: Valence and arousal ratings, colored by the",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the distributions of valence and",
      "page": 8
    },
    {
      "caption": "Figure 2011: One Day [Film]. Focus Features.",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Listing and information about the 12 movie clips used to elicit discrete emotions in the main study.",
      "page": 4
    },
    {
      "caption": "Table 1: We trimmed clips",
      "page": 4
    },
    {
      "caption": "Table 2: Human evaluations of GPT-4o text segmenta-",
      "page": 5
    },
    {
      "caption": "Table 3: Prompt for extracting descriptive and expres-",
      "page": 6
    },
    {
      "caption": "Table 4: Statistics of relationships between movie intended emotion tags and evoked emotions. Predicting the",
      "page": 7
    },
    {
      "caption": "Table 4: shows the relationship between the in-",
      "page": 7
    },
    {
      "caption": "Table 5: shows the classification accuracy",
      "page": 7
    },
    {
      "caption": "Table 6: indicate that using",
      "page": 7
    },
    {
      "caption": "Table 5: Model performances on classifying intended emotion associated with the movies.",
      "page": 8
    },
    {
      "caption": "Table 6: Average model performances on classifying evoked emotions (std is always less than 0.1 over 5 run).",
      "page": 8
    },
    {
      "caption": "Table 7: show that expres-",
      "page": 8
    },
    {
      "caption": "Table 8: shows that the differences in the prediction",
      "page": 8
    },
    {
      "caption": "Table 7: Model performances on regression of emotion valence and arousal. Expressive semantics leads to smaller",
      "page": 9
    },
    {
      "caption": "Table 8: Wilcoxon signed-rank tests results to compare MSE between descriptive and expressive semantics for each",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "The experience of emotion",
      "authors": [
        "Lisa Feldman",
        "Batja Mesquita",
        "Kevin Ochsner",
        "James Gross"
      ],
      "year": "2007",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "3",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus",
      "authors": [
        "Batliner",
        "Steidl",
        "Nöth"
      ],
      "year": "2006",
      "venue": "Programme of the Workshop on Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "4",
      "title": "Altynbek Sharipbay, and Assel Mukanova. 2022. Emotional speech recognition method based on word transcription",
      "authors": [
        "Gulmira Bekmanova",
        "Banu Yergesh"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "Santosh Kumar Bharti",
        "S Varadhaganapathy",
        "Rajeev Gupta",
        "Prashant Kumar Shukla",
        "Mohamed Bouye",
        "Simon Karanja Hingaa",
        "Amena Mahmoud"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "6",
      "title": "Class-level spectral features for emotion recognition",
      "authors": [
        "Dmitri Bitouk",
        "Ragini Verma",
        "Ani Nenkova"
      ],
      "year": "2010",
      "venue": "Speech communication"
    },
    {
      "citation_id": "7",
      "title": "Trainspotting film",
      "authors": [
        "Danny Boyle"
      ],
      "year": "1996",
      "venue": "Trainspotting film"
    },
    {
      "citation_id": "8",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Unveiling embedded features in wav2vec2 and hubert msodels for speech emotion recognition",
      "authors": [
        "Adil Chakhtouna",
        "Sara Sekkate"
      ],
      "year": "2024",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "11",
      "title": "Selection and validation of emotional videos: Dataset of professional and amateur videos that elicit basic emotions",
      "authors": [
        "Hongyi Chen",
        "Kai Ling Chin",
        "B Chrystalle",
        "Tan"
      ],
      "year": "2021",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "12",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "14",
      "title": "Leveraging content and acoustic representations for efficient speech emotion recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2024",
      "venue": "Leveraging content and acoustic representations for efficient speech emotion recognition",
      "arxiv": "arXiv:2409.05566"
    },
    {
      "citation_id": "15",
      "title": "Facial expressions of emotion: New findings",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Facial expressions of emotion: New findings"
    },
    {
      "citation_id": "16",
      "title": "Opensmile: the munich versatile and fast opensource audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Lssed: a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "Weiquan Fan",
        "Xiangmin Xu",
        "Xiaofen Xing",
        "Weidong Chen",
        "Dongyan Huang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Ratings for emotion film clips",
      "authors": [
        "Crystal Gabert-Quillen",
        "Ellen Bartolini",
        "Benjamin Abravanel",
        "Charles Sanislow"
      ],
      "year": "2015",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "19",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "Sayan Ghosh",
        "Eugene Laksana",
        "Louis-Philippe Morency",
        "Stefan Scherer"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Eliciting rich positive emotions in dialogue generation",
      "authors": [
        "Ziwei Gong",
        "Min Qingkai",
        "Yue Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the First Workshop on Social Influence in Conversations"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition in conversation with multi-step prompting using large language model",
      "authors": [
        "Kenta Hama",
        "Atsushi Otsuka",
        "Ryo Ishii"
      ],
      "year": "2024",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "22",
      "title": "Recognition of Emotions in Video Clips: The Self-Assessment Manikin Validation",
      "authors": [
        "Dini Handayani",
        "Abdul Wahab",
        "Hamwira Yaacob"
      ],
      "year": "2015",
      "venue": "TELKOMNIKA (Telecommunication Computing Electronics and Control)"
    },
    {
      "citation_id": "23",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "Pengcheng He",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2020",
      "venue": "Deberta: Decoding-enhanced bert with disentangled attention",
      "arxiv": "arXiv:2006.03654"
    },
    {
      "citation_id": "24",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "25",
      "title": "An extended emotion-eliciting film clips set (egefilm): assessment of emotion ratings for 104 film clips in a turkish sample",
      "authors": [
        "Merve Elvan Arıkan İyilikci",
        "Elif Boga",
        "Yıldız Yüvrük",
        "Osman Özkılıç",
        "Sonia İyilikci",
        "Amado"
      ],
      "year": "2014",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "26",
      "title": "Muse: a multimodal dataset of stressed emotion",
      "authors": [
        "Mimansa Jaiswal",
        "Cristian-Paul Bara"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "27",
      "title": "American history x, new line cinema",
      "authors": [
        "Tony Kaye"
      ],
      "year": "1998",
      "venue": "American history x, new line cinema"
    },
    {
      "citation_id": "28",
      "title": "Emotional expression: Advances in basic emotion theory",
      "authors": [
        "Dacher Keltner",
        "Disa Sauter",
        "Jessica Tracy",
        "Alan Cowen"
      ],
      "year": "2019",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "29",
      "title": "Mser: Multimodal speech emotion recognition using cross-attention with deep fusion",
      "authors": [
        "Mustaqeem Khan",
        "Wail Gueaieb"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "30",
      "title": "Computing krippendorff's alpha-reliability",
      "authors": [
        "Klaus Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing krippendorff's alpha-reliability"
    },
    {
      "citation_id": "31",
      "title": "Evaluating methods for emotion recognition based on facial and vocal features",
      "authors": [
        "Klaus Krippendorff",
        "; Kwokleung Chan",
        "Jiucang Hao",
        "Te-Won Lee",
        "; Citeseer",
        "Maria Ley",
        "Sten Egger",
        "Hanke"
      ],
      "year": "2003",
      "venue": "Content analysis: An introduction to its methodology. Sage publications. Oh-Wook Kwon"
    },
    {
      "citation_id": "32",
      "title": "Speech based human emotion recognition using mfcc",
      "authors": [
        "Keding Li",
        "Xunbing Shen",
        "Zhencai Chen",
        "Liping He",
        "Zhennan Liu",
        "; Ms Likitha",
        "Sri Raksha R Gupta",
        "K Hasitha",
        "Raju Upendra"
      ],
      "year": "2017",
      "venue": "2017 international conference on wireless communications, signal processing and networking (WiSPNET)"
    },
    {
      "citation_id": "33",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Yinhan Liu"
      ],
      "year": "2017",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "34",
      "title": "Detecting conversational mental manipulation with intent-aware prompting",
      "authors": [
        "Jiayuan Ma",
        "Hongbin Na",
        "Zimu Wang",
        "Yining Hua",
        "Yue Liu",
        "Wei Wang",
        "Ling Chen"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "The msp-conversation corpus",
      "authors": [
        "Luz Martinez-Lucas",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "The msp-conversation corpus"
    },
    {
      "citation_id": "36",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "37",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "38",
      "title": "Latemo-e: a film database to elicit discrete emotions and evaluate emotional dimensions in latin-americans",
      "authors": [
        "Yanina Michelini",
        "Ignacio Acuña",
        "Juan Ignacio Guzmán",
        "Juan Godoy"
      ],
      "year": "2019",
      "venue": "Trends in Psychology"
    },
    {
      "citation_id": "39",
      "title": "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon",
      "authors": [
        "Saif Mohammad",
        "Peter Turney"
      ],
      "year": "2010",
      "venue": "Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text"
    },
    {
      "citation_id": "40",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2022",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Theories of emotion causation: A review",
      "authors": [
        "Agnes Moors"
      ],
      "year": "2009",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "42",
      "title": "An overview of contemporary theories of emotions in psychology",
      "authors": [
        "Agnes Moors"
      ],
      "year": "2024",
      "venue": "Emotion Theory: The Routledge Comprehensive Guide: Volume I: History, Contemporary Theories, and Key Elements"
    },
    {
      "citation_id": "43",
      "title": "Multimodal machine learning: integrating language, vision and speech",
      "authors": [
        "Louis-Philippe Morency",
        "Tadas Baltrušaitis"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics: Tutorial abstracts"
    },
    {
      "citation_id": "44",
      "title": "The Blair witch project. Artisan Entertainment Santa",
      "authors": [
        "Daniel Myrick",
        "Eduardo Sánchez",
        "Heather Donahue",
        "Michael Williams",
        "Joshua Leonard"
      ],
      "year": "1999",
      "venue": "The Blair witch project. Artisan Entertainment Santa"
    },
    {
      "citation_id": "45",
      "title": "",
      "authors": [
        "J Andrew",
        "Nicholas Cohen",
        "Brendan O' Stoller",
        "Brien"
      ],
      "year": "2014",
      "venue": ""
    },
    {
      "citation_id": "46",
      "title": "Capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder",
      "authors": [
        "Minxue Niu",
        "Amrit Romana",
        "Mimansa Jaiswal",
        "Melvin Mcinnis",
        "Emily Mower_Provost"
      ],
      "year": "2023",
      "venue": "Capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder"
    },
    {
      "citation_id": "47",
      "title": "Semantic segmentation of music audio",
      "authors": [
        "Bee Suan",
        "Perfecto Herrera"
      ],
      "year": "2005",
      "venue": "Proceedings of the International Computer Music Conference"
    },
    {
      "citation_id": "48",
      "title": "Bert: Behavioral regression testing",
      "authors": [
        "Alessandro Orso",
        "Tao Xie"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 international workshop on dynamic analysis: held in conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2008)"
    },
    {
      "citation_id": "49",
      "title": "Measuring agreement on set-valued items (masi) for semantic and pragmatic annotation",
      "authors": [
        "Rebecca Passonneau"
      ],
      "year": "2006",
      "venue": "5th International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "50",
      "title": "Think twice: A human-like two-stage conversational agent for emotional response generation",
      "authors": [
        "Yushan Qian",
        "Bo Wang",
        "Shangzhao Ma",
        "Wu Bin",
        "Shuo Zhang",
        "Dongming Zhao",
        "Kun Huang",
        "Yuexian Hou"
      ],
      "year": "2023",
      "venue": "Think twice: A human-like two-stage conversational agent for emotional response generation",
      "arxiv": "arXiv:2301.04907"
    },
    {
      "citation_id": "51",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "52",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "53",
      "title": "Planet Terror [Film]. Dimension Films",
      "authors": [
        "Robert Rodriguez"
      ],
      "year": "2007",
      "venue": "Planet Terror [Film]. Dimension Films"
    },
    {
      "citation_id": "54",
      "title": "Validating video stimulus for eliciting human emotion: A preliminary study for e-health monitoring system",
      "authors": [
        "Adhi Lantana Dioren Rumpa",
        "Mauridhi Dharma Wibawa",
        "Harmelia Heri Purnomo",
        "Tulak"
      ],
      "year": "2015",
      "venue": "2015 4th International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering (ICICI-BME)"
    },
    {
      "citation_id": "55",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "Alexandre Schaefer",
        "Frédéric Nils",
        "Xavier Sanchez",
        "Pierre Philippot"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "56",
      "title": "Experimental methods for inducing basic emotions: A qualitative review",
      "year": "2011",
      "venue": "Lone Scherfig"
    },
    {
      "citation_id": "57",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Platoon [Film]",
      "authors": [
        "Oliver Stone"
      ],
      "year": "1986",
      "venue": "Platoon [Film]"
    },
    {
      "citation_id": "59",
      "title": "Text regression analysis: A review, empirical, and experimental insights",
      "authors": [
        "Kamal Taha"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "60",
      "title": "Speech emotion recognition using semantic information",
      "authors": [
        "Panagiotis Tzirakis",
        "Anh Nguyen",
        "Stefanos Zafeiriou",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "61",
      "title": "Revealing emotional clusters in speaker embeddings: A contrastive learning strategy for speech emotion recognition",
      "authors": [
        "Zongyang Ismail Rasim Ulgen",
        "Carlos Du",
        "Berrak Busso",
        "Sisman"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "62",
      "title": "Emotion recognition from speech",
      "authors": [
        "Kannan Venkataramanan",
        "Haresh Rengaraj Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech",
      "arxiv": "arXiv:1912.10458"
    },
    {
      "citation_id": "63",
      "title": "The Conjuring",
      "authors": [
        "James Wan"
      ],
      "year": "2013",
      "venue": "The Conjuring"
    },
    {
      "citation_id": "64",
      "title": "Speech segmentation without speech recognition",
      "authors": [
        "Dong Wang",
        "Lie Lu",
        "Hong-Jiang Zhang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "Siqing Wu",
        "Tiago Falk",
        "Wai-Yip Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "66",
      "title": "BERxiT: Early exiting for BERT with better finetuning and extension to regression",
      "authors": [
        "Ji Xin",
        "Raphael Tang",
        "Yaoliang Yu",
        "Jimmy Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
      "doi": "10.18653/v1/2021.eacl-main.8"
    },
    {
      "citation_id": "67",
      "title": "Exploring zero-shot emotion recognition in speech using semantic-embedding prototypes",
      "authors": [
        "Xinzhou Xu",
        "Jun Deng",
        "Nicholas Cummins",
        "Zixing Zhang",
        "Li Zhao",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "68",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE spoken language technology workshop (SLT)"
    },
    {
      "citation_id": "69",
      "title": "Baby laughing hysterically at ripping paper",
      "authors": [
        "Youtube"
      ],
      "year": "2011",
      "venue": "Baby laughing hysterically at ripping paper"
    },
    {
      "citation_id": "70",
      "title": "Mom cat shows baby kittens that golden retriever is safe for them",
      "venue": "Mom cat shows baby kittens that golden retriever is safe for them"
    },
    {
      "citation_id": "71",
      "title": "2022b. The saddest video ever captured",
      "authors": [
        "Youtube"
      ],
      "venue": "2022b. The saddest video ever captured"
    },
    {
      "citation_id": "72",
      "title": "Baum-1: A spontaneous audio-visual face database of affective and mental states",
      "authors": [
        "Sara Zhalehpour",
        "Onur Onder",
        "Zahid Akhtar",
        "Cigdem Erdem"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "Knowledge enhancement for speech emotion recognition via multi-level acoustic feature",
      "authors": [
        "Huan Zhao",
        "Nianxin Huang",
        "Haijiao Chen"
      ],
      "year": "2024",
      "venue": "Connection Science"
    },
    {
      "citation_id": "74",
      "title": "My Girl",
      "authors": [
        "Howard Zieff",
        "Laurice Elehwany"
      ],
      "year": "1991",
      "venue": "My Girl"
    },
    {
      "citation_id": "75",
      "title": "",
      "authors": [
        "Columbia Pictures"
      ],
      "venue": ""
    },
    {
      "citation_id": "76",
      "title": "Eliciting emotion ratings for a set of film clips: A preliminary archive for research in emotion",
      "authors": [
        "Barbra Zupan",
        "Michelle Eskritt"
      ],
      "year": "2020",
      "venue": "The Journal of Social Psychology"
    }
  ]
}