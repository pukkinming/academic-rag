{
  "paper_id": "2109.04081v1",
  "title": "Deepemo: Deep Learning For Speech Emotion Recognition",
  "published": "2021-09-09T07:51:57Z",
  "authors": [
    "Enkhtogtokh Togootogtokh",
    "Christian Klasen"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Deep learning for Speech Emotion Recognition",
    "DeepEMO",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We proposed the industry level deep learning approach for speech emotion recognition task. In industry, carefully proposed deep transfer learning technology shows real results due to mostly low amount of training data availability, machine training cost, and specialized learning on dedicated AI tasks. The proposed speech recognition framework, called DeepEMO, consists of two main pipelines such that preprocessing to extract efficient main features and deep transfer learning model to train and recognize. Main source code is in https://github.com/enkhtogtokh/deepemo repository.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Ability to understand and manage emotions, called emotional intelligence, has been shown to play an important role in decision-making. Some researchers suggest that emotional intelligence can be learned and strengthened it refers to the ability to perceive, control, and evaluate emotions. For machine intelligence, it is also important role to understand and even generate such emotional intelligence. Here we proposed the simple yet effective speech emotional recognition modern deep learning technique called DeepEMO framework. It has been conceived considering the main AI pipeline (from sensors to results) together with modern technology trends. The DeepEMO has two main pipelines which are to extract strong speech features and deep transfer learning for the emotion recognition task. We applied them on english emotional speech case. Generally it is possible to apply them on any natural language. There are inevitable demands to recognize the speech emotion with advanced technology.\n\nConcretely, the key contributions of the proposed work are:\n\n• The industry level AI technology for speech emotion recognition • The speech recognition general modern deep learning framework for similar tasks Systematic experiments conducted on real-world acquired data have shown as:\n\n• It is possible to be common framework for many type of speech recognition task. • It is possible to achieve 99.9% accuracy on well prepared training data to recognize. In this section, we discuss the proposed DeepEMO model for speech emotion recognition AI applications as shown in Figure  1 . The DeepEMO has two main pipelines which are the preprocessing to extract efficient features and deep transfer learning mechanism. We discuss them in detail with coming sections.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. The Preprocessing Feature Extraction",
      "text": "Audio significant feature extraction is the important part of modern deep learning. There are many mechanisms to do it. Here we extract melspectrogram audio feature later to train machine with high accuracy.\n\nSpecifically, it consists of following general steps:\n\n• To compute fast Fourier transform (FFT)\n\n• To generate mel scale • To generate spectrogram The FFT is an algorithm which efficiently computes the Fourier transform. The Fourier transform is a mathematical formula which decomposes a signal into it's individual frequencies and the frequency's amplitude. In other words, it arXiv:2109.04081v1 [cs.SD] 9 Sep 2021 converts the signal from the time domain into the frequency domain. The result is called a spectrum. The mathematical operation converts frequencies to the mel scale. Researchers proposed a unit of pitch such that equal distances in pitch sounded equally distant to the listener, which is called the mel scale. When signal's frequency content varies over time as non periodic signals, it needs a right representation. As we can compute several spectrums by performing FFT on several windowed segments. It is called the short-time Fourier transform. The FFT is computed on overlapping windowed segments of the signal, which is called the spectrogram.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. The Deep Transfer Learning",
      "text": "Specifically, we define the transfer learning model by  [1] :\n\n• To prepare the pre-trained model • To re-define the last output layer as n (in case, n=8) neurons layer for the new task • To train the network This is called transfer learning, i.e. we have a model trained on another task, and we need to tune it for the new dataset we have in hand.\n\nTo recognize speech emotion, we propose the deep convolutional transfer neural network. Since after melspectrogram feature extraction preprocessing, it is now generally computer vision problem. The deep convolutional backbone model is ResNet18  [2]  which consists of assemble of convolutional layers and batch norms as shown in Algorithm 1. For simplicity, the Pytorch  [3]  style pseudo code is provided. Cross Entropy Loss function (CE) and Adam optimize implemented for the model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experimental Results",
      "text": "In this section, we discuss first about the setup, and then evaluate the deep transfer learning recognition and melspectrogram generation results are experimented in systematic scenarios.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Setup",
      "text": "We train and test on ubuntu 18 machine with capacity of (CPU: Intel(R) Xeon(R) CPU @ 2.20GHz, RAM:16GB, GPU: NVidia GeForce GTX 1070, 16 GB).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. The Dataset",
      "text": "We use the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [4]  dataset. It has 8 speech label emotions as neutral, calm, happy, sad, angry, fearful, disgust, and surprised speeches.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. The Recognition Results",
      "text": "Table  I",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The DeepEMO framework",
      "page": 1
    },
    {
      "caption": "Figure 1: The DeepEMO has two main pipelines which are",
      "page": 1
    },
    {
      "caption": "Figure 2: , 3, 4, and 5 show the recognition results of testing",
      "page": 2
    },
    {
      "caption": "Figure 2: The recognition result for happy emotional speech",
      "page": 3
    },
    {
      "caption": "Figure 3: The recognition result for calm emotional speech",
      "page": 3
    },
    {
      "caption": "Figure 4: The recognition result for sad emotional speech",
      "page": 3
    },
    {
      "caption": "Figure 5: The recognition result for surprised emotional speech",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Numberofepoch Trainingaccuracy Loss Validationaccuracy": "10 0.88 0.470 0.970"
        },
        {
          "Numberofepoch Trainingaccuracy Loss Validationaccuracy": "20 0.991 0.011 1.000"
        },
        {
          "Numberofepoch Trainingaccuracy Loss Validationaccuracy": "42 1.000 0.009 1.000"
        },
        {
          "Numberofepoch Trainingaccuracy Loss Validationaccuracy": "50 1.000 0.006 1.000"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning approach for very similar objects recognition application on chihuahua and muffin problem",
      "authors": [
        "Enkhtogtokh Togootogtokh",
        "Amarzaya Amartuvshin"
      ],
      "year": "2018",
      "venue": "Deep learning approach for very similar objects recognition application on chihuahua and muffin problem",
      "arxiv": "arXiv:1801.09573"
    },
    {
      "citation_id": "2",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    }
  ]
}