{
  "paper_id": "2210.07642v1",
  "title": "Training Speech Emotion Classifier Without Categorical Annotations",
  "published": "2022-10-14T08:47:41Z",
  "authors": [
    "Meysam Shamsi",
    "Marie Tahon"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Emotion representation",
    "Classification",
    "Regression"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "There are two paradigms of emotion representation, categorical labeling and dimensional description in continuous space. Therefore, the emotion recognition task can be treated as a classification or regression. The main aim of this study is to investigate the relation between these two representations and propose a classification pipeline that uses only dimensional annotation. The proposed approach contains a regressor model which is trained to predict a vector of continuous values in dimensional representation for given speech audio. The output of this model can be interpreted as an emotional category using a mapping algorithm. We investigated the performances of a combination of three feature extractors, three neural network architectures, and three mapping algorithms on two different corpora. Our study shows the advantages and limitations of the classification via regression approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The importance of extracting the paralinguistic information from speech has led the research community into Speech Emotion Recognition (SER). But the definition of emotions is ambiguous  [1] . Consequently, there is no consensus on emotion representation and annotation. The two main emotional theories used in computer science are the followings: emotions can be described with categorical labels mostly based on Ekman representations  [2]  or emotional dimensions such as arousal (or activation), valence, dominance (AVD)  [3]  to precise the emotional state.\n\nThese two representations have merits and disadvantages. Usually, using categorical labels for describing emotional states would be more understandable for the public  [4] . But it makes the representation of emotional states limited to certain categories, which may not cover all human emotions. On the other hand, using continuous value can precisely assign the emotional state to a point in dimensional space, which is less close to human language.\n\nFrom a machine learning point of view, the advantages of dimensional representation are in favor compared with cate-gorical representation  [5] . In the following, the benefits of using continuous values for emotions in dimensional space are detailed. A supervised machine learning model typically uses ground truth annotation. But due to the complexity of human emotions, there is always a disagreement on the perceived emotions and then annotations. So usually the assigned value of annotators would be aggregated to generate one single annotation per input. One of the main differences between categorical representation, which makes emotion recognition a classification task, and the dimensional representation, which makes emotion recognition a regression task, is the conserved information after aggregation of annotations. The most commonly used method for the aggregation is getting the majority vote of the annotator's opinion to have a hard label. Although, some studies such as  [6, 7, 8]  followed a soft labeling approach to deal with the labeling complexity and ambiguity. For example, in the standard protocols of IEMOCAP dataset  [9]  and MSP-Podcast corpus  [10] , the samples with disagreement of annotators are discarded.\n\nThe most common approach for encoding emotional categories is one hot vector, which ignores the relation or distance between emotions. For example, anger can be very close to irritation, frustration, and rage, and they are usually perceived or expressed in similar situations. On the contrary, dimensional annotation which provides a Distributed Representation can keep the intra and inter categories distance information. This continuous representation helps to break out the limitation of discrete labels.\n\nIt has been claimed by  [10]  that the perceived and expressed frequency of categorical emotions are not the same in the real life. In this case, an imbalanced classification problem would be faced  [11] . As the dispersion of emotional labels in different corpus has been shown in  [10] , the impact of the non-homogeneous frequency of class would be less important when the dimensional representation is employed.\n\nLast but not least, the dimensional representation has its application where a continuous value is needed more than only a category. In the task of continuous emotion recognition  [12, 13] , when a sequence of predictions over time is the goal, the dimensional approach helps to smooth the transition and take into account temporal dependencies. All these conveniences emphasize on the capacity of dimen-sional representation of emotional states.\n\nIn this paper, the coherence of these two annotation types in two common used corpora, IEMOCAP  [9]  and MSP-Podcast  [10] , is studied. Moreover, the capacity of classification models without using categorical annotations is investigated. This approach can show the advantages of dimensional annotation and representation, which is theoretically and empirically supported in  [5] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Challenge Of Emotion Annotation",
      "text": "One of the main challenges of the emotional dataset creation and annotating human emotions is the annotator agreement. As an example, two common datasets (IEMOCAP and MSP-Podcast) are compared in the following.\n\nUsing hard labeling of categorical annotation, 19.4% of samples in MSP-Podcast corpus and 25% of samples in IEMOCAP could not get an agreed annotation from evaluators. The reliabilities of agreement in these two corpora are not high. The Fleiss' Kappa of categorical annotations in the MSP-Podcast is only 0.23 and in the IEMOCAP is 0.48. A perceptual test in  [14]  showed that the human performance for emotion recognition of four main classes in IEMOCAP is only 69% overall accuracy.\n\nThe evaluators' agreement on dimensional annotation is not high as well, except for the Arousal on the IEMOCAP. Table  1  shows the inter-evaluator reliability (Krippendorff's alpha coefficient). To find a relation between two types of annotation, the density of samples with the same emotional label has been mapped in two main dimensions, arousal, and valence (See Figure  1 ). In order to have comparable experimental results and as it has been suggested by  [15, 16] , only the 4 main emotions (Neutral, Happy, Sad and Angry) from IEMOCAP and 5 main emotions (Neutral, Happy, Sad, Angry and Disgust) from MSP-Podcast have been used in the rest of this study.\n\nFigure  1  confirms that the annotator in the different corpus has a different definition or perception of emotions. While the Angry class in the IEMOCAP has been evaluated with higher valence and lower arousal than Neutral, it has been assigned to a lower arousal and higher valence values than Neutral in the MSP-Podcast. This conflict indicates the ambiguity of emotions' definition, which makes the emotion classification challenging in the cross corpus cases.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Classification Via Regressor",
      "text": "In order to investigate the relation between these categorical and dimensional annotations, the main goal of this study is to evaluate the ability of classification, based on the continuous values in the dimensional space. Figure  2  demonstrates two pipelines for recognizing a class of emotion, a one-step classification on the left and classification via regression on the right. The main idea is that the categorical label of samples can be predicted based on the dimensional values, as long as the annotations are consistence. Some studies such as  [5, 17]  support the hypothesis. In  [17] , it has been observed that a model for the prediction of arousal and valence values can be useful to detect categorical emotions. Second, the problem of imbalanced data would be less important since the frequency of samples in different categories would not have a high impact on the training process in the dimensional representation.\n\nThird, training on a distributed representation feature, which contains between and within class distances, can inject additional information into the training.\n\nForth, a trained regression model can be used for a classification task as well. In this case, based on the definition of categorical labels in dimensional space, the output of the re- gression model can be mapped to emotional vocabularies. It means the parallel annotations, categorical and dimensional, of a dataset would not be necessary. Only dimensional annotation and a mapping definition would be enough to have a prediction in two representations. This potential is the main focus of this study's experiments.\n\nFor the mapping, three algorithms are proposed; Gaussian classifier (Gaussian), K-Nearest Neighbors (KNN) (optimized K=50) and Tow Layer Perceptrons, 5*5, (2LP). These models are constructed to predict the categorical label based on dimensional values. In order to have an upper bound of classification performance based on dimensional values in three dimensions, the result of these mapping algorithms on reference annotation of IEMOCAP and MSP-Podcast is evaluated. The results show that the valance is the most discriminative attribute (followed by arousal and dominance) to predict samples' labels in both corpora. The combination of three attributes achieves the highest accuracy. Table  2  compares three dummy models with proposed mapping algorithms which predicts the class label based on the ground truth values of arousal, valance and dominance. The dummy models are Random labels, Prob. Random which generates labels randomly with respecting the probability of each class, Major Class which always generates the label of the most frequent class.\n\nIt is important to compare the performance of a Machine Learning model with these dummy models, especially in an imbalanced dataset. Based on the application, the evaluation metric can be different. When the task is emotion recognition in a real-life situation, the weighted performance can be more important. On the other hand, when the task is only to distinguish between different emotions, the unweighted performance can be applied. For this purpose, it is decided to report unweighted recall (UR) and weighted recall (WR). Needless to mention, contrary to classification models, the im-balanced data can cause less impact on regression models' performance. As it is noted in Table  2 , the weighted recall (WR) of selecting the only major class in the MSP-Podcast is 50.6%. The 2LP archives the best performance for the prediction of the emotional labels based on three attributes in the IEMO-CAP (with 4 classes). Although, it shows that a perfect regressor can map only 72.7% of samples from AVD space to the classes of emotion. The mapping algorithms on MSP-Podcast (with 5 classes) have different performances. It reveals the limitation of the proposed idea of using regression models for the classification task.\n\nIt should be mentioned that the objective of this study is not to outperform the classification task, but only to compare the performance of the two approaches. The main idea of this paper is to study the performance of a regression model on AVD space, which can be used for classification as well. The regression model can use all available information in a corpus (samples are not limited to certain categories) for training. Moreover, the class label can be an interpretation of the model's output, which means it is not necessary to have a categorical annotation of samples. This interpretation or mapping from dimensional space to categorical labels can be simply done by defining the emotional classes in continuous space in the posterior.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiment Design",
      "text": "We propose to build a regression model to predict a vector of values in the continuous space as the representation of the emotional state. The output of a trained regressor can be fed to a mapping model to transform into emotional labels. The training of mapping models defines the categorical emotions in dimensional space. In our experiment, they are constructed using the training set of the corresponding corpus. The performance of classification via the regressor model (the right pipeline in the Figure  2 ) is compared to a classification model (the left pipeline in the Figure  2 ). The main aim is not to outperform the state-of-the-art system, but only to propose a different view toward emotion recognition and its potential. Although, the state-of-the-art methods such as data augmentation  [16] , reject option  [18] , use fine-tuning or more fancy feature extractors  [19, 20]  can be applied to improve the performances.\n\nUsing a similar architecture for the classifier and regressor provides the chance of comparing two approaches with the almost same capacity of learning (number of network's weights). The regression models employ a linear layer as output and MSE as their loss function. The classification models are similarly designed, with some modifications. Their output layer is adapted to the number of classes, the output layer is modified to the softmax, and cross-entropy is used as their loss function.\n\nIn order to create the classifiers and regressors, a combination of three feature extractors and three different neural network architectures has been tested.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Feature Extractors",
      "text": "By emerging of pretrained neural network models and their decent performance on different tasks, in particular for emotion recognition  [16, 19] , we propose to use pretrained wav2vec2  [21]  and wavLM  [22]  models as the feature extractor.\n\nThe wav2vec2  [21]  used self-supervised learning on raw audio to transform it into an embedding representation. We used the wav2vec 2.0 base model, pre-trained on Librispeech (960 hours of speech) without fine-tuning. Same as wav2vec2, the wavLM Base+  [22]  extracts universal speech representations. But unlike wav2vec2, the wavLM model has been trained on massive unlabeled speech data (94k hours of speech). By using these two pre-trained models, the raw audios are encoded into a sequence of embeddings with a window length of 25ms and a stride of 20ms.\n\nMoreover, the Mel spectrogram (MelSpg), which showed a decent performance in  [23] , has been used. In order to have the same length of feature sequence, the same configuration of the sliding window has been set for MelSpg.\n\nThese three feature extractors generate a vector for each frame of given audio. To treat all audio signals with variable lengths, in the same way, a padding/truncation method has been applied to have a fixed length (500 frames from the first 6.9 sec) of features. 128 features per frame have been extracted using MelSpg, while using wav2vec2 and wavLM would return 512 features per frame.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classifier/Regressor Architectures",
      "text": "In order to predict the categorical emotions or the continuous values in AVD space, three different architecture has been designed (see Figure  3 ).\n\nThese downstream models are constructed with almost the same number of trainable parameters. The MLP model (see Figures  3 -a ) constructed with 5 stacked fully connected layers. In order to generate a prediction, the last layer is a fully connected layer adapted corresponding to the number of classes in the classification task or the number of dimensions (three in AVD) in the regression task. While the MLP model is limited to the mean and variance of frames' features, two other models can profit from the temporal information. The CNN model, inspired by  [19] , (see Figures  3 -b ) is consecutive feed forward of 5 blocks of convolutional layer, batch normalization layer and max polling. The CNN-Trans model (see Figures  3 -c ) is a parallel downstream architecture. On one side, it is 4 stacked convolutional layers. On the other side, the sequential information would be passed through an average pooling to reduce the dimensionality, afterward, it would be fed to two transformer encoder blocks with two heads of attention. Then the Transformer embedding will be concatenated to the CNN side. Finally, the result will pass through a fully connected layer to generate the prediction in the same way as the last layer of MLP.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Different Datasets Different Challenges",
      "text": "Besides the challenges of different definitions and the consistency of two representation approach (mentioned in the section 2), the context of speech emotion recognition application plays a major role in the evaluation metrics. The context of application makes the designing of speech emotion corpora different. While IEMOCAP dataset  [9]  is recording of acted (some may call exaggerated) emotions with respecting the frequency of each class, the MSP-Podcast dataset  [10]  is extracted from recorded spontaneous speech without considering balancing the classes.\n\nIn this study, we use IEMOCAP and MSP-Podcast datasets to investigate our hypothesis on different contexts. The IEMOCAP dataset with 12h is smaller than the MSP-Podcast datasets with 27h of speech. In order to follow the previous studies  [16, 19] , four main emotions (Natural 30.9%, Happy 29.6%, Angry 19.9%, Sad 19.6%) in IEMOCAP has been selected which makes 5531 samples from all sessions. Contrary to this balanced distribution of emotions, for the MSP-Podcast datasets, like  [15, 16] , five main emotion classes (Natural 53.3%, Happy 29.3%, Angry 6.6%, Sad 5.4%, Disgust 5.3%) have been selected which contains 48754 samples. The impact of imbalanced classes in the MSP-Podcast can be observed in the performance of dummy classifiers in table  2 .\n\nThe reported performance in this study is based on 5-fold cross-validation for the IEMOCAP dataset. The original partitioning of the MSP-Podcast dataset into train/dev/test is respected, and evaluations are based on the sum of test1 and test2 partitions  [10] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "The performance of regressor models (three feature types, three architectures) has been calculated based on the Concordance Correlation Coefficient (CCC)  [24] . The output of the regressor model as AVD values has been mapped to categorical emotions using three algorithms mentioned in the section 3. The classification performance of these two approaches with different configurations on the IEMOCAP dataset and MSP-Podcast test set, are shown in tables 3 and 4.\n\nThe results on two corpora show a low performance of classification if temporal information would not be taken into account. Using MelSpg in MLP architecture achieves a lower performance (almost equal to a dummy performance in MSP-Podcast) comparing with cases which temporal information have not been used in extracted features (wav2vec2 or wavLM) or in the model architectures (CNN or CNN-Trans).\n\nIn the IEMOCAP dataset (see table  3 ), the best classification results obtained by feeding the extracted embedding from wav2vec2 into MLP model with UR=63.16% and WR=61.81%. The same configuration achieved the best regression performance with the CCC of Arousal equal to 0.44, the CCC of Valence equal to 0.72, and the CCC of Dominance equal to 0.57. The best performance of classification via regression is obtained by applying Gaussian classifier as the mapping algorithm. Our experiment shows a lower performance of Gaussian Naive Bayes algorithm, which uses the frequency of classes in the training set as the prior probability. Comparing \"classification\" and \"classification via regression\" approaches shows that using dimensional annotation can successfully classify emotions with more than 80% proportional performance to using direct categorical annotation (UR=51.44% and WR=53.00%).\n\nThe best performances achieved using wav2vec2 or wavLM features and MLP or CNN-Trans models, which is decent compare to  [16] . Same as IEMOCAP corpus, the best performance of regression and classification via regression obtained using wav2vec2 and MLP. We believe that there are several reasons for poor performance of models on the MSP-Podcast dataset, table 4. As it has been observed in the table 1, the annotator agreement is low in this corpus. Also, the emotions are not acted, which makes them more difficult to recognize. Moreover, most of the samples in this corpus are labeled as Natural, which makes classification more difficult in imbalanced problems (see poor UR in table  4 ).\n\nThe performance of classification via regression on these two corpora shows the potential and the limitation of this approach. Although following this approach can reduce the performance of classification, there are still several advantages. Using only dimensional annotation for training a regressor model can reduce the cost of preparing a dataset and using all available data. It also can prepare a model which can be used for the recognition of new emotion categories by only providing its definition in the AVD space. In order to show the potential of \"classification via regression\", we trained an MLP model using wav2vec2 features on samples from IEMOCAP with 4 emotions (Natural, Happy, Angry, Sad) as a regressor. Then we asked the pipeline to classify emotions with a",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we investigated the relationship between dimensional representation and categorical labeling of emotions. We proposed to consider the speech emotion recognition task as a regression problem whose output can be interpreted as categorical emotions by using a mapping algorithm. However, there are several benefits of following the \"classification via regression\" approach, our experiment on two different corpora shows degradation of performance compared to a classifier that profits from categorical labeled data. We compared the performance of these two approaches by employing three feature types, three architectures as regressor/classifier, and three mapping algorithms for transforming the continuous value in the AVD space to categorical labels. While the main objective of this study was not to outperform the state of the art, we showed the potential and limitations of the \"classification via regression\" approach.\n\nThe benefits of the proposed approach are reducing the cost of data preparation and breaking the limitation of the predefined number of emotional categories. The performance of the model can be improved by applying the state-of-theart techniques, such as fine-tuning the pre-trained wav2vec2 or wavLM feature extractors  [19, 20] . Although the performance of the best regressor model and mapping algorithm as the classifier is decent, particularly in the IEMOCAP dataset.\n\nThe importance of dimensions in predicting categorical labels (see Section 3) suggests modifying the loss function of the regressor model to a weighted loss in the future works.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). In order to have comparable experimental results",
      "page": 2
    },
    {
      "caption": "Figure 1: conﬁrms that the annotator in the different corpus",
      "page": 2
    },
    {
      "caption": "Figure 2: demonstrates two",
      "page": 2
    },
    {
      "caption": "Figure 2: Classiﬁcation (the left pipeline) versus classiﬁcation",
      "page": 2
    },
    {
      "caption": "Figure 1: Density of the main emotion categories on normalized Arousal and Valence space. Non homogeneity of densities in",
      "page": 3
    },
    {
      "caption": "Figure 2: ) is compared to a classiﬁcation model",
      "page": 4
    },
    {
      "caption": "Figure 2: ). The main aim is not to",
      "page": 4
    },
    {
      "caption": "Figure 3: Downstream models for regression and classiﬁcation (by adding a softmax layer as the last layer and adapting the output",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Feature\nArchitecture",
          "Classiﬁcation": "UR\nWR",
          "Regression": "CCC (A, V, D)",
          "Classiﬁcation via Regression (UR, WR)": "2LP\nKNN\nGaussian"
        },
        {
          "Model": "MelSpg\nMLP\nMelSpg\nCNN\nMelSpg\nCNN-Trans\nwav2vec2\nMLP\nwav2vec2\nCNN\nwav2vec2\nCNN-Trans\nwavLM\nMLP\nwavLM\nCNN\nwavLM\nCNN-Trans",
          "Classiﬁcation": "25.02\n26.00\n54.14\n53.73\n56.60\n56.13\n63.16\n61.81\n31.93\n35.79\n61.42\n59.16\n57.61\n57.26\n35.38\n38.41\n57.78\n55.95",
          "Regression": "(0.15, 0.65, 0.46)\n(0.36, 0.66, 0.50)\n(0.26, 0.66, 0.50)\n(0.44, 0.72, 0.57)\n(0.10, 0.42, 0.32)\n(0.31, 0.67, 0.52)\n(0.37, 0.72, 0.55)\n(0.04, 0.24, 0.20)\n(0.19, 0.47, 0.38)",
          "Classiﬁcation via Regression (UR, WR)": "(35.56, 40.03)\n(34.49, 38.84)\n(38.30, 41.81)\n(44.86, 48.00)\n(42.81, 46.17)\n(45.38, 47.79)\n(43.33, 45.21)\n(42.07, 44.57)\n(45.06, 46.05)\n(51.44, 53.00)\n(49.63, 51.40)\n(47.07, 48.63)\n(32.48, 36.37)\n(30.83, 34.70)\n(34.56, 37.82)\n(45.35, 47.57)\n(43.06, 45.93)\n(46.74, 48.17)\n(47.48, 50.44)\n(45.55, 48.85)\n(50.05, 52.10)\n(27.59, 32.27)\n(27.99, 32.51)\n(28.10, 32.40)\n(40.05, 42.14)\n(37.61, 40.18)\n(40.76, 42.14)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Feature\nArchitecture",
          "Classiﬁcation": "UR\nWR",
          "Regression": "CCC (A, V, D)",
          "Classiﬁcation via Regression (UR, WR)": "2LP\nKNN\nGaussian"
        },
        {
          "Model": "MelSpg\nMLP\nMelSpg\nCNN\nMelSpg\nCNN-Trans\nwav2vec2\nMLP\nwav2vec2\nCNN\nwav2vec2\nCNN-Trans\nwavLM\nMLP\nwavLM\nCNN\nwavLM\nCNN-Trans",
          "Classiﬁcation": "20.00\n50.60\n28.34\n54.88\n29.08\n54.10\n36.89\n57.92\n26.42\n51.36\n58.76\n33.98\n34.87\n57.14\n27.39\n52.70\n33.05\n56.89",
          "Regression": "(0.30, 0.05, 0.21)\n(0.41, 0.09, 0.31)\n(0.43, 0.10, 0.30)\n(0.51, 0.17, 0.40)\n(0.13, 0.01, 0.09)\n(0.46, 0.06, 0.37)\n(0.49, 0.13, 0.39)\n(0.00, -0.00, 0.01)\n(0.41, 0.03, 0.32)",
          "Classiﬁcation via Regression (UR, WR)": "(20.04, 51.35)\n(20.16, 51.48)\n(20.47, 51.72)\n(22.87, 53.62)\n(22.92, 53.39)\n(23.45, 54.04)\n(22.80, 54.60)\n(22.54, 54.34)\n(23.85, 55.14)\n(25.88, 56.25)\n(21.87, 53.62)\n(21.34, 53.05)\n(20.02, 51.30)\n(20.09, 51.38)\n(20.39, 51.55)\n(20.41, 51.86)\n(20.21, 51.54)\n(23.46, 55.24)\n(21.23, 53.12)\n(20.79, 52.50)\n(23.47, 55.89)\n(20.00, 51.28)\n(20.00, 51.28)\n(20.01, 51.24)\n(20.15, 51.52)\n(20.21, 51.61)\n(23.28, 55.52)"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Describing the emotional states that are expressed in speech",
      "authors": [
        "Roddy Cowie",
        "Randolph Cornelius"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "3",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "4",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "5",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller",
        "Maja Pantic",
        "Roddy Cowie"
      ],
      "year": "2011",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "6",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "Roddy Georgios N Yannakakis",
        "Carlos Cowie",
        "Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Soft-target training with ambiguous emotional utterances for dnn-based speech emotion classification",
      "authors": [
        "Atsushi Ando",
        "Satoshi Kobashikawa",
        "Hosana Kamiyama",
        "Ryo Masumura",
        "Yusuke Ijima",
        "Yushi Aono"
      ],
      "year": "2018",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Maximilian Schmitt",
        "Maja Pantic",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "11",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Meta-learning for speech emotion recognition considering ambiguity of emotion labels",
      "authors": [
        "Takuya Fujioka",
        "Takeshi Homma",
        "Kenji Nagamatsu"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "14",
      "title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies",
      "authors": [
        "Martin Wöllmer",
        "Florian Eyben",
        "Stephan Reiter",
        "Björn Schuller",
        "Cate Cox",
        "Ellen Douglas-Cowie",
        "Roddy Cowie"
      ],
      "year": "2008",
      "venue": "Proc. Interspeech incorp. 12th Australasian Int. Conf. on Speech Science and Technology"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "Vladimir Chernykh",
        "Pavel Prikhodko"
      ],
      "year": "2017",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "16",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Tianzi Wang",
        "Jesus Villalba",
        "Nanxin Chen",
        "Najim Dehak"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Detecting emotion primitives from speech and their use in discerning categorical emotions",
      "authors": [
        "Vasudha Kowtha",
        "Vikramjit Mitra",
        "Chris Bartels",
        "Erik Marchi",
        "Sue Booker",
        "William Caruso",
        "Sachin Kajarekar",
        "Devang Naik"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with a reject option",
      "authors": [
        "Kusha Sridhar",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "21",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "22",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2021",
      "venue": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "arxiv": "arXiv:2110.13900"
    },
    {
      "citation_id": "24",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio",
      "authors": [
        "Felix Weninger",
        "Fabien Ringeval",
        "Erik Marchi",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "IJCAI"
    }
  ]
}