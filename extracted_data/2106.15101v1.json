{
  "paper_id": "2106.15101v1",
  "title": "Smart And Context-Aware System Employing Emotions Recognition",
  "published": "2021-06-29T05:36:19Z",
  "authors": [
    "Stuti Sehgal",
    "Harsh Sharma",
    "Akshat Anand"
  ],
  "keywords": [
    "human emotions",
    "color therapy",
    "music recommendation",
    "human computer interaction",
    "context aware system"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "People have the ability to make sensible assumptions about other people's emotional states by being sympathetic, and because of our common sense of knowledge and the ability to think visually. Over the years, much research has been done on providing machines with the ability to detect human emotions and to develop automated emotional intelligence systems. The computer's ability to detect human emotions is gaining popularity in creating sensitive systems such as learning environments, health care systems and real-world. Improving people's health has been the subject of much research. This paper describes the formation as conceptual evidence of emotional acquisition and control in intelligent health settings. The authors of this paper aim for an unconventional approach with a friendly look to get emotional scenarios from the system to establish a functional, non-intrusive and emotionally-sensitive environment where users can do their normal activities naturally and see the program only when pleasant mood activating services are received. The contextsensitive system interacts with users to detect and differentiate emotions through facial expressions or speech recognition, to make music recommendations and mood color treatments with the services installed on their IoT devices.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotional monitoring is important as it contains information that can help improve people's well-being. Behavioral automated modeling is essential for the development of modern applications in AI. Automatic emotional recognition has many applications in areas where equipment or machines need to communicate or monitor people. Gen-Y does not have time to visit psychiatrists as they are busy with devices connected on the internet, and during such testing times, people's mental health has taken a toll. In today's world, stigma and subjugation associated with mental health continue to plague mankind. An efficient system can greatly help people to improve their emotional state by monitoring their behavior.\n\nTo fix this subjugation; electrodes penetrating the skin are attached to the head in an electroencephalogram (EEG) for detecting human emotions by analyzing delta, theta, alpha, beta, and gamma-band waves  [1] ; or hefty fees are paid to psychiatrists; mood detection using facial expressions and speech sensitivity becomes need of the hour.\n\nAn alternative approach not undertaken by the authors of this paper, heterogeneous and wearable sensors send biometric data to monitor the subject, generating large amounts of sensitive data (e.g. images)  [2] . Also, the location where such systems are installed should be considered and agreed upon by users, and it is considered that the sensors are located in strategic locations that require the optimal location of the target. In line with ethical standards, monitoring a person in their private living space should be considered a sensitive issue.  [3]  Concretely, a person's emotional state can be reflected in a certain image or speech. This problem is widely studied in computer view especially on two sides:\n\n(1) facial analysis  [4] , and (2) posture and gesture analysis. Face Recognition (FER)  [5]  can be widely used in a variety of research areas, such as psychiatric diagnosis and social / physical interactions. Speech emotion recognition (SER)  [6]  is still a challenging task in the field of high computation because there are no defined levels of sensitivity. The speech signal contains large amounts of information related to the emotions conveyed by a person. The speech recognition system fails miserably if powerful strategies are not developed to deal with speech sensitive emotion recognition.\n\nThe authors of this paper propose a context-aware system consisting of user-controlled device cameras such as video sensors and microphones as sound sensors, for interpreting facial expressions from static image annotations and companion text or speech sensitive emotion recognition, aimed to create a smart, advanced AI environment for sentiment analysis. A non-intrusive approach has been demonstrated, using user devices equipped with web cameras as image sensors to perform facial recognition (FER) and microphones to perform speech emotion recognition (SER). Due to the proliferation of inexpensive sensors such as cameras and microphones, as well as embedded processors, there are unprecedented opportunities for realizing real-time applications such as real computer immersion, and gesture control, and other human-centered applications such as smart and assistive environments.\n\nSmart environments are important for the well-being of people with disabilities as it can greatly improve their daily lives. People with mobility impairments tend to choose camera and speech connectors, as this is customized, comfortable, expensive, and does not require user-borne accessories that can draw attention to their disability. Image and speech input is processed in real-time so that users can work effectively with assistive software to improve their lifestyle.\n\nThe remainder of this paper is structured as follows. In Section II, a review of the previous research pertaining to Emotion recognition and Context-Aware systems is presented and in Section III, we discuss the Emotions Module of this research which includes a detailed description of the proposed system. Music recommendation and Color therapy actuation services of the proposed system have been introduced in sections IV and V, respectively followed by the analysis of the results of the experiments in Section VI and the concluding remarks in Section VII. Future work and visualization of a truly efficient, smart, natural and healthy environment system has been discussed in Section VIII.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "Recent research has focused on improving the quality of life of a person by designing and building sensors that are directly or indirectly connected to the human body  [3] . This method, previously researched in the field of intelligent design, however intrusive, is a fusion of a number of components including physiological signals (blood volume and heart rate, respiration, EEG, and skin conductance), sensory integration, human-computer connectors, networking, and extensive computing for the purpose of multimodal recognition of emotions and actuation of response services  [2] . One of the most challenging domains in multi-sensor environments  [7]  is the automatic analysis of multi-group conversations from sensory data.\n\nThe importance of context in emotion perception is well supported by various studies in psychology. Non-verbal communication is important, as it is substantially documented in social psychology and cognition, it opens up new opportunities for new cameras and microphone-equipped spaces  [8] . Human interactions with the computer will be natural and effective when interfaces are dealing with human emotions or stress. Previous studies have focused on the acquisition of emotion via multiple sensors but the recognition of facial expression and speech sensitive emotion is gaining importance because of its broader system. From a computer vision perspective, most previous attempts have focused on analyzing facial expressions and, in some cases, on body movements and gestures. Some of these methods work best in certain settings. The two most popular methods used in the literature for automated FER systems are based on geometry and appearance. Most researchers use variations of Russel's circumplex model (Fig.  1 ) which provides a distribution of basic emotions in a dual space in relation to valence and arousal  [9] : Valence (V), which measures how pleasant emotions are, from bad to good; Arousal (A), which measures a person's level of disturbance, from inactivity / calm to anger/willingness to act; and Dominance (D) which measures the level of control a person feels about the condition, from compliance / uncontrolled to incontrol / management. As emotion recognition presumes the modeling of the dynamics of acoustic or visual features, some classification strategies in the field of AER make use of dynamic classifiers like Hidden Markov Models (HMM) and Dynamic Bayesian Networks (DBN)  [10] . Alternative strategies apply static techniques such as Support Vector Machines (SVM) that process statistical functions of low-level features which are computed over longer data segments  [11] . Quality of the human-computer interface that mimics human speech emotions relies heavily on the types of features used and also on the classifier employed for recognition  [12] . A unimodal framework for short-term context modeling in dyadic interactions was proposed in  [13] .\n\nThe emotional state of a speaker can be identified from the facial expression (Ekman, 1973; Davis and College, 1975; Scherer and Ekman, 1984)  [1]  and spoken words  (McGilloway et al., 2000; Dellaert et al., 1996; Nicholson et al., 1999) . Ultimately, a combined analysis of these features leads to high accuracy of recognition. In this paper, the focus is multimodal: on facial expression and speech emotion recognition, allowing for the definition of a desired emotion and evaluating its intensity.  [14]  III. EMOTIONS MODULE:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset Construction",
      "text": "Over the past decade, much effort has been put into building facial expressions and speech emotion recognizers. Because most researchers use limited data sets, the generalizability of these various methods remains unknown. Currently, technological approaches to image-related activities such as image classification and object acquisition are all based on Convolution Neural Networks (CNNs). These operations require the construction of CNN with millions of parameters. Commonly used CNN extraction features include a set of layers that are fully integrated at the end. Fully connected layers most of the parameters in a CNN  [15] .\n\nThe basis of FER is the Facial Action Coding System, which incorporates facial expressions using a set of specific local facial movements, called Action Units. These facialbased approaches often use facial geometry-based features to describe the face. Afterward, the extracted features are used to identify Action Units which include regions such as eyes, cheeks, lips that are used to perceive basic emotions: anger, joy, sadness and neutral speech.\n\nTwo datasets were used in this research work. The first dataset was Extended Yale Face database (Fig.  2 ) consisting of:\n\nSix basic emotion classes: anger, happy, neutral, sad and disgust containing 16128 images of 28 human subjects under 9 poses and 64 illumination conditions. The dataset creator had cropped the images to face-only, performed operations like gray-scaling and resizing to 48x48. The second dataset was the Kaggle Facial Emotion Recognition (FER2013) dataset  [16]  [17] comprising of 35,887 images, each 48 x 48 pixels (8-bit grayscale) with disgust being the only underrepresented one within the Kaggle dataset (Fig.  2 ), at 1.\n\nWe found these datasets to be representative because of their size, and unstructured nature of faces (in terms of facial orientation, ethnicity, age, and gender of the subjects). Data Preprocessing Steps: • Haar Cascades to crop out only faces from the images from live feed while getting real-time predictions We chose RAVDESS dataset  [18]  for Speech Emotion Recognitions :- Depending on the frequency, the higher the pitch the higher the frequency.   The FER and SER models (Fig.  5 ) have been designed with the idea of attaining the best generalization accuracy. The former layer of each block represent the layers of the proposed FER model. The latter layer of each block represent the layers of the proposed SER model architecture.\n\nWe propose a model for four-class facial emotion classification (anger, happy, sad and neutral) which we evaluate in accordance to their test and generalization accuracy. These primary emotions are often referred to as \"archetypal\" emotions. Although these archetypal emotions cover a rather small part of emotions in psychological research, they represent the popularly known emotions and are recommended for testing the capabilities of an automatic emotion recognizer. The FER model(Fig.  5 ) was designed with the idea of attaining the best generalization accuracy.\n\n1) Input Layer: The input layer has fixed and predetermined dimensions. So, for pre-processing the detected face, we used OpenCV library for face detection in the image before feeding it into the layer by passing through pretrained filters from Haar Cascades. • Categorical Cross Entropy H ( p, q) was chosen as loss function and the equation is given as follows,\n\nwhere p i is the truth label and q i is the softmax probability for i ith class and n is the number of classes or 4. We propose a model(Fig.  5 ) for sixteen-class Speech emotion recognition classification (neutral, calm, happy, sad, angry, fearful, disgust, surprise, 8 for female and 8 for male speech) which we evaluate in accordance with their test and generalization accuracy. The model was designed to get the best generalization accuracy.\n\n1) Image Data Segregation: All the images are of RGB Channels, separated with a fixed batch size of 16, and are segregated based on the actual 16 emotion classes. 2) A custom class of neural networks has been defined to achieve the goal. In this class, we first defined the core sequential model with the image data generator as a pretraining parameter. Convolution 1-D Layers along with normal Dense layers are used for the core model. Since this was a multi-class classification, we implemented a Conv1D Model along with a 'selu' activation function initially and a 'softmax' activation function in the end with a relevant optimizer like 'adam' to implement the core model for this purpose. 3) Fine Tuning: After Defining the initial architecture, the following were implemented to ensure the smooth process of training and to expect a better-generalized accuracy:\n\n• Conv1D(256,input shape=(66,1))\n\n• Activation('selu') to have a smooth learning process with help of 'Adam' Optimiser with learning rate=0.001 for 65 epochs. Early callbacks have been imposed with a patience=10, to wait initially for achieving better accuracy and minimizing the loss function along with stopping the training if the learning worsens. Also, we defined a custom class to directly export the model with the best results achieved,i.e, best accuracy in the entire training epochs in between, and to keep track of whether the next epoch had a better accuracy or the previous one.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Emotion Recognition Module",
      "text": "The system has to decide automatically the proper reaction in response to the detected emotion, in order to simulate a positive response in the user. Decision making can vary from one user to another and also in accordance with his/her health evolution. It is in \"Decision making\" where the intelligence of the system lies  [20] . Considering the emotion detected the system reacts and decides the action to execute. For example, considering that our system reacts to the user's mood, once this is detected, the system will vary the color and change the music. For instance, if the action is to create a relaxing environment, the system will launch commands addressed to project warm colors, and play pleasant music (in broad terms)  [21] .A variety of application services in ambient intelligence environments can be realized at reduced cost by encapsulating attractive services like music recommendation  [22]  and color/light-actuation in middleware applications that are shared by the IoT connected devices. High level abstractions offered by middle-ware infrastructures make it possible to hide the complexities in ambient intelligence environments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Music Recommendation Module",
      "text": "Balkwill and Thompson found that even listeners who are unfamiliar with the tonal system are sensitive to the emotion expressed by a piece of music. Hunter et al. explored how tempo and musical harmony affect the emotional responses of listeners  [23] . They found that quick-tempo and high-speed routes add to the excitement, while slow-moving, low-key modes make listeners feel sad. (Interestingly, both levels of happiness and sorrow were high when fast tempo was combined with major modes or slower tempos with minor modes.) These findings are consistent with Hevner's conclusions, and support the use of tempo and tonality modulation within the proposed application. Zentner, Grandjean, and Scherer's Lundqvist et al.  [7]  demonstrated that listeners' responses to happy stimuli generated \"more zygomatic facial muscle activity, more pleasure, and less sadness\" than sad stimuli which is often seen in greater magnitude than it may sound. These results can be used as a basis for building a response to the proposed plan. Of Saarikallio's seven strategies for MMR, AMAI  [23]  focuses on discharge (\"venting anger or sadness through music that expresses these emotions\"), diversion (\"forgetting thoughts and feelings with the help of good music \"), and progression from discharge to diversion. Given the results of Lundqvist et al, their findings have confirmed that the use of subsequent diversion can have a consistent effect on all listeners rather than the use of diversion technique alone.\n\nIn this paper we described the design, and implementation of a system for generation and playback of music that adapts to listeners' affective states with the goal of increasing positive affect-\"diversion\". In particular, we explored the effectiveness of employing the diversion MMR strategy. A case has been made of dynamic music that has the ability to direct the listener results more efficiently than standard music such as Fig.  7 . Architecture of proposed system MP3, like in the case of AMAI, the conversion from discharge to diversion was controlled by the user's situation, with the system having control of the precise timing of when to switch sections and strategies. Using Selenium automation in Python, whenever you make prediction from the proposed model, you get a word as emotion -'ANGRY', 'HAPPY', 'NEUTRAL', 'SAD' which is used in automation  [16]  for parsing the YouTube web pages using 'chromedriver 87.0.4280.88' for automatically playing music to simulate a positive effect when users' facial emotion is detected, and redirects to the recommended YouTube MP4 video.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Color/Light Actuation",
      "text": "Color is another factor that is omnipresent in an environment. Moreover, evidence of the influence of color on human emotions is validated by metonymic and metaphoric thinking, formation of specific emotional reactions for color perception, and sharing connotative structure in the language for color and emotion terms. Color is a powerful communication tool and can be used to express action, influence emotions, and influence the body's response. Certain colors are associated with increased blood pressure, weight gain, and eyestrain. Many authors emphasize that certain colors have a profound effect on mood and control. For instance, in a trial, muscle strength is decreased within 2: 7 s inmates in prisons who resided for a limited time in bubble-gum pink cells (Baker-Miller pink) cells  [7] . It is important to note that it has been shown that color features such as chroma, hue, or light also have an effect on emotions. It is known that while perceiving color, the brain associates it with a particular emotion. This phenomenon is known as color emotion. One can appreciate the colors that provide the desired change from negative to positive according to previous works. Also, depending on the initial sensitivity to the orthogonal scale, we have(fig.  8  In facial emotion recognition, we were able to achieve a four-class generalization accuracy of 77.92%, test accuracy 75.39%, and train accuracy of 87.14% for classifying an image of a face under labels \"happy\", \"sad\", \"angry\", \"neutral\". In speech sensitive emotion recognition, we were able to achieve a validation accuracy of 77.3%, test accuracy of 73.3% and train accuracy of 93% for classifying 8 different emotions in speech.\n\nWith the emerging advanced technologies in hardware and sensors, FER and SER systems  [24]  have been developed to support real-world application scenes, instead of laboratory environments  [25] . Although the laboratory-controlled systems achieve very high accuracy, around 97%, the technical transferring from the laboratory to real-world applications faces a great barrier of very low accuracy, approximately 50%. Undoubtedly, user feedback related to color depends on a large set of external factors, such as gender, age, culture, preferences, emotions, and content (e.g. time of day or location). Identifying emotional state and interpreting well using deep-learning algorithms  [26]  [27] has proven to be complex due to the high variability of samples in each activity.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "The main purpose is to keep the user's emotional state healthy. The proposed constructs for the acquisition of emotions and their regulation in the Smart Home Environment are novel, open, and flexible. As a result, the detection of facial expressions and speech is done in a non-intrusive manner. The architecture is not pre-designed to target a particular health care problem, but should be able to deal with most emotionally-related problems. Mixing multimodal data significantly increased recognition levels compared to unimodal programs: multimodal method demonstrated more than 10% improvement in relation to the most successful unimodal system. Moreover, decision making of the smart contextaware system environment improved. This paper described the complete structure (fig.  7 ) of providing all the necessary functions and interfaces for emotional recognition and control. First, the purpose is to determine the user's feelings by analyzing facial expressions or sensitive speech emotions. The algorithm has three stages. In the image processing phase, the facial region and facial action elements are extracted. Haar Cascade is adopted to extract the facial region from an image. Speech sensitive emotion recognition was performed in two main steps: feature (mel scale, pitch, frequency, chroma, fourier transform) extraction and classification by employing deep-learning algorithms. Then, the system makes decisions to steer the emotions of the user to a positive state with music and color/light actuation. On the output end, music and color rendering is done by changing the color of their IoT devices from a variety of colors to soften the acquired mood, for instance, a person in an angry mood is toned down by colors lying on the soft side of the color spectra whereas a person feeling sad is uplifted by bright colors for driving the user to a pleasant state of mood.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Viii. Future Work",
      "text": "Ingenuine and forced expressions pose a challenge to actual emotion detection and probing systems towards a healthier and happier environment for the user. A system operating in the real world would not rely completely on single or separate information sources due to problems such as noise, false positives or occlusions.\n\nThe difficulty in the introduction of a new intelligent system is in the uncertainty of its effect in the society. The benefit of any system appears when it is implemented deeply into the society and impacts social activities in a large scope. Social simulation, especially, through a method of inducing mood, will provide an important tool to evaluate how the system may change the behavior of the society. Researchers will be in permanent contact with the users to solve problems that may arise and to monitor the level of the users' satisfaction, so as to improve the system's feedback in regulating emotions by stimuli such as color and music.\n\nColors can be subjective -what can make one person feel happy can make another person feel annoyed depending on the user's past experience or cultural differences. Therefore, the smart context-aware environment system should be tailored to each user according to his or her preference. Providing immersion in AR, and giving listeners an active agency (rather than a system that controls all aspects of music) can further improve the system's performance. Therefore, some userassociated information is welcome.\n\nWhile recent researches show much promise, they are first and foremost indicative of the fact that there's a long way to go before we arrive at true user-centered approach to ambient intelligence systems, visual lie-detectors and threat detection security systems, which can combine automated facial emotion, body language analysis and speech sensitive emotion for spotting potentially risky situations. Relevant to the development of AI systems that can make an environment truly intelligent is the need for such an environment to be able to:\n\n1) learn habits, preferences and needs of the specific user, 2) enhance user capabilities and comfort by providing new and automated service execution, 3) enhance user capabilities and comfort by providing new and automated service execution, 4) integrate IoT connected elements like smart color blinds, smart home and mobile robots, and 5) provide a structured way to analyze, decide and react over that smart, friendly environment.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) which provides a distribution of basic emotions in a dual",
      "page": 2
    },
    {
      "caption": "Figure 1: The Russel’s circumplex model of emotions",
      "page": 2
    },
    {
      "caption": "Figure 2: ) consisting",
      "page": 3
    },
    {
      "caption": "Figure 2: Dataset used for training and testing [17]",
      "page": 3
    },
    {
      "caption": "Figure 3: Image output of the audio by libROSA python library",
      "page": 3
    },
    {
      "caption": "Figure 4: Mel spectrogram for a characteristic emotion",
      "page": 4
    },
    {
      "caption": "Figure 5: ) have been designed",
      "page": 4
    },
    {
      "caption": "Figure 5: ) was designed with the idea of attaining the best",
      "page": 4
    },
    {
      "caption": "Figure 5: ) for sixteen-class Speech emo-",
      "page": 4
    },
    {
      "caption": "Figure 5: Architecture of FER and SER model",
      "page": 5
    },
    {
      "caption": "Figure 6: A glimpse of our application",
      "page": 5
    },
    {
      "caption": "Figure 7: Architecture of proposed system",
      "page": 6
    },
    {
      "caption": "Figure 8: Schematic ﬂowchart of Color/Light Actuation",
      "page": 6
    },
    {
      "caption": "Figure 9: Application of the color therapy for emotion sad",
      "page": 6
    },
    {
      "caption": "Figure 10: Training Loss and Accuracy of FER model",
      "page": 7
    },
    {
      "caption": "Figure 11: Training Accuracy of SER model",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "FERModel",
          "Trainacc.(%)": "87.14%",
          "Testacc.(%)": "75.39%",
          "GeneralizationAcc.(%)": "75.39%"
        },
        {
          "Column_1": "SERModel",
          "Trainacc.(%)": "93%",
          "Testacc.(%)": "73.3%",
          "GeneralizationAcc.(%)": "77.3%"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On the discovery of events in EEG data utilizing information fusion",
      "authors": [
        "M Schels",
        "S Scherer",
        "M Glodek",
        "H Kestler",
        "G Palm",
        "F Schwenker"
      ],
      "year": "2013",
      "venue": "Comput. Stat",
      "doi": "10.1007/s00180-011-0292-y"
    },
    {
      "citation_id": "2",
      "title": "Human emotion recognition: Review of sensors and methods",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors (Switzerland)",
      "doi": "10.3390/s20030592"
    },
    {
      "citation_id": "3",
      "title": "Human Emotion Recognition Using Smart Sensors A Thesis submitted in fulfilment of the Master of Engineering in",
      "authors": [
        "M Quazi"
      ],
      "year": "2012",
      "venue": "Human Emotion Recognition Using Smart Sensors A Thesis submitted in fulfilment of the Master of Engineering in"
    },
    {
      "citation_id": "4",
      "title": "Facial Expression Recognition: A Survey",
      "authors": [
        "J Kumari",
        "R Rajesh",
        "K Pooja"
      ],
      "year": "2015",
      "venue": "Procedia Comput. Sci",
      "doi": "10.1016/j.procs.2015.08.011"
    },
    {
      "citation_id": "5",
      "title": "Facial expression (mood) recognition from facial images using committee neural networks",
      "authors": [
        "S Kulkarni",
        "N Reddy",
        "S Hariharan"
      ],
      "year": "2009",
      "venue": "Biomed. Eng. Online",
      "doi": "10.1186/1475-925X-8-16"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition methods: A literature review",
      "authors": [
        "B Basharirad",
        "M Moradhaseli"
      ],
      "year": "2017",
      "venue": "AIP Conf. Proc",
      "doi": "10.1063/1.5005438"
    },
    {
      "citation_id": "7",
      "title": "Smart environment architecture for emotion detection and regulation",
      "authors": [
        "A Fernández-Caballero"
      ],
      "year": "2016",
      "venue": "J. Biomed. Inform",
      "doi": "10.1016/j.jbi.2016.09.015"
    },
    {
      "citation_id": "8",
      "title": "Ambient Intelligence : A Survey",
      "authors": [
        "A Intelligence"
      ],
      "year": "2011",
      "venue": "Ambient Intelligence : A Survey",
      "doi": "10.1145/1978802.1978815"
    },
    {
      "citation_id": "9",
      "title": "Emotion Based Music System",
      "authors": [
        "M Rabashette"
      ],
      "year": "2016",
      "venue": "Int. J. Emerg. Trends Sci. Technol",
      "doi": "10.18535/ijetst/v3i05.12"
    },
    {
      "citation_id": "10",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/ICASSP.2003.1202279"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "M Jain"
      ],
      "year": "2014",
      "venue": "arXiv",
      "doi": "10.5120/431-636"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommun. Syst",
      "doi": "10.1007/s11235-011-9624-z"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using convolutional and Recurrent Neural Networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "doi": "10.1109/APSIPA.2016.7820699"
    },
    {
      "citation_id": "14",
      "title": "Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional LSTM modeling",
      "authors": [
        "M Wöllmer",
        "A Metallinou",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. 11th Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH 2010, no. September"
    },
    {
      "citation_id": "15",
      "title": "Touchy Feely : An Emotion Recognition Challenge",
      "authors": [
        "D Amin",
        "K Sinha"
      ],
      "venue": "Touchy Feely : An Emotion Recognition Challenge"
    },
    {
      "citation_id": "16",
      "title": "Recognizing Facial Expressions Using Deep Learning",
      "authors": [
        "A Savoiu",
        "J Wong"
      ],
      "year": "2017",
      "venue": "Recognizing Facial Expressions Using Deep Learning"
    },
    {
      "citation_id": "17",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2015",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2014.09.005"
    },
    {
      "citation_id": "18",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)"
    },
    {
      "citation_id": "19",
      "title": "Human vocal sentiment analysis",
      "authors": [
        "M Bao",
        "A Huang"
      ],
      "year": "2019",
      "venue": "arXiv"
    },
    {
      "citation_id": "20",
      "title": "Ambient intelligence for health environments",
      "authors": [
        "J Bravo",
        "D Cook",
        "G Riva"
      ],
      "year": "2016",
      "venue": "J. Biomed. Inform",
      "doi": "10.1016/j.jbi.2016.10.009"
    },
    {
      "citation_id": "21",
      "title": "Smart music player integrating facial emotion recognition and music mood recommendation",
      "authors": [
        "S Gilda",
        "H Zafar",
        "C Soni",
        "K Waghurdekar"
      ],
      "year": "2017",
      "venue": "Proc. 2017 Int. Conf. Wirel. Commun. Signal Process. Networking, WiSPNET",
      "doi": "10.1109/WiSPNET.8299738"
    },
    {
      "citation_id": "22",
      "title": "EMOTION BASED MUSIC",
      "authors": [
        "S Bhutada",
        "T Iv"
      ],
      "year": "2020",
      "venue": "EMOTION BASED MUSIC"
    },
    {
      "citation_id": "23",
      "title": "AMAI: Adaptive music for affect improvement",
      "authors": [
        "D Su",
        "R Picard",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "ICMC 2018 -Proc. 2018 Int. Comput. Music Conf"
    },
    {
      "citation_id": "24",
      "title": "Facial expression recognition via a boosted deep belief network",
      "authors": [
        "P Liu",
        "S Han",
        "Z Meng",
        "Y Tong"
      ],
      "year": "2014",
      "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2014.233"
    },
    {
      "citation_id": "25",
      "title": "The Learning Healthcare System: Where are we now? A systematic review",
      "authors": [
        "A Budrionis",
        "J Bellika"
      ],
      "year": "2016",
      "venue": "J. Biomed. Inform",
      "doi": "10.1016/j.jbi.2016.09.018"
    },
    {
      "citation_id": "26",
      "title": "Combining recurrent and convolutional neural networks for relation classification",
      "authors": [
        "N Vu",
        "H Adel",
        "P Gupta",
        "H Schütze"
      ],
      "year": "2016",
      "venue": "NAACL HLT 2016 -Proc. Conf",
      "doi": "10.18653/v1/n16-1065"
    },
    {
      "citation_id": "27",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "3rd Int. Conf. Learn. Represent. ICLR 2015 -Conf. Track Proc"
    }
  ]
}