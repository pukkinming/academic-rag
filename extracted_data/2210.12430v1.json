{
  "paper_id": "2210.12430v1",
  "title": "Speech Emotion Recognition Via An Attentive Time-Frequency Neural Network",
  "published": "2022-10-22T12:18:26Z",
  "authors": [
    "Cheng Lu",
    "Wenming Zheng",
    "Hailun Lian",
    "Yuan Zong",
    "Chuangao Tang",
    "Sunan Li",
    "Yan Zhao"
  ],
  "keywords": [
    "speech emotion recognition",
    "time-frequency neural network",
    "attention mechanism",
    "spectrogram"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Spectrogram is commonly used as the input feature of deep neural networks to learn the high(er)-level time-frequency pattern of speech signal for speech emotion recognition (SER). Generally, different emotions correspond to specific energy activations both within frequency bands and time frames on spectrogram, which indicates the frequency and time domains are both essential to represent the emotion for SER. However, recent spectrogram-based works mainly focus on modeling the long-term dependency in time domain, leading to these methods encountering the following two issues: (1) neglecting to model the emotion-related correlations within frequency domain during the time-frequency joint learning; (2) ignoring to capture the specific frequency bands associated with emotions. To cope with the issues, we propose an attentive time-frequency neural network (ATFNN) for SER, including a time-frequency neural network (TFNN) and time-frequency attention. Specifically, aiming at the first issue, we design a TFNN with a frequency-domain encoder (F-Encoder) based on the Transformer encoder and a timedomain encoder (T-Encoder) based on the Bidirectional Long Short-Term Memory (Bi-LSTM). The F-Encoder and T-Encoder model the correlations within frequency bands and time frames, respectively, and they are embedded into a time-frequency joint learning strategy to obtain the time-frequency patterns for speech emotions. Moreover, to handle the second issue, we also adopt time-frequency attention with a frequency-attention network (F-Attention) and a time-attention network (T-Attention) to focus on the emotion-related frequency band ranges and time frame ranges, which can enhance the discriminability of speech emotion features. Extensive experimental results on three public emotional databases, i. e., IEMOCAP, ABC, and CASIA, show that the proposed ATFNN outperforms the state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH emotion recognition (SER) task is to make the machine automatically recognize the emotional state from the human speech. Recently, it has been a research hotspot in affective computing and pattern recognition  [1] ,  [2] ,  [3] ,  [4] ,  [5] . Generally speaking, the key to deal with the SER problem is to extract the discriminative and generalized features of emotional speech  [6] ,  [7] ,  [8] ,  [9] ,  [10] . These features mainly contain two categories at present: the hand-crafted features and the deep features.\n\nThe hand-crafted features are primarily adopted in the earlier SER works, known as the low-level descriptors (LLDs)  [11] ,  [12] , e. g., Frame Energy/Loudness, Fundamental Frequency, Zero/Mean-Crossing Rate, Mel-Frequency-Cepstral Coefficients (MFCC), etc. A mount of related works explored various LLDs or these combinations to improve the performance of the SER model. For instance, Schuller et al.  [11]  integrated 6 552 hand-crafted features into an opensource toolkit, i. e., openEAR, for four affective recognition tasks and then extended the openEAR to a multi-functional and convenient toolkit for diverse speech-related tasks, i .e., openSMILE  [12] . In the earlier works, these LLDs and their combinations were widely utilized as the benchmark features for SER  [6] ,  [7] .\n\nRecently, with the emergence of deep learning, handcrafted features are gradually replaced by deep features extracted by Deep Neural Networks (DNNs), e. g. Deep Convolutional Neural Networks (DCNNs)  [8] ,  [9] , Recurrent Neural Networks (RNNs)  [13] ,  [14] ,  [15] . Instead of LLDs, these deep features are high(er)-level features. In the extraction of deep features, generally, spectrogram features (e. g., MFCC, magnitude spectrogram, and Mel-spectrogram) with rich time-frequency information, are often used as input features of DNNs to learn the high(er)-level time-frequency patterns of speech signal for SER  [7] ,  [8] ,  [9] ,  [13] . Mao et al.  [8]  investigated to utilize the dimensionality-reduced magnitude spectrogram to learn the salient features by CNNs. Zhang et al.  [9]  extended the spectrogram to a 3-D Mel-Spectrogram to learn utterancelevel speech emotion features from segment features by a discriminant temporal pyramid matching strategy. Wang et al.  [13]  combined the MFCC features and Mel-spectrogram to input a special Long Short-Term Memory (LSTM), called Dual-Sequence LSTM, to extract the utterance-level emotion features.\n\nFurthermore, a time-frequency joint learning strategy is increasingly integrated into DNNs to extract more effective time-frequency representations of speech  [16] ,  [17] ,  [18] ,  [19] . Satt et al.  [18]  proposed a combined convolution-LSTM network to extract the utterance-level feature with segmentlevel features. Chen et al.  [19]  also adopt the 3-D attentionbased convolutional recurrent neural network  to represent the utterance level affective-salient features of emotional speech.\n\nNevertheless, the related works of the time-frequency feature extraction using spectrograms commonly encounter two issues at present. The first issue is that the existing works mainly focus on the correlations between speech frames to characterize the long-term dependency in time domain  [8] ,  [13] ,  [17] . There are also some works, however, which have revealed that the energy of the frequency band is changing in different emotions  [20] ,  [21] ,  [22] . In  [21] , Cowie et al. reported the variations of acoustic parameters associated with emotions, and the results of the acoustic spectral parameters indicated that each emotion usually corresponds to some energy activations of specific frequency band ranges on the spectrogram. For instance, the investigation in  [21]  reveals that happiness has increased in high-frequency energy, while sadness has decreased in high-frequency energy. These situations indicate that capturing the energy variations in different frequency ranges is also essential for the representation of speech emotions. Therefore, this paper will jointly model the correlations both within the frequency and time domains to obtain robust time-frequency representations of emotions.\n\nAnother issue faced by the time-frequency representation of speech emotion is that the distribution of emotion information in an utterance is sparse in both frequency domain and time domain  [19] ,  [23] . Specifically, it is intuitive that there are some non-speech frames without contents in a long utterance. Thus, not all frames in a sentence contain emotional information. In other words, the frames of an utterance have different contribution degrees in the representation of emotional speech, i. e., there are key frame regions highly associated with emotions. For the purpose of capturing these emotion-related regions, Chen et al.  [19]  investigated adding an attention layer after CNN and LSTM to capture specific temporal frames for the speech emotion representation. Also, Zhang et al.  [23]  adopted an attention mechanism to empower the sub-layer to focus on emotion salient regions of the spectrogram.\n\nHowever, these studies mainly embed attention into highlevel time-frequency features, leading to introduce the redundant information in the feature extraction and roughly locate the emotion-related regions in the time-frequency domain. In addition, they mainly focus on the selection on the time frames while ignoring the selection on the frequency bands. Actually, some studies have proven that each emotion does not have an energy activation on all frequency bands in the frequency domain  [21] ,  [22] ,  [20] . Therefore, we need to capture the key time frames in the time domain, and the key frequency bands in the frequency domain, which are all highly related to emotions.\n\nTo deal with the issues mentioned above, in this paper, we propose an Attentive Time-Frequency Neural Network (ATFNN) to learn the discriminative speech emotion feature for SER, as shown in Figure .1. In detail, we propose a timefrequency neural network (TFNN), including a frequencydomain encoder (F-Encoder) to model frequency features and a time-domain encoder (T-Encoder) to model time features, to capture the correlations within the frequency domain and within the time domain. Moreover, an attention mechanism with a frequency attention network (F-Attention) and a time attention network (T-Attention) is also embedded into TFNN to focus on the specific frequency bands and time frames highly corresponding to emotions.\n\nIn summary, the contributions of the paper mainly include the following three points:\n\n1) We propose a TFNN to jointly model the correlations both in the time and frequency domains, capturing more emotion information in the time-frequency representation of the speech signal. 2) We propose a time-frequency attention mechanism, measuring the contribution degree of frequency bands and time frames associated with different emotions, to capture the critical frequency bands and time frames. 3) By visualizing the attentions in the frequency and time domains, we demonstrate the correlations both within frequency bands and within time frames under different emotions on three public speech emotion databases, i. e., IEMOCAP, ABC, and CASIA. The rest of the paper is organized as follows. The proposed ATFNN method is described in detail in Section II. Section III describes the experimental settings and analyzes the experimental results. In the end, Section IV concludes the paper and discusses some future works.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Atfnn For Ser",
      "text": "Basically, the ATFNN, shown in Figure .1, is to embed the time-frequency attention strategy (i. e., F-Attention and T-Attention) into the TFNN. Thus, in this section, we firstly introduce the TFNN, then describe the ATFNN model in detail.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "A. Tfnn",
      "text": "The TFNN aims to learn the high(er)-level time-frequency representation from the input spectrogram feature for SER, which includes two modules, i. e., F-Encoder and T-Encoder.\n\n1) F-Encoder: The role of F-Encoder is to characterize the input spectrogram from the frequency domain, including the encoding of frequency information and the correlation modeling within frequency bands. Since different emotions correspond to the activations of specific frequency bands  [21] , the modeling in frequency domain can ignore the order relations between frequency bands. In the Transformer  [24] , the self-attention is adopted to calculate the correlations between tokens in Natural Language Processing (NLP). Inspired by this point, we utilize self-attention to aggregate the correlations across frequency bands into the frequency domain encoding. Thus, we propose an F-Encoder based on the revised encoder of the Transformer to learn the frequency domain representation of emotional speech, shown in Figure . 2, in which the position encoding module is abandoned to ignore the order relationship modeling between frequency bands. The proposed F-Encoder is introduced in detail below.\n\nGiven the input log-Mel-spectrogram feature of emotional speech as x ∈ R f ×t×1 , where t corresponds the frame number and f is the number of frequency bands onto the Mel-scale, the F-Encoder F e (•) transforms the input x to the frequency domain encoding x ∈ R f ×t×c , represented as x = F e (x). Specifically, the input x is firstly extended the channel information to x ∈ R f ×t×c by a linear layer. Then, the multi-head self-attention is utilized to calculate the correlations within frequency bands into frequency features x. After a series of convolution (with the kernel size is 1 × 5 and the channel number is 8) and add-norm operations (i. e., residual connection operations followed by layer normalization  [24] ), the frequency domain encoding x is obtained by integrating\n\nFig.  2 : Architecture of the F-Encoder, where the input x is firstly transformed to x by a linear layer. Then, after the multihead self-attention and two convolution layers (with the kernel size is 1 × 5 and the channel number are 32 and 8) and two add-norm layers, the correlations within frequency bands are integrated into the frequency domain encoding x.\n\nthe learning of correlations within frequency bands into the time-frequency representation. In addition, it is noting that, f , t, and c in the paper are set to 80, 128, and 8, respectively.\n\n2",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": ") T-Encoder:",
      "text": "The T-Encoder needs to capture the longterm dependence in time frames of speech for the representation of emotional information. RNNs, especially LSTM, have demonstrated their powerful capabilities in modeling the long-range dependence of the time series signal, and have been widely used in speech signal processing  [13] ,  [14] ,  [15] . Therefore, we adopt a T-Encoder based on a revised Bidirectional LSTM (Bi-LSTM)  [13] ,  [15] .\n\nThe proposed T-Encoder T e (•) takes the i th frame xi ∈ R f ×c of the frequency encoding x as the input of each time step in Bi-LSTM to learn the time-frequency representation h of emotional speech. In detail, the time domain encoding process can be defined as\n\n, and h i is the i th time-step output of the T-Encoder. Notably, since the T-Encoder is based on three-layer Bi-LSTM with 128 hidden nodes in the paper, the final time-frequency feature h ∈ R 1×256 is obtained by concatenating the last hidden states in the forward direction -→ h t ∈ R 1×128 and the last hidden states in reversed direction ← -",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3) Tfnn:",
      "text": "The TFNN integrates F-Encoder and T-Encoder into time-frequency joint learning strategy to ensure that the emotion patterns of speech are captured in both frequency domain and time domain. Specifically, the input log-Melspectrogram feature x is firstly encoded in frequency bands by F-Encoder to obtain frequency domain encoding x, represented as x = F e (x). Then, x is used as the input of T-Encoder to capture the long-term dependence between time frames to obtain the time domain encoding h through a timefrequency joint learning strategy, which can be formalized as h = T e (F e (x)).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Atfnn",
      "text": "In the ATFNN, two attention networks, i. e., F-Attention and T-Attention, are employed in the TFNN to focus on the emotion-related regions in the frequency and time domains.\n\n1) F-Attention: F-Attention aims to capture salient frequency bands contributing to different emotions, which is implemented by a convolution based block with the kernel size (1 × 5 × c), shown in Figure . 3. To calculate the frequency attention, we firstly represent the input log-Mel-spectrogram x = {x i } t i=1 as a set of x i ∈ R f ×c , where i ∈ [1, 2, ..., t] is the frame index and x i represents the frequency domain feature of i th frame on x. Since the frequency bands related to emotions are within a certain range, e. g., high frequency, intermediate frequency, and low frequency  [18] ,  [21] , we employ 5 frequency bands of x i as a band group for the convolution to preserve the correlations within frequency bands. Thus, after the convolution operation, the frequency attention weights a f i ∈ R 1×(f /5)×c of different frequency band groups can be obtained, expressed as a f i = F a (x i ). Moreover, as shown in Figure . 3, we utilize the frequency attention a f i to perform a weighted average operation on x i with respect to the channel c to produce the attention-based frequency features, then integrate these frequency features into the origin input features x i by the element-wise addition operation to produce the enhanced frequency feature x i , which can be represented as\n\nwhere a f i,m ∈ R 1×(f /5) represents the attention weight of the m th channel on the frequency attention weight a f i , and m ∈ [1, 2, ..., c]. Moreover, ⊗ and ⊕ represent the broadcasting element-wise multiplication and element-wise addition, respectively. It is noting that, in ⊗ of F-Attention, each 5 frequency bands of x i share the same attention weights for duplication.\n\nAfter that, we embeds the F-Attention into the F-Encoder, called as the attentive frequency neural network (AFNN), to produce the enhanced frequency encoding x ∈ R f ×t×c . The AFNN can be defined as x i = F e (x i ), where x i ∈ R f ×c and the enhanced frequency encoding x ∈ R f ×t×c is a set of x i , written as x = {x i } t i=1 . 2) T-Attention: Similar to F-Attention, the T-Attention is also performed by a convolution based block with the kernel size (1 × 8 × c) to capture salient frames that contribute to emotions, shown in Figure . 4. To calculate the time domain attention, we firstly divide the input lo-Mel-spectrogram x into multiple x j in frequency domain, where j ∈ [1, 2, ..., f ] represents the j th frequency band of the input x and x j ∈ R t×c is the time domain feature of the j th frequency band on x, i. e, x = {x j } f j=1 . In T-Attention, 8 time frames of x j are employed as a frame group for the convolution to obtain the time attention weight a t j ∈ R 1×(t/8)×c , expressed as a t j = T a (x j ). Then, as shown in Figure . 4, we also employ the time attention a t j to perform a weighted average operation on x j with respect to the channel c to generate the attention-based time features, and integrate them into the enhance frequency encoding xj by the element-wise addition operation to obtain the enhanced time feature x j ∈ R t×c , which can be written as\n\nwhere a t j,n ∈ R 1×(t/8) represents the attention weight of the n th channel on the time attention weight a t j , n ∈ [1, 2, ..., c], and the enhanced time feature x ∈ R f ×t×c is a set of x j , i .e., x = {x j } f j=1 . In ⊗ of T-Attention, each 8 time frames of x j share the same attention weights for duplication. Since the T-Encoder is encoded according to the time frame, the ATFNN can also be expressed as\n\nis the enhance time encoding.\n\n3) ATFNN: Integrating F-Attention and T-Attention into F-Encoder and T-Encoder respectively, we extend TFNN to attention-based TFNN, i. e., ATFNN. With F-Attention, the ATFNN can fucus on the key frequency bands related to emotions to generate the enhanced frequency domain coding x , expressed as\n\nThen, the T-Attention is adopt to capture key time frames related to emotions to obtain a more discriminative time- frequency representation h t , which can be formalized as\n\nFinally, the final time-domain encoding h can be regarded as an enhanced time-frequency representation to input into the classifier for SER. Actually, the ATFNN can be regarded as the TFNN that embeds time-frequency attention into the process of time-frequency joint learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiments",
      "text": "In this section, we describe the used speech emotion databases, then analyze and discuss the experimental results on our proposed ATFNN.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Experimental Databases",
      "text": "Three public emotional speech databases, i. e., the Interactive Emotional Dyadic Motion Capture database (IEMOCAP)  [25] , the Airplane Behaviour Corpus (ABC)  [26] , and China Emotional Database (CASIA)  [27] ,  [28] , are adopted in our experiments to prove the effectiveness of the proposed ATFNN method.\n\nIEMOCAP is a multimodal database with video, speech, and text scripts, collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). It is recorded in dyadic sessions where 10 actors (5 females and 5 males) perform improvised or scripted scenarios to elicit several emotional expressions, i.e., angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted, and others. In the paper, the improvised sentences with 4 emotions (i.e., angry, happy, sad, and neutral) are all used to perform the experiments according to  [18] , which has 2280 speech samples.\n\nABC is an audiovisual emotion database with the German collected for the particular target application of public transport surveillance. Also, it has 430 speech samples generated by 8 subjects (4 females and 4 males) in gender balance. They were induced to express one of 6 emotions, i.e., aggressive, cheerful, intoxicated, nervous, neutral, and tired, by the given scripts.\n\nCASIA is a Chinese speech emotion database released by the Institute of Automation of Chinese Academy of Sciences. It totally consists of 9,600 wave files with 6 emotional states, i. e., angry, fear, happy, neutral, sad, and surprise. Notably, in the paper, we adopt 1200 utterances of ABC released publicly for the experiment section. 4 volunteers with 2 males and 2 females are required to simulate these 6 emotions to produce 300 utterances for each emotion. To describe the selected databases clearly, we also list the detailed parameters of three databases in Table . I.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Experimental Protocol",
      "text": "To evaluate the performance of the proposed method, we adopt Leave-One-Speaker-Out (LOSO) cross-validation protocol for three selected databases according to  [6] ,  [7] . Specifically, as shown in Table . I, the IEMOCAP, ABC, and CASIA databases consist of 10 speakers, 8 speakers, and 4 speakers, respectively. Therefore, in our experiments, the speech samples of one speaker are utilized as the testing data, while the samples of other speakers are used as the training data. It is noting that, since the IEMOCAP includes 5 sessions on 10 speakers, the leave-one-session-out strategy is widely adopt in IEMOCAP database according to  [18] , where four sessions (8 speakers) are for training, and one session (2 speakers) is for testing. Therefore, we also select several state-of-theart methods based on this protocol for comparison  [18] ,  [29] ,  [23] ,  [30] , shown in Figure . II.\n\nBesides, we also utilize two evaluation metrics  [6] ,  [7] , i. e., the weighted average recall (WAR) and the unweighted average recall (UAR), to effectively measure the performance of the proposed method, which are commonly adopted in SER task. WAR is known as the 'normal' recognition actuary, while UAR reflects the class-wise recognition accuracy defined as the recall per class divided by the number of classes. Because the selected IEMOCAP and ABC databases are both class- imbalanced in our experiments, the WAR and UAR can better measure the performance of comparison methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Experimental Setting",
      "text": "To perform experiments conveniently, we make the preprocessing for the speech samples. In detail, following the works in  [9] ,  [8] ,  [18] , speech utterances are all divided into small segments with 128 frames (i. e., 20ms), which can not only preserve the completeness of speech emotion but also augment the dataset. Then, the log-Mel-spectrogram is extracted by the Short-Time Fourier Transform (STFT), in which 20 ms Hamming window size with 50% frame overlapping is adopted and 512-point FFT is used on each frame. Besides, the number of Mel-filter bands is set as 80.\n\nThe proposed ATFNN is implemented by the deep learning framework of Pytorch with NVIDIA GeForce RTX3090 GPUs, which is trained from scratch with the batch size of 128 and optimized by the Adam Optimizer with the initialized learning rate of 0.0005.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Results And Analysis 1) Results On Iemocap:",
      "text": "To compare the proposed ATFNN model with other methods, we select several state-ofthe-art works on the IEMOCAP database, i. e., DNN-HMM using an alignment generated from the subspace based Gaussian Mixture Model based HMMs (DNN-HMM SGMM-Ali.)  [31] , 3 convolution layers and LSTM with 10Hz grid resolution (CNN+LSTM Model)  [18] , GRU layer upon CNN layers with sequential Capsules (CNN GRU-SeqCap)  [29] , attention based fully convolutional network (FCN+Attention Model)  [23] , fusion model of acoustic and linguistic features (Model-3 fusion)  [30] , and an adaptive domain-aware representation learning based model (ADARL)  [32] .\n\nThe experimental results on the IEMOCAP database are shown in the Table. II. From these results, it is obvious that our proposed ATFNN achieves the state-of-the-art performance. Specifically, our ATFNN obtains the best result on WAR (73.81%) than all comparison methods, and it also obtains suboptimal results on UAR (64.48%) than ADARL (65.86%)  [32] . It is worth noting that, since ADARL adds a domain adaptation strategy to eliminate the discrepancy between speakers of the training data and the testing data, it obtains better performance than ATFNN on UAR. Nevertheless, ADARL depends on the hypothesis that the distribution between training and testing data is within a certain upper error bound, such that its universality is worse than our ATFNN.\n\nBesides, the confusion matrix of ATFNN reported in Figure . \n5(a) also demonstrates the performance of our method in detail, which reveals that ATFNN achieved high recognition accuracies on three emotions, i. e., angry, neutral, and sad, while obtains poor performance on happy. It is because IEMOCAP is an extremely class imbalance database shown in Table . I, where happy has the smallest number of samples while neutral has the largest number of samples. Therefore, this situation may affect the trained classifier to make it easily recognize happy as neutral.\n\n2) Results on ABC: For the comparison purpose, we also select several public works on the ABC database, i. e., LLDs with HMM/GMM (LLDs+HMM/GMM)  [6] , random anchor points generalized spectral regression with locally penalized discriminant analysis (RGSR-LPDA)  [33] , LLDs with SVM (LLDs+SVM)  [6] , and generalized discriminant analysis (GerDA) based on DNNs  [7] .\n\nThe results on the ABC database, as shown in Table . II, also demonstrate that our proposed ATFNN is superior both on WAR (68.84%) and UAR (57.57%) to other comparison methods not only on traditional methods (i. e., LLDs+HMM/GMM, RGSR-LPDA, and LLDs with SVM), but also on deep learning based methods (i. e., GerDA), which proves that the ATFNN can extract more discriminative and instance-adaptive representation of speech emotion on the ABC database. Moreover, we also report the confusion matrix on the ABC database, shown in Figure . 5(b). It is clear that the ATFNN have high recognition accuracies on four emotions, i. e., aggressive, cheerful, nervous, and neutral, while have poor performance on two emotions, i. e., intoxicate and tired. Specifically, on the ABC, the recognition of intoxicate is easily confusing with cheerful and neutral, whereas tired is more recognized as nervous and neutral. The cause of these results on ABC is similar to reasons for IEMOCAP, that is, the numbers of speech samples on intoxicate and tired are obviously smaller than other emotions, leading to influence the trained classifier. In addition, another reason may be that the arousal and valance of intoxicate and cheerful are all close, resulting in the confusing recognition.\n\n3) Results on CASIA: For the fair comparison on the CASIA, four sate-of-the-art methods are chosen in the experiments, including LLDs with a dimension reduction combining PCA and LDA (LLD+DR)  [34] , DNNs with the extreme learning machine (DNN ELM)  [35] , weighted spectral feature learning based on local Hu moment (HuWSF)  [36] , and deep convolutional neural network with discriminant temporal pyramid matching (DTPM)  [9] .  The experimental results on the CASIA are reported in the Table . II, which reveal that our proposed ATFNN achieves the highest recognition accuracies than other comparison methods in terms of both WAR (48.75%) and UAR (48.75%). In detail, deep learning based methods, i. e., DNN ELM and DTPM, outperform the LLDs based traditional method (LLD+DR), while our ATFNN is significantly superior than the current sate-of-the-art methods. It is noted that since the number of samples used in each emotion of the CASIA in the paper is equal, the WAR in the experimental results is equal to UAR. Furthermore, the confusion matrix of ATFNN on the CASIA is also calculated to verify the detailed performance of our proposed method, which is given in Figure . 5(c). From these results, we can observe that our ATFNN achieve the high performance on four emotions, i. e., angry, happy, neutral, and sad, whereas has a certain confusion in the recognition of other two emotions, i. e., fear and surprise. Specifically, fear is easier to confuse with sad, while surprise is always confused with angry and happy. The reason may be that fear and sad are relatively close in valence and arousal, causing two emotions to be induced by each other sometimes, while surprise, angry, and happy are the high arousal emotions, which may affect the recognition of each other slightly.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4) Experimental Result",
      "text": "Discussion: Throughout all experiments on IEMOCAP, ABC, and CASIA, some results are also worthy of discussing, from which we can investigate the applicability and limitation of our proposed ATFNN. Firstly, two selected databases, i.e., IEMOCAP, ABC, are class-imbalanced, resulting in the discrepancy between WAR and UAR. Moreover, the recognition of different emotions has obvious confusion, as shown in Figure . 5, where 58.27% of speech samples on happy are recognized as neutral in IEMOCAP, and 45.45% of samples on intoxicate in ABC are recognized as cheerful. These issues may be caused by the following reasons: one reason is that these emotions are relatively close in valence or arousal, and the other reason is that the long-tailed distribution of speech samples leads the classifier being biased towards emotions with more samples. Next, although the speech samples of CASIA are classbalanced, its recognition performance is much lower than that of IEMOCAP and ABC. This may be because CASIA is collected by forcing the speaker to express different emotions in the same sentence, limiting the diversity of speech samples. Also, CASIA is based on a tonal language (i. e., Chinese) rather than the non-tonal language (i. e., English). In addition, since the used LOSO protocol in our experiments is all based on speaker-independent, the results on three databases demonstrate that our ATFNN without any domain adaptation strategy not only preserves the discriminability of emotional speech representation but also eliminates the influence of unrelated information in the emotional speech, e. g., speakers, speaking styles.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "5) Ablation Experiment:",
      "text": "To verify the effectiveness of our proposed framework, the additional experiments are implemented for different architectures of ATFNN, and the results in terms of WAR and UAR are reported in Table . III, where AFNN and TFNN represent the FNN with F-Attention and the TNN with T-Attention respectively, described in Section II. From the ablation results, obviously, we can observe that the attention based time-frequency joint learning method (i. e., ATFNN) has better performance than the time-frequency joint learning method (i. e., TFNN), which indicates the proposed attention-based method can focus on the emotion-related parts in the time-frequency domain. In addition, we also observe that the time-frequency based method (i. e., ATFNN) also perform better than the time or frequency based method (i. e., AFNN or ATNN), which also shows that time-frequency joint learning can indeed obtain the discriminative representation of speech emotion. It is also interesting to see that, although the frequency domain representation is meaningful for emotions, the experimental results also show that ATNN performs better than AFNN.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Visualization Of F-Attention And T-Attention",
      "text": "To further investigate the correlations of the frequency band activations on frequency domain and key frame regions on time domain, we visualize the F-Attention and the T-Attention on the log-Mel-spectrogram, as shown in Figure . 6. For this purpose, high-arousal and positive-valence emotions, e. g., happy, cheerful, and low-arousal and negative-valence emotions, e. g., sad, tired, are selected as comparison for attention visualization  [37] ,  [38] . Among them, we choose happy and sad for a comparison on IEMOCAP and CASIA, whereas on cheerful and tired, which have close valence and arousal with happy and sad, are chosen as an alternative on ABC.   [20] ,  [21] ,  [22] . Notably, the results in Figure . 6(j) and 6(k) also reveal that the F-Attention of tired on ABC has frequent energy activations in the high-frequency bands from 10 th frame to 40 th frame and 125 th frame to 250 th frame. This is because the used speech sample has obvious sound of 'yawning' in these two frame intervals. On the contrary, the sample from 50 th frame to 80 th frame contains a conventional emotion expression of tired, thus its F-Attention reveals infrequent energy activations above 50 Mel-scaled frequency, which is consist with the above demonstration. Moreover, the visualization of F-Attention confirms that our proposed method can focus on the frequency band activations related emotions, and then combine the correlations between these frequency bands into the frequency domain representation.\n\nAs for T-Attention, the Figure.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In the paper, we propose an attentive time-frequency neural network (ATFNN) to extract the discriminative presentation for the SER. We first utilize a time-frequency neural network (TFNN), integrating a domain encoder and a time domain encoder into the time-frequency joint learning strategy, to model the correlations within frequency bands and within time frames. Then, a frequency attention network and a time attention network are also embedded into the TFNN to capture the specific frequency bands and time frames related to emotions. Experimental results on the two public databases, i. g., IEMOCAP and ABC, prove that our ATFNN outperforms the state-of-the-art methods. Furthermore, the visualization of attentions reveals that the ATFNN can focus on the regions related to emotions of the frequency domain and the time domain. Moreover, precise correspondence between different emotions and frequency bands should be deeply explored in future research to improve the robustness and discriminability of speech emotion features.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the Attentive Tim-Frequency Neural Network (ATFNN) framework for SER, including a time-frequency",
      "page": 2
    },
    {
      "caption": "Figure 2: Architecture of the F-Encoder, where the input x is",
      "page": 3
    },
    {
      "caption": "Figure 3: Pipeline of the F-Attention in ATFNN, in which the log-Mel-spectrogram x is adopted to calculate the frequency",
      "page": 4
    },
    {
      "caption": "Figure 4: Pipeline of the T-Attention in ATFNN, where we also use the log-Mel-spectrogram x to calculate the time attention",
      "page": 5
    },
    {
      "caption": "Figure 5: Confusion matrices of ATFNN on three selected speech emotion databases, i. e., IEMOCAP, ABC, and CASIA.",
      "page": 7
    },
    {
      "caption": "Figure 6: Visualization of log-Mel-spectrogram, T-Attention, and F-Attention under different emotions. (a)-(f) show the attentions",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out\n(LOSO)\n(5 Sessions or 10 Speakers)": "",
          "DNN-HMM SGMM-Ali.\n[31]": "CNN+LSTM Model\n[18]∗",
          "62.28": "68.80",
          "58.02": "59.40"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out\n(LOSO)\n(5 Sessions or 10 Speakers)": "",
          "DNN-HMM SGMM-Ali.\n[31]": "CNN GRU-SeqCap [29]∗",
          "62.28": "72.73",
          "58.02": "59.71"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out\n(LOSO)\n(5 Sessions or 10 Speakers)": "",
          "DNN-HMM SGMM-Ali.\n[31]": "FCN+Attention Model\n[23]∗",
          "62.28": "70.40",
          "58.02": "63.90"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out\n(LOSO)\n(5 Sessions or 10 Speakers)": "",
          "DNN-HMM SGMM-Ali.\n[31]": "Model-3 Fusion [30]∗",
          "62.28": "72.34",
          "58.02": "58.31"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out\n(LOSO)\n(5 Sessions or 10 Speakers)": "",
          "DNN-HMM SGMM-Ali.\n[31]": "ADARL [32]",
          "62.28": "73.02",
          "58.02": "65.86"
        },
        {
          "IEMOCAP": "",
          "Leave One Session/Speaker Out\n(LOSO)\n(5 Sessions or 10 Speakers)": "",
          "DNN-HMM SGMM-Ali.\n[31]": "ATFNN (ours)",
          "62.28": "73.81",
          "58.02": "64.48"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABC": "",
          "Leave One Speaker Out\n(LOSO)\n(8 Speakers)": "",
          "LLDs+HMM/GMM [6]": "RGSR-LPDA [33]∗∗",
          "57.70": "N/A",
          "48.80": "49.40"
        },
        {
          "ABC": "",
          "Leave One Speaker Out\n(LOSO)\n(8 Speakers)": "",
          "LLDs+HMM/GMM [6]": "LLDs+SVM [6]",
          "57.70": "61.40",
          "48.80": "55.50"
        },
        {
          "ABC": "",
          "Leave One Speaker Out\n(LOSO)\n(8 Speakers)": "",
          "LLDs+HMM/GMM [6]": "GerDA [7]",
          "57.70": "61.50",
          "48.80": "56.10"
        },
        {
          "ABC": "",
          "Leave One Speaker Out\n(LOSO)\n(8 Speakers)": "",
          "LLDs+HMM/GMM [6]": "ATFNN (ours)",
          "57.70": "68.84",
          "48.80": "57.57"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CASIA": "",
          "Leave One Speaker Out\n(LOSO)\n(4 Speakers)": "",
          "LLD+DR [34]": "DNN+ELM [35]",
          "39.50": "41.17"
        },
        {
          "CASIA": "",
          "Leave One Speaker Out\n(LOSO)\n(4 Speakers)": "",
          "LLD+DR [34]": "HuWSF [36]",
          "39.50": "43.50"
        },
        {
          "CASIA": "",
          "Leave One Speaker Out\n(LOSO)\n(4 Speakers)": "",
          "LLD+DR [34]": "DTPM [9]",
          "39.50": "45.42"
        },
        {
          "CASIA": "",
          "Leave One Speaker Out\n(LOSO)\n(4 Speakers)": "",
          "LLD+DR [34]": "ATFNN (ours)",
          "39.50": "48.75"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AFNN": "ATNN",
          "(cid:51)": "(cid:55)",
          "(cid:55)": "(cid:51)",
          "70.71": "71.76",
          "59.10": "61.90",
          "58.14": "66.05",
          "48.22": "55.01",
          "32.50": "46.42"
        },
        {
          "AFNN": "TFNN",
          "(cid:51)": "(cid:51)",
          "(cid:55)": "(cid:51)",
          "70.71": "72.99",
          "59.10": "63.60",
          "58.14": "67.21",
          "48.22": "55.99",
          "32.50": "46.67"
        },
        {
          "AFNN": "ATFNN",
          "(cid:51)": "(cid:51)",
          "(cid:55)": "(cid:51)",
          "70.71": "73.81",
          "59.10": "64.48",
          "58.14": "68.84",
          "48.22": "57.57",
          "32.50": "48.75"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "2",
      "title": "Social emotions in nature and artifact: emotions in human and humancomputer interaction",
      "authors": [
        "C Busso",
        "M Bulut",
        "S Narayanan",
        "J Gratch",
        "S Marsella"
      ],
      "year": "2013",
      "venue": "Social emotions in nature and artifact: emotions in human and humancomputer interaction"
    },
    {
      "citation_id": "3",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu",
        "E Morais",
        "R Hoory"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Learning affective representations based on magnitude and dynamic relative phase information for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "J Dang",
        "E Chng",
        "S Nakagawa"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "5",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "G Rigoll",
        "A Wendemuth"
      ],
      "year": "2009",
      "venue": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "7",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "8",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Openear-introducing the munich open-source emotion and affect recognition toolkit",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2009",
      "venue": "2009 3rd international conference on affective computing and intelligent interaction and workshops"
    },
    {
      "citation_id": "12",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Convolutional, long short-term memory, fully connected deep neural networks",
      "authors": [
        "T Sainath",
        "O Vinyals",
        "A Senior",
        "H Sak"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Lstm time and frequency recurrence for automatic speech recognition",
      "authors": [
        "J Li",
        "A Mohamed",
        "G Zweig",
        "Y Gong"
      ],
      "year": "2015",
      "venue": "2015 IEEE workshop on automatic speech recognition and understanding"
    },
    {
      "citation_id": "18",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "19",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "20",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "22",
      "title": "Communication of affects by single vowels",
      "authors": [
        "L Kaiser"
      ],
      "year": "1962",
      "venue": "Synthese"
    },
    {
      "citation_id": "23",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "24",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Audiovisual behavior modeling by combined feature spaces",
      "authors": [
        "B Schuller",
        "D Arsic",
        "G Rigoll",
        "M Wimmer",
        "B Radig"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion verification using emotion variance modeling and discriminant scale-frequency maps",
      "authors": [
        "J.-C Wang",
        "Y.-H Chin",
        "B.-W Chen",
        "C.-H Lin",
        "C.-H Wu"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge 2008 workshop"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Deep encoded linguistic and acoustic cues for attention based end to end speech emotion recognition",
      "authors": [
        "S Bhosale",
        "R Chakraborty",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Adaptive domain-aware representation learning for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "D Huang"
      ],
      "year": "2020",
      "venue": "Adaptive domain-aware representation learning for speech emotion recognition"
    },
    {
      "citation_id": "33",
      "title": "Connecting subspace learning and extreme learning machine in speech emotion recognition",
      "authors": [
        "X Xu",
        "J Deng",
        "E Coutinho",
        "C Wu",
        "L Zhao",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "Y Mei",
        "J.-W Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "36",
      "title": "Weighted spectral features based on local hu moments for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen",
        "J Wang"
      ],
      "year": "2015",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "37",
      "title": "The emotion probe: Studies of motivation and attention",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "American psychologist"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "J Kim",
        "E André"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    }
  ]
}