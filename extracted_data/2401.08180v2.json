{
  "paper_id": "2401.08180v2",
  "title": "Control-Free And Efficient Integrated Photonic Neural Networks Via Hardware-Aware Training And Pruning",
  "published": "2024-01-16T07:53:49Z",
  "authors": [
    "Tengji Xu",
    "Weipeng Zhang",
    "Jiawei Zhang",
    "Zeyu Luo",
    "Qiarong Xiao",
    "Benshan Wang",
    "Mingcheng Luo",
    "Xingyuan Xu",
    "Bhavin J. Shastri",
    "Paul R. Prucnal",
    "Chaoran Huang"
  ],
  "keywords": [
    "Silicon Photonics",
    "Photonic neural network",
    "Analogue computing",
    "Hardware-aware training. MNIST MNIST CIFAR-10 Two-layer CNN LeNet-5 ResNet-18 Accuracy: 67.0% Accuracy: 10.9% Accuracy: 9.15% Accuracy: 95.0% Accuracy: 98.0% Accuracy: 80.6%"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Integrated photonic neural networks (PNNs) are at the forefront of AI computing, leveraging on light's unique properties, such as large bandwidth, low latency, and potentially low power consumption. Nevertheless, the integrated optical components within PNNs are inherently sensitive to external disturbances and thermal interference, which can detrimentally affect computing accuracy and reliability. Current solutions often use complicated control methods, resulting in",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Neural networks (NNs) are powerful tools with diverse applications, including image recognition, natural language processing, and autonomous driving  [1] . However, the ever-increasing task complexity widens the gap between the computational requirements and the processing power of traditional electronic processors. Photonic neural networks (PNNs) emerge as a promising frontier for AI computing by leveraging the unique properties of light, such as large bandwidth, low latency, and potentially low power consumption. PNNs show the potential of bridging the computing gap posed by traditional electronic processors . They have found applications in diverse computing and signal processing systems  [5, [23] [24] [25] .\n\nHowever, the accuracy of PNNs can be easily affected by various mechanisms, such as ambient disturbances, particularly those induced by thermal fluctuations, thermal crosstalk from microheaters, limited resolvable weight levels, and device degradation, particularly in phase change materials (PCMs)  [26] . The challenge is further intensified in large-scale PNNs  [27] [28] [29] [30] [31] . For example, we experimentally observe in this work that a small resonance drift caused by thermal fluctuations in a microring (MRR)based PNN by only tens of pm can cause an accuracy degradation from 99.0% to only 67.0% in a small two-layer convolutional neural network (CNN) performing Modified National Institute of Standards and Technology (MNIST) classification task  [32] . As the size of the PNN further increases, the degradation in accuracy due to resonance drift becomes even more pronounced--the accuracy of the PNN drops from the theoretical value of 83.6% to 9.15% when performing Canadian Institute for Advanced Research, 10 classes (CIFAR-10) classification task  [33] . For another example, PCMbased in-memory PNNs encounter challenges related to limited weight resolutions and reproducibility caused by factors including material degradation, stochasticity in the crystallization process, and fluctuations in experimental switching conditions  [26] . Consequently, the accuracy of a PNN with Sb 2 Se 3 -based phase shifter may reduce from 99.0% to 63.2% when the crystallization temperatures vary by 5 K (corresponding to 8 distinguishable weight levels). These detrimental effects can be addressed by using external electronic control circuits to provide dynamic stabilizations  [26, [29] [30] [31] [34] [35] [36] [37] [38] . However, such conventional methods incur additional hardware costs and increase system complexity, making it impractical to build large-scale PNNs comprising hundreds or even thousands of components.\n\nIn this work, we introduce a general approach to address all these challenges in the PNN by introducing a hardware-algorithm codesign approach. This approach enhances the robustness of PNNs while simultaneously reducing power consumption. These improvements are achieved without needing cumbersome control algorithms or introducing additional hardware complexity. Instead, we attain these benefits by intentionally training the PNN towards its noise-robust and energy-efficient region. Our approach shares similarities with \"neural network (NN) pruning\", a technique commonly used to optimize the energy and memory efficiency in software-based NNs. Pruning in software-based NNs is to remove parameters from the NN model by setting weights to zero. In contrast, our approach focuses on encouraging weights to move to the variance-insensitive region of the device's transfer function, thus improving the robustness of PNNs. Importantly, the variance-insensitive region coincides with the region requiring minimal tuning power for weight assignment. Therefore, our approach also leads to a substantial power consumption reduction. While pruning concepts have been independently proposed in MZI networks, their primary focus is on reducing power consumption without effectively addressing the issue of noise  [39] [40] [41] [42] .\n\nOur approach solves the problem that PNN requires highly complicated control hardware to achieve and maintain its computing accuracy. We demonstrate that our approach can achieve 4-bit weight precision improvement for a MRR-based PNN without using thermoelectric controller (TEC) or other dynamic weight control methods. Based on this, the experimental classification accuracy for a two-layer CNN conducting MNIST dataset classification reaches 95.0%, comparable to the theoretical value. In contrast, the accuracy without using our approach drops to only 67.0%, due to the resonance drift caused by ambient fluctuations. Moreover, this enhancement occurs alongside 10-fold tuning power consumption reduction. As the NN size expands, the improvement in classification accuracy and reduction in power consumption becomes more pronounced. Using it to tackle a more intricate classification task like CIFAR-10, our approach demonstrates its capability by elevating the accuracy from 9.15% to 80.6% with 160 times power reduction.\n\nWe further extend the validation to various architectures, including MRRs with single-end detection  [12, 43]  and crossbar arrays  [44] . Furthermore, our approach effectively solves the problem of limited accuracy in PCM-based PNNs  [8]  caused by the limited weight resolutions and reproducibility of PCMs. We have demonstrated the improvement of the classification accuracy in MNIST dataset classification from 63.2% to 98.5% without re-training the neural network, even in the presence of various PCM degradations.\n\nOur experiments and simulations show our approach is broadly applicable to influential PNN systems and effectively addresses accuracy reduction challenges caused by various issues, including ambient disturbances, thermal crosstalk, limited resolvable weight levels, and device degradation. Our work represents a pathway towards practical, energy-efficient, and noise-resilient large-scale PNN implementations.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Operation Principle",
      "text": "PNNs can be realized by high-speed optical devices that are densely interconnected by parallel waveguides. Synaptic weights in convolutional and fully connected layers can be mathematically transformed into matrix multiplication operations. The matrix can be achieved through either incoherent summations of light intensity or coherent summations resulting from light interference  [16] . Widely used incoherent approaches include micro-ring resonator (MRR) weight banks  [45, 46]  and crossbar arrays  [8] . To apply weights, partial signal transmission is tuned using a phase shifter and added by either a single-end detector or a balanced detector. Compared to coherent approaches such as Mach-Zehnder interferometer (MZI) networks, incoherent PNNs offer advantages in power-efficient tuning, straightforward weight assignment, and high computing density due to the small device footprint  [8, 12, 45] . However, the accuracy of weights may be affected by various mechanisms, including ambient disturbances, thermal crosstalk from microheaters, limited resolvable weight levels, and device degradations. Although these issues can be mitigated by retraining the PNNs or by locking weight states using control circuits, these approaches add significant hardware complexity.\n\nTo solve these problems, we propose training the weights to a variance-robust region within a PNN. This can be achieved by incorporating a regularization term into the loss function during training. The regularization term serves as a penalty function, encouraging the relocation of weights from variance-sensitive to noise-robust regions:\n\nwhere L is the defined loss function, f (•) is the cross-entropy loss, y is the NN output, ŷ is the target label, α is the regularization coefficient, ∂W ∂n is the slope of weight to the refractive index change caused by various detrimental mechanisms. Here ||A|| 2 denotes the L2 norm of a matrix A. W is the weight matrix of each NN layer. By inducing this regularization term, the weights are effectively pushed towards a noise-robust region during training  [47] . Moreover, for certain filter banks such as MRR weight banks, this noise-robust region aligns with areas requiring minimal tuning power and exhibiting the highest resilience to thermal fluctuations. Consequently, our approach offers the following advantages. Firstly, weights are shifted to a region less susceptible to noise and other detrimental mechanisms, resulting in substantially smaller weight errors. Secondly, it significantly reduces power consumption as it requires minimal tuning power. Additionally, by minimizing total tuning power, thermal crosstalk can be effectively mitigated. In the subsequent sections (section 3 and 4) of this paper, we will showcase the application of our approach to different integrated PNN systems. Specifically, in the upcoming section, we provide a comprehensive experimental investigation of MRRbased PNNs, as MRRs are particularly prone to resonance drift. In section 4, we further extend the validation to PCM-based PNNs, demonstrating our approach's ability to address device imperfection and degradation-induced accuracy reduction.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Mrr-Based Pnns",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mrr Weight Banks With Balanced Photodetection",
      "text": "In a MRR-based PNNs, weight synapses can be realized by MRR weight banks, as shown in Fig.  1(a-i ). The input matrix is represented by an array of analog signals, each modulated onto a laser with a distinct wavelength. The multiplied matrix is realized by an array of MRR weight banks, shown in Fig.  1 (a-ii). MRRs on each weight bank have a slight radius offset, leading to a distinct resonance wavelength that aligned with the input laser's wavelength. As shown in Fig.  1(b-i ) and (b-ii), by tuning MRR's resonance wavelength, one can precisely control the portion of light divided into the Drop and Through ports of the MRR, thereby realizing tunable weights. Thermal tuning is the most commonly used mechanism for controlling MRRs due to its easy implementation on Silicon Photonics (SiP) chips. The weighted input signals are subsequently detected by a balanced photodetector (BPD). It subtracts signals from the Through port and Drop port. This results in an effective weight value ranging from -1 to 1, shown in Fig.  1(b-iii ). MRR weight banks in other architectures, such as single-end detection and crossbar arrays  [12, 43, 44]  will be discussed in later sections.\n\nWe position the input lasers at the edge of each MRR's transmission spectrum, as shown in Fig.  1(b-i ). In this configuration, the initial weight is 1. At the spectral edge, the weight value of \"1\" is resilient to the resonance drift caused by the thermal fluctuations. After applying a current to the microheater embedded in the MRR, its resonant wavelength shifts towards a longer wavelength, resulting in a different weight value. Taking weight 0 as an example, weight 0 is realized by tuning the resonance such that the input optical signal is equally divided into the Through and Drop ports. Weight 0 is sensitive to thermal fluctuations, as the signal's wavelength is located at the steep region of the MRR's transfer fluctuations. Fig.  1 (b-ii), obtained from experiments, plots tuning power and weight error variance as functions of weight values. It is clear that weights near 1 require the least tuning power and are most robust to thermal fluctuations. In contrast, weights near 0 require large tuning power and are sensitive to thermal fluctuations. Furthermore, weight values also lead to different signal-tonoise ratios (SNRs) at the BPD. SNR as a function of various weights is plotted in Fig.  1 (b-iv), showing weight \"1\" can provide a much higher SNR as compared to weight \"0\" considering various noise in BPD (Details are derived in supplement document). Considering all these effects, we call the weights close to 1 as good region (the blue region in Fig.  1(b-ii) ). It provides advantages of nearly zero tuning power, the highest resilience to thermal fluctuations, and the largest SNRs, simultaneously. By contrast, weights near 0 is bad region (the red region in Fig.  1(b-ii )), as it is power-consuming, vulnerable to thermal fluctuations, and suffers the lowest SNR.\n\nIn a large-scale PNN, this performance contrast will become even more significant. An important observation through training a NN is, when we use the standard setting in backpropagation to train a NN, most weights are most likely to cluster around 0, the bad region of MRR-based PNN, as shown in Fig.  1(c-i ). Consequently, a slight resonance drift can detriment the accuracy in a PNN due to error accumulation unless dedicated wavelength-locking is applied to every MRR. However, the associated hardware complexity can be prohibitively high. To solve these problems, we relocate the weights from the bad region to the good region (i.e., weights approaching \"1\") by incorporating a regularization term into the loss function when training the NNs. In the MRR-based PNN with BPD, the loss function is specified from Eq. 1 to Eq. 2,\n\nTo achieve effective weight pruning, we use iterative fine-tuning, a strategy commonly used in software NN, to train the PNN. The strategy involves two steps. The system is first trained using the loss function without the regularization term. Next, the system is fine-tuned using a modified loss function, adding the regularization term. The second step only fine-tunes those weights that are close to zero while keeping other weights frozen  [48] . This process goes layer by layer. This selective retraining is executed with continuous accuracy monitoring to ensure adherence to performance requirements. After iterative optimization, the majority of weights are moved to 1, the good region. Meanwhile, the computing accuracy stays comparable to the original NN without pruning  [49] .\n\nOur approach offers notable benefits. First, weights are relocated to the region that is resilient to resonance drift, leading to a much smaller weight error. Second, it leads to a significantly reduced power consumption as weights around 1 require almost zero tuning power. In addition, by reducing the total tuning power, thermal crosstalk could also be effectively mitigated. Finally, the overall SNR of the PNN system will have a substantial improvement. These benefits concurrently improve the accuracy and robustness of MRR-based PNN, without requiring cumbersome control of every MRR in the PNN.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Realization Of Control-Free And Robust Pnns",
      "text": "Our experiments primarily involve two key aspects, the first is to validate the effectiveness of our approach in reducing weight error. Next is to determine how our approach can significantly enhance NN inference classification accuracy. Here we assess the effectiveness of our approach across three distinct NNs of varying sizes and for different tasks, chosen according to a classic NN pruning work  [49] .\n\nOur experiment first validates the effectiveness of our approach in reducing weight error on a two-layer CNN for MNIST digits classification. The first layer of the CNN comprises a convolution layer with 4 convolution kernels, each with a size of 3×3. The output is activated by the ReLU nonlinear activation function, succeeded by a 2×2 max-pooling operation. The subsequent layer is a fully connected layer with a size of 676×10, mapping 10 categories of handwritten digits (0 to 9). We begin by training the CNN on a computer without incorporating a regularization term. This obtains 96.5% training accuracy on the MNIST test dataset comprising 10,000 pictures. The trained weights are subsequently implemented on a MRR weight bank, as illustrated in Fig.  2(a-i ). The spectrum of four MRRs is depicted in Fig.  2(a-ii ). The resonant wavelengths of four MRRs are situated at 1551.7, 1553.0, 1554.7, and 1555.8 nm, respectively. The MRR weight bank design and fabrication are detailed in the supplement document. The implementation process goes by flattening the weight matrix and then adding tuning currents to MRRs sequentially based on their tuning curves.\n\nDuring the implementation of weights, the output wavelengths of four lasers are initially adjusted to the off-resonance regions of four MRRs, at 1552.0, 1553.3, 1555.0, and 1556.1 nm, respectively. These lasers are modulated by signals generated by an arbitrary waveform generator (AWG) using 4 Mach-Zehnder modulators (MZMs). The modulated signals are combined by a wavelength division multiplexer (WDM) and then directed into the MRR weight bank through a grating coupler. Subsequently, the output signal, which is the photocurrent generated in BPD, goes into an off-chip biastee and transimpedance amplifier (TIA). Then, the signal is sampled and digitized by a real-time scope.\n\nBefore implementing the weights, we sweep the actuating current of each MRR to obtain a lookup table between the actuating current and weight. In the meanwhile, we record the power consumption for each weight, shown in Fig.  2(a-iii ). Due to thermal drift, the weights deviate from the target weights. The actual weights implemented on the four MRRs are obtained through the pseudo-inverse method  [30] . We define weight error as:\n\nwhere w true is the obtained weight value, w target is the target weight value. TEC is a power-consuming module. In the experiment, we do not use TEC to show our approach can realize control-free operation. We implement 6760 trained weights in the two-layer CNN experimentally without TEC (Details are illustrated in the supplement document). The initial weight mainly distributes around 0, as is shown in Fig.  2(b-i ). We measure the actual weight value using the pseudo-inverse method and calculate the error associated with each weight, generating a histogram represented in Fig.  2(b-ii ).\n\nFor weights without pruning, the mean value of weight error is -0.19, and the standard deviation of weight error is 0.12 for all weights, primarily due to a resonance drift at around 12 pm caused by thermal fluctuations (Details are illustrated in the supplement document). To mitigate the error, we re-train the CNN using the proposed pruning method, and implement the trained weights on the 4×1 MRR again. Weight distribution changes clearly during training, shown in Fig.  2(b-i ). Around 55.4% weights are pruned to 1. As shown in Fig.  2 (b-iii), after pruning, the weight error histogram converges around 0, the mean value increases to -0.01 and the standard deviation decreases to 0.02. The results are obtained without TEC under similar thermal drift conditions.\n\nIn our experimental setup, the primary cause of thermal drift stems from temperature fluctuations, because the duration of the experiment extends over several hours. As depicted in Fig.  2(c ), the 2D-relation figures illustrate the absolute error between neighboring MRR weights. They show a significant 4-bit enhancement in weight error precision without TEC (Details are illustrated in the supplement document). A strong linear correlation is evident among weight errors without pruning. It affirms that external temperature fluctuations play a more substantial role than thermal crosstalk. When the MRR weight bank's size is larger and thermal crosstalk significantly affects the weight error, we anticipate our method could also reduce the thermal crosstalk due to most weights being achievable without actuating the current.\n\nBased on the prior experiment, we have confirmed the significant enhancement of our approach in strengthening MRR weight bank's resilience to resonance drift. Building upon this, we conduct another experiment to evaluate the improved inference accuracy of CNN. The inference is executed layer by layer. For the convolution layer, the 'im2col' method is applied to transform the convolution operation into matrix multiplication  [50] . This allows us to decompose large matrix multiplications into small dot products implementable on the MRR weight bank. Subsequently, the results from the first layer are modulated onto light as the input for the second layer. A similar matrix multiplication decomposition is performed for the second layer, which is implemented on the MRR weight bank. Throughout this experiment, no external TEC is used. The inference results, showcased in Fig.  3(a-i)  and (b-i ), reveal that the accuracy without our approach is merely 67.0%. In contrast, after applying our approach, the experimental accuracy increases to 95.0%, close to the theoretical value. This substantial improvement is attributed to the marked reduction in weight error and thermal crosstalk and an enhancement in the SNR.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "Extension To Larger And Deeper Nns And Various Machine Learning Tasks",
      "text": "Moreover, we extend the application of our approach to larger and deeper NNs to validate its effectiveness with increased NN size. Initially, we utilize the LeNet-5 architecture with 60,000 weights for MNIST handwritten digits classification. The training dataset has 60,000 MNIST images, and the testing dataset has 10,000 images. Training the network without incorporating a regularization term yields a classification accuracy of 98.8%.\n\nTo add experimental error, we introduce weight error by randomly sampling from the histogram of experimental weight error. The error is reshaped to match the NN's size and added to the NN for dataset inference. The weight error leads to a drastic reduction in classification accuracy to 10.9%. After applying our approach, around 92.1% weights are pruned to 1. Weight error reduction contributes to an impressive improvement in the classification accuracy to 98.0%. Subsequently, we extend our method to a larger and deeper NN, ResNet-18, which comprises approximately 12 million weights. We employ this NN for a more challenging image classification task, the CIFAR-10 dataset which comprises 60,000 32×32 color images in 10 different classes. We select 50,000 images randomly for training and reserve 10,000 for testing. The trained accuracy without any experimental error is 82.7%. While it decreases to 9.15% even only introducing one-eighth of the experimental weight error. Our approach improves the classification accuracy to 80.6%, comparable to the theoretical accuracy, in the presence of one-eighth of the experimental weight error. Our experiments show the general applicability of our approach across various NN sizes and machine learning tasks.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Mrr Pruning",
      "text": "Without TEC",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Power Consumption Reduction",
      "text": "Finally, we evaluate the power consumption reduction achieved through our method.\n\nLarger NN allows more weights to be pruned to \"1\", thus leading to a more pronounced power consumption reduction. We denote the average MRR tuning power as the total tuning power over the number of weights. We find that differently sized NNs have similar average tuning power at 1.6mW. After applying our approach, the average tuning power is reduced by a factor of 10, 20, and 160 for three types of NNs, respectively, as shown in Fig.  4 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Application To Other Mrr-Based Pnns",
      "text": "Previous experimental results have demonstrated that our approach increases the robustness of PNN and enhances precision in MRR-based PNN with BPD. To further validate our method's applicability across various MRR-based PNNs, we conduct simulations on two distinct architectures. One architecture is an MRR array with single-end  detection, and the other is an MRR array with a crossbar structure, as depicted in Fig.  5 (a-i) and (b-i). In these two architectures, negative weights and positive weights are separated to perform matrix multiplication independently  [12, 43, 44] . Fig.  5 (a-ii) shows the Through port transmission tuning curve in the single-end detection MRR array. The tunable weight values range from 0 to 1. Through conventional training methods, these weights tend to concentrate around 0, corresponding to the region that is sensitive to noise and consumes more power. Similar to the MRR weight banks with BPD, the region weight value of \"1\" remains resilient to resonance drift caused by thermal fluctuations. Therefore, we prune the weights to \"1\" using the revised loss function Eq. 2.\n\nIn the crossbar MRR array, the weights range is also from 0 to 1. However, the signal outputs from the Drop port, therefore, the noise-robust region changes from weight value \"1\" to weight value \"0\" as depicted in Fig.  5(b-ii ). In this case, we slightly modify Eq. 1 into Eq. 4\n\nwhere ∂W ∂λ is the slope of MRR spectrum.\n\nWe employ the LeNet-5 architecture for classifying MNIST handwritten digits to evaluate the resilience of various MRR-based PNNs. In weight banks featuring singleend detection (Fig.  5 (a-iii)), we successfully prune the weights to '1'. Conversely, in MRR crossbar arrays, more weights are compressed to '0' compared to arrays without pruning, as illustrated in Fig.  5 (b-iii). In both cases, the computing precisions are improved with the additional benefit of power consumption.\n\nTo quantify the improvement, we emulate the resonance drift caused by thermal fluctuation and introduce sensitivity-related Gaussian noise ∆w noise ∼ N (0, | dw dλ * ∆λ| 2 ) to evaluate the robustness, where dw dλ represents the slope of the MRR spectrum, ∆λ is the estimated resonance drift. (Details are derived in the supplement document).\n\nOur simulation results show significant control precision improvement both in the MRR weight bank with single-end detection and the crossbar MRR array, as shown in Fig.  5 (a-iii) and (b-iii). In the MRR weight bank with single-end detection, the computing precision increases from 3.05 bits to 6.77 bits, boosting the MNIST classification accuracy from 74.2% to 97.4%, the average MRR tuning power is reduced by 15 times, as depicted in Table .1. The result is estimated assuming a 50 pm resonance drift. In contrast, the MRR crossbar array exhibits higher control precision without pruning compared to the MRR weight bank with single-end detection because weights have been centered around 0. With pruning, weights can get closer to 0, resulting in an improvement in computing precision from 4.59 bits to 6.79 bits. The enhancement in computing precision leads to improvement in classification accuracy from 74.2% to 97.4%. This result is estimated at a 150 pm resonance drift level.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Pcm-Based Pnns",
      "text": "PCM-based PNNs achieve tunable weights by thermally adjusting the proportion of crystalline and amorphous states in the PCM  [26] . In the crystalline state, most of the incoming light is absorbed, corresponding to a weight value of \"0\". In the amorphous state, most of the light is transmitted, representing a weight value of \"1\"  [8] . Therefore, weight tuning is achieved by heating the PCM beyond melting temperature T m and then rapidly quenching, followed by heating the PCM to various temperatures between crystallization temperature T c and melting temperature T m . In this way, PCM will be partially crystallized to a certain optical state; this corresponds to the specific weight value between 0 and 1, as shown in Fig.  6 (b)  [51] . However, imperfections, especially temperature fluctuations, will affect the crystallization fraction and introduce weight error. These imperfections also lead to limited resolvable weight levels and reproducibility caused by various factors, including material degradation, stochasticity in the crystallization process, and fluctuations in experimental switching  [26] . Take Sb 2 Se 3 as an example, Fig.  6(c ) shows crystallization fraction changes with quenching temperature for a heating duration of 1ms  [26] . The curve exhibits a sigmoid function, indicating the state changing from a pure crystalline state to an amorphous state. Our simulation results show that only 5 K thermal fluctuation, corresponding to 8 distinguishable levels, will reduce MNIST classification from 99.0% to 63.2% in the PCM crossbar array  [8] .  To solve this problem with our approach, we first analyze the stable region of the PCM-based PNNs. We can observe that both the regions of pure crystalline state and amorphous state are insensitive to temperature fluctuation. The crystallization fraction can also be linearly transformed into weight values. The relation between weight and quenching temperature is shown in Fig.  6(d ). We use the data in reference  [26]  to draw the specific numerical curve. We define T 0 as the intermediate temperature between T c and T m , and the difference between T c and T m is about 80 K.\n\nTo solve the inaccuracy problem of PCM, our approach is to relocate most weights to these two stable regions during training. We slightly modify Eq.1 into Eq.5, and optimize the NN with Eq.5.\n\nwhere ∂W ∂T is the slope of weight to the temperature change. This loss function forces most weights to relocate to the stable region where ∂W ∂T → 0. The weights are initialized with a uniform distribution of -1 to 1 to avoid only relocating weights to the weight 0 region. The weight distribution after pruning is shown in Fig.  6(e) . We can see most weights are successfully relocated to \"0\" and \"1\".\n\nTo simulate the impact of thermal fluctuations and assess the robustness of our approach, we add noises in weight ∆w noise ∼ N (0, | dw dT * ∆T 8 distinguishable weight levels. The simulation results (Fig.  6 (e) shows that after pruning, most weights are relocated to noise-robust regions, corresponding to the crystalline state and the amorphous state of PCM. The weight precision improves from 3.99 bits to 6.40 bits. The MNIST classification accuracy improves from 63.2% to 98.5%.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In summary, we propose and demonstrate control-free and energy-efficient photonic computing through hardware-aware training and pruning. This approach is helpful for solving accuracy problems in PNNs. Our method can boost control precision by 4 bit, leading to a significant increase in PNN inference accuracy. Furthermore, our approach concurrently reduces power consumption associated with tuning device states to match desired weights, all achieved without needing cumbersome control algorithms or introducing additional hardware complexity. Our work presents an important step towards practical, energy-efficient, and noise-resilient implementation of large-scale PNNs. Furthermore, our novel idea of training the parameters of a physical neural network towards its noise-robust and energy-efficient region can be broadly applied to other physical neural networks, addressing the noise issue commonly faced by such systems. We envision that future work can explore the integration of our training method with in-situ training  [15, [52] [53] [54] [55] [56]  to further enhance the robustness and efficiency of photonic computing. The limit of our approach is the computation cost when the PNN system becomes complex. For example, in MZI-based PNNs each element within the linear transmission matrix is simultaneously determined by several variables rather than independently determined by each variable. The Mote-Carlo method and perturbation method can be applied to simulate such systems and find the stable regions  [57, 58] .",
      "page_start": 15,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of conducting NN inference on MRR weight bank, MRR operation error perfor-",
      "page": 5
    },
    {
      "caption": "Figure 2: (a-i) 4×1 MRR weight bank perform dot product. Different modulated wavelength lights",
      "page": 10
    },
    {
      "caption": "Figure 3: Experiment inference confusion matrices under experiment error. (a-i) Two-layer CNN",
      "page": 11
    },
    {
      "caption": "Figure 4: Average MRR tuning power across different size neural networks. MRR: Micro-ring resonator.",
      "page": 12
    },
    {
      "caption": "Figure 5: (a-ii) shows the Through port transmission tuning curve in the single-end",
      "page": 12
    },
    {
      "caption": "Figure 5: (b-ii). In this case, we slightly",
      "page": 12
    },
    {
      "caption": "Figure 5: (a-iii)), we successfully prune the weights to ’1’. Conversely, in",
      "page": 13
    },
    {
      "caption": "Figure 5: (b-iii). In both cases, the computing precisions are",
      "page": 13
    },
    {
      "caption": "Figure 6: (b) [51]. However, imperfections,",
      "page": 13
    },
    {
      "caption": "Figure 5: Application to other integrated PNNs. (a-i) Diagram of Single-end detection MRR array.",
      "page": 14
    },
    {
      "caption": "Figure 6: PCM-based PNNs. (a) Diagram of PCM array. (b) Schematic diagram of PCM weight tuning.",
      "page": 15
    },
    {
      "caption": "Figure 6: (e) shows that after",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PDs": "",
          "…": "…"
        },
        {
          "PDs": "",
          "…": "…"
        },
        {
          "PDs": "",
          "…": "…"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "2",
      "title": "Deep learning with coherent nanophotonic circuits",
      "authors": [
        "Y Shen",
        "N Harris",
        "S Skirlo",
        "M Prabhu",
        "T Baehr-Jones",
        "M Hochberg",
        "X Sun",
        "S Zhao",
        "H Larochelle",
        "D Englund"
      ],
      "year": "2017",
      "venue": "Nature Photonics"
    },
    {
      "citation_id": "3",
      "title": "All-optical machine learning using diffractive deep neural networks",
      "authors": [
        "X Lin",
        "Y Rivenson",
        "N Yardimci",
        "M Veli",
        "Y Luo",
        "M Jarrahi",
        "A Ozcan"
      ],
      "year": "2018",
      "venue": "Science"
    },
    {
      "citation_id": "4",
      "title": "Large-scale optical neural networks based on photoelectric multiplication",
      "authors": [
        "R Hamerly",
        "L Bernstein",
        "A Sludds",
        "M Soljačić",
        "D Englund"
      ],
      "year": "2019",
      "venue": "Physical Review X"
    },
    {
      "citation_id": "5",
      "title": "A silicon photonic-electronic neural network for fibre nonlinearity compensation",
      "authors": [
        "C Huang",
        "S Fujisawa",
        "T Lima",
        "A Tait",
        "E Blow",
        "Y Tian",
        "S Bilodeau",
        "A Jha",
        "F Yaman",
        "H.-T Peng"
      ],
      "year": "2021",
      "venue": "Nature Electronics"
    },
    {
      "citation_id": "6",
      "title": "11 tops photonic convolutional accelerator for optical neural networks",
      "authors": [
        "X Xu",
        "M Tan",
        "B Corcoran",
        "J Wu",
        "A Boes",
        "T Nguyen",
        "S Chu",
        "B Little",
        "D Hicks",
        "R Morandotti"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "7",
      "title": "Optical coherent dot-product chip for sophisticated deep learning regression",
      "authors": [
        "S Xu",
        "J Wang",
        "H Shu",
        "Z Zhang",
        "S Yi",
        "B Bai",
        "X Wang",
        "J Liu",
        "W Zou"
      ],
      "year": "2021",
      "venue": "Light: Science & Applications"
    },
    {
      "citation_id": "8",
      "title": "Parallel convolutional processing using an integrated photonic tensor core",
      "authors": [
        "J Feldmann",
        "N Youngblood",
        "M Karpov",
        "H Gehring",
        "X Li",
        "M Stappers",
        "M Le Gallo",
        "X Fu",
        "A Lukashchuk",
        "A Raja"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "9",
      "title": "Space-efficient optical computing with an integrated chip diffractive neural network",
      "authors": [
        "H Zhu",
        "J Zou",
        "H Zhang",
        "Y Shi",
        "S Luo",
        "N Wang",
        "H Cai",
        "L Wan",
        "B Wang",
        "X Jiang"
      ],
      "year": "2022",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "10",
      "title": "Photonic machine learning with on-chip diffractive optics",
      "authors": [
        "T Fu",
        "Y Zang",
        "Y Huang",
        "Z Du",
        "H Huang",
        "C Hu",
        "M Chen",
        "S Yang",
        "H Chen"
      ],
      "year": "2023",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "11",
      "title": "An on-chip photonic deep neural network for image classification",
      "authors": [
        "F Ashtiani",
        "A Geers",
        "F Aflatouni"
      ],
      "year": "2022",
      "venue": "Nature"
    },
    {
      "citation_id": "12",
      "title": "All-optical spiking neurosynaptic networks with self-learning capabilities",
      "authors": [
        "J Feldmann",
        "N Youngblood",
        "C Wright",
        "H Bhaskaran",
        "W Pernice"
      ],
      "year": "2019",
      "venue": "Nature"
    },
    {
      "citation_id": "13",
      "title": "Image sensing with multilayer nonlinear optical neural networks",
      "authors": [
        "T Wang",
        "M Sohoni",
        "L Wright",
        "M Stein",
        "S.-Y Ma",
        "T Onodera",
        "M Anderson",
        "P Mcmahon"
      ],
      "year": "2023",
      "venue": "Nature Photonics"
    },
    {
      "citation_id": "14",
      "title": "Photonic matrix multiplication lights up photonic accelerator and beyond",
      "authors": [
        "H Zhou",
        "J Dong",
        "J Cheng",
        "W Dong",
        "C Huang",
        "Y Shen",
        "Q Zhang",
        "M Gu",
        "C Qian",
        "H Chen"
      ],
      "year": "2022",
      "venue": "Light: Science & Applications"
    },
    {
      "citation_id": "15",
      "title": "Silicon photonic architecture for training deep neural networks with direct feedback alignment",
      "authors": [
        "M Filipovich",
        "Z Guo",
        "M Al-Qadasi",
        "B Marquez",
        "H Morison",
        "V Sorger",
        "P Prucnal",
        "S Shekhar",
        "B Shastri"
      ],
      "year": "2022",
      "venue": "Optica"
    },
    {
      "citation_id": "16",
      "title": "Photonics for artificial intelligence and neuromorphic computing",
      "authors": [
        "B Shastri",
        "A Tait",
        "T Lima",
        "W Pernice",
        "H Bhaskaran",
        "C Wright",
        "P Prucnal"
      ],
      "year": "2021",
      "venue": "Nature Photonics"
    },
    {
      "citation_id": "17",
      "title": "Large-scale neuromorphic optoelectronic computing with a reconfigurable diffractive processing unit",
      "authors": [
        "T Zhou",
        "X Lin",
        "J Wu",
        "Y Chen",
        "H Xie",
        "Y Li",
        "J Fan",
        "H Wu",
        "L Fang",
        "Q Dai"
      ],
      "year": "2021",
      "venue": "Nature Photonics"
    },
    {
      "citation_id": "18",
      "title": "All-analog photoelectronic chip for high-speed vision tasks",
      "authors": [
        "Y Chen",
        "M Nazhamaiti",
        "H Xu",
        "Y Meng",
        "T Zhou",
        "G Li",
        "J Fan",
        "Q Wei",
        "J Wu",
        "F Qiao"
      ],
      "year": "2023",
      "venue": "Nature"
    },
    {
      "citation_id": "19",
      "title": "Photonic tensor cores for machine learning",
      "authors": [
        "M Miscuglio",
        "V Sorger"
      ],
      "year": "2020",
      "venue": "Applied Physics Reviews"
    },
    {
      "citation_id": "20",
      "title": "Electrical programmable multilevel nonvolatile photonic random-access memory",
      "authors": [
        "J Meng",
        "Y Gui",
        "B Nouri",
        "X Ma",
        "Y Zhang",
        "C.-C Popescu",
        "M Kang",
        "M Miscuglio",
        "N Peserico",
        "K Richardson"
      ],
      "year": "2023",
      "venue": "Electrical programmable multilevel nonvolatile photonic random-access memory"
    },
    {
      "citation_id": "21",
      "title": "Design and model of on-chip metalens for silicon photonics convolutional neural network",
      "authors": [
        "N Peserico",
        "H Yang",
        "X Ma",
        "S Li",
        "J George",
        "P Gupta",
        "C Wong",
        "V Sorger"
      ],
      "year": "2023",
      "venue": "Design and model of on-chip metalens for silicon photonics convolutional neural network"
    },
    {
      "citation_id": "22",
      "title": "Integrated optical memristors",
      "authors": [
        "N Youngblood",
        "C Ocampo",
        "W Pernice",
        "H Bhaskaran"
      ],
      "year": "2023",
      "venue": "Nature Photonics"
    },
    {
      "citation_id": "23",
      "title": "Multi-wavelength photonic neuromorphic computing for intra and inter-channel distortion compensations in wdm optical communication systems",
      "authors": [
        "B Wang",
        "T Lima",
        "B Shastri",
        "P Prucnal",
        "C Huang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics"
    },
    {
      "citation_id": "24",
      "title": "Broadband physical layer cognitive radio with an integrated photonic processor for blind source separation",
      "authors": [
        "W Zhang",
        "A Tait",
        "C Huang",
        "T Lima",
        "S Bilodeau",
        "E Blow",
        "A Jha",
        "B Shastri",
        "P Prucnal"
      ],
      "year": "2023",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "25",
      "title": "A system-on-chip microwave photonic processor solves dynamic rf interference in real time with picosecond latency",
      "authors": [
        "W Zhang",
        "J Lederman",
        "T Lima",
        "J Zhang",
        "S Bilodeau",
        "L Hudson",
        "A Tait",
        "B Shastri",
        "P Prucnal"
      ],
      "year": "2024",
      "venue": "A system-on-chip microwave photonic processor solves dynamic rf interference in real time with picosecond latency"
    },
    {
      "citation_id": "26",
      "title": "Performance limits of phase change integrated photonics",
      "authors": [
        "J Li",
        "Y Yun",
        "K Xu",
        "J Zhang",
        "H Lin",
        "Y Zhang",
        "J Hu",
        "T Gu"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics"
    },
    {
      "citation_id": "27",
      "title": "Multi-channel control for microring weight banks",
      "authors": [
        "A Tait",
        "T Lima",
        "M Nahmias",
        "B Shastri",
        "P Prucnal"
      ],
      "year": "2016",
      "venue": "Optics Express"
    },
    {
      "citation_id": "28",
      "title": "Feedback control for microring weight banks",
      "authors": [
        "A Tait",
        "H Jayatilleka",
        "T Lima",
        "P Ma",
        "M Nahmias",
        "B Shastri",
        "S Shekhar",
        "L Chrostowski",
        "P Prucnal"
      ],
      "year": "2018",
      "venue": "Optics Express"
    },
    {
      "citation_id": "29",
      "title": "Demonstration of scalable microring weight bank control for large-scale photonic integrated circuits",
      "authors": [
        "C Huang",
        "S Bilodeau",
        "T Lima",
        "A Tait",
        "P Ma",
        "E Blow",
        "A Jha",
        "H.-T Peng",
        "B Shastri",
        "P Prucnal"
      ],
      "year": "2020",
      "venue": "APL Photonics"
    },
    {
      "citation_id": "30",
      "title": "Silicon microring synapses enable photonic deep learning beyond 9-bit precision",
      "authors": [
        "W Zhang",
        "C Huang",
        "H.-T Peng",
        "S Bilodeau",
        "A Jha",
        "E Blow",
        "T Lima",
        "B Shastri",
        "P Prucnal"
      ],
      "year": "2022",
      "venue": "Silicon microring synapses enable photonic deep learning beyond 9-bit precision"
    },
    {
      "citation_id": "31",
      "title": "Self-calibrating microring synapse with dual-wavelength synchronization",
      "authors": [
        "J Cheng",
        "Z He",
        "Y Guo",
        "B Wu",
        "H Zhou",
        "T Chen",
        "Y Wu",
        "W Xu",
        "J Dong",
        "X Zhang"
      ],
      "year": "2023",
      "venue": "Photonics Research"
    },
    {
      "citation_id": "32",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "33",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "A Krizhevsky",
        "G Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "34",
      "title": "Wavelength locking and thermally stabilizing microring resonators using dithering signals",
      "authors": [
        "K Padmaraju",
        "D Logan",
        "T Shiraishi",
        "J Ackert",
        "A Knights",
        "K Bergman"
      ],
      "year": "2013",
      "venue": "Journal of Lightwave Technology"
    },
    {
      "citation_id": "35",
      "title": "Self-homodyne wavelength locking of a silicon microring resonator",
      "authors": [
        "Q Zhu",
        "C Qiu",
        "Y He",
        "Y Zhang",
        "Y Su"
      ],
      "year": "2019",
      "venue": "Optics Express"
    },
    {
      "citation_id": "36",
      "title": "Fast wavelength locking of a microring resonator",
      "authors": [
        "X Zhu",
        "K Padmaraju",
        "L.-W Luo",
        "S Yang",
        "M Glick",
        "R Dutt",
        "M Lipson",
        "K Bergman"
      ],
      "year": "2014",
      "venue": "IEEE Photonics Technology Letters"
    },
    {
      "citation_id": "37",
      "title": "Pwmdriven thermally tunable silicon microring resonators: design, fabrication, and characterization",
      "authors": [
        "P Pintus",
        "M Hofbauer",
        "C Manganelli",
        "M Fournier",
        "S Gundavarapu",
        "O Lemonnier",
        "F Gambini",
        "L Adelmini",
        "C Meinhart",
        "C Kopp"
      ],
      "year": "2019",
      "venue": "Laser & Photonics Reviews"
    },
    {
      "citation_id": "38",
      "title": "Towards adaptively tuned silicon microring resonators for optical networks-on-chip applications",
      "authors": [
        "Y Zhang",
        "Y Li",
        "S Feng",
        "A Poon"
      ],
      "year": "2014",
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics"
    },
    {
      "citation_id": "39",
      "title": "Bayesian photonic accelerators for energy efficient and noise robust neural processing",
      "authors": [
        "G Sarantoglou",
        "A Bogris",
        "C Mesaritakis",
        "S Theodoridis"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics"
    },
    {
      "citation_id": "40",
      "title": "Pruning coherent integrated photonic neural networks",
      "authors": [
        "S Banerjee",
        "M Nikdast",
        "S Pasricha",
        "K Chakrabarty"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics"
    },
    {
      "citation_id": "41",
      "title": "Heavy tails and pruning in programmable photonic circuits for universal unitaries",
      "authors": [
        "S Yu",
        "N Park"
      ],
      "year": "2023",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "42",
      "title": "Efficient on-chip learning for optical neural networks through power-aware sparse zeroth-order optimization",
      "authors": [
        "J Gu",
        "C Feng",
        "Z Zhao",
        "Z Ying",
        "R Chen",
        "D Pan"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Compact, efficient, and scalable nanobeam core for photonic matrix-vector multiplication",
      "authors": [
        "J Zhang",
        "B Wu",
        "J Cheng",
        "J Dong",
        "X Zhang"
      ],
      "year": "2024",
      "venue": "Optica"
    },
    {
      "citation_id": "44",
      "title": "Si microring resonator crossbar array for on-chip inference and training of the optical neural network",
      "authors": [
        "S Ohno",
        "R Tang",
        "K Toprasertpong",
        "S Takagi",
        "M Takenaka"
      ],
      "year": "2022",
      "venue": "ACS Photonics"
    },
    {
      "citation_id": "45",
      "title": "Microring weight banks",
      "authors": [
        "A Tait",
        "A Wu",
        "T Lima",
        "E Zhou",
        "B Shastri",
        "M Nahmias",
        "P Prucnal"
      ],
      "year": "2016",
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics"
    },
    {
      "citation_id": "46",
      "title": "Design automation of photonic resonator weights",
      "authors": [
        "T Lima",
        "E Doris",
        "S Bilodeau",
        "W Zhang",
        "A Jha",
        "H.-T Peng",
        "E Blow",
        "C Huang",
        "A Tait",
        "B Shastri"
      ],
      "year": "2022",
      "venue": "Nanophotonics"
    },
    {
      "citation_id": "47",
      "title": "Squeezelight: A multi-operand ring-based optical neural network with cross-layer scalability",
      "authors": [
        "J Gu",
        "C Feng",
        "H Zhu",
        "Z Zhao",
        "Z Ying",
        "M Liu",
        "R Chen",
        "D Pan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems"
    },
    {
      "citation_id": "48",
      "title": "Learning both weights and connections for efficient neural network",
      "authors": [
        "S Han",
        "J Pool",
        "J Tran",
        "W Dally"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "49",
      "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "authors": [
        "J Frankle",
        "M Carbin"
      ],
      "year": "2018",
      "venue": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "arxiv": "arXiv:1803.03635[cs.LG"
    },
    {
      "citation_id": "50",
      "title": "cudnn: Efficient primitives for deep learning",
      "authors": [
        "S Chetlur",
        "C Woolley",
        "P Vandermersch",
        "J Cohen",
        "J Tran",
        "B Catanzaro",
        "E Shelhamer"
      ],
      "year": "2014",
      "venue": "cudnn: Efficient primitives for deep learning",
      "arxiv": "arXiv:1410.0759[cs.NE"
    },
    {
      "citation_id": "51",
      "title": "Electrically programmable phase-change photonic memory for optical neural networks with nanoseconds in situ training capability",
      "authors": [
        "M Wei",
        "J Li",
        "Z Chen",
        "B Tang",
        "Z Jia",
        "P Zhang",
        "K Lei",
        "K Xu",
        "J Wu",
        "C Zhong"
      ],
      "year": "2023",
      "venue": "Advanced Photonics"
    },
    {
      "citation_id": "52",
      "title": "Deep physical neural networks trained with backpropagation",
      "authors": [
        "L Wright",
        "T Onodera",
        "M Stein",
        "T Wang",
        "D Schachter",
        "Z Hu",
        "P Mcmahon"
      ],
      "year": "2022",
      "venue": "Nature"
    },
    {
      "citation_id": "53",
      "title": "Experimentally realized in situ backpropagation for deep learning in photonic neural networks",
      "authors": [
        "S Pai",
        "Z Sun",
        "T Hughes",
        "T Park",
        "B Bartlett",
        "I Williamson",
        "M Minkov",
        "M Milanizadeh",
        "N Abebe",
        "F Morichetti"
      ],
      "year": "2023",
      "venue": "Science"
    },
    {
      "citation_id": "54",
      "title": "Multipurpose selfconfiguration of programmable photonic circuits",
      "authors": [
        "D Pérez-López",
        "A López",
        "P Dasmahapatra",
        "J Capmany"
      ],
      "year": "2020",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "55",
      "title": "Optical neural network via loose neuron array and functional learning",
      "authors": [
        "Y Huo",
        "H Bao",
        "Y Peng",
        "C Gao",
        "W Hua",
        "Q Yang",
        "H Li",
        "R Wang",
        "S.-E Yoon"
      ],
      "year": "2023",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "56",
      "title": "Artificial neural network training on an optical processor via direct feedback alignment",
      "authors": [
        "K Müller",
        "J Launay",
        "I Poli",
        "M Filipovich",
        "A Capelli",
        "D Hesslow",
        "I Carron",
        "L Daudet",
        "F Krzakala",
        "S Gigan"
      ],
      "year": "2023",
      "venue": "The European Conference on Lasers and Electro-Optics"
    },
    {
      "citation_id": "57",
      "title": "Large-scale methods for distributionally robust optimization",
      "authors": [
        "D Levy",
        "Y Carmon",
        "J Duchi",
        "A Sidford"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "Adversarial weight perturbation helps robust generalization",
      "authors": [
        "D Wu",
        "S.-T Xia",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}