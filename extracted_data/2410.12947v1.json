{
  "paper_id": "2410.12947v1",
  "title": "Multi-View Multi-Task Modeling With Speech Foundation Models For Speech Forensic Tasks",
  "published": "2024-10-16T18:34:06Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Devyani Koshal",
    "Swarup Ranjan Behera",
    "Arun Balaji Buduru",
    "Rajesh Sharma"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech forensic tasks (SFTs), such as automatic speaker recognition (ASR), speech emotion recognition (SER), gender recognition (GR), and age estimation (AE), find use in different security and biometric applications. Previous works have applied various techniques, with recent studies focusing on applying speech foundation models (SFMs) for improved performance. However, most prior efforts have centered on building individual models for each task separately, despite the inherent similarities among these tasks. This isolated approach results in higher computational resource requirements, increased costs, time consumption, and maintenance challenges. In this study, we address these challenges by employing a multitask learning strategy. Firstly, we explore the various state-of-the-art (SOTA) SFMs by extracting their representations for learning these SFTs and investigating their effectiveness at each task specifically. Secondly, we analyze the performance of the extracted representations on the SFTs in a multi-task learning framework. We observe a decline in performance when SFTs are modeled together compared to individual task-specific models, and as a remedy, we propose multi-view learning (MVL). Views are representations from different SFMs transformed into distinct abstract spaces by characteristics unique to each SFM. By leveraging MVL, we integrate these diverse representations to capture complementary information across tasks, enhancing the shared learning process. We introduce a new framework called TANGO (Task Alignment with INterview Gated Optimal Transport) to implement this approach. With TANGO, we achieve the topmost performance in comparison to individual SFM representations as well as baseline fusion techniques across benchmark datasets such as CREMA-D, emo-DB, and BAVED.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Forensic speech science is an indispensable discipline in criminal investigations and encompasses a wide range of applications, including security, user authentication, healthcare, customer service, and the entertainment industry, among others. Key tasks central to this field, such as Automatic Speaker Recognition (ASR), Speech Emotion Recognition (SER), Gender Recognition (GR), and Age Estimation (AE) -provide critical insights by analyzing paralinguistic features of speech, including pitch, intensity, tone, and other variations in speech. These tasks assist in identifying individuals, verifying claims based on vocal evidence, recognizing emotions for psychological assessments, enabling voice-based authentication, and facilitating demographic profiling for personalized services. As the prevalence of digital communication continues to surge, the imperative to extract intricate information from vocal data becomes increasingly pressing. This underscores the necessity for sophisticated models capable of concurrently addressing multiple forensic speech tasks.\n\nEarly research on speech forensic tasks (SFTs) primarily relied on handcrafted features, such as MFCCs, and classical ML techniques like SVMs and k-NN  (Abdulsatar et al., 2019) . Although foundational, these methods often struggled with scalability and generalization across varied speech conditions. Neural Networks such as CNN and LSTM significantly enhanced the capacity to capture the complex acoustic and temporal dynamics inherent in speech signals  (Zazo et al., 2018) . This transition marked a substantial improvement, particularly in high-variability environments.\n\nWith the dawn of this decade, speech foundation models (SFMs) have transformed speech forensic analysis. SFMs such as Wav2vec2, HuBERT, and WavLM generate robust, task-agnostic representations from raw speech, significantly improving performance across ASR, SER, GR, and AE  (Yang et al., 2021; Shor et al., 2022; Lebourdais et al., 2022) . By leveraging large-scale pre-training, these SFMs exploit large volumes of data, capturing intricate paralinguistic features essential for forensic applications. Despite advancements in SFMs, their application in learning multiple SFTs simultaneously in a multi-task format remains underexplored. This multi-task learning strategy provides an efficient solution for saving computational, time, monetary, and maintenance challenges. Few prolific works have explored the use of SFMs in this direction  (Zheng et al., 2022; wook Lee, 2023) , however, there is a lack of comprehensive studies evaluating the effectiveness of various such SOTA SFMs across different SFTs, with no clear consensus on the best-performing models within a unified framework.\n\nAdditionally, SFMs vary significantly in design and training paradigms; some, like Wav2vec2 and WavLM, are self-supervised and learn representations from unlabelled data, while others, such as Whisper, are trained using labeled datasets. This diversity presents an opportunity for research to identify the most effective models for multi-task speech forensic analysis and optimize their integration within a unified framework. In response to these challenges posed, we conduct a detailed investigation for the first time, to the best of our knowledge, to determine the optimal SFM for concurrent training on ASR, SER, GR, and AE. Our analysis reveals that while the multi-task framework promises improved efficiency, it often leads to diminished performance when tasks are integrated due to task interference, underscoring shortcomings in current methodologies. This interference can be due to each individual SFT being dependent on different paralinguistic aspects of the input speech and the failure of individual SFM to effectively disentangle task-specific information.\n\nTo address this issue, we propose a novel approach known as multi-view learning (MVL), which combines representations from various SFMs 1 and each SFM provides unique insights from their distinct abstract representations. This methodology facilitates the integration of diverse informational facets across tasks, thereby augmenting the comprehensive learning process and mitigating task interference. To our end, we intro-1 Here, each unique SFM representation is considered as view and used interchangeably with representation duce TANGO (Task Alignment with INter-view Gated Optimal Transport), a framework for effective MVL to synchronize these representations effectively. Our findings demonstrate that TANGO not only enhances performance compared to individual SFM outputs but also significantly surpasses baseline fusion methods on benchmark datasets, including CREMA-D, emo-DB, and BAVED.\n\nTo summarize, the contributions of the work are as follows:\n\n• We present a comprehensive comparative analysis of SOTA SFMs for learning these SFTs individually.\n\n• We perform an investigative analysis of SOTA SFMs for learning the FSTs ASR, SER, GR, and AE in a multi-task learning format.\n\n• Our findings highlight the performance tradeoffs of jointly modeling these tasks, revealing significant challenges in current multi-task learning approaches due to the poor information disentanglement for each task through individual SFM representations.\n\n• We introduce an MVL paradigm that integrates diverse representations from multiple SFMs, enhancing shared learning across tasks.\n\n• We propose TANGO for aligning multi-view representations, outperforming baseline fusion techniques and individual SFMs.\n\nThe models and code developed for this study will be released after the review process.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "In this section, we will briefly discuss previous research on SFTs modeled using SFMs. Automatic Speaker Recognition:  Shor et al. (2022)  proposed universal paralinguistic conformer as a representation learning model for paralinguistic speech processing and showing SOTA performance for ASR.  Peplinski et al. (2021)  leveraged TRILL to develop FRILL, which provides embeddings for paralinguistic applications in lowresource settings and demonstrates improved ASR performance compared to previous approaches. Speech Emotion Recognition: Chen and Rudnicky (2023) leveraged Wav2vec2 representations for SER followed by the use of WavLM by  Diatlova et al. (2024) . Building upon this, Chetia Phukan et al. (  2023 ) presented a unique view, where they showed SFM primarily trained for ASR provides better representations than other SFMs for SER. Gender Recognition:  Lebourdais et al. (2022)  used SOTA PTM wavLM representations for GR as a additional task together with overlapped speech detection. wook Lee (2023) has leveraged Hu-BERT and modeled GR as a auxiliary task together with language indetification for improved SER. Age Estimation:  Truong et al. (2022)  presented an investigative study into various self-supervised SFMs such as Wav2vec2, WavLM, etc for AE. Further,  Gupta et al. (2022)  proposed a bi-encoder based mixture of experts model coupled with Wav2vec2 for jointly modeling AE and height estimation.\n\nFrom these studies, we observe that initial efforts have been made to model SFTs simultaneously with SFMs; however, there has been no comprehensive study investigating various SOTA SFMs in this context. Therefore, in this study, we aim to address this gap.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Foundation Models",
      "text": "In this section, we briefly describe the SOTA SFMs utilized in our study. XLS-R  (Babu et al., 2022) : It is a multilingual representation learning model based on Wav2vec2 architecture, trained on 436k hours of speech data and 128 languages. We utilize the base version comprising 1 billion parameters 2 . Whisper  (Radford et al., 2023) : It is encoderdecoder model pre-trained on 680k hours of data using a multitask format in a weakly-supervised way. Whisper excels in speech recognition, outperforming XLS-R, and we employ the base version with 74 million parameters 3 . Wav2vec2  (Baevski et al., 2020)  Wav2vec2. We use the 1 billion parameters version 5  . Unispeech-SAT  (Chen et al., 2022b) : It is a contrastive loss model utilizing multitask learning with speaker-aware pre-training. It is pre-trained on 960 hours of Librispeech English data. We utilize the base version with 94.68 million parameters 6  . WavLM  (Chen et al., 2022a) : It is a SOTA SFM on SUPERB trained for general-purpose speech representation learning. We use the base version 7  of 94.70 million parameters.\n\nx-vector  (Snyder et al., 2018)     (Shor et al., 2022) .We use 63.4 million parameters version  9  .\n\nThe audio input is resampled to 16 kHz before being fed into the SFMs. We retrieve representations from the last hidden states of the frozen SFMs by average pooling. We extract representations of 512 (x-vector, Whisper), 768 (WavLM, Wav2vec2, Unispeech-SAT), 1024 (TRILLsson), and 1280 (MMS, XLS-R) -dimension respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Modeling",
      "text": "In this section, we present various modeling setups for carrying out the experiments in our work. Single View Single Task (SVST): In SVST, we build downstream models for the SFTs ASR, SER, GR, and AE as individual tasks using different SFM representations. The architecture is illustrated in Figure  1  (a). We use two convolution blocks consisting of 1-D CNN and max-pooling layers, followed by a fully connected network (FCN). For output layer, we use the number of neurons depending on the FST. Single View Multi Task (SVMT): We employ individual SFM representations to handle multiple FSTs simultaneously. By integrating task-specific heads after shared layers, this approach facilitates Multi View Multi Task (MVMT): It incorporates two SFMs, leveraging their diverse representations to address all four FSTs concurrently. Each view from the SFMs is passed through a network consisting of a convolutional block and FCN, which is the same as in SVST and SVMT. The fusion is performed using concatenation followed by a FCN block. The FCN block is followed by a taskspecific head. The MVMT architecture is presented in Figure  2  (a). The modeling hyperparameters and training details for SVST, SVMT, and MVMT can be found in Appendix 9.1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tango",
      "text": "In this subsection, we present, TANGO for effective MVL for learning multiple SFTs parallely. The modeling architecture diagram is shown in  Figure 2 (b) .TANGO also follows the same modeling as SVST, SVMT, and MVMT for each viewspecific network, i.e., consisting of two convolutional blocks. After the convolutional blocks, we use a gating mechanism f (x) that consists of a sigmoid function and a multiplication operation.\n\nHere, x is the input to the function, and σ(x) is the sigmoid function defined as 1 1+e -x . The sigmoid function squashes the input x into a range between 0 and 1. The resultant value is the scaled output of the original input. This gating mechanism selectively forwards relevant features and enhances taskspecific attribute detection. We then attach a fusion block (refer to Figure  2 (b) ) that leverages Optimal Transport (OT) or Earth's mover distance as the fusion mechanism. OT measures dissimilarity between views and has shown its effectiveness for multimodal fusion tasks  (Pramanick et al., 2022) . The fusion block will align different views by minimizing their distance during optimization with the OT plan computed using the Sinkhorn algorithm.\n\nThe transported features from two SFMs are represented as:\n\n(1)\n\nwhere x 1 , x 2 represents views from two different SFMs and γ is the OT plan derived from the feature distance matrix M :\n\nThese transported features are then concatenated with the original SFM-specific features to form enriched fused representations. Further, the resultant features are fused again with alternate SFM features through concatenation. The final concatenated features are then passed through an FCN, followed by a task-specific head. TANGO performs joint optimization with task-specific while optimal alignment of different views.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "5.1 Datasets CREMA-D  (Cao et al., 2014) : It is a comprehensive resource for ASR, SER, GR, and AE. It offers a rich variety of speaker data, representing diverse ethnicities and age groups, making it an ideal benchmark for training and evaluating ML models. The dataset contains recordings of 7442 words spoken by 91 distinct speakers, balanced by gender, in English. It spans six distinct emotions: anger, happiness, sadness, fear, disgust, and neutrality. The utterances are generated by 43 female and 48 male speakers, each articulating twelve sentences. Further details regarding the datasets gender and age distribution can be found in Appendix Figure  3 , 4 and 5.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Loss Function",
      "text": "The multi-task total loss can be expressed as:\n\nwhere: L total , L CE1 , L CE2 , L BCE , L RMSE represents total loss, cross-entropy (ASR), cross-entropy (SER), BCE (GR), RMSE (AE) respectively. λ 1 , λ 2 , λ 3 , λ 4 represents weight parameter for ASR, SER, GR, AE losses respectively. In our experiments for multi-task learning, we kept the weightage same as 0.33.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "We consider accuracy as an evaluation metric for ASR, SER, and GR; RMSE for AE following previous research  (Zheng et al., 2022; Shor et al., 2022) . All the models are evaluated in a 5-fold manner, and we present the average scores of 5fold. The greater the accuracy values and the lower the RMSE, the better the models are. Table  6  summarizes the results across different models and we present the only best scores for each setup for each SFT. The evaluation results and discussion of the various models are presented below, organized by specific tasks: SVST: Table  1 presents      SVMT: Table  2  presents the results of individual SFMs representations for multi-task learning of the SFTs. Here we can observe that the performance dropped than the models trained for individual SFT.\n\nTRILLsson is still holding the topmost position, however, we observe drop than SVST. This degradation in performance can be attributed to task interference. This interference arises because each SFT depends on distinct paralinguistic features in the input speech, and individual SFMs may struggle to disentangle task-specific information effectively. MVMT: Two modeling techniques are used for MVMT, first, fusion of the views through concatenation, which we consider as a baseline fusion technique. Secondly, fusion through, TANGO.\n\nTable  3 , 4, 5 presents the scores for MVMT results on CREMA-D, BAVED, and emo-DB for the SFTs ASR, SER, GR, and AE. Across the three datasets, we observe that fusion through TANGO gives better than concatenation-based baseline fusion in most instances and as well as than SVMT scenarios. These results show that MVL through TANGO from SFMs can mitigate task interference during multi-task learning of SFTs. Different SFM capture different aspects of the input speech signal due to different SFM's unique characteristics inherent to it, helping to disentangle task-specific information more effectively when combined, thus showing complementary behavior. This approach reduces interference by allowing each task to access the most relevant features from the appropriate view, leading to improved performance and better generalization across tasks. The combination of TRILLsson and x-vector with TANGO has shown the topmost performance in comparison to all the different combinations. As seen with the SVST re-sults (Table  1 ), better individual task SFMs, when combined, leverage their unique advantages and offer richer, more task-specific information, leading to superior overall performance.\n\nWe also observe an interesting phenomenon: the synergy between MFCC, despite being statistical handcrafted features, and SFMs like x-vector or TRILLsson outperforms combinations involving only MMS, UniSpeech-SAT, or other SFMs. This spectral features with complementary SFMs reduces redundancy and enhances noise resilience, leading to more effective representations. Overall, from these results, we can observe the effectiveness of TANGO for aligning views from different SFMs, thus inducing complementary strengths of the SFMs. Additionally, we also conducted an ablation study of TANGO.\n\nIn TANGO, we transport features from both the SFMs to each other and for ablation experiments, we only transport features from each SFM to other and vice versa. The evaluation results are shown in Appendix Table  7 , 8, 9 for CREMA-D, BAVED, emo-DB respectively, however, we observe one direction transportation is not able to outerperform TANGO.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, for the first time, we gave an comprehensive comparative investigation of various SOTA SFMs for learning SFT such as ASR, SER, GR, and AE simultaneously as training individuals models for each task comes with resource, time, cost, and maintenence challenges. We conducted our experiments on three benchmark datasets consisting of CREMA-D, BAVED, and emo-DB. We show that multitask learning of SFTs with individual SFMs representations is prone to task interference thus degration in performance across the tasks and as a remedy, we propose, MVL. MVL aims to exploit the complementary behavior of different SFMs unique abstract space for effective learning of the SFTs parallely. For better MVL, we propose, TANGO, a multi-task learning framework that leverages OT as fusion mechanism.\n\nThrough our experiments, we show that combination of TRILLsson and x-vector SFMs coupled with TANGO attains far better performance than the individual SFMs representations, and as well as the baseline fusion techniques for learning SFTs concurrenty.\n\nCREMA-D, BAVED, and emo-DB were the only easily openly available datasets that contain these four tasks' information, so we only experimented with them. We experimented with only CNN as the downstream network, and this may limit our study, as previous research has shown that the downstream behavior of the SFMs changes according to the downstream network chosen  (Zaiem et al., 2023) . So, we plan to extend our study, by experimenting with different downstream networks.\n\nOne more limitation is the selection of optimal values for the task-specific loss in the total multitask learning loss function. In this study, we have tried with only one combination, however, different values can give different results maybe better. so, in future, we plan to come up with learnable loss function, that learns the weights of the losses dynamically.    We keep the number of neurons as 200, 64, 56 for SVST setup. For SVMT, MVMT, TANGO, for the task-specific head we keep the number of neurons as 30. we use softmax in the output for the models for ASR, SER and GR tasks. We use linear activation function for AE. We use Adam as the optimizer and set the learning rate to 1e-3 with a batch size of 32.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sfms",
      "text": "",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a). We use two convolution blocks",
      "page": 3
    },
    {
      "caption": "Figure 1: Single view models: (a) single view single",
      "page": 4
    },
    {
      "caption": "Figure 2: (b)) that leverages Opti-",
      "page": 4
    },
    {
      "caption": "Figure 2: Multi-view models: (a) Multi View Multi Task with concatenation fusion and (b) TANGO. Here, X11 and",
      "page": 5
    },
    {
      "caption": "Figure 3: , 4 and 5.",
      "page": 5
    },
    {
      "caption": "Figure 6: and Figure 7) of raw SFM repre-",
      "page": 5
    },
    {
      "caption": "Figure 3: Age and Gender Distribution for CREMA-D.",
      "page": 12
    },
    {
      "caption": "Figure 4: Age and Gender Distribution for BAVED.",
      "page": 12
    },
    {
      "caption": "Figure 5: Age and Gender Distribution for emo-DB.",
      "page": 12
    },
    {
      "caption": "Figure 6: t-SNE plots.",
      "page": 13
    },
    {
      "caption": "Figure 7: t-SNE plots.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "90.93": "69.84",
          "80.15": "73.00",
          "99.57": "98.79",
          "5.81": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "90.19",
          "Column_2": "88.11",
          "100.00": "100.00",
          "Column_4": "2.34"
        },
        {
          "Column_1": "87.08",
          "Column_2": "82.17",
          "100.00": "99.22",
          "Column_4": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "95.33",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "99.07",
          "Column_4": ""
        },
        {
          "Column_1": "100.00",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "98.13\n80.37",
          "Column_2": "97.20",
          "100.00": "100.00",
          "Column_4": "1.74"
        },
        {
          "Column_1": "",
          "Column_2": "91.59",
          "100.00": "100.00",
          "Column_4": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "81.13": "58.43",
          "75.22": "63.26",
          "98.79": "97.68",
          "6.87": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "89.41": "83.72",
          "83.98": "72.09",
          "100.00": "98.71",
          "2.58": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "96.26": "88.31",
          "95.33": "82.71",
          "99.07": "96.27",
          "2.00": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.53\n94.39\n91.59\n98.13\n97.66\n97.66\n99.07\n96.73\n98.13\n93.93\n95.79\n99.07\n99.53\n93.46\n95.33\n97.66"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.06\n99.07\n99.07\n99.53"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.53"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.06"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.05"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "100.00": "97.16\n96.90\n97.93\n98.45\n97.16"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.74\n98.45\n99.74\n98.71\n99.74\n99.48"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "98.97\n99.48\n98.71\n98.97"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "98.19\n99.22\n99.48\n99.74\n99.48\n99.74"
        },
        {
          "100.00": "100.00"
        },
        {
          "100.00": "99.22\n99.74\n99.72\n99.74"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "98.13\n99.07\n99.07",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "97.20",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "97.20\n98.13",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "99.07",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "99.07",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "97.20\n99.07",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "99.07\n99.07",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "98.13",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "99.53",
          "Column_2": "100.00",
          "100.00": "100.00",
          "Column_4": "1.38"
        },
        {
          "Column_1": "96.26\n97.20",
          "Column_2": "95.33\n98.13",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "99.87\n98.97\n96.51\n98.45\n98.19\n99.09\n98.71\n99.87\n99.61\n99.74\n98.45\n98.71\n98.45\n99.87\n99.74\n99.48\n98.71\n99.22",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "97.67\n98.71\n99.74\n99.87\n99.74\n99.48",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "98.97",
          "Column_4": ""
        },
        {
          "Column_1": "91.99",
          "Column_2": "87.60",
          "100.00": "100.00",
          "Column_4": "1.44"
        },
        {
          "Column_1": "90.05\n91.08",
          "Column_2": "78.16\n83.98",
          "100.00": "99.74",
          "Column_4": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: , 4, 5 presents the scores for MVMT re- SOTASFMsforlearningSFTsuchasASR,SER,",
      "data": [
        {
          "90.93": "81.13",
          "80.15": "75.22",
          "99.57": "98.79",
          "5.81": "6.87"
        },
        {
          "90.93": "90.19",
          "80.15": "75.85",
          "99.57": "99.60",
          "5.81": "5.68"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: , 4, 5 presents the scores for MVMT re- SOTASFMsforlearningSFTsuchasASR,SER,",
      "data": [
        {
          "90.19": "89.41",
          "88.11": "83.98",
          "100.00": "100.00",
          "2.34": "2.58"
        },
        {
          "90.19": "91.99",
          "88.11": "87.60",
          "100.00": "100.00",
          "2.34": "1.44"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: , 4, 5 presents the scores for MVMT re- SOTASFMsforlearningSFTsuchasASR,SER,",
      "data": [
        {
          "100": "96.26",
          "97.20": "95.33",
          "100.00": "99.07",
          "1.74": "2.00"
        },
        {
          "100": "99.53",
          "97.20": "100.00",
          "100.00": "100.00",
          "1.74": "1.38"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Age and gender recognition from speech signals",
      "authors": [
        "Ara Assim",
        "Abdulsatar",
        "Vv Davydov",
        "Vv Yushkova",
        "Yu Ap Glinushkin",
        "Rud"
      ],
      "year": "2019",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "2",
      "title": "Basic arabic vocal emotions dataset (baved)",
      "authors": [
        "Aouf"
      ],
      "year": "2019",
      "venue": "Basic arabic vocal emotions dataset (baved)"
    },
    {
      "citation_id": "3",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "Arun Babu",
        "Changhan Wang",
        "Andros Tjandra",
        "Kushal Lakhotia",
        "Qiantong Xu",
        "Naman Goyal",
        "Kritika Singh",
        "Yatharth Patrick Von Platen",
        "Juan Saraf",
        "Alexei Pino",
        "Alexis Baevski",
        "Michael Conneau",
        "Auli"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-143"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech 2005",
      "doi": "10.21437/Interspeech.2005-446"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "7",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "2022a. Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "2022b. Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Zhuo Chen",
        "Shujie Liu",
        "Jian Wu",
        "Yao Qian",
        "Furu Wei",
        "Jinyu Li"
      ],
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "authors": [
        "Orchid Chetia Phukan",
        "Arun Balaji Buduru",
        "Rajesh Sharma"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-2561"
    },
    {
      "citation_id": "11",
      "title": "Adapting wavlm for speech emotion recognition",
      "authors": [
        "Daria Diatlova",
        "Anton Udalov",
        "Vitalii Shutov",
        "Egor Spirin"
      ],
      "year": "2024",
      "venue": "Adapting wavlm for speech emotion recognition",
      "arxiv": "arXiv:2405.04485"
    },
    {
      "citation_id": "12",
      "title": "Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model",
      "authors": [
        "Tarun Gupta",
        "Tuan Truong",
        "Tran The",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-567"
    },
    {
      "citation_id": "13",
      "title": "Overlapped speech and gender detection with WavLM pre-trained features",
      "authors": [
        "Martin Lebourdais",
        "Marie Tahon",
        "Laurent Antoine",
        "Sylvain Meignier"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-10825"
    },
    {
      "citation_id": "14",
      "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
      "authors": [
        "Jacob Peplinski",
        "Joel Shor",
        "Sachin Joglekar",
        "Jake Garrison",
        "Shwetak Patel"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-2070"
    },
    {
      "citation_id": "15",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "Shraman Pramanick",
        "Aniket Roy",
        "M Vishal",
        "Johns"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV51458.2022.00062"
    },
    {
      "citation_id": "16",
      "title": "Scaling speech technology to 1,000+ languages",
      "authors": [
        "Andros Vineel Pratap",
        "Bowen Tjandra",
        "Paden Shi",
        "Arun Tomasello",
        "Sayani Babu",
        "Ali Kundu",
        "Zhaoheng Elkahky",
        "Apoorv Ni",
        "Maryam Vyas",
        "Fazel-Zarandi"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "17",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "18",
      "title": "Universal paralinguistic speech representations using self-supervised conformers",
      "authors": [
        "Joel Shor",
        "Aren Jansen",
        "Wei Han",
        "Daniel Park",
        "Yu Zhang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "Joel Shor",
        "Subhashini Venugopalan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-118"
    },
    {
      "citation_id": "20",
      "title": "Xvectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Exploring speaker age estimation on different self-supervised learning models",
      "authors": [
        "Duc-Tuan Truong",
        "Tran The",
        "Chng Siong"
      ],
      "year": "2022",
      "venue": "2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)",
      "doi": "10.23919/APSIPAASC55919.2022.9979878"
    },
    {
      "citation_id": "22",
      "title": "Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition",
      "authors": [
        "Lee Shi"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-1425"
    },
    {
      "citation_id": "23",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "24",
      "title": "Speech selfsupervised representation benchmarking: Are we doing it right?",
      "authors": [
        "Salah Zaiem",
        "Youcef Kemiche",
        "Titouan Parcollet",
        "Slim Essid",
        "Mirco Ravanelli"
      ],
      "year": "2023",
      "venue": "INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-1087"
    },
    {
      "citation_id": "25",
      "title": "Age estimation in short speech utterances based on lstm recurrent neural networks",
      "authors": [
        "Ruben Zazo",
        "Phani Sankar Nidadavolu",
        "Nanxin Chen",
        "Joaquin Gonzalez-Rodriguez",
        "Najim Dehak"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Exploring Multi-task Learning Based Gender Recognition and Age Estimation for Classimbalanced Data",
      "authors": [
        "Weiqiao Zheng",
        "Ping Yang",
        "Rongfeng Lai",
        "Kongyang Zhu",
        "Tao Zhang",
        "Junpeng Zhang",
        "Hongcheng Fu"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-682"
    },
    {
      "citation_id": "27",
      "title": "Hyperparameter and Training details We keep the convolutional block 1D-CNN kernel to be of size 3 with 32 and 64 as number of kernels",
      "venue": "Hyperparameter and Training details We keep the convolutional block 1D-CNN kernel to be of size 3 with 32 and 64 as number of kernels"
    }
  ]
}