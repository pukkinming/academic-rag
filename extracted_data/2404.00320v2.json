{
  "paper_id": "2404.00320v2",
  "title": "Advancing Pain Recognition Through Statistical Correlation-Driven Multimodal Fusion*",
  "published": "2024-03-30T11:13:18Z",
  "authors": [
    "Xingrui Gu",
    "Zhixuan Wang",
    "Irisa Jin",
    "Zekun Wu"
  ],
  "keywords": [
    "Pain Recognition",
    "Behaviour Recognition",
    "Human Centered Computing",
    "Statistics",
    "Explainable AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This research presents a novel multimodal data fusion methodology for pain behavior recognition, integrating statistical correlation analysis with human-centered insights. Our approach introduces two key innovations: 1) integrating datadriven statistical relevance weights into the fusion strategy to effectively utilize complementary information from heterogeneous modalities, and 2) incorporating human-centric movement characteristics into multimodal representation learning for detailed modeling of pain behaviors. Validated across various deep learning architectures, our method demonstrates superior performance and broad applicability. We propose a customizable framework that aligns each modality with a suitable classifier based on statistical significance, advancing personalized and effective multimodal fusion. Furthermore, our methodology provides explainable analysis of multimodal data, contributing to interpretable and explainable AI in healthcare. By highlighting the importance of data diversity and modality-specific representations, we enhance traditional fusion techniques and set new standards for recognizing complex pain behaviors. Our findings have significant implications for promoting patientcentered healthcare interventions and supporting explainable clinical decision-making.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective Computing, an interdisciplinary field within Human-Computer Interaction, holds immense potential for enhancing human-computer interfaces and health surveillance by recognizing and responding to human emotions  [1] . Its application in understanding human pain behavior is particularly promising, as it acknowledges pain as a complex emotional state rather than merely a physical condition  [2] [3] . As illustrated in Figure  1 , pain is closely intertwined with anxiety, which can lead to protective behaviors such as guarding  [4] . However, accurate pain recognition remains challenging due to the multi-dimensionality and subjectivity of pain experiences, which stem from the intricate interplay of physiological, psychological, and social factors  [5]  [6]  [7] .\n\nFig.  1 . Relationship between pain, emotion and protective behaviour  [4]  Multimodal data fusion has emerged as a powerful approach in human-centered computing to address these challenges by integrating information from diverse sources such as physiological signals, behavioral cues, and self-reports  [8] . Nevertheless, existing multimodal pain recognition methods often struggle with data heterogeneity, alignment issues, and limited interpretability  [9] . Moreover, conventional machine learning and deep learning techniques often fail to capture the complexities of pain experiences and lack the adaptability to personalize pain recognition  [10] .\n\nTo overcome these limitations, this research introduces a novel approach to multimodal pain recognition that synergistically integrates statistical methods with a human-centric perspective. Our key innovations lie in: 1) employing statistical inference and hypothesis testing to explore the relationships between different modalities and the target variable (pain states), identifying the most informative features for pain recognition; and 2) incorporating human-centered insights into the representation and modeling of pain experiences to ensure the interpretability and ethical alignment of our models. By dynamically adjusting the contributions of different modalities through adaptive weighting, our data-driven, personalized approach enhances the precision, efficiency, and adaptability of pain recognition. Ultimately, this research contributes to the advancement of pain recognition, affective computing, AI-assisted healthcare, and the development of empathetic and socially responsible AI systems, paving the way for more effective and ethically grounded pain management strategies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Previous studies have suggested that traditional machine learning and deep learning approaches can serve as alternatives to manual pain scoring. For instance, using a conventional neural network based on Google's InceptionV3 model, researchers predicted binary labels of \"pain\" and \"no pain\" from mouse facial images  [11] . Additionally, Surface Electromyography (SEMG) recordings have proven valuable in assessing chronic low back pain  [12] . Building on this foundational research, a new perspective in pain recognition emphasizes the integration of multimodal data to leverage the strengths of various sources, thereby enhancing the accuracy and precision of predicting pain-related behaviors. However, it is crucial first to establish and evaluate pain as a human emotion. Previous research has identified overlapping brain regions within the Central Nervous System (CNS) that are involved in both pain and other emotions, including the amygdala, thalamus, and Anterior Cingulate Cortex (ACC)  [13] .\n\nBuilding upon this foundation of research, there emerges a novel perspective within affective computing and human centered computing, wherein the integration of multimodal data can use the strengths of various sources of data to enhance the accuracy and precision of predicting pain-related behavior. The recognition of emotion and pain requires a nuanced understanding of complex constructs, synthesizing from a diverse array of modal features, such as facial expressions, vocal tones, postures, physiological signals, sensory experiences, emotional states, and cognitive evaluations  [6] [7] .\n\nThis complexity necessitates an integrated, multimodal analytical approach that goes beyond singular data sources. Consequently, state-of-the-art deep learning algorithms, including CNN-LSTMs, have been leveraged to significantly improve the accuracy and speed of pain recognition  [14]    [15] . However, the challenge of effectively integrating and processing this heterogeneous data persists, and overcoming this challenge is essential for advancing multimodal data fusion and maximizing the efficacy of analytical methods for pain recognition  [10] .\n\nReflecting on this necessity, historical research reinforces the value of integrating multiple data types, such as body movements and muscle activity, to enhance pain recognition accuracy  [16] . These findings highlight the crucial interconnectivity of different modalities, underpinning their essential role in pain recognition and supporting the advancement of multimodal methodologies in this study.\n\nSubsequent explorations have further elucidated this concept, with studies illustrating the efficacy of combining muscle and motion signals for the accurate identification of protective behaviors-a key aspect of pain recognition. Notably, central (model-level) fusion approaches have been shown to outperform both feature and decision-level fusion methods in the context of Protective Behavior Detection (PBD), showcasing their superior capability in harnessing the potential of multimodal data  [17] [18] . This advancement signals a critical insight into the differential impact of various fusion layers on the process of pain recognition, highlighting the imperative of selecting and implementing the most effective fusion strategies to achieve a more integrative and holistic understanding of pain behaviors through the synthesis of multimodal information.\n\nInnovative model architectures have been developed to enhance the accuracy and applicability of pain recognition studies. The P-STEMR framework is a notable example, utilizing human activity recognition datasets to classify pain levels, showcasing the utility of supervised learning in situations with limited labeled data sources  [19] . Additionally, the introduction of an advanced hierarchical HAR-PBD architecture represents significant progress in real-time pain recognition. This architecture combines Human Activity Recognition (HAR) with Protective Behavior Detection (PBD) using graph convolution and Long Short-Term Memory (LSTM) networks. Moreover, this model addresses the common challenge of data classification imbalance by implementing the Class-Balanced Focal Classification Cross-Entropy (CFCC) loss function  [20] , thereby enhancing the reliability and effectiveness of pain monitoring systems.\n\nThe challenges in multimodal data fusion involve integrating heterogeneous data from diverse sources for accurate pain state recognition, where increased dimensionality can reduce model interpretability. While previous research has employed machine learning and deep learning for automated pain recognition, these methods often struggle with complex, multisource pain behaviors. Our novel approach emphasizes a multimodal data fusion strategy grounded in statistical relevance, complemented by a human-centric perspective to enhance model effectiveness. This methodology not only accounts for data diversity but also transforms the relationships between different modalities and outcomes into weighted contributions for model decision-making, resulting in more precise and comprehensive detection of pain behaviors.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this study, our objective is to investigate and substantiate the efficacy and relevance of statistical approaches in amalgamating multimodal data, with a particular focus on domains centered around human computation, such as the analysis of pain. The endeavor is to amalgamate a variety of statistical instruments to refine the integration process of multimodal data, thereby augmenting the precision and efficiency in the analysis of pain behaviors. We postulate that employing this strategy will facilitate a more nuanced comprehension of the intricacies and multifaceted nature of pain, consequently enabling the provision of solutions for pain management that are both more targeted and individualized.\n\nThe central focus of this research is on developing methodologies for the effective fusion and processing of multimodal data from diverse sources and types. This involves utilizing statistical techniques to integrate modal features, conducting hypothesis tests on target variables, and performing correlation analyses. Additionally, the study aims to evaluate the significance of different modalities and determine dynamic weight distribution based on statistical insights. It also seeks to optimize the combination of features to improve the accuracy and effectiveness of models in predicting pain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset Introduction",
      "text": "In this research, we utilized the EmoPain dataset, a key resource for exploring the relationship between body movements and pain intensity levels  [21] . The dataset is divided into training and validation sets, with data from 10 chronic pain sufferers and 6 healthy controls in the training set, and 4 chronic pain individuals and 3 healthy controls in the validation set. As detailed in Table  III -A and Figure  2 , the dataset includes X, Y, and Z coordinates of body joints, categorized in columns 1-22, 23-44, and 45-66, respectively. The core of our analysis focuses on vector 73, which measures protective behavior, distinguishing non-protective actions (coded as 0) from protective behaviors (coded as 1). This complex interplay between protective behaviors and pain, mediated by emotional states, positions our study at the intersection of behavioral analysis, pain recognition, and emotional computation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset Analysis",
      "text": "The histograms and Q-Q plots in Figure  3  for the training and validation data provide crucial insights into the normality of the dataset's distribution. From the histograms, both datasets display a pronounced peak, but it doesn't align with the theoretical normal distribution curve, suggesting a discrepancy  from normality. This is further evidenced by the asymmetry and apparent deviations from the central peak.\n\nThe Q-Q plots reinforce these findings. Ideally, data points should closely follow the red line if they were normally distributed. However, in both training and validation datasets, significant deviations occur, particularly in the tails of the distribution. These deviations manifest as pronounced curves away from the expected line, indicating heavier tails than those of a normal distribution and suggesting a skew in the data.\n\nGiven the distribution characteristics of our dataset, as outlined in Table  I , we identify the limitations and suitability of various statistical methods for our analysis. The assumption of normality renders ANOVA and Pearson correlation ineffective for our purposes, while Kendall's rank correlation, despite its robustness, is impractical due to computational demands and is more suited for smaller datasets. Consequently, we opt for Spearman's rank correlation as the most fitting choice, owing to its efficiency with large datasets and applicability to non-linear relationships, ensuring a more accurate and relevant analysis of our multimodal data. This decision strategically aligns with our analytical needs, highlighting Spearman's correlation as the optimal tool for exploring the complex relationships within our heterogeneous dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Experiment Design",
      "text": "The experimental design rigorously explores the influence of diverse data fusion strategies on the effectiveness of protective behavior recognition models. It delves into how decision-level fusion, enhanced by statistical analysis of data heterogeneity and human-centered modalities, affects model performance in the spectrum of protective behavior identification.\n\n1) Decision-Level Fusion with Singular Modality (Benchmark Model): The foundational experiment initiates with all 70 features amalgamated as a singular modality, subjected to a single Classification model. This setup, while ostensibly a feature-level fusion, is underscored as a decision-level fusion where the entire feature set is implicitly accorded a 100% weightage. This model establishes the baseline for performance metrics against which the outcomes of subsequent fusion techniques are compared (see Figure  4 ). 2) Bifurcated Modality Approach: Progressing to acknowledge the heterogeneity inherent in the data, the experiment bifurcates the features into two distinct modalities: the XYZ coordinates and sEMG signals. Each modality is then independently processed through an identical Classification model framework. Post-training, a weighted voting mechanism-rooted in decision-level fusion-is employed to amalgamate the predictive insights from each modality. The weights are meticulously derived from Spearman rank correlation coefficients, mirroring the relative significance of each modality's contribution to pain level prediction. This phase aims to unravel whether segmenting features based on their heterogeneity and integrating them through a weighted decision-making process can elevate the model's performance beyond the benchmark (see Figure  5 ).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "4) Average Weighted Voting As A Comparative Benchmark:",
      "text": "To enhance the comparison within our study, we have implemented an average weighted voting mechanism across the four modalities, which presumes each modality contributes equally, without considering their statistical correlation to the protective behaviour states being analysed. This baseline method is essential for our analysis as it starkly contrasts with the statistically weighted voting approach, thereby underscoring the advantages of utilizing statistical correlations for modality weighting. This comparative analysis not only highlights the efficacy of statistical correlations but also emphasizes the significance of adopting a human-centered perspective in modality segmentation, showcasing its potential to yield more nuanced and accurate emotion recognition results.\n\n5) Comparative Analysis & Model Performance Evaluation: By comparing the performance of the singular modality (benchmark model) with both the bifurcated and quadrifurcated modality approaches, as well as the average weighted voting benchmark, the experiment aspires to illuminate whether a granular, human-centered feature segmentation supplemented by statistically weighted decision-level fusion markedly optimizes model accuracy in pain recognition. The transition from a singular, homogenously weighted modality to a nuanced, statistically or evenly weighted integration of multiple modalities endeavors to elucidate the symbiotic relationship between data-driven and human-centered segmentation, and their collective prowess in enhancing pain recognition models. This comprehensive exploration aims to crystallize the efficacy of each strategic approach and underscore their cumulative contribution towards refining model performance in the nuanced domain of pain recognition.s",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Results And Evaluation",
      "text": "In our study, as delineated in Table II and Figure  7 , we elucidate the efficacy of integrated strategies, ranging from foundational neural networks to an advanced BodyAttention network and CNN with Multi-head Self-Attention mechanism, on pivotal performance metrics. This investigation clearly  delineates the impact of varied data fusion strategies on precision, accuracy, recall, and F1 scores during protective behavior detection. Through the integration of statistical weighting in LSTM and CNN base models, expansion to BodyAttention Network (BANet)  [23] , and the adoption of Multi-head Self-Attention, we aim to evaluate the enhancement in model performance facilitated by these methodologies across diverse neural network architectures. Our research particularly explores how the amalgamation of statistical correlations with a human-centered approach can ameliorate model outcomes, especially within the challenges posed by imbalanced datasets.\n\nOur findings emphasize the pivotal role of modality segmentation and decision-level fusion, informed by statistical insights, in augmenting the accuracy of protective behavior detection. They also highlight the broad applicability of statistically driven weighting strategies within various complex neural frameworks. Through an in-depth analysis of performance shifts across different configurations, we reveal the critical role of integrating sophisticated data processing techniques, human-centered perspectives, and statistical correlations. This holistic strategy markedly enhances the precision and effectiveness of protective behavior recognition in complex physiological datasets, representing a significant advance toward nuanced patient-centered healthcare solutions. Given the imbalanced nature of our dataset, we prioritize precision, recall, and F1 score over accuracy to more accurately reflect model performance across labels. We employed four distinct models-LSTM, CNN, CNN-Attention, and CNN with Multihead Self-Attention-to diversify our evaluation and mitigate single-model reliance risks. It is important to note that a comparative performance analysis among these models is not within this paper's scope. This methodology underscores our dedication to a thorough evaluation, aiming to deepen the understanding of multi-modal data fusion's impact on model efficacy amidst dataset imbalances.\n\nThe data presented in Table  II  provide a detailed analysis of performance differences across models using various modality integrations. A key observation is that models based on a single modality can achieve high accuracy, up to 0.93, but often at the expense of other metrics like precision, recall, and F1-score, which hover around 0.55. This indicates a potential trade-off between maximizing accuracy and ensuring balanced performance across all metrics. Examining models with dual modalities reveals different impacts on performance metrics. For example, while the LSTM model experiences a slight drop in accuracy with dual-modality integration, other architectures show improvements. This variation highlights the limitations of relying solely on a single modality, which, despite high accuracy, may not fully capture a broader range of evaluative measures. Conversely, incorporating an additional modality significantly enhances precision, recall, and F1-scores, with increases ranging from 12% to 62%. This strongly supports the notion that integrating multiple modalities enhances model robustness. Specifically, segregating input features into distinct spatial and sEMG modalities, rather than combining all 70 features, proves to be a strategic advantage. This approach improves the model's ability to identify and evaluate protective behaviors, resulting in better precision and sensitivity.\n\nThe analysis delineates that while models based on a singular modality bypass the complexities of decision-layer weighting, rendering the choice between statistical and average weighting irrelevant, the introduction of a statistically driven weighting strategy in dual-modality configurations significantly enhances model performance by leveraging data diversity and modality-specific importance. This strategic use of statistical weighting in models integrating two modalities distinctly outperforms single-modality models, affirming the superiority of a multimodal fusion approach. This research underscores the efficacy of partitioning input features into distinct modalities coupled with the judicious use of statistical weighting at the decision layer, thereby substantially improving the precision and reliability of protective behavior detection. It paves the way for sophisticated multimodal integration in human-centered computing, setting a benchmark for handling complex, varied datasets with enhanced accuracy.\n\nAdopting a human-centered approach to segment the dataset into four modalities demonstrates potential enhancements in model performance. Particularly, models employing CNN-Attention mechanisms exhibit slight but positive differences, suggesting an improved capability in capturing pain behavior features. This indicates that modality segmentation, guided by human-centered principles, can amplify model effectiveness. Further analysis reveals that models employing statistical weighting generally outperform those using mean weighting, except in the case of LSTM with four modalities employing average weighting-a scenario that mirrors the trade-offs observed in single-modality LSTM models.\n\nThis investigation emphasizes the advantage of multimodal strategies, where strategic feature grouping and statistical decision-making markedly elevate model efficacy. Our findings particularly highlight the utility of attention mechanisms, like CNN-Attention and CNN with Multi-head Self-Attention, in a four-modality framework, reinforcing the benefits of humancentered modality segmentation. Overall, the transition to a four-modality model, grounded in human-centered design and statistical weighting, is validated as superior to single-modality approaches, bolstering the case for sophisticated multimodal fusion in enhancing protective behavior recognition.\n\nThe examination distinctly emphasizes the superiority of statistical weighting over average weighting in multimodal configurations, enhancing model performance. However, an exception was observed in the LSTM model utilizing average weighting, suggesting a nuanced balance between accuracy and other metrics. Despite this, the evidence strongly supports the advantages of the multimodal approach over singular modality frameworks. This finding reinforces the idea that strategic segmentation of modalities, when combined with statistical weighting, markedly advances protective behavior detection models.\n\nThis study highlights the critical importance of a humancentered modality segmentation strategy and the precise application of statistical methods in decision-making processes to optimize model outcomes. The integration of diverse modalities, guided by careful grouping and statistical weighting, not only elevates model performance but also marks a significant advance towards crafting more nuanced, interpretative models within protective behavior recognition. Consequently, this methodological approach not only pushes the boundaries of multimodal data fusion but also underlines the necessity of integrating human-centric perspectives and statistical insights into the broader narrative of human-centered computing.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "A. Cross Validation",
      "text": "From the experimental metrics in Table  II , the CNN-Attention model demonstrates robust performance on the Emopain dataset. To substantiate the efficacy of our approach, we adopt a Leave-One-Out Cross-Validation (LOOCV) strategy, meticulously testing each dataset instance as an individual test case, with the remaining data serving for training. This   III  demonstrate that both dual-modality and fourmodality configurations surpass the single-modality training approach, affirming the benefits of data heterogeneity segmentation and human-centered modality segmentation. Notably, the CNN-Attention model, when expanded to include two modalities with statistical weighting, shows a significant improvement in accuracy from 0.800 to 0.907 and an increase in F1-score from 0.547 to 0.631. This enhancement highlights the clear advantage of integrating multiple data sources over a single modality framework, indicating that the inclusion of diverse data types enriches the model's performance in predicting outcomes.\n\nFurthermore, transitioning from a two-modality to a fourmodality configuration with statistical weighting (CNN-Attention+Stat (4 mod.)) marginally enhances the model's accuracy to 0.908, maintaining superior recall (Rec.) and F1score metrics compared to the single-modality model. This slight improvement evidences the nuanced benefits of adopting a human-centered approach to modality segmentation, where data is divided into four distinct modalities based on its relevance and interaction with human behavioral patterns.\n\nMoreover, a comparison between the four-modality configurations-statistical weighting versus average weighting (CNN-Attention+Avg (4 mod.))-reveals a distinct advantage for the former. The model employing statistical weighting (CNN-Attention+Stat (4 mod.)) achieves higher accuracy, recall, and F1-score than the model utilizing average weighting, with respective metrics of 0.908, 0.642, and 0.628 against 0.886, 0.635, and 0.603. This differential highlights the effectiveness of our statistical relevance weighting strategy, proving it to be a more effective method for integrating diverse modalities than merely averaging their contributions.\n\nIn summation, the Leave-One-Out Cross-Validation (LOOCV) findings robustly advocate for the deployment of multimodal fusion frameworks, amalgamating statistical correlations and human-centered methodologies. The analysis distinctly highlights the pivotal role of statistical weighting in augmenting model efficacy. It evidences that modality segmentation, underpinned by human-centered considerations, not only contributes positively but that the strategic application of statistical weighting across such segmented modalities markedly optimizes model performance. This approach signifies a notable progression in protective behavior detection, establishing the profound impact of integrating statistical insights with human-centered design principles on enhancing computational models within this domain.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Discussion",
      "text": "This research introduces an innovative approach to affective computing and multimodal data fusion by integrating statistical methods with human-centered computation. Our findings indicate that statistical algorithms can more effectively extract data representations across various modalities, which enhances our understanding and modeling of human behavior. Traditional machine learning methods often struggle with the complexity of human behavior and the interplay between different factors  [24] . However, our approach successfully identifies key features linked to pain behavior by combining statistical correlations with human expert knowledge. This not only enhances model performance but also improves interpretability, providing valuable insights for the development of future interactive intelligent systems  [25] .\n\nThe study underscores the importance of data-driven statistical methods in pain recognition representation extraction, particularly in situations where complete reliance on patient self-reporting is not feasible. While self-reporting is a crucial means of pain recognition, it is limited by subjectivity, communication barriers, psychological factors, sociocultural influences, and feedback time delays  [26] . Feature extraction based on statistical correlations provides an objective, continuous, and non-invasive method for pain representation extraction, complementing the limitations of self-reporting. This datadriven feature extraction can assist healthcare professionals in more accurately assessing patients' pain conditions, especially when patients are unable to self-report  [27] .\n\nFurthermore, given the substantial variability in human samples, personalisation has long been a potential barrier to digital healthcare. However, by utilising modality feature extraction driven by statistical correlations, our system can capture unique pain expression patterns for each individual, contributing to more precise and effective treatment strategies. This approach not only has the potential to improve patients' quality of life but may also reduce the risk of drug abuse, particularly in the management of chronic pain  [28] .\n\nFinally, the statistical correlation-driven multimodal fusion framework and its representation learning approach that we have demonstrated possess extensive application potential, with the possibility of further extension to other domains involving complex human-centred computing. This research provides a viable framework for integrating statistical methods, machine learning, and human expertise to address complex challenges of human-computer interaction. This interdisciplinary approach not only enhances technical performance but also augments its credibility and acceptability in practical applications. As technology continues to advance, we anticipate seeing more innovative applications based on this framework, ultimately realising more intelligent and humanised interactive systems. This integrated approach opens up new possibilities for future human-machine collaboration, with the potential to play a crucial role in various complex human-centred computing tasks.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This research venture has systematically unveiled the efficacy of integrating statistical methods and human-centered perspectives within the ambit of multimodal pain behaviours recognition, employing an array of deep learning architectures including convolutional neural networks (CNN), long shortterm memory networks (LSTM), CNN-Attention networks and CNN with Multi-head Self Attention. The cornerstone of our exploration was to enhance the precision and utility of complex pain recognition endeavors through the lens of statistical relevance and human-centered modality segmentation.\n\nThe incorporation of statistically correlated vote weights and a human-centered approach to data segmentation stands as this study's central innovation. This methodology significantly enhances model performance and paves the way for a deeper understanding and interpretation of multimodal data. It signals a shift towards choosing optimal classifiers for each modality, refining our voting strategy for the ultimate decisionmaking process. Considering the diversity and weak correlations among modalities, selecting a classifier tailored to the characteristics of each modality is crucial. This sophisticated strategy highlights the importance of a customized approach to modality-specific model selection, thereby boosting the efficacy of multimodal fusion.\n\nFurthermore, the research integrates statistical significance with a human-centered perspective, paving the way for explainable AI in pain recognition. This approach delves into the distinct impact of each data modality on model outcomes, promoting a model of explainability that not only clarifies the workings of complex models but also enhances their adaptability and accuracy. This shift highlights our methodology's broad applicability, not only in pain management but across diverse domains of human-centered computing, spotlighting its potential to revolutionize how we interact with and leverage AI technologies.\n\nIn summary, this research not only pioneers the integration of statistical correlations with human-centered methods for multimodal data fusion in pain recognition but also advances the field by offering novel insights into modality fusion strategies. It enhances the discussion on improving model performance and interpretability within human-centered computing. The comprehensive framework established for multimodal fusion application spotlights the potential of statistical insights for model explainability, fostering trust in AI systems. Consequently, this work paves the way for transformative applications in AI systems closely aligned with human needs and behaviors, promising significant advancements across various human-focused domains.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This research intersects affective computing, multimodal data analysis, and pain recognition, aiming to enhance the precision and personalization of pain management through statistical methods. We conscientiously address the potential societal, environmental, and ethical impacts, focusing on aspects such as explainability, transparency, liability, fairness, efficacy, robustness, privacy, security, and sustainability. a) Explainability and Transparency: Our study is committed to ensuring that advancements in pain recognition are both explainable and transparent. We leverage statistical models and human-centered perspectives to allow users and stakeholders to understand the rationale behind our predictions. This fosters trust and facilitates deeper understanding and acceptance of our technology in clinical settings. b) Liability: Acknowledging the profound implications of our work in enhancing pain recognition, we recognize our responsibility to ensure the reliability and accuracy of our findings. Misinterpretations or inaccuracies could significantly impact patient care and treatment outcomes. Therefore, we adhere to stringent validation protocols, underpinned by ethical considerations, to mitigate potential risks and liabilities associated with our research outputs. c) Fairness: Our research actively addresses the issue of fairness by incorporating the EmoPain dataset  [21]  that includes chronic pain participants and healthy individuals. This diversity ensures our models do not inadvertently perpetuate biases against certain demographics. By integrating a humancentered approach, we actively seek to understand and incorporate diverse pain expressions and experiences, thereby mitigating the risk of biases that could otherwise compromise the fairness of our models. d) Efficacy and Robustness: The intersection of statistical methods and human-centered design in our work aims to enhance the efficacy and robustness of pain recognition technologies. Our approach ensures that our models are not only accurate but also resilient to the complexities and variabilities inherent in human pain experiences, thereby supporting reliable pain management practices. e) Privacy and Security: The sensitivity of health-related data necessitates stringent privacy and security measures. Our research select the EmoPain dataset  [21]  that adheres to the highest standards of data protection, ensuring all participant data is anonymized and securely stored. Access controls, encryption, and ethical data handling practices protect against unauthorized access and data breaches.\n\nIn conclusion, our work not only signifies a step forward in the technical domain but also embodies a comprehensive ethical approach. Through our commitment to ethical considerations, we aspire to contribute meaningfully to the field of affective computing, ensuring that our innovations in pain recognition are responsible, equitable, and beneficial for all stakeholders involved.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , pain is closely intertwined",
      "page": 1
    },
    {
      "caption": "Figure 1: Relationship between pain, emotion and protective behaviour [4]",
      "page": 1
    },
    {
      "caption": "Figure 2: , the dataset",
      "page": 3
    },
    {
      "caption": "Figure 2: The arrangement of the 22 body joints [22]",
      "page": 3
    },
    {
      "caption": "Figure 3: for the training",
      "page": 3
    },
    {
      "caption": "Figure 3: Left: Train Data Distribution. Right: Valid Data Distribution.",
      "page": 3
    },
    {
      "caption": "Figure 4: Singular Modality (Benchmark Model)",
      "page": 4
    },
    {
      "caption": "Figure 5: Bifurcated Modality Approach",
      "page": 4
    },
    {
      "caption": "Figure 6: Quadrifurcated Modality Approach (Incorporating Human Factors)",
      "page": 4
    },
    {
      "caption": "Figure 7: Three experimental model architectures",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "London, UK": "zekun.wu@holisticai.com"
        },
        {
          "London, UK": "Abstract—This\nresearch\npresents\na\nnovel multimodal\ndata"
        },
        {
          "London, UK": "fusion methodology\nfor pain behavior\nrecognition,\nintegrating"
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "statistical correlation analysis with human-centered insights. Our"
        },
        {
          "London, UK": "approach introduces\ntwo key innovations: 1)\nintegrating data-"
        },
        {
          "London, UK": "driven statistical\nrelevance weights\ninto\nthe\nfusion strategy\nto"
        },
        {
          "London, UK": "effectively\nutilize\ncomplementary\ninformation\nfrom heteroge-"
        },
        {
          "London, UK": "neous modalities,\nand\n2)\nincorporating\nhuman-centric move-"
        },
        {
          "London, UK": "ment characteristics into multimodal representation learning for"
        },
        {
          "London, UK": "detailed modeling\nof pain behaviors. Validated across\nvarious"
        },
        {
          "London, UK": "deep learning architectures, our method demonstrates\nsuperior"
        },
        {
          "London, UK": "performance and broad applicability. We propose a customizable"
        },
        {
          "London, UK": "framework that aligns\neach modality with a suitable\nclassifier"
        },
        {
          "London, UK": "based\non\nstatistical\nsignificance,\nadvancing\npersonalized\nand"
        },
        {
          "London, UK": "effective multimodal\nfusion. Furthermore, our methodology pro-"
        },
        {
          "London, UK": "vides\nexplainable analysis of multimodal data,\ncontributing to"
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "interpretable and explainable AI\nin healthcare. By highlighting"
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "the\nimportance\nof\ndata diversity\nand modality-specific\nrepre-"
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "sentations, we\nenhance\ntraditional\nfusion\ntechniques\nand\nset"
        },
        {
          "London, UK": "new standards\nfor\nrecognizing\ncomplex\npain\nbehaviors. Our"
        },
        {
          "London, UK": "findings\nhave\nsignificant\nimplications\nfor\npromoting\npatient-"
        },
        {
          "London, UK": "centered\nhealthcare\ninterventions\nand\nsupporting\nexplainable"
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "clinical decision-making."
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "Index Terms—Pain Recognition, Behaviour Recognition, Hu-"
        },
        {
          "London, UK": ""
        },
        {
          "London, UK": "man Centered Computing, Statistics, Explainable AI"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "socially\nresponsible AI\nsystems,\npaving\nthe way\nfor more",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "timodal data\n[17]\n[18]. This\nadvancement\nsignals\na\ncritical"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "effective and ethically grounded pain management strategies.",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "insight\ninto the differential\nimpact of various fusion layers on"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "the process of pain recognition, highlighting the imperative of"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "II. RELATED WORK",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "selecting and implementing the most effective fusion strategies"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "to achieve a more integrative and holistic understanding of pain"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "Previous\nstudies\nhave\nsuggested\nthat\ntraditional machine",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "behaviors through the synthesis of multimodal\ninformation."
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "learning and deep learning approaches can serve as alternatives",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "Innovative model\narchitectures\nhave\nbeen\ndeveloped\nto"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "to manual pain scoring. For instance, using a conventional neu-",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "enhance\nthe\naccuracy\nand\napplicability\nof\npain\nrecognition"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "ral network based on Google’s InceptionV3 model, researchers",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "studies.\nThe\nP-STEMR framework\nis\na\nnotable\nexample,"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "predicted binary labels of “pain” and “no pain” from mouse",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "utilizing human activity recognition datasets\nto classify pain"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "facial\nimages\n[11]. Additionally, Surface Electromyography",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "levels, showcasing the utility of supervised learning in situa-"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "(SEMG) recordings have proven valuable in assessing chronic",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "tions with limited labeled data sources [19]. Additionally,\nthe"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "low back pain [12]. Building on this foundational\nresearch, a",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "introduction of an advanced hierarchical HAR-PBD architec-"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "new perspective in pain recognition emphasizes the integration",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "ture represents significant progress in real-time pain recogni-"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "of multimodal data to leverage the strengths of various sources,",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "tion. This architecture combines Human Activity Recognition"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "thereby enhancing the\naccuracy and precision of predicting",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "(HAR) with Protective Behavior Detection (PBD) using graph"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "pain-related behaviors. However,\nit\nis crucial first\nto establish",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "convolution and Long Short-Term Memory (LSTM) networks."
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "and\nevaluate\npain\nas\na\nhuman\nemotion. Previous\nresearch",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "Moreover,\nthis model addresses the common challenge of data"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "has\nidentified\noverlapping\nbrain\nregions within\nthe Central",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "classification imbalance by implementing the Class-Balanced"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "Nervous System (CNS) that are involved in both pain and other",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "Focal Classification Cross-Entropy (CFCC) loss function [20],"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "emotions,\nincluding\nthe\namygdala,\nthalamus,\nand Anterior",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "thereby\nenhancing\nthe\nreliability\nand\neffectiveness\nof\npain"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "Cingulate Cortex (ACC)\n[13].",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "monitoring systems."
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "Building upon this\nfoundation of\nresearch,\nthere\nemerges",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "The challenges in multimodal data fusion involve integrating"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "a\nnovel\nperspective within\naffective\ncomputing\nand\nhuman",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "heterogeneous\ndata\nfrom diverse\nsources\nfor\naccurate\npain"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "centered\ncomputing, wherein\nthe\nintegration\nof multimodal",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "state recognition, where increased dimensionality can reduce"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "data can use the strengths of various sources of data to enhance",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "model\ninterpretability. While previous research has employed"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "the accuracy and precision of predicting pain-related behavior.",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "machine learning and deep learning for automated pain recog-"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "The recognition of emotion and pain requires a nuanced un-",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "nition,\nthese methods\noften\nstruggle with\ncomplex, multi-"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "derstanding of complex constructs, synthesizing from a diverse",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "source pain behaviors. Our novel approach emphasizes a mul-"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "array of modal features, such as facial expressions, vocal tones,",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "timodal data fusion strategy grounded in statistical\nrelevance,"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "postures, physiological signals, sensory experiences, emotional",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "complemented\nby\na\nhuman-centric\nperspective\nto\nenhance"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "states, and cognitive evaluations [6]\n[7].",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "model effectiveness. This methodology not only accounts for"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "This complexity necessitates an integrated, multimodal ana-",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "data diversity but\nalso transforms\nthe\nrelationships between"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "lytical approach that goes beyond singular data sources. Con-",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "different modalities and outcomes into weighted contributions"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "sequently, state-of-the-art deep learning algorithms,\nincluding",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "for model\ndecision-making,\nresulting\nin more\nprecise\nand"
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "CNN-LSTMs, have been leveraged to significantly improve the",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "",
          "their\nsuperior\ncapability in harnessing the potential of mul-": "comprehensive detection of pain behaviors."
        },
        {
          "assisted healthcare,\nand the development of\nempathetic\nand": "accuracy and speed of pain recognition [14]\n[15]. However,",
          "their\nsuperior\ncapability in harnessing the potential of mul-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "hypothesis\ntests on target variables, and performing correla-": "",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "tion\nanalyses. Additionally,\nthe\nstudy\naims\nto\nevaluate\nthe",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "significance\nof\ndifferent modalities\nand\ndetermine\ndynamic",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "weight distribution based on statistical insights. It also seeks to",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "optimize the combination of features to improve the accuracy",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "and effectiveness of models in predicting pain.",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "A. Dataset\nIntroduction",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "",
          "TABLE I": "sample"
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "In this\nresearch, we utilized the EmoPain dataset,\na key",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "resource for exploring the relationship between body move-",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "ments\nand pain intensity levels\n[21]. The dataset\nis divided",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "into training and validation sets, with data from 10 chronic",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "pain sufferers and 6 healthy controls in the training set, and 4",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "chronic pain individuals and 3 healthy controls in the valida-",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "tion set. As detailed in Table III-A and Figure 2,\nthe dataset",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "includes X, Y, and Z coordinates of body joints, categorized",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "in columns 1-22, 23-44, and 45-66,\nrespectively. The core of",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "our analysis focuses on vector 73, which measures protective",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "behavior, distinguishing non-protective\nactions\n(coded as 0)",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "from protective behaviors (coded as 1). This complex interplay",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "between protective behaviors and pain, mediated by emotional",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "states, positions our\nstudy at\nthe\nintersection of behavioral",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "analysis, pain recognition, and emotional computation.",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "Columns\nDescription",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "1-22\nX coordinates of 22 body joints.",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "23-44\nY coordinates of 22 body joints.",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "45-66\nZ coordinates of 22 body joints.",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "67-70\nSurface electromyography data from the lumbar and upper trapez-",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "ius muscles.",
          "TABLE I": ""
        },
        {
          "hypothesis\ntests on target variables, and performing correla-": "73\nProtective behaviour\nlabel\n(0 for not protective, 1 for protective).",
          "TABLE I": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "The experimental design rigorously explores the influence of",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "diverse data fusion strategies on the effectiveness of protective",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "behavior recognition models. It delves into how decision-level",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "modality",
          "this segment of the experiment categorizes the": "processed"
        },
        {
          "C. Experiment Design": "fusion, enhanced by statistical analysis of data heterogeneity",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "and human-centered modalities, affects model performance in",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "the spectrum of protective behavior\nidentification.",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "1) Decision-Level Fusion with Singular Modality (Bench-",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "mark Model): The foundational experiment\ninitiates with all",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "70 features amalgamated as a singular modality, subjected to",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "a single Classification model. This\nsetup, while ostensibly a",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "feature-level\nfusion,\nis underscored as a decision-level\nfusion",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "where\nthe\nentire\nfeature\nset\nis\nimplicitly accorded a 100%",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "weightage. This model\nestablishes\nthe\nbaseline\nfor\nperfor-",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "mance metrics\nagainst which\nthe\noutcomes\nof\nsubsequent",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "fusion techniques are compared (see Figure 4).",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        },
        {
          "C. Experiment Design": "",
          "human factors,": "",
          "this segment of the experiment categorizes the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": ""
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "Multi-head SA+Avg (4 mod.)\n0.90\n0.69\n0.69\n0.69"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": ""
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": ""
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": ""
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "delineates the impact of varied data fusion strategies on preci-"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "sion, accuracy, recall, and F1 scores during protective behavior"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "detection. Through the integration of\nstatistical weighting in"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "LSTM and CNN base models,\nexpansion to BodyAttention"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "Network (BANet)\n[23], and the adoption of Multi-head Self-"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "Attention, we\naim to\nevaluate\nthe\nenhancement\nin model"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "performance facilitated by these methodologies across diverse"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "neural\nnetwork\narchitectures. Our\nresearch\nparticularly\nex-"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "plores how the amalgamation of\nstatistical correlations with"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "a human-centered approach can ameliorate model outcomes,"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "especially within the challenges posed by imbalanced datasets."
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "Our findings\nemphasize\nthe pivotal\nrole of modality seg-"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "mentation and decision-level\nfusion,\ninformed by statistical"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "insights,\nin\naugmenting\nthe\naccuracy\nof\nprotective\nbehav-"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "ior detection. They also highlight\nthe broad applicability of"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "statistically driven weighting strategies within various\ncom-"
        },
        {
          "0.72\n0.71\nMulti-head SA+Stat\n(4 mod.)\n0.90\n0.72": "plex\nneural\nframeworks. Through\nan\nin-depth\nanalysis\nof"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "performance shifts across different configurations, we reveal": "the\ncritical\nrole of\nintegrating sophisticated data processing"
        },
        {
          "performance shifts across different configurations, we reveal": "techniques, human-centered perspectives, and statistical corre-"
        },
        {
          "performance shifts across different configurations, we reveal": "lations. This holistic strategy markedly enhances the precision"
        },
        {
          "performance shifts across different configurations, we reveal": "and effectiveness of protective behavior\nrecognition in com-"
        },
        {
          "performance shifts across different configurations, we reveal": "plex physiological datasets, representing a significant advance"
        },
        {
          "performance shifts across different configurations, we reveal": "toward nuanced patient-centered healthcare\nsolutions. Given"
        },
        {
          "performance shifts across different configurations, we reveal": "the imbalanced nature of our dataset, we prioritize precision,"
        },
        {
          "performance shifts across different configurations, we reveal": "recall, and F1 score over accuracy to more accurately reflect"
        },
        {
          "performance shifts across different configurations, we reveal": "model performance across labels. We employed four distinct"
        },
        {
          "performance shifts across different configurations, we reveal": "models— LSTM, CNN, CNN-Attention, and CNN with Multi-"
        },
        {
          "performance shifts across different configurations, we reveal": "head Self-Attention—to diversify our evaluation and mitigate"
        },
        {
          "performance shifts across different configurations, we reveal": "single-model\nreliance\nrisks.\nIt\nis\nimportant\nto\nnote\nthat\na"
        },
        {
          "performance shifts across different configurations, we reveal": "comparative performance analysis among these models is not"
        },
        {
          "performance shifts across different configurations, we reveal": "within this paper’s scope. This methodology underscores our"
        },
        {
          "performance shifts across different configurations, we reveal": "dedication\nto\na\nthorough\nevaluation,\naiming\nto\ndeepen\nthe"
        },
        {
          "performance shifts across different configurations, we reveal": "understanding of multi-modal data fusion’s impact on model"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "efficacy amidst dataset\nimbalances."
        },
        {
          "performance shifts across different configurations, we reveal": "The data presented in Table II provide a detailed analysis of"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "performance differences across models using various modality"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "integrations. A key\nobservation\nis\nthat models\nbased\non\na"
        },
        {
          "performance shifts across different configurations, we reveal": "single modality can achieve high accuracy, up to 0.93, but"
        },
        {
          "performance shifts across different configurations, we reveal": "often at\nthe expense of other metrics like precision, recall, and"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "F1-score, which hover around 0.55. This indicates a potential"
        },
        {
          "performance shifts across different configurations, we reveal": "trade-off between maximizing accuracy and ensuring balanced"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "performance across all metrics. Examining models with dual"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "modalities\nreveals different\nimpacts on performance metrics."
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "For example, while the LSTM model experiences a slight drop"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "in accuracy with dual-modality integration, other architectures"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "show improvements. This variation highlights\nthe limitations"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "of\nrelying solely on a\nsingle modality, which, despite high"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "accuracy, may not\nfully capture a broader\nrange of evaluative"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "measures. Conversely,\nincorporating\nan\nadditional modality"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "significantly enhances precision,\nrecall,\nand F1-scores, with"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "increases\nranging from 12% to 62%. This\nstrongly supports"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "the notion that integrating multiple modalities enhances model"
        },
        {
          "performance shifts across different configurations, we reveal": ""
        },
        {
          "performance shifts across different configurations, we reveal": "robustness. Specifically, segregating input features into distinct"
        },
        {
          "performance shifts across different configurations, we reveal": "spatial\nand sEMG modalities,\nrather\nthan combining all 70"
        },
        {
          "performance shifts across different configurations, we reveal": "features, proves\nto be\na\nstrategic\nadvantage. This\napproach"
        },
        {
          "performance shifts across different configurations, we reveal": "improves the model’s ability to identify and evaluate protective"
        },
        {
          "performance shifts across different configurations, we reveal": "behaviors,\nresulting in better precision and sensitivity."
        },
        {
          "performance shifts across different configurations, we reveal": "The\nanalysis\ndelineates\nthat while models\nbased\non\na"
        },
        {
          "performance shifts across different configurations, we reveal": "singular modality bypass\nthe\ncomplexities of decision-layer"
        },
        {
          "performance shifts across different configurations, we reveal": "weighting,\nrendering the\nchoice between statistical\nand av-"
        },
        {
          "performance shifts across different configurations, we reveal": "erage weighting irrelevant,\nthe introduction of a statistically"
        },
        {
          "performance shifts across different configurations, we reveal": "driven weighting strategy in dual-modality configurations sig-"
        },
        {
          "performance shifts across different configurations, we reveal": "nificantly\nenhances model\nperformance\nby\nleveraging\ndata"
        },
        {
          "performance shifts across different configurations, we reveal": "diversity and modality-specific importance. This strategic use"
        },
        {
          "performance shifts across different configurations, we reveal": "of\nstatistical weighting in models\nintegrating two modalities"
        },
        {
          "performance shifts across different configurations, we reveal": "distinctly outperforms\nsingle-modality models,\naffirming the"
        },
        {
          "performance shifts across different configurations, we reveal": "superiority of a multimodal fusion approach. This research un-"
        },
        {
          "performance shifts across different configurations, we reveal": "derscores the efficacy of partitioning input features into distinct"
        },
        {
          "performance shifts across different configurations, we reveal": "modalities coupled with the judicious use of statistical weight-"
        },
        {
          "performance shifts across different configurations, we reveal": "ing at\nthe decision layer,\nthereby substantially improving the"
        },
        {
          "performance shifts across different configurations, we reveal": "precision and reliability of protective behavior detection.\nIt"
        },
        {
          "performance shifts across different configurations, we reveal": "paves\nthe way\nfor\nsophisticated multimodal\nintegration\nin"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "0.908\n0.642\n0.628\nCNN-Attention+Stat\n(4 mod.)"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "CNN-Attention+Avg (4 mod.)\n0.886\n0.635\n0.603"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": ""
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": ""
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": ""
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "technique guarantees a thorough model evaluation across di-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "verse configurations, notably different modalities delineated by"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "human-centered principles. By scrutinizing the CNN-Attention"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "architecture\nand integrating\nstatistical weighting for\nfeature"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "selection, we methodically investigate how modality fusion"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": ""
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "and statistical weighting influence model effectiveness."
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": ""
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "The\nLeave-One-Out Cross-Validation\n(LOOCV) metrics"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "in Table\nIII\ndemonstrate\nthat\nboth\ndual-modality\nand\nfour-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "modality configurations\nsurpass\nthe\nsingle-modality training"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "approach,\naffirming\nthe\nbenefits\nof\ndata\nheterogeneity\nseg-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "mentation\nand\nhuman-centered modality\nsegmentation. No-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "tably,\nthe CNN-Attention model, when expanded to include"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "two modalities with statistical weighting, shows a significant"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "improvement\nin accuracy from 0.800 to 0.907 and an increase"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "in F1-score from 0.547 to 0.631. This enhancement highlights"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "the clear advantage of\nintegrating multiple data sources over"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "a\nsingle modality\nframework,\nindicating\nthat\nthe\ninclusion"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "of\ndiverse\ndata\ntypes\nenriches\nthe model’s\nperformance\nin"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "predicting outcomes."
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "Furthermore,\ntransitioning from a two-modality to a four-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "modality\nconfiguration\nwith\nstatistical\nweighting\n(CNN-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "Attention+Stat\n(4 mod.)) marginally\nenhances\nthe model’s"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "accuracy to 0.908, maintaining superior\nrecall\n(Rec.) and F1-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "score metrics\ncompared to the\nsingle-modality model. This"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "slight improvement evidences the nuanced benefits of adopting"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "a human-centered approach to modality segmentation, where"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "data\nis\ndivided\ninto\nfour\ndistinct modalities\nbased\non\nits"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "relevance and interaction with human behavioral patterns."
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "Moreover, a comparison between the four-modality configu-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "rations—statistical weighting versus average weighting (CNN-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "Attention+Avg\n(4 mod.))—reveals\na\ndistinct\nadvantage\nfor"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "the former. The model employing statistical weighting (CNN-"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "Attention+Stat (4 mod.)) achieves higher accuracy, recall, and"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "F1-score\nthan\nthe model\nutilizing\naverage weighting, with"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "respective metrics of 0.908, 0.642, and 0.628 against 0.886,"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "0.635, and 0.603. This differential highlights the effectiveness"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "of our\nstatistical\nrelevance weighting strategy, proving it\nto"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "be a more effective method for\nintegrating diverse modalities"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "than merely averaging their contributions."
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": ""
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "In\nsummation,\nthe\nLeave-One-Out\nCross-Validation"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "(LOOCV)\nfindings\nrobustly\nadvocate\nfor\nthe\ndeployment"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "of multimodal\nfusion\nframeworks,\namalgamating\nstatistical"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "correlations and human-centered methodologies. The analysis"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "distinctly highlights\nthe pivotal\nrole of\nstatistical weighting"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "in\naugmenting model\nefficacy.\nIt\nevidences\nthat modality"
        },
        {
          "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)": "segmentation, underpinned by human-centered considerations,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "LEAVE-ONE-OUT CROSS-VALIDATION (LOOCV)"
        },
        {
          "human-centered computing, setting a benchmark for handling": "complex, varied datasets with enhanced accuracy.",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "Adopting a human-centered approach to segment the dataset",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "Model\nAcc.\nRec.\nF1"
        },
        {
          "human-centered computing, setting a benchmark for handling": "into four modalities demonstrates potential enhancements\nin",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "score"
        },
        {
          "human-centered computing, setting a benchmark for handling": "model\nperformance.\nParticularly, models\nemploying CNN-",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "CNN-Attention (1 mod.)\n0.800\n0.631\n0.547"
        },
        {
          "human-centered computing, setting a benchmark for handling": "Attention mechanisms exhibit\nslight but positive differences,",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "0.907\n0.647\n0.631\nCNN-Attention+Stat\n(2 mod.)"
        },
        {
          "human-centered computing, setting a benchmark for handling": "suggesting an improved capability in capturing pain behavior",
          "TABLE III": "0.908\n0.642\n0.628\nCNN-Attention+Stat\n(4 mod.)"
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "CNN-Attention+Avg (4 mod.)\n0.886\n0.635\n0.603"
        },
        {
          "human-centered computing, setting a benchmark for handling": "features. This indicates that modality segmentation, guided by",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "human-centered principles, can amplify model effectiveness.",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "Further\nanalysis\nreveals\nthat models\nemploying\nstatistical",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "weighting generally outperform those using mean weighting,",
          "TABLE III": "technique guarantees a thorough model evaluation across di-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "except\nin the case of LSTM with four modalities employing",
          "TABLE III": "verse configurations, notably different modalities delineated by"
        },
        {
          "human-centered computing, setting a benchmark for handling": "average weighting—a\nscenario\nthat mirrors\nthe\ntrade-offs",
          "TABLE III": "human-centered principles. By scrutinizing the CNN-Attention"
        },
        {
          "human-centered computing, setting a benchmark for handling": "observed in single-modality LSTM models.",
          "TABLE III": "architecture\nand integrating\nstatistical weighting for\nfeature"
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "selection, we methodically investigate how modality fusion"
        },
        {
          "human-centered computing, setting a benchmark for handling": "This investigation emphasizes the advantage of multimodal",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "and statistical weighting influence model effectiveness."
        },
        {
          "human-centered computing, setting a benchmark for handling": "strategies, where\nstrategic\nfeature\ngrouping\nand\nstatistical",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "decision-making markedly elevate model efficacy. Our findings",
          "TABLE III": "The\nLeave-One-Out Cross-Validation\n(LOOCV) metrics"
        },
        {
          "human-centered computing, setting a benchmark for handling": "particularly highlight\nthe utility of attention mechanisms,\nlike",
          "TABLE III": "in Table\nIII\ndemonstrate\nthat\nboth\ndual-modality\nand\nfour-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "CNN-Attention and CNN with Multi-head Self-Attention,\nin a",
          "TABLE III": "modality configurations\nsurpass\nthe\nsingle-modality training"
        },
        {
          "human-centered computing, setting a benchmark for handling": "four-modality framework,\nreinforcing the benefits of human-",
          "TABLE III": "approach,\naffirming\nthe\nbenefits\nof\ndata\nheterogeneity\nseg-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "centered modality segmentation. Overall,\nthe\ntransition to a",
          "TABLE III": "mentation\nand\nhuman-centered modality\nsegmentation. No-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "four-modality model, grounded in human-centered design and",
          "TABLE III": "tably,\nthe CNN-Attention model, when expanded to include"
        },
        {
          "human-centered computing, setting a benchmark for handling": "statistical weighting, is validated as superior to single-modality",
          "TABLE III": "two modalities with statistical weighting, shows a significant"
        },
        {
          "human-centered computing, setting a benchmark for handling": "approaches, bolstering the case for\nsophisticated multimodal",
          "TABLE III": "improvement\nin accuracy from 0.800 to 0.907 and an increase"
        },
        {
          "human-centered computing, setting a benchmark for handling": "fusion in enhancing protective behavior\nrecognition.",
          "TABLE III": "in F1-score from 0.547 to 0.631. This enhancement highlights"
        },
        {
          "human-centered computing, setting a benchmark for handling": "The\nexamination distinctly\nemphasizes\nthe\nsuperiority of",
          "TABLE III": "the clear advantage of\nintegrating multiple data sources over"
        },
        {
          "human-centered computing, setting a benchmark for handling": "statistical weighting\nover\naverage weighting\nin multimodal",
          "TABLE III": "a\nsingle modality\nframework,\nindicating\nthat\nthe\ninclusion"
        },
        {
          "human-centered computing, setting a benchmark for handling": "configurations,\nenhancing model\nperformance. However,\nan",
          "TABLE III": "of\ndiverse\ndata\ntypes\nenriches\nthe model’s\nperformance\nin"
        },
        {
          "human-centered computing, setting a benchmark for handling": "exception was observed in the LSTM model utilizing average",
          "TABLE III": "predicting outcomes."
        },
        {
          "human-centered computing, setting a benchmark for handling": "weighting,\nsuggesting a nuanced balance between accuracy",
          "TABLE III": "Furthermore,\ntransitioning from a two-modality to a four-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "and other metrics. Despite this,\nthe evidence strongly supports",
          "TABLE III": "modality\nconfiguration\nwith\nstatistical\nweighting\n(CNN-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "the\nadvantages\nof\nthe multimodal\napproach\nover\nsingular",
          "TABLE III": "Attention+Stat\n(4 mod.)) marginally\nenhances\nthe model’s"
        },
        {
          "human-centered computing, setting a benchmark for handling": "modality\nframeworks. This finding\nreinforces\nthe\nidea\nthat",
          "TABLE III": "accuracy to 0.908, maintaining superior\nrecall\n(Rec.) and F1-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "strategic\nsegmentation\nof modalities, when\ncombined with",
          "TABLE III": "score metrics\ncompared to the\nsingle-modality model. This"
        },
        {
          "human-centered computing, setting a benchmark for handling": "statistical weighting, markedly advances protective behavior",
          "TABLE III": "slight improvement evidences the nuanced benefits of adopting"
        },
        {
          "human-centered computing, setting a benchmark for handling": "detection models.",
          "TABLE III": "a human-centered approach to modality segmentation, where"
        },
        {
          "human-centered computing, setting a benchmark for handling": "This\nstudy highlights\nthe critical\nimportance of a human-",
          "TABLE III": "data\nis\ndivided\ninto\nfour\ndistinct modalities\nbased\non\nits"
        },
        {
          "human-centered computing, setting a benchmark for handling": "centered modality segmentation strategy and the precise ap-",
          "TABLE III": "relevance and interaction with human behavioral patterns."
        },
        {
          "human-centered computing, setting a benchmark for handling": "plication of\nstatistical methods\nin decision-making processes",
          "TABLE III": "Moreover, a comparison between the four-modality configu-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "to optimize model outcomes. The integration of diverse modal-",
          "TABLE III": "rations—statistical weighting versus average weighting (CNN-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "ities, guided by careful grouping and statistical weighting, not",
          "TABLE III": "Attention+Avg\n(4 mod.))—reveals\na\ndistinct\nadvantage\nfor"
        },
        {
          "human-centered computing, setting a benchmark for handling": "only elevates model performance but also marks a significant",
          "TABLE III": "the former. The model employing statistical weighting (CNN-"
        },
        {
          "human-centered computing, setting a benchmark for handling": "advance\ntowards\ncrafting more nuanced,\ninterpretative mod-",
          "TABLE III": "Attention+Stat (4 mod.)) achieves higher accuracy, recall, and"
        },
        {
          "human-centered computing, setting a benchmark for handling": "els within protective behavior\nrecognition. Consequently,\nthis",
          "TABLE III": "F1-score\nthan\nthe model\nutilizing\naverage weighting, with"
        },
        {
          "human-centered computing, setting a benchmark for handling": "methodological approach not only pushes\nthe boundaries of",
          "TABLE III": "respective metrics of 0.908, 0.642, and 0.628 against 0.886,"
        },
        {
          "human-centered computing, setting a benchmark for handling": "multimodal data\nfusion but\nalso underlines\nthe necessity of",
          "TABLE III": "0.635, and 0.603. This differential highlights the effectiveness"
        },
        {
          "human-centered computing, setting a benchmark for handling": "integrating human-centric perspectives and statistical\ninsights",
          "TABLE III": "of our\nstatistical\nrelevance weighting strategy, proving it\nto"
        },
        {
          "human-centered computing, setting a benchmark for handling": "into the broader narrative of human-centered computing.",
          "TABLE III": "be a more effective method for\nintegrating diverse modalities"
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "than merely averaging their contributions."
        },
        {
          "human-centered computing, setting a benchmark for handling": "A. Cross Validation",
          "TABLE III": ""
        },
        {
          "human-centered computing, setting a benchmark for handling": "",
          "TABLE III": "In\nsummation,\nthe\nLeave-One-Out\nCross-Validation"
        },
        {
          "human-centered computing, setting a benchmark for handling": "From the\nexperimental metrics\nin\nTable\nII,\nthe CNN-",
          "TABLE III": "(LOOCV)\nfindings\nrobustly\nadvocate\nfor\nthe\ndeployment"
        },
        {
          "human-centered computing, setting a benchmark for handling": "Attention model\ndemonstrates\nrobust\nperformance\non\nthe",
          "TABLE III": "of multimodal\nfusion\nframeworks,\namalgamating\nstatistical"
        },
        {
          "human-centered computing, setting a benchmark for handling": "Emopain dataset. To substantiate the efficacy of our approach,",
          "TABLE III": "correlations and human-centered methodologies. The analysis"
        },
        {
          "human-centered computing, setting a benchmark for handling": "we adopt a Leave-One-Out Cross-Validation (LOOCV) strat-",
          "TABLE III": "distinctly highlights\nthe pivotal\nrole of\nstatistical weighting"
        },
        {
          "human-centered computing, setting a benchmark for handling": "egy, meticulously testing each dataset instance as an individual",
          "TABLE III": "in\naugmenting model\nefficacy.\nIt\nevidences\nthat modality"
        },
        {
          "human-centered computing, setting a benchmark for handling": "test case, with the remaining data serving for\ntraining. This",
          "TABLE III": "segmentation, underpinned by human-centered considerations,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "not only contributes positively but that the strategic application": "of\nstatistical weighting\nacross\nsuch\nsegmented modalities",
          "seeing more innovative applications based on this framework,": "ultimately realising more intelligent and humanised interactive"
        },
        {
          "not only contributes positively but that the strategic application": "markedly\noptimizes\nmodel\nperformance.\nThis\napproach",
          "seeing more innovative applications based on this framework,": "systems. This integrated approach opens up new possibilities"
        },
        {
          "not only contributes positively but that the strategic application": "signifies\na\nnotable\nprogression\nin\nprotective\nbehavior",
          "seeing more innovative applications based on this framework,": "for\nfuture\nhuman-machine\ncollaboration, with\nthe\npotential"
        },
        {
          "not only contributes positively but that the strategic application": "detection,\nestablishing\nthe\nprofound\nimpact\nof\nintegrating",
          "seeing more innovative applications based on this framework,": "to\nplay\na\ncrucial\nrole\nin\nvarious\ncomplex\nhuman-centred"
        },
        {
          "not only contributes positively but that the strategic application": "statistical\ninsights with human-centered design principles on",
          "seeing more innovative applications based on this framework,": "computing tasks."
        },
        {
          "not only contributes positively but that the strategic application": "enhancing computational models within this domain.",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "VI. CONCLUSION"
        },
        {
          "not only contributes positively but that the strategic application": "V. DISCUSSION",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "This\nresearch venture has\nsystematically unveiled the\nef-"
        },
        {
          "not only contributes positively but that the strategic application": "This research introduces an innovative approach to affective",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "ficacy of\nintegrating statistical methods and human-centered"
        },
        {
          "not only contributes positively but that the strategic application": "computing and multimodal data fusion by integrating statis-",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "perspectives within the ambit of multimodal pain behaviours"
        },
        {
          "not only contributes positively but that the strategic application": "tical methods with\nhuman-centered\ncomputation. Our find-",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "recognition, employing an array of deep learning architectures"
        },
        {
          "not only contributes positively but that the strategic application": "ings\nindicate\nthat\nstatistical\nalgorithms\ncan more\neffectively",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "including convolutional neural networks\n(CNN),\nlong short-"
        },
        {
          "not only contributes positively but that the strategic application": "extract data representations across various modalities, which",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "term memory networks (LSTM), CNN-Attention networks and"
        },
        {
          "not only contributes positively but that the strategic application": "enhances our understanding and modeling of human behavior.",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "CNN with Multi-head Self Attention. The cornerstone of our"
        },
        {
          "not only contributes positively but that the strategic application": "Traditional machine learning methods often struggle with the",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "exploration was to enhance the precision and utility of complex"
        },
        {
          "not only contributes positively but that the strategic application": "complexity\nof\nhuman\nbehavior\nand\nthe\ninterplay\nbetween",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "pain\nrecognition\nendeavors\nthrough\nthe\nlens\nof\nstatistical"
        },
        {
          "not only contributes positively but that the strategic application": "different\nfactors\n[24]. However,\nour\napproach\nsuccessfully",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "",
          "seeing more innovative applications based on this framework,": "relevance and human-centered modality segmentation."
        },
        {
          "not only contributes positively but that the strategic application": "identifies key features\nlinked to pain behavior by combining",
          "seeing more innovative applications based on this framework,": ""
        },
        {
          "not only contributes positively but that the strategic application": "statistical correlations with human expert knowledge. This not",
          "seeing more innovative applications based on this framework,": "The\nincorporation\nof\nstatistically\ncorrelated\nvote weights"
        },
        {
          "not only contributes positively but that the strategic application": "only enhances model\nperformance but\nalso\nimproves\ninter-",
          "seeing more innovative applications based on this framework,": "and a human-centered approach to data segmentation stands"
        },
        {
          "not only contributes positively but that the strategic application": "pretability, providing valuable insights for the development of",
          "seeing more innovative applications based on this framework,": "as\nthis\nstudy’s central\ninnovation. This methodology signifi-"
        },
        {
          "not only contributes positively but that the strategic application": "future interactive intelligent systems [25].",
          "seeing more innovative applications based on this framework,": "cantly enhances model performance and paves the way for a"
        },
        {
          "not only contributes positively but that the strategic application": "The\nstudy underscores\nthe\nimportance of data-driven sta-",
          "seeing more innovative applications based on this framework,": "deeper understanding and interpretation of multimodal data."
        },
        {
          "not only contributes positively but that the strategic application": "tistical methods in pain recognition representation extraction,",
          "seeing more innovative applications based on this framework,": "It signals a shift\ntowards choosing optimal classifiers for each"
        },
        {
          "not only contributes positively but that the strategic application": "particularly in situations where complete reliance on patient",
          "seeing more innovative applications based on this framework,": "modality, refining our voting strategy for the ultimate decision-"
        },
        {
          "not only contributes positively but that the strategic application": "self-reporting is not\nfeasible. While self-reporting is a crucial",
          "seeing more innovative applications based on this framework,": "making process. Considering the diversity and weak correla-"
        },
        {
          "not only contributes positively but that the strategic application": "means of pain recognition,\nit\nis limited by subjectivity, com-",
          "seeing more innovative applications based on this framework,": "tions among modalities,\nselecting a classifier\ntailored to the"
        },
        {
          "not only contributes positively but that the strategic application": "munication barriers, psychological factors, sociocultural\ninflu-",
          "seeing more innovative applications based on this framework,": "characteristics of each modality is crucial. This sophisticated"
        },
        {
          "not only contributes positively but that the strategic application": "ences, and feedback time delays [26]. Feature extraction based",
          "seeing more innovative applications based on this framework,": "strategy highlights\nthe importance of a customized approach"
        },
        {
          "not only contributes positively but that the strategic application": "on statistical\ncorrelations provides\nan objective,\ncontinuous,",
          "seeing more innovative applications based on this framework,": "to modality-specific model\nselection,\nthereby\nboosting\nthe"
        },
        {
          "not only contributes positively but that the strategic application": "and non-invasive method for pain representation extraction,",
          "seeing more innovative applications based on this framework,": "efficacy of multimodal\nfusion."
        },
        {
          "not only contributes positively but that the strategic application": "complementing\nthe\nlimitations\nof\nself-reporting. This\ndata-",
          "seeing more innovative applications based on this framework,": "Furthermore,\nthe research integrates\nstatistical\nsignificance"
        },
        {
          "not only contributes positively but that the strategic application": "driven feature extraction can assist healthcare professionals in",
          "seeing more innovative applications based on this framework,": "with a human-centered perspective, paving the way for\nex-"
        },
        {
          "not only contributes positively but that the strategic application": "more accurately assessing patients’ pain conditions, especially",
          "seeing more innovative applications based on this framework,": "plainable AI\nin pain recognition. This\napproach delves\ninto"
        },
        {
          "not only contributes positively but that the strategic application": "when patients are unable to self-report\n[27].",
          "seeing more innovative applications based on this framework,": "the distinct\nimpact of each data modality on model outcomes,"
        },
        {
          "not only contributes positively but that the strategic application": "Furthermore,\ngiven\nthe\nsubstantial\nvariability\nin\nhuman",
          "seeing more innovative applications based on this framework,": "promoting a model of explainability that not only clarifies the"
        },
        {
          "not only contributes positively but that the strategic application": "samples,\npersonalisation\nhas\nlong\nbeen\na\npotential\nbarrier",
          "seeing more innovative applications based on this framework,": "workings of complex models but also enhances\ntheir adapt-"
        },
        {
          "not only contributes positively but that the strategic application": "to digital healthcare. However, by utilising modality feature",
          "seeing more innovative applications based on this framework,": "ability and accuracy. This shift highlights our methodology’s"
        },
        {
          "not only contributes positively but that the strategic application": "extraction driven by statistical\ncorrelations, our\nsystem can",
          "seeing more innovative applications based on this framework,": "broad applicability, not only in pain management but across"
        },
        {
          "not only contributes positively but that the strategic application": "capture unique pain expression patterns\nfor each individual,",
          "seeing more innovative applications based on this framework,": "diverse domains of human-centered computing, spotlighting its"
        },
        {
          "not only contributes positively but that the strategic application": "contributing to more precise and effective treatment strategies.",
          "seeing more innovative applications based on this framework,": "potential\nto revolutionize how we interact with and leverage"
        },
        {
          "not only contributes positively but that the strategic application": "This approach not only has the potential\nto improve patients’",
          "seeing more innovative applications based on this framework,": "AI\ntechnologies."
        },
        {
          "not only contributes positively but that the strategic application": "quality of\nlife but may also reduce\nthe\nrisk of drug abuse,",
          "seeing more innovative applications based on this framework,": "In summary,\nthis research not only pioneers the integration"
        },
        {
          "not only contributes positively but that the strategic application": "particularly in the management of chronic pain [28].",
          "seeing more innovative applications based on this framework,": "of\nstatistical\ncorrelations with\nhuman-centered methods\nfor"
        },
        {
          "not only contributes positively but that the strategic application": "Finally,\nthe statistical correlation-driven multimodal\nfusion",
          "seeing more innovative applications based on this framework,": "multimodal data fusion in pain recognition but also advances"
        },
        {
          "not only contributes positively but that the strategic application": "framework and its\nrepresentation learning approach that we",
          "seeing more innovative applications based on this framework,": "the\nfield\nby\noffering\nnovel\ninsights\ninto modality\nfusion"
        },
        {
          "not only contributes positively but that the strategic application": "have\ndemonstrated\npossess\nextensive\napplication\npotential,",
          "seeing more innovative applications based on this framework,": "strategies.\nIt\nenhances\nthe\ndiscussion\non\nimproving model"
        },
        {
          "not only contributes positively but that the strategic application": "with\nthe\npossibility\nof\nfurther\nextension\nto\nother\ndomains",
          "seeing more innovative applications based on this framework,": "performance and interpretability within human-centered com-"
        },
        {
          "not only contributes positively but that the strategic application": "involving complex human-centred computing. This\nresearch",
          "seeing more innovative applications based on this framework,": "puting. The comprehensive framework established for multi-"
        },
        {
          "not only contributes positively but that the strategic application": "provides a viable framework for integrating statistical methods,",
          "seeing more innovative applications based on this framework,": "modal\nfusion application spotlights the potential of statistical"
        },
        {
          "not only contributes positively but that the strategic application": "machine\nlearning,\nand human expertise\nto address\ncomplex",
          "seeing more innovative applications based on this framework,": "insights for model explainability, fostering trust in AI systems."
        },
        {
          "not only contributes positively but that the strategic application": "challenges\nof\nhuman-computer\ninteraction. This\ninterdisci-",
          "seeing more innovative applications based on this framework,": "Consequently,\nthis work paves the way for\ntransformative ap-"
        },
        {
          "not only contributes positively but that the strategic application": "plinary approach not only enhances technical performance but",
          "seeing more innovative applications based on this framework,": "plications in AI systems closely aligned with human needs and"
        },
        {
          "not only contributes positively but that the strategic application": "also augments its credibility and acceptability in practical ap-",
          "seeing more innovative applications based on this framework,": "behaviors, promising significant advancements across various"
        },
        {
          "not only contributes positively but that the strategic application": "plications. As technology continues to advance, we anticipate",
          "seeing more innovative applications based on this framework,": "human-focused domains."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "We’d like to thank Prof. Nadia Berthouze and Dr. Chuang"
        },
        {
          "ETHICAL IMPACT STATEMENT": "This\nresearch\nintersects\naffective\ncomputing, multimodal",
          "ACKNOWLEDGMENT": "Yu for helpful\nsuggestion. This\nresearch is\nfully supporteds"
        },
        {
          "ETHICAL IMPACT STATEMENT": "data\nanalysis,\nand\npain\nrecognition,\naiming\nto\nenhance\nthe",
          "ACKNOWLEDGMENT": "by Holistic AI and partially supported by University College"
        },
        {
          "ETHICAL IMPACT STATEMENT": "precision\nand\npersonalization\nof\npain management\nthrough",
          "ACKNOWLEDGMENT": "London Interaction Center, University College London, UK."
        },
        {
          "ETHICAL IMPACT STATEMENT": "statistical methods. We conscientiously address\nthe potential",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "REFERENCES"
        },
        {
          "ETHICAL IMPACT STATEMENT": "societal, environmental, and ethical\nimpacts,\nfocusing on as-",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "pects\nsuch as\nexplainability,\ntransparency,\nliability,\nfairness,",
          "ACKNOWLEDGMENT": "[1] R. W. Picard, Affective computing.\nMIT press, 2000."
        },
        {
          "ETHICAL IMPACT STATEMENT": "efficacy,\nrobustness, privacy, security, and sustainability.",
          "ACKNOWLEDGMENT": "[2]\nS. M. S. A. Abdullah, S. Y. A. Ameen, M. A. Sadeeq, and S. Zeebaree,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "Journal\nof\n“Multimodal\nemotion\nrecognition\nusing\ndeep\nlearning,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "a) Explainability and Transparency: Our study is com-",
          "ACKNOWLEDGMENT": "Applied Science and Technology Trends, vol. 2, no. 02, pp. 52–58, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "mitted\nto\nensuring\nthat\nadvancements\nin\npain\nrecognition",
          "ACKNOWLEDGMENT": "[3]\nJ. B. Wade, D. D. Price, R. M. Hamer, S. M. Schwartz, and R. P. Hart,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "“An emotional component analysis of chronic pain,” Pain, vol. 40, no. 3,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "are both explainable and transparent. We leverage statistical",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "pp. 303–310, 1990."
        },
        {
          "ETHICAL IMPACT STATEMENT": "models and human-centered perspectives\nto allow users and",
          "ACKNOWLEDGMENT": "[4]\nT. Olugbade, N. Bianchi-Berthouze,\nand A. C. d. C. Williams,\n“The"
        },
        {
          "ETHICAL IMPACT STATEMENT": "stakeholders\nto understand the\nrationale behind our predic-",
          "ACKNOWLEDGMENT": "relationship between guarding, pain, and emotion,” Pain reports, vol. 4,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "no. 4, 2019."
        },
        {
          "ETHICAL IMPACT STATEMENT": "tions. This\nfosters\ntrust\nand facilitates deeper understanding",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[5] Y. Wang, W. Song, W. Tao, A. Liotta, D. Yang, X. Li, S. Gao, Y. Sun,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "and acceptance of our\ntechnology in clinical settings.",
          "ACKNOWLEDGMENT": "W. Ge, W. Zhang et al., “A systematic review on affective computing:"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "Emotion models, databases, and recent advances,” Information Fusion,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "b) Liability: Acknowledging the profound implications",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "vol. 83, pp. 19–52, 2022."
        },
        {
          "ETHICAL IMPACT STATEMENT": "of our work in enhancing pain recognition, we recognize our",
          "ACKNOWLEDGMENT": "life of\n[6]\nL. F. Barrett, How emotions are made: The secret\nthe brain.\nPan"
        },
        {
          "ETHICAL IMPACT STATEMENT": "responsibility\nto\nensure\nthe\nreliability\nand\naccuracy\nof\nour",
          "ACKNOWLEDGMENT": "Macmillan, 2017."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[7]\nT. Khera and V. Rangasamy, “Cognition and pain: a review,” Frontiers"
        },
        {
          "ETHICAL IMPACT STATEMENT": "findings. Misinterpretations or inaccuracies could significantly",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "in psychology, vol. 12, p. 1819, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "impact patient\ncare\nand treatment outcomes. Therefore, we",
          "ACKNOWLEDGMENT": "[8] A. B. Arrieta, N. D´ıaz-Rodr´ıguez,\nJ. Del Ser, A. Bennetot, S. Tabik,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "adhere to stringent validation protocols, underpinned by eth-",
          "ACKNOWLEDGMENT": "A. Barbado, S. Garc´ıa, S. Gil-L´opez, D. Molina, R. Benjamins et al.,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "“Explainable artificial intelligence (xai): Concepts, taxonomies, opportu-"
        },
        {
          "ETHICAL IMPACT STATEMENT": "ical\nconsiderations,\nto mitigate potential\nrisks\nand liabilities",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "nities and challenges toward responsible ai,” Information fusion, vol. 58,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "associated with our\nresearch outputs.",
          "ACKNOWLEDGMENT": "pp. 82–115, 2020."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[9] M. S. Salekin, G. Zamzmi, D. Goldgof, P. R. Mouton, K.\nJ. Anand,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "c) Fairness: Our\nresearch actively addresses\nthe\nissue",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "T. Ashmeade, S. Prescott, Y. Huang, and Y. Sun, “Attentional generative"
        },
        {
          "ETHICAL IMPACT STATEMENT": "of\nfairness\nby\nincorporating\nthe EmoPain\ndataset\n[21]\nthat",
          "ACKNOWLEDGMENT": "multimodal\nnetwork\nfor\nneonatal\npostoperative\npain\nestimation,”\nin"
        },
        {
          "ETHICAL IMPACT STATEMENT": "includes chronic pain participants and healthy individuals. This",
          "ACKNOWLEDGMENT": "International Conference on Medical\nImage Computing and Computer-"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "Assisted Intervention.\nSpringer, 2022, pp. 749–759."
        },
        {
          "ETHICAL IMPACT STATEMENT": "diversity ensures our models do not\ninadvertently perpetuate",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[10]\nJ. Gao, P. Li, Z. Chen, and J. Zhang, “A survey on deep learning for"
        },
        {
          "ETHICAL IMPACT STATEMENT": "biases against certain demographics. By integrating a human-",
          "ACKNOWLEDGMENT": "multimodal data fusion,” Neural Computation, vol. 32, no. 5, pp. 829–"
        },
        {
          "ETHICAL IMPACT STATEMENT": "centered\napproach, we\nactively\nseek\nto\nunderstand\nand\nin-",
          "ACKNOWLEDGMENT": "864, 2020."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[11] A. H. Tuttle, M. J. Molinaro, J. F. Jethwa, S. G. Sotocinal, J. C. Prieto,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "corporate diverse pain expressions\nand experiences,\nthereby",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "M. A. Styner,\nJ. S. Mogil, and M.\nJ. Zylka, “A deep neural network"
        },
        {
          "ETHICAL IMPACT STATEMENT": "mitigating the risk of biases that could otherwise compromise",
          "ACKNOWLEDGMENT": "to assess\nspontaneous pain from mouse facial expressions,” Molecular"
        },
        {
          "ETHICAL IMPACT STATEMENT": "the fairness of our models.",
          "ACKNOWLEDGMENT": "pain, vol. 14, p. 1744806918763658, 2018."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[12] C. Ambroz, A. Scott, A. Ambroz,\nand E. O. Talbott,\n“Chronic\nlow"
        },
        {
          "ETHICAL IMPACT STATEMENT": "d) Efficacy and Robustness:\nThe\nintersection of\nstatis-",
          "ACKNOWLEDGMENT": "Journal\nof\nback\npain\nassessment\nusing\nsurface\nelectromyography,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "tical methods and human-centered design in our work aims",
          "ACKNOWLEDGMENT": "occupational and environmental medicine, vol. 42, no. 6, pp. 660–669,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "2000."
        },
        {
          "ETHICAL IMPACT STATEMENT": "to\nenhance\nthe\nefficacy\nand\nrobustness\nof\npain\nrecognition",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[13] G. Gilam, J. J. Gross, T. D. Wager, F. J. Keefe, and S. C. Mackey, “What"
        },
        {
          "ETHICAL IMPACT STATEMENT": "technologies. Our approach ensures\nthat our models are not",
          "ACKNOWLEDGMENT": "is the relationship between pain and emotion? bridging constructs and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "only accurate but also resilient to the complexities and variabil-",
          "ACKNOWLEDGMENT": "communities,” Neuron, vol. 107, no. 1, pp. 17–21, 2020."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[14] M. A. Haque, R. B. Bautista, F. Noroozi, K. Kulkarni, C. B. Laursen,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "ities\ninherent\nin human pain experiences,\nthereby supporting",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "R.\nIrani, M. Bellantonio, S. Escalera, G. Anbarjafari, K. Nasrollahi"
        },
        {
          "ETHICAL IMPACT STATEMENT": "reliable pain management practices.",
          "ACKNOWLEDGMENT": "et al., “Deep multimodal pain recognition: a database and comparison"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "of spatio-temporal visual modalities,” in 2018 13th IEEE International"
        },
        {
          "ETHICAL IMPACT STATEMENT": "e) Privacy and Security: The sensitivity of health-related",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "Conference\non Automatic Face & Gesture Recognition\n(FG 2018)."
        },
        {
          "ETHICAL IMPACT STATEMENT": "data necessitates stringent privacy and security measures. Our",
          "ACKNOWLEDGMENT": "IEEE, 2018, pp. 250–257."
        },
        {
          "ETHICAL IMPACT STATEMENT": "research select\nthe EmoPain dataset\n[21]\nthat adheres\nto the",
          "ACKNOWLEDGMENT": "[15]\nP. Werner, A. Al-Hamadi, R. Niese, S. Walter, S. Gruss,\nand H. C."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "Traue, “Automatic pain recognition from video and biomedical signals,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "highest\nstandards of data protection, ensuring all participant",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "in 2014 22nd international conference on pattern recognition.\nIEEE,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "data\nis\nanonymized\nand\nsecurely\nstored. Access\ncontrols,",
          "ACKNOWLEDGMENT": "2014, pp. 4582–4587."
        },
        {
          "ETHICAL IMPACT STATEMENT": "encryption, and ethical data handling practices protect against",
          "ACKNOWLEDGMENT": "[16]\nT. A. Olugbade, M. H. Aung, N. Bianchi-Berthouze, N. Marquardt, and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "A. C. Williams,\n“Bi-modal detection of painful\nreaching for\nchronic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "unauthorized access and data breaches.",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "the 16th international\npain rehabilitation systems,”\nin Proceedings of"
        },
        {
          "ETHICAL IMPACT STATEMENT": "In conclusion, our work not only signifies a step forward",
          "ACKNOWLEDGMENT": "conference on multimodal\ninteraction, 2014, pp. 455–458."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[17] C. Wang, T. A. Olugbade, A. Mathur, A. C. D. C. Williams, N. D. Lane,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "in the technical domain but also embodies a comprehensive",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "and N. Bianchi-Berthouze, “Chronic pain protective behavior detection"
        },
        {
          "ETHICAL IMPACT STATEMENT": "ethical\napproach. Through\nour\ncommitment\nto\nethical\ncon-",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "with deep learning,” ACM Transactions on Computing for Healthcare,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "siderations, we aspire to contribute meaningfully to the field",
          "ACKNOWLEDGMENT": "vol. 2, no. 3, pp. 1–24, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "[18] G. Cen, C. Wang, T. A. Olugbade, A. C. d. C. Williams, and N. Bianchi-"
        },
        {
          "ETHICAL IMPACT STATEMENT": "of affective computing, ensuring that our\ninnovations in pain",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "Berthouze,\n“Exploring multimodal\nfusion\nfor\ncontinuous\nprotective"
        },
        {
          "ETHICAL IMPACT STATEMENT": "recognition are\nresponsible,\nequitable,\nand beneficial\nfor\nall",
          "ACKNOWLEDGMENT": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "ACKNOWLEDGMENT": "behavior detection,” in 2022 10th International Conference on Affective"
        },
        {
          "ETHICAL IMPACT STATEMENT": "stakeholders involved.",
          "ACKNOWLEDGMENT": "Computing and Intelligent\nInteraction (ACII).\nIEEE, 2022, pp. 1–8."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "“Movement\nrepresentation learning for pain level classification,” IEEE"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "Transactions on Affective Computing, 2023."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[20] C. Wang, Y. Gao, A. Mathur, A. C. De C. Williams, N. D. Lane,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "and N. Bianchi-Berthouze,\n“Leveraging activity recognition to enable"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "the\nprotective behavior detection in continuous data,” Proceedings of"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "vol. 5, no. 2, pp. 1–27, 2021."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[21] M. S. Aung, S. Kaltwang, B. Romera-Paredes, B. Martinez, A. Singh,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "M. Cella, M. Valstar, H. Meng, A. Kemp, M. Shafizadeh et al., “The"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "automatic detection of\nchronic pain-related expression:\nrequirements,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "challenges and the multimodal emopain dataset,” IEEE transactions on"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "affective computing, vol. 7, no. 4, pp. 435–451, 2015."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[22] C. Wang, Y. Gao, A. Mathur, A. C. De C. Williams, N. D. Lane,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "and N. Bianchi-Berthouze,\n“Leveraging activity recognition to enable"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "protective behavior detection in continuous data,” Proc. ACM Interact."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "Mob. Wearable Ubiquitous Technol., vol. 5, no. 2,\njun 2021.\n[Online]."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "Available: https://doi.org/10.1145/3463508"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[23] C. Wang, M. Peng, T. A. Olugbade, N. D. Lane, A. C. D. C. Williams,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "and N. Bianchi-Berthouze, “Learning temporal and bodily attention in"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "protective movement behavior detection,” in 2019 8th International Con-"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "ference on Affective Computing and Intelligent\nInteraction Workshops"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "and Demos (ACIIW).\nIEEE, 2019, pp. 324–330."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[24] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "no. 7553, pp. 436–444, 2015."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[25] A. Holzinger, “From machine learning to explainable ai,” in 2018 world"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "symposium on digital\nintelligence\nfor\nsystems and machines\n(DISA)."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "IEEE, 2018, pp. 55–66."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[26] M. A. Hanson, H. C. Powell\nJr, A. T. Barth, K. Ringgenberg, B. H."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "Calhoun,\nJ. H. Aylor,\nand\nJ.\nLach,\n“Body\narea\nsensor\nnetworks:"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "Challenges\nand opportunities,” Computer, vol. 42, no. 1, pp. 58–65,"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "2009."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[27]\nT. B. Moeslund, A. Hilton, and V. Kr¨uger, “A survey of advances\nin"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "vision-based human motion capture and analysis,” Computer vision and"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "image understanding, vol. 104, no. 2-3, pp. 90–126, 2006."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "[28] R. H. Dworkin, D. C. Turk, J. T. Farrar, J. A. Haythornthwaite, M. P."
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "Jensen, N. P. Katz, R. D. Kerns, G. Stucki, R. R. Allen, N. Bellamy"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "et al., “Core outcome measures for chronic pain clinical\ntrials: Immpact"
        },
        {
          "[19]\nT. Olugbade, A. C. de C Williams, N. Gold, and N. Bianchi-Berthouze,": "recommendations,” pain, vol. 113, no. 1, pp. 9–19, 2005."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "S Abdullah",
        "S Ameen",
        "M Sadeeq",
        "S Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "3",
      "title": "An emotional component analysis of chronic pain",
      "authors": [
        "J Wade",
        "D Price",
        "R Hamer",
        "S Schwartz",
        "R Hart"
      ],
      "year": "1990",
      "venue": "Pain"
    },
    {
      "citation_id": "4",
      "title": "The relationship between guarding, pain, and emotion",
      "authors": [
        "T Olugbade",
        "N Bianchi-Berthouze",
        "A Williams"
      ],
      "year": "2019",
      "venue": "Pain reports"
    },
    {
      "citation_id": "5",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "7",
      "title": "Cognition and pain: a review",
      "authors": [
        "T Khera",
        "V Rangasamy"
      ],
      "year": "2021",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "8",
      "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai",
      "authors": [
        "A Arrieta",
        "N Díaz-Rodríguez",
        "J Del",
        "A Ser",
        "S Bennetot",
        "A Tabik",
        "S Barbado",
        "S García",
        "D Gil-López",
        "R Molina",
        "Benjamins"
      ],
      "year": "2020",
      "venue": "Information fusion"
    },
    {
      "citation_id": "9",
      "title": "Attentional generative multimodal network for neonatal postoperative pain estimation",
      "authors": [
        "M Salekin",
        "G Zamzmi",
        "D Goldgof",
        "P Mouton",
        "K Anand",
        "T Ashmeade",
        "S Prescott",
        "Y Huang",
        "Y Sun"
      ],
      "year": "2022",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention"
    },
    {
      "citation_id": "10",
      "title": "A survey on deep learning for multimodal data fusion",
      "authors": [
        "J Gao",
        "P Li",
        "Z Chen",
        "J Zhang"
      ],
      "year": "2020",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "11",
      "title": "A deep neural network to assess spontaneous pain from mouse facial expressions",
      "authors": [
        "A Tuttle",
        "M Molinaro",
        "J Jethwa",
        "S Sotocinal",
        "J Prieto",
        "M Styner",
        "J Mogil",
        "M Zylka"
      ],
      "year": "2018",
      "venue": "Molecular pain"
    },
    {
      "citation_id": "12",
      "title": "Chronic low back pain assessment using surface electromyography",
      "authors": [
        "C Ambroz",
        "A Scott",
        "A Ambroz",
        "E Talbott"
      ],
      "year": "2000",
      "venue": "Journal of occupational and environmental medicine"
    },
    {
      "citation_id": "13",
      "title": "What is the relationship between pain and emotion? bridging constructs and communities",
      "authors": [
        "G Gilam",
        "J Gross",
        "T Wager",
        "F Keefe",
        "S Mackey"
      ],
      "year": "2020",
      "venue": "Neuron"
    },
    {
      "citation_id": "14",
      "title": "Deep multimodal pain recognition: a database and comparison of spatio-temporal visual modalities",
      "authors": [
        "M Haque",
        "R Bautista",
        "F Noroozi",
        "K Kulkarni",
        "C Laursen",
        "R Irani",
        "M Bellantonio",
        "S Escalera",
        "G Anbarjafari",
        "K Nasrollahi"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "15",
      "title": "Automatic pain recognition from video and biomedical signals",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "R Niese",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2014",
      "venue": "Automatic pain recognition from video and biomedical signals"
    },
    {
      "citation_id": "16",
      "title": "Bi-modal detection of painful reaching for chronic pain rehabilitation systems",
      "authors": [
        "T Olugbade",
        "M Aung",
        "N Bianchi-Berthouze",
        "N Marquardt",
        "A Williams"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th international conference on multimodal interaction"
    },
    {
      "citation_id": "17",
      "title": "Chronic pain protective behavior detection with deep learning",
      "authors": [
        "C Wang",
        "T Olugbade",
        "A Mathur",
        "A Williams",
        "N Lane",
        "N Bianchi-Berthouze"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Computing for Healthcare"
    },
    {
      "citation_id": "18",
      "title": "Exploring multimodal fusion for continuous protective behavior detection",
      "authors": [
        "G Cen",
        "C Wang",
        "T Olugbade",
        "A Williams",
        "N Bianchi-Berthouze"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Movement representation learning for pain level classification",
      "authors": [
        "T Olugbade",
        "A De C Williams",
        "N Gold",
        "N Bianchi-Berthouze"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Leveraging activity recognition to enable protective behavior detection in continuous data",
      "authors": [
        "C Wang",
        "Y Gao",
        "A Mathur",
        "A De",
        "C Williams",
        "N Lane",
        "N Bianchi-Berthouze"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "21",
      "title": "The automatic detection of chronic pain-related expression: requirements, challenges and the multimodal emopain dataset",
      "authors": [
        "M Aung",
        "S Kaltwang",
        "B Romera-Paredes",
        "B Martinez",
        "A Singh",
        "M Cella",
        "M Valstar",
        "H Meng",
        "A Kemp",
        "M Shafizadeh"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "Leveraging activity recognition to enable protective behavior detection in continuous data",
      "authors": [
        "C Wang",
        "Y Gao",
        "A Mathur",
        "A De",
        "C Williams",
        "N Lane",
        "N Bianchi-Berthouze"
      ],
      "year": "2021",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
      "doi": "10.1145/3463508"
    },
    {
      "citation_id": "23",
      "title": "Learning temporal and bodily attention in protective movement behavior detection",
      "authors": [
        "C Wang",
        "M Peng",
        "T Olugbade",
        "N Lane",
        "A Williams",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "24",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "25",
      "title": "in 2018 world symposium on digital intelligence for systems and machines (DISA)",
      "authors": [
        "A Holzinger"
      ],
      "year": "2018",
      "venue": "in 2018 world symposium on digital intelligence for systems and machines (DISA)"
    },
    {
      "citation_id": "26",
      "title": "Body area sensor networks: Challenges and opportunities",
      "authors": [
        "M Hanson",
        "H Powell",
        "A Barth",
        "K Ringgenberg",
        "B Calhoun",
        "J Aylor",
        "J Lach"
      ],
      "year": "2009",
      "venue": "Computer"
    },
    {
      "citation_id": "27",
      "title": "A survey of advances in vision-based human motion capture and analysis",
      "authors": [
        "T Moeslund",
        "A Hilton",
        "V Krüger"
      ],
      "year": "2006",
      "venue": "Computer vision and image understanding"
    },
    {
      "citation_id": "28",
      "title": "Core outcome measures for chronic pain clinical trials: Immpact recommendations",
      "authors": [
        "R Dworkin",
        "D Turk",
        "J Farrar",
        "J Haythornthwaite",
        "M Jensen",
        "N Katz",
        "R Kerns",
        "G Stucki",
        "R Allen",
        "N Bellamy"
      ],
      "year": "2005",
      "venue": "pain"
    }
  ]
}