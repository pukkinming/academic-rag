{
  "paper_id": "2203.14867v1",
  "title": "Continuous Metric Learning For Transferable Speech Emotion Recognition And Embedding Across Low-Resource Languages",
  "published": "2022-03-28T16:14:41Z",
  "authors": [
    "Sneha Das",
    "Nicklas Leander Lund",
    "Nicole Nadine Lønfeldt",
    "Anne Katrine Pagsberg",
    "Line H. Clemmensen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) refers to the technique of inferring the emotional state of an individual from speech signals. SERs continue to garner interest due to their wide applicability. Although the domain is mainly founded on signal processing, machine learning, and deep learning, generalizing over languages continues to remain a challenge. However, developing generalizable and transferable models are critical due to a lack of sufficient resources in terms of data and labels for languages beyond the most commonly spoken ones. To improve performance over languages, we propose a denoising autoencoder with semi-supervision using a continuous metric loss based on either activation or valence. The novelty of this work lies in our proposal of continuous metric learning, which is among the first proposals on the topic to the best of our knowledge. Furthermore, to address the lack of activation and valence labels in the transfer datasets, we annotate the signal samples with activation and valence levels corresponding to a dimensional model of emotions, which were then used to evaluate the quality of the embedding over the transfer datasets 1 . We show that the proposed semi-supervised model consistently outperforms the baseline unsupervised method, which is a conventional denoising autoencoder, in terms of emotion classification accuracy as well as correlation with respect to the dimensional variables. Further evaluation of classification accuracy with respect to the reference, a BERT based speech representation model, shows that the proposed method is comparable to the reference method in classifying specific emotion",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is the process of inferring the emotional state from speech signals. The domain has found applicability in diverse fields, in healthcare in the detection of disorders, risk assessment in criminal justice system, monitoring the attentiveness of students in school, etc.\n\nMethods for SER have evolved from solely signal processing and conventional machine learning methods to using more advanced deep learning. Notable methods employed the use of hidden Markov models and Gaussian mixture models for emotion classification. Support vector machines have been a widely used tool for SER, either as a standalone tool or in tandem with other methods to improve classification  [26] . Furthermore, advanced machine learning methods including deep learning models have recently shown promise for SER. This includes the use of CNNs, RNNs and LSTMs  [30] .\n\nAn enduring struggle of SER models is their ability to generalize over languages. This is addressed by using supervision when the languages are supported with sufficient resources, both in data and labels. However, supervised learning is inapplicable to languages beyond the commonly spoken ones, due to insufficient resources in terms of small data sets and few-to-no labels. Therefore, to cater to low-resource languages, it is necessary to develop methods that generalize better over languages. The associated challenge is the subjectivity in emotion perception and classification. For instance, the perception of speech signals demonstrating a neutral emotion can vary between languages, owing to phonetic and cultural differences. Despite this, most models use class labels to train models, instead of employing a more universal and continuous metric like activation and valence from the dimensional model of emotions  [25] .\n\nLatent representation methods, like the autoencoder (AE), can compress the input features to a smaller and ideally more target relevant latent embedding and are commonly employed in various applications, for instance biosignal processing, computer-vision, and speech-processing  [33] . Furthermore, these methods are relatively more interpretable. This is a favourable characteristic as the decision making process of the model is more transparent thereby enabling smoother deployment of such systems in medical and clinical setup. Therefore, latent representation methods can also be useful in the modelling of emotions from speech signals, such that the models retain only the emotion-relevant paralinguistic content from the speech signal. This may also lead to better knowledge transfer between data sets by transferring only generic emotion representations to unseen languages and corpora and not the syntactical variations between languages. Transferable emotion representations aid in addressing the acute label shortage for SER tasks in under-resourced languages.\n\nIn this paper, we use the denoising autoencoder (DAE) to obtain a highly compressed emotion embedding that is more consistent over different languages. Additionally, we strive to keep the system relatively simple and interpretable as the target application of this work is clinical psychiatry. While a",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Input-Layer",
      "text": "Output-layer Bottleneck conventional DAE will learn a compressed latent space representative of the input features, it is not necessary that the model learns to discriminate between the desired emotions or emotion representation. Therefore, we propose to use semi-supervision to direct the DAE to learn factors relevant to emotion discrimination like activation and valence, wherein the semi-supervision is provided by the activation and valence labels. We address the following questions in the process: 1. How to provide semi-supervision to preserve the label distances between data samples in the latent space, where the labels span a continuous space. In contrast, most works on metric learning, like triplet loss and contrastive loss are discrete in nature. 2. How to validate the embedding quality in the absence of dimensional labels (activation and valence) for the transfer datasets considered in this paper. Therefore, the contributions of this work are: 1. We propose a method for continuous metric learning to order the samples in the latent space. To the best of our knowledge, this is one of the few methods addressing continuous metric learning. 2. We annotate the transfer datasets with the activation and valence labels that we use to validate the effectiveness of the proposed methods over languages. The labels will be shared for research purposes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Relation To Prior Work",
      "text": "Autoencoders and variants: DAE was one of the earliest deep learning based unsupervised learning techniques for SER  [31] . This was followed by the use of sparse AE for feature transfer  [7]  and for SER on spontaneous data sets  [8] . Furthermore, end-to-end representation learning for affect recognition from speech was proposed and showed performance comparable to existing methods  [13] . In recent years, techniques like variational and adversarial AEs and adversarial variational Bayes have been exploited to learn the latent representations of speech emotions with input features ranging from the raw signals to hand crafted features  [20, 24, 9, 23] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Metric Learning In Ser:",
      "text": "In a recent paper on SER, the authors proposed a convolutional autoencoder that employs a pre-trained autoencoder and a convolutional neural network, and a triplet loss in the autoencoder is used for metric learning  [12] . Further on, a contrastive loss was used for metric learning in a Siamese network  [21] . On similar lines, a class specific triplet-loss based LSTM neural network was proposed for SER  [16] . A combination of the centre-loss, that clusters members of a class together, and the cross-entropy loss was investigated to better cluster features  [5, 22] . The authors proposed multiple f-similarity preserving losses for metric learning using soft labels and tested it on classification  [34] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "Dimensional Model of Emotion: We consider the circumplex model of emotion, wherein Russell et al proposed that emotions can be represented in a circular space, wherein the x-axis corresponds to valence, and y-axis corresponds to the activation  [25] , as illustrated in Fig.  1 . Activation refers to how arousing an emotion is and valence refers to the level of positivity in the emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Denoising Autoencoder With Continuous Metric Learning",
      "text": "We represent the learning function in a denoising autoencoder as:\n\nwhere x and x n are the clean and partially noisy feature vectors, f θ represents the encoder, g φ the decoder, and L rec is the reconstruction loss  [28] . It is known to be more robust than the AE, since it is designed to learn a subspace of the clean feature vectors from a noisy input, therefore learning the characteristics of the desired input. Let z ∈ R b×2 be the latent space embedding, and l ∈ R b×1 be the label we are modelling. Then\n\n, where d(a 1 ,a 2 ) is the Euclidean distance between the vectors a 1 and a 2 , and z d is the distance between two latent samples. Similarly,\n\n, is the distance between the labels of the data samples, and i=1,2,...,(b-1). We assume that z d is a linear function of l d such that ẑd =pl d ; we obtain the optimum p by minimizing the squared error z d -ẑ d 2 2 , which yields:\n\nAs motivated in the Sec. 1, our goal is to obtain a latent space through training, wherein the distance between the embedding of two samples is close to the distance between the labels of the corresponding samples. We can achieve this by 1. obtaining a slope that is close to one, and 2. minimizing the residual between z d and ẑd . Therefore, the loss factor corresponding to the slope between z d and l d is given as:\n\nwhere a 1 ,a 2 are two arbitrary data instances. Furthermore, the residual component is the standard mean squared error:\n\nTherefore, our proposed total metric loss is composed of:\n\nThe final equation for the DAE with metric learning is given as: argmin\n\nwhere L rec has the same formulation as in Eq. 1  To obtain an insight on the effectiveness of the proposed metric loss, we study the correlation between z d and l d by using ordinary least squares (OLS) to model l d as a function of z d as follows:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pair-Wise Distance Preservation",
      "text": "The assumption of a linear relation between the pair-wise point distances in the latent space and the labels is motivated from our proposed metric loss. The R 2 adjusted is presented in Table  1 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this section, we describe the dataset and the input feature space, the architecture of the models, the pre-processing steps followed by the description of the methods of comparison and our evaluation setup.\n\nDatasets and input features: IEMOCAP, an audio-visual affect data set, is used to train and validate the models  [4] . The data set comprises of annotations representing both the categorical and dimensional emotional model  [2] . The models are trained with data from the emotional categories neutral (N), sad (S), happy (H), angry (A). We use the extended Geneva minimalistic acoustic parameter set (eGeMAPS)  [10] , specifically the functionals of lower-level features. Each speech sample yields a feature vector comprising of 88 features and we use the OpenSmile toolkit to extract the features  [11] . To study how the latent representations are transferred between corpora and languages, we test classification accuracy over the following transfer datasets : 1. the Surrey Audio-Visual Expresses Emotion (SAVEE) database that is primarily English and consists of male speakers only, 2. the Berlin Database of Emotional Speech (Emo-DB) recorded in German and, 3. the Canadian French Emotional (CAFE) speech database comprising of French audio samples  [17, 3, 14] , 4. the URDU-dataset consisting of Urdu speech, and 5. the Acted Emotional Speech Dynamic (AESD) Database comprising of Greek speech  [19, 29] . Note that AESD does not comprise of speech samples from the neutral class, whereby evaluation includes AESD speech samples from three and two classes only. For correlation analysis, due to the lack of activation and valence labels, we annotate the following datasets: 1. EMO-DB, 2. URDU 3. AESD and 4. CAFE. These labels are then used to test the baseline DAE-unsupervised, and the proposed DAE-metric-act and DAE-metric-val models.\n\nSystem architecture: Past works using AEs and variants have mostly addressed novel network architectures for better classification accuracy  [8, 20, 23, 24, 31] . However, since the focus of this work is to investigate the potential of metric learning in obtaining a more transferable embedding space for emotion recognition, we employ a simple architecture for the DAE baseline with performance that is comparable to existing methods. The proposed methods share the same architecture as the baseline model, illustrated in Fig.  2 . The size of the input features is 88 and we compress the latent space to two dimensions. As described above, the input feature vector is comprised of the descriptive statistics of each speech signal, without temporal correlation. Therefore, in the encoder and decoder we employ fully connected NNs, instead of convolutional or recurrent NNs. The fully connected layers are followed by rectified linear units (ReLU) to incorporate non-linearity within the model.\n\nPreprocessing: Prior to using the data sets for training and testing, we remove the outliers by computing the z-score and eliminating the data samples that have a z-score, -10 > z > 10. We chose a threshold of 10 instead of the standard value of 3 because the goal of this work is to understand the behavior of the models for both typical and atypical rendition of emotions in speech. Therefore, we only remove the extreme outliers. For run-time evaluation over the transfer datasets, we employ statistics from 20% of the transfer data set to standardize the remaining 80%.\n\nProposed methods: We train and validate the following models: 1. DAE-Unsupervised is used as a baseline, 2. DAE-Metric-Act that utilizes the proposed semi-supervision via the activation labels from Sec. 3.1, and 3. DAE-Metric-Val utilizes the proposed semi-supervision via the valence labels (Sec. 3.1). The input features to the DAE is corrupted by a noise component, x input =x true +N and N ∈N (0,1). We use the mean squared error (MSE) to optimize the baseline model as shown in Eq: 1. To study the consistency of the results, 5-fold cross-validation is used on the IEMO-CAP database while the transfer data sets are identical over the iterations and are used for model testing only. The models were trained over 50 epochs with a batch size of 64, and we used the Adam optimizer with the learning rate set to 1e-3. Additional methods used for reference are listed in the following section.\n\nReference methods: In addition to comparing the performance of the DAE models with the proposed metric loss to the unsupervised DAE, we employ the following as reference methods to gauge the relative performance of the models developed in this work:\n\n1. Since the DAE-unsupervised model is employed as a baseline to evaluate the effectiveness of the metric-loss, we compared its classification performance with similar methods from literature as presented in Table  2 .\n\n2. We implemented the support vector classifier, SVC, trained separately on all data sets towards the downstream task of classifying the input eGeMAPS features into target classes. The results are shown in Table  4 . We use the supervised SVC to study the upper-bound of the performance limits of the SER task. However, the work in this paper is focused on enabling reliable SER for languages with few or no labelled data, and supervised learning is therefore inapplicable within this scope.\n\n3. We used the SUPERB: Speech processing Universal PERformance Benchmark for emotion classification on the transfer datasets  [32] . The method employs the HuBERT (Hidden-Unit BERT), specifically hubertlarge-ll60k model, as the base model to first extract speech representations  [15] . The speech representations are then given as input to a linear downstream task to classify the speech signal into emotions. The base model is trained on English data sets and the linear downstream model for emotion classification is trained on the IEMOCAP dataset. We employ the model for classification only as the model is specifically trained for that. The results are shown in Table  4 . Relative to the complexity of the models developed in this paper (<4×10 2 parameters), note that the considered model is highly complex (>3×10 8 parameters).\n\n4. Lastly, besides comparing the correlation between the embedding and the labels for the proposed models and the unsupervised model, we employ supervision to train models with the proposed metric losses. In other words, we use the labels from the transfer data sets in the metric loss during training. We do this to obtain insights on the performance limits of the proposed method and the results are presented in Table  3  as metric-act (supervised) and metric-val (supervised).\n\nExperiments for evaluation: To investigate the efficacy of the proposed methods, in the following parts we investigate the quality of the latent embedding by using them as the input features for classification and correlation downstream tasks. Towards that, we evaluate the models in terms of 1. the correlation coefficient, and 2. the classification accuracy.\n\n1. Correlation analysis : We study the correlation between the latent dimensions and dimensional variables (activation, valence). With the main focus to study the effectiveness of the proposed continuous metric learning method using the dimensional variables, we evaluate how large a proportion of the labels can be explained by the latent dimensions. Therefore, as described in Sec. 3.2, we compute the correlation coefficients between z and l, via the adjusted R 2 . The mean and standard deviation over 5-folds for R 2 for the models are shown in Table  3 ; Note that the R 2 , where the p-value of the F-statistic >0.05, are indicated by an asterisk. Models with larger R 2 are more effective in preserving the distance between samples in the embedding space with respect to the dimensional variables.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Classification Of Emotion Classes :",
      "text": "We inspect the performance of the methods by classifying the speech samples into emotional categories using the support vector classifier (SVC) with a linear kernel. For evaluation, we use balanced accuracy for the 4-class (N-S-H-A) and 3-class (N-S-A) scenarios to account for imbalanced classes. Furthermore, supervised training on eGeMAPS using SVC and the SUPERB model are employed as references, as described above. The results are presented in Table  4 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Method",
      "text": "Features+Dataset classes Accuracy GAN  [18]  eGeMAPS  [10] +EMO-DB 2 66% (UAR) FLUDA  [1]  IS10  [27] +IEMOCAP(+) 4 50% (UA) VAE+LSTM  [20]  LogMel+IEMOCAP 4 56.08% (UA) AE+LSTM  [20]  LogMel+IEMOCAP 4 55.42% (UA) Stacked-AE+BLSTM-RNN  [13]  COVAREP+IEMOCAP  [6]  4 50.26% (UA) DAE+Linear-SVM (baseline) eGeMAPS+IEMOCAP 4 52.09% (UA)\n\nTable  2 : Performance in terms of unweighted accuracy (UA) and unweighted average recall (UAR) of the reference methods (from cited papers) and the unsupervised baseline model developed in this paper.\n\nIn Table . 2, we list current models similar to the DAE-unsupervised baseline in terms of the architecture, network size and input-output format. We observe that the performance of the developed unsupervised model is comparable to state of the art. Therefore, we consider the DAE+Linear-SVM model a reasonable baseline to gauge the performance of the proposed metric-loss in this paper.\n\nCorrelation analysis: From Table  3 , we observe that the mean and standard deviation of R 2 is consistently higher for the proposed methods relative to the unsupervised baseline, over the transfer data sets. However, DAE-unsupervised is as good as DAE-metric-act in terms of the correlation between z and the activation for EMO-DB. With supervision, as anticipated metric-act and metric-val models seem to have a higher R 2 relative to the unsupervised and semi-supervised models, specifically for the activation variable. To summarize the observations, the proposed methods show higher adjusted R 2 between the modeled z and the label considered. However, we observe that R 2 corresponding to the valence variable is lower than the activation variable. An unaccounted non-linear relation between valence variable and the latent space could be a reason for the observation. Nevertheless, both metric-act and metric-val seem to effectively order and arrange data samples in the latent space, relative to the activation label. This is also evident from Fig.  3 , wherein we can observe that the samples in the latent space are better distributed for metric-act and metric-val then for the unsupervised model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Classification Of Emotion Classes:",
      "text": "We observe that for both 3-and 4-class scenarios, the proposed methods have higher accuracy than the baseline unsupervised method. Furthermore, while SUPERB outperforms all the methods for IEMOCAP and SAVEE, it seems to have lower accuracy than the baseline and the proposed models for the remaining data sets, specifically for the 3-class classification task. Lastly, although classification accuracy of the supervised SVC is superior and implies that there is class-discriminating information in the data sets, how much of that information corresponds to the paralinguistic aspects of speech and emotion is worth investigating. The balanced accuracy scores (Table  4 ) for metric-val and metric-act is consistently better than the unsupervised baseline over the transfer datasets, indicating that incorporating a distance preservation metric in the loss function aids in shaping the distribution of features in the latent space that is more consistent over languages. It is also interesting to note that while metric-val did not show a large difference in R 2 with respect to the baseline (Table  1 ), in terms of classification accuracy   it is often better than metric-act. Furthermore, the proposed models are unable to effectively differentiate between anger and happy, whereby its performance drops in 4-class classification. A similar trend was observed for the reference SUPERB model (lower 3class classification accuracy), despite its much higher complexity. This indicates that more complex models do not necessary lead to learning meaningful representations of the more subtle aspects of speech.\n\nEffectiveness of metric loss function: From Table 1 we observe that metric-act, i.e., the DAE trained with semi-supervision from the activation labels of the IEMOCAP dataset, shows the highest adjusted R 2 value between l d and z d , l d corresponding to the activation labels. This suggests that the proposed formulation of the continuous metric loss (Eq. 5) is effective when activation labels are employed for semi-supervision. In contrast, for metric-val wherein l d corresponds to valence, the R 2 value is similar to the baseline. A potential reason for this could be that the relation between valence and the embedding is inherently non-linear. This implies that different approaches are necessary to model valence and activation variables. Investigating and including aspects of the true relation between valence, activation and the embedding will be addressed in future work. In conclusion, we can state that using the dimensional variables, activation and valence, to learn emotion representations yield embeddings that are more transferable to unseen datasets and new languages. Furthermore, the proposed continuous metric loss with semi-supervision enables us to incorporate information on the dimensional variables within the model, hence aiding transferability.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we proposed a method for continuous metric learning, such that the difference between two feature points is continuous. We apply the method for speech emotion recognition, wherein we model a DAE that is semi-supervised using the activation and valence labels and the continuous metric loss. Our results show that the embedding from the proposed method is generally more consistent and thereby more transferable to different languages. The proposed methods are evaluated in terms of classification performance, and the proposed models outperform the baseline method on all the transfer datasets. Furthermore, to investigate the correspondence of the latent space to activation and valence variables, we compute the adjusted R 2 that indicates how much variation in the labels can be explained by a linear combination of the latent space. While the R 2 with respect to valence is generally lower than that for activation, between the methods, the proposed models outperform the baseline method for the datasets. Addressing the lack of labels corresponding to activation and valence variables in the transfer dataset, the annotated transfer dataset is the second contribution of this work.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the circumplex dimensional",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of the (denoising) autoencoder",
      "page": 2
    },
    {
      "caption": "Figure 1: Activation refers to how",
      "page": 3
    },
    {
      "caption": "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)",
      "page": 4
    },
    {
      "caption": "Figure 2: The size of the input features is 88 and we compress",
      "page": 5
    },
    {
      "caption": "Figure 3: , wherein we can observe that the samples",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Activation"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "High"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Angry"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Happy"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Negative\nPositive\nValence\nNeutral"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Sad"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Low"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Figure 1:\nIllustration of the circumplex dimensional"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "model of emotion [25]."
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "classes at a much lower complexity."
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "1\nIntroduction"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": ""
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Speech emotion recognition (SER)\nis\nthe process"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "of\ninferring the emotional state from speech signals."
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "The domain has found applicability in diverse fields,"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "in\nhealthcare\nin\nthe\ndetection\nof\ndisorders,\nrisk"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "assessment in criminal justice system, monitoring the"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "attentiveness of students in school, etc."
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "Methods for SER have evolved from solely signal pro-"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "cessing and conventional machine learning methods to"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "using more advanced deep learning. Notable methods"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "employed the use of hidden Markov models and Gaus-"
        },
        {
          "Sneha Das1, Nicklas Leander Lund1,": "sian mixture models for emotion classification. Support"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "methods to improve classification [26]. Furthermore,"
        },
        {
          "either as a standalone tool or in tandem with other": "advanced machine learning methods including deep"
        },
        {
          "either as a standalone tool or in tandem with other": "learning models have recently shown promise for SER."
        },
        {
          "either as a standalone tool or in tandem with other": "This includes the use of CNNs, RNNs and LSTMs\n[30]."
        },
        {
          "either as a standalone tool or in tandem with other": "An enduring struggle of SER models is their ability"
        },
        {
          "either as a standalone tool or in tandem with other": "to generalize over\nlanguages.\nThis\nis addressed by"
        },
        {
          "either as a standalone tool or in tandem with other": "using supervision when the languages are supported"
        },
        {
          "either as a standalone tool or in tandem with other": "with sufficient\nresources, both in data and labels."
        },
        {
          "either as a standalone tool or in tandem with other": "However,\nsupervised\nlearning\nis\ninapplicable\nto"
        },
        {
          "either as a standalone tool or in tandem with other": "languages beyond the commonly spoken ones, due to"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "insufficient resources in terms of small data sets and"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "few-to-no labels. Therefore, to cater to low-resource"
        },
        {
          "either as a standalone tool or in tandem with other": "languages,\nit\nis necessary to develop methods\nthat"
        },
        {
          "either as a standalone tool or in tandem with other": "generalize better\nover\nlanguages.\nThe\nassociated"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "challenge is the subjectivity in emotion perception and"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "classification. For instance, the perception of speech"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "signals demonstrating a neutral\nemotion can vary"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "between languages, owing to phonetic and cultural"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "differences. Despite this, most models use class labels"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "to train models,\ninstead of employing a more universal"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "and continuous metric\nlike activation and valence"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "from the dimensional model of emotions [25]."
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "Latent\nrepresentation methods,\nlike\nthe autoen-"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "coder\n(AE),\ncan\ncompress\nthe\ninput\nfeatures\nto"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "a\nsmaller\nand ideally more\ntarget\nrelevant\nlatent"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "embedding and are commonly employed in various"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "applications,\nfor\ninstance\nbiosignal\nprocessing,"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "computer-vision, and speech-processing [33]. Further-"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "more, these methods are relatively more interpretable."
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "This\nis a favourable\ncharacteristic as\nthe decision"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "making process of\nthe model\nis more\ntransparent"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "thereby\nenabling\nsmoother\ndeployment\nof\nsuch"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "systems\nin medical and clinical\nsetup.\nTherefore,"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "latent representation methods can also be useful in the"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "modelling of emotions from speech signals, such that"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "the models retain only the emotion-relevant paralin-"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "guistic content from the speech signal. This may also"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "lead to better knowledge transfer between data sets by"
        },
        {
          "either as a standalone tool or in tandem with other": ""
        },
        {
          "either as a standalone tool or in tandem with other": "transferring only generic emotion representations to"
        },
        {
          "either as a standalone tool or in tandem with other": "unseen languages and corpora and not the syntactical"
        },
        {
          "either as a standalone tool or in tandem with other": "variations between languages. Transferable emotion"
        },
        {
          "either as a standalone tool or in tandem with other": "representations\naid\nin\naddressing\nthe\nacute\nlabel"
        },
        {
          "either as a standalone tool or in tandem with other": "shortage for SER tasks in under-resourced languages."
        },
        {
          "either as a standalone tool or in tandem with other": "In\nthis\npaper,\nwe\nuse\nthe\ndenoising\nautoen-"
        },
        {
          "either as a standalone tool or in tandem with other": "coder (DAE) to obtain a highly compressed emotion"
        },
        {
          "either as a standalone tool or in tandem with other": "embedding\nthat\nis more\nconsistent\nover\ndifferent"
        },
        {
          "either as a standalone tool or in tandem with other": "languages. Additionally, we strive to keep the system"
        },
        {
          "either as a standalone tool or in tandem with other": "relatively\nsimple\nand\ninterpretable\nas\nthe\ntarget"
        },
        {
          "either as a standalone tool or in tandem with other": "application of this work is clinical psychiatry. While a"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Adjusted squared correlation coefficient",
      "data": [
        {
          "comparable to existing methods [13].\nIn recent years,": "techniques like variational and adversarial AEs and",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "d(a1,a2) is the Euclidean distance between the vectors"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "adversarial variational Bayes have been exploited to",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "a1 and a2, and zd is the distance between two latent"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "learn the latent\nrepresentations of\nspeech emotions",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "samples.\nSimilarly,\nis the\nld = d(li,li+1) ∈ R(b−1)×1,"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "with input features ranging from the raw signals to",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "distance between the labels of the data samples, and"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "hand crafted features [20, 24, 9, 23].",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "i=1,2,...,(b−1). We assume that zd is a linear function"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "of ld such that ˆzd =pld; we obtain the optimum p by"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "Metric\nlearning\nin\nSER:\nIn\na\nrecent\npaper",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "minimizing the squared error (cid:107)zd−ˆzd (cid:107)2\n2, which yields:"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "on\nSER,\nthe\nauthors\nproposed\na\nconvolutional",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "(2)\np=(ld\nT ld)−1ld\nT zd"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "autoencoder that employs a pre-trained autoencoder",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "and a convolutional neural network, and a triplet loss",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "As motivated in the Sec. 1, our goal\nis to obtain"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "in the autoencoder\nis used for metric learning [12].",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "a latent space through training, wherein the distance"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "Further on, a contrastive loss was used for metric",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "between the embedding of two samples is close to the"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "learning in a Siamese network [21]. On similar lines, a",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "distance between the labels of the corresponding sam-"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "class specific triplet-loss based LSTM neural network",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "ples. We can achieve this by 1. obtaining a slope that"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "was proposed for SER [16].\nA combination of\nthe",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "is close to one, and 2. minimizing the residual between"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "centre-loss, that clusters members of a class together,",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "zd and ˆzd. Therefore, the loss factor corresponding"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "and the cross-entropy loss was investigated to better",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "to the slope between zd and ld is given as:"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "cluster features [5, 22]. The authors proposed multiple",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "f-similarity preserving losses for metric learning using",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "zd(a1)−ˆzd(a2)"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "soft labels and tested it on classification [34].",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "(cid:13)(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)(cid:13)\n,\n(3)\n−1\nLsl ="
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "ld(a1)−ld(a2)\n2"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "where a1,a2 are two arbitrary data instances. Further-"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "3\nMethodology",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "more, the residual component is the standard mean"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "squared error:"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "Dimensional Model of Emotion: We\nconsider",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "the\ncircumplex model of\nemotion, wherein Russell",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "(4)\nLres =E(cid:107)zd−ˆzd(cid:107)2\n2."
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "et al proposed that emotions can be represented in",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "a circular space, wherein the x-axis corresponds to",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "Therefore, our proposed total metric loss is composed"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "valence, and y-axis corresponds to the activation [25],",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "of:"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "as\nillustrated in Fig. 1.\nActivation refers\nto how",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "(5)\nLmet =Lres+Lsl"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "arousing an emotion is and valence refers to the level",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "of positivity in the emotion.",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "The final equation for the DAE with metric learning"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "is given as:"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "argmin\nLrec+Lmet,"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "(6)"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "3.1\nDenoising\nautoencoder with con-",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": "fθ,gφ"
        },
        {
          "comparable to existing methods [13].\nIn recent years,": "tinuous metric learning",
          "such that\nzd = d(zi, zi+1),\nzd ∈ R(b−1)×1, where": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "over five datasets, color-coded by activation levels. The models have been trained on IEMOCAP only."
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "To obtain an insight on the effectiveness of the pro-"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "posed metric loss, we study the correlation between zd"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "and ld by using ordinary least squares (OLS) to model"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "ld as a function of zd as follows:\nld =c+β1zd1 +β2zd2."
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "The\nassumption of\na\nlinear\nrelation between the"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "pair-wise point distances in the latent space and the"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "labels\nis motivated from our proposed metric\nloss."
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "The R2 adjusted is presented in Table 1."
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "4\nExperimental setup"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "In this section, we describe the dataset and the input"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "feature\nspace,\nthe architecture of\nthe models,\nthe"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "pre-processing steps followed by the description of the"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "methods of comparison and our evaluation setup."
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": ""
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "Datasets\nand\ninput\nfeatures:\nIEMOCAP,\nan"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "audio-visual affect data set, is used to train and validate"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "the models [4]. The data set comprises of annotations"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "representing both the\ncategorical and dimensional"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "emotional model [2]. The models are trained with data"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "from the emotional categories neutral (N), sad (S),"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "happy (H), angry (A). We use the extended Geneva"
        },
        {
          "Figure 3: Latent embedding for DAE-unsupervised (Top), DAE-metric-act (Middle) and DAE-metric-val (Lower)": "minimalistic acoustic parameter set (eGeMAPS) [10],"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "System architecture:\nPast works using AEs and": "variants have mostly addressed novel network architec-",
          "Reference methods:\nIn addition to comparing the": "performance of the DAE models with the proposed"
        },
        {
          "System architecture:\nPast works using AEs and": "tures for better classification accuracy [8, 20, 23, 24, 31].",
          "Reference methods:\nIn addition to comparing the": "metric loss to the unsupervised DAE, we employ the"
        },
        {
          "System architecture:\nPast works using AEs and": "However, since the focus of this work is to investigate",
          "Reference methods:\nIn addition to comparing the": "following as reference methods to gauge the relative"
        },
        {
          "System architecture:\nPast works using AEs and": "the potential of metric learning in obtaining a more",
          "Reference methods:\nIn addition to comparing the": "performance of the models developed in this work:"
        },
        {
          "System architecture:\nPast works using AEs and": "transferable embedding space for emotion recognition,",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "1.\nSince the DAE-unsupervised model\nis employed"
        },
        {
          "System architecture:\nPast works using AEs and": "we employ a simple architecture for the DAE baseline",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "as\na baseline\nto\nevaluate\nthe\neffectiveness\nof\nthe"
        },
        {
          "System architecture:\nPast works using AEs and": "with\nperformance\nthat\nis\ncomparable\nto\nexisting",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "metric-loss, we compared its classification performance"
        },
        {
          "System architecture:\nPast works using AEs and": "methods.\nThe proposed methods\nshare\nthe\nsame",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "with similar methods from literature as presented in"
        },
        {
          "System architecture:\nPast works using AEs and": "architecture as the baseline model, illustrated in Fig. 2.",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "Table 2."
        },
        {
          "System architecture:\nPast works using AEs and": "The size of the input features is 88 and we compress",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "the latent\nspace to two dimensions.\nAs described",
          "Reference methods:\nIn addition to comparing the": "2. We\nimplemented the\nsupport vector\nclassifier,"
        },
        {
          "System architecture:\nPast works using AEs and": "above,\nthe input feature vector is comprised of the",
          "Reference methods:\nIn addition to comparing the": "SVC, trained separately on all data sets towards the"
        },
        {
          "System architecture:\nPast works using AEs and": "descriptive statistics of each speech signal, without",
          "Reference methods:\nIn addition to comparing the": "downstream task of classifying the input eGeMAPS"
        },
        {
          "System architecture:\nPast works using AEs and": "temporal correlation. Therefore,\nin the encoder and",
          "Reference methods:\nIn addition to comparing the": "features\ninto target classes. The results are shown"
        },
        {
          "System architecture:\nPast works using AEs and": "decoder we employ fully connected NNs,\ninstead of",
          "Reference methods:\nIn addition to comparing the": "in Table 4. We use the supervised SVC to study the"
        },
        {
          "System architecture:\nPast works using AEs and": "convolutional or recurrent NNs. The fully connected",
          "Reference methods:\nIn addition to comparing the": "upper-bound of the performance limits of the SER"
        },
        {
          "System architecture:\nPast works using AEs and": "layers are followed by rectified linear units (ReLU) to",
          "Reference methods:\nIn addition to comparing the": "task. However, the work in this paper is focused on"
        },
        {
          "System architecture:\nPast works using AEs and": "incorporate non-linearity within the model.",
          "Reference methods:\nIn addition to comparing the": "enabling reliable SER for languages with few or no"
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "labelled data,\nand supervised learning is\ntherefore"
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "inapplicable within this scope."
        },
        {
          "System architecture:\nPast works using AEs and": "Preprocessing:\nPrior\nto using the data sets\nfor",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "training and testing, we remove the outliers by comput-",
          "Reference methods:\nIn addition to comparing the": "3. We used the SUPERB: Speech processing Univer-"
        },
        {
          "System architecture:\nPast works using AEs and": "ing the z-score and eliminating the data samples that",
          "Reference methods:\nIn addition to comparing the": "sal PERformance Benchmark for emotion classification"
        },
        {
          "System architecture:\nPast works using AEs and": "have a z-score, −10 > z > 10. We chose a threshold",
          "Reference methods:\nIn addition to comparing the": "on the transfer datasets [32]. The method employs the"
        },
        {
          "System architecture:\nPast works using AEs and": "of 10 instead of the standard value of 3 because the",
          "Reference methods:\nIn addition to comparing the": "HuBERT (Hidden-Unit BERT), specifically hubert-"
        },
        {
          "System architecture:\nPast works using AEs and": "goal of\nthis work is\nto understand the behavior of",
          "Reference methods:\nIn addition to comparing the": "large-ll60k model, as the base model to first extract"
        },
        {
          "System architecture:\nPast works using AEs and": "the models for both typical and atypical rendition of",
          "Reference methods:\nIn addition to comparing the": "speech representations [15]. The speech representa-"
        },
        {
          "System architecture:\nPast works using AEs and": "emotions in speech. Therefore, we only remove the",
          "Reference methods:\nIn addition to comparing the": "tions are then given as input to a linear downstream"
        },
        {
          "System architecture:\nPast works using AEs and": "extreme outliers.\nFor\nrun-time evaluation over\nthe",
          "Reference methods:\nIn addition to comparing the": "task to classify the speech signal\ninto emotions. The"
        },
        {
          "System architecture:\nPast works using AEs and": "transfer datasets, we employ statistics from 20% of the",
          "Reference methods:\nIn addition to comparing the": "base model\nis trained on English data sets and the"
        },
        {
          "System architecture:\nPast works using AEs and": "transfer data set to standardize the remaining 80%.",
          "Reference methods:\nIn addition to comparing the": "linear downstream model\nfor emotion classification"
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "is trained on the IEMOCAP dataset. We employ the"
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "model for classification only as the model is specifically"
        },
        {
          "System architecture:\nPast works using AEs and": "Proposed methods: We train and validate the fol-",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "trained for that. The results are shown in Table 4. Rel-"
        },
        {
          "System architecture:\nPast works using AEs and": "lowing models:\n1. DAE-Unsupervised is used as a",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "ative to the complexity of the models developed in this"
        },
        {
          "System architecture:\nPast works using AEs and": "baseline, 2. DAE-Metric-Act that utilizes the proposed",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "paper (<4×102 parameters), note that the considered"
        },
        {
          "System architecture:\nPast works using AEs and": "semi-supervision via the activation labels from Sec. 3.1,",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "model\nis highly complex (>3×108 parameters)."
        },
        {
          "System architecture:\nPast works using AEs and": "and 3. DAE-Metric-Val utilizes the proposed semi-su-",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "pervision via the valence labels (Sec. 3.1). The input",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "Lastly, besides comparing the correlation between\n4."
        },
        {
          "System architecture:\nPast works using AEs and": "features to the DAE is corrupted by a noise component,",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "the embedding and the labels for the proposed models"
        },
        {
          "System architecture:\nPast works using AEs and": "xinput =xtrue+N and N ∈N (0,1). We use the mean",
          "Reference methods:\nIn addition to comparing the": "and the unsupervised model, we employ supervision to"
        },
        {
          "System architecture:\nPast works using AEs and": "squared error (MSE) to optimize the baseline model",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "train models with the proposed metric losses.\nIn other"
        },
        {
          "System architecture:\nPast works using AEs and": "as shown in Eq: 1. To study the consistency of the",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "words, we use the labels from the transfer data sets in"
        },
        {
          "System architecture:\nPast works using AEs and": "results, 5-fold cross-validation is used on the IEMO-",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "the metric loss during training. We do this to obtain"
        },
        {
          "System architecture:\nPast works using AEs and": "CAP database while the transfer data sets are identical",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "insights on the performance limits of\nthe proposed"
        },
        {
          "System architecture:\nPast works using AEs and": "over the iterations and are used for model testing only.",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "method and the results are presented in Table 3 as"
        },
        {
          "System architecture:\nPast works using AEs and": "The models were trained over 50 epochs with a batch",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "",
          "Reference methods:\nIn addition to comparing the": "metric-act (supervised) and metric-val (supervised)."
        },
        {
          "System architecture:\nPast works using AEs and": "size of 64, and we used the Adam optimizer with the",
          "Reference methods:\nIn addition to comparing the": ""
        },
        {
          "System architecture:\nPast works using AEs and": "learning rate set to 1e-3. Additional methods used for",
          "Reference methods:\nIn addition to comparing the": "Experiments for evaluation:\nTo investigate the"
        },
        {
          "System architecture:\nPast works using AEs and": "reference are listed in the following section.",
          "Reference methods:\nIn addition to comparing the": "efficacy of the proposed methods, in the following parts"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , we observe",
      "data": [
        {
          "we investigate the quality of\nthe latent embedding": "by using them as the input features for classification",
          "unsupervised model\nis comparable to state of the art.": "Therefore, we consider the DAE+Linear-SVM model"
        },
        {
          "we investigate the quality of\nthe latent embedding": "and correlation downstream tasks. Towards that, we",
          "unsupervised model\nis comparable to state of the art.": "a reasonable baseline to gauge the performance of the"
        },
        {
          "we investigate the quality of\nthe latent embedding": "evaluate\nthe models\nin terms of 1.\nthe correlation",
          "unsupervised model\nis comparable to state of the art.": "proposed metric-loss in this paper."
        },
        {
          "we investigate the quality of\nthe latent embedding": "coefficient, and 2. the classification accuracy.",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "Correlation analysis:\nFrom Table 3, we observe"
        },
        {
          "we investigate the quality of\nthe latent embedding": "1. Correlation analysis\n: We study the correla-",
          "unsupervised model\nis comparable to state of the art.": "that the mean and standard deviation of R2 is consis-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "tion between the latent dimensions and dimensional",
          "unsupervised model\nis comparable to state of the art.": "tently higher for the proposed methods relative to the"
        },
        {
          "we investigate the quality of\nthe latent embedding": "variables (activation, valence). With the main focus to",
          "unsupervised model\nis comparable to state of the art.": "unsupervised baseline, over the transfer data sets. How-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "study the effectiveness of the proposed continuous met-",
          "unsupervised model\nis comparable to state of the art.": "ever, DAE-unsupervised is as good as DAE-metric-act"
        },
        {
          "we investigate the quality of\nthe latent embedding": "ric learning method using the dimensional variables,",
          "unsupervised model\nis comparable to state of the art.": "in terms of the correlation between z and the activa-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "we evaluate how large a proportion of the labels can",
          "unsupervised model\nis comparable to state of the art.": "tion for EMO-DB. With supervision, as anticipated"
        },
        {
          "we investigate the quality of\nthe latent embedding": "be explained by the latent dimensions. Therefore, as",
          "unsupervised model\nis comparable to state of the art.": "metric-act and metric-val models seem to have a higher"
        },
        {
          "we investigate the quality of\nthe latent embedding": "described in Sec. 3.2, we compute the correlation coeffi-",
          "unsupervised model\nis comparable to state of the art.": "R2 relative to the unsupervised and semi-supervised"
        },
        {
          "we investigate the quality of\nthe latent embedding": "cients between z and l, via the adjusted R2. The mean",
          "unsupervised model\nis comparable to state of the art.": "models, specifically for the activation variable."
        },
        {
          "we investigate the quality of\nthe latent embedding": "and standard deviation over 5-folds for R2 for the mod-",
          "unsupervised model\nis comparable to state of the art.": "To summarize the observations, the proposed methods"
        },
        {
          "we investigate the quality of\nthe latent embedding": "els are shown in Table 3; Note that the R2, where the",
          "unsupervised model\nis comparable to state of the art.": "show higher adjusted R2 between the modeled z and"
        },
        {
          "we investigate the quality of\nthe latent embedding": "p-value of the F-statistic >0.05, are indicated by an",
          "unsupervised model\nis comparable to state of the art.": "the label considered. However, we observe that R2"
        },
        {
          "we investigate the quality of\nthe latent embedding": "asterisk. Models with larger R2 are more effective in",
          "unsupervised model\nis comparable to state of the art.": "corresponding to the valence variable is lower than"
        },
        {
          "we investigate the quality of\nthe latent embedding": "preserving the distance between samples in the embed-",
          "unsupervised model\nis comparable to state of the art.": "the activation variable. An unaccounted non-linear"
        },
        {
          "we investigate the quality of\nthe latent embedding": "ding space with respect to the dimensional variables.",
          "unsupervised model\nis comparable to state of the art.": "relation between valence variable and the latent space"
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "could be a reason for the observation. Nevertheless,"
        },
        {
          "we investigate the quality of\nthe latent embedding": "2. Classification\nof\nemotion\nclasses\n:\nWe",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "both metric-act and metric-val\nseem to effectively"
        },
        {
          "we investigate the quality of\nthe latent embedding": "inspect the performance of the methods by classifying",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "order and arrange data samples in the latent space,"
        },
        {
          "we investigate the quality of\nthe latent embedding": "the speech samples\ninto emotional categories using",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "relative to the activation label. This is also evident"
        },
        {
          "we investigate the quality of\nthe latent embedding": "the\nsupport\nvector\nclassifier\n(SVC) with a\nlinear",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "from Fig. 3, wherein we can observe that the samples"
        },
        {
          "we investigate the quality of\nthe latent embedding": "kernel. For evaluation, we use balanced accuracy for",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "in the latent space are better distributed for metric-act"
        },
        {
          "we investigate the quality of\nthe latent embedding": "the 4-class (N-S-H-A) and 3-class (N-S-A) scenarios",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "and metric-val then for the unsupervised model."
        },
        {
          "we investigate the quality of\nthe latent embedding": "to\naccount\nfor\nimbalanced\nclasses.\nFurthermore,",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "Classification of\nemotion classes: We observe"
        },
        {
          "we investigate the quality of\nthe latent embedding": "supervised training on eGeMAPS using SVC and",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "that for both 3- and 4-class scenarios, the proposed"
        },
        {
          "we investigate the quality of\nthe latent embedding": "the SUPERB model are employed as references, as",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "methods have higher accuracy than the baseline un-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "described above. The results are presented in Table 4.",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "supervised method. Furthermore, while SUPERB out-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "performs all the methods for IEMOCAP and SAVEE,"
        },
        {
          "we investigate the quality of\nthe latent embedding": "5\nResults and discussion",
          "unsupervised model\nis comparable to state of the art.": "it\nseems\nto have lower accuracy than the baseline"
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "and the proposed models for the remaining data sets,"
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "specifically for the 3-class classification task. Lastly, al-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "Method\nFeatures+Dataset\nclasses\nAccuracy",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "GAN [18]\neGeMAPS [10]+EMO-DB\n2\n66% (UAR)",
          "unsupervised model\nis comparable to state of the art.": "though classification accuracy of the supervised SVC is"
        },
        {
          "we investigate the quality of\nthe latent embedding": "FLUDA [1]\nIS10 [27]+IEMOCAP(+)\n4\n50% (UA)",
          "unsupervised model\nis comparable to state of the art.": "superior and implies that there is class-discriminating"
        },
        {
          "we investigate the quality of\nthe latent embedding": "VAE+LSTM [20]\nLogMel+IEMOCAP\n4\n56.08% (UA)",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "AE+LSTM [20]\nLogMel+IEMOCAP\n4\n55.42% (UA)",
          "unsupervised model\nis comparable to state of the art.": "information in the data sets, how much of that infor-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "Stacked-AE+BLSTM-RNN [13]\nCOVAREP+IEMOCAP [6]\n4\n50.26% (UA)",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "DAE+Linear-SVM (baseline)\neGeMAPS+IEMOCAP\n4\n52.09% (UA)",
          "unsupervised model\nis comparable to state of the art.": "mation corresponds to the paralinguistic aspects of"
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "speech and emotion is worth investigating."
        },
        {
          "we investigate the quality of\nthe latent embedding": "Table\n2:\nPerformance\nin\nterms\nof\nunweighted",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "The balanced accuracy scores (Table 4) for metric-val"
        },
        {
          "we investigate the quality of\nthe latent embedding": "accuracy (UA) and unweighted average recall (UAR)",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "and metric-act is consistently better than the unsuper-"
        },
        {
          "we investigate the quality of\nthe latent embedding": "of the reference methods (from cited papers) and the",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "vised baseline over the transfer datasets, indicating that"
        },
        {
          "we investigate the quality of\nthe latent embedding": "unsupervised baseline model developed in this paper.",
          "unsupervised model\nis comparable to state of the art.": ""
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "incorporating a distance preservation metric in the loss"
        },
        {
          "we investigate the quality of\nthe latent embedding": "",
          "unsupervised model\nis comparable to state of the art.": "function aids in shaping the distribution of\nfeatures in"
        },
        {
          "we investigate the quality of\nthe latent embedding": "In Table.\n2, we\nlist\ncurrent models\nsimilar\nto",
          "unsupervised model\nis comparable to state of the art.": "the latent space that is more consistent over languages."
        },
        {
          "we investigate the quality of\nthe latent embedding": "the DAE-unsupervised\nbaseline\nin\nterms\nof\nthe",
          "unsupervised model\nis comparable to state of the art.": "It is also interesting to note that while metric-val did"
        },
        {
          "we investigate the quality of\nthe latent embedding": "architecture, network size and input-output format.",
          "unsupervised model\nis comparable to state of the art.": "not show a large difference in R2 with respect to the"
        },
        {
          "we investigate the quality of\nthe latent embedding": "We observe that\nthe performance of\nthe developed",
          "unsupervised model\nis comparable to state of the art.": "baseline (Table 1),\nin terms of classification accuracy"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation",
      "data": [
        {
          "IEMOCAP": "",
          "EMO-DB": "",
          "CAFE": "",
          "URDU": "",
          "AESD": ""
        },
        {
          "IEMOCAP": "R2-Val",
          "EMO-DB": "R2-Act",
          "CAFE": "R2-Val",
          "URDU": "R2-Val",
          "AESD": "R2-Val"
        },
        {
          "IEMOCAP": "NA",
          "EMO-DB": "0.38±0.05",
          "CAFE": "0.16±0.01",
          "URDU": "0.15±0.04",
          "AESD": "0.18±0.01"
        },
        {
          "IEMOCAP": "NA",
          "EMO-DB": "0.45±0.03",
          "CAFE": "0.29±0.06",
          "URDU": "0.16±0.04",
          "AESD": "0.17±0.03"
        },
        {
          "IEMOCAP": "0.06±0.02",
          "EMO-DB": "0.63±0.04",
          "CAFE": "0.14±0.02",
          "URDU": "0.14±0.03",
          "AESD": "−0.0±0.0∗"
        },
        {
          "IEMOCAP": "0.05±0.01",
          "EMO-DB": "0.63±0.04",
          "CAFE": "0.13±0.03",
          "URDU": "0.13±0.02",
          "AESD": "−0.0±0.0∗"
        },
        {
          "IEMOCAP": "0.11±0.01",
          "EMO-DB": "0.61±0.03",
          "CAFE": "0.15±0.01",
          "URDU": "0.17±0.03",
          "AESD": "0.01±0.01∗"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation",
      "data": [
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": ""
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": ""
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "Method"
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": ""
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "SVC (supervised)"
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "SUPERB (>3×108)"
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "DAE-Unsupervised†"
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "DAE-Metric-act‡"
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "DAE-Metric-val‡"
        },
        {
          "Table 3: Adjusted squared correlation coefficient presenting the linear dependence of l on z, the activation": "(<4×102 parameters)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "(ICASSP), pages 5099–5103. IEEE, 2018."
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "[10] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sund-"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "berg, E. Andr´e, C. Busso, L. Y. Devillers, J. Epps,"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "P. Laukka, S. S. Narayanan, et al. The geneva min-"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "imalistic acoustic parameter set (gemaps) for voice"
        },
        {
          "References": "[2]",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "research and affective computing.\nIEEE transac-"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "tions on affective computing, 7(2):190–202, 2015."
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "[11] F. Eyben, M. W¨ollmer, and B. Schuller. Opens-"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "mile:\nthe munich versatile and fast open-source"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "audio feature extractor. In Proceedings of the 18th"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "ACM international\nconference\non Multimedia,"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "pages 1459–1462, 2010."
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "[12] Y. Gao, J. Liu, L. Wang, and J. Dang. Metric"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "learning based feature representation with gated"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "fusion model\nfor\nspeech\nemotion\nrecognition."
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "Proc. Interspeech 2021, pages 4503–4507, 2021."
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "E. Laksana,\nL.-P. Morency,\nand\n[13] S. Ghosh,"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "S. Scherer. Representation learning for speech"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "emotion\nrecognition.\nIn\nInterspeech,\npages"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "3603–3607, 2016."
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": ""
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "and R. Lefebvre.\nA\n[14] P. Gournay, O. Lahaie,"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "canadian french emotional\nspeech dataset.\nIn"
        },
        {
          "References": "",
          "on Acoustics,\nSpeech\nand\nSignal\nProcessing": "Proceedings of the 9th ACM Multimedia Systems"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Cross lingual speech emotion recognition: Urdu",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "INTERSPEECH 2010, Makuhari, Japan, pages"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "vs. western languages. In 2018 International Con-",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "2794–2797, 2010."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "ference on Frontiers of Information Technology",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "[28] P. Vincent,\nH.\nLarochelle,\nY. Bengio,\nand"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "(FIT), pages 88–93. IEEE, 2018.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "P.-A. Manzagol.\nExtracting\nand\ncomposing"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "[20] S.\nLatif, R. Rana,\nJ. Qadir,\nand\nJ. Epps.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "robust features with denoising autoencoders.\nIn"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Variational\nautoencoders\nfor\nlearning\nlatent",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Proceedings of the 25th international conference"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "representations of speech emotion: a preliminary",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "on Machine learning, pages 1096–1103, 2008."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "study.\nInterspeech\n2018:\nProceedings,\npages",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Liatsou,\nC. A.\n[29] N. Vryzas, R. Kotsakis, A."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "3107–3111, 2018.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Dimoulas, and G. Kalliris. Speech emotion recog-"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Speech\n[21] Z. Lian, Y. Li, J. Tao, and J. Huang.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "nition for performance interaction.\nJournal of the"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "emotion recognition via contrastive loss under",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Audio Engineering Society, 66(6):457–467, 2018."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "siamese networks.\nIn Proceedings of\nthe Joint",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "[30] M. W¨ollmer, B. Schuller, F. Eyben, and G. Rigoll."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Workshop of the 4th Workshop on Affective Social",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Combining long short-term memory and dynamic"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Multimedia Computing\nand\nfirst Multi-Modal",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "bayesian\nnetworks\nfor\nincremental\nemotion-"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Affective Computing of Large-Scale Multimedia",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "sensitive artificial\nlistening.\nIEEE Journal of"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Data, pages 21–26, 2018.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "selected topics in signal processing, 4(5):867–881,"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "[22] B. Mocanu, R. Tapu, and T. Zaharia. Utterance",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "2010."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "level\nfeature\naggregation\nwith\ndeep metric",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "[31] R. Xia and Y. Liu. Using denoising autoencoder"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "learning for speech emotion recognition.\nSensors,",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "for emotion recognition.\nIn Interspeech, pages"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "21(12):4233, 2021.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "2886–2889, 2013."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "[23] M. Neumann and N. T. Vu.\nImproving speech",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "emotion recognition with unsupervised represen-",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "[32] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "tation learning on unlabeled speech.\nIn ICASSP",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi,"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "2019-2019\nIEEE\nInternational\nConference",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "X. Chang, G.-T. Lin,\net al.\nSuperb:\nSpeech"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "on Acoustics,\nSpeech\nand\nSignal\nProcessing",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "processing\nuniversal\nperformance\nbenchmark."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "(ICASSP), pages 7390–7394. IEEE, 2019.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Proc. Interspeech 2021, pages 1194–1198, 2021."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "[24] S. Parthasarathy,\nV. Rozgic, M.\nSun,\nand",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "[33] O. Yildirim, R. San Tan, and U. R. Acharya. An"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "C. Wang.\nImproving\nemotion\nclassification",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "efficient compression of ecg signals using deep"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "through variational\ninference of\nlatent variables.",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "convolutional autoencoders. Cognitive Systems"
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "In\nICASSP\n2019-2019\nIEEE\nInternational",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": "Research, 52:198–211, 2018."
        },
        {
          "[19] S. Latif, A. Qayyum, M. Usman, and J. Qadir.": "Conference on Acoustics, Speech and Signal Pro-",
          "interspeech 2010 paralinguistic challenge.\nIn Proc.": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "2",
      "title": "Pleasure, arousal, dominance: Mehrabian and russell revisited",
      "authors": [
        "I Bakker",
        "T Van Der",
        "P Voordt",
        "J Vink",
        "De Boon"
      ],
      "year": "2014",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "D Dai",
        "Z Wu",
        "R Li",
        "X Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "7",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition 'in the wild'using an autoencoder",
      "authors": [
        "V Dissanayake",
        "H Zhang",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "S Eskimez",
        "Z Duan",
        "W Heinzelman"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Metric learning based feature representation with gated fusion model for speech emotion recognition",
      "authors": [
        "Y Gao",
        "J Liu",
        "L Wang",
        "J Dang"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "13",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "14",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM Multimedia Systems Conference"
    },
    {
      "citation_id": "15",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition from variable-length inputs with triplet loss function",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "18",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "20",
      "title": "Variational autoencoders for learning latent representations of speech emotion: a preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Interspeech 2018: Proceedings"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition via contrastive loss under siamese networks",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "22",
      "title": "Utterance level feature aggregation with deep metric learning for speech emotion recognition",
      "authors": [
        "B Mocanu",
        "R Tapu",
        "T Zaharia"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "Improving emotion classification through variational inference of latent variables",
      "authors": [
        "S Parthasarathy",
        "V Rozgic",
        "M Sun",
        "C Wang"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "2004 IEEE international conference on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "27",
      "title": "The interspeech 2010 paralinguistic challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. INTERSPEECH 2010"
    },
    {
      "citation_id": "28",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "authors": [
        "P Vincent",
        "H Larochelle",
        "Y Bengio",
        "P.-A Manzagol"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "30",
      "title": "Combining long short-term memory and dynamic bayesian networks for incremental emotionsensitive artificial listening",
      "authors": [
        "M Wöllmer",
        "B Schuller",
        "F Eyben",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "31",
      "title": "Using denoising autoencoder for emotion recognition",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2013",
      "venue": "Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "33",
      "title": "An efficient compression of ecg signals using deep convolutional autoencoders",
      "authors": [
        "O Yildirim",
        "R Tan",
        "U Acharya"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "34",
      "title": "f-similarity preservation loss for soft labels: A demonstration on cross-corpus speech emotion recognition",
      "authors": [
        "B Zhang",
        "Y Kong",
        "G Essl",
        "E Provost"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}