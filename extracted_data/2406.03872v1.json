{
  "paper_id": "2406.03872v1",
  "title": "Blsp-Emo: Towards Empathetic Large Speech-Language Models",
  "published": "2024-06-06T09:02:31Z",
  "authors": [
    "Chen Wang",
    "Minpeng Liao",
    "Zhongqiang Huang",
    "Junhong Wu",
    "Chengqing Zong",
    "Jiajun Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible. In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speechlanguage model capable of understanding both semantics and emotions in speech and generate empathetic responses. BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process. The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data. The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data. Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations. 1 * Work was done while at Alibaba. â€  Corresponding author. 1 Visit https://github.com/cwang621/blsp-emo for code and https://cwang621.github.io/blsp-emo. github.io for demo. That's wonderful news! Congratulations on your upcoming graduation! Graduating is a significant achievement and a major milestone in your life ... Cheerful I'm sorry to hear that you're feeling sad about graduating soon. It's natural to feel a mix of emotions when it comes to leaving behind the ... ðŸ”ŠðŸ”Š I'm graduating soon. ðŸ”ŠðŸ”Š I'm graduating soon.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in intent understanding  (Lu et al., 2023) , instruction following  (Chung et al., 2022) , and problem-solving  (Achiam et al., 2023; Touvron et al., 2023) , revolutionizing humanmachine interaction. Speech, as the primary mode of human communication, conveys rich paralinguistic features related to emotions, tones, and in-Figure  1 : Illustrative example of an empathetic large language model responding to speeches with identical linguistic content but different emotional tones.\n\ntentions that cannot be fully captured in text. Figure  1  illustrates that LLMs equipped with the ability to understand both linguistic content and emotion cues in speech can enhance interaction experiences by providing empathetic responses.\n\nRecent work on end-to-end modeling of speech inputs with LLMs falls into two categories. The first category focuses on adapting LLMs for a wide range of speech and audio-related tasks, such as speech recognition, translation, and emotion recognition  (Rubenstein et al., 2023; Chen et al., 2023) . However, these models lack the ability to retain the general instruction-following capabilities of LLMs and cannot engage in conversations with speech inputs. The second category aims to extend LLMs' instruction-following capability to speech inputs, enabling direct speech interaction with LLMs  (Zhang et al., 2023; Wang et al., 2023a) . Nevertheless, these approaches primarily focus on the semantics in speech and fail to capture paralinguistic cues related to emotions. Some studies have attempted to train models to understand emotions in speech and respond empathetically  (Xue et al., 2023; Lin et al., 2024) . However, these efforts rely on speech instruction data constructed with expressive text-to-speech synthesis tools, which limits their generalization capability with natural human speech. Annotating large quantities of new emotion-sensitive instruction or conversation data for natural speech would be costly.\n\nIn this paper, we present the BLSP-Emo approach, which aims to develop an end-to-end speech-language model capable of understanding semantics and emotions in speech and generating empathetic responses, using only existing speech recognition (ASR) and speech emotion recognition (SER) datasets. BLSP-Emo builds upon recent work on speech-language models developed with the BLSP method  (Wang et al., 2023a (Wang et al., , 2024)) , which are bootstrapped from and aligned at the semantic level with an LLM using ASR data. These speech-language models exhibit generation behaviors consistent with the LLM when presented with speech input containing the same linguistic content.\n\nWe propose to perform emotion alignment to understand emotions, in addition to semantics, in speech and generate empathetic responses. Specifically, we first prompt an LLM to generate emotionaware continuations of transcripts in the SER data given the reference emotion label. We then adapt a speech-language model bootstrapped from the same LLM to generate these continuations directly from speech. This adaptation step encourages the model to comprehend and react to both the linguistic content and paralinguistic emotion cues in speech, generating text continuations that are aligned with those the LLM would produce if provided with the same linguistic content and emotion label.\n\nThe contributions of our work are as follows:\n\nâ€¢ We introduce a new empathetic large speechlanguage model, adapted from an instructionfollowing LLM, that can understand and respond to emotion cues in speech with empathy, while maintaining its ability to follow speech instructions and engage in conversations.\n\nâ€¢ We develop a two-stage approach to adapt LLMs to empathetic large speech-language models, using existing ASR data for semantic alignment and SER data for emotion alignment, aiming to ensure that responses to speech input align with those the LLMs would produce if provided with the same linguistic content and emotion label.\n\nâ€¢ We conduct quantitative evaluations and provide demonstrations to showcase that the BLSP-Emo approach enables LLMs with competitive capabilities to perform standalone speech emotion recognition, generate empathetic responses, and engage in empathetic conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "Our proposed approach, termed BLSP-Emo, aims to develop an end-to-end speech-language model that understands both linguistic content and paralinguistic emotion cues in speech and generates empathetic responses. BLSP-Emo builds upon bootstrapped speech-language models developed with the BLSP method  (Wang et al., 2023a (Wang et al., , 2024)) , which are adapted from a text-only LLM using ASR data. BLSP-Emo leverage SER data to enable these bootstrapped speech-language models to also comprehend and react to the paralinguistic emotion cues. In what follows, we will describe the model architecture and introduce how we achieve semantic alignment and emotion alignment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Architecture",
      "text": "BLSP-Emo models share a similar architecture as those in BLSP, comprising three components: a speech encoder (with parameters Ïˆ), an instructionfollowing LLM (with parameters Ï•), and a modality adapter (with parameters Î¸) between the speech encoder and LLM. Figure  2  provides an overview of our model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Semantic Alignment Stage",
      "text": "To achieve speech-text alignment at the semantic level and enable general instruction-following capabilities for LLMs with speech inputs, we adopt the behavior alignment approach used in BLSP  (Wang et al., 2023a) . The core concept is that if speech and text are well-aligned, the LLM's text generation behavior given speech input should closely match its behavior when given the corresponding transcript. This alignment is accomplished by training on synthesized speech instruction data derived from existing ASR datasets with a continuation prompt as follows:\n\nUser: Continue the following sentence in a coherent style: <transcript> Assistant:\n\nThis process extends an ASR training sample (s, x) into a tuple (s, x, y), where y is the LLM's response, representing a natural continuation of the transcript x and the corresponding speech s. The model is trained to generate the same continuation when given speech input, using the same",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "User Instruction",
      "text": "Continue the following sentence that reflects a 'sad' emotion tone in a coherent style: so i was like wandering around before it takes forever to find her headstone i always lose it you know  continuation prompt. This is achieved by applying a KL-divergence loss according to the knowledge distillation framework described in  (Wang et al., 2024) , leading to the semantic alignment loss: â„“ Semantic (s, x, y) = j,y p Ï• (y|x, y <j ) log p Ïˆ,Î¸,Ï• (y|s, y <j ) (1)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Adapter",
      "text": "In this semantic alignment stage, we focus on tuning the parameters Î¸ of the modality adapter, keeping the parameters Ïˆ and Ï• of the speech encoder and LLM frozen.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Alignment Stage",
      "text": "As studied in  Busso et al. (2008) ;  Castro et al. (2019) , humans convey emotions in speech through both linguistic and paralinguistic cues. A model trained with the BLSP approach captures the linguistic cues for emotion but lacks the ability to understand paralinguistic cues, as it is aligned at the semantic level based on linguistic content. Ideally, an emotion-aware speech-language model should be pretrained on large amounts of speech-text data to understand the relationship between paralinguistic emotion cues and linguistic context, and then fine-tuned on emotion-aware speech instruction data, following the training paradigm used for textonly LLMs. However, this approach requires ex-tensive curated data and significant computational resources, neither of which is readily accessible.\n\nOur approach to emotion alignment builds upon and extends the behavior alignment method by creating natural continuations of speech transcripts that reflect the emotional tones in the speech. This is achieved by leveraging existing speech emotion recognition (SER) datasets. Given a sample (s, x, e) from a SER dataset, where e is the emotion label annotated for speech s, we prompt the LLM with the following instruction:\n\nUser: Continue the following sentence that reflects a <emotion> emotion tone in a coherent style: <transcript> Assistant:\n\nThis generates a text continuation y of the speech s that is consistent with the emotion label e. We then initialize the BLSP-Emo model with parameters of the BLSP model trained from the semantic alignment stage and fine-tune it to generate these continuations given only the speech as input, as follows:\n\nUser: Continue the following sentence based on the conveyed emotion tone in a coherent style: <speech features> Assistant: <text continuation> This results in the primary emotion alignment loss based on emotion-aware continuations:\n\nWe also introduce an auxiliary speech emotion recognition loss by directly predicting the emotion label e from the hidden states output by the modality adapter, using pooling and a classification layer (with additional parameters Î·):\n\nIn this emotion alignment stage, we unfreeze the parameters Ïˆ of the speech encoder and parameters Ï• of the LLM, in addition to the parameters Î¸ of the modality adapter and Î· of the classification layer. This allows the speech encoder to capture paralinguistic emotion cues and provides additional modeling power in the LLM to address the discrepancy between speech and text. We follow the PLoRA approach proposed in  (Dong et al., 2024; Wang et al., 2024)  to adapt parameters Ï• of the LLM. The LoRA module is selectively applied only to speech tokens, preserving the LLM's ability to encode text instructions and generate text.\n\n3 Experiment Setup",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "We use publicly available ASR datasets in the semantic alignment stage and SER datasets in the emotion alignment stage.\n\nThe ASR datasets include LibriSpeech  (Panayotov et al., 2015) , CommonVoice 13.0  (Ardila et al., 2019) , and the GigaSpeech M set  (Chen et al., 2021) , totaling approximately 1.9 million English (speech, transcript) pairs, along with a comparable number of Chinese ASR samples randomly selected from WeNetSpeech  (Zhang et al., 2022) .\n\nThe details of the SER datasets and train/test splits can be found in Appendix B. In summary, we train on IEMOCAP, MELD, CMU MOSEI, MEAD, and ESD, covering approximately 70k utterances in English and Chinese, and evaluate SER performance on IEMOCAP and MELD as in-domain test sets, on RAVDESS and MerBench as out-of-domain test sets, as well as on three languages not seen in training: AESDD for Greek, CaFE for French, and RESD for Russian. We focus on five emotion categories: neutral, happy, sad, angry, and surprise across all datasets.\n\nWe conduct evaluations on emotion-aware speech instruction capabilities based on a synthesized version of Alpaca-52k  (Taori et al., 2023) , and emotion-aware multi-turn conversation based on IEMOCAP  (Busso et al., 2008) , with details presented in Section 4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training Details",
      "text": "We utilize the encoder part of Whisper-large-v2  (Radford et al., 2022)  as the speech encoder, convolution-based subsampler as the modality adapter, and Qwen-7B-Chat  (Bai et al., 2023)  as the LLM. More details can be found in Appendix C.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Baselines",
      "text": "We compare with the following baselines:\n\nText|Whisper+LLM These are cascaded systems where the LLM input is either the groundtruth transcript or the recognition output from Whisper-large-v2, which includes a speech encoder, as used in BLSP-Emo, and a speech decoder.\n\nBLSP This model undergoes the semantic alignment stage described in Section 2.2 and initializes BLSP-Emo before the emotion alignment stage.\n\nBLSP-SER This model is initialized from BLSP and fine-tuned directly on the SER task. The only difference between BLSP-SER and BLSP-Emo is that the former is fine-tuned to predict the ground-truth emotion label, while the latter generates emotion-aware continuations, both utilizing the same SER training datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Hubert|Wav2Vec2|Wavlm+Whisper+Llm",
      "text": "These are cascaded systems composed of a standalone SER module in addition to the Whisper+LLM pipeline. The SER component is fine-tuned on the SER training datasets from respective speech encoder models, including HuBERT large  (Hsu et al., 2021) , Wav2Vec 2.0 large  (Baevski et al., 2020) , or WavLM large  (Chen et al., 2022) , with the addition of an average pooling layer and a linear classifier to predict the ground-truth emotion label. During evaluation, we directly report the performance of the SER module for the SER task. For other tasks, we first use the SER module and the Whisper model to respectively predict the emotion label and transcript, and then use the following prompt to generate responses:\n\nUser: The user's speech instruction, transcribed as \"<transcript>\", conveys a <emotion> emotion tone. Please provide a response. Assistant:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "Although BLSP-Emo is trained only on continuation tasks, we have found that the resulting model has the ability to comprehend both linguistic content and paralinguistic emotion cues in speech and respond accordingly. This enables the model to not only follow task instructions but also demonstrate empathy toward the emotional tone conveyed in the speech. Next, we will present results on speech emotion recognition, instruction-following with empathetic responses, multi-turn conversation, and generalization to other languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Main Results",
      "text": "Speech Emotion Recognition To prompt the LLM-based generative models to perform the SER task, we use the following prompt:\n\nUser: Please identify the emotion tone of the sentence provided below. Select from the following options: neutral, sad, angry, happy, or surprise. \\n\\nSentence: <transcript|speech> Assistant:\n\nwhere <transcript|speech> represents the transcript for cascaded systems or speech features for end-toend systems. Results are shown in Table  1 .\n\nThe BLSP-Emo model achieves the highest overall recognition accuracy across five test sets, along with the BLSP-SER model, which is fine-tuned from the same BLSP model but specifically for the SER task. BLSP-Emo significantly outperforms all other models, including SALMONN-7B  (Tang et al., 2023) , which adapts a large language model to various speech tasks, including speech emotion recognition.\n\nThe Text|Whisper+LLM cascaded systems achieve comparable or better results than the encoder-based classification models on the MELD and MerBench test sets, but they perform the worst on the IEMOCAP and RAVDESS test sets. This suggests that while an LLM can capture linguistic cues for emotions, the text-only mode limits its ability for comprehensive emotion recognition. The BLSP model can process speech input but cannot pick up paralinguistic cues for emotion as it is only trained with semantic alignment. Conversely, the encoder-based classification models can capture paralinguistic cues but lack a semantic understanding of emotion. In contrast, BLSP-Emo can simultaneously model linguistic and paralinguistic emotion cues in speech, thanks to its end-to-end modeling and two-stage alignment process.\n\nEmpathetic Response Beyond speech emotion recognition, our primary concern is whether the model can understand both the semantic content and paralinguistic emotion cues in speech and generate high-quality, empathetic responses. To evaluate this, we construct a synthetic emotion-aware speech instruction dataset named SpeechAlpaca, derived from the open-source instruction dataset Alpaca-52k  (Taori et al., 2023) . Additionally, we use a modified system prompt 2 that emphasizes both quality and empathy for all systems. We then employ GPT-4 as an evaluator to independently score the responses generated by different systems in terms of quality and empathy on a scale from 0 to 10. For details on test set construction and evaluation prompts, please refer to Appendix D.\n\nThe results are shown in Table  2 . Consistent with findings in the SER evaluation on natural speech, BLSP-Emo achieves the highest emotion recognition accuracy of 83.8% on synthetic speech. Additionally, BLSP-Emo scores competitively in both quality (8.8) and empathy (7.7) as measured by GPT-4. In contrast, the BLSP-SER model, fine-tuned specifically for the SER task, achieves a lower performance in SER (80.3%) and performs poorly in empathetic response (quality: 1.9, empathy: 2.1), as it loses the ability to follow speech instructions learned during semantic alignment.\n\nThe BLSP model, despite having a significantly lower SER score (36.8%), achieves decent ratings in quality (8.6) and empathy (7.1), as it is able to comprehend semantics and linguistic emotion cues thanks to semantic alignment. The improvements from BLSP to BLSP-Emo in all three metrics-SER (36.8% to 83.8%), quality (8.6 to 8.8), and empathy (7.1 to 7.7)-suggest that the BLSP-Emo approach effectively understands both linguistic and paralinguistic emotion cues in speech while maintaining its instruction-following capability, resulting in overall better responses.\n\nThe Text|Whisper+LLM systems achieve a slightly higher quality score (8.9 vs. 8.8) than BLSP-Emo but a lower empathy score (7.4 vs. 7.7) and significantly lower SER scores (40.0% vs. 83.8%). This signifies that while LLMs have a strong capability to capture linguistic emotion cues, they are limited by their inability to understand paralinguistic emotion cues. As the examples in Appendix D show, a text-only LLM can provide an empathetic response to the instruction \"Suggest the best way to avoid a traffic jam\" based on the semantic content alone. However, it cannot provide empathetic responses to a neutral instruction \"Come up with a 5-step process for making a decision\" stated in an angry voice.\n\nThe HuBERT|wav2vec2|WavLM+Whisper+LLM systems with standalone SER modules achieve comparable quality ratings to the Text|Whisper+LLM systems but higher empathy ratings (7.6âˆ¼7.8 vs 7.4), further underlining the importance of capturing paralinguistic emotion cues in generating empathetic responses. It is worth noting that these cascaded systems also have slightly higher ratings in quality than BLSP-Emo. We attribute this to the room for improvement in semantic alignment for BLSP pretraining, as the Whisper model contains a separate speech decoder that is trained on significantly more speech data  (Wang et al., 2023a (Wang et al., , 2024)) . Additionally, despite being trained on various speech tasks, large speech-language models like SALMONN  (Tang et al., 2023)  exhibit limitations in following general speech instructions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Turn Conversation",
      "text": "We next evaluate multi-turn conversations, an important application scenario for empathetic large speech-language models. This evaluation allows us to determine if the emotion understanding capability of BLSP-Emo, learned from a simple emotion-aware continuation task, can generalize to scenarios with extended conversational context. Following a setup similar to  Lin et al. (2024) , whose test set is not publicly available, we extract 3-turn dialogues between two speakers from IEMOCAP  (Busso et al., 2008) , treating the first speaker as the user and the second as the assistant. The conversation history consists of the reference dialog transcripts from the first two turns, plus the current input-either a transcript for a cascaded system or speech features for an end-to-end model-from the user, along with the predicted emotion label if the system has a standalone SER module. The LLM is then prompted to generate a response. For examples, please refer to Appendix E. Given that typical user inputs in conversations are not specific task instructions, we found it difficult for GPT-4 to separately assess quality and empathy as done on SpeechAlpaca. Instead, we employ GPT-4 as an evaluator to determine which system's output is better, based on reference transcripts in the conversation history and the emotion label of the user's most recent input. For details, please refer to Appendix E. As shown in Figure  3 , BLSP-Emo demonstrates higher win rates compared to Whisper+LLM, BLSP, and WavLM+Whisper+LLM. This advantage mirrors BLSP-Emo's comparative performance on SpeechAlpaca, highlighting its capability to understand and respond to paralinguistic emotion cues in speech. Notably, BLSP-Emo's superiority over WavLM+Whisper+LLM is somewhat unexpected, given that the latter performed comparably or slightly better on SpeechAlpaca in both quality and empathy ratings. We speculate that this discrepancy may be attributed to the specific prompt used, which incorporates both the transcript and the recognized emotion tone for the user's last speech input (as illustrated in Appendix E). This could introduce inconsistency compared to the simpler transcript representation of the conversation history. In contrast, BLSP-Emo does not necessitate special prompting for speech input, as it implicitly captures emotion cues in the speech features. While prompt engineering could potentially enhance the performance of WavLM+Whisper+LLM, this also underscores the simplicity and advantage of the BLSP-Emo approach.\n\nLanguage Generalization To explore whether the knowledge learned about emotion cues can generalize across languages, we evaluate zero-shot SER performance on three languages not included during training. As shown in Table  3 , BLSP-Emo achieves the best overall performance across the languages, performing comparably or better than BLSP-SER and significantly better than the other models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation studies to understand the impact of two training strategies within the BLSP-Emo approach, with results presented in Table  4 . Directly applying emotion alignment without first performing BLSP semantic alignment leads to a sig-nificant drop in both standalone SER performance and quality/empathy ratings in empathetic response. This underscores the importance of having a bootstrapped speech-language model that is aligned at the semantic level before attending to paralinguistic cues.\n\nFurthermore, incorporating the auxiliary SER classification task proves beneficial for achieving higher performance in speech emotion recognition on natural speech, even though it does not lead to any noticeable differences on the SpeechAlpaca test set or in the evaluation of empathetic responses.",
      "page_start": 1,
      "page_end": 7
    },
    {
      "section_name": "Analysis",
      "text": "We perform additional analysis comparing our training strategies against two recent approaches in the literature of speech-language models with emotion-aware capabilities.\n\nFirst, we compare our approach to the method of E-chat  (Xue et al., 2023)  and Spoken-LLM  (Lin et al., 2024) , which constructed synthesized emotion-aware speech instruction data using expressive text-to-speech tools and ChatGPT. As noted previously and found in our preliminary studies, models trained on synthesized speech fail to generalize to natural human speech. Given that our approach also requires constructing synthesized emotion-aware continuation data for natural speech, a critical question arises: is it better to use ChatGPT for data construction, as commonly done in the literature, or to use the same LLM that BLSP-Emo is adapted from?\n\nTo address this, we trained a new model named BLSP-ChatGPT, utilizing ChatGPT to generate emotion-aware continuations for emotion alignment, starting from the same pretrained BLSP model as BLSP-Emo. As shown in Table  5 , while BLSP-ChatGPT achieves higher SER performance than BLSP, its quality and empathy ratings in empathetic responses are notably lower. BLSP-ChatGPT performs worse than BLSP-Emo across all metrics. We hypothesize that the emotion-aware continuations generated by ChatGPT may not align well with the likely responses generated by the internal LLM in BLSP-Emo. Consequently, the alignment process may focus on narrowing the distribution gap between ChatGPT and the internal LLM, rather than learning to capture the paralinguistic emotion cues in speech to fit into the aligned semantic space established during semantic alignment.\n\nNext, we compare our approach against the multi-task learning strategy employed by other large speech-language models, such as SALMONN  (Tang et al., 2023) , which aims to understand semantic content and various paralinguistic cues. As demonstrated in previous sessions, BLSP-Emo significantly outperforms SALMONN-7B in both standalone emotion recognition and emotion-aware instruction following. However, a question remains: can we replace the emotionaware continuation task employed in the emotion alignment stage with a multi-task framework involving two tasks: emotion-agnostic continuation and speech emotion recognition?\n\nTo answer this, we use the SER training datasets to construct two tasks: one for standalone SER and another for emotion-agnostic continuation. The resulting model is named BLSP-MultiTask. As shown in Table  5 , while BLSP-MultiTask significantly improves the SER accuracy of the BLSP model, its response quality is lower than that of BLSP. BLSP-MultiTask also performs worse than BLSP-Emo across all metrics. This comparison highlights the importance of the emotion-aware continuation task in developing effective empathetic speech-language models.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Related Works",
      "text": "See Appendix A for a discussion on related works.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In summary, this paper presents BLSP-Emo, a novel approach to build empathetic large speechlanguage models by utilizing existing speech recognition and speech emotion recognition datasets, through a two stage alignment process: semantic alignment and emotion alignment. Through quantitative evaluations, we demonstrate that the BLSP-Emo approach extends instruction-following LLMs with competitive abilities to understand both semantics and emotions in speech and perform standalone speech emotion recognition, generate empathetic responses, and engage in multi-turn conversations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "Evaluation of Empathy. While our methods for assessing empathetic responses provide valuable insights, there are several limitations. Synthesized speech, as in SpeechAlpaca, lacks variations in factors such as speaker ids and emotion expressions, potentially limiting the accuracy of model performance evaluation on natural human speech. Additionally, in the evaluation of multi-turn conversations on IEMOCAP, we only assess a single-turn response within a multi-turn context. This may not fully capture the model's performance in continuous conversations and how empathetic responses, sometimes repetitive, are perceived from a user experience perspective.\n\nBroader Applicability. Our current approach to modeling emotions in speech relies on a limited number of emotion states annotated in SER datasets. However, human speech has rich expressions of emotions that are more nuanced and may include variations of emotion in lengthy speech segments. Additionally, there are other types of paralinguistic cues in human speech, such as tones and intentions, that are important in communication but not addressed in this work. The two-stage alignment approach, however, could be expanded to achieve general modeling of paralinguistic cues through end-to-end modeling on large speech-text datasets, while retaining instruction-following capabilities. We leave this to future work.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Related Works",
      "text": "Large Speech-Language Models Large Language Models (LLMs) have achieved remarkable performance on various natural language processing tasks  (Achiam et al., 2023; Touvron et al., 2023) . Ongoing research aims to integrate speech signals into pre-trained, decoder-only text-based LLMs, creating unified models capable of handling diverse speech processing tasks. Models like  Au-dioPaLM (Rubenstein et al., 2023) , VIOLA  (Wang et al., 2023b) , and LauraGPT  (Chen et al., 2023)  have emerged from such efforts, primarily trained through multi-task learning for various speech processing tasks, without utilizing conversational competencies inherent in LLMs. Recent models like SALMONN  (Tang et al., 2023)  and WavLLM  (Hu et al., 2024)  processing abilities using textual instructions, still struggle with following general speech instructions.\n\nOther efforts focus on generalized cross-modal instruction-following capabilities through end-toend frameworks, enabling direct interaction with LLMs via speech, such as SpeechGPT  (Zhang et al., 2023) , LLaSM  (Shu et al., 2023) , and BLSP  (Wang et al., 2023a (Wang et al., , 2024)) . However, these models primarily base responses on linguistic content and cannot utilize paralinguistic features.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Interact With Llms Through Emotional Speech",
      "text": "Recent advancements in GPT-4o underscore the significance of integrating paralinguistic emotion cues from user speech into LLM interactions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B Ser Datasets",
      "text": "A summary of the SER datasets employed in our experiments is presented in Table  6 , with each dataset categorized based on the following attributes:\n\nâ€¢ Source: The origin of the collected samples.\n\nâ€¢ Language: The language of the transcript.\n\nâ€¢ Emotion: The labeled emotion categories.\n\nâ€¢ #Utts: The number of utterances.\n\nThe SER datasets used during emotion alignment consist of sessions 1-4 of IEMOCAP  (Busso et al., 2008) , the training set of MELD  (Poria et al., 2018 ), CMU MOSEI (Zadeh et al., 2018) , MEAD  (Wang et al., 2020) , and ESD  (Zhou et al., 2022) . Together, these datasets contribute to a corpus of approximately 70k utterances in English and Chinese. It's worth noting that CMU MOSEI is a multi-emotion-labeled dataset, meaning a speech segment could be annotated with multiple emotions. However, we only utilize the single-label samples from this dataset. In this work, we focus on the five emotion categories that are widely annotated across datasets: neutral, happy, sad, angry, and surprise 3 . To ensure the transcripts provide sufficient semantic content for LLMs to generate meaningful continuations, we filter out samples whose tran-script contains fewer than 5 words in English or fewer than 5 characters in Chinese.\n\nWe evaluate SER performance on both indomain datasets (IEMOCAP session 5, MELD test set) and out-of-domain datasets (RAVDESS (Livingstone and Russo, 2018), MerBench  (Lian et al., 2024) ). Additionally, we report the generalizability of SER performance on three other languages: AESDD  (Vryzas et al., 2018)  for Greek, CaFE  (Gournay et al., 2018)  for French, and RESD  (Vryzas et al., 2018)  for Russian.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C Training Details",
      "text": "We utilize the encoder part of Whisper-large-v2  (Radford et al., 2022)  as the speech encoder and employ Qwen-7B-Chat  (Bai et al., 2023)  as the LLM. The modality adapter is composed of three 1-dimensional convolution layers followed by a bottleneck layer with a hidden dimension of 512. The convolution layers are designed to reduce the length of the speech features by a factor of 8, with each layer having a stride size of 2, a kernel size of 5, and a padding of 2.\n\nDuring the semantic alignment stage, we freeze the speech encoder and LLM, and fine-tune the modality adapter for 1 epoch with a batch size of 768. This process takes about 2.5 days on 4 A100 GPUs. During the emotion alignment stage, we fine-tune the speech encoder, modality adapter, LLM 4  , and SER classifier for 3 epochs with a batch size of 128. This process takes about 3 hours on 4 A100 GPUs.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D Evaluation On Empathetic Responses",
      "text": "Due to the lack of publicly available emotion-aware speech instruction datasets to evaluate performance on empathetic responses, we construct a test set named SpeechAlpaca from the open-source instruction dataset Alpaca-52k  (Taori et al., 2023) . Specifically, we employ GPT-4 to deduce a set of plausible emotional tones from a text instruction in Alpaca-52k, focusing on four distinct emotions (neutral, cheerful, sad, and angry) that are supported by Microsoft's Text-to-Speech (TTS) API 5  . On average, GPT-4 suggests 1.4 plausible emotions per utterance due to ambiguities in determining the emotion state from linguistic content alone. From these, we randomly select one as the emotion label for the instruction. This process is used to select 100 instructions for each of the four emotion categories. Subsequently, we synthesize expressive speech using the selected emotion label with Microsoft's TTS API.\n\nWe present examples of model outputs on the SpeechAlpaca test set in Table  7 . To evaluate the empathetic responses, we use GPT-4 to assess the quality of responses with the prompt in Listing 1 and the empathy of responses with the prompt in Listing 2.\n\nListing 1: Prompt for response quality evaluation on SpeechAlpaca\n\nGiven the original instruction provided by the user , the user ' s emotion tone when delivering the instruction , and the model ' s response to the instruction . You are a helpful and precise assistant for checking the quality of the response . < instruction > { instruction } </ instruction > < emotion > { emotion } </ emotion > < response > { response } </ response > Please evaluate the response with your justification having less than three sentences , and provided a score ranging from 0 to 10 after your justification . When evaluate the response , you should consider the helpfulness , harmlessness , honesty of the response . The score should be wrapped by < score > and </ score >.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "E Evaluation On Multi-Turn Conversation",
      "text": "We present examples in Table  8  to illustrate the differences in responses among various systems. To assess the comparative quality, we employ GPT-4 with the prompt specified in Listing 3 for pairwise evaluation. To mitigate the order bias of the GPT-4 evaluator, we conduct two evaluations for the outputs of models A and B for the same sample: one in the AB sequence and the other in the BA sequence. Model A is deemed the winner only if it is consistently judged as better than B in both evaluations, while a loss is assigned only if B is consistently superior in both. Otherwise, it is considered a tie.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustrative example of an empathetic large",
      "page": 1
    },
    {
      "caption": "Figure 2: provides an overview",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the BLSP-Emo approach. In the first step, an LLM generates emotion-aware text continuations",
      "page": 3
    },
    {
      "caption": "Figure 3: Results on multi-turn conversation.",
      "page": 6
    },
    {
      "caption": "Figure 3: , BLSP-Emo demonstrates",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "tasks and conversations.1"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "1\nIntroduction"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "Large Language Models\n(LLMs) have demon-"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "strated remarkable capabilities in intent understand-"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "ing (Lu et al., 2023), instruction following (Chung"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "et al., 2022), and problem-solving (Achiam et al.,"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "2023; Touvron et al., 2023), revolutionizing human-"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "machine interaction. Speech, as the primary mode"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "of human communication, conveys rich paralin-"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "guistic features related to emotions, tones, and in-"
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "*Work was done while at Alibaba."
        },
        {
          "thetic responses, both in instruction-following": "â€ Corresponding author."
        },
        {
          "thetic responses, both in instruction-following": ""
        },
        {
          "thetic responses, both in instruction-following": "https://github.com/cwang621/blsp-emo\n1Visit"
        },
        {
          "thetic responses, both in instruction-following": "https://cwang621.github.io/blsp-emo."
        },
        {
          "thetic responses, both in instruction-following": "for\ncode\nand"
        },
        {
          "thetic responses, both in instruction-following": "github.io for demo."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "Abstract"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "The recent release of GPT-4o showcased the"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "potential of end-to-end multimodal models, not"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "just\nin terms of\nlow latency but also in their"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "ability to understand and generate expressive"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "speech with rich emotions. While the details"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "are unknown to the open research community,"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "it\nlikely involves\nsignificant amounts of cu-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "rated data and compute, neither of which is"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "readily accessible.\nIn this paper, we present"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "BLSP-Emo (Bootstrapped Language-Speech"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "Pretraining with Emotion support),\na novel"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "approach to developing an end-to-end speech-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "language model capable of understanding both"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "semantics and emotions in speech and gener-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "ate empathetic responses. BLSP-Emo utilizes"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "existing speech recognition (ASR) and speech"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "emotion recognition (SER) datasets through"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "a two-stage process.\nThe first stage focuses"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "on semantic alignment, following recent work"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "on pretraining speech-language models using"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "ASR data. The second stage performs emotion"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "alignment with the pretrained speech-language"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "model on an emotion-aware continuation task"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "constructed from SER data. Our experiments"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "demonstrate that the BLSP-Emo model excels"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "in comprehending speech and delivering empa-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "thetic responses, both in instruction-following"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "tasks and conversations.1"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "1\nIntroduction"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "Large Language Models\n(LLMs) have demon-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "strated remarkable capabilities in intent understand-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "ing (Lu et al., 2023), instruction following (Chung"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "et al., 2022), and problem-solving (Achiam et al.,"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "2023; Touvron et al., 2023), revolutionizing human-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "machine interaction. Speech, as the primary mode"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "of human communication, conveys rich paralin-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "guistic features related to emotions, tones, and in-"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "*Work was done while at Alibaba."
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "â€ Corresponding author."
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": ""
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "https://github.com/cwang621/blsp-emo\n1Visit"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "https://cwang621.github.io/blsp-emo."
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "for\ncode\nand"
        },
        {
          "{minpeng.lmp,z.huang}@alibaba-inc.com": "github.io for demo."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for natural speech would be costly.": "In this paper, we present\nthe BLSP-Emo ap-",
          "speech emotion recognition, generate empa-": "thetic responses, and engage in empathetic"
        },
        {
          "for natural speech would be costly.": "proach, which\naims\nto\ndevelop\nan\nend-to-end",
          "speech emotion recognition, generate empa-": "conversations."
        },
        {
          "for natural speech would be costly.": "speech-language model capable of understanding",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "semantics and emotions in speech and generating",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "2\nMethod"
        },
        {
          "for natural speech would be costly.": "empathetic responses, using only existing speech",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "recognition (ASR) and speech emotion recogni-",
          "speech emotion recognition, generate empa-": "Our proposed approach, termed BLSP-Emo, aims"
        },
        {
          "for natural speech would be costly.": "tion (SER) datasets. BLSP-Emo builds upon re-",
          "speech emotion recognition, generate empa-": "to develop an end-to-end speech-language model"
        },
        {
          "for natural speech would be costly.": "cent work on speech-language models developed",
          "speech emotion recognition, generate empa-": "that understands both linguistic content and par-"
        },
        {
          "for natural speech would be costly.": "with the BLSP method (Wang et al., 2023a, 2024),",
          "speech emotion recognition, generate empa-": "alinguistic emotion cues in speech and generates"
        },
        {
          "for natural speech would be costly.": "which are bootstrapped from and aligned at the se-",
          "speech emotion recognition, generate empa-": "empathetic responses.\nBLSP-Emo builds upon"
        },
        {
          "for natural speech would be costly.": "mantic level with an LLM using ASR data. These",
          "speech emotion recognition, generate empa-": "bootstrapped speech-language models developed"
        },
        {
          "for natural speech would be costly.": "speech-language models exhibit generation behav-",
          "speech emotion recognition, generate empa-": "with the BLSP method (Wang et al., 2023a, 2024),"
        },
        {
          "for natural speech would be costly.": "iors consistent with the LLM when presented with",
          "speech emotion recognition, generate empa-": "which are adapted from a text-only LLM using"
        },
        {
          "for natural speech would be costly.": "speech input containing the same linguistic content.",
          "speech emotion recognition, generate empa-": "ASR data. BLSP-Emo leverage SER data to enable"
        },
        {
          "for natural speech would be costly.": "We propose to perform emotion alignment\nto",
          "speech emotion recognition, generate empa-": "these bootstrapped speech-language models to also"
        },
        {
          "for natural speech would be costly.": "understand emotions, in addition to semantics, in",
          "speech emotion recognition, generate empa-": "comprehend and react\nto the paralinguistic emo-"
        },
        {
          "for natural speech would be costly.": "speech and generate empathetic responses. Specifi-",
          "speech emotion recognition, generate empa-": "tion cues.\nIn what follows, we will describe the"
        },
        {
          "for natural speech would be costly.": "cally, we first prompt an LLM to generate emotion-",
          "speech emotion recognition, generate empa-": "model architecture and introduce how we achieve"
        },
        {
          "for natural speech would be costly.": "aware continuations of transcripts in the SER data",
          "speech emotion recognition, generate empa-": "semantic alignment and emotion alignment."
        },
        {
          "for natural speech would be costly.": "given the reference emotion label. We then adapt",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "2.1\nArchitecture"
        },
        {
          "for natural speech would be costly.": "a speech-language model bootstrapped from the",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "same LLM to generate these continuations directly",
          "speech emotion recognition, generate empa-": "BLSP-Emo models share a similar architecture as"
        },
        {
          "for natural speech would be costly.": "from speech. This adaptation step encourages the",
          "speech emotion recognition, generate empa-": "those in BLSP, comprising three components:\na"
        },
        {
          "for natural speech would be costly.": "model\nto comprehend and react\nto both the lin-",
          "speech emotion recognition, generate empa-": "speech encoder (with parameters Ïˆ), an instruction-"
        },
        {
          "for natural speech would be costly.": "guistic content and paralinguistic emotion cues",
          "speech emotion recognition, generate empa-": "following LLM (with parameters Ï•), and a modal-"
        },
        {
          "for natural speech would be costly.": "in speech, generating text continuations that are",
          "speech emotion recognition, generate empa-": "ity adapter (with parameters Î¸) between the speech"
        },
        {
          "for natural speech would be costly.": "aligned with those the LLM would produce if pro-",
          "speech emotion recognition, generate empa-": "encoder and LLM. Figure 2 provides an overview"
        },
        {
          "for natural speech would be costly.": "vided with the same linguistic content and emotion",
          "speech emotion recognition, generate empa-": "of our model."
        },
        {
          "for natural speech would be costly.": "label.",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "2.2\nSemantic Alignment Stage"
        },
        {
          "for natural speech would be costly.": "The contributions of our work are as follows:",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "To achieve speech-text alignment at the semantic"
        },
        {
          "for natural speech would be costly.": "â€¢ We introduce a new empathetic large speech-",
          "speech emotion recognition, generate empa-": "level and enable general instruction-following capa-"
        },
        {
          "for natural speech would be costly.": "language model, adapted from an instruction-",
          "speech emotion recognition, generate empa-": "bilities for LLMs with speech inputs, we adopt the"
        },
        {
          "for natural speech would be costly.": "following LLM,\nthat can understand and re-",
          "speech emotion recognition, generate empa-": "behavior alignment approach used in BLSP (Wang"
        },
        {
          "for natural speech would be costly.": "spond to emotion cues in speech with empathy,",
          "speech emotion recognition, generate empa-": "et al., 2023a). The core concept\nis that\nif speech"
        },
        {
          "for natural speech would be costly.": "while maintaining its ability to follow speech",
          "speech emotion recognition, generate empa-": "and text are well-aligned,\nthe LLMâ€™s text gener-"
        },
        {
          "for natural speech would be costly.": "instructions and engage in conversations.",
          "speech emotion recognition, generate empa-": "ation behavior given speech input should closely"
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "match its behavior when given the corresponding"
        },
        {
          "for natural speech would be costly.": "â€¢ We develop a two-stage approach to adapt",
          "speech emotion recognition, generate empa-": "transcript. This alignment is accomplished by train-"
        },
        {
          "for natural speech would be costly.": "LLMs to empathetic large speech-language",
          "speech emotion recognition, generate empa-": "ing on synthesized speech instruction data derived"
        },
        {
          "for natural speech would be costly.": "models, using existing ASR data for semantic",
          "speech emotion recognition, generate empa-": "from existing ASR datasets with a continuation"
        },
        {
          "for natural speech would be costly.": "alignment and SER data for emotion align-",
          "speech emotion recognition, generate empa-": "prompt as follows:"
        },
        {
          "for natural speech would be costly.": "ment,\naiming\nto\nensure\nthat\nresponses\nto",
          "speech emotion recognition, generate empa-": "User:\nContinue\nthe\nfollowing sentence\nin\na"
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "coherent style:\n<transcript>"
        },
        {
          "for natural speech would be costly.": "speech input align with those the LLMs would",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "Assistant:"
        },
        {
          "for natural speech would be costly.": "produce if provided with the same linguistic",
          "speech emotion recognition, generate empa-": ""
        },
        {
          "for natural speech would be costly.": "content and emotion label.",
          "speech emotion recognition, generate empa-": "This process extends an ASR training sample"
        },
        {
          "for natural speech would be costly.": "",
          "speech emotion recognition, generate empa-": "(s, x) into a tuple (s, x, y), where y is the LLMâ€™s"
        },
        {
          "for natural speech would be costly.": "â€¢ We conduct quantitative evaluations and pro-",
          "speech emotion recognition, generate empa-": "response,\nrepresenting a natural continuation of"
        },
        {
          "for natural speech would be costly.": "vide\ndemonstrations\nto\nshowcase\nthat\nthe",
          "speech emotion recognition, generate empa-": "the transcript x and the corresponding speech s."
        },
        {
          "for natural speech would be costly.": "BLSP-Emo\napproach\nenables LLMs with",
          "speech emotion recognition, generate empa-": "The model is trained to generate the same contin-"
        },
        {
          "for natural speech would be costly.": "competitive capabilities to perform standalone",
          "speech emotion recognition, generate empa-": "uation when given speech input, using the same"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "label.": ""
        },
        {
          "label.": "The contributions of our work are as follows:"
        },
        {
          "label.": ""
        },
        {
          "label.": "â€¢ We introduce a new empathetic large speech-"
        },
        {
          "label.": "language model, adapted from an instruction-"
        },
        {
          "label.": "following LLM,\nthat can understand and re-"
        },
        {
          "label.": "spond to emotion cues in speech with empathy,"
        },
        {
          "label.": "while maintaining its ability to follow speech"
        },
        {
          "label.": "instructions and engage in conversations."
        },
        {
          "label.": ""
        },
        {
          "label.": "â€¢ We develop a two-stage approach to adapt"
        },
        {
          "label.": "LLMs to empathetic large speech-language"
        },
        {
          "label.": "models, using existing ASR data for semantic"
        },
        {
          "label.": "alignment and SER data for emotion align-"
        },
        {
          "label.": "ment,\naiming\nto\nensure\nthat\nresponses\nto"
        },
        {
          "label.": ""
        },
        {
          "label.": "speech input align with those the LLMs would"
        },
        {
          "label.": ""
        },
        {
          "label.": "produce if provided with the same linguistic"
        },
        {
          "label.": "content and emotion label."
        },
        {
          "label.": ""
        },
        {
          "label.": "â€¢ We conduct quantitative evaluations and pro-"
        },
        {
          "label.": "vide\ndemonstrations\nto\nshowcase\nthat\nthe"
        },
        {
          "label.": "BLSP-Emo\napproach\nenables LLMs with"
        },
        {
          "label.": "competitive capabilities to perform standalone"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SECOND STEP": "loss"
        },
        {
          "SECOND STEP": "her"
        },
        {
          "SECOND STEP": "the"
        },
        {
          "SECOND STEP": "her"
        },
        {
          "SECOND STEP": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LLM": ""
        },
        {
          "LLM": ""
        },
        {
          "LLM": ""
        },
        {
          "LLM": ""
        },
        {
          "LLM": ""
        },
        {
          "LLM": ""
        },
        {
          "LLM": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Transcript: so i was â€¦ you know)": "( Emotion: sad)"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "speech"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "tensive curated data and significant computational"
        },
        {
          "(Transcript: so i was â€¦ you know)": "resources, neither of which is readily accessible."
        },
        {
          "(Transcript: so i was â€¦ you know)": "Our approach to emotion alignment builds upon"
        },
        {
          "(Transcript: so i was â€¦ you know)": "and extends the behavior alignment method by cre-"
        },
        {
          "(Transcript: so i was â€¦ you know)": "ating natural continuations of speech transcripts"
        },
        {
          "(Transcript: so i was â€¦ you know)": "that reflect the emotional tones in the speech. This"
        },
        {
          "(Transcript: so i was â€¦ you know)": "is achieved by leveraging existing speech emo-"
        },
        {
          "(Transcript: so i was â€¦ you know)": "tion recognition (SER) datasets. Given a sample"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "(s, x, e) from a SER dataset, where e is the emotion"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "label annotated for speech s, we prompt the LLM"
        },
        {
          "(Transcript: so i was â€¦ you know)": "with the following instruction:"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "User: Continue the following sentence that reflects"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "a\n<emotion>\nemotion\ntone\nin\na coherent\nstyle:"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "<transcript>"
        },
        {
          "(Transcript: so i was â€¦ you know)": "Assistant:"
        },
        {
          "(Transcript: so i was â€¦ you know)": "This generates\na\ntext\ncontinuation y of\nthe"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "speech s that is consistent with the emotion label"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "e. We then initialize the BLSP-Emo model with"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "parameters of the BLSP model trained from the se-"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "mantic alignment stage and fine-tune it to generate"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "these continuations given only the speech as input,"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "as follows:"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "User: Continue the following sentence based on the"
        },
        {
          "(Transcript: so i was â€¦ you know)": "conveyed emotion\ntone in\na coherent\nstyle:"
        },
        {
          "(Transcript: so i was â€¦ you know)": "<speech\nfeatures>"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "Assistant:\n<text\ncontinuation>"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        },
        {
          "(Transcript: so i was â€¦ you know)": "This results in the primary emotion alignment"
        },
        {
          "(Transcript: so i was â€¦ you know)": "loss based on emotion-aware continuations:"
        },
        {
          "(Transcript: so i was â€¦ you know)": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We also introduce an auxiliary speech emotion": "recognition loss by directly predicting the emotion",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "We\nutilize\nthe\nencoder\npart\nof Whisper-large-"
        },
        {
          "We also introduce an auxiliary speech emotion": "label e from the hidden states output by the modal-",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "v2 (Radford et al., 2022) as the speech encoder,"
        },
        {
          "We also introduce an auxiliary speech emotion": "ity adapter, using pooling and a classification layer",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "convolution-based\nsubsampler\nas\nthe modality"
        },
        {
          "We also introduce an auxiliary speech emotion": "(with additional parameters Î·):",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "adapter, and Qwen-7B-Chat (Bai et al., 2023) as the"
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "LLM. More details can be found in Appendix C."
        },
        {
          "We also introduce an auxiliary speech emotion": "â„“ser",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "(3)\nEmotion(s, e) = âˆ’ log pÏˆ,Î¸,Î·(e|s)",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "3.3\nBaselines"
        },
        {
          "We also introduce an auxiliary speech emotion": "In this emotion alignment stage, we unfreeze the",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "parameters Ïˆ of the speech encoder and parameters",
          "3.2\nTraining Details": "We compare with the following baselines:"
        },
        {
          "We also introduce an auxiliary speech emotion": "Ï• of the LLM, in addition to the parameters Î¸ of the",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "Text|Whisper+LLM\nThese\nare\ncascaded sys-"
        },
        {
          "We also introduce an auxiliary speech emotion": "modality adapter and Î· of the classification layer.",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "tems where the LLM input\nis either\nthe ground-"
        },
        {
          "We also introduce an auxiliary speech emotion": "This allows the speech encoder to capture paralin-",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "truth transcript or\nthe\nrecognition output\nfrom"
        },
        {
          "We also introduce an auxiliary speech emotion": "guistic emotion cues and provides additional mod-",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "Whisper-large-v2, which includes a speech encoder,"
        },
        {
          "We also introduce an auxiliary speech emotion": "eling power in the LLM to address the discrepancy",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "as used in BLSP-Emo, and a speech decoder."
        },
        {
          "We also introduce an auxiliary speech emotion": "between speech and text. We follow the PLoRA",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "approach proposed in (Dong et al., 2024; Wang",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "BLSP\nThis model undergoes the semantic align-"
        },
        {
          "We also introduce an auxiliary speech emotion": "et al., 2024)\nto adapt parameters Ï• of\nthe LLM.",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "ment stage described in Section 2.2 and initializes"
        },
        {
          "We also introduce an auxiliary speech emotion": "The LoRA module is selectively applied only to",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "BLSP-Emo before the emotion alignment stage."
        },
        {
          "We also introduce an auxiliary speech emotion": "speech tokens, preserving the LLMâ€™s ability to en-",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "code text instructions and generate text.",
          "3.2\nTraining Details": "BLSP-SER\nThis model is initialized from BLSP"
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "and fine-tuned directly on the SER task.\nThe"
        },
        {
          "We also introduce an auxiliary speech emotion": "3\nExperiment Setup",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "only difference between BLSP-SER and BLSP-"
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "Emo is that the former is fine-tuned to predict the"
        },
        {
          "We also introduce an auxiliary speech emotion": "3.1\nDatasets",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "ground-truth emotion label, while the latter gen-"
        },
        {
          "We also introduce an auxiliary speech emotion": "We use publicly available ASR datasets in the se-",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "erates emotion-aware continuations, both utilizing"
        },
        {
          "We also introduce an auxiliary speech emotion": "mantic alignment stage and SER datasets in the",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "the same SER training datasets."
        },
        {
          "We also introduce an auxiliary speech emotion": "emotion alignment stage.",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "The ASR datasets include LibriSpeech (Panay-",
          "3.2\nTraining Details": "HuBERT|wav2vec2|WavLM+Whisper+LLM"
        },
        {
          "We also introduce an auxiliary speech emotion": "otov et al., 2015), CommonVoice 13.0 (Ardila et al.,",
          "3.2\nTraining Details": "These\nare\ncascaded\nsystems\ncomposed\nof\na"
        },
        {
          "We also introduce an auxiliary speech emotion": "2019),\nand the GigaSpeech M set\n(Chen et al.,",
          "3.2\nTraining Details": "standalone\nSER module\nin\naddition\nto\nthe"
        },
        {
          "We also introduce an auxiliary speech emotion": "2021), totaling approximately 1.9 million English",
          "3.2\nTraining Details": "Whisper+LLM pipeline.\nThe SER component"
        },
        {
          "We also introduce an auxiliary speech emotion": "(speech,\ntranscript) pairs, along with a compara-",
          "3.2\nTraining Details": "is fine-tuned on the SER training datasets from"
        },
        {
          "We also introduce an auxiliary speech emotion": "ble number of Chinese ASR samples randomly",
          "3.2\nTraining Details": "respective\nspeech\nencoder models,\nincluding"
        },
        {
          "We also introduce an auxiliary speech emotion": "selected from WeNetSpeech (Zhang et al., 2022).",
          "3.2\nTraining Details": "HuBERT large (Hsu et al., 2021), Wav2Vec 2.0"
        },
        {
          "We also introduce an auxiliary speech emotion": "The details of\nthe SER datasets and train/test",
          "3.2\nTraining Details": "large (Baevski et al., 2020), or WavLM large (Chen"
        },
        {
          "We also introduce an auxiliary speech emotion": "splits can be found in Appendix B. In summary,",
          "3.2\nTraining Details": "et\nal., 2022), with the\naddition of\nan average"
        },
        {
          "We also introduce an auxiliary speech emotion": "we train on IEMOCAP, MELD, CMU MOSEI,",
          "3.2\nTraining Details": "pooling layer and a linear classifier to predict the"
        },
        {
          "We also introduce an auxiliary speech emotion": "MEAD, and ESD, covering approximately 70k",
          "3.2\nTraining Details": "ground-truth emotion label.\nDuring evaluation,"
        },
        {
          "We also introduce an auxiliary speech emotion": "utterances in English and Chinese, and evaluate",
          "3.2\nTraining Details": "we directly report\nthe performance of\nthe SER"
        },
        {
          "We also introduce an auxiliary speech emotion": "SER performance on IEMOCAP and MELD as",
          "3.2\nTraining Details": "module for\nthe SER task.\nFor other\ntasks, we"
        },
        {
          "We also introduce an auxiliary speech emotion": "in-domain test sets, on RAVDESS and MerBench",
          "3.2\nTraining Details": "first use the SER module and the Whisper model"
        },
        {
          "We also introduce an auxiliary speech emotion": "as out-of-domain test sets, as well as on three lan-",
          "3.2\nTraining Details": "to\nrespectively\npredict\nthe\nemotion\nlabel\nand"
        },
        {
          "We also introduce an auxiliary speech emotion": "guages not seen in training: AESDD for Greek,",
          "3.2\nTraining Details": "transcript, and then use the following prompt\nto"
        },
        {
          "We also introduce an auxiliary speech emotion": "CaFE for French, and RESD for Russian. We fo-",
          "3.2\nTraining Details": "generate responses:"
        },
        {
          "We also introduce an auxiliary speech emotion": "cus on five emotion categories: neutral, happy, sad,",
          "3.2\nTraining Details": "User: The user's speech instruction, transcribed as"
        },
        {
          "We also introduce an auxiliary speech emotion": "angry, and surprise across all datasets.",
          "3.2\nTraining Details": "\"<transcript>\", conveys a <emotion> emotion tone."
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "Please\nprovide\na response."
        },
        {
          "We also introduce an auxiliary speech emotion": "We\nconduct\nevaluations\non\nemotion-aware",
          "3.2\nTraining Details": ""
        },
        {
          "We also introduce an auxiliary speech emotion": "",
          "3.2\nTraining Details": "Assistant:"
        },
        {
          "We also introduce an auxiliary speech emotion": "speech instruction capabilities based on a synthe-",
          "3.2\nTraining Details": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tunable": "",
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Tunable": "Modality",
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Tunable": "",
          "Speech Emotion Recognition (Acc%)": "RAVDESS"
        },
        {
          "Tunable": "Adapter",
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Tunable": "",
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Tunable": "",
          "Speech Emotion Recognition (Acc%)": "11.1"
        },
        {
          "Tunable": "",
          "Speech Emotion Recognition (Acc%)": "13.7"
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "11.1"
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "70.5"
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "72.0"
        },
        {
          "Tunable": "",
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "70.5"
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "64.0"
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "70.3"
        },
        {
          "Tunable": "âœ“",
          "Speech Emotion Recognition (Acc%)": "38.8"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "The BLSP model can process speech input but can-"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "not pick up paralinguistic cues for emotion as it is"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "only trained with semantic alignment. Conversely,"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "the encoder-based classification models can cap-"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "ture paralinguistic cues but lack a semantic under-"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "standing of emotion.\nIn contrast, BLSP-Emo can"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "simultaneously model linguistic and paralinguistic"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "emotion cues in speech,\nthanks to its end-to-end"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "modeling and two-stage alignment process."
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "Empathetic Response\nBeyond speech emotion"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "recognition, our primary concern is whether\nthe"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "model can understand both the semantic content"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "and paralinguistic emotion cues in speech and gen-"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "erate high-quality, empathetic responses. To eval-"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "uate this, we construct a synthetic emotion-aware"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "speech instruction dataset named SpeechAlpaca,"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "derived from the open-source instruction dataset"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "Alpaca-52k (Taori et al., 2023). Additionally, we"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "use a modified system prompt2\nthat emphasizes"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "both quality and empathy for all systems. We then"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "employ GPT-4 as an evaluator\nto independently"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "score the responses generated by different systems"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "in terms of quality and empathy on a scale from"
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "Your re-\n2System prompt: You are a helpful assistant."
        },
        {
          "Table 2: Results on SpeechAlpaca.": ""
        },
        {
          "Table 2: Results on SpeechAlpaca.": "sponse should fulfill requests with empathy toward the userâ€™s"
        },
        {
          "Table 2: Results on SpeechAlpaca.": "emotional tone."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "Table 1: SER results on various datasets. \"n/a\" used for Text+LLM when reference transcripts are not available.",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "has the ability to comprehend both linguistic con-",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Empathetic Response\nMethod\nSER"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "tent and paralinguistic emotion cues in speech and",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Quality\nEmpathy"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "respond accordingly. This enables the model\nto",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Text+LLM\n40.0\n8.9\n7.4"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Whisper+LLM\n40.1\n8.9\n7.4"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "not only follow task instructions but also demon-",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "BLSP\n36.8\n8.6\n7.1"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "BLSP-SER\n80.3\n1.9\n2.1"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "strate empathy toward the emotional tone conveyed",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "BLSP-Emo\n83.8\n8.8\n7.7"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "in the speech. Next, we will present\nresults on",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "HuBERT+Whisper+LLM\n76.3\n8.9\n7.6"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "speech emotion recognition, instruction-following",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "wav2vec2+Whisper+LLM\n83.3\n9.0\n7.7"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "WavLM+Whisper+LLM\n80.8\n8.9\n7.8"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "with empathetic responses, multi-turn conversation,",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "SALMONN-7B\n43.8\n2.4\n1.9"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "and generalization to other languages.",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Table 2: Results on SpeechAlpaca."
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "4.1\nMain Results",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "Speech Emotion Recognition\nTo prompt\nthe",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "The BLSP model can process speech input but can-"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "LLM-based generative models to perform the SER",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "not pick up paralinguistic cues for emotion as it is"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "task, we use the following prompt:",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "only trained with semantic alignment. Conversely,"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "User:\nPlease identify the emotion\ntone\nof\nthe",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "sentence provided below. Select from the following",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "the encoder-based classification models can cap-"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "options: neutral, sad, angry, happy, or surprise.",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "ture paralinguistic cues but lack a semantic under-"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "\\n\\nSentence:\n<transcript|speech>",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "standing of emotion.\nIn contrast, BLSP-Emo can"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "Assistant:",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "simultaneously model linguistic and paralinguistic"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "where <transcript|speech> represents the transcript",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "emotion cues in speech,\nthanks to its end-to-end"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "for cascaded systems or speech features for end-to-",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "modeling and two-stage alignment process."
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "end systems. Results are shown in Table 1.",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "The BLSP-Emo model achieves the highest over-",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Empathetic Response\nBeyond speech emotion"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "all recognition accuracy across five test sets, along",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "recognition, our primary concern is whether\nthe"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "with the BLSP-SER model, which is fine-tuned",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "model can understand both the semantic content"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "from the same BLSP model but specifically for the",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "and paralinguistic emotion cues in speech and gen-"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "SER task. BLSP-Emo significantly outperforms",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "erate high-quality, empathetic responses. To eval-"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "all other models, including SALMONN-7B (Tang",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "uate this, we construct a synthetic emotion-aware"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "et al., 2023), which adapts a large language model",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "speech instruction dataset named SpeechAlpaca,"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "to various speech tasks, including speech emotion",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "derived from the open-source instruction dataset"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "recognition.",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Alpaca-52k (Taori et al., 2023). Additionally, we"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "The\nText|Whisper+LLM cascaded\nsystems",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "use a modified system prompt2\nthat emphasizes"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "achieve\ncomparable\nor\nbetter\nresults\nthan\nthe",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "both quality and empathy for all systems. We then"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "encoder-based classification models on the MELD",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "employ GPT-4 as an evaluator\nto independently"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "and MerBench test sets, but they perform the worst",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "score the responses generated by different systems"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "on the IEMOCAP and RAVDESS test sets. This",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "in terms of quality and empathy on a scale from"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "suggests that while an LLM can capture linguis-",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "Your re-\n2System prompt: You are a helpful assistant."
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "tic cues for emotions,\nthe text-only mode limits",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": ""
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "sponse should fulfill requests with empathy toward the userâ€™s"
        },
        {
          "âœ“\nâœ“\nSALMONN-7B": "its ability for comprehensive emotion recognition.",
          "67.0": "",
          "32.9\n38.8\n45.8\n41.7": "emotional tone."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: establishedduringsemanticalignment.",
      "data": [
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "on natural speech, even though it does not\nlead"
        },
        {
          "higher performance in speech emotion recognition": "to any noticeable differences on the SpeechAlpaca"
        },
        {
          "higher performance in speech emotion recognition": "test set or in the evaluation of empathetic responses."
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "4.3\nAnalysis"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "We perform additional\nanalysis\ncomparing our"
        },
        {
          "higher performance in speech emotion recognition": "training strategies against\ntwo recent approaches"
        },
        {
          "higher performance in speech emotion recognition": "in the literature of speech-language models with"
        },
        {
          "higher performance in speech emotion recognition": "emotion-aware capabilities."
        },
        {
          "higher performance in speech emotion recognition": "First, we compare our approach to the method"
        },
        {
          "higher performance in speech emotion recognition": "of E-chat (Xue et al., 2023) and Spoken-LLM (Lin"
        },
        {
          "higher performance in speech emotion recognition": "et\nal.,\n2024),\nwhich\nconstructed\nsynthesized"
        },
        {
          "higher performance in speech emotion recognition": "emotion-aware speech instruction data using ex-"
        },
        {
          "higher performance in speech emotion recognition": "pressive\ntext-to-speech tools\nand ChatGPT. As"
        },
        {
          "higher performance in speech emotion recognition": "noted previously and found in our preliminary stud-"
        },
        {
          "higher performance in speech emotion recognition": "ies, models trained on synthesized speech fail\nto"
        },
        {
          "higher performance in speech emotion recognition": "generalize to natural human speech. Given that our"
        },
        {
          "higher performance in speech emotion recognition": "approach also requires constructing synthesized"
        },
        {
          "higher performance in speech emotion recognition": "emotion-aware continuation data for natural speech,"
        },
        {
          "higher performance in speech emotion recognition": "a critical question arises:\nis it better to use ChatGPT"
        },
        {
          "higher performance in speech emotion recognition": "for data construction, as commonly done in the lit-"
        },
        {
          "higher performance in speech emotion recognition": "erature, or to use the same LLM that BLSP-Emo is"
        },
        {
          "higher performance in speech emotion recognition": "adapted from?"
        },
        {
          "higher performance in speech emotion recognition": "To address this, we trained a new model named"
        },
        {
          "higher performance in speech emotion recognition": "BLSP-ChatGPT, utilizing ChatGPT to generate"
        },
        {
          "higher performance in speech emotion recognition": "emotion-aware continuations\nfor emotion align-"
        },
        {
          "higher performance in speech emotion recognition": "ment,\nstarting from the same pretrained BLSP"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "model as BLSP-Emo. As shown in Table 5, while"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "BLSP-ChatGPT achieves higher SER performance"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "than BLSP, its quality and empathy ratings in empa-"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "thetic responses are notably lower. BLSP-ChatGPT"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "performs worse than BLSP-Emo across all metrics."
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "We hypothesize that the emotion-aware continua-"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "tions generated by ChatGPT may not align well"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "with the likely responses generated by the internal"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "LLM in BLSP-Emo. Consequently, the alignment"
        },
        {
          "higher performance in speech emotion recognition": "process may focus on narrowing the distribution"
        },
        {
          "higher performance in speech emotion recognition": ""
        },
        {
          "higher performance in speech emotion recognition": "gap between ChatGPT and the internal LLM, rather"
        },
        {
          "higher performance in speech emotion recognition": "than learning to capture the paralinguistic emotion"
        },
        {
          "higher performance in speech emotion recognition": "cues in speech to fit into the aligned semantic space"
        },
        {
          "higher performance in speech emotion recognition": "established during semantic alignment."
        },
        {
          "higher performance in speech emotion recognition": "Next,\nwe\ncompare\nour\napproach\nagainst"
        },
        {
          "higher performance in speech emotion recognition": "the multi-task\nlearning\nstrategy\nemployed\nby"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: , while BLSP-MultiTask signif-",
      "data": [
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": ""
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": "Training Task"
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": ""
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": "continuation"
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": "SER"
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": "emotion-aware continuation"
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": "emotion-aware continuation"
        },
        {
          "Table 4: Ablation study on the BLSP pretraining stage for semantic alignment and the auxiliary SER loss.": "continuation + SER"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: , while BLSP-MultiTask signif-",
      "data": [
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "BLSP-MultiTask\ncontinuation + SER\nsame LLM + Human",
          "68.9\n54.2\n68.0\n6.1\n6.0": "75.3\n71.5\n77.8\n8.3\n7.2"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "Table 5: Comparison with ChatGPT data construction and multi-task learning.",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "other\nlarge\nspeech-language models,\nsuch\nas",
          "68.9\n54.2\n68.0\n6.1\n6.0": "with competitive abilities to understand both seman-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "SALMONN (Tang et al., 2023), which aims to",
          "68.9\n54.2\n68.0\n6.1\n6.0": "tics and emotions in speech and perform standalone"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "understand semantic content and various paralin-",
          "68.9\n54.2\n68.0\n6.1\n6.0": "speech emotion recognition, generate empathetic"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "guistic cues. As demonstrated in previous sessions,",
          "68.9\n54.2\n68.0\n6.1\n6.0": "responses, and engage in multi-turn conversations."
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "BLSP-Emo significantly outperforms SALMONN-",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "7B in both standalone emotion recognition and",
          "68.9\n54.2\n68.0\n6.1\n6.0": "Limitations"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "emotion-aware instruction following.\nHowever,",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "Evaluation of Empathy.\nWhile our methods for"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "a question remains: can we replace the emotion-",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "assessing empathetic responses provide valuable"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "aware continuation task employed in the emotion",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "insights, there are several limitations. Synthesized"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "alignment stage with a multi-task framework in-",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "speech, as in SpeechAlpaca,\nlacks variations in"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "volving two tasks: emotion-agnostic continuation",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "factors such as speaker\nids and emotion expres-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "and speech emotion recognition?",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "sions, potentially limiting the accuracy of model"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "To answer this, we use the SER training datasets",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "performance evaluation on natural human speech."
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "to construct two tasks: one for standalone SER and",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "Additionally, in the evaluation of multi-turn conver-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "another\nfor emotion-agnostic continuation.\nThe",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "sations on IEMOCAP, we only assess a single-turn"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "resulting model\nis named BLSP-MultiTask. As",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "response within a multi-turn context. This may not"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "shown in Table 5, while BLSP-MultiTask signif-",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "fully capture the modelâ€™s performance in continu-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "icantly improves the SER accuracy of the BLSP",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "ous conversations and how empathetic responses,"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "model,\nits response quality is lower than that of",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "sometimes repetitive, are perceived from a user"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "BLSP. BLSP-MultiTask also performs worse than",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "experience perspective."
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "BLSP-Emo across all metrics. This comparison",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "highlights the importance of\nthe emotion-aware",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "Broader Applicability.\nOur current\napproach"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "continuation task in developing effective empa-",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "to modeling emotions in speech relies on a lim-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "thetic speech-language models.",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "ited number of emotion states annotated in SER"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "datasets. However, human speech has rich expres-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "5\nRelated Works",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "sions of emotions that are more nuanced and may"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "See Appendix A for a discussion on related works.",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "include variations of emotion in lengthy speech"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "segments. Additionally,\nthere are other types of"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "6\nConclusion",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "",
          "68.9\n54.2\n68.0\n6.1\n6.0": "paralinguistic cues in human speech, such as tones"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "In summary,\nthis paper presents BLSP-Emo,\na",
          "68.9\n54.2\n68.0\n6.1\n6.0": "and intentions,\nthat are important\nin communica-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "novel approach to build empathetic large speech-",
          "68.9\n54.2\n68.0\n6.1\n6.0": "tion but not addressed in this work. The two-stage"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "language models by utilizing existing speech recog-",
          "68.9\n54.2\n68.0\n6.1\n6.0": "alignment approach, however, could be expanded"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "nition and speech emotion recognition datasets,",
          "68.9\n54.2\n68.0\n6.1\n6.0": "to achieve general modeling of paralinguistic cues"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "through a two stage alignment process: semantic",
          "68.9\n54.2\n68.0\n6.1\n6.0": "through end-to-end modeling on large speech-text"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "alignment and emotion alignment. Through quanti-",
          "68.9\n54.2\n68.0\n6.1\n6.0": "datasets, while retaining instruction-following ca-"
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "tative evaluations, we demonstrate that the BLSP-",
          "68.9\n54.2\n68.0\n6.1\n6.0": "pabilities. We leave this to future work."
        },
        {
          "BLSP-ChatGPT\nemotion-aware continuation\nGPT-3.5-turbo": "Emo approach extends instruction-following LLMs",
          "68.9\n54.2\n68.0\n6.1\n6.0": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "IEEE Journal of Se-\nfull stack speech processing."
        },
        {
          "References": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "lected Topics in Signal Processing, 16(6):1505â€“1518."
        },
        {
          "References": "Ahmad,\nIlge Akkaya,\nFlorencia Leoni Aleman,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "Diogo Almeida, Janko Altenschmidt, Sam Altman,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Hyung Won Chung, Le Hou, Shayne Longpre, Bar-"
        },
        {
          "References": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi"
        },
        {
          "References": "arXiv preprint arXiv:2303.08774.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Wang, Mostafa Dehghani, Siddhartha Brahma, et al."
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "2022. H. chi, jeff dean, jacob devlin, adam roberts,"
        },
        {
          "References": "Rosana Ardila, Megan Branson, Kelly Davis, Michael",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "denny zhou, quoc v.\nle, and jason wei. 2022. scal-"
        },
        {
          "References": "Henretty, Michael Kohler,\nJosh Meyer, Reuben",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "arXiv\ning instruction-finetuned language models."
        },
        {
          "References": "Morais, Lindsay Saunders, Francis M Tyers, and",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "preprint arXiv:2210.11416."
        },
        {
          "References": "Gregor Weber. 2019. Common voice: A massively-",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "arXiv\npreprint\nmultilingual\nspeech\ncorpus.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,"
        },
        {
          "References": "arXiv:1912.06670.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Bin Wang, Linke Ouyang, Xilin Wei, Songyang"
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Zhang, Haodong Duan, Maosong Cao, Wenwei"
        },
        {
          "References": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue"
        },
        {
          "References": "and Michael Auli. 2020. wav2vec 2.0: A framework",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Zhang, Wei Li,\nJingwen Li, Kai Chen, Conghui"
        },
        {
          "References": "for self-supervised learning of speech representations.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and"
        },
        {
          "References": "Advances in neural information processing systems,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Jiaqi Wang. 2024.\nInternlm-xcomposer2: Mastering"
        },
        {
          "References": "33:12449â€“12460.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "free-form text-image composition and comprehen-"
        },
        {
          "References": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,",
          "Wavlm: Large-scale self-supervised pre-training for": "sion in vision-language large model. arXiv preprint"
        },
        {
          "References": "Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei",
          "Wavlm: Large-scale self-supervised pre-training for": "arXiv:2401.16420."
        },
        {
          "References": "Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,",
          "Wavlm: Large-scale self-supervised pre-training for": "Philippe Gournay, Olivier Lahaie, and Roch Lefebvre."
        },
        {
          "References": "Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,",
          "Wavlm: Large-scale self-supervised pre-training for": "2018. A canadian french emotional speech dataset."
        },
        {
          "References": "Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong",
          "Wavlm: Large-scale self-supervised pre-training for": "In Proceedings of the 9th ACM multimedia systems"
        },
        {
          "References": "Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-",
          "Wavlm: Large-scale self-supervised pre-training for": "conference, pages 399â€“402."
        },
        {
          "References": "guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,",
          "Wavlm: Large-scale self-supervised pre-training for": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,"
        },
        {
          "References": "Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-",
          "Wavlm: Large-scale self-supervised pre-training for": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-"
        },
        {
          "References": "uan Zhang, Yichang Zhang, Zhenru Zhang, Chang",
          "Wavlm: Large-scale self-supervised pre-training for": "rahman Mohamed. 2021. Hubert: Self-supervised"
        },
        {
          "References": "Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang",
          "Wavlm: Large-scale self-supervised pre-training for": "speech representation learning by masked prediction"
        },
        {
          "References": "Zhu. 2023. Qwen technical report.",
          "Wavlm: Large-scale self-supervised pre-training for": "IEEE/ACM Transactions on Audio,\nof hidden units."
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Speech, and Language Processing, 29:3451â€“3460."
        },
        {
          "References": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen,"
        },
        {
          "References": "nette N Chang,\nSungbok Lee,\nand Shrikanth S",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit"
        },
        {
          "References": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Sivasankaran, Linquan Liu, et al. 2024. Wavllm:"
        },
        {
          "References": "dyadic motion capture database. Language resources",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Towards robust and adaptive speech large language"
        },
        {
          "References": "and evaluation, 42:335â€“359.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "model. arXiv preprint arXiv:2404.00656."
        },
        {
          "References": "Santiago Castro, Devamanyu Hazarika, VerÃ³nica PÃ©rez-",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang"
        },
        {
          "References": "Rosas, Roger Zimmermann, Rada Mihalcea,\nand",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Sun, Lan Chen, Bin Liu, and Jianhua Tao. 2024."
        },
        {
          "References": "Soujanya Poria. 2019. Towards multimodal sarcasm",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Merbench:\nA unified\nevaluation\nbenchmark\nfor"
        },
        {
          "References": "detection.\nIn Proceedings of the 57th Annual Meet-",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "arXiv preprint\nmultimodal emotion recognition."
        },
        {
          "References": "ing of the Association for Computational Linguistics,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "arXiv:2401.03429."
        },
        {
          "References": "pages 4619â€“4629, Florence,\nItaly. Association for",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "Computational Linguistics.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee."
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "2024. Advancing large language models to capture"
        },
        {
          "References": "Guoguo Chen, Shuzhou Chai, Guanbo Wang,\nJiayu",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "varied speaking styles and respond properly in spoken"
        },
        {
          "References": "Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "conversations. arXiv preprint arXiv:2402.12786."
        },
        {
          "References": "Povey,\nJan Trmal,\nJunbo Zhang, et al. 2021.\nGi-",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "gaspeech: An evolving, multi-domain asr corpus with",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Steven R Livingstone and Frank A Russo. 2018. The"
        },
        {
          "References": "arXiv preprint\n10,000 hours of\ntranscribed audio.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "ryerson audio-visual database of emotional speech"
        },
        {
          "References": "arXiv:2106.06909.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "and song (ravdess): A dynamic, multimodal set of fa-"
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "cial and vocal expressions in north american english."
        },
        {
          "References": "Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu,",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "PloS one, 13(5):e0196391."
        },
        {
          "References": "Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "Zheng, et al. 2023. Lauragpt: Listen, attend, under-",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-"
        },
        {
          "References": "stand, and regenerate audio with gpt. arXiv preprint",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "yang Lin, Chuanqi Tan, Chang Zhou, and Jingren"
        },
        {
          "References": "arXiv:2310.04673.",
          "Wavlm: Large-scale self-supervised pre-training for": ""
        },
        {
          "References": "",
          "Wavlm: Large-scale self-supervised pre-training for": "Zhou. 2023. # instag: Instruction tagging for analyz-"
        },
        {
          "References": "Sanyuan Chen, Chengyi Wang,\nZhengyang Chen,",
          "Wavlm: Large-scale self-supervised pre-training for": "ing supervised fine-tuning of large language models."
        },
        {
          "References": "Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki",
          "Wavlm: Large-scale self-supervised pre-training for": "In The Twelfth International Conference on Learning"
        },
        {
          "References": "Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.",
          "Wavlm: Large-scale self-supervised pre-training for": "Representations."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Sanjeev Khudanpur. 2015. Librispeech: an asr cor-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "visual dataset for emotional talking-face generation."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "pus based on public domain audio books.\nIn 2015",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "In European Conference on Computer Vision, pages"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "IEEE international conference on acoustics, speech",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "700â€“717. Springer."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "and signal processing (ICASSP), pages 5206â€“5210.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shu-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "IEEE.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "jie Liu, Yashesh Gaur, Zhuo Chen,\nJinyu Li, and"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Furu Wei. 2023b. Viola: Unified codec language"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "models for speech recognition, synthesis, and trans-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "halcea. 2018.\nMeld:\nA multimodal multi-party",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "lation. arXiv preprint arXiv:2305.16107."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "dataset\nfor\nemotion recognition in conversations.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "arXiv preprint arXiv:1810.02508.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Zhang, Qian Chen,\nand Lei Xie. 2023.\nE-chat:"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Emotion-sensitive spoken dialogue system with large"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "language models. arXiv preprint arXiv:2401.00475."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "man, Christine McLeavey, and Ilya Sutskever. 2022.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Robust speech recognition via large-scale weak su-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria,"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "pervision. arxiv. arXiv preprint arXiv:2212.04356.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Erik Cambria, and Louis-Philippe Morency. 2018."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Multimodal\nlanguage analysis\nin the wild: Cmu-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Paul\nK\nRubenstein,\nChulayuth\nAsawaroengchai,",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "mosei dataset and interpretable dynamic fusion graph."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Duc Dung Nguyen, Ankur Bapna, ZalÃ¡n Borsos,",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "In Proceedings of the 56th Annual Meeting of the As-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "FÃ©lix de Chaumont Quitry, Peter Chen, Dalia El",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "sociation for Computational Linguistics (Volume 1:"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Badawy, Wei Han, Eugene Kharitonov, et al. 2023.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Long Papers), pages 2236â€“2246."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Audiopalm: A large language model that can speak",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "and listen. arXiv preprint arXiv:2306.12925.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao,"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen,"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang,",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Chenchen Zeng, et al. 2022. Wenetspeech: A 10000+"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "hours multi-domain mandarin corpus\nfor\nspeech"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Shi. 2023. Llasm: Large language and speech model.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "recognition.\nIn ICASSP 2022-2022 IEEE Interna-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "tional Conference on Acoustics, Speech and Signal"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Processing (ICASSP), pages 6182â€“6186. IEEE."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Zhang. 2023.\nSalmonn: Towards generic hearing",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Dong Zhang,\nShimin Li, Xin Zhang,\nJun Zhan,"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "abilities for large language models. arXiv preprint",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "arXiv:2310.13289.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Speechgpt: Empowering large language models with"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "intrinsic cross-modal conversational abilities. arXiv"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Rohan Taori,\nIshaan Gulrajani, Tianyi Zhang, Yann",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "preprint arXiv:2305.11000."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "and Tatsunori B. Hashimoto. 2023. Stanford alpaca:",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Kun Zhou, Berrak Sisman, Rui Liu,\nand Haizhou"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "https://",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "An instruction-following llama model.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Li. 2022.\nEmotional voice\nconversion:\nTheory,"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "github.com/tatsu-lab/stanford_alpaca.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "databases and esd. Speech Communication, 137:1â€“"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "18."
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "bert, Amjad Almahairi, Yasmine Babaei, Nikolay",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "A\nRelated Works"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Bhosale,\net\nal.\n2023.\nLlama 2:\nOpen founda-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "arXiv preprint\ntion and fine-tuned chat models.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "Large Speech-Language Models\nLarge Lan-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "arXiv:2307.09288.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "guage Models (LLMs) have achieved remarkable"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "performance on various natural language process-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Nikolaos Vryzas, Rigas Kotsakis, Aikaterini Liatsou,",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Charalampos A Dimoulas, and George Kalliris. 2018.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "ing tasks\n(Achiam et al., 2023; Touvron et al.,"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Speech emotion recognition for performance inter-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "2023). Ongoing research aims to integrate speech"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Journal of\naction.\nthe Audio Engineering Society,",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "signals into pre-trained, decoder-only text-based"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "66(6):457â€“467.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "LLMs, creating unified models capable of handling"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Chen Wang, Minpeng Liao, Zhongqiang Huang, Jin-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "diverse speech processing tasks. Models like Au-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "liang Lu,\nJunhong Wu, Yuchen Liu, Chengqing",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "dioPaLM (Rubenstein et al., 2023), VIOLA (Wang"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Zong, and Jiajun Zhang. 2023a. Blsp: Bootstrapping",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "et al., 2023b), and LauraGPT (Chen et al., 2023)"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "language-speech pre-training via behavior alignment.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "have emerged from such efforts, primarily trained"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "Chen Wang, Minpeng Liao, Zhongqiang Huang, and Ji-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "through multi-task learning for various speech pro-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "ajun Zhang. 2024. Blsp-kd: Bootstrapping language-",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "cessing tasks, without utilizing conversational com-"
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "speech pre-training via knowledge distillation.",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": ""
        },
        {
          "Vassil Panayotov, Guoguo Chen, Daniel Povey, and": "",
          "Chen Change Loy. 2020. Mead: A large-scale audio-": "petencies inherent\nin LLMs. Recent models like"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Train Data",
          "Source": "",
          "Language": "",
          "Emotion": "",
          "#Utts": ""
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy, sad, angry, excited, frustrated, fear, surprise,",
          "#Utts": ""
        },
        {
          "Dataset": "IEMOCAP Session 1-4",
          "Source": "Act",
          "Language": "English",
          "Emotion": "",
          "#Utts": "2610"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "disappointed",
          "#Utts": ""
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (joy), sad (sadness), angry (anger), surprise,",
          "#Utts": ""
        },
        {
          "Dataset": "MELD train",
          "Source": "Friends TV",
          "Language": "English",
          "Emotion": "",
          "#Utts": "5920"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "disgust, fear",
          "#Utts": ""
        },
        {
          "Dataset": "ESD",
          "Source": "Act",
          "Language": "English&Chinese",
          "Emotion": "neutral, happy, sad, angry, surprise",
          "#Utts": "33443"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (happiness), sad (sadness), angry (anger), sur-",
          "#Utts": ""
        },
        {
          "Dataset": "CMU MOSEI",
          "Source": "YouTube",
          "Language": "English",
          "Emotion": "",
          "#Utts": "13557"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "prise, disgust, fear",
          "#Utts": ""
        },
        {
          "Dataset": "MEAD",
          "Source": "Act",
          "Language": "English",
          "Emotion": "neutral, happy, sad, angry, surprise, contempt, disgust, fear",
          "#Utts": "15345"
        },
        {
          "Dataset": "Test Data",
          "Source": "",
          "Language": "",
          "Emotion": "",
          "#Utts": ""
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy, sad, angry, excited, frustrated, fear, surprise,",
          "#Utts": ""
        },
        {
          "Dataset": "IEMOCAP Session 5",
          "Source": "Act",
          "Language": "English",
          "Emotion": "",
          "#Utts": "942"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "disappointed",
          "#Utts": ""
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (joy), sad (sadness), angry (anger), surprise,",
          "#Utts": ""
        },
        {
          "Dataset": "MELD test",
          "Source": "Friends TV",
          "Language": "English",
          "Emotion": "",
          "#Utts": "1577"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "disgust, fear",
          "#Utts": ""
        },
        {
          "Dataset": "RAVDESS",
          "Source": "Act",
          "Language": "English",
          "Emotion": "neutral, happy, sad, angry, surprise, calm, fearful, disgust",
          "#Utts": "864"
        },
        {
          "Dataset": "SpeechAlpaca",
          "Source": "Microsoft TTS",
          "Language": "English",
          "Emotion": "neutral, happy (cheerful), sad, angry",
          "#Utts": "400"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (happiness), sad (sadness), angry (anger), sur-",
          "#Utts": ""
        },
        {
          "Dataset": "MerBench test1",
          "Source": "Movies&TV series",
          "Language": "Chinese",
          "Emotion": "",
          "#Utts": "354"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "prise, worried",
          "#Utts": ""
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (happiness), sad (sadness), angry (anger), sur-",
          "#Utts": ""
        },
        {
          "Dataset": "MerBench test2",
          "Source": "Movies&TV series",
          "Language": "Chinese",
          "Emotion": "",
          "#Utts": "360"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "prise, worried",
          "#Utts": ""
        },
        {
          "Dataset": "AESDD",
          "Source": "Act",
          "Language": "Greek",
          "Emotion": "happy (happiness), sad (sadness), angry (anger), disgust, fear",
          "#Utts": "724"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (happiness), sad (sadness), angry (anger), sur-",
          "#Utts": ""
        },
        {
          "Dataset": "CaFE",
          "Source": "Act",
          "Language": "French",
          "Emotion": "",
          "#Utts": "648"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "prise, disgust, fear",
          "#Utts": ""
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "neutral, happy (happiness), sad (sadness), angry (anger), fear,",
          "#Utts": ""
        },
        {
          "Dataset": "RESD test",
          "Source": "Act",
          "Language": "Russian",
          "Emotion": "",
          "#Utts": "158"
        },
        {
          "Dataset": "",
          "Source": "",
          "Language": "",
          "Emotion": "enthusiasm, disgust",
          "#Utts": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": "prise, disgust, fear"
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": "neutral, happy (happiness), sad (sadness), angry (anger), fear,"
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": "enthusiasm, disgust"
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": "consistency, while struck-out labels signify emotion categories not considered in our experiment."
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": "B"
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        },
        {
          "neutral, happy (happiness), sad (sadness), angry (anger), sur-": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "script contains fewer than 5 words in English or": "fewer than 5 characters in Chinese.",
          "randomly select one as the emotion label for the": "instruction. This process is used to select 100 in-"
        },
        {
          "script contains fewer than 5 words in English or": "We\nevaluate\nSER performance\non\nboth\nin-",
          "randomly select one as the emotion label for the": "structions for each of the four emotion categories."
        },
        {
          "script contains fewer than 5 words in English or": "domain datasets (IEMOCAP session 5, MELD test",
          "randomly select one as the emotion label for the": "Subsequently, we synthesize expressive speech us-"
        },
        {
          "script contains fewer than 5 words in English or": "set) and out-of-domain datasets (RAVDESS (Liv-",
          "randomly select one as the emotion label for the": "ing the selected emotion label with Microsoftâ€™s"
        },
        {
          "script contains fewer than 5 words in English or": "ingstone and Russo, 2018), MerBench (Lian et al.,",
          "randomly select one as the emotion label for the": "TTS API."
        },
        {
          "script contains fewer than 5 words in English or": "2024)).\nAdditionally, we\nreport\nthe generaliz-",
          "randomly select one as the emotion label for the": "We present examples of model outputs on the"
        },
        {
          "script contains fewer than 5 words in English or": "ability of SER performance on three other\nlan-",
          "randomly select one as the emotion label for the": "SpeechAlpaca test set in Table 7. To evaluate the"
        },
        {
          "script contains fewer than 5 words in English or": "guages: AESDD (Vryzas et al., 2018) for Greek,",
          "randomly select one as the emotion label for the": "empathetic responses, we use GPT-4 to assess the"
        },
        {
          "script contains fewer than 5 words in English or": "CaFE (Gournay\net\nal.,\n2018)\nfor French,\nand",
          "randomly select one as the emotion label for the": "quality of responses with the prompt in Listing 1"
        },
        {
          "script contains fewer than 5 words in English or": "RESD (Vryzas et al., 2018) for Russian.",
          "randomly select one as the emotion label for the": "and the empathy of responses with the prompt in"
        },
        {
          "script contains fewer than 5 words in English or": "",
          "randomly select one as the emotion label for the": "Listing 2."
        },
        {
          "script contains fewer than 5 words in English or": "C\nTraining Details",
          "randomly select one as the emotion label for the": ""
        },
        {
          "script contains fewer than 5 words in English or": "We\nutilize\nthe\nencoder\npart\nof Whisper-large-",
          "randomly select one as the emotion label for the": "Listing 1: Prompt for response quality evaluation on"
        },
        {
          "script contains fewer than 5 words in English or": "v2 (Radford et al., 2022) as the speech encoder",
          "randomly select one as the emotion label for the": "SpeechAlpaca"
        },
        {
          "script contains fewer than 5 words in English or": "",
          "randomly select one as the emotion label for the": "Given\nthe\noriginal\ninstruction\nprovided"
        },
        {
          "script contains fewer than 5 words in English or": "and employ Qwen-7B-Chat (Bai et al., 2023) as",
          "randomly select one as the emotion label for the": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C\nTraining Details": "We\nutilize\nthe\nencoder\npart\nof Whisper-large-"
        },
        {
          "C\nTraining Details": "v2 (Radford et al., 2022) as the speech encoder"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "and employ Qwen-7B-Chat (Bai et al., 2023) as"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "the LLM. The modality adapter\nis composed of"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "three 1-dimensional convolution layers followed"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "by a bottleneck layer with a hidden dimension of"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "512. The convolution layers are designed to reduce"
        },
        {
          "C\nTraining Details": "the length of the speech features by a factor of 8,"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "with each layer having a stride size of 2, a kernel"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "size of 5, and a padding of 2."
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "During the semantic alignment stage, we freeze"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "the speech encoder and LLM, and fine-tune the"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "modality adapter\nfor 1 epoch with a batch size"
        },
        {
          "C\nTraining Details": "of 768.\nThis process takes about 2.5 days on 4"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "A100 GPUs. During the emotion alignment stage,"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "we fine-tune the speech encoder, modality adapter,"
        },
        {
          "C\nTraining Details": "LLM4, and SER classifier for 3 epochs with a batch"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "size of 128. This process takes about 3 hours on 4"
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "A100 GPUs."
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": ""
        },
        {
          "C\nTraining Details": "D\nEvaluation on Empathetic Responses"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: Examples of model outputs in multi-turn conversation constructed from IEMOCAP. The user inputs",
      "data": [
        {
          "Example 1": "Whisper+LLM",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Assistant: Iâ€™m sorry to hear that youâ€™re experiencing traffic congestion. Here are some tips that may help you avoid a traffic jam:\n..."
        },
        {
          "Example 1": "WavLM+Whisper+LLM",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Assistant:\nIâ€™m sorry to hear that youâ€™re feeling sad about the possibility of getting stuck in a traffic jam. Here are some tips that"
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "may help you avoid one:\n..."
        },
        {
          "Example 1": "BLSP-Emo",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Assistant: Iâ€™m sorry to hear that youâ€™re feeling sad about the traffic jam. It can be really frustrating and stressful to be stuck in traffic"
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "for an extended period of time. Here are a few suggestions that might help you avoid traffic jams in the future:\n..."
        },
        {
          "Example 1": "Example 2",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "User: <speech>Come up with a 5-step process for making a decision.<angry/></speech>"
        },
        {
          "Example 1": "Whisper+LLM",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Assistant: Sure, Iâ€™d be happy to help you come up with a five-step process for making a decision. Here are the steps:\n..."
        },
        {
          "Example 1": "WavLM+Whisper+LLM",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Assistant: Iâ€™m sorry to hear that youâ€™re feeling angry. Itâ€™s understandable that making decisions can be frustrating and overwhelming"
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "at times. Hereâ€™s a five-step process that may help you make a decision:\\n\\n1. Define the problem:\n... 2. Gather information:\n... 3."
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Consider your options:\n... 4. Make a decision:\n... 5. Take action:\n..."
        },
        {
          "Example 1": "BLSP-Emo",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "Assistant: Iâ€™m sorry to hear that youâ€™re feeling angry. Itâ€™s important to take the time to process your emotions and make a decision"
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "that feels right for you. Hereâ€™s a five-step process that you can use to make a decision when youâ€™re feeling angry:\\n\\n 1. Take a"
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "deep breath and allow yourself to feel your anger.\n... 2. Identify the source of your anger.\n... 3. Consider the potential consequences"
        },
        {
          "Example 1": "",
          "User: <speech>Suggest the best way to avoid a traffic jam.<sad/></speech>": "of each option.\n... 4. Seek advice from a trusted friend or mentor.\n... 5. Make a decision and take action.\n..."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 8: Examples of model outputs in multi-turn conversation constructed from IEMOCAP. The user inputs",
      "data": [
        {
          "User: yeah": "Assistant: i donâ€™t okay okay we can do this we can do this itâ€™s okay weâ€™ll make it work"
        },
        {
          "User: yeah": ""
        },
        {
          "User: yeah": "User: yeah"
        },
        {
          "User: yeah": "Assistant: i love you"
        },
        {
          "User: yeah": "User: i love you iâ€™m going to miss you so much sigh"
        },
        {
          "User: yeah": "Assistant: i love you too and iâ€™ll miss you too"
        },
        {
          "User: yeah": "User: <speech>i love you iâ€™m going to miss you so much sigh<sad/></speech>"
        },
        {
          "User: yeah": "Assistant: i love you too and iâ€™ll miss you too. but we can still stay in touch and talk whenever we want. and who knows, maybe weâ€™ll even"
        },
        {
          "User: yeah": "be able to see each other again someday."
        },
        {
          "User: yeah": "User: The userâ€™s speech input, transcribed as \"i love you iâ€™m going to miss you so much sigh\", conveys a \"sad\" emotion tone. Please provide"
        },
        {
          "User: yeah": ""
        },
        {
          "User: yeah": "a response."
        },
        {
          "User: yeah": ""
        },
        {
          "User: yeah": "Assistant: Iâ€™m sorry to hear that. Is there anything I can do to help ease your sadness? Remember that you are not alone and that there are"
        },
        {
          "User: yeah": ""
        },
        {
          "User: yeah": "people who care about you."
        },
        {
          "User: yeah": "User: <speech>i love you iâ€™m going to miss you so much sigh<sad/></speech>"
        },
        {
          "User: yeah": "Assistant:\niâ€™m sorry to hear that youâ€™re going to miss me.\niâ€™m here for you and iâ€™ll always be here to support you. remember that even"
        },
        {
          "User: yeah": "though we may be apart, our love is still strong and real."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Listing 3: The prompt used to evaluate the win rate of": "response."
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Based\non\nthe\ndialogue\nhistory\nand\nthe"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "emotional\ntone\nexpressed\nby\nthe\nuser\nin"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "their\nlast\nstatement ,\nyou\nare\ntasked\nto"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "precisely\nevaluate\ntwo\npossible"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "responses\n( responses\nA\nand\nB )\nfrom"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Assistants\nA\nand\nB ,\nrespectively .\nYou"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "should\nact\nas\na\nthorough\nand\naccurate"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "evaluator\nto\ndetermine\nwhich\nassistant ' s"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "response\nbetter\naligns\nwith\nthe"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "preceding\ncontext\nand\nthe\nemotional\ntone"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "expressed ."
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "< history >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "User :\n{ text _u 1}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Assistant :\n{ text _a 1}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "User :\n{ text _u 2}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Assistant :\n{ text _a 2}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "User :\n{ text _u 3}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "</ history >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "< emotion >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "{ emotion }"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "</ emotion >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "< response _A >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Assistant :\n{ response _a}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "</ response _A >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "< response _B >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Assistant :\n{ response _b}"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "</ response _B >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "Provide\na\nconcise\njustification\nfor\nyour"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "choice\nin\nno\nmore\nthan\nthree\nsentences"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "and\nconclude\nwith\na\ndefinitive\nselection"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "between\nResponse\nA\nand\nResponse\nB.\nYour"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": ""
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "evaluation\nshould\nreflect\nhow\nwell\neach"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "assistant ' s\nresponse\nadheres\nto\nthe"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "previous\nelements\nof\nthe\nconversation ,"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "including\nthe\nmost\nrecent\nemotional\ntone"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "presented\nby\nthe\nuser ."
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "The\nchoice\nshould\nbe\nwrapped\nby\n< choice >"
        },
        {
          "Listing 3: The prompt used to evaluate the win rate of": "and\n</ choice >."
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Common voice: A massivelymultilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massivelymultilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang",
        "Binyuan Hui",
        "Luo Ji",
        "Mei Li",
        "Junyang Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "Keming Lu",
        "Jianxin Ma",
        "Rui Men",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Chuanqi Tan",
        "Sinan Tan",
        "Jianhong Tu",
        "Peng Wang",
        "Shijie Wang",
        "Wei Wang",
        "Shengguang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "Jian Yang",
        "Shusheng Yang",
        "Yang Yao",
        "Bowen Yu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Jianwei Zhang",
        "Xingxuan Zhang",
        "Yichang Zhang",
        "Zhenru Zhang",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Towards multimodal sarcasm detection",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "VerÃ³nica PÃ©rez-Rosas",
        "Roger Zimmermann",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "Guoguo Chen",
        "Shuzhou Chai",
        "Guanbo Wang",
        "Jiayu Du",
        "Wei-Qiang Zhang",
        "Chao Weng",
        "Dan Su",
        "Daniel Povey",
        "Jan Trmal",
        "Junbo Zhang"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "arxiv": "arXiv:2106.06909"
    },
    {
      "citation_id": "8",
      "title": "Lauragpt: Listen, attend, understand, and regenerate audio with gpt",
      "authors": [
        "Qian Chen",
        "Yunfei Chu",
        "Zhifu Gao",
        "Zerui Li",
        "Kai Hu",
        "Xiaohuan Zhou",
        "Jin Xu",
        "Ziyang Ma",
        "Wen Wang",
        "Siqi Zheng"
      ],
      "year": "2023",
      "venue": "Lauragpt: Listen, attend, understand, and regenerate audio with gpt",
      "arxiv": "arXiv:2310.04673"
    },
    {
      "citation_id": "9",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2022",
      "venue": "scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "11",
      "title": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
      "authors": [
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Bin Wang",
        "Linke Ouyang",
        "Xilin Wei",
        "Songyang Zhang",
        "Haodong Duan",
        "Maosong Cao",
        "Wenwei Zhang",
        "Yining Li",
        "Hang Yan",
        "Yang Gao",
        "Xinyue Zhang",
        "Wei Li",
        "Jingwen Li",
        "Kai Chen",
        "Conghui He",
        "Xingcheng Zhang",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2024",
      "venue": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
      "arxiv": "arXiv:2401.16420"
    },
    {
      "citation_id": "12",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "Philippe Gournay",
        "Olivier Lahaie",
        "Roch Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM multimedia systems conference"
    },
    {
      "citation_id": "13",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Wavllm: Towards robust and adaptive speech large language model",
      "authors": [
        "Shujie Hu",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Hongkun Hao",
        "Jing Pan",
        "Xunying Liu",
        "Jinyu Li",
        "Sunit Sivasankaran",
        "Linquan Liu"
      ],
      "year": "2024",
      "venue": "Wavllm: Towards robust and adaptive speech large language model",
      "arxiv": "arXiv:2404.00656"
    },
    {
      "citation_id": "15",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "16",
      "title": "Advancing large language models to capture varied speaking styles and respond properly in spoken conversations",
      "authors": [
        "Guan-Ting Lin",
        "Cheng-Han Chiang",
        "Hung-Yi Lee"
      ],
      "year": "2024",
      "venue": "Advancing large language models to capture varied speaking styles and respond properly in spoken conversations",
      "arxiv": "arXiv:2402.12786"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "# instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
      "authors": [
        "Keming Lu",
        "Hongyi Yuan",
        "Zheng Yuan",
        "Runji Lin",
        "Junyang Lin",
        "Chuanqi Tan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "21",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "22",
      "title": "Audiopalm: A large language model that can speak and listen",
      "authors": [
        "Chulayuth Paul K Rubenstein",
        "Asawaroengchai",
        "Dung Duc",
        "Ankur Nguyen",
        "ZalÃ¡n Bapna",
        "FÃ©lix Borsos",
        "De Chaumont",
        "Peter Quitry",
        "Dalia Chen",
        "Wei Badawy",
        "Eugene Han",
        "Kharitonov"
      ],
      "year": "2023",
      "venue": "Audiopalm: A large language model that can speak and listen",
      "arxiv": "arXiv:2306.12925"
    },
    {
      "citation_id": "23",
      "title": "Llasm: Large language and speech model",
      "authors": [
        "Yu Shu",
        "Siwei Dong",
        "Guangyao Chen",
        "Wenhao Huang",
        "Ruihua Zhang",
        "Daochen Shi",
        "Qiqi Xiang",
        "Yemin Shi"
      ],
      "year": "2023",
      "venue": "Llasm: Large language and speech model"
    },
    {
      "citation_id": "24",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "25",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Tianyi Zhang",
        "Yann Dubois",
        "Xuechen Li",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model"
    },
    {
      "citation_id": "26",
      "title": "Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "Nikolaos Vryzas",
        "Rigas Kotsakis",
        "Aikaterini Liatsou",
        "Charalampos Dimoulas",
        "George Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "28",
      "title": "2023a. Blsp: Bootstrapping language-speech pre-training via behavior alignment",
      "authors": [
        "Chen Wang",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Jinliang Lu",
        "Junhong Wu",
        "Yuchen Liu",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "venue": "2023a. Blsp: Bootstrapping language-speech pre-training via behavior alignment"
    },
    {
      "citation_id": "29",
      "title": "Blsp-kd: Bootstrapping languagespeech pre-training via knowledge distillation",
      "authors": [
        "Chen Wang",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Jiajun Zhang"
      ],
      "year": "2024",
      "venue": "Blsp-kd: Bootstrapping languagespeech pre-training via knowledge distillation"
    },
    {
      "citation_id": "30",
      "title": "Mead: A large-scale audiovisual dataset for emotional talking-face generation",
      "authors": [
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Linsen Song",
        "Zhuoqian Yang",
        "Wayne Wu",
        "Chen Qian",
        "Ran He",
        "Yu Qiao",
        "Chen Loy"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "2023b. Viola: Unified codec language models for speech recognition, synthesis, and translation",
      "authors": [
        "Tianrui Wang",
        "Long Zhou",
        "Ziqiang Zhang",
        "Yu Wu",
        "Shujie Liu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Jinyu Li",
        "Furu Wei"
      ],
      "venue": "2023b. Viola: Unified codec language models for speech recognition, synthesis, and translation",
      "arxiv": "arXiv:2305.16107"
    },
    {
      "citation_id": "32",
      "title": "E-chat: Emotion-sensitive spoken dialogue system with large language models",
      "authors": [
        "Yuhao Hongfei Xue",
        "Bingshen Liang",
        "Shiliang Mu",
        "Qian Zhang",
        "Lei Chen",
        "Xie"
      ],
      "year": "2023",
      "venue": "E-chat: Emotion-sensitive spoken dialogue system with large language models",
      "arxiv": "arXiv:2401.00475"
    },
    {
      "citation_id": "33",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "34",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "Binbin Zhang",
        "Hang Lv",
        "Pengcheng Guo",
        "Qijie Shao",
        "Chao Yang",
        "Lei Xie",
        "Xin Xu",
        "Hui Bu",
        "Xiaoyu Chen",
        "Chenchen Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "authors": [
        "Dong Zhang",
        "Shimin Li",
        "Xin Zhang",
        "Jun Zhan",
        "Pengyu Wang",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "year": "2023",
      "venue": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "arxiv": "arXiv:2305.11000"
    },
    {
      "citation_id": "36",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    }
  ]
}