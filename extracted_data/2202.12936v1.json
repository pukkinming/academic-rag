{
  "paper_id": "2202.12936v1",
  "title": "Automated Parkinson'S Disease Detection And Affective Analysis From Emotional Eeg Signals",
  "published": "2022-02-21T00:34:34Z",
  "authors": [
    "Ravikiran Parameshwara",
    "Soujanya Narayana",
    "Murugappan Murugappan",
    "Ramanathan Subramanian",
    "Ibrahim Radwan",
    "Roland Goecke"
  ],
  "keywords": [
    "Parkinson's diagnosis",
    "EEG signals",
    "Emotion perception",
    "Dimensional and categorical emotions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While Parkinson's disease (PD) is typically characterized by motor disorder, there is evidence of diminished emotion perception in PD patients. This study examines the utility of affective Electroencephalography (EEG) signals to understand emotional differences between PD vs Healthy Controls (HC), and for automated PD detection. Employing traditional machine learning and deep learning methods, we explore (a) dimensional and categorical emotion recognition, and (b) PD vs HC classification from emotional EEG signals. Our results reveal that PD patients comprehend arousal better than valence, and amongst emotion categories, fear, disgust and surprise less accurately, and sadness most accurately. Mislabeling analyses confirm confounds among opposite-valence emotions with PD data. Emotional EEG responses also achieve near-perfect PD vs HC recognition. Cumulatively, our study demonstrates that (a) examining implicit responses alone enables (i) discovery of valence-related impairments in PD patients, and (ii) differentiation of PD from HC, and (b) emotional EEG analysis is an ecologically-valid, effective, facile and sustainable tool for PD diagnosis vis-á-vis self reports, expert assessments and restingstate analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "P ARKINSON'S disease (PD) is a neurodegenerative disor- der of the central nervous system that affects movements, often causing tremors. PD is characterised by the progressive loss of dopaminergic neurons in the substantia nigra  [1] . Beside motor dysfunctions, cognitive, behavioural and emotional defects are common in PD  [2] ,  [3] , affecting over 10 million people globally (https:// www.parkinson.org/ ).\n\nA number of studies have examined motor and cognitive impairments in PD patients by examining explicit user responses (e.g., performance in recognition tasks, self-reports) or implicit responses such as Electroencephalogram (EEG) signals  [4] . Some works detect PD from abnormalities in resting-state EEG  [5] ,  [6] ,  [7] . Resting-state EEG is acquired in a highly controlled setting, e.g., requiring the subject to remain motionless with eyes closed in a dim and quiet room, which makes this setting ecologically invalid. A more realistic setting involves EEG acquisition during routine tasks such as music listening  [8]  or movie watching  [9] .\n\nAs movie and musical stimuli are often emotion eliciting  [10] , they enable researchers to understand how PD patients perceive emotions. Prior research has identified emotion deficits in PD patients, as they emote less spontaneously to emotion eliciting video clips  [11]  and unpleasant odours  [12] , and their posed facial expressions are disturbed and impaired  [13] . Apart from specific emotions, a few studies focus on PD perception of the valence (feeling of pleasantness/aversion) and arousal (emotional intensity) dimensions  [14] . Prior studies show that PD patients have a deficit in recognising positive and negative valence emotions from prosody  [15]  and facial expressions  [16] , and reduced reactivity to highly arousing pictures  [17] . Recognising emotions is critical to successful social interaction and communication, apart from inferring non-verbal social behavior such as emotional voice and facial expressions  [18] .\n\nImplicit physiological or biosignals reflect characteristic activity of the central nervous system, and cannot be intentionally suppressed. Recent studies have extensively employed biosignals  [19] ,  [20] ,  [21]  for emotion perception in healthy subjects. EEG, functional Magnetic Resonance Imaging (fMRI), Magnetoencephalogram (MEG) and Positron Emission Tomography (PET) provide reliable information on emotional states compared to other modalities  [22] . EEG is non-invasive, has high temporal resolution and can detect changes in brain activity over a span of milliseconds. EEG frequency bands are known to correlate with emotions  [23] ,  [24] . Handcoded EEG descriptors such as Spectral Power Vectors enable emotion detection while Convolutional Neural Networks (CNNs) can automatically learn cognitive and emotional correlates  [25] ,  [21] .\n\nThis study examines EEG-based PD emotion perception via a comparative analysis of data acquired from PD patients vis-á-vis Healthy Controls (HC). We explore both low-level EEG descriptors such as Spectral Power Vectors (SPV) and Common Spatial Patterns (CSP), and the intermediate EEG image and movie representations  [25]  to this end. We employ classical machine learning and deep learning frameworks such as 1D, 2D and 3D Convolutional Neural Networks (CNN) for emotion decoding. As shown in Fig.  1 , we perform (a) categorical and dimensional emotion recognition (binary valence and arousal classification), and (b) PD vs HC recognition from emotional EEG data.\n\nKey findings from our study are as follows: (1) Dimensional analysis reveals that arousal is better perceived in PD than valence; similar or superior classification is achieved with HC data for both attributes. (2) Fine-grained analyses of emotion class mislabeling reveals confounds among opposite-valence emotions for PD data; this trend is not discernible for HC.\n\n(3) Near-ceiling PD vs HC classification (F1 ≥ 0.97) is achieved with a 2D-CNN on emotional EEG data, implying that affective neural responses of PD and HC subjects are highly discriminable. Analyzing emotional neural responses can, therefore, enable facile PD diagnosis and treatment. Our study makes the following contributions:\n\n• It is the first study to examine (a) PD vs HC recognition, and (b) emotion perception in PD exclusively from EEG classification trends. While resting-state EEG analysis has achieved high PD recognition accuracy  [7] ,  [26] , it requires EEG acquisition in a highly controlled setting.\n\nIn contrast, we examine EEG signals acquired during the routine task of emotional media consumption, which also allows for PD emotion understanding.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "To highlight the novelty of our study, this section reviews related work on (a) emotional impairments in PD patients, and (b) the use of biosignals to assess impairments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotional Impairments In Pd Patients",
      "text": "PD patients not only show motor symptoms, but also cognitive  [28] ,  [29] , and emotional  [30]  deficits. Kan et al.  [31]  report PD deficits in recognising the fear and disgust facial emotions, while Suzuki et al.  [32]  observe impaired recognition of disgust. Clark et al.  [16]  note impaired anger recognition in PD patients with left hemisphere pathology, and reduced surprise recognition with right hemisphere pathology. Baggio et al.  [33]  observe PD deficits in recognizing sad, anger and disgust, while Narme et al.  [34]  note impaired recognition of anger and fear. A meta-analysis indicates an initial PD deficit for negative emotions  [30] , and later for positive emotions  [35] . Some studies employ non-visual stimuli, e.g., auditory and verbal, to assess PD emotion deficits. In an emotional voice test  [36] , PD patients in general exhibit impaired recognition and expression. Kan et al.  [31]  observe reduced recognition of fear, surprise and disgust from text, and this finding is mirrored by Pell et al.  [37] . Some studies  [38] ,  [39] , however, indicate that PD minimally impacts facial expression recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Using Biosignals To Assess Impairments",
      "text": "Emotion is a psycho-physiological expression related to mood and personality  [10] ,  [20] . Wearable sensing technologies can help examine biosignals and interpret associated emotions. Examining fMRI brain activations reveals a stronger activation in somatosensory regions during emotion processing for PD patients  [40] . Recognising emotive facial expressions requires somatosensory cortices  [41]  connected to the basal ganglia, the primary neurodegeneration site in PD. fMRI analyses show reduced functional activity in the left and right posterior putamen  [42] , which disturbs emotional processing.\n\nSpontaneous facial expressivity in PD observed via Electromyogram (EMG) and Electrocardiogram (ECG) signals reveal differences between PD patients and controls  [43] . Examining eye movements, PD patients are found to make fewer fixations while viewing affective scenes  [44] . Dietz et al. found ocular movements to be more compromised than pupil dilation in PD due to the disruption in basal ganglia circuitry. Measuring eye blinks via EMG, an attenuated reactivity to aversive stimuli is observed in PD due to an amygdalabased translational defect  [45] . A neuroimaging study involving fearful faces notes that dopamine levels modulate the amygdala's response in PD  [46] , and amygdala dysfunction induces impaired reactions to fear-inducing stimuli. Miller et al.  [17]  report that PD patients show reduced reactivity to highly arousing negative stimuli.\n\nDeep learning has become popular for a variety of pattern recognition tasks including neural applications  [47] ,  [48] ,  [49] . CNNs have been employed for EEG-based seizure prediction  [50] , Alzheimer's detection  [51] , fMRI-based schizophrenia detection  [52] , etc. Deep learning methods learn salient and latent neural representations  [53] . EEG-based categorical emotion recognition in PD patients has been pursued via higher-order spectral statistics  [54] ,  [55] ,  [9] ,  [56] . PD recognition via spectral analysis of resting-state EEG has been performed with k-Nearest Neighbour and Support Vector Machine classifiers  [26] . A 13-layer 1D-CNN for PD vs HC classification with resting-state EEG is proposed in  [7] . Binary PD vs HC EEG classification using a convolutional-recurrent neural network is proposed in  [57] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Identifying Research Gaps",
      "text": "While it is largely known that PD patients face emotional deficits, only a few studies examine these deficits via implicitly acquired biosignals such as EEG. Moreover, prior studies on automated PD diagnosis have only examined restingstate EEG, but not emotional EEG signals. Emotional EEG signals can be (a) acquired via easy-to-use, portable headsets under routine settings, and (b) utilized for PD recognition as well as studying PD emotion deficits as described in this work. Table I compares and contrasts our work against the literature; evidently, our approach allows for state-of-the-art PD recognition with ecologically valid data, different from prior work on resting-state and emotional EEG. PD group showed significantly less facial activity than controls.\n\nPurely behavioral study. Automated PD detection not attempted.\n\nImplicit  [12]  PD and normal subjects presented with odours and their emotional reactions encoded via Facial Action Coding System.\n\nSpontaneous facial activity disturbed in PD, and also impaired ability to pose and mask facial expressions.\n\nPurely behavioral study. Automated PD detection not attempted.\n\nImplicit  [13]  PD and controls presented with emotional faces, and were asked to pose those expressions.\n\nPD patients were impaired relative to controls on making emotional faces.\n\nPurely behavioral study. Automated PD detection not attempted.\n\nExplicit  [15]  Emotion recognition for PD and controls compared via standardized tests across three information channels: lexical-semantic, prosody, and facial.\n\nPD emotion recognition capability increased with more channels, but PD group performed worse than controls across all channels.\n\nBehavioral study with no automated PD detection.\n\nExplicit+Implicit  [17]  24 PD and 24 HC subjects presented with emotional pictures, and responses acquired via self-ratings (explicit) and EMG activity (implicit).\n\nReduced PD reactivity to low-valence, higharousal pictures; behavior was not specific to any emotion category (e.g., fear, disgust).\n\nBehavioral study with no automated PD detection.\n\nImplicit (Rest-state EEG)  [26] ,  [7]  PD vs HC recognition via resting-state EEG analysis using machine learning  [26]  and deep learning  [7]  approaches. Dataset proposed in  [26]  examined.\n\nMean recognition accuracy of 99.1% achieved with SVM  [26] , and 88.3% achieved with 1D-CNN  [7] .\n\nResting-state EEG compiled under highly controlled conditions, ecologically invalid.\n\nImplicit (Emotional EEG)  [58]  PD vs HC recognition via emotional EEG analysis with SVM classification. Dataset proposed in  [54]",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Eeg Signals Preprocessing",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Materials And Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset",
      "text": "The dataset comprises EEG signals from 20 nondemented PD (10 males/10 females) and 20 HC (9 males/11 females) subjects from Hospital Universiti Kebangsaan Malaysia, Kuala Lumpur, upon ethics approval (approval no. UKM1.5.3.5/244/FF-354-2012)  [54] ,  [56] . EEG data was recorded via the 14-channel wireless Emotiv Epoc headset (128 Hz sampling rate). Audio-visual stimuli are used to induce the six Ekman emotions (sadness, happiness, fear, disgust, surprise, anger), resulting in a total of 1440 samples (2 classes × 20 subjects × 6 emotions × 6 trials/emotion). Stimuli from the IADS  [59]  (audio) and IAPS  [60]  (visual) datasets were combined. Each trial (stimulus viewing episode) lasts 4-5 min. PD patients were optimally medicated to reduce tremors, and informed consent obtained from all participants. Data acquisition, PD clinical history, ethics approval, PD inclusion and exclusion criteria are described in  [54] ,  [9] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "EEG outlier samples were discarded by limiting the signal amplitude to 85µV , followed by an IIR bandpass Butterworth filter to retain the 8-49 Hz range  [9] . The filtered EEG signal, termed as raw data henceforth, was segmented into 5-second epochs to preserve temporal information following  [61] . Thus, 1440 original EEG samples were segmented into 13193 epochs for feature extraction. For classical machine learning methods, raw features were z-normalised followed by Principal Component Analysis (PCA) to retain 95% data variance (PCA was not part of the CNN pipeline).\n\nC. Feature Extraction from raw EEG 1) Spectral Power Vector (SPV): Power Spectral Analysis was performed to estimate EEG spectral density upon spectral transformation  [62] . On each epoch, a Butterworth bandpass filter was applied to extract the α (8 -13 Hz), β (13 -30 Hz) and γ (30 -49 Hz) spectral bands. A Fast Fourier Transform (FFT) was performed, followed by summation of squared FFT values within each three frequency band over the 14 electrodes to obtain the concatenated spectral power vector [α 1 , β 1 , γ 1 , . . . , α 14 , β 14 , γ 14 ].\n\n2) Common Spatial Patterns (CSP): Common Spatial Patterns were extracted by learning a linear combination of the original features  [63] . Filters (transformations) were designed so that the transformed signal variance was maximal for one class and minimal for the other. Apart from dimensionality reduction, CSPs enable recovery of the original signal by gathering relevant information spread over multiple channels and are, hence, popular EEG features  [62] . We learn the spatial transform w, which maximises the function:\n\nwhere C i , X i are, respectively, the spatial covariance matrix and the bandpass-filtered signal matrix for class i. In Eq. 1, wX i is the spatially filtered EEG signal for class i and wX i X T i w T is the the transformed signal variance, i.e., band power of the filtered signal. Thus, maximising J CSP (w) leads to spatially filtered signals whose inter-class band power ratio is maximum, and can be solved via eigenvalue decomposition. The spatial filters w that maximise J CSP (w) are the eigenvectors of the highest and lowest eigenvalues for matrices C 1 , C 2 . Hence, w gives feature vectors that are optimal for class discrimination. For each class i, the variances of only a small number of signals most suitable for distinguishing are used. Six filters corresponding to the three largest and smallest eigenvalues are used for generating the CSP feature f = log(wC 2 w T ) = log(var(wX 2 )) of dimension  (1, 6)  for each (14, 640) EEG epoch. It is shown that the classification accuracy does not improve with larger number of filters  [64] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Classical Machine Learning (Ml) Algorithms",
      "text": "Raw EEG data or extracted features were input to machine/deep learning classifiers (Fig.  1 ). We explored the following ML algorithms.\n\n• k-Nearest Neighbour (kNN), where the test sample is assigned the label corresponding to the mode of its kclosest neighbours based on a suitable distance metric. • Support Vector Machine (SVM), where input data are transformed to a high-dimensional space where the two classes are linearly separable and the inter-class distance is maximum. • Gaussian Naive Bayes (GNB), a generative classifier assuming class-conditional feature independence. • Decision Tree (DT), which uses a tree-like graph structure where each leaf node represents a category label. • Linear Discriminant Analysis (LDA), which linearly transforms data to achieve maximal inter-class distance. • Logistic Regression (LR), which maps the input to class labels via the sigmoid function.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Convolutional Neural Network Pipeline",
      "text": "Deep neural networks are the state-of-the-art in text, speech, image, video and EEG-based recognition  [48] ,  [65] ,  [49] ,  [47] ,  [25] ,  [21] , and have outperformed traditional machine learning methods obviating the need for handcrafted features  [62] . We explored 1D, 2D and 3D-CNNs to learn EEG representations. Raw or extracted EEG features were fed to the 1D-CNN; feature dimensions input to the 1D-CNN with raw, spectral and CSP descriptors were, respectively, (640, 14), 42 and 6. However, this representation ignores the EEG spatial structure; therefore, we synthesised the EEG image and EEG movie descriptors to preserve the spatial structure.\n\nExtracted SPVs were transformed to an EEG image as in  [25] . EEG electrodes, distributed on the scalp in 3D were projected onto a 2D surface to capture the spatial activity distribution. Azimuthal Equidistant Projection was used to preserve the relative inter-electrode distance  [66] . Scattered scalp power measurements were interpolated to derive a 32 × 32 pixel EEG image. Repeating this process for the α, β and γ bands produced three topo-maps, which were then merged to form a 3-channel (32 × 32 × 3) EEG image. To learn the temporal EEG structure, given that 3D-CNNs effectively learn from video chunks  [67] ,  [68] , we synthesised EEG movie samples comprising five images generated by sliding nonoverlapping 1s windows over the 5s epoch. The 3D-CNN input dimensionality is 5 × 32 × 32 × 3.  The general architecture of the three-layered 1D/2D/3D-CNN employed for classification is shown in Fig.  2 . Output dimensions for each CNN layer are presented in Table  II . Three convolutional layers convolve the input signal with a stride of 3, and comprise 16, 32 and 32 filters of size 3, 3 × 3 and 3 × 3 × 3, respectively. Each convolutional layer is followed by average pooling over 2-unit regions. Batch normalisation is applied to normalise prior activations, and a dropout of 0.1 -0.5 is employed for regularisation. The dense layer comprises 128 neurons, followed by a softmax layer composed of two neurons (for dimensional emotion and PD vs HC classification) or 6 neurons (for categorical emotion recognition) conveying class probabilities. CNN hyper-parameters (learning rate ∈ [10 -5 , . . . , 10 -1 ], optimizer ∈ {SGD, Adam, RMS Propogation} and dropout rate) were tuned via 10-fold cross-validation identical to the ML models 1 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "F. Performance Evaluation:",
      "text": "All models were fine-tuned via exhaustive grid-search and performance evaluated via ten-fold cross-validation. For all results, we report the weighted F1 measure or the weighted mean of the per-class F1-scores, which accounts for class imbalance noted in valence and arousal classification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments & Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Valence Classification",
      "text": "To examine emotional perception in PD, we first performed valence classification by training binary classifiers with (a) PD, (b) HC, and (c) PD+HC or full EEG data. Happiness and surprise were grouped in the high valence (HV) category, while sadness, fear, disgust and anger data were grouped in the low valence (LV) category. The HV:LV class ratio within PD, HC and full data is 1:2.\n\n1) Results: Table  III  presents valence classification results on full, PD and HC data with various models. Higher F1scores were achieved with HC data, implying reduced discriminability with PD EEG data. We discuss the results below.\n\na) Classification with PD Data: The impact of descriptors on the efficacy of ML techniques is evident from Table III. A one-way Analysis of Variance (ANOVA) to examine the effect of features (Raw, SPV and CSP) on F1-scores revealed confirmed the impact of descriptor type (F (2, 27) = 969.05, p < 0.0001). Comparing F1-scores from ten classifier runs, post-hoc Tukey tests revealed significant differences between predictive powers of SPV vs CSP (p < 0.001), CSP vs Raw (p < 0.001), and SPV vs Raw features (p < 0.001). Maximum F1-scores were achieved with CSPs.\n\nHigher F1-scores were observed with the 1D-CNN for all features, revealing the superior learning ability of CNNs. A one-way ANOVA revealed the minimal impact of different features on 1D-CNN performance, even as CSP features achieved the highest F1 of 0.86. 2D and 3D-CNN achieved even higher F1-scores, conveying that the EEG-image and movie representations are most efficient for valence prediction. The 3D-CNN achieved the maximum F1-score of 0.91. Fig.  3  (left) presents model sensitivity and specificity with PD data. Sensitivity denotes the true positive rate or proportion 1 Code available at https://github.com/ravikiranrao/PD-EEG of HV samples classified correctly, while Specificity denotes the true negative rate or the proportion of correctly classified LV samples. With ML algorithms, a significantly higher mean specificity (0.94) was observed than sensitivity (0.67). A similar trend was observed with 1D and 2D-CNN, with much higher specificity scores noted in both cases. Comparable mean specificity (0.95) and sensitivity (0.85) scores were noted, however, with the 3D-CNN. Overall, these trends convey reduced positive valence recognition with PD data. b) Classification using HC Data: Higher F1-scores were obtained on HC data for all features and methods (Table  III ). The impact of features on ML performance was confirmed by an ANOVA test (F (2, 27) = 1382.90, p < 0.0001), with CSP features outperforming SPV and Raw features. Higher F1-scores were noted with the 1D-CNN, with all features performing similarly. 2D and 3D-CNN performed better than the 1D-CNN, with the 3D-CNN achieving the best mean F1score of 0.93. c) PD vs HC F1 Comparison: Fig.  3  (centre) compares the valence F1-scores obtained with PD and HC data over all models, with CSP scores plotted for the ML and 1D-CNN methods. While identical scores were achieved on PD and HC data employing ML methods, marginally higher F1-scores were noted on HC data with the 1D, 2D and 3D-CNN. Overall, better classification was achieved with HC rather than PD data. Fig.  3  (right) compares sensitivity with PD data and HC data across models. With ML algorithms, sensitivity on PD data (0.67) was significantly lower than HC (0.73) as per a t-test (t(18) = 5.39, p < 0.0001). Lower sensitivity scores were again noted on PD data with the 1D, 2D and 3D-CNN even if the differences were insignificant. Cumulatively, these results reveal lower sensitivity for PD EEG data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Arousal Classification",
      "text": "To examine arousal perception in PD, we grouped the anger, disgust, fear, happiness and surprise data in the high arousal (HA) category, with the sadness samples constituting the low arousal (LA) category as in the circumplex model  [14] . The HA:LA class ratio within PD, HC and full data is, thus, 5:1.\n\n1) Results: Table  IV  presents arousal classification results with full, PD and HC data. Evidently, similar F1-scores were achieved for these subsets. We again compare PD vs HC results.\n\na) Classification using PD Data: Impact of features on ML classification performance is confirmed by a one-way ANOVA (F (2, 27) = 1332.94, p < 0.0001). CSP features achieved optimal arousal prediction and significant F1-score differences were noted via a Tukey test for CSP vs SPV (p < 0.005), CSP vs Raw (p < 0.001) and SPV vs Raw (p < 0.001). Higher F1-scores were obtained for the 1D-CNN with spectral features performing best, even if the differences among descriptors were not significant. The 2D and 3D-CNN models achieved an identical, near-ceiling F1 of 0.98.\n\nFig.  4  (left) presents specificity (LA classification rates) and sensitivity (HA classification rate) scores for PD data. Significantly higher sensitivity (0.97) than specificity (0.76) was observed for ML algorithms (p < 0.0001). This trend   repeated for the 1D-CNN (p < 0.0001) and 3D-CNN (sensitivity = 0.98 > specificity = 0.91 with p < 0.01), while comparable measures were achieved for the 2D-CNN. Overall, higher sensitivity than specificity was achieved on PD data with the different models. b) Classification using HC Data: CSP features produced a maximum F1-score of 0.93 with ML methods on HC data. The 1D-CNN achieved a higher F1 of 0.95 with CSP features, but all features performed comparably. F1-scores of 0.94 and 0.97 were achieved with the 2D and 3D-CNN, respectively, revealing that the spectral EEG image and movie descriptors effectively encode emotion information. c) PD vs HC F1-score Comparison: F1-scores achieved with PD and HC data are presented in Fig.  4  (right), with CSP results shown for the ML and 1D-CNN methods. Very similar F1-scores were found on PD and HC data for ML algorithms. The 1D-CNN achieved a much higher score with HC data (p < 0.0001), while the trend reversed for the 2D-CNN (PD F1= 0.98 > HC F1= 0.94, with p < 0.05). Similar F1-scores with PD and HC data were again noted with the 3D-CNN.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Categorical Emotion Classification",
      "text": "Since reduced HV-LV discriminability was noted in PD patients, we further explored categorical emotion recognition and nature of misclassificaions with PD vs HC data.\n\n1) Results: Table V presents multi-class emotion classification results across models with full, PD and HC data. An equal number of samples were available for the sadness, happiness, fear, disgust, surprise and anger emotion classes for both PD and HC subjects. F1-scores averaged over all emotion classes are shown. For most conditions, highest scores achieved with HC data, while lowest scores were achieved with full data.\n\na) Classification with PD Data: As with valence and arousal, EEG features significantly impacted ML performance  Fig.  5  (left) depicts emotion-specific F1-scores obtained on the PD and HC data across models, with CSP results presented for the ML and 1D-CNN models. On PD data, ML methods produced the highest and lowest F1-scores for sadness (F1= 0.81) and surprise (F1= 0.71), respectively, and a significant variation in F1-scores for different emotions was found per one-way ANOVA (F (5, 54) = 8.68, p < 0.0001). A significant effect of emotions on F1-scores was also noted for 1D, 2D and 3D-CNN (p < 0.05 in all cases). Sadness was easiest to recognise with all three models (F1= 0.87, 0.93 and 0.94 for 1D, 2D and 3D-CNN), while disgust (F1= 0.78), surprise (F1= 0.80) and fear (F1= 0.85) were recognised worst by the 1D, 2D and 3D-CNN, respectively. Across models, sadness was easiest to recognise, while disgust, fear and surprise were commonly confused with other emotions. b) Classification with HC Data: Trends similar to PD were observed with HC data. CSP features produced the highest score (F1= 0.74) with ML methods, significantly outperforming SPV and Raw features (p < 0.001 for both comparisons). All features performed comparably with the 1D-CNN, while the EEG image and movie descriptors produced mean F1-scores of 0.86 and 0.90 respectively via the 2D and 3D-CNN frameworks.\n\nc) Misclassification Analyses: Hitherto, (a) sensitivityspecificity analyses show lower recognition rates for HV emotions, and (b) Emotion-specific results convey that surprise, disgust and fear are often confounded with other emotions. We further examined the nature of misclassifications with PD and HC data to discover any underlying patterns.\n\nFig.  5  (right) depicts the maximum misclassification rate and most mis-predicted label per model and emotion class. For instance, the first row shows that the sad PD samples are often misclassified as happy by the best-performing ML, 1D, 2D and 3D-CNN models, with the misclassification rates specified in brackets. For HC data (2nd row), sadness is respectively mislabeled as happy, anger and fear. We note that:\n\n• The happiness and surprise high-valence emotions are most commonly mislabeled as low-valence emotions, namely, sadness, fear and anger for both PD and HC data. As per Fig.  3  (right), misclassification rates are slightly higher with PD data than HC data. • Among low-valence emotions, sadness is consistently predicted as happiness with PD data. Conversely on HC data, sadness is often confounded with other low-valence emotions such as fear and anger. • Fear and disgust are often misclassified with both PD and HC data. On HC data, fear is frequently confounded with disgust, and disgust with fear and anger. With PD data, however, disgust is often confused with happiness and surprise, and fear with happiness. • From the above, one can infer a greater propensity to confound with opposite-valence emotions on PD data. Overall, our findings convey that valence-related differences are not effectively encoded in PD EEG responses.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "D. Pd Vs Hc Classification",
      "text": "The above sections reveal some differences in the emotional EEG characteristics of the PD and HC groups. We examined whether the emotional EEG responses were discriminable for PD vs HC classification. To this end, we attempted classification from emotion-agnostic (Full) and emotion-specific data. Results are shown in Table  VI . Our data included equal proportions of the PD and HC classes (1:1 class ratio).\n\n1) Results: Emotion-agnostic and specific PD recognition results are presented below. a) Classification using Full Data: While raw EEG features achieved fair recognition with ML methods (max F1= 0.66), CSP and SPV features performed much better producing the maximum F1 of 0.97. A one-way ANOVA revealed significant differences in feature performance (p < 0.0001). All features performed better with the 1D-CNN, with SPV features producing a near-ceiling F1= 0.99 and outperforming the CSP and Raw features as per a post-hoc Tukey test (p < 0.001 for both comparisons). 2D-CNN performed marginally better, achieving a mean F1= 0.99 on full, sadness and fear data.\n\nFig.  6  presents sensitivity and specificity rates on full and emotion-specific data with different methods. ML algorithms on full data achieved a marginally higher sensitivity (0.97) than specificity (0.96); with the 1D CNN, the specificity (0.99) was marginally higher than sensitivity (0.98). Identical sensitivity and specificity rates of 0.99 were noted on full data with the 2D CNN.\n\nb) Classification with emotion-specific Data: Facile PD vs HC classification was achieved with all emotion classes, where only 1/6 th of the full data were available. Raw EEG and SPV features respectively produced the lowest and highest F1scores with ML methods, with SPV F1= 0.97 for happiness and sadness. Mixed trends were noted for the 1D-CNN, where all features performed comparably. Maximum F1 for the raw, CSP and SPV descriptors were respectively noted for sadness (F1= 0.96), fear (F1= 0.95) and anger (F1= 0.98). Peak and near-ceiling classification performance was noted for the 2D-CNN (identical F1= 0.99 for sadness and fear).\n\nSimilar sensitivity and specificity rates were noted across classifiers with emotion-specific data (see Fig.  6 ). For the 1D-CNN, near-identical sensitivity and specificity rates were noted for fear (sensitivity = 0.96 vs specificity = 0.97) and disgust (sensitivity = 0.98 vs specificity = 0.97), while for the 2D-CNN, near-identical sensitivity and specificity were noted for all emotion categories.\n\nV. DISCUSSION A. Valence & Arousal Classification 1) Valence: We examined PD valence perception, since valence is a fundamental emotional attribute  [69] ,  [70] . While prior studies have found valence-related differences between PD and HC groups via their explicit responses to visual  [33] , verbal  [36]  and textual  [31]  stimuli, we differently examined implicit emotional EEG responses to this end. Valence classification with multiple features and methods convey reduced performance and lower sensitivity on PD data. Therefore, PD data exhibits lower valence discriminability and sensitivity.\n\nLower PD sensitivity is consistent with findings in  [35] , where PD patients are found to have deficits in processing both positive and negative emotions. Dysfunction of the basal ganglia thalamocortical circuits in PD patients impairs their general emotional valence recognition  [71] . With respect to models and features, CNNs expectedly achieved higher F1scores than ML methods, confirming that they can efficiently learn spatio-temporal EEG patterns  [67] . CSP features predominantly achieve the best scores, and their utility in EEGbased analysis is well known  [63] . The EEG-movie descriptor optimally encodes spatio-temporal patterns in spectral EEG.\n\n2) Arousal: With respect to arousal,  [17] ,  [45]  observed muted reactivity or fewer startled eye-blinks from PD patients to high-arousal, low-valence aversive pictures. Similar findings were reported in  [31]  and  [15] , where PD patients showed deficits in recognizing emotions from lexical, prosody and facial cues. While these findings are based on implicit EMG data and/or explicit self-ratings, our inferences are based on EEG classification patterns. Given the identical class distributions for PD and HC data and corresponding F1 comparisons, our results do not indicate any PD vs HC differences in arousal recognition. Very comparable F1-scores are obtained for the PD and HC groups, conveying their similar characteristics.\n\n3) Models & Features: Sensitivity and specificity scores observed for PD data also convey an interesting trend. Given the imbalanced class proportions for both the valence and arousal conditions, significant sensitivity vs specificity disparities are noted particularly for the ML and 1D-CNN models. However, these differences become less conspicuous for the 2D and 3D-CNN models, conveying that they are able to efficiently learn minority-class representations. Regarding algorithms and features, F1-scores gradually improve while advancing from ML algorithms to the 3D-CNN. CSP features performed best with ML algorithms, but mixed results were observed for the 1D-CNN. EEG image and movie descriptors achieved a maximum and identical F1-score of 0.98 for arousal, with the 3D-CNN producing the maximum F1 of 0.93 for valence, showing their efficacy in encoding emotional information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Categorical Emotion Recognition",
      "text": "To the best of our knowledge, we are the first to examine categorical emotional classification with PD and HC EEG data. Prior studies on PD emotional perception typically examined facial expression recognition tasks  [34] ,  [16] ,  [31]  or studied physiological signals along with self-assessment reports to understand emotional deficits  [40] ,  [43] ,  [44] . These studies observe PD impairment in recognising negative emotions such as sadness, fear, anger and disgust.\n\nWe performed categorical emotion recognition to better understand which emotions are recognized better/worse with PD and HC data. Our results revealed that while sadness and happiness were well recognized with both PD and HC groups, fear, disgust and surprise were poorly recognized with the PD EEG data. Disturbances in the orbitofrontal cortex and the anterior cingulate cortex, which are active in negative emotion processing, can be attributed to these deficits  [35] . We then studied the nature of misclassifications for each emotion class. Classification results in Fig.  5  (right) show frequent confounds among opposite-valence emotions with PD data, indicating weaker valence encodings in emotional PD responses.\n\nWith respect to features and models, the trends are consistent with valence and arousal classification. We observe a steady increase in emotion-specific and overall F1-scores as we progress from classical machine learning methods to the 3D-CNN. CSP and raw EEG features produce the best performance with the ML and 1D-CNN approaches respectively. The 2D and 3D-CNN models, however, achieve higher F1scores, implying that spatio-temporal spectral EEG patterns best encode emotional information.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Pd Vs Nc Classification",
      "text": "To our knowledge, only one study  [58]  has performed PD recognition from emotional EEG signals, and achieved a mean accuracy of 87.9% (see Table  I ). Others  [26] ,  [7]  have performed PD vs HC classification from resting-state EEG signals, which is an ecologically invalid setting requiring a highly controlled environment for EEG acquisition. In this regard, we attempted PD recognition from both emotion-specific and emotion-agnostic (or full) EEG data acquired during the routine task of audio-visual media consumption.\n\nEmpirical results presented in Table  VI  show that accurate PD recognition is achieved even with emotion-specific EEG data. Near-perfect F1s are noted with the 2D-CNN, implying that PD and HC emotional responses are highly discriminable upon learning from only a few training samples. Some correlations can also be noted between Table VI and Section IV-C. PD recognition is highest for the sadness and fear emotions in Table  VI , and the PD emotion recognition rates are noted to be high and low, respectively, for sadness and fear in Sec. IV-C1a. These findings suggest that PD-related differences may be better encoded in EEG for negative valence emotions.\n\nFurther examining sensitivity and specificity measures, balanced recognition of PD and HC classes across models was achieved mainly for the negative disgust, fear and sadness emotions. That negative emotions best reflect PD impairments has been observed in prior studies  [33] ,  [34] . Focusing on features and models, spectral features achieved the best results with ML methods. With the 1D-CNN, superior F1-scores were achieved with all features even if no clear trends were discernible. Similar to emotion recognition, 2D-CNN again achieved optimal PD recognition, demonstrating that the EEG movie features best encode PD-related emotional differences.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "Parkinson's disease patients may often have difficulty expressing their emotions and internal feelings in real-life owing to (a) PD effects especially in its advanced stages, and (b) the effect of associated medications. Given these limitations, an assistive and sustainable diagnostic tool based on non-invasive detection of emotional disturbances can facilitate treatment and help improve life quality for PD patients. While many studies identify PD-related impairments based on the patients' explicit and implicit responses  [11] ,  [15] ,  [17] , or cognitive dissimilarities based on resting-state EEG  [4] ,  [26] ,  [7] , we differently examined emotional EEG responses to achieve both emotion and PD recognition.\n\nWhile studies examining facial behavior and resting-state EEG  [4]  typically derive their findings based on statistical patterns observed for the PD and HC groups, our inferences are entirely derived from classification patterns. Interesting trends and similarities with prior work were revealed our analyses. Dimensional emotion recognition experiments conveyed reduced discriminability with PD EEG data, while arousal-related differences vis-á-vis the HC group were not apparent. Furthermore, categorical emotion recognition results revealed that disgust, fear and surprise were associated with low recognition rates on PD data, while sadness was well recognized. Mislabeling analyses showed frequent confounds among opposite-valence emotions with PD data, but not with HC data. Reduced recognition of low-valence emotions, and confounds noted with positive emotions mirrors with deficits noted in the perception of these emotions from pictorial  [17] ,  [45]  and prosodic stimuli  [15] . Given some differences in emotion perception between the PD and HC groups, we then examined if the PD vs HC emotional responses were discriminable, and if this discriminability differed across emotions. Empirical results revealed that differences were apparent for both emotion-specific and emotion-agnostic data, with high F1-scores achieved in all conditions. Here again, the maximum F1 of 0.99 was achieved for the sad and fear emotions, which respectively corresponded to a high and low recognition rate on PD data. Also, most similar sensitivity and specificity rates across models were noted for negative emotions such as disgust, sadness and fear, implying that the PD and HC classes were most discriminable for these emotions.\n\nWith respect to features and models, CSPs considerably outperformed spectral features with machine learning models for emotion recognition. Conversely, spectral descriptors outperformed CSPs for PD vs HC classification. The efficacy of spectral features for isolating PD characteristics has been observed in prior studies  [72] . No single feature performed best with the 1D-CNN, even if the 1D-CNN consistently outperformed classical ML methods. The 2D and 3D-CNN models consistently achieved optimal recognition performance, conveying that spectral spatio-temporal models best encode EEG patterns as noted in  [25] ,  [68] .\n\nThe key finding from our presented study is that both emotion and PD recognition can be reliably performed from EEG responses passively compiled during audio-visual stimulus viewing; given that we effortlessly interact with media routinely, EEG signals can be captured easily over longer time-intervals as compared to resting-state EEG, which can practically be acquired only over short episodes. Also, while many EEG differences between PC and NC groups have been noted from rest-state analysis, the exact relation between EEG and motor symptoms is unknown  [4] .\n\nStudy limitations include analysis of data compiled from a limited number of PD subjects with only mild-to-moderate disease severity (Hoehn and Yahr scale  [73]  of 1-3). Future work will also focus on severity levels 4 and 5. While perceptual differences between PD vs HC subjects were captured via classification results in this study, an assistive diagnostic tool should also be able to provide explanations behind decisionmaking. Future work will focus on generating explanatory predictions, building on recent work  [74]  in this regard.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vii. Acknowledgement",
      "text": "",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we perform (a) cate-",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview: Our pipeline involves (a) EEG pre-processing and extraction of low-level features such as spectral power vectors (SPV) and common",
      "page": 3
    },
    {
      "caption": "Figure 1: ). We explored the fol-",
      "page": 4
    },
    {
      "caption": "Figure 2: Basic architecture of the 1D/2D/3D-CNN.",
      "page": 4
    },
    {
      "caption": "Figure 3: (left) presents model sensitivity and speciﬁcity with",
      "page": 5
    },
    {
      "caption": "Figure 3: (centre) compares",
      "page": 5
    },
    {
      "caption": "Figure 3: (right) compares sensitivity with PD data and HC data",
      "page": 5
    },
    {
      "caption": "Figure 4: (left) presents speciﬁcity (LA classiﬁcation rates)",
      "page": 5
    },
    {
      "caption": "Figure 3: Binary valence classiﬁcation: (Left) Sensitivity and speciﬁcity with PD data across various models. (Centre) F1-scores with PD and HC data across",
      "page": 6
    },
    {
      "caption": "Figure 4: Binary arousal classiﬁcation: (Left) Sensitivity and speciﬁcity with PD data across models. (Right) F1 on PD and HC data across models. Error",
      "page": 6
    },
    {
      "caption": "Figure 4: (right), with CSP",
      "page": 6
    },
    {
      "caption": "Figure 5: Categorical emotion classiﬁcation: (left) Emotion-wise F1-scores on HC (left) and PD data (right) across models. Error bars denote the standard",
      "page": 7
    },
    {
      "caption": "Figure 5: (left) depicts emotion-speciﬁc F1-scores obtained",
      "page": 7
    },
    {
      "caption": "Figure 5: (right) depicts the maximum misclassiﬁcation rate",
      "page": 7
    },
    {
      "caption": "Figure 3: (right), misclassiﬁcation rates are slightly",
      "page": 7
    },
    {
      "caption": "Figure 6: presents sensitivity and speciﬁcity rates on full and",
      "page": 8
    },
    {
      "caption": "Figure 6: ). For the 1D-",
      "page": 8
    },
    {
      "caption": "Figure 6: PD vs HC classiﬁcation: Sensitivity and speciﬁcity on all and emotional data with (left) ML algorithms, (centre) 1D-CNN and (right) 2D CNN.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Behavior Studied": "Explicit+Implicit",
          "References": "[11]",
          "Description": "PD and\nnormal\nsubjects\nshown\nemotional\nvideo\nclips\nand\ntheir\nemotional\nreactions\nencoded via Facial Action Coding System.\nEmotional self-ratings compiled.",
          "Findings": "PD group\nshowed\nsigniﬁcantly\nless\nfacial\nactivity than controls.",
          "Remarks": "Purely\nbehavioral\nstudy.\nAuto-\nmated PD detection not attempted."
        },
        {
          "Behavior Studied": "Implicit",
          "References": "[12]",
          "Description": "PD\nand\nnormal\nsubjects\npresented\nwith\nodours and their emotional reactions encoded\nvia Facial Action Coding System.",
          "Findings": "Spontaneous\nfacial activity disturbed in PD,\nand also impaired ability to pose and mask\nfacial expressions.",
          "Remarks": "Purely\nbehavioral\nstudy.\nAuto-\nmated PD detection not attempted."
        },
        {
          "Behavior Studied": "Implicit",
          "References": "[13]",
          "Description": "PD and\ncontrols\npresented with\nemotional\nfaces, and were asked to pose those expres-\nsions.",
          "Findings": "PD patients were impaired relative to controls\non making emotional\nfaces.",
          "Remarks": "Purely\nbehavioral\nstudy.\nAuto-\nmated PD detection not attempted."
        },
        {
          "Behavior Studied": "Explicit",
          "References": "[15]",
          "Description": "Emotion\nrecognition\nfor\nPD\nand\ncon-\ntrols compared via standardized tests across\nthree information channels:\nlexical-semantic,\nprosody, and facial.",
          "Findings": "PD emotion recognition capability increased\nwith more channels, but PD group performed\nworse than controls across all channels.",
          "Remarks": "Behavioral\nstudy with\nno\nauto-\nmated PD detection."
        },
        {
          "Behavior Studied": "Explicit+Implicit",
          "References": "[17]",
          "Description": "24 PD and 24 HC subjects presented with\nemotional\npictures,\nand\nresponses\nacquired\nvia\nself-ratings\n(explicit)\nand EMG activity\n(implicit).",
          "Findings": "Reduced PD reactivity to low-valence, high-\narousal pictures; behavior was not speciﬁc to\nany emotion category (e.g.,\nfear, disgust).",
          "Remarks": "Behavioral\nstudy with\nno\nauto-\nmated PD detection."
        },
        {
          "Behavior Studied": "Implicit\n(Rest-state EEG)",
          "References": "[26],\n[7]",
          "Description": "PD vs HC recognition via resting-state EEG\nanalysis\nusing machine\nlearning\n[26]\nand\ndeep\nlearning\n[7]\napproaches. Dataset\npro-\nposed in [26] examined.",
          "Findings": "Mean\nrecognition\naccuracy\nof\n99.1%\nachieved\nwith\nSVM\n[26],\nand\n88.3%\nachieved with 1D-CNN [7].",
          "Remarks": "Resting-state EEG compiled under\nhighly controlled conditions,\neco-\nlogically invalid."
        },
        {
          "Behavior Studied": "Implicit\n(Emotional EEG)",
          "References": "[58]",
          "Description": "PD vs HC recognition\nvia\nemotional EEG\nanalysis with\nSVM classiﬁcation. Dataset\nproposed in [54] examined.",
          "Findings": "Mean accuracy of 87.9% achieved.",
          "Remarks": "Routine and ecologically valid set-\nting. Only PD vs HC classiﬁcation\nattempted."
        },
        {
          "Behavior Studied": "Implicit\n(Emotional EEG)",
          "References": "Our work",
          "Description": "Dimensional\nand\ncategorical\nemotion\nplus\nPD recognition via machine and deep learn-\ning approaches. Dataset\nin\n[54] examined.",
          "Findings": "Valence-speciﬁc mislabeling\nobserved with\nPD data, while no arousal-related differences\nnoted\nbetween PD and HC groups. Maxi-\nmum accuracy/F1-score\nof\n93%,\n98% and\n99% achieved for valence,\narousal\nand PD\nrecognition,\nrespectively.",
          "Remarks": "PD and\nemotion EEG recogni-\ntion with multiple\nclassiﬁcation\nmethods. PD recognition higher\naccuracy with\nemotional EEG,\nand\ncomparable\nto\nprior work\nwith resting-state EEG."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5, 32, 32, 16": "3, 16, 16, 16"
        },
        {
          "5, 32, 32, 16": "3, 16, 16, 32"
        },
        {
          "5, 32, 32, 16": "2, 8, 8, 32"
        },
        {
          "5, 32, 32, 16": "2, 8, 8, 32"
        },
        {
          "5, 32, 32, 16": "1, 4, 4, 32"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer": "",
          "1D CNN": "Spectral\nCSP\nRaw",
          "2D CNN": "",
          "3D CNN": ""
        },
        {
          "Layer": "Convolution layer\n- 1",
          "1D CNN": "42, 16\n6, 16\n640, 16",
          "2D CNN": "32, 32, 16\n16, 16, 16\n16, 16, 32\n8, 8, 32\n8, 8, 32\n4, 4, 32\n512\n512\n128\n2 or 6",
          "3D CNN": "5, 32, 32, 16"
        },
        {
          "Layer": "Average Pooling - 1",
          "1D CNN": "21, 16\n3, 16\n320, 16",
          "2D CNN": "",
          "3D CNN": "3, 16, 16, 16"
        },
        {
          "Layer": "Convolution layer\n- 2",
          "1D CNN": "21, 32\n3, 32\n320, 32",
          "2D CNN": "",
          "3D CNN": "3, 16, 16, 32"
        },
        {
          "Layer": "Average Pooling - 2",
          "1D CNN": "11, 32\n2, 32\n160, 32",
          "2D CNN": "",
          "3D CNN": "2, 8, 8, 32"
        },
        {
          "Layer": "Convolution layer\n- 3",
          "1D CNN": "11, 32\n2, 32\n160, 32",
          "2D CNN": "",
          "3D CNN": "2, 8, 8, 32"
        },
        {
          "Layer": "Average Pooling - 3",
          "1D CNN": "6, 32\n1, 32\n80, 32",
          "2D CNN": "",
          "3D CNN": "1, 4, 4, 32"
        },
        {
          "Layer": "Flatten\nBatch Normalisation\nFully Connected\nSoftmax",
          "1D CNN": "192\n32\n2560\n192\n32\n2560\n128\n128\n128\n2 or 6\n2 or 6\n2 or 6",
          "2D CNN": "",
          "3D CNN": "512\n512\n128\n2 or 6"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data": "",
          "ML": "SPV\nCSP\nRaw",
          "1D-CNN": "SPV\nCSP\nRaw",
          "2D-CNN": "",
          "3D-CNN": ""
        },
        {
          "Data": "Full\nPD\nHC",
          "ML": "0.78 ± 0.01\n0.81 ± 0.01\n0.55 ± 0.01\n0.75 ± 0.02\n0.84 ± 0.01\n0.55 ± 0.01\n0.81 ± 0.02\n0.84 ± 0.01\n0.56 ± 0.01",
          "1D-CNN": "0.85 ± 0.01\n0.82 ± 0.02\n0.88 ± 0.11\n0.82 ± 0.09\n0.86 ± 0.03\n0.75 ± 0.17\n0.85 ± 0.08\n0.88 ± 0.04\n0.90 ± 0.11",
          "2D-CNN": "0.89 ± 0.06\n0.86 ± 0.07\n0.91 ± 0.07",
          "3D-CNN": "0.91 ± 0.05\n0.91 ± 0.07\n0.93 ± 0.05"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data": "",
          "ML": "SPV\nCSP\nRaw",
          "1D CNN": "SPV\nCSP\nRaw",
          "2D CNN": "",
          "3D CNN": ""
        },
        {
          "Data": "Full\nPD\nHC",
          "ML": "0.92 ± 0.01\n0.93 ± 0.00\n0.76 ± 0.00*\n0.92 ± 0.01\n0.93 ± 0.01\n0.76 ± 0.00\n0.91 ± 0.01\n0.94 ± 0.01\n0.76 ± 0.00*",
          "1D CNN": "0.95 ± 0.03\n0.92 ± 0.01\n0.95 ± 0.07\n0.96 ± 0.03\n0.92 ± 0.02\n0.94 ± 0.06\n0.91 ± 0.05\n0.95 ± 0.01\n0.94 ± 0.07",
          "2D CNN": "0.97 ± 0.02\n0.98 ± 0.02\n0.94 ± 0.03",
          "3D CNN": "0.97 ± 0.02\n0.98 ± 0.03\n0.97 ± 0.02"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data": "",
          "ML": "SPV\nCSP\nRaw",
          "1D CNN": "SPV\nCSP\nRaw",
          "2D CNN": "",
          "3D CNN": ""
        },
        {
          "Data": "Full\nPD\nHC",
          "ML": "0.64 ± 0.01\n0.72 ± 0.01\n0.18 ± 0.01\n0.62 ± 0.02\n0.76 ± 0.01\n0.17 ± 0.01*\n0.68 ± 0.02\n0.74 ± 0.02\n0.20 ± 0.01",
          "1D CNN": "0.76 ± 0.10\n0.69 ± 0.02\n0.77 ± 0.22\n0.77 ± 0.08\n0.80 ± 0.05\n0.81 ± 0.21\n0.76 ± 0.09\n0.76 ± 0.03\n0.78 ± 0.22",
          "2D CNN": "0.82 ± 0.09\n0.84 ± 0.10\n0.86 ± 0.08",
          "3D CNN": "0.83 ± 0.09\n0.88 ± 0.09\n0.90 ± 0.07"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data": "",
          "ML": "SPV\nCSP\nRaw",
          "1D-CNN": "SPV\nCSP\nRaw",
          "2D-CNN": ""
        },
        {
          "Data": "Full\nSadness\nHappiness\nFear\nDisgust\nSurprise\nAnger",
          "ML": "0.97 ± 0.01\n0.88 ± 0.01\n0.66 ± 0.01\n0.97 ± 0.01\n0.92 ± 0.01\n0.58 ± 0.03\n0.97 ± 0.01\n0.90 ± 0.02\n0.59 ± 0.02*\n0.96 ± 0.01\n0.93 ± 0.02\n0.61 ± 0.03*\n0.96 ± 0.02\n0.91 ± 0.02\n0.59 ± 0.01*\n0.95 ± 0.01\n0.91 ± 0.03\n0.60 ± 0.02*\n0.96 ± 0.01\n0.92 ± 0.01\n0.59 ± 0.03*",
          "1D-CNN": "0.99 ± 0.01\n0.87 ± 0.02\n0.91 ± 0.07\n0.97 ± 0.02\n0.94 ± 0.02\n0.96 ± 0.06\n0.91 ± 0.08\n0.92 ± 0.02\n0.95 ± 0.09\n0.96 ± 0.02\n0.95 ± 0.02\n0.89 ± 0.15\n0.98 ± 0.03\n0.93 ± 0.03\n0.93 ± 0.09\n0.92 ± 0.03\n0.94 ± 0.02\n0.94 ± 0.10\n0.98 ± 0.01\n0.94 ± 0.03\n0.87 ± 0.14",
          "2D-CNN": "0.99 ± 0.01\n0.99 ± 0.01\n0.98 ± 0.02\n0.99 ± 0.01\n0.98 ± 0.02\n0.97 ± 0.03\n0.98 ± 0.02"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Parkinson's disease: mechanisms and models",
      "authors": [
        "W Dauer",
        "S Przedborski"
      ],
      "year": "2003",
      "venue": "Neuron"
    },
    {
      "citation_id": "2",
      "title": "Cognitive and behavioral disorders in Parkinson's disease: an update. i: cognitive impairments",
      "authors": [
        "C Papagno",
        "L Trojano"
      ],
      "year": "2018",
      "venue": "Neurological Sciences"
    },
    {
      "citation_id": "3",
      "title": "Cognitive and behavioral disorders in Parkinson's disease: an update. ii: behavioral disorders",
      "authors": [
        "L Trojano",
        "C Papagno"
      ],
      "year": "2018",
      "venue": "Neurological Sciences"
    },
    {
      "citation_id": "4",
      "title": "Characterization of EEG data revealing relationships with cognitive and motor symptoms in parkinson's disease: A systematic review",
      "authors": [
        "Q Wang",
        "L Meng",
        "J Pang",
        "X Zhu",
        "D Ming"
      ],
      "year": "2020",
      "venue": "Frontiers in Aging Neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Investigation of eeg abnormalities in the early stage of parkinson's disease",
      "authors": [
        "C.-X Han",
        "J Wang",
        "G.-S Yi",
        "Y.-Q Che"
      ],
      "year": "2013",
      "venue": "Cognitive neurodynamics"
    },
    {
      "citation_id": "6",
      "title": "Quantitative eeg (qeeg) measures differentiate parkinson's disease (pd) patients from healthy controls (hc)",
      "authors": [
        "M Chaturvedi",
        "F Hatz",
        "U Gschwandtner",
        "J Bogaarts",
        "A Meyer",
        "P Fuhr",
        "V Roth"
      ],
      "year": "2017",
      "venue": "Frontiers in aging neuroscience"
    },
    {
      "citation_id": "7",
      "title": "A deep learning approach for parkinson's disease diagnosis from EEG signals",
      "authors": [
        "S Oh",
        "Y Hagiwara",
        "U Raghavendra",
        "R Yuvaraj",
        "N Arunkumar",
        "M Murugappan",
        "U Acharya"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "8",
      "title": "Effective connectivity during rest and music listening: An EEG study on parkinson's disease",
      "authors": [
        "E Maggioni",
        "F Arienti",
        "S Minella",
        "F Mameli",
        "L Borellini",
        "M Nigro",
        "F Cogiamanian",
        "A Bianchi",
        "S Cerutti",
        "S Barbieri",
        "P Brambilla",
        "G Ardolino"
      ],
      "year": "2021",
      "venue": "Frontiers in Aging Neuroscience"
    },
    {
      "citation_id": "9",
      "title": "Brain functional connectivity patterns for emotional state classification in Parkinson's disease patients without dementia",
      "authors": [
        "R Yuvaraj",
        "M Murugappan",
        "U Acharya",
        "H Adeli",
        "N Ibrahim",
        "E Mesquita"
      ],
      "year": "2016",
      "venue": "Behavioural Brain Research"
    },
    {
      "citation_id": "10",
      "title": "Decaf: Meg-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Spontaneous and posed facial expression in Parkinson's disease",
      "authors": [
        "M Smith",
        "M Smith",
        "H Ellgring"
      ],
      "year": "1996",
      "venue": "Journal of the Int'l Neuropsychological Society"
    },
    {
      "citation_id": "12",
      "title": "Disturbance of spontaneous and posed facial expressions in Parkinson's disease",
      "authors": [
        "G Simons",
        "H Ellgring",
        "M Pasqualini"
      ],
      "year": "2003",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "13",
      "title": "Emotional facial imagery, perception, and expression in Parkinson's disease",
      "authors": [
        "D Jacobs",
        "J Shuren",
        "D Bowers",
        "K Heilman"
      ],
      "year": "1995",
      "venue": "Neurology"
    },
    {
      "citation_id": "14",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "15",
      "title": "Dynamic emotion processing in Parkinson's disease as a function of channel availability",
      "authors": [
        "S Paulmann",
        "M Pell"
      ],
      "year": "2010",
      "venue": "Journal of Clinical and Experimental Neuropsychology"
    },
    {
      "citation_id": "16",
      "title": "Specific impairments in the recognition of emotional facial expressions in Parkinson's disease",
      "authors": [
        "U Clark",
        "S Neargarder",
        "A Cronin-Golomb"
      ],
      "year": "2008",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "17",
      "title": "Startle reflex hyporeactivity in Parkinson's disease: an emotion-specific or arousal-modulated deficit?",
      "authors": [
        "K Miller",
        "M Okun",
        "M Marsiske",
        "E Fennell",
        "D Bowers"
      ],
      "year": "2009",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "18",
      "title": "Facial expressions, their communicatory functions and neurocognitive substrates",
      "authors": [
        "R Blair"
      ],
      "year": "2003",
      "venue": "Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences"
    },
    {
      "citation_id": "19",
      "title": "The role of nonlinear dynamics in affective valence and arousal recognition",
      "authors": [
        "G Valenza",
        "A Lanata",
        "E Scilingo"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Recognition of advertisement emotions with application to computational advertising",
      "authors": [
        "A Shukla",
        "S Gullapuram",
        "H Katti",
        "M Kankanhalli",
        "S Winkler",
        "R Subramanian"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2964549"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from brain signals using hybrid adaptive filtering and higher order crossings analysis",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Consciousness and arousal effects on emotional face processing as revealed by brain oscillations. a gamma band analysis",
      "authors": [
        "M Balconi",
        "C Lucchiari"
      ],
      "year": "2008",
      "venue": "Int'l Journal of Psychophysiology"
    },
    {
      "citation_id": "24",
      "title": "Neurophysiological correlates of induced discrete emotions in humans: an individually oriented analysis",
      "authors": [
        "L Aftanas",
        "N Reva",
        "L Savotina",
        "V Makhnev"
      ],
      "year": "2006",
      "venue": "Neuroscience and Behavioral Physiology"
    },
    {
      "citation_id": "25",
      "title": "Learning representations from EEG with deep recurrent-convolutional neural networks",
      "authors": [
        "P Bashivan",
        "I Rish",
        "M Yeasin",
        "N Codella"
      ],
      "year": "2016",
      "venue": "Int'l Conference on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Diagnosis of Parkinson's disease from electroencephalography signals using linear and self-similarity features",
      "authors": [
        "A Bhurane",
        "S Dhok",
        "M Sharma",
        "R Yuvaraj",
        "M Murugappan",
        "U Acharya"
      ],
      "year": "2019",
      "venue": "Diagnosis of Parkinson's disease from electroencephalography signals using linear and self-similarity features"
    },
    {
      "citation_id": "27",
      "title": "Comparing selfreported and objective monitoring of physical activity in parkinson disease",
      "authors": [
        "S Mantri",
        "S Wood",
        "J Duda",
        "J Morley"
      ],
      "year": "2019",
      "venue": "Parkinsonism & Related Disorders"
    },
    {
      "citation_id": "28",
      "title": "The role of the subthalamic nucleus in the preparation of volitional movement termination in Parkinson's disease",
      "authors": [
        "Y.-T Hsu",
        "H.-Y Lai",
        "Y.-C Chang",
        "S.-M Chiou",
        "M.-K Lu",
        "Y.-C Lin",
        "Y.-L Liu",
        "C.-C Chen",
        "H.-C Huang",
        "T.-F Chien"
      ],
      "year": "2012",
      "venue": "Experimental neurology"
    },
    {
      "citation_id": "29",
      "title": "Non-motor symptoms of Parkinson's disease",
      "authors": [
        "J.-G Hou",
        "E Lai"
      ],
      "year": "2007",
      "venue": "Int'l Journal of Gerontology"
    },
    {
      "citation_id": "30",
      "title": "A meta-analysis of performance on emotion recognition tasks in Parkinson's disease",
      "authors": [
        "H Gray",
        "L Tickle-Degnen"
      ],
      "year": "2010",
      "venue": "Neuropsychology"
    },
    {
      "citation_id": "31",
      "title": "Recognition of emotion from facial, prosodic and written verbal stimuli in Parkinson's disease",
      "authors": [
        "Y Kan",
        "M Kawamura",
        "Y Hasegawa",
        "S Mochizuki",
        "K Nakamura"
      ],
      "year": "2002",
      "venue": "Cortex"
    },
    {
      "citation_id": "32",
      "title": "Disgustspecific impairment of facial expression recognition in Parkinson's disease",
      "authors": [
        "A Suzuki",
        "T Hoshino",
        "K Shigemasu",
        "M Kawamura"
      ],
      "year": "2006",
      "venue": "Brain"
    },
    {
      "citation_id": "33",
      "title": "Structural correlates of facial emotion recognition deficits in Parkinson's disease patients",
      "authors": [
        "H Baggio",
        "B Segura",
        "N Ibarretxe-Bilbao",
        "F Valldeoriola",
        "M Marti",
        "Y Compta",
        "E Tolosa",
        "C Junque"
      ],
      "year": "2012",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "34",
      "title": "Understanding facial emotion perception in Parkinson's disease: the role of configural processing",
      "authors": [
        "P Narme",
        "A.-M Bonnet",
        "B Dubois",
        "L Chaby"
      ],
      "year": "2011",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "35",
      "title": "Degraded impairment of emotion recognition in Parkinson's disease extends from negative to positive emotions",
      "authors": [
        "C.-Y Lin",
        "Y.-M Tien",
        "J.-T Huang",
        "C.-H Tsai",
        "L.-C Hsu"
      ],
      "year": "2016",
      "venue": "Behavioural Neurology"
    },
    {
      "citation_id": "36",
      "title": "Altered emotional recognition and expression in patients with Parkinson's disease",
      "authors": [
        "Y Jin",
        "Z Mao",
        "Z Ling",
        "X Xu",
        "Z Zhang",
        "X Yu"
      ],
      "year": "2017",
      "venue": "Neuropsychiatric Disease and Treatment"
    },
    {
      "citation_id": "37",
      "title": "Processing emotional tone from speech in Parkinson's disease: a role for the basal ganglia",
      "authors": [
        "M Pell",
        "C Leonard"
      ],
      "year": "2003",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience"
    },
    {
      "citation_id": "38",
      "title": "Facial expression decoding in early Parkinson's disease",
      "year": "2005",
      "venue": "Cognitive Brain Research"
    },
    {
      "citation_id": "39",
      "title": "Intact recognition of facial emotion in Parkinson's disease",
      "authors": [
        "R Adolphs",
        "R Schul",
        "D Tranel"
      ],
      "year": "1998",
      "venue": "Neuropsychology"
    },
    {
      "citation_id": "40",
      "title": "Facial emotion recognition in Parkinson's disease: An fMRI investigation",
      "authors": [
        "A Wabnegger",
        "R Ille",
        "P Schwingenschuh",
        "P Katschnig-Winter",
        "M Kögl-Wallner",
        "K Wenzel",
        "A Schienle"
      ],
      "year": "2015",
      "venue": "PLoS One"
    },
    {
      "citation_id": "41",
      "title": "A role for somatosensory cortices in the visual recognition of emotion as revealed by three-dimensional lesion mapping",
      "authors": [
        "R Adolphs",
        "H Damasio",
        "D Tranel",
        "G Cooper",
        "A Damasio"
      ],
      "year": "2000",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "42",
      "title": "An fMRI study into emotional processing in Parkinson's disease: Does increased medial prefrontal activation compensate for striatal dysfunction?",
      "authors": [
        "A Moonen",
        "P Weiss",
        "M Wiesing",
        "R Weidner",
        "G Fink",
        "J Reijnders",
        "W Weber",
        "A Leentjens"
      ],
      "year": "2017",
      "venue": "PloS One"
    },
    {
      "citation_id": "43",
      "title": "Objectifying facial expressivity assessment of Parkinson's patients: preliminary study",
      "authors": [
        "P Wu",
        "I Gonzalez",
        "G Patsis",
        "D Jiang",
        "H Sahli",
        "E Kerckhofs",
        "M Vandekerckhove"
      ],
      "year": "2014",
      "venue": "Computational and Mathematical Methods in Medicine"
    },
    {
      "citation_id": "44",
      "title": "Emotion and ocular responses in Parkinson's disease",
      "authors": [
        "J Dietz",
        "M Bradley",
        "M Okun",
        "D Bowers"
      ],
      "year": "2011",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "45",
      "title": "Startling facts about emotion in Parkinson's disease: blunted reactivity to aversive stimuli",
      "authors": [
        "D Bowers",
        "K Miller",
        "A Mikos",
        "L Kirsch-Darrow",
        "U Springer",
        "H Fernandez",
        "K Foote",
        "M Okun"
      ],
      "year": "2006",
      "venue": "Brain"
    },
    {
      "citation_id": "46",
      "title": "Dopamine modulates the response of the human amygdala: a study in Parkinson's disease",
      "authors": [
        "A Tessitore",
        "A Hariri",
        "F Fera",
        "W Smith",
        "T Chase",
        "T Hyde",
        "D Weinberger",
        "V Mattay"
      ],
      "year": "2002",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "47",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Speech recognition with deep recurrent neural networks",
      "authors": [
        "A Graves",
        "A -R. Mohamed",
        "G Hinton"
      ],
      "year": "2013",
      "venue": "2013 IEEE Int'l Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Classification of patterns of EEG synchronization for seizure prediction",
      "authors": [
        "P Mirowski",
        "D Madhavan",
        "Y Lecun",
        "R Kuzniecky"
      ],
      "year": "2009",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "51",
      "title": "Discrimination of Alzheimer's disease and normal aging by EEG data",
      "authors": [
        "C Besthorn",
        "R Zerfass",
        "C Geiger-Kabisch",
        "H Sattel",
        "S Daniel",
        "U Schreiter-Gasser",
        "H Förstl"
      ],
      "year": "1997",
      "venue": "Electroencephalography and Clinical Neurophysiology"
    },
    {
      "citation_id": "52",
      "title": "3D-CNN based discrimination of schizophrenia using resting-state fMRI",
      "authors": [
        "M Qureshi",
        "J Oh",
        "B Lee"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence in Medicine"
    },
    {
      "citation_id": "53",
      "title": "Deep learning for neuroimaging: a validation study",
      "authors": [
        "S Plis",
        "D Hjelm",
        "R Salakhutdinov",
        "E Allen",
        "H Bockholt",
        "J Long",
        "H Johnson",
        "J Paulsen",
        "J Turner",
        "V Calhoun"
      ],
      "year": "2014",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "54",
      "title": "Emotion classification in Parkinson's disease by higher-order spectra and power spectrum features using EEG signals: A comparative study",
      "authors": [
        "R Yuvaraj",
        "M Murugappan",
        "N Ibrahim",
        "M Omar",
        "K Sundaraj",
        "K Mohamad",
        "R Palaniappan",
        "M Satiyan"
      ],
      "year": "2014",
      "venue": "Journal of Integrative Neuroscience"
    },
    {
      "citation_id": "55",
      "title": "Optimal set of EEG features for emotional state classification and trajectory visualization in Parkinson's disease",
      "authors": [
        "R Yuvaraj",
        "M Murugappan",
        "N Ibrahim",
        "K Sundaraj",
        "M Omar",
        "K Mohamad",
        "R Palaniappan"
      ],
      "year": "2014",
      "venue": "Int'l Journal of Psychophysiology"
    },
    {
      "citation_id": "56",
      "title": "Hemispheric asymmetry non-linear analysis of EEG during emotional responses from idiopathic Parkinson's disease patients",
      "authors": [
        "R Yuvaraj",
        "M Murugappan"
      ],
      "year": "2016",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "57",
      "title": "A deep convolutionalrecurrent neural network architecture for Parkinson's disease EEG classification",
      "authors": [
        "S Lee",
        "R Hussein",
        "M Mckeown"
      ],
      "year": "2019",
      "venue": "2019 IEEE Global Conference on Signal and Information Processing"
    },
    {
      "citation_id": "58",
      "title": "Detection of emotions in Parkinson's disease using higher order spectral features from brain's electrical activity",
      "authors": [
        "R Yuvaraj",
        "M Murugappan",
        "N Mohamed Ibrahim",
        "K Sundaraj",
        "M Omar",
        "K Mohamad",
        "R Palaniappan"
      ],
      "year": "2014",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "59",
      "title": "The int'l affective digitized sounds: Affective ratings of sounds and instruction manual (technical report no. b-3)",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "2007",
      "venue": "The int'l affective digitized sounds: Affective ratings of sounds and instruction manual (technical report no. b-3)"
    },
    {
      "citation_id": "60",
      "title": "Int'l affective picture system (iaps): Technical manual and affective ratings",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "1997",
      "venue": "NIMH Center for the Study of Emotion and Attention"
    },
    {
      "citation_id": "61",
      "title": "Bimodal emotion recognition using speech and physiological changes",
      "authors": [
        "J Kim"
      ],
      "year": "2007",
      "venue": "Robust Speech Recognition and Understanding"
    },
    {
      "citation_id": "62",
      "title": "Deep learning-based electroencephalography analysis: a systematic review",
      "authors": [
        "Y Roy",
        "H Banville",
        "I Albuquerque",
        "A Gramfort",
        "T Falk",
        "J Faubert"
      ],
      "year": "2019",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "63",
      "title": "Optimal spatial filtering of single trial EEG during imagined hand movement",
      "authors": [
        "H Ramoser",
        "J Muller-Gerking",
        "G Pfurtscheller"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Rehabilitation Engineering"
    },
    {
      "citation_id": "64",
      "title": "Designing optimal spatial filters for single-trial eeg classification in a movement task",
      "authors": [
        "J Müller-Gerking",
        "G Pfurtscheller",
        "H Flyvbjerg"
      ],
      "year": "1999",
      "venue": "Clinical neurophysiology"
    },
    {
      "citation_id": "65",
      "title": "Teaching machines to read and comprehend",
      "authors": [
        "K Hermann",
        "T Kočiskỳ",
        "E Grefenstette",
        "L Espeholt",
        "W Kay",
        "M Suleyman",
        "P Blunsom"
      ],
      "year": "2015",
      "venue": "NIPS'15: Proceedings of the 28th Int'l Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "66",
      "title": "Map projections-A working manual. US Government Printing Office",
      "authors": [
        "J Snyder"
      ],
      "year": "1987",
      "venue": "Map projections-A working manual. US Government Printing Office"
    },
    {
      "citation_id": "67",
      "title": "Beyond short snippets: Deep networks for video classification",
      "authors": [
        "J Yue-Hei",
        "M Ng",
        "S Hausknecht",
        "O Vijayanarasimhan",
        "R Vinyals",
        "G Monga",
        "Toderici"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Not made for each other-audio-visual dissonance-based deepfake detection and localization",
      "authors": [
        "K Chugh",
        "P Gupta",
        "A Dhall",
        "R Subramanian"
      ],
      "year": "2020",
      "venue": "in ACM Int'l Conference on Multimedia"
    },
    {
      "citation_id": "69",
      "title": "The Oxford companion to emotion and the affective sciences",
      "authors": [
        "D Sander",
        "K Scherer"
      ],
      "year": "2009",
      "venue": "The Oxford companion to emotion and the affective sciences"
    },
    {
      "citation_id": "70",
      "title": "Levels of valence",
      "authors": [
        "V Shuman",
        "D Sander",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "71",
      "title": "Recognition of happy facial affect in panic disorder: an fMRI study",
      "authors": [
        "S Pillay",
        "J Rogowska",
        "S Gruber",
        "N Simpson",
        "D Yurgelun-Todd"
      ],
      "year": "2007",
      "venue": "Journal of Anxiety Disorders"
    },
    {
      "citation_id": "72",
      "title": "Eeg frequency analysis in demented and nondemented Parkinsonian patients",
      "authors": [
        "M Neufeld",
        "S Blumen",
        "I Aitkin",
        "Y Parmet",
        "A Korczyn"
      ],
      "year": "1994",
      "venue": "Dementia"
    },
    {
      "citation_id": "73",
      "title": "Parkinsonism: onset, progression, and mortality",
      "authors": [
        "M Hoehn",
        "M Yahr"
      ],
      "year": "1998",
      "venue": "Neurology"
    },
    {
      "citation_id": "74",
      "title": "Explainable deep neural networks for multivariate time series predictions",
      "authors": [
        "R Assaf",
        "A Schumann"
      ],
      "year": "2019",
      "venue": "Int'l Joint Conference on Artificial Intelligence"
    }
  ]
}