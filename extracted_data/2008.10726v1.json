{
  "paper_id": "2008.10726v1",
  "title": "Unsupervised Multi-Modal Representation Learning For Affective Computing With Multi-Corpus Wearable Data",
  "published": "2020-08-24T22:01:55Z",
  "authors": [
    "Kyle Ross",
    "Paul Hungler",
    "Ali Etemad"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With recent developments in smart technologies, there has been a growing focus on the use of artificial intelligence and machine learning for affective computing to further enhance the user experience through emotion recognition. Typically, machine learning models used for affective computing are trained using manually extracted features from biological signals. Such features may not generalize well for large datasets and may be sub-optimal in capturing the information from the raw input data. One approach to address this issue is to use fully supervised deep learning methods to learn latent representations of the biosignals. However, this method requires human supervision to label the data, which may be unavailable or difficult to obtain. In this work we propose an unsupervised framework reduce the reliance on human supervision. The proposed framework utilizes two stacked convolutional autoencoders to learn latent representations from wearable electrocardiogram (ECG) and electrodermal activity (EDA) signals. These representations are utilized within a random forest model for binary arousal classification. This approach reduces human supervision and enables the aggregation of datasets allowing for higher generalizability. To validate this framework, an aggregated dataset comprised of the AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI datasets is created. The results of our proposed method are compared with using convolutional neural networks, as well as methods that employ manual extraction of hand-crafted features. The methodology used for fusing the two modalities is also investigated. Lastly, we show that our method outperforms current state-ofthe-art results that have performed arousal detection on the same datasets using ECG and EDA biosignals. The results show the wide-spread applicability for stacked convolutional autoencoders to be used with machine learning for affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S MART technologies are quickly becoming ubiquitous in our everyday lives. These technologies aim to understand, analyze, and interact with users seamlessly, providing a user-centered experience. Affective computing is one area of research aiming to create smart technologies that can better understand and ultimately react to their user. Affective computing is the notion of detecting, modeling, and reacting to the user's affective states by a computer  [1] . The integration of affective computing into devices can make users perceive the computers as more intelligent, effectively making the smart technology \"smarter\"  [2] .\n\nWith the increasing pervasiveness of smart technologies, research into determining the user's affective state to facilitate affective computing is of growing importance. Affective states can be broken down into two dimensions, valence and arousal  [3] . Both valence and arousal have been shown to be closely linked with activity in the autonomic nervous system  [4] . Biosignals are one source through which changes in the autonomic nervous system can be observed, allowing for changes in valence and arousal to be elucidated  [5] . Accordingly, machine learning approaches utilizing biosignals have been widely used for the classification of valence and arousal.\n\nTo collect the biosignals for affective computing through machine learning, wearable sensors have been widely used. While original wearable devices were bulky and cumbersome to use, with recent advancements, they have become lightweight, affordable, and unobtrusive  [6] . In fact, wearables have become small enough to be incorporated into jewelry and clothing so that they can be used in everyday life  [7] . It is estimated that by 2022 the market for wearables will double to be worth 27 billion dollars  [8] . This will result in 233 million total sales of wearable devices  [9] .\n\nWearable devices that collect Electrocardiogram (ECG)  [10] ,  [11] , Electrodermal Activity (EDA)  [12] ,  [13] , Electroencephalogram (EEG)  [14] ,  [15] , Electrooculogram (EOG)  [16] ,  [17] , and Electromyography (EMG)  [18] ,  [19]  have all been used for a variety of applications including affective computing. Among these signals, ECG and EDA have been shown to be the most closely correlated to arousal  [20] .\n\nTypically, machine learning models used for affective computing are trained using time domain and frequency domain features that are hand-crafted and manually extracted from biosignals  [21] -  [24] . However, these features may not generalize well between multiple datasets as the biosignals can be vastly different in terms of the quality of the data, the placement of sensors on the body, and the data collection protocol as a whole. Another approach for extracting biosignal features for affect classification is to use deep learning methods to learn latent representations of the input data. These models have been shown to better predict mood, health, and stress, which are all factors in affect, with less error than manually extracted hand-crafted features  [25] . Deep learning allows for the model to be trained on a large dataset allowing it to find more complex representations that can be better utilized for affect classification  [26] .\n\nDeep learning models commonly employ fully supervised learning, which requires human supervision in the form la-belled data for both representation learning and classification. However, this reliance on human labelling can reduce the amount of data available to train the model. Additionally, the output labels can make it difficult for the model to generalize across multiple datasets as the differences between the stimuli used in the datasets may elicit different levels of affective response, resulting in vastly different output labels  [27] . Combining datasets to create a multi-corpus pipeline is also difficult with supervised learning as the output labels may be carried out with different protocols or standards. Our goal in this work is to reduce the reliance on human supervision by proposing a framework where a significant portion of the processing, i.e. representation learning, can take place in an unsupervised manner, followed by a supervised classification task. This approach should reduce reliance on human supervision and enable the aggregation of several datasets for the representation learning stage of the system, allowing for higher generalizability.\n\nIn this paper, an unsupervised solution for representation learning followed by arousal classification using wearablebased biosignals is proposed.Our solution utilizes stacked convolution autoencoders for ECG and EDA representation learning. Autoencoders are unsupervised neural network that attempt to generate output values approximately identical to the inputs successive to an intermediate step where latent representations are learned  [28] . This allows the model to automatically learn important latent representations of the input biosignals without the need for supervised labelling. Next, successive to unsupervised representation learning, we fuse the latent representations and utilize a random forest classifier for classification of low and high arousal.\n\nThe contributions of this work can be summarized as follows:\n\n• We present a novel affective state classification solution consisting of unsupervised multi-modal representation learning using stacked convolutional autoencoders followed by supervised learning of arousal states using a random forest classifier. Our proposed solution benefits from the lack of dependence on user input and supervision throughout the representation learning stage and as a result facilitates the aggregation of datasets, leading to higher generalizability.\n\n• We train and test the proposed solution with a aggregated dataset created by combining 3 publicly available datasets, AMIGOS  [29] , ASCERTAIN  [30] , and MAHNOB-HCI  [31] , along with CLEAS dataset  [32]  collected in our previous work. Moreover, we compare the performance with a number of baseline techniques for both feature extraction and classification. For feature extraction, we explore a large number of hand-crafted features as well as automatically learned representations using a convolutional neural network (CNN), while for classification we implement a handful of classifiers for comparison. Additionally, we compare the performance of our to a large number of related works in this area.\n\n• The results demonstrate the superiority of our proposed approach, outperforming all the baseline techniques as well as the related works, achieving state-of-the-art on AMIGOS, ASCERTAIN, CLEAS and MAHNOB-HCI datasets.\n\n• Lastly, our analysis shows the added benefit of utilizing a multi-modal approach versus a uni-modal one. Additionally, the added advantage of our unsupervised representation learning method in facilitating an easy aggregation of multiple datasets is demonstrated through comparing our multi-corpus versus single dataset results, in which our multi-corpus approach achieve better performance. The rest of this paper is organized as follows. Section II describes the background and related work while section III describes our proposed framework utilizing stacked convolutional autoencoders. Section IV gives details on the experiments performed using the proposed solution. Section V highlights the results of our framework on the 4 datasets, along with a detailed comparison with other methods, and Section VI presents a summary and future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background And Related Work",
      "text": "The term affective computing was first introduced by Rosalind Picard  [1]  to describe a new form of human computer interaction where the computer can recognize or influence the emotions of the users. Since its introduction, many studies have looked into how best to recognize emotions, as well as how computers should react to those emotions to enhance the user experience.\n\nThe most widely accepted model for emotion (affect), is the circumplex model proposed by Russel  [33] . The model utilizes 2 dimensions to describe emotional states, valence and arousal. Valence refers to how positively or negatively a person is feeling, while arousal refers to how relaxed or stressed they are feeling. This model allows for different emotions to be placed on this circle such that they can be defined as a combination of valence and arousal  [34] .\n\nUser-generated information such as facial expressions  [35] ,  [36] , gait  [37] ,  [38] , speech  [39] ,  [40] , and bio-signals  [41] ,  [42] , among others, are then used as inputs to emotion recognition or analysis methods, while the quantified and captured user affective states (e.g. SAM score) are used as outputs. Accordingly, machine learning techniques are often exploited to learn to classify or estimate the target affective states. In the following sub-sections, we present the past works that have used biosignals for affective computing, with a particular focus on works that have utilized the datasets being used in this study. These works can be divided into two categories: unimodal and multi-modal. While our approach in this paper is multi-modal, specifically using ECG and EDA, to provide a better summary of the work done in this field, we also review other uni-modal techniques.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Affective Computing With Uni-Modal Biosignals",
      "text": "Biosignals are the most widely used source of user generated input for machine learning classification of affect. This is due to advancements in the quality and usability of wearable sensors for data collection, along with the correlation between changes in affect, changes in the autonomic nervous system response, and changes in biosignals  [5] . The biosignals that are typically used for emotion classification are Electroencephalogram (EEG)  [43] , Electrodermal response (EDA), Electromyography (EMG), and Electrocardiogram (ECG)  [44]  among others. The use of EDA and ECG based features has been a particularly large focus of study when it comes to classifying the valence and arousal components of emotions. This is due to the fact that these signals have been found to be the most closely correlated with changes in affect  [20] , and are also relatively easy to record (for example compared to EEG).\n\n1) Classical Machine Learning Techniques: Gjoreski et al.  [45]  used manual feature extraction along with various classical machine learning models to find which models achieve the best performance for classifying arousal from biosignals. They extracted time domain features from ECG and EDA signals from the AMIGOS, ASCERTAIN, and MAHNOB-HCI datasets, separately. With the AMIGOS dataset the best results for ECG features was obtained using a K-Nearest neighbours (KNN) classifier achieving an accuracy of 0.53 while the best result for EDA was with a classifier using AdaBoosting with a decision tree as the base classifier. The best classifier for arousal classification with the ASCERTAIN dataset was a support vector machine (SVM) classifier achieving 0.66 accuracy for both modalities. Lastly, with the MAHNOB-HCI dataset the best performance found for ECG-based features was using an SVM with an accuracy of 0.62 while the best EDA-based classifier was a Naive-Bayes classifier with an accuracy of 0.62. Wiem et al.  [46]  also used an SVM classifier, instead utilizing only features extracted from ECG signals in the MAHNOB-HCI dataset. The features were used for binary arousal classification achieving an accuracy of 62%.\n\nFeatures extracted from ECG signals were used in  [44]  for the classification of valence and arousal using SVM classifiers with the DREAMER dataset. Affective data was collected from participants as they were asked to watch videos. Features were extracted by first identifying the PQRST waves within the ECG signal. Frequency based features were also extracted from the Power Spectrum Density (PSD) of the signal. Their work demonstrated the applicability for the use of these features with classical methods for arousal classification achieving an accuracy of 62% and F1 score of 0.53 for valence and 62% and 0.58 for arousal.\n\nThe capability for EDA to be used to classify both valence and arousal using classical machine learning classifiers was explored in  [47]  where EDA was used to discern 4 levels of arousal and 2 levels of valence. The EDA signals were collected as subjects were stimulated with sounds from the International Affective Digitized Sound System database. Features were then extracted and used with a KNN classifier to obtain 84% for valence and 77.33% accuracy for arousal.\n\n2) Deep Learning Techniques: Santamaria et al.  [48]  looked at using different feature extraction and arousal classification methodologies with the AMIGOS dataset. They compared manual feature extraction with classical machine learning models, with using a deep CNN for feature representation learning. The input to the CNN was pre-processed ECG and EDA signals. The CNN was comprised of 4 convolutional layers with max-pooling and dropout layers in between them and was compiled using the RMSProp optimizer with a learning rate of 0.0001. The output of the CNN was fed into a Multi-layer Perceptron (MLP) with 4 fully connected layers that produced an arousal classification. They found that their deep learning approach outperformed the use of classical machine learning methods achieving an accuracy and F1 score of 0.81 and 0.76 when using ECG signals, and 0.71 and 0.67 when using EDA signals.\n\nGjoreski et al.  [49]  also looked at comparing deep networks with classical machine learning models. In their study they utilized Deep Neural Networks (DNN) with the ASCERTAIN dataset for arousal classification using ECG signals. The DNN contained 7 hidden layers using the ReLU activation with L2 activity regularization. Dropout with a probability of 0.75 was used between each of the dense layers. The network was trained with the ADAM optimizer with a learning rate of 0.0001. The output from this network utilized the softmax activation function in order to give a class probability distribution for arousal. A random forest, SVM, and decision tree classifier using AdaBoosting classifier were also developed for comparison. The best results were found through using their developed DNN obtaining an accuracy of 0.69.\n\nAnother study that looked at using deep networks for arousal classification was performed by Sicheng et al.  [50] . In the study, a hypergraph learning framework was developed to classify arousal using the ASCERTAIN dataset. The framework produces a correlation between the changes in physiological signals and personality of the subjects, based on the stimuli used in the dataset to evoke emotions. The framework was utilized to classify 2 classes of arousal with ECG and EDA signals, separately. EDA outperformed the ECG classifier obtaining an accuracy of 0.75 compared to 0.72. These results were the best found when using ECG and EDA signals from the ASCERTAIN dataset.\n\nInstead of classifying valence or arousal, Sarkar et al.  [51]  instead looked at utilizing DNN with only ECG biosignals for expertise and cognitive load classification with the CLEAS dataset. Time and frequency domain features were manually extracted from the ECG signal and then utilized within the DNN to classify binary levels of cogntive load and arousal. The DNN consisted of an input layer accepting the extracted features as input vectors, followed by 7 fully connected dense hidden layers and a final output layer. After each hidden layer, the leaky Rectified Linear Unit (ReLu) activation function was used along with a dropout of 0.5 to prevent over-fitting. The results obtained achieved an accuracy and F1 score of 0.89 and 0.88, and 0.97 and 0.97, respectively. These findings are the current best results for affective state classification using the CLEAS dataset.\n\nIn  [52] ,  [53] , Sarkar and Etemad proposed a self-supervised method for ECG representation learning in the context of affective computing. The method first used the input signals to generate transformed versions of the data and automatically generated labels for the transformed signals corresponding to the transformation functions. These signals and labels were exploited to train a multi-task CNN, which upon successful training learned generalized representations of the unlabelled ECG signals. Transfer learning was then used to train a super-vised CNN for classification of valence and arousal in 4 public datasets including AMIGOS. The arousal classification using the fully supervised model achieved 0.84 accuracy and an F1 score of 0.83 while the self supervised model achieved better results with 0.89 accuracy, a 0.88 F1 score. These are the best results that could be found, to the best of our knowledge, for the AMIGOS dataset. The findings demonstrate the possible benefits when utilizing self supervised learning as opposed to fully supervised methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Affective Computing With Multi-Modal Biosignals",
      "text": "In addition to using biosignals as uni-modal inputs for machine learning classification, studies have also used multimodal inputs as it can utilize the complementary nature that different modalities can present  [54] .\n\n1) Classical Machine Learning Techniques: The multimodal use of ECG and EDA signals has been a large area of study. In  [55]  the results of using uni-modal and multimodal manually extracted features was compared for classification of 3 emotional states; happy, sad, and neutral. The results showed that using the multi-modal features with an SVM classifier outperformed the use of uni-modal features with an average accuracy of 91.62. In  [32]  ECG and EDA time and frequency domain features were extracted from the CLEAS dataset to classify 2 levels of expertise, novice and expert. The study utilized both uni-modal and multi-modal features in a variety of machine learning classifiers. The best result was achieved using multi-modal features with a KNN classifier obtaining an accuracy and F1 score of 0.83 and 0.80. Both studies highlighted the benefits of combining modalities for affective state classification. Anderson et al.  [16]  investigated the multi-modal fusion of EDA and ECG signals with other biosignals. The other signals utilized were EOG, EEG, and photoplethysmogram (PPG). The biosignals were obtained from participants as they participated in a number of stimulating tasks such as listening to music, watching videos, or playing video games. 98 features were extracted from the time and frequency domains of the varying modalities using manual feature extraction. The features were then used within a SVM for arousal classification achieving the best accuracy of 89% when all modalities were utilized.\n\nA number of datasets have been collected that were subsequently utilized for multi-modal arousal classification. Miranda-Correa et al.  [29]  collected the AMIGOS dataset and then utilized it for arousal classification by manually extracting 77 features from the collected ECG signals, 31 features from the EDA signals. The features were used separately, in addition to being fused to create a multi-modal feature set. The features were used with a Naive-Bayes classifier to classify high and low arousal. F1 scores of 0.55, 0.54, and 0.56 were achieved when using ECG, EDA, and fused features, respectively, demonstrating that fused ECG and EDA signals can achieve better performance than uni-modal features. Subramanian et al.  [30]  collected and then utilized the ASCERTAIN dataset for classification of 2 levels of arousal. They extracted 32 ECG and 31 EDA features for use in machine learning models for valence and arousal classification. SVM and Naive-Bayes classifiers were used with both the uni-modal and multi-modal features. The best results were obtained using a Naive Bayes classifier with fused ECG and EDA features obtaining an F1 score of 0.69. The MAHNOB-HCI dataset was introduced by Soleymani et al.  [31] . In their preliminary study, 64 ECG features, and 20 EDA features were extracted and subsequently used with an SVM classifier. This method obtained an accuracy of 0.46.\n\n2) Deep Learning Techniques: Deep learning methods have also been used for affective computing with multi-modal biosignals as inputs. Similar to our work, the use of autoencoders for learning representations towards arousal classification have also been investigated. Yang et al.  [56]  utilized an attribute-invariant loss embedded variational autoencoder (VAE) to extract latent representations from ECG, EDA, and EEG signals. These latent representations were used with an SVM classifier to classify binary levels of arousal achieving an accuracy of 0.69 and an F1 score of 0.69. Another method for latent feature extraction for valence and arousal classification was investigated by Liu et al.  [57] . In the study a bi-modal Deep Autoencoder was developed to extract latent features from EEG, ECG, and EDA signals from the DEAP dataset. The features were subsequently used by an SVM with a linear kernel to classify valence and arousal, achieving an accuracy of 85% and 81% for valence and arousal, respectively.\n\nThe benefits of using multi-modal features in deep learning frameworks was further explored by Siddharth et al.  [58] .\n\nIn their study, they utilized ECG, EDA, and EEG biosignals from the AMIGOS, DEAP, DREAMER, and MAHNOB-HCI datasets for the classification of valence, arousal, liking, and emotions. Their method utilized a combination of manually extracted statistical features in addition to deep-learning-based features. These feature were obtained by first converting the biosignal time series data to a spectrogram image and then using a pre-trained VGG-16  [59]  CNN to learn latent features from the image. The features were concatenated used within an Extreme Learning Machine (ELM) for classification. For the AMIGOS, and DEAP datasets, the best performance was obtained using the multi-modal features, showing the potential benefits of using multi-modal data. The DREAMER and MAHBOB-HCI datasets had the best results when using only uni-modal ECG features. The result for the MAHNOB-HCI dataset is noteworthy as it presents the current state-of-the-art performance for the dataset with an accuracy of 0.82 and a F1 score 0f 0.75.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "Our goal is to develop a framework for multi-modal affective computing with minimal human supervision. Accordingly, we propose a framework that utilizes stacked convolutional autoencoders for unsupervised ECG and EDA representation learning, and subsequent classification of arousal using a supervised classifier. The method takes filtered and normalized ECG and EDA signals and utilizes 2 separate unsupervised autoencoders, 1 for each modality, to learn latent representations. These representations are then used with a random forest classifier for 2-class classification of arousal. The proposed framework is summarized in Figure  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Pre-Processing",
      "text": "The Pan-Tompkins algorithm was utilized to filter the raw ECG signals  [60] ,  [61] . The Pan-Tompkins algorithm first uses a Butterworth bandpass filter with a passband frequency of 5-15 Hz to reduce the EMG noise, powerline noise, baseline wander and T-wave interference. The filter was applied in both the the forward and reverse directions to achieve zero phase distortion. Following the methods used in  [62]  the EDA signals were first filtered using a low-pass filter with a cutoff frequency of 1 Hz. High frequency artifacts were then removed with a moving average filter of 100 samples.\n\nAfter filtering, the ECG and EDA signals were re-sampled to 256 Hz and 128 Hz, respectively. This was done to ensure that the input data from various datasets were the same size. The signals of individual subjects were also normalized to values between 0 and 1 for better use with activation functions within the framework. Examples of the raw ECG and EDA signals along with the filtered and normalized signals are shown in Figure  2 .\n\nThe ECG and EDA signals were subsequently segmented into 10-second windows to form individual samples. The window size was selected similar to that in  [63]  for maximum performance while being small enough to better enable realtime applications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Unsupervised Multi-Modal Representation Learning",
      "text": "An autoencoder is an unsupervised learning technique that utilizes backpropagation and takes unlabelled input values x i , where x i ∈ R, and attempts to map them to an output, xi , where xi ∈ R. The autoencoder is divided into two components, the encoder and the decoder. In the encoder component, the input values x i are mapped to a latent representation h i .\n\nThe input values are mapped to the latent representation h i by using a deterministic function such that  [28] :\n\nwhere σ enc is the deterministic encoding function, W enc is the the weight matrix of the encoding section of the autoencoder and b enc is the bias of the encoder  [64] . The decoder section of the model follows the same approach to map to the output values xi given that:\n\nwhere σ dec is the deterministic decoding function, W dec is the the weight matrix of the decoding component, and b dec is the bias of the decoder. Through the encoding and decoding functions, the autoencoder seeks to approximate the identity function such that the output values xi are the same as the input x i  [65] . The autoencoder is trained to minimize the reconstruction error based on the loss function L(x, x). In our study, the loss function used was the mean squared error such that:\n\nThe methodology of an autoencoder allows for the latent representations h i to be comprised of only features that are the most relevant for the reconstruction of the input values. A sparse autoencoder can be created using the same framework, but introduces a sparsity constraint to the hidden layers  [65] . By introducing this constraint, the autoencoder is still able to learn latent representations relevant to the structure of the input, even if the number of units in the latent space is large  [65] . In our proposed method the sparsity constraint is added to the autoencoder through the use of L1 regularization  [66] . The L1 regularizer is added to the loss function L such that:\n\nwhere w is the weight. When multiple hidden layers are combined one after another, the framework is known as a stacked autoencoder. In this framework, the input to each subsequent layer is the output from the previous hidden layer, which is the latent representation of the signal generated by that layer  [28] .  searching a number of possible combinations with the aim of obtaining the best results for arousal classification.\n\nThe encoder component of the AE ECG consists of an input layer followed by 16 hidden layers, and a final fully connected layer. The inputs to the encoder are 10 second ECG segments (2560 samples) as discussed earlier. The first hidden layer performs a 1D convolution on the input of size 128 × 200 using the ReLU activation function. A max-pooling layer is then utilized to reduce the size of the samples in half. After this layer a number of convolutional blocks are used. The convolutional blocks consists of 2 1D convolutional layers with different dimensions followed by a batch normalization layer to reduce covariant shift  [67] , and then a max-pooling layer. Within the ECG encoder component this convolutional block is repeated 3 times. After the convolutional blocks there is a 10 × 1 1D convolutional layer, a max-pooling layer, and then finally a fully connected dense layer. The dense layer contains 80 units, with an L1 activity regularizer with a value of 10 -9 that creates a latent feature representation of size 80.\n\nThe latent representations learned by the encoder are fed to the decoder component. The decoder consists of 12 hidden layers followed by an output convolutional layer using the ReLU activation function. The hidden layers are comprised of upsampling and convolution layers in the reverse order of the encoder section. The parameters of each layer matches the corresponding encoder layer. The final output from the decoder, and thus the autoencoder as a whole, is a reconstructed ECG segment of the same size as the input (2560).\n\nThe structure of the layers within the AE EDA are similar to that of the developed AE ECG . The main differences stem from the fact that the input EDA samples inputted to the encoder section have a shape of 1280 × 1. The encoder section is comprised of an input layer followed by 12 hidden layer, and a fully connected layer representing the latent EDA features space. The first hidden is a 1D convolution with 32 filters, and a kernel size of 100 × 1. All the convolutional layers in the AE EDA use the ReLU activation function. This layer is followed by a max-pooling layer to reduce the shape of the sample by half. These layers are followed by 2 convolutional blocks. This is followed by a convolutional layer with 1 filter and a kernel size of 10 × 1 and a max-pooling layer to reduce the sample to a size of 80 × 1. Identical to the AE ECG this layer is fed into a fully connected dense layer with 80 units, and an L1 activity regularizer of 10 -9 giving 80 latent EDA features.\n\nThe decoder section of the AE EDA contains 9 hidden layers and a final convolutional output layer that are arranged to be the inverse of the encoder section. This results in the stacked convolutional autoencoder reconstructing the original EDA sample with it's initial size of 1280 × 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Modality Fusion And Classification",
      "text": "The latent representations learned from the ECG and EDA signals by the respective unsupervised networks are concatenated with a feature-level fusion strategy to form a combined representation for supervised arousal classification with a random forest classifier. Random forest was selected for this application as it is one of the most commonly used supervised classification models for learning latent representations derived from autoencoders for classification tasks in other fields  [68] -  [71] .\n\nRandom forest  [72]  is an ensemble method consisting of several decision trees that each perform an individual classification which contributes to the final decision. The ensemble decision is made based on a maximum vote among the individual classification results. The number of decision trees used in the random forest, along with the other parameters used, were searched for and determined systematically to achieve the highest accuracy. It was found that a random forest of 100 trees achieved the best results. Bootstrap samples were used when building the decision trees. Class weights were selected to be inversely proportional to the class frequencies within each bootstrap sample.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "Four datasets were utilized to enable a multi-corpus evaluation of our proposed solution in order to ensure the approach can work effectively using different wearable devices in varying settings. The four datasets used in this study were AMIGOS  [29] , ASCERTAIN  [30] , CLEAS  [32] ,  [51] , and MAHNOB-HCI  [31] . The following sections provide a detailed description of each dataset, followed by a summary of properties and differences presented in Table  I .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1) Amigos: A Dataset For Multi-Modal Research Of Affect, Personality Traits And Mood On Individuals And Groups [29]:",
      "text": "This dataset contains collected biosignals as well as and Big-Five personality data  [73]  from 40 participants during short and long emotional videos. The short videos were 250 seconds in length while the long videos were more than 14 minutes long. The participants' EEG, ECG, and EDA signals have been collected with wearable sensors. The ECG system used had 5 channels, while the EDA system had 1 channel. Both signals were captured at a sampling rate of 128 Hz. Arousal values for each video were obtained from the subjects using the 9-point Self-Assessment Manikin (SAM) scale  [74] . 2) ASCERTAIN: A multi-modal databaASe for impliCit pERsonaliTy and Affect recognitIoN  [30] : This dataset contains the Big-Five personality trait data from 58 participants along with their EEG, ECG, EDA, and facial activity. These data were collected as the participants watched affective movie clips. The average length of the movie clips were 80 seconds. ECG data was captured using a 3-channel system with a sampling rate of 256 Hz, and EDA was single channel with a sampling rate of 128 Hz. Arousal values were elicited from the participants using a 7 point scale from 0 (very boring) to 6 (very exciting).\n\n3) CLEAS: Cognitive Load and Expertise for Adaptive Simulation  [32] ,  [51] : We collected this dataset which contains biosignals as well as cognitive load and arousal information acquired during medical simulations. This dataset was collected in collaboration with researchers from Kingston General Hospital  [75] . Ethics approval was secured from the Queen's University Research Ethics Board.\n\nA total of 10 participants were recruited for the study, 5 experts (3 male, 2 female) and 5 novices (2 male, 3 female). The participant in the simulation played the role of a trauma team leader put in charge of directing a trauma team on how to best provide care for the patient. The participant was placed in 2 different 10 minute long simulations were developed in which a patient had suffered trauma. A SimMan patient simulator (a mannequin)  [76]  was used as the patient in these simulations.\n\nShimmer3 wearable sensors  [77]  were used to collect ECG, EDA, body temperature and inertial measurement unit (IMU) data during the simulations at . Videos of the simulations were recorded with a Microsoft HoloLens  [78]  worn by the participants. The first person videos recorded during the simulations were used during debriefs with the participants successive to completing the simulation. The videos allowed for the participants to review their performance and to annotate their arousal throughout the simulations. The arousal values were collected using a 4 point scale from 1 (calm) to 4 (anxious).  [31] : This dataset contains biosignal data as well as valence and arousal score obtained when showing participants fragments of movies and pictures. The 30 participants in the study were shown the movies and pictures for between 12 and 22 seconds. The valence and arousal data collected were obtained by having the participants annotate their own affective states using the SAM scale. The biosignals obtained during the study were ECG, EEG, EDA, respiration amplitude, and skin temperature. Both the ECG and EDA data were captured with a sampling rate of 256 Hz.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "4) Mahnob-Hci: A Multi-Modal Database For Affect Recognition And Implicit Tagging",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Data Labelling",
      "text": "The learned latent ECG and EDA representations based on the aggregated dataset used in this study are labelled for arousal. Of the 4 datasets, 2 of them, AMIGOS and MAHNOB-HCI, used a 9-point SAM scale for annotating arousal values during the various trials. The ASCERTAIN dataset used a 7-point scale, and the CLEAS dataset used a 4-point scale. To ensure consisted labelling across the multicorpus the arousal values from each dataset were normalized to values between 1 and 9. This allowed for the datasets to all reflect the 9-point SAM scale. Similar to many works in this area  [45] ,  [46] ,  [48] -  [50] ,  [58] , the arousal values were split into two classes: low and high. Following the work done in  [43] , an arousal value less than 5 was labelled as 'low' while an arousal value greater than or equal to 6 was considered high 'arousal'.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Implementation Details, Training, And Validation",
      "text": "The baseline machine learning models (discussed later in Section IV-F), along with the proposed stacked convolutional autoencoder are developed using scikit-learn  [79] , TensorFlow  [80]  and Keras  [81]  on a computer with a NVIDIA GeForce RTX 2080 TI graphics card and a Intel Core i7-9700k CPU. The two stacked convolutional autoencoders are trained and validated using a 10-fold validation scheme. Within this validation scheme the aggregated dataset are randomly divided into 10 folds while being stratified such that the percentage of samples in each fold from the individual datasets is representative of the percentage of samples from that dataset in the aggregated dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "To evaluate the efficacy of our proposed framework for arousal classification the results were compared with a variety of other methodologies. The comparisons were based on accuracy and F1 score similar to most other studies in the area  [48] ,  [52] ,  [56] ,  [58] . To do this the number of True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) classifications were calculated. TP and TN measures were the number of correctly classified arousal classes where, while FP and False Negative FN were defined as the number of incorrect classifications. For the purposes of our study the high arousal class was treated as positive, while the low arousal class was negative.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Manual Feature Extraction And Selection",
      "text": "We compare the multi-modal representations learned by the unsupervised autoencoders to manually extracted and selected features, generally referred to as hand-crafted features. We select ECG and EDA features that are popular in the field of biosignal analysis, especially those int the field of affective computing  [30] ,  [31] ,  [45] ,  [49] ,  [62] ,  [82] ,  [83] .\n\nIn order to extract features from the ECG signals, the Pan-Tompkins algorithm was used to detect the QRS complexes of the ECG waves  [60] ,  [61] . This is accomplished by differentiating the filtered ECG signal using a 5-point derivative transfer function in order to gain the QRS slope information. The absolute value of the signal is taken, and then a moving average filter is applied to obtain the wave form features. To detect the R-peaks of the ECG waveform, two threshold values are selected in order to differentiate the peaks from noise. The two threshold values are determined iteratively. If there no peaks are detected within a two second time period then a search-back technique was used to identify any R-peaks that were missed.\n\n12 time domain and 8 frequency domain features are extracted from the pre process ECG signals following the work done in  [82] ,  [83] . The time domain features are extracted from the the distance between two subsequent R-peaks identified in the signal, known as the RR intervals. The frequency domain features are extracted using a power spectrum density (PSD) analysis utilizing a Lomb periodogram  [84] . These extracted features are summarized in Table IX in Appendix A.\n\n30 time domain features are extracted from the EDA signals, similar to those extracted in  [62] . Time domain features were extracted from 5 components of identified SCR event peaks, the rise time, half recovery time, amplitude, area, and prominence. The SCL of the EDA signal was also found and used to extract time domain features. Next, frequency domain features were extracted successive to performing PSD to obtain the total power estimate among other features. The manually extracted EDA features are summarized in Table X in Appendix A.\n\nSubsequent to feature extraction from the ECG and EDA signals Least Absolute Shrinkage and Selection Operator (LASSO)  [85]  is used to select the suitable features for arousal classification. LASSO is used to determine the regression coefficients of each feature where the larger the coefficient, the greater it's importance for the classification task. Features that have values close to or equal to zero are not suitable for use in models  [86] . The results from using LASSO to select important features is presented and discussed in Section V-B.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Baseline Classifiers",
      "text": "Several machine learning models were utilized for comparison with our proposed random forest classifier. These classifiers were evaluated using automatically extracted latent features from the stacked convolutional autoencoders, as well as manually extracted features, separately. The parameters of the machine learning models are tuned empirically successive to iterative experiments to optimize the results for both the use of latent representations of features, and hand crafted features. The details of the 6 models developed are described below.\n\n• Support Vector Machine (SVM): Several kernels, namely linear, polynomial with a degree of 2 and 3, and radial basis function (RBF) were evaluated. The best results were obtained with an SVM using an RBF kernel, with a regularization parameter of 0.6, a gamma value (kernel coefficient) equal to 1 divided by the number of features, and class weights inversely proportional to the class frequencies.\n\n• K-Nearest Neighbours (KNN): A k value of 7 is found to produce the best results when used in conjunction with weighting the neighbours based on the inverse of the distance to the sample. The Euclidean distance metric is used.\n\n• Decision Tree: The decision tree (DT) with the best results utilizes entropy for the information gain of evaluating the quality of the split at each node, requires a minimum of 10% of the samples to split an internal node, uses 90% of the samples for determining the best split, and has class weights inversely proportional to the class frequencies.\n\n• Multilayer Perceptron (MLP): The MLP developed for use with the automatically extracted latent features has an input dense layer of 160 neurons. All dense layers utilize the ReLU activation function and are followed by a dropout layer with a coefficient of 0.5. Following the input layer the hidden layers are dense layers with 80, 40, and 40 neurons respectively. The output layer is a dense layer with 1 neuron, and uses the sigmoid activation function. The MLP is optimized with the RMSProp optimizer over 200 epochs minimizing binary cross-entropy loss. For use with the manually extracted features the MLP was modified such that the neurons in each layer are 36, 24, 16, and 1, respectively.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "G. Cnn-Based Representation Learning And Classification",
      "text": "Our proposed framework is also compared with utilizing deep CNNs that learns latent representations of the input signals and utilizes them for arousal classification using fully connected layers. 2 CNNs are developed, an ECG CNN and an EDA CNN. The CNN architectures is similar to the architectures developed for the stacked convolutional autoencoders. For the ECG CNN the first 17 layers are identical to that of the AE ECG , while in the EDA CNN the first 13 layers are identical to the AE EDA . The convolutional layers of these CNNs reduce the ECG size from 2560 × 1 to 80 × 1, and the EDA size from 1280 × 1 to 80 × 1. After the convolutional layers, there are a series of 4 fully connected layers containing 80, 40, 20, and 1 neurons respectively. The first dense layer contains L1 activity regularization with a value of 1.0 × 10 exp -9. In between every two consecutive dense layers a dropout layer with a rate of 0.5 is utilized. The final dense layer uses a sigmoid activation function while the other dense layers use the ReLU activation function. Both of the CNNs are trained using the Adam optimizer with a learning rate of 1.0 × 10 exp -4. The two CNNs (ECG and EDA) are trained for 1500 and 4000 epochs respectively, both with a batch size of 64. Training is stopped early based on the validation loss no longer decreasing after 50 epochs. The outputs from the last dropout layer of each CNN are taken and fused together and fed into a MLP consisting of a dense layer with 40 neurons and using the ReLU activation function, followed by a dropout layer with a ratio of 0.5, and finally a dense layer with 1 unit, using the sigmoid activation function. This MLP was trained using the RMSProp optimizer with a learning rate of 1.0 exp -3, for 200 epochs.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "H. Fusion Strategies",
      "text": "The method through which the ECG and EDA features are fused is also compared for the machine learning models using latent and hand crafted features in addition to the developed CNN. For this comparison only the machine learning models found to have the best performance for automatically learned and hand-crafted features are used. The two methods of fusion compared are feature-level fusion and decision-level fusion.\n\nFor feature-level fusion, the ECG and EDA features are combined prior to arousal classification. Within our proposed framework, this entailed using the respective stacked convolutional autoencoders to extract latent representations from the two modalities. These latent features are then simply concatenated prior to use within a machine learning model for arousal classification. A similar process is followed for the manually extracted features in which they are combined after extraction and selection before use within machine learning models. In the case of the CNN, the representation obtained from the two CNNs after the final convolutional layers are taken and fed into a separate MLP. This MLP is identical, in terms of details and parameters, to the dense layers within the developed CNN. The MLP is trained to take the signal representations and output an arousal class value.\n\nDecision-level fusion involves classifying arousal based on ECG and EDA features separately, then combining the outputs to form a final classification result. Within the context of using traditional machine learning models with latent and handcrafted features, this implies that the machine learning models are first trained for each modality separately. The classification probabilities are then taken as the output from the model and fed into an MLP consisting of an input layer of 2 neurons, and an output layer of 1 neuron. Both layers used linear activation function. This method allows for the relative weights of the 2 inputs to be automatically learned to achieve the best results. The frameworks used for feature level and decision level fusion are shown in Figure  4  The best results from comparing feature fusion techniques are taken and compared with the use of uni-modal features. This is accomplished by using the respective ECG and EDA frameworks separately. This is done in order to examine which modality performs the best for arousal classification, and if there is a benefit in combining the two modalities.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "I. Single Corpus Vs. Multi-Corpus",
      "text": "One of the intentions behind our unsupervised representation learning framework was easy aggregation of multiple datasets to take advantage of an increased amount of data from an aggregated dataset. However, to evaluate the impact of combining several datasets, we also train the solution with each dataset separately and compare with the multi-corpus approach.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "J. Other Works And State-Of-The-Art",
      "text": "We compare our results to related works in arousal classification for each of the datasets used in this study, described in Section II. For a fair comparison, only studies that utilize ECG and/or EDA signals are used. Additionally, we compare to current state-of-the-art results for each individual dataset. Specifically, we compare to  [52]  for the AMIGOS datasets,  [50]  for the ASCERTAIN dataset, and  [58]  for the MAHNOB-HCI dataset. For the CLEAS dataset, where there are not any related works on arousal classification, our results are compared with related works that have performed other affective state classification tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Results And Discussions",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Performance",
      "text": "In this section we explore the performance of the unsupervised autoencoder methodology for developing latent representations in addition to the performance of the supervised arousal classification models. To evaluate the impact of the supervised classifier in our proposed framework, we utilize the multi-modal representations learned from the proposed unsupervised autoencoders for classification with 5 supervised methods (as discussed in Section IV-F). The results are presented in Table  II . It can be seen that the random forest classifier obtained the best results on every dataset with an accuracy/F1 score of 0.93/0.95, 0.79/0.86, 0.99/0.98, and 0.83/0.76 for AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI, respectively, and consequently the best overall results with an accuracy of 0.89 and an F1 score of 0.91.\n\nWhen comparing the performance of the machine learning models used, the performances are ranked in the order of SVM, MLP, KNN, decision tree, and random forest. It is particularly interesting to note the difference in performance between the decision tree, and the random forest as it demonstrates the advantages of using ensemble learning methods. The use of a decision tree classifier achieves an accuracy and F1 score of 0.74 and 0.79, while the random forest classifier with 100 decision trees greatly increases the performance to an accuracy of 0.89 and an F1 score of 0.91. These findings support the use of ensemble learning using the learned multimodal latent representations for arousal classification.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Representation Learning Vs. Feature Extraction Techniques",
      "text": "To investigate the impact of the unsupervised representation learning achieved using our proposed framework, we compare the discriminability of the representations to hand-crafted features manually extracted and selected from the ECG and EDA signals. Using LASSO on the extracted features, 11 ECG features and 25 EDA features are found to have nonzero regression coefficient. These features are considered to be important and thus are used within the developed machine learning models for classification.\n\nThe EDA feature with the greatest regression coefficient, and thus the greatest importance for arousal classification, is found to be the standard deviation of the half recovery time with a regression coefficient of 0.3083. The features with the next highest importance are those from the rise time of SCR events, followed by the amplitude of SCR events and then the area of the SCR events. The lowest regression coefficients within the EDA important features are those taken from the skin conductance level. This indicates that the SCR events within an EDA signal are likely applicable for the classification of arousal while the SCL of the signal may be less useful. None of the frequency domain features are found to be of importance for this classification task.\n\nAlthough more EDA features are found to be important for the classification of arousal, the regression coefficients of the important ECG features were higher than that of EDA. This finding suggests that the ECG features may be more discriminative for detection of arousal than EDA. The highest regression coefficient is 0.8861 for the minimum RR interval. 3 other features based on RR intervals are found to be important: RR interval maximum, difference, and standard deviation with regression coefficients of 0.2635, 0.2341, and 0.2211, respectively. This suggests that the RR peaks are the most important feature that can be extracted from the ECG signal for the purpose of this arousal classification.\n\nThe features that are selected using LASSO are used as inputs to the same machine learning models developed for use with our proposed unsupervised model. As was the case with the learned latent representations, the best overall results when using hand-crafted features was obtained using the random forest, achieving an accuracy of 0.66 and an F1 score of 0.72. The machine learning models using automatically extracted latent features were also compared with using a deep convolutional neural network for arousal classification. 2 separate CNN were trained using ECG and EDA signals separately.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "C. Feature Space Exploration",
      "text": "We perform t-SNE  [87]  on the uni-modal and multi-modal latent representations learned by our proposed unsupervised solution, for visualization. For comparison we also perform t-SNE on uni-modal and multi-modal manually extracted features and latent representations learned by CNN.\n\nt-SNE was performed using 10000 iterations with a perplexity of 30, and learning rate of 10. The outcome is shown in Figure  5 . The figure illustrates that the learned representations for ECG, EDA, and multi-modal are visibly more separable compared to hand-crafted features and learned representations from CNN, further validating the advantage of using our proposed solution. Additionally, it can be observed that the latent representations learned from ECG exhibit an enhanced separability compared to that of EDA, indicating that ECG is likely to be a more discriminative modality for arousal detection. This could lead to the ECG features being a better modality for arousal classification than EDA.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D. Impact Of Multi-Modality And Fusion",
      "text": "We evaluate the impact using both ECG and EDA modalities (multi-modal) versus the two modalities individually  (uni-modal). Moreover, we evaluate the performance when a feature-level fusion strategy is used for the learned representations of the two modalities in our solution compared to decision-level fusion. Lastly, we perform the same comparisons for hand-crafted features and CNN-based learned representations. The results of this analysis is are presented in Table  III . Looking at the uni-modal results, it is observed that EDA features prove more discriminative when hand-crafted features are used, when automatically learned representations are em-ployed (CNN-based and ours) ECG is a more suitable modality for arousal classification compared to EDA. Moreover, the results show that, as expected, multi-modal approaches (both feature-level and decision-level fusion) outperform the use of uni-modal features for the hand-crafted features, CNN-based representations, and the unsupervised learned representations (our method). Lastly, the results illustrate that while for handcrafted features and CNN-based representations, feature-level and decision-level fusion strategies perform on-par with one another, for our unsupervised learned representations, featurelevel fusion performs noticeably better than decision-level fusion.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Individual Vs. Multi-Corpus",
      "text": "As discussed earlier, in our experiments, 4 datasets are combined to create a single aggregated dataset. In order to investigate the impact of combining the 4 datasets, we compare the outcome of the 3 methodologies (hand-crafted features, CNN-based representations, and ours) when individual datasets are used to train and test the models as opposed to the aggregated dataset. The results presented in Table  IV  illustrate that while hand-crafted features show very close performance for single datasets and the multi-corpus approach, the CNN-based method and our framework both show a clear advantage to using a larger aggregated dataset. This observation is expected as the quality or type of hand-crafted features do not change with more data, i.e. they remain identical from one dataset to the other. Meanwhile, with different training data, learned representations change, and with a multi-corpus approach for training, their generalization and discriminability improves. Although the use of multi-corpus increased the performance of our method, it is important to note that the",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Comparison To Other Works And State-Of-The-Art",
      "text": "We compare our results to other related works that utilized the 4 datasets used in this study. Table V presents the results for the AMIGOS dataset where our proposed framework outperforms the state-of-the-art. The best result on this dataset was previously achieved in  [52] , where a self-supervised approach was used for ECG-based arousal detection with an accuracy and F1 of 0.89 and 0.88 respectively. Our solution outperformed this work with an accuracy and F1 score of 0.93 and 0.95, achieving respective gains of 0.04 and 0.07 for accuracy and F1.\n\nThe comparison between our results, and the related works utilizing the ASCERTAIN dataset are shown in Table  VI . The state-of-the-art result was found in  [50]  with an accuracy of 0.75 using EDA alone with manual feature extraction and subsequent classification with hypergraph learning. Our proposed unsupervised learning framework achieves better results on ASCERTAIN with an accuracy of 0.79 and F1 score of 0.86, obtaining a gain in accuracy of 0.04.\n\nAlthough the CLEAS dataset has not been previously used for arousal classification, we compare our results to related works that have utilized the dataset for other classification tasks as shown in Table  VII . The state-of-the-art results for classifying cognitive load and expertise were found in  [51]  with an accuracy and F1 score of 0.89 and 0.88 for cognitive load, and 0.97 and 0.97 for expertise. Our results using the unsupervised autoencoders and random forest classifier achieve an accuracy of 0.99 and an F1 score of 0.98 for arousal classification.\n\nLastly, the comparison between our results and the related works for the MAHNOB-HCI dataset are shown in Table  VIII . The best result was found in  [58]  with an accuracy of 0.82 and an F1 score of 0.75 using a uni-modal EDA representation extracted by a CNN with an ELM classifier. Our result outperformed their results with an increase in accuracy and F1 score of 0.01 to 0.83 and 0.76, respectively.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusions And Future Work",
      "text": "In this study we examined the use of stacked convolutional autoencoders for learning multi-modal latent representations from wearable biosignal data for subsequent use with a random forest classifier for arousal classification. We utilized 4 datasets and compared our proposed framework with a number of other techniques for feature extraction and machine learning. Our method showed an overall superior performance with the other methodologies examined with an accuracy and F1 score of 0.89 and 0.91 respectively. Specifically, we obtained accuracies and F1 scores of 0.93 and 0.95 for AMIGOS, 0.79 and 0.86 for ASCERTAIN, 0.99 and 0.98 for CLEAS, and 0.83 and 0.76 for the MAHNOB-HCI datasets. Moreover, our results outperformed the previous state-of-the-art results using the same modalities for arousal classification with the AMIGOS, ASCERTAIN, and MAHNOB-HCI datasets. Arousal detection was performed for the first time on CLEAS. We believe that our method provides a valuable solution for unsupervised learning of multimodal ECG and EDA representations along with supervised learning of affect using a random forest. Our solution performs accurately and is capable of aggregating several datasets to leverage a multi-corpus approach, all the while reducing reliance on human supervision.\n\nDespite the advantages of our method in reduced reliance on human-annotated data, there are a number of areas which can be considered for future work. For instance, the ability for the solution to generalize to new data can be further explored by changing the method of training and testing validation. The results in this study were obtained by training and validating the framework with 10-fold cross-validation. While most works in this area, including the state-of-the-art references with which we compared our results also used k-fold cross-validation, the use of leave-one-subject-out schemes can be used to better evaluate generalization to unseen data. Moreover, a leaveone-data-out validation scheme can provide further insight into the performance of the method to unseen full datasets. Additionally, more diverse CNN architectures can be explored for comparison with our architecture. Moreover, architectures that exploit both CNNs and recurrent neural networks can provide additional reference points for the performance of our unsupervised representation learning approach.\n\nIn the end, while our method achieved great performance for classification of arousal, in future work, additional affective factors, namely valence or dominance, can be explored. In addition, the problem space can be modified from classification to regression with the aim of estimating the affect scores across different dimensions or as the intensity of a particular emotion.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed framework for latent biosignal feature representations",
      "page": 5
    },
    {
      "caption": "Figure 2: The ECG and EDA signals were subsequently segmented",
      "page": 5
    },
    {
      "caption": "Figure 2: Example of a) 10 seconds of raw ECG signal; b) 10 seconds of",
      "page": 5
    },
    {
      "caption": "Figure 3: The architectures of the proposed autoencoder networks to learn latent",
      "page": 6
    },
    {
      "caption": "Figure 4: Feature level fusion, also known as early fusion, and decision level",
      "page": 9
    },
    {
      "caption": "Figure 4: The best results from comparing feature fusion techniques",
      "page": 9
    },
    {
      "caption": "Figure 5: The ﬁgure illustrates that the learned representations",
      "page": 10
    },
    {
      "caption": "Figure 5: tSNE of a) ECG representation learned from the ECG autoencoder;",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "EDA",
          "0.63": "0.62",
          "0.73": "0.75",
          "0.57": "0.66",
          "0.67": "0.79",
          "0.65": "0.71",
          "0.42": "0.67",
          "0.54": "0.61",
          "0.39": "0.43",
          "0.59": "0.64"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "Decision-\nLevel",
          "0.63": "0.66",
          "0.73": "0.79",
          "0.57": "0.68",
          "0.67": "0.81",
          "0.65": "0.73",
          "0.42": "0.77",
          "0.54": "0.59",
          "0.39": "0.45",
          "0.59": "0.65"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "Feature-\nLevel",
          "0.63": "0.66",
          "0.73": "0.76",
          "0.57": "0.67",
          "0.67": "0.79",
          "0.65": "0.72",
          "0.42": "0.77",
          "0.54": "0.64",
          "0.39": "0.42",
          "0.59": "0.66"
        },
        {
          "Hand-crafted\nFeatures": "Learned CNN\nRepresentations",
          "ECG": "ECG",
          "0.63": "0.86",
          "0.73": "0.89",
          "0.57": "0.79",
          "0.67": "0.85",
          "0.65": "0.85",
          "0.42": "0.68",
          "0.54": "0.73",
          "0.39": "0.66",
          "0.59": "0.82"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "EDA",
          "0.63": "0.67",
          "0.73": "0.77",
          "0.57": "0.65",
          "0.67": "0.77",
          "0.65": "0.74",
          "0.42": "0.36",
          "0.54": "0.58",
          "0.39": "0.58",
          "0.59": "0.65"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "Decision-\nLevel",
          "0.63": "0.88",
          "0.73": "0.91",
          "0.57": "0.79",
          "0.67": "0.86",
          "0.65": "0.87",
          "0.42": "0.67",
          "0.54": "0.75",
          "0.39": "0.67",
          "0.59": "0.85"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "Feature-\nLevel",
          "0.63": "0.88",
          "0.73": "0.91",
          "0.57": "0.79",
          "0.67": "0.85",
          "0.65": "0.87",
          "0.42": "0.65",
          "0.54": "0.75",
          "0.39": "0.68",
          "0.59": "0.84"
        },
        {
          "Hand-crafted\nFeatures": "Ours\n(Autoencoders)",
          "ECG": "ECG",
          "0.63": "0.85",
          "0.73": "0.89",
          "0.57": "0.65",
          "0.67": "0.73",
          "0.65": "0.82",
          "0.42": "0.77",
          "0.54": "0.67",
          "0.39": "0.62",
          "0.59": "0.78"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "EDA",
          "0.63": "0.64",
          "0.73": "0.72",
          "0.57": "0.59",
          "0.67": "0.69",
          "0.65": "0.68",
          "0.42": "0.79",
          "0.54": "0.58",
          "0.39": "0.48",
          "0.59": "0.63"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "Decision-\nLevel",
          "0.63": "0.76",
          "0.73": "0.80",
          "0.57": "0.60",
          "0.67": "0.67",
          "0.65": "0.75",
          "0.42": "0.80",
          "0.54": "0.68",
          "0.39": "0.57",
          "0.59": "0.73"
        },
        {
          "Hand-crafted\nFeatures": "",
          "ECG": "Feature-\nLevel",
          "0.63": "0.93",
          "0.73": "0.95",
          "0.57": "0.79",
          "0.67": "0.86",
          "0.65": "0.91",
          "0.42": "0.98",
          "0.54": "0.83",
          "0.39": "0.76",
          "0.59": "0.89"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hand-crafted\nFeatures": "",
          "Separated": "Multi-Corpus",
          "0.67": "0.66",
          "0.77": "0.77",
          "0.66": "0.67",
          "0.79": "0.79",
          "0.91": "0.91",
          "0.64": "0.64",
          "0.41": "0.42",
          "0.72": "0.72"
        },
        {
          "Hand-crafted\nFeatures": "Learned CNN\nRepresentations",
          "Separated": "Separated",
          "0.67": "0.66",
          "0.77": "0.55",
          "0.66": "0.68",
          "0.79": "0.81",
          "0.91": "0.86",
          "0.64": "0.59",
          "0.41": "0.38",
          "0.72": "0.73"
        },
        {
          "Hand-crafted\nFeatures": "",
          "Separated": "Multi-Corpus",
          "0.67": "0.85",
          "0.77": "0.67",
          "0.66": "0.79",
          "0.79": "0.85",
          "0.91": "0.84",
          "0.64": "0.75",
          "0.41": "0.67",
          "0.72": "0.87"
        },
        {
          "Hand-crafted\nFeatures": "Ours\n(Autoencoders)",
          "Separated": "Separated",
          "0.67": "0.85",
          "0.77": "0.98",
          "0.66": "0.76",
          "0.79": "0.82",
          "0.91": "0.99",
          "0.64": "0.78",
          "0.41": "0.74",
          "0.72": "0.88"
        },
        {
          "Hand-crafted\nFeatures": "",
          "Separated": "Multi-Corpus",
          "0.67": "0.89",
          "0.77": "0.98",
          "0.66": "0.79",
          "0.79": "0.86",
          "0.91": "0.99",
          "0.64": "0.83",
          "0.41": "0.76",
          "0.72": "0.91"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "GNB": "",
          "-": "-",
          "0.55": "0.54"
        },
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "ECG, EDA, EEG",
          "Hand-crafted": "",
          "GNB": "",
          "-": "-",
          "0.55": "0.56"
        },
        {
          "Miranda-Correa et al.\n[29]": "Gjoreski et al.\n[45]",
          "2018": "2018",
          "ECG": "ECG",
          "Hand-crafted": "Hand-crafted",
          "GNB": "KNN",
          "-": "0.53",
          "0.55": "-"
        },
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "GNB": "AdaBoost DT",
          "-": "0.56",
          "0.55": "-"
        },
        {
          "Miranda-Correa et al.\n[29]": "Yang et al.\n[56]",
          "2018": "2019",
          "ECG": "ECG, EDA, EEG",
          "Hand-crafted": "VAE",
          "GNB": "SVM",
          "-": "0.69",
          "0.55": "0.69"
        },
        {
          "Miranda-Correa et al.\n[29]": "Siddharth et al.\n[58]",
          "2018": "2019",
          "ECG": "ECG",
          "Hand-crafted": "CNN",
          "GNB": "ELM",
          "-": "0.83",
          "0.55": "0.76"
        },
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "GNB": "",
          "-": "0.81",
          "0.55": "0.74"
        },
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "ECG, EDA, EEG",
          "Hand-crafted": "",
          "GNB": "",
          "-": "0.83",
          "0.55": "0.76"
        },
        {
          "Miranda-Correa et al.\n[29]": "Santamaria et al.\n[48]",
          "2018": "2019",
          "ECG": "ECG",
          "Hand-crafted": "CNN",
          "GNB": "MLP",
          "-": "0.81",
          "0.55": "0.76"
        },
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "GNB": "",
          "-": "0.71",
          "0.55": "0.67"
        },
        {
          "Miranda-Correa et al.\n[29]": "Sarkar\n[52]",
          "2018": "2020",
          "ECG": "ECG",
          "Hand-crafted": "Fully Supervised\nCNN",
          "GNB": "MLP",
          "-": "0.84",
          "0.55": "0.83"
        },
        {
          "Miranda-Correa et al.\n[29]": "",
          "2018": "",
          "ECG": "",
          "Hand-crafted": "Self-Supervised\nCNN",
          "GNB": "",
          "-": "0.89",
          "0.55": "0.88"
        },
        {
          "Miranda-Correa et al.\n[29]": "Ours",
          "2018": "2020",
          "ECG": "ECG, EDA",
          "Hand-crafted": "Convolutional\nAutoencoder",
          "GNB": "RF",
          "-": "0.93",
          "0.55": "0.95"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Subramanian et al.\n[30]": "",
          "2016": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "NB": "",
          "-": "-",
          "0.59": "0.66"
        },
        {
          "Subramanian et al.\n[30]": "",
          "2016": "",
          "ECG": "ECG, EDA",
          "Hand-crafted": "",
          "NB": "",
          "-": "-",
          "0.59": "0.69"
        },
        {
          "Subramanian et al.\n[30]": "Gjoreski et al.\n[49]",
          "2016": "2017",
          "ECG": "ECG",
          "Hand-crafted": "Hand-crafted",
          "NB": "DNN",
          "-": "0.69",
          "0.59": "-"
        },
        {
          "Subramanian et al.\n[30]": "Gjoreski et al.\n[45]",
          "2016": "2018",
          "ECG": "ECG",
          "Hand-crafted": "Hand-crafted",
          "NB": "SVM",
          "-": "0.66",
          "0.59": "-"
        },
        {
          "Subramanian et al.\n[30]": "",
          "2016": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "NB": "",
          "-": "0.66",
          "0.59": "-"
        },
        {
          "Subramanian et al.\n[30]": "Sicheng et al.\n[50]",
          "2016": "2018",
          "ECG": "ECG",
          "Hand-crafted": "Hand-crafted",
          "NB": "Hypergraph Learning",
          "-": "0.72",
          "0.59": "-"
        },
        {
          "Subramanian et al.\n[30]": "",
          "2016": "",
          "ECG": "EDA",
          "Hand-crafted": "",
          "NB": "",
          "-": "0.75",
          "0.59": "-"
        },
        {
          "Subramanian et al.\n[30]": "Ours",
          "2016": "2020",
          "ECG": "ECG, EDA",
          "Hand-crafted": "Convolutional\nAutoencoder",
          "NB": "RF",
          "-": "0.79",
          "0.59": "0.86"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Soleymani et al.\n[31]": "Wiem et al.\n[46]",
          "2010": "2017",
          "ECG, EDA": "ECG",
          "Hand-crafted": "Hand-crafted",
          "SVM": "SVM",
          "0.46": "0.62",
          "-": "-"
        },
        {
          "Soleymani et al.\n[31]": "Gjoreski et al.\n[45]",
          "2010": "2018",
          "ECG, EDA": "ECG",
          "Hand-crafted": "Hand-crafted",
          "SVM": "SVM",
          "0.46": "0.62",
          "-": "-"
        },
        {
          "Soleymani et al.\n[31]": "",
          "2010": "",
          "ECG, EDA": "EDA",
          "Hand-crafted": "",
          "SVM": "NB",
          "0.46": "0.62",
          "-": "-"
        },
        {
          "Soleymani et al.\n[31]": "Siddharth et al.\n[58]",
          "2010": "2019",
          "ECG, EDA": "ECG",
          "Hand-crafted": "CNN",
          "SVM": "ELM",
          "0.46": "0.79",
          "-": "0.74"
        },
        {
          "Soleymani et al.\n[31]": "",
          "2010": "",
          "ECG, EDA": "EDA",
          "Hand-crafted": "",
          "SVM": "",
          "0.46": "0.82",
          "-": "0.75"
        },
        {
          "Soleymani et al.\n[31]": "",
          "2010": "",
          "ECG, EDA": "ECG, EDA, EEG",
          "Hand-crafted": "",
          "SVM": "",
          "0.46": "0.81",
          "-": "0.71"
        },
        {
          "Soleymani et al.\n[31]": "Ours",
          "2010": "2020",
          "ECG, EDA": "ECG, EDA",
          "Hand-crafted": "Convolutional\nAutoencoder",
          "SVM": "RF",
          "0.46": "0.83",
          "-": "0.76"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Toward machine emotional intelligence: analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "An approach to environmental psychology",
      "authors": [
        "A Mehrabian",
        "J Russell"
      ],
      "year": "1974",
      "venue": "An approach to environmental psychology"
    },
    {
      "citation_id": "4",
      "title": "Autonomic nervous system reactivity within the valence-arousal affective space: Modulation by sex and age",
      "authors": [
        "P Gomez",
        "A Von Gunten",
        "B Danuser"
      ],
      "year": "2016",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "5",
      "title": "A survey of affective computing for stress detection: Evaluating technologies in stress detection for better health",
      "authors": [
        "S Greene",
        "H Thapliyal",
        "A Caban-Holt"
      ],
      "year": "2016",
      "venue": "IEEE Consumer Electronics Magazine"
    },
    {
      "citation_id": "6",
      "title": "Wearable, yes, but able. . . ?: it is time for evidence-based marketing claims!",
      "authors": [
        "B Sperlich",
        "H.-C Holmberg"
      ],
      "year": "2017",
      "venue": "British Journal of Sports Medicine"
    },
    {
      "citation_id": "7",
      "title": "Affective wearables",
      "authors": [
        "R Picard",
        "J Healey"
      ],
      "year": "1997",
      "venue": "Digest of Papers. First International Symposium on Wearable Computers"
    },
    {
      "citation_id": "8",
      "title": "Wearables market to grow to $27 billion with 137 million units sold in 2022",
      "authors": [
        "C Russey"
      ],
      "year": "2018",
      "venue": "Wearables market to grow to $27 billion with 137 million units sold in 2022"
    },
    {
      "citation_id": "9",
      "title": "Smart wearables market to double by 2022: $27 billion industry forecast",
      "authors": [
        "P Lamkin"
      ],
      "year": "2018",
      "venue": "Smart wearables market to double by 2022: $27 billion industry forecast"
    },
    {
      "citation_id": "10",
      "title": "Electrocardiography recording, feature extraction and classification for emotion recognition",
      "authors": [
        "W Wan-Hui",
        "Q Yu-Hui",
        "L Guang-Yuan"
      ],
      "year": "2009",
      "venue": "WRI World congress on computer science and information engineering"
    },
    {
      "citation_id": "11",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "F Agrafioti",
        "D Hatzinakos",
        "A Anderson"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A wavelet-based approach to emotion classification using eda signals",
      "authors": [
        "H Feng",
        "H Golshan",
        "M Mahoor"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "13",
      "title": "Human emotion recognition using deep belief network architecture",
      "authors": [
        "M Hassan",
        "M Alam",
        "M Uddin",
        "S Huda",
        "A Almogren",
        "G Fortino"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "14",
      "title": "Robust eeg emotion classification using segment level decision fusion",
      "authors": [
        "V Rozgić",
        "S Vitaladevuni",
        "R Prasad"
      ],
      "year": "2013",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "15",
      "title": "Classification of human emotion from eeg using discrete wavelet transform",
      "authors": [
        "M Murugappan",
        "N Ramachandran",
        "Y Sazali"
      ],
      "year": "2010",
      "venue": "Journal of biomedical science and engineering"
    },
    {
      "citation_id": "16",
      "title": "Classification of emotional arousal during multimedia exposure",
      "authors": [
        "A Anderson",
        "T Hsiao",
        "V Metsis"
      ],
      "year": "2017",
      "venue": "Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments (PETRA)"
    },
    {
      "citation_id": "17",
      "title": "Inner emotion recognition using multi bio-signals",
      "authors": [
        "J Shin",
        "J Maeng",
        "D Kim"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Consumer Electronics -Asia"
    },
    {
      "citation_id": "18",
      "title": "Electromyogram signal based human emotion classification using knn and lda",
      "authors": [
        "M Murugappan"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on System Engineering and Technology"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition of emg based on improved lm bp neural network and svm",
      "authors": [
        "S Yang",
        "G Yang"
      ],
      "year": "2011",
      "venue": "JSW"
    },
    {
      "citation_id": "20",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamas"
      ],
      "year": "2016",
      "venue": "Medical Technologies National Congress"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods",
      "year": "2017",
      "venue": "Istanbul University-Journal of Electrical & Electronics Engineering"
    },
    {
      "citation_id": "23",
      "title": "Eeg-based emotion recognition using frequency domain features and support vector machines",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2011",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "24",
      "title": "Adaptive emotional information retrieval from eeg signals in the time-frequency domain",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Extraction and interpretation of deep autoencoderbased temporal features from wearables for forecasting personalized mood, health, and stress",
      "authors": [
        "B Li",
        "A Sano"
      ],
      "year": "2020",
      "venue": "Proceeding of ACM Interactive Mobile Wearable and Ubiquitous Technology"
    },
    {
      "citation_id": "26",
      "title": "Nonlinear principal component analysis using autoassociative neural networks",
      "authors": [
        "M Kramer"
      ],
      "year": "1991",
      "venue": "AIChE journal"
    },
    {
      "citation_id": "27",
      "title": "Affective reactions to acoustic stimuli",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "2000",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "28",
      "title": "Stacked convolutional auto-encoders for hierarchical feature extraction",
      "authors": [
        "J Masci",
        "U Meier",
        "D Cires ¸an",
        "J Schmidhuber"
      ],
      "year": "2011",
      "venue": "Artificial Neural Networks and Machine Learning -ICANN"
    },
    {
      "citation_id": "29",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Toward dynamically adaptive simulation: Multimodal classification of user expertise using wearable devices",
      "authors": [
        "K Ross",
        "P Sarkar",
        "D Rodenburg",
        "A Ruberto",
        "P Hungler",
        "A Szulewski",
        "D Howes",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "33",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "34",
      "title": "A description of the affective quality attributed to environments",
      "authors": [
        "J Russell",
        "G Pratt"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "35",
      "title": "A deep framework for facial emotion recognition using light field images",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "P Correia",
        "F Pereira"
      ],
      "year": "2019",
      "venue": "8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "36",
      "title": "Facial emotion recognition using light field images with deep attention-based bidirectional lstm",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Expert-driven perceptual features for modeling style and affect in human motion",
      "authors": [
        "S Etemad",
        "A Arya"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "38",
      "title": "Extracting movement, posture, and temporal style features from human motion",
      "year": "2014",
      "venue": "Biologically Inspired Cognitive Architectures"
    },
    {
      "citation_id": "39",
      "title": "Classical and novel discriminant features for affect recognition from speech",
      "authors": [
        "R Fernandez",
        "R Picard"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "40",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "arxiv": "arXiv:1912.07812"
    },
    {
      "citation_id": "42",
      "title": "Single trial classification of eeg and peripheral physiological signals for recognition of emotions induced by music videos",
      "authors": [
        "S Koelstra",
        "A Yazdani",
        "M Soleymani",
        "C Mühl",
        "J.-S Lee",
        "A Nijholt",
        "T Pun",
        "T Ebrahimi",
        "I Patras"
      ],
      "year": "2010",
      "venue": "International Conference on Brain Informatics"
    },
    {
      "citation_id": "43",
      "title": "Fusion of facial expressions and eeg for implicit affective tagging",
      "authors": [
        "S Koelstra",
        "I Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "44",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "45",
      "title": "An inter-domain study for arousal recognition from physiological signals",
      "authors": [
        "M Gjoreski",
        "B Mitrevski",
        "M Lustrek",
        "M Gams"
      ],
      "year": "2018",
      "venue": "Informatica (Slovenia)"
    },
    {
      "citation_id": "46",
      "title": "Emotion classification in arousal valence model using mahnob-hci database",
      "authors": [
        "M Wiem",
        "Z Lachiri"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "47",
      "title": "Arousal and valence recognition of affective sounds based on electrodermal activity",
      "authors": [
        "A Greco",
        "G Valenza",
        "L Citi",
        "E Scilingo"
      ],
      "year": "2017",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "48",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-González",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "49",
      "title": "Deep affect recognition from r-r intervals",
      "authors": [
        "M Gjoreski",
        "H Gjoreski",
        "M Luštrek",
        "M Gams"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp) and Proceedings of the ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "50",
      "title": "Personality-aware personalized emotion recognition from physiological signals",
      "authors": [
        "S Zhao",
        "G Ding",
        "J Han",
        "Y Gao"
      ],
      "year": "2018",
      "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "51",
      "title": "Classification of cognitive load and expertise for adaptive simulation using deep multitask learning",
      "authors": [
        "P Sarkar",
        "K Ross",
        "A Ruberto",
        "D Rodenbura",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "52",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition employing ecg and gsr signals as markers of ans",
      "authors": [
        "P Das",
        "A Khasnobish",
        "D Tibarewala"
      ],
      "year": "2016",
      "venue": "Conference on Advances in Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "An attribute-invariant variational learning for emotion recognition using physiology",
      "authors": [
        "H Yang",
        "C Lee"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition using multimodal deep learning",
      "authors": [
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "58",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing"
    },
    {
      "citation_id": "59",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "60",
      "title": "A real-time qrs detection algorithm",
      "authors": [
        "J Pan",
        "W Tompkins"
      ],
      "year": "1985",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "61",
      "title": "Performance comparison of ann classifiers for sleep apnea detection based on ecg signal analysis using hilbert transform",
      "authors": [
        "J Bali",
        "A Nandi",
        "P Hiremath"
      ],
      "year": "2018",
      "venue": "International Journal Of Computers and Technology"
    },
    {
      "citation_id": "62",
      "title": "A guide for analysing eda & skin conductance responses for psychological experiments",
      "authors": [
        "J Braithwaite",
        "D Watson",
        "R Jones",
        "M Rowe"
      ],
      "year": "2013",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "63",
      "title": "Ecg biometric recognition without fiducial detection",
      "authors": [
        "K Plataniotis",
        "D Hatzinakos",
        "J Lee"
      ],
      "year": "2006",
      "venue": "Ecg biometric recognition without fiducial detection"
    },
    {
      "citation_id": "64",
      "title": "A stacked autoencoder-based deep neural network for achieving gearbox fault diagnosis",
      "authors": [
        "G Liu",
        "H Bao",
        "B Han"
      ],
      "year": "2018",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "65",
      "title": "Sparse autoencoder",
      "authors": [
        "A Ng"
      ],
      "year": "2011",
      "venue": "CS294A Lecture notes"
    },
    {
      "citation_id": "66",
      "title": "Feature selection, l1 vs. l2 regularization, and rotational invariance",
      "authors": [
        "A Ng"
      ],
      "year": "2004",
      "venue": "Proceedings of the Twenty-First International Conference on Machine Learning, ser. ICML"
    },
    {
      "citation_id": "67",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "citation_id": "68",
      "title": "Spectralspatial classification of hyperspectral imagery based on stacked sparse autoencoder and random forest",
      "authors": [
        "C Zhao",
        "X Wan",
        "G Zhao",
        "B Cui",
        "W Liu",
        "B Qi"
      ],
      "year": "2017",
      "venue": "European journal of remote sensing"
    },
    {
      "citation_id": "69",
      "title": "Computer-aided diagnosis of clinically significant prostate cancer from mri images using sparse autoencoder and random forest classifier",
      "authors": [
        "B Abraham",
        "M Nair"
      ],
      "year": "2018",
      "venue": "Biocybernetics and Biomedical Engineering"
    },
    {
      "citation_id": "70",
      "title": "High-voltage circuit breaker fault diagnosis using a hybrid feature transformation approach based on random forest and stacked autoencoder",
      "authors": [
        "S Ma",
        "M Chen",
        "J Wu",
        "Y Wang",
        "B Jia",
        "Y Jiang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Industrial Electronics"
    },
    {
      "citation_id": "71",
      "title": "A short-term load forecasting scheme based on auto-encoder and random forest",
      "authors": [
        "M Son",
        "J Moon",
        "S Jung",
        "E Hwang"
      ],
      "year": "2018",
      "venue": "International Conference on Applied Physics, System Science and Computers"
    },
    {
      "citation_id": "72",
      "title": "The decision tree classifier: Design and potential",
      "authors": [
        "P Swain",
        "H Hauska"
      ],
      "year": "1977",
      "venue": "IEEE Transactions on Geoscience Electronics"
    },
    {
      "citation_id": "73",
      "title": "The big five personality factors and personal values",
      "authors": [
        "S Roccas",
        "L Sagiv",
        "S Schwartz",
        "A Knafo"
      ],
      "year": "2002",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "74",
      "title": "Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "75",
      "title": "Khsc kingston health sciences centre",
      "year": "2020",
      "venue": "Khsc kingston health sciences centre"
    },
    {
      "citation_id": "76",
      "title": "Laerdal Medical",
      "year": "2019",
      "venue": "Laerdal Medical"
    },
    {
      "citation_id": "77",
      "title": "Consensys ecg",
      "authors": [
        "Consensys"
      ],
      "venue": "Consensys ecg"
    },
    {
      "citation_id": "78",
      "title": "Microsoft hololens -mixed reality technology for business",
      "authors": [
        "Microsoft"
      ],
      "year": "2019",
      "venue": "Microsoft hololens -mixed reality technology for business"
    },
    {
      "citation_id": "79",
      "title": "Scikit-learn: Machine learning in python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg"
      ],
      "year": "2011",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "80",
      "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "authors": [
        "M Abadi",
        "A Agarwal",
        "P Barham",
        "E Brevdo",
        "Z Chen",
        "C Citro",
        "G Corrado",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "I Goodfellow",
        "A Harp",
        "G Irving",
        "M Isard",
        "Y Jia",
        "R Jozefowicz",
        "L Kaiser",
        "M Kudlur",
        "J Levenberg",
        "D Mané",
        "R Monga",
        "S Moore",
        "D Murray",
        "C Olah",
        "M Schuster",
        "J Shlens",
        "B Steiner",
        "I Sutskever",
        "K Talwar",
        "P Tucker",
        "V Vanhoucke",
        "V Vasudevan",
        "F Viégas",
        "O Vinyals",
        "P Warden",
        "M Wattenberg",
        "M Wicke",
        "Y Yu",
        "X Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-scale machine learning on heterogeneous systems"
    },
    {
      "citation_id": "81",
      "title": "Keras",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "82",
      "title": "Heart rate variability. standards of measurement, physiological interpretation, and clinical use: Task force of the european society of cardiology and the north american society for pacing and electrophysiology",
      "authors": [
        "M Malik"
      ],
      "year": "1996",
      "venue": "Annals of Noninvasive Electrocardiology"
    },
    {
      "citation_id": "83",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "84",
      "title": "Least-squares frequency analysis of unequally spaced data",
      "authors": [
        "N Lomb"
      ],
      "year": "1976",
      "venue": "Astrophysics and space science"
    },
    {
      "citation_id": "85",
      "title": "Regression shrinkage and selection via the lasso",
      "authors": [
        "R Tibshirani"
      ],
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)"
    },
    {
      "citation_id": "86",
      "title": "Data classification: Algorithms and applications",
      "authors": [
        "J Tang",
        "S Alelyani",
        "H Liu"
      ],
      "year": "2014",
      "venue": "Data classification: Algorithms and applications"
    },
    {
      "citation_id": "87",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}