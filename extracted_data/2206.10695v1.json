{
  "paper_id": "2206.10695v1",
  "title": "Exploring The Effectiveness Of Self-Supervised Learning And Classifier Chains In Emotion Recognition Of Nonverbal Vocalizations",
  "published": "2022-06-21T19:29:54Z",
  "authors": [
    "Detai Xin",
    "Shinnosuke Takamichi",
    "Hiroshi Saruwatari"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present an emotion recognition system for nonverbal vocalizations (NVs) submitted to the ExVo Few-Shot track of the ICML Expressive Vocalizations Competition 2022. The proposed method uses self-supervised learning (SSL) models to extract features from NVs and uses a classifier chain to model the label dependency between emotions. Experimental results demonstrate that the proposed method can significantly improve the performance of this task compared to several baseline methods. Our proposed method obtained a mean concordance correlation coefficient (CCC) of 0.725 in the validation set and 0.739 in the test set, while the best baseline method only obtained 0.554 in the validation set. We publicate our code at https://github. com/Aria-K-Alethia/ExVo to help others to reproduce our experimental results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Nonverbal vocalizations (NVs), also called affect bursts, refer to short and expressive vocalizations containing no linguistic information like laughter, sobs, and screams  (Scherer, 1994; Trouvain & Truong, 2012) . NVs are important for spoken language processing, since (1) NVs play an important role in expressing emotions in spoken languages  (Hall et al., 2009; Scherer & Scherer, 2011) , and (2) they are common components of human communication existing in different cultures and languages  (Sauter et al., 2010) . Although both emotional prosody and NVs contribute to emotional expressions in speech, NVs are ignored by most previous research on speech emotions  (Lima et al., 2013) , which necessitates further work in this field.\n\nThe ICML Expressive Vocalizations Competition (hereafter ExVo) launched in 2022 aims to develop technologies for the recognition, generation, and personalization of NVs  (Baird et al., 2022) . The competition includes three tracks: ExVo Multi-Task, ExVo Generate, and ExVo Few-Shot, which correspond to the three goals of recognizing, generating, and personalizing NVs, respectively. Specifically, the ExVo Multi-Task track aims to recognize not only emotions but also demographic information like the age and native country of the speakers from NVs. In the ExVo Generate track, the participants need to generate NVs of various emotions. Similar to the Multi-Task track, the ExVo Few-Shot track also aims to recognize emotions from NVs, but further requires the prediction systems to adapt to new speakers which are not in the training set. All tracks are evaluated by appropriate subjective and objective metrics to reflect the performance of the proposed systems.\n\nIn this paper, we describe our emotion recognition system for NVs submitted to the ExVo Few-Shot track. Our system consists of two components: a feature extractor and a classifier chain (CC). As the feature extractor, we use a selfsupervised learning (SSL) model like Wav2vec2  (Schneider et al., 2019; Baevski et al., 2020)  or HuBERT  (Hsu et al., 2021a) . The extracted feature is then fed to a classifier chain, which predicts the score for each emotion sequentially. The proposed classifier chain predicts the score by conditioning on both the extracted feature and the predicted scores in previous steps, so can utilize information from label dependency. We conducted comprehensive experiments to verify the effectiveness of these two components, and show that the proposed method can significantly improve the performance compared to several baseline methods. Our contributions can be summarized as follows:\n\n• We propose an emotion recognition system for NVs using SSL models and CC, and conduct experiments to show the effectiveness of the proposed method. • We conduct experiments to show the best SSL models for this task.\n\n• We analyze the prediction results and give insights for future research.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Exvo Few-Shot Track",
      "text": "The task of the ExVo Few-Shot track is recognizing emotions from NVs for unseen speakers that are not in the training set. During the training phase, the participants can train their emotion recognition models on the data of the seen speakers. In the test phase, two samples for each unseen -speaker will be provided by the organizer, and the participants can then adapt their models to unseen speakers by few-shot learning.\n\nExVo provides a large-scale multilingual (Chinese, English, Spanish) NVs dataset  (Cowen et al., 2022) , which contains approximately 36 hours NVs uttered by 1, 702 speakers from the USA, China, South Africa, and Venezuela. Ten emotions consisting of amusement, awe, awkwardness, distress, excitement, fear, horror, sadness, surprise, and triumph are covered by the dataset. Each NV is annotated with ten emotion scores ranging from 0 to 1 by crowdsourcing. Some statistics of the dataset are summarized in Table  1 . The participants should submit the predictions of their models on test samples. Concordance correlation coefficient (CCC) is used to evaluate the performance, which can measure the agreement between the ground-truth (GT) and predicted emotion scores.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we describe the proposed method. We first introduce the general architecture of the proposed method and then describe each component separately.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Architecture",
      "text": "The architecture of the proposed method is illustrated in Figure  1 . Specifically, the NVs are first fed to the SSL model to extract sequential features from them. Then, an attentive pooling module is used to collect information over the time axis and convert the sequential features into fixed-length features. Finally, a CC is used to predict the score for each emotion sequentially. Each emotion has a separate predictor, which contains a fully connected (FC) layer followed by a sigmoid activation. The predictor in the CC not only uses the extracted features but also the predicted scores of previous classifiers as input, which can thus utilize information of label dependency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Learning Models",
      "text": "SSL is a popular and powerful method to leverage largescale unlabeled data. The idea of SSL is to set a sophisticated pretext task to learn nontrivial data representations.\n\nRecently, several works based on SSL have been proposed in speech processing, which showed promising results in speech-to-text and other speech-based tasks  (Schneider et al., 2019; Baevski et al., 2020; Conneau et al., 2020    2022) , since most of the SSL models were trained on speech corpora that rarely contain NVs, whether these models are effective on NVs remains unknown. Therefore, in the experiments, we select several SSL models and verify their effectiveness in the task of recognizing emotions from NVs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Classifier Chain",
      "text": "The task of the ExVo Few-Shot track is a multivariate regression problem. While similar tasks like continuous speech emotion recognition usually use separate predictors to predict the emotions independently  (Atmaja & Akagi, 2021) , it is noteworthy that the emotion labels in the provided dataset have an intrinsic dependency. For example, if the score of amusement is high, the scores of negative emotions like sadness and fear may be low, which implies the possibility to utilize information from label dependency. To this end, we propose to use CC  (Read et al., 2011) , which has been proved to be useful for modeling label dependency in multilabel classification  (Dembszynski et al., 2010; Dembczynski et al., 2010; Nam et al., 2017) .\n\nFormally, denoting the predictor of the i-th emotion and the extracted feature as f i (•) and z, respectively, the emotion score y i is computed by:\n\n, where σ(•) is the sigmoid function, ⊕ is the vector concatenation operator, and y <i indicates a vector concatenating previously predicted emotion scores. During training, we feed GT emotion scores y <i to the predictor to prevent error propagation.\n\nWhile it is possible to use powerful sequential models like a recurrent neural network to implement the predictor f i in CC  (Nam et al., 2017) , in the preliminary experiments we found simple linear CC was more stable than other models,   (Read et al., 2021) . In our implementation, we adopt a heuristic method in which we first train ten separate base predictors for each emotion, then use the descending order of the performance of the base predictors as the chain order. This method can intuitively alleviate the error propagation problem by first predicting emotions with high confidence, hence can improve the performance  (Read et al., 2021) .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Loss Function",
      "text": "We use CCC as the objective for the model training. Previous work has demonstrated that CCC was better for speech emotion recognition than error-based loss functions like L1 loss  (Atmaja & Akagi, 2021) . Generally, given a set of paired data ({s j } n j=1 , {t j } n j=1 ) with length n, the CCC between them is defined as: CCC({s j } n j=1 , {t j } n j=1 ) = 2σ 2 st σ 2 s +σ 2 t +(µs-µt) 2 . In the proposed method the objective value of a mini-batch is computed by averaging CCC values of all emotions. Formally, denoting the number of emotions and the mini-batch size as C and B, respectively, the loss function of the proposed model is defined as:\n\n, where y j i and y j i are GT and predicted scores of the i-th emotion of the j-th sample in the mini-batch, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation",
      "text": "To improve the robustness of the model, we additionally use data augmentation. Specifically, we use two strategies: pitch-shifting and speaking-rate-changing  (Saeki et al., 2022) . Pitch-shifting raises or lowers the pitch of NVs, and can change the speaker identity of NVs. Speaking-ratechanging slows down or speeds up the NVs. We tune the shifting range and the speaking-rate-changing range so that the emotion of the augmented NVs has little difference from the original ones.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We constructed several baseline systems based on the features provided by the organizer  (Baird et al., 2022) . These   (Schmitt & Schuller, 2017)  from the low-level descriptors of ComParE set, the 4096-dimensional spectrum representations extracted by the DeepSpectrum toolkit  (Amiriparian et al., 2017) . We used multi-layer neural networks with LeakyReLU activation  (Maas et al., 2013)  to process each of the features. Batch normalization  (Ioffe & Szegedy, 2015)  was used in each layer to normalize the features.\n\nTo verify the effectiveness of SSL models and find the best model for NVs, we selected six SSL models: Wav2vec2-{base, large}  (Baevski et al., 2020) , HuBERT-{base, large}  (Hsu et al., 2021a) , Wav2vec2-robust  (Hsu et al., 2021b) , and XLSR  (Conneau et al., 2020) . Wav2vec2-{base, large} and HuBERT-{base, large} are common models used in previous work  (Chen & Rudnicky, 2021; Wang et al., 2021) . The Wav2vec2-{base, large} and HuBERT-large were trained on the Librispeech  (Panayotov et al., 2015)  corpus containing 960 hours of audio, while the HuBERT-large model was trained on the Libri-Light  (Kahn et al., 2020)  corpus containing 60k hours of audio. These corpora are all from clean audiobooks that rarely contain NVs. Also, since the NVs in the dataset were recorded by the speakers themselves and sometimes contain noises, we selected Wav2vec2-robust that was trained on not only the Libri-Light corpus but also noisy telephone corpora including Switchboard  (Godfrey & Holliman, 1997) , Fisher  (Cieri et al., 2004) , and CommonVoice  (Ardila et al., 2019) . Finally, to handle multilingual NVs we select XLSR, which is a multilingual version of Wav2vec2 trained on 56k hours of multilingual audio (multilingual Librispeech  (Pratap et al., 2020) , CommonVoice, and Babel  (Gales et al., 2014) ) covering 53 languages.\n\nInstead of using the original train-validation split, we first combined the train and validation sets and then used 5-fold cross-validation to train all models. To ensure the validation results can reflect the generalization ability of the models, we split the data by speaker so that the speakers of the validation set of each split are unseen for the training set. Each split had about 912 seen speakers and 227 unseen speakers. All silence in the audios was trimmed by voice activity detection. The chain order was set to awe, surprise, amusement, fear, horror, sadness, distress, excitement, triumph, and awkwardness. We used Adam  (Kingma & Ba, 2015)  to optimize the model, the learning rate was set to 1e-4 for the CC and 1e-5 for the feature extractor. The learning rate was halved once the loss value on the validation set did not decrease. The batch size was set to 16 when Wav2vec-base or HuBERT-base was used, otherwise, it was set to 8. We used WavAugment  (Kharitonov et al., 2020)  to implement the data augmentation, the pitch-shifting range was set to  [-300, 300]  cents, and the speaking-rate-changing range was set to [0.8, 1.2]. The minimal and maximal number of epochs were set to 10 and 50, respectively. Early stopping was used to prevent overfitting with a patience of 10 epoch. We selected the model that had the best CCC on the validation set. To adapt the models to unseen speakers, we further fine-tuned the models on the 2 samples of each test speaker, which roughly took 10 epochs for convergence. The learning rate was set to 1e-6 for the whole network.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Evaluations Of Ssl Models",
      "text": "We first evaluate all SSL models to verify their effectiveness and find the best model for NVs. To avoid the the influence on performance from the CC, in this experiment the CC is replaced by a single FC layer. The results are shown in Table 2. It can be seen that the XLSR model obtained the best performance. We suppose this is because the multilingual XLSR model can handle more phonetic tokens of different languages, unlike other SSL models that are only trained on English corpora. Also, large models are always better than base models, which is consistent with the results of previous work  (Wagner et al., 2022) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluations Of Cc And Data Augmentation",
      "text": "We select the top-2 SSL models (XLSR and HuBERT-large) in the previous experiment as the feature extractors to evaluate CC and data augmentation (\"Aug.\"). The results are shown in Table  3 . It can be seen that CC consistently improves the performance compared to the results in Table  2  The data augmentation brings improvements for the XLSR model but failed to improve the performance of the CC HuBERT-large model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluations For Each Emotion",
      "text": "We then evaluate the proposed method for each emotion.\n\nWe selected the best performing model CC XLSR Aug. obtained in previous experiments. All baseline models were trained using cross-validation for comparison. After training, we combined the inference results on all 5 splits of each model and computed the CCC value for each emotion.\n\nThe result is demonstrated in Table  4 . It can be seen that the proposed method significantly outperformed all baseline methods on all emotions, which indicates the effectiveness of SSL models and CC. Besides, it can be observed that all models have difficulties to recognize some emotions like awkwardness, triumph, distress.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluations On Test Set",
      "text": "We finally fine-tuned the best model (CC XLSR Aug.) on the test samples provided by the organizer. Our best CCC on the test set is 0.739, which demonstrates the proposed method has a strong generalization ability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "This paper described an emotion recognition system for NVs submitted to the Few-Shot track of the ICML ExVo competition. The proposed method uses a SSL model to extract contextual speech representations from NVs, and uses a CC to predict emotion scores by utilizing the features and the emotion scores predicted in previous steps together.\n\nExperimental results demonstrated that the proposed method significantly outperformed several baseline methods.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Speciﬁcally, the NVs are ﬁrst fed to the SSL model",
      "page": 2
    },
    {
      "caption": "Figure 1: Architecture of the proposed method. FC denotes a fully",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Multi-Task, ExVo Generate, and ExVo Few-Shot, which\nAbstract"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "correspond to the three goals of recognizing, generating,"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "We present an emotion recognition system for"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "and personalizing NVs, respectively. Speciﬁcally, the ExVo"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "nonverbal vocalizations (NVs) submitted to the"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Multi-Task track aims to recognize not only emotions but"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "ExVo Few-Shot\ntrack of\nthe ICML Expressive"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "also demographic information like the age and native coun-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Vocalizations Competition 2022. The proposed"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "try of the speakers from NVs. In the ExVo Generate track,"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "method uses self-supervised learning (SSL) mod-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "the participants need to generate NVs of various emotions."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "els to extract features from NVs and uses a clas-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Similar to the Multi-Task track, the ExVo Few-Shot track"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "siﬁer chain to model\nthe label dependency be-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "also aims\nto recognize emotions\nfrom NVs, but\nfurther"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "tween emotions.\nExperimental\nresults demon-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "requires the prediction systems to adapt\nto new speakers"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "strate that the proposed method can signiﬁcantly"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "which are not in the training set. All tracks are evaluated by"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "improve the performance of this task compared to"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "appropriate subjective and objective metrics to reﬂect the"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "several baseline methods. Our proposed method"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "performance of the proposed systems."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "obtained a mean concordance correlation coefﬁ-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "In this paper, we describe our emotion recognition system\ncient\n(CCC) of 0.725 in the validation set and"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "for NVs submitted to the ExVo Few-Shot\ntrack. Our sys-\n0.739 in the\ntest\nset, while\nthe best baseline"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "tem consists of two components: a feature extractor and a\nmethod only obtained 0.554 in the validation set."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "classiﬁer chain (CC). As the feature extractor, we use a self-\nWe publicate our code at https://github."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "supervised learning (SSL) model like Wav2vec2 (Schneider\ncom/Aria-K-Alethia/ExVo to help others"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "et al., 2019; Baevski et al., 2020) or HuBERT (Hsu et al.,\nto reproduce our experimental results."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "2021a). The extracted feature is then fed to a classiﬁer chain,"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "1. Introduction"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "which predicts the score for each emotion sequentially. The"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Nonverbal vocalizations (NVs), also called affect bursts,\nproposed classiﬁer chain predicts the score by conditioning"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "refer to short and expressive vocalizations containing no lin-\non both the extracted feature and the predicted scores in"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "guistic information like laughter, sobs, and screams (Scherer,\nprevious steps, so can utilize information from label depen-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "1994; Trouvain & Truong, 2012). NVs are important for spo-\ndency. We conducted comprehensive experiments to verify"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "ken language processing, since (1) NVs play an important\nthe effectiveness of these two components, and show that the"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "role in expressing emotions in spoken languages (Hall et al.,\nproposed method can signiﬁcantly improve the performance"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "2009; Scherer & Scherer, 2011), and (2) they are common\ncompared to several baseline methods. Our contributions"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "components of human communication existing in different\ncan be summarized as follows:"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "cultures and languages (Sauter et al., 2010). Although both"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "• We propose an emotion recognition system for NVs"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "emotional prosody and NVs contribute to emotional expres-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "using SSL models and CC, and conduct experiments"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "sions in speech, NVs are ignored by most previous research"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "to show the effectiveness of the proposed method."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "on speech emotions (Lima et al., 2013), which necessitates"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "• We conduct experiments to show the best SSL models"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "further work in this ﬁeld."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "for this task."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "The ICML Expressive Vocalizations Competition (hereafter\n• We analyze the prediction results and give insights for"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "ExVo) launched in 2022 aims to develop technologies for the\nfuture research."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "recognition, generation, and personalization of NVs (Baird"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "2. The ExVo Few-Shot track"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "et al., 2022). The competition includes three tracks: ExVo"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "The task of the ExVo Few-Shot track is recognizing emo-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "1The University of Tokyo, Tokyo, Japan. Correspondence to:"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "tions from NVs for unseen speakers that are not in the train-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Detai Xin <detai xin@ipc.i.u-tokyo.ac.jp>."
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "ing set. During the training phase, the participants can train"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "Proceedings of\nthe ICML Expressive Vocalizations Workshop held in conjunction"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "their emotion recognition models on the data of the seen"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "with the 39 th International Conference on Machine Learning, Baltimore, Mary-"
        },
        {
          "Detai Xin 1\nShinnosuke Takamichi 1 Hiroshi Saruwatari 1": "land, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\nspeakers.\nIn the test phase,\ntwo samples for each unseen"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "ExVo provides a large-scale multilingual (Chinese, English,"
        },
        {
          "few-shot learning.": "Spanish) NVs dataset (Cowen et al., 2022), which contains"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "approximately 36 hours NVs uttered by 1, 702 speakers"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "from the USA, China, South Africa, and Venezuela. Ten"
        },
        {
          "few-shot learning.": "emotions consisting of amusement, awe, awkwardness, dis-"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "tress, excitement,\nfear, horror, sadness, surprise, and tri-"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "umph are covered by the dataset. Each NV is annotated"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "with ten emotion scores ranging from 0 to 1 by crowd-"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "sourcing. Some statistics of the dataset are summarized in"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "Table 1. The participants should submit the predictions of"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "their models on test samples. Concordance correlation co-"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "efﬁcient (CCC) is used to evaluate the performance, which"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "can measure the agreement between the ground-truth (GT)"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "and predicted emotion scores."
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "3. Proposed method"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "In this section, we describe the proposed method. We ﬁrst"
        },
        {
          "few-shot learning.": "introduce the general architecture of the proposed method"
        },
        {
          "few-shot learning.": "and then describe each component separately."
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "3.1. Architecture"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "The architecture of the proposed method is illustrated in"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "Figure 1. Speciﬁcally, the NVs are ﬁrst fed to the SSL model"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "to extract sequential features from them. Then, an attentive"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "pooling module is used to collect information over the time"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "axis and convert\nthe sequential features into ﬁxed-length"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "features. Finally, a CC is used to predict the score for each"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "emotion sequentially. Each emotion has a separate predictor,"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "which contains a fully connected (FC) layer followed by a"
        },
        {
          "few-shot learning.": "sigmoid activation. The predictor in the CC not only uses the"
        },
        {
          "few-shot learning.": "extracted features but also the predicted scores of previous"
        },
        {
          "few-shot learning.": "classiﬁers as input, which can thus utilize information of"
        },
        {
          "few-shot learning.": "label dependency."
        },
        {
          "few-shot learning.": "3.2. Self-supervised learning models"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "SSL is a popular and powerful method to leverage large-"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "scale unlabeled data. The idea of SSL is to set a sophisti-"
        },
        {
          "few-shot learning.": ""
        },
        {
          "few-shot learning.": "cated pretext\ntask to learn nontrivial data representations."
        },
        {
          "few-shot learning.": "Recently, several works based on SSL have been proposed"
        },
        {
          "few-shot learning.": "in speech processing, which showed promising results in"
        },
        {
          "few-shot learning.": "speech-to-text\nand other\nspeech-based tasks\n(Schneider"
        },
        {
          "few-shot learning.": "et al., 2019; Baevski et al., 2020; Conneau et al., 2020; Hsu"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "validation. Bold indicates the best score.",
          "Table 3. Average CCC of the proposed method using CC and data": "augmentation in 5-fold cross-validation. Bold indicates the best"
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "",
          "Table 3. Average CCC of the proposed method using CC and data": "score."
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "SSL Model\nCCC",
          "Table 3. Average CCC of the proposed method using CC and data": ""
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "",
          "Table 3. Average CCC of the proposed method using CC and data": "Model\nCCC"
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "0.699 ± 0.013\nWav2vec2-base",
          "Table 3. Average CCC of the proposed method using CC and data": ""
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "0.703 ± 0.013\nHuBERT-base",
          "Table 3. Average CCC of the proposed method using CC and data": "0.722 ± 0.012\nCC HuBERT-large"
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "0.709 ± 0.012\nWav2vec2-robust",
          "Table 3. Average CCC of the proposed method using CC and data": "0.722 ± 0.012\nCC HuBERT-large Aug."
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "0.709 ± 0.009\nWav2vec2-large",
          "Table 3. Average CCC of the proposed method using CC and data": "0.724 ± 0.012\nCC XLSR"
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "0.718 ± 0.011\nHuBERT-large",
          "Table 3. Average CCC of the proposed method using CC and data": "0.726 ± 0.013\nCC XLSR Aug."
        },
        {
          "Table 2. Average CCC of different SSL models in 5-fold cross-": "0.722 ± 0.012\nXLSR",
          "Table 3. Average CCC of the proposed method using CC and data": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "< 0.05."
        },
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "Model"
        },
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "DeepSpectrum"
        },
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "eGeMAPS"
        },
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "BoAW"
        },
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "ComParE"
        },
        {
          "Table 4. CCC of the proposed and baseline methods for each emotion on all validation data. Bold indicates the best score with p-value": "CC XLSR Aug."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "results can reﬂect the generalization ability of the models,",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "shown in Table 3.\nIt can be seen that CC consistently im-"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "we split the data by speaker so that the speakers of the vali-",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "proves the performance compared to the results in Table 2"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "dation set of each split are unseen for the training set. Each",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "The data augmentation brings improvements for the XLSR"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "split had about 912 seen speakers and 227 unseen speak-",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "model but\nfailed to improve the performance of\nthe CC"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "ers. All silence in the audios was trimmed by voice activity",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "HuBERT-large model."
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "detection. The chain order was set to awe, surprise, amuse-",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "ment, fear, horror, sadness, distress, excitement,\ntriumph,",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "4.4. Evaluations for each emotion"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "and awkwardness. We used Adam (Kingma & Ba, 2015) to",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "We then evaluate the proposed method for each emotion."
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "optimize the model,\nthe learning rate was set\nto 1e−4 for",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "We selected the best performing model CC XLSR Aug. ob-"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "the CC and 1e−5 for the feature extractor. The learning rate",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "tained in previous experiments. All baseline models were"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "was halved once the loss value on the validation set did not",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "trained using cross-validation for comparison. After train-"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "decrease. The batch size was set to 16 when Wav2vec-base",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "ing, we combined the inference results on all 5 splits of"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "or HuBERT-base was used, otherwise, it was set to 8. We",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "each model and computed the CCC value for each emotion."
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "used WavAugment (Kharitonov et al., 2020) to implement",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "The result\nis demonstrated in Table 4.\nIt can be seen that"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "the data augmentation,\nthe pitch-shifting range was set to",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "the proposed method signiﬁcantly outperformed all baseline"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "[−300, 300] cents, and the speaking-rate-changing range",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "methods on all emotions, which indicates the effectiveness"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "was set\nto [0.8, 1.2]. The minimal and maximal number",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "of SSL models and CC. Besides, it can be observed that all"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "of epochs were set\nto 10 and 50, respectively. Early stop-",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "models have difﬁculties to recognize some emotions like"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "ping was used to prevent overﬁtting with a patience of 10",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "awkwardness, triumph, distress."
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "epoch. We selected the model\nthat had the best CCC on",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "the validation set. To adapt the models to unseen speakers,",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "4.5. Evaluations on test set"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "we further ﬁne-tuned the models on the 2 samples of each",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "We ﬁnally ﬁne-tuned the best model (CC XLSR Aug.) on"
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "test speaker, which roughly took 10 epochs for convergence.",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": ""
        },
        {
          "0.801\n0.687\n0.796\n0.608\n0.755\nCC XLSR Aug.": "",
          "0.735\n0.685\n0.674\n0.703\n0.804\n0.725": "the test samples provided by the organizer. Our best CCC"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "References"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Amiriparian, S., Gerczuk, M., Ottl, S., Cummins, N., Fre-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "itag, M., Pugachevskiy, S., Baird, A., and Schuller, B."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Snore sound classiﬁcation using image-based deep spec-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "trum features. Proc. Interspeech 2017, pp. 3512–3516,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "2017."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "and Weber, G. Common voice: A massively-multilingual"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "speech corpus. arXiv preprint arXiv:1912.06670, 2019."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Atmaja, B. T. and Akagi, M.\nEvaluation of error-and"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "correlation-based loss functions for multitask learning"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "dimensional speech emotion recognition.\nIn Journal of"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Physics: Conference Series, volume 1896, pp. 012004."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "IOP Publishing, 2021."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "2.0: A framework for self-supervised learning of speech"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "representations. Advances in Neural Information Process-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "ing Systems, 33:12449–12460, 2020."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Baird, A., Tzirakis, P., Gidel, G., Jiralerspong, M., Muller,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "E. B., Mathewson, K., Schuller, B., Cambria, E., Keltner,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "D., and Cowen, A.\nThe icml 2022 expressive vocal-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "izations workshop and competition: Recognizing, gen-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "erating, and personalizing vocal bursts. arXiv preprint"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "arXiv:2205.01780, 2022."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Chen, L.-W. and Rudnicky, A. Exploring wav2vec 2.0 ﬁne-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "tuning for improved speech emotion recognition. arXiv"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "preprint arXiv:2110.06309, 2021."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Cieri, C., Miller, D., and Walker, K. The ﬁsher corpus: A"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "resource for the next generations of speech-to-text.\nIn"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "LREC, volume 4, pp. 69–71, 2004."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V.,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "´"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Wenzek, G., Guzm´an, F., Grave,\nE., Ott, M., Zettlemoyer,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "L., and Stoyanov, V. Unsupervised cross-lingual repre-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "the 58th\nsentation learning at scale.\nIn Proceedings of"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Annual Meeting of\nthe Association for Computational"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Linguistics, pp. 8440–8451, 2020."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Cowen, A., Baird, A., Tzirakis, P., Opara, M., Kim, L.,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Brooks, J., and Metrick, J. The hume vocal burst competi-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "tion dataset (h-vb) — raw data [exvo: updated 02.28.22]"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "[data set]. Zenodo, 2022. doi: https://doi.org/10.5281/"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "zenodo.6308780."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Dembczynski, K., Cheng, W., and H¨ullermeier, E. Bayes op-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "timal multilabel classiﬁcation via probabilistic classiﬁer"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "chains.\nIn ICML, 2010."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "(ICASSP), pp. 7669–7673, 2020. https://github."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "com/facebookresearch/libri-light."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Kharitonov, E., Rivi`ere, M., Synnaeve, G., Wolf, L., Mazar´e,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "P.-E., Douze, M., and Dupoux, E.\nData augmenting"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "contrastive learning of speech representations in the time"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "domain. arXiv preprint arXiv:2007.00991, 2020."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Kingma, D. P. and Ba, J. Adam: A method for stochastic"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "optimization. In Proc. ICLR, San Diego, USA, May 2015."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Lima, C. F., Castro, S. L., and Scott, S. K. When voices"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "get emotional: A corpus of nonverbal vocalizations for"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "research on emotion processing. Behavior research meth-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "ods, 45(4):1234–1245, 2013."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Maas, A. L., Hannun, A. Y., and Ng, A. Y. Rectiﬁer non-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "linearities improve neural network acoustic models.\nIn"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "ICML Workshop on Deep Learning for Audio, Speech"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "and Language Processing, 2013."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Nam, J., Loza Menc´ıa, E., Kim, H. J., and F¨urnkranz, J."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Maximizing subset accuracy with recurrent neural net-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "works in multi-label classiﬁcation. Advances in neural"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "information processing systems, 30, 2017."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Panayotov, V., Chen, G., Povey, D., and Khudanpur, S."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Librispeech: an asr corpus based on public domain au-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "dio books.\nIn 2015 IEEE international conference on"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "acoustics, speech and signal processing (ICASSP), pp."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "5206–5210. IEEE, 2015."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Pepino, L., Riera, P.,\nand Ferrer, L.\nEmotion recogni-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "tion from speech using wav2vec 2.0 embeddings. arXiv"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "preprint arXiv:2104.03502, 2021."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": ""
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "R. Mls: A large-scale multilingual dataset for speech"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "research. arXiv preprint arXiv:2012.03411, 2020."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Read, J., Pfahringer, B., Holmes, G., and Frank, E. Classiﬁer"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "chains for multi-label classiﬁcation. Machine learning,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "85(3):333–359, 2011."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Read, J., Pfahringer, B., Holmes, G., and Frank, E. Classiﬁer"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "chains: a review and perspectives. Journal of Artiﬁcial"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Intelligence Research, 70:683–718, 2021."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Saeki, T., Xin, D., Nakata, W., Koriyama, T., Takamichi,"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "S.,\nand Saruwatari, H.\nUtmos: Utokyo-sarulab sys-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "arXiv\npreprint\ntem for\nvoicemos\nchallenge\n2022."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "arXiv:2204.02152, 2022."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Sauter, D. A., Eisner, F., Ekman, P., and Scott, S. K. Cross-"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "cultural recognition of basic emotions through nonverbal"
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Proceedings of\nthe National\nemotional vocalizations."
        },
        {
          "Exploring the Effectiveness of SSL and CC in Emotion Recognition of Nonverbal Vocalizations": "Academy of Sciences, 107(6):2408–2412, 2010."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "2",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "3",
      "title": "Evaluation of error-and correlation-based loss functions for multitask learning dimensional speech emotion recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2021",
      "venue": "In Journal of Physics: Conference Series"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "The icml 2022 expressive vocalizations workshop and competition: Recognizing, generating, and personalizing vocal bursts",
      "authors": [
        "A Baird",
        "P Tzirakis",
        "G Gidel",
        "M Jiralerspong",
        "E Muller",
        "K Mathewson",
        "B Schuller",
        "E Cambria",
        "D Keltner",
        "A Cowen"
      ],
      "year": "2022",
      "venue": "The icml 2022 expressive vocalizations workshop and competition: Recognizing, generating, and personalizing vocal bursts",
      "arxiv": "arXiv:2205.01780"
    },
    {
      "citation_id": "6",
      "title": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "7",
      "title": "The fisher corpus: A resource for the next generations of speech-to-text",
      "authors": [
        "C Cieri",
        "D Miller",
        "K Walker"
      ],
      "year": "2004",
      "venue": "LREC"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "É Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "The hume vocal burst competition dataset (h-vb) -raw data",
      "authors": [
        "A Cowen",
        "A Baird",
        "P Tzirakis",
        "M Opara",
        "L Kim",
        "J Brooks",
        "J Metrick"
      ],
      "venue": "The hume vocal burst competition dataset (h-vb) -raw data"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Zenodo"
      ],
      "year": "2022",
      "venue": "",
      "doi": "10.5281/zenodo.6308780"
    },
    {
      "citation_id": "11",
      "title": "Bayes optimal multilabel classification via probabilistic classifier chains",
      "authors": [
        "K Dembczynski",
        "W Cheng",
        "E Hüllermeier"
      ],
      "year": "2010",
      "venue": "ICML"
    },
    {
      "citation_id": "12",
      "title": "On label dependence in multilabel classification",
      "authors": [
        "K Dembszynski",
        "W Waegeman",
        "W Cheng",
        "E Hüllermeier"
      ],
      "year": "2010",
      "venue": "LastCFP: ICML Workshop on Learning from Multi-label data"
    },
    {
      "citation_id": "13",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "15",
      "title": "Speech recognition and keyword spotting for low-resource languages: Babel project research at cued",
      "authors": [
        "M Gales",
        "K Knill",
        "A Ragni",
        "S Rath"
      ],
      "year": "2014",
      "venue": "Fourth International workshop on spoken language technologies for under-resourced languages (SLTU-2014)"
    },
    {
      "citation_id": "16",
      "title": "Switchboard-1 release 2. Linguistic Data Consortium",
      "authors": [
        "J Godfrey",
        "E Holliman"
      ],
      "year": "1997",
      "venue": "Switchboard-1 release 2. Linguistic Data Consortium"
    },
    {
      "citation_id": "17",
      "title": "Psychosocial correlates of interpersonal sensitivity: A metaanalysis",
      "authors": [
        "J Hall",
        "S Andrzejewski",
        "J Yopchick"
      ],
      "year": "2009",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "18",
      "title": "Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "20",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Librilight: A benchmark for asr with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P Mazaré",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen",
        "T Likhomanenko",
        "G Synnaeve",
        "A Joulin",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Data augmenting contrastive learning of speech representations in the time domain",
      "authors": [
        "E Kharitonov",
        "M Rivière",
        "G Synnaeve",
        "L Wolf",
        "P.-E Mazaré",
        "M Douze",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "Data augmenting contrastive learning of speech representations in the time domain",
      "arxiv": "arXiv:2007.00991"
    },
    {
      "citation_id": "23",
      "title": "A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "24",
      "title": "When voices get emotional: A corpus of nonverbal vocalizations for research on emotion processing",
      "authors": [
        "C Lima",
        "S Castro",
        "S Scott"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "25",
      "title": "Rectifier nonlinearities improve neural network acoustic models",
      "authors": [
        "A Maas",
        "A Hannun",
        "A Ng"
      ],
      "year": "2013",
      "venue": "ICML Workshop on Deep Learning for Audio, Speech and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Maximizing subset accuracy with recurrent neural networks in multi-label classification",
      "authors": [
        "J Nam",
        "E Loza Mencía",
        "H Kim",
        "J Fürnkranz"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "29",
      "title": "A large-scale multilingual dataset for speech research",
      "authors": [
        "V Pratap",
        "Q Xu",
        "A Sriram",
        "G Synnaeve",
        "R Collobert",
        "Mls"
      ],
      "year": "2020",
      "venue": "A large-scale multilingual dataset for speech research",
      "arxiv": "arXiv:2012.03411"
    },
    {
      "citation_id": "30",
      "title": "Classifier chains for multi-label classification",
      "authors": [
        "J Read",
        "B Pfahringer",
        "G Holmes",
        "E Frank"
      ],
      "year": "2011",
      "venue": "Machine learning"
    },
    {
      "citation_id": "31",
      "title": "Classifier chains: a review and perspectives",
      "authors": [
        "J Read",
        "B Pfahringer",
        "G Holmes",
        "E Frank"
      ],
      "year": "2021",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "32",
      "title": "Utokyo-sarulab system for voicemos challenge 2022",
      "authors": [
        "T Saeki",
        "D Xin",
        "W Nakata",
        "T Koriyama",
        "S Takamichi",
        "H Saruwatari",
        "Utmos"
      ],
      "year": "2022",
      "venue": "Utokyo-sarulab system for voicemos challenge 2022",
      "arxiv": "arXiv:2204.02152"
    },
    {
      "citation_id": "33",
      "title": "Crosscultural recognition of basic emotions through nonverbal emotional vocalizations",
      "authors": [
        "D Sauter",
        "F Eisner",
        "P Ekman",
        "S Scott"
      ],
      "year": "2010",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "34",
      "title": "Affect bursts. Emotions: Essays on emotion theory",
      "authors": [
        "K Scherer"
      ],
      "year": "1994",
      "venue": "Affect bursts. Emotions: Essays on emotion theory"
    },
    {
      "citation_id": "35",
      "title": "Assessing the ability to recognize facial and vocal expressions of emotion: Construction and validation of the emotion recognition index",
      "authors": [
        "K Scherer",
        "U Scherer"
      ],
      "year": "2011",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "36",
      "title": "Openxbow: introducing the passau open-source crossmodal bag-of-words toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "37",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "38",
      "title": "The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "J Hirschberg",
        "J Burgoon",
        "A Baird",
        "A Elkins",
        "Y Zhang",
        "E Coutinho",
        "K Evanini"
      ],
      "year": "2016",
      "venue": "17TH Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "39",
      "title": "Comparing non-verbal vocalisations in conversational speech corpora",
      "authors": [
        "J Trouvain",
        "K Truong"
      ],
      "year": "2012",
      "venue": "Proceedings of the LREC Workshop on Corpora for Research on Emotion Sentiment and Social Signals"
    },
    {
      "citation_id": "40",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "arxiv": "arXiv:2203.07378"
    },
    {
      "citation_id": "41",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    }
  ]
}