{
  "paper_id": "2508.10828v1",
  "title": "A Multimodal Neural Network For Recognizing Subjective Self-Disclosure Towards Social Robots",
  "published": "2025-08-14T16:50:51Z",
  "authors": [
    "Henry Powell",
    "Guy Laban",
    "Emily S. Cross"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Subjective self-disclosure is an important feature of human social interaction. While much has been done in the social and behavioural literature to characterise the features and consequences of subjective self-disclosure, little work has been done thus far to develop computational systems that are able to accurately model it. Even less work has been done that attempts to model specifically how human interactants selfdisclose with robotic partners. It is becoming more pressing as we require social robots to work in conjunction with and establish relationships with humans in various social settings. In this paper, our aim is to develop a custom multimodal attention network based on models from the emotion recognition literature, training this model on a large self-collected selfdisclosure video corpus, and constructing a new loss function, the scale preserving cross entropy loss, that improves upon both classification and regression versions of this problem. Our results show that the best performing model, trained with our novel loss function, achieves an F1 score of 0.83, an improvement of 0.48 from the best baseline model. This result makes significant headway in the aim of allowing social robots to pick up on an interaction partner's self-disclosures, an ability that will be essential in social robots with social cognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Self-disclosure is the sharing of one's thoughts, feelings, or personal information during a social interaction  [1] . It is an import facet of human social behaviour and can contribute to many aspects of our lives. It can contribute to the extent to which we form bonds with one another -i.e. how intimate and important we consider our relationship with others to be -as well as contributing significantly to our mental and physical health  [1] -  [3] . In this paper, we focus specifically on subjective self-disclosure, which picks out the degree to which one perceives themselves to be sharing personal information with others. For example, it may be the case that someone shares some information which may, in general, not be perceived to be particularly personal or sensitive. However, this information might well be meaningful to the person disclosing it. Here the term subjective is supposed to clarify that what is important in an act of self-disclosure is that the person performing it believes themselves to be sharing meaningful thoughts, feelings, or personal information.\n\nConsidering the importance of subjective self-disclosure in developing meaningful personal relationships, we argue that sensitivity to these self-disclosures becomes a crucial attribute of social robots when establishing relationships with them. Social robots are increasingly being studied in social contexts  [4] , with a focus on their affective roles  [5] ,  [6] .Previous research has highlighted how individuals develop social relationships with these agents over time  [7] ,  [8] , why users open up to them  [9] ,  [10] , and the diverse ways in which they can support human users  [7] ,  [8] ,  [11] -  [13] . Therefore, these robotic agents, designed to operate among humans and communicate with them, must be equipped to handle such disclosures appropriately  [10] . However, despite their potential, social robots currently lack the nuanced cognitive capacity to infer such social information and understand subjective perceptions as effortlessly as humans do  [14] , which is critical when these robots take social and affective roles. Moreover, there is very little, if any, work in the field of human-robot interaction (HRI) that seeks to model the ability to detect and measure self-disclosure with the aim bestowing a socially oriented artificial agent with this ability. One such exception is a previous study from our group, where we found that a number of standard deep learning architectures were able to perform well above average on the task of ranking the degree of users' subjective self-disclosure in recorded interactions  [15] .\n\nIn this study, we aim to address this problem and to improve previous results. We did this in a number of ways. Firstly, by developing a significantly larger data set that included an visual as well as an audio modality in order to capture markers of subjective self-disclosure that may be present in how facial features evolve over time. Secondly, by developing a more sophisticated deep learning model that was better suited to the task i.e. one that was developed using domain knowledge of the problem and the data representations we used as input to our model. Thirdly, to address the problem of experimental framing that we experienced in that study, i.e. how to model data that was both categorical and scaled.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. The Current Paper",
      "text": "The remainder of the paper takes the following form: in Section II we detail the design, data collection and data pre-processing for the experiment that we conducted in order to form the dataset used to perform our deep learning experiments. Next, in Section III we outline which features we extracted from the processed dataset and the means by which we extracted them. In Section IV, we describe the architecture of our multi modal attention network in detail. We then describe the experiments we conducted to produce baselines to which we could compare the performance of this model. Further, we detail the parameters of our ablation experiment to test the effects of the loss functions, feature sets, and experimental framings that we used, and finally, the specific details of the training implementation. Then, in Section V we present the results of the ablation study before finally, in Section VI, discussing some areas for further improvement to our approach and some issues with it.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Our Contribution",
      "text": "Our contributions to the field are as follows: 1) We present the most extensive attempt to model subjective self-disclosure in HRI so far, 2) A multi modal attention based architecture designed specifically for self-disclosure modelling from audio and video data, 3) A novel loss function, the scale preserving cross entropy loss, that effectively deals with problems that fall between regression and classification and outperforms both squared error and cross entropy approaches to self-disclosure modelling.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Data Set And Data Collection",
      "text": "In order to generate data for the models, a long-term mediated online experiment was conducted, as reported in  [7] . We repeat that protocol here verbatim for consistency: A 2 (Discussion Theme: COVID-19 related vs. general) by 10 (chat sessions across time) between-groups repeated measures experimental design was followed. 39 Participants were randomly assigned to one of the two discussion topic groups, according to which they conversed with the robot Pepper (SoftBank Robotics) via Zoom video chats about general everyday topics (e.g., social relationships, work-life balance, health and well-being). One group's conversation topics were framed within the context of the COVID-19 pandemic (e.g., social relationships during the pandemic), whereas the other group's conversation topics were similar, except that no explicit mention of the COVID-19 pandemic was ever made. Participants were scheduled to interact with the robot twice a week during prearranged times for five weeks, resulting in 10 interactions in total. Each interaction consisted of the robot asking the participant 3 questions (x3 repetitions), starting with a generic question to build rapport (e.g., how was your week/weekend), followed by two additional questions that corresponded to one of the 10 randomly ordered topics (for the topics, questions, and examples see  [7] ). The topic of each interaction was assigned randomly, as was the order of the questions. After conversing with Pepper via the zoom chat, participants filled a questionnaire reporting for their perceptions of their subjective disclosure via an adapted version of Jourad self-disclosure questionnaire  [1] . The zoom chats were recorded for analysis purposes. Each interaction with the robot lasted between 5 to 10 minutes, and another 10-20 minutes were taken up completing questionnaires. The study followed rigorous ethical standards and all study procedures were approved by the research ethics committee of the School of Psychology and Neuroscience, University of Glasgow, UK.\n\nThis lead to 39×10 = 390 interactions each comprising of at least 3 conversational segments that we were able to use to train our models. Once the dataset was collected the videos were segmented by hand to isolate the sections that contained only the participants' speech. Most videos contained three speech segments comprised of the participants' answers to each of Pepper's questions. However, some participants followed up on Pepper's responses to their answers resulting in a number of additional speech segments that we were able to add to the corpus. Each of the segments was then labelled by an experimenter in accordance to the self-disclosure score that each participant had assigned to their respective interaction instances. This lead to a total of 1,248 speech and video segments that were used in our deep learning experiments.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Feature Extraction A. Visual Features",
      "text": "We extracted a number of visual feature types using a combination of state-of-the art feature extraction models. First, we extracted frame-by-frame gaze and action unit features using the OpenFace 2.2 library  [16]  (see Figure  2  for visual example). To account for missing frames in each time series that came about as a result of the OpenFace models not registering the presence of a human face, we interpolated the missing frames with the recorded data using spline interpolation. We then filtered and smoothed the resulting multivariate time series with a Savitsky-Golay filter (using a sliding window of 11 frames and a polynomial order of 3). To test the the affects of smoothing and filtering on the results we treated smoothed/filtered and non-smoothed/filtered feature sets as separate in our initial experiments.\n\nNext, we extracted facial features using an InceptionV1 ResNet  [17]  [18] architecture pretrained on the VGGFace2 dataset  [19] . VGGFace2 consists of 3.31 million images of celebrity faces organized into 9131 subject categories with large variances in pose, age, illumination, and ethnicity. The InceptionV1 ResNet that we used scored an accuracy of 99.6% on this dataset. Pre-processing of the video frames in this case consisted of extracting a 160x160 pixel subregion of each frame that contained pixel and feature-wise normalization of the subject's face (an example of the  MTCNN output can be seen in Figure  1 ). This was done using a pretrained multi-task cascaded convolutional neural network  [20]  on each video frame. The pretrained ResNet produces 512 facial features for each video frame. Similar to our approach with the the OpenFace features we interpolated and filtered the resulting time series to experiment with the effects that this would have the models' scores.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Audio Features",
      "text": "For audio features we first produced a mel-filter cepstral coefficient (MFCC) matrix for each video's audio modality. This was done using PyTorch's audio feature extraction library using 256 mel-filter banks. This feature set was chosen due MFCCs well established ability to capture significant audio features for human speech recognition tasks  [21]    [22] [23] . This was an extension from previous work on using log-mel spectrograms to recognize subjective self-disclosures  [15] , where we found that spectrogram features were more effective at capturing significant self-disclosure related features from subjects' speech. In the case of this study, we found that MFCC features produced better results at initial testing and accordingly we went with MFCC features over the log mel spectrogram alternative. We also experimented with the effects of cepstral mean and variance normalization of MFCC features on our baseline models' performance (detailed in Section IV-A as this was a factor that would also have to be taken into account when training our deep learning models).\n\nSecond, we extracted audio features directly from each sound file's amplitude array using Facebook AIs wav2vec2.0 architecure  [24] . Wav2vec2.0 uses a stack of convolutional neural network based feature encoders and generates contextualised audio representations using a transformer model  [25] . We used a wav2vec2.0 model pretrained on 960 hours of unlabelled audio data from the LibriSpeech dataset  [26] . To get the feature sets for each wav file we took the outputs from the models 12 transformer layers which resulted in 12 t x 768 feature matrices where the value t was determined by the number of frames in the audio file.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Deep Learning Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Support Vector Machine Baselines",
      "text": "Since we were working with a novel dataset designed specifically for our deep learning experiments we needed some way of establishing a baseline that we were able to compare our results to. Following  [27]  we used Gaussian kernel support vector machines (SVM) trained on our extracted audio and visual features separately to establish such a baseline. For each feature type, a vector representing the mean over all frames in each example was computed and the SVMs were tasked with classifying the self-disclosure score for each interaction. Each model was trained using 3 fold cross validation and the average f1 score was used as a means to measure the overall performance of each model.\n\nThe results of these baseline experiments (illustrated in Figure  3 ) indicate that the facial features extracted using InceptionV1 pretrained on VGGFace2 were significantly the most informative for the task while for the audio features, the MFCC representation was the most informative. Overall video features were the most useful feature sets in discrmiitinating the self-disclosure score classes. The results also show that the problem is a difficult one given that the best f1 score measured was only 0.36. One surprising result was that the word2vec2.0 features performed so poorly. We hypothesised that, given the strong relative performance of the InceptionV1 features, that word2vec2.0 would also perform relatively well given that both models are pretrained on large amounts of task relevant data. One possible explanation of why word2vec2.0 features performed so poorly in the baseline test is with respect to how the mean vector for each frame was computed. The word2vec2.0 features for each frame were of far higher dimension than both the MFCC features (12×t W × 178 vs. 256 × t M ) and the visual feature set of the highest dimensionality (InceptionV1 features at t I × 512)  1  . Thus condensing the word2vec2.0 features across both the time and attention-head dimensions into a single 178 dimensional vector could have meant that too much information was lost leading to the feature dramatically losing its discriminative ability with respect to the task.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Multimodal Attention Network",
      "text": "We designed a multimodal attention network that processes the audio and visual features of each video in separate streams and then combines these representations in a late fusion fashion before being classified by a linear neural network layer. This approach was motivated by previous observations communicated in  [15]  that concluded that 'off the shelf' neural network architectures, i.e. ones that were not designed specifically for the task at hand and used no pretraining, produced less than desirable results on the audio-only version of this task. We aimed to improve our previous attempt (i.e.,  [15] ) by: firstly, taking into account video recordings of the interactions. Secondly, designing a custom neural network architecture that deals with audio and visual features separately before combining them into one latent representation. Thirdly, using pretrained neural network backbones in each feature processing stream and finally, experimenting with feature fusion using principle components analysis to prevent our results from being limited by being only able to use a single feature representation.\n\nThe design of this architecture is inspired by other deep learning approaches that utilize attention mechanisms leveraged from deep convolutional neural networks for recognition tasks involving visual and audio data captured from human subjects -specifically in emotion recognition and related tasks  [28]    [29] . Our approach is similar to that in  [29]  in that we use their convolutional architecture for each of the attention mechanisms, although in our case we use only frame-wise attention in both the audio and visual streams. We also use an InceptionV1 ResNet trained on VGGFace2 instead of the 3DResnet used in that study as it was more suited to our problem and our baseline SVM experiment showed convincingly that this feature representation was the most informative for the task. As in  [29]  we compute the frame-wise attention (i.e. along the time dimension in each case) for the audio and visual streams in the following way. We adapt their formulation here for the sake of completeness and clarity with respect to how we have modified their approach. The full architecture is displayed in Figure  4  1) Audio Temporal Attention Subnetwork: Let x A i be the i th audio feature matrix input. We first center crop x A i to a fixed length l such that l s ∈ N for some positive integer s giving x A ′ i . If the time dimension of x A i is less than l then we pad the input on either side with zeros such that it's length is now equal to l. We then split x A ′ i into s segments and stack them on top of one another such that x A ′ i ∈ R s× l s ×n . The model then receives a batch of size b of these tensors which is then fed through the model's audio stream.\n\nThe first step of the audio stream is to process each of the b × s feature segments through a ResNet18 model  [18]  pretrained on the ImageNet dataset. This may sound surprising given that we are using using an ImageNet trained model on MFCC audio representations (since ImageNet contains no MFCC examples) but research has shown that using such ResNets on MFCC features matrices dependably improves model scores  [30]  and indeed we also found this to be the case in our experiments. We then take the output F A j of the fifth convolutional stack of the pretrained ResNet18 model and perform spatial average pooling over the feature maps producing F A ′ j (where j indexes over the feature matrix segments). This downsamples the output of the ResNet from\n\nwhere s is the number of segments, h and w are the height and width of the feature maps respectively, and c is the number of channels, creating a 1 × c length descriptor for each of the segments. The goal is now to learn an s × 1 length descriptor for the audio feature matrix segments where the k th element of the descriptor weights the k th segment according to its importance in classifying the input sample. This descriptor is learned using a convolutional stack that consists of a 1D convolutional layer, a fully connected linear layer, and a ReLU non-linearity such that:\n\nWhere W A 1 and W A 2 are s × s and 1 × c learnable parameter matrices for the linear and convolutional layers respectively. We then compute the activation of the audio attention subnetwork A A i.e. the s × 1 length segment descriptor as:\n\nThe output embedding for the audio stream, i.e. the representation of which audio segments are most relevant to the classification of the input example to a particular selfdisclosure class, is computed via:\n\n2) Visual Temporal Attention Subnetwork: The approach to achieve the audio embedding E V for the visual features extracted from the videos follows precisely the same steps as the audio temporal attention algorithm. The principal differences in practice are that we use the InceptionV1 ResNet architecture trained on VGGFace2 that we used in our SVM baseline experiments instead of the ResNet18 model.\n\nGiven the output embeddings E A and E V for the audio and visual processing streams we then summarize the Fig.  4 : Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar streams. MFCC segments are fed through an ImageNet pretrained 2DResNet backbone before being average pooled, and cloned. One copy is then sent through the attention subnetwork before being multiplied to the other ResNet output copy. This representation is then average pooled once again producing the final audio embedding. The same process occurs with the frame input except that the backbone is a InceptionV1 ResNet architecture pretrained on VGGFace2. The resulting audio and visual embeddings are then concatenated and fed through a linear classification layer. The network probabilities are then used to compute the scale-preserving cross entropy loss by which the parameters of the network are optimised. features using average pooling by computing the mean of each embedding vector along the time domain (i.e. across segments) giving E A ′ and E V ′ . These are then concatenated before being fed to a linear layer containing 7 neurons representing each of the self-disclosure score classes. This produces output ŷ:\n\nwhere W AV is a learnable parameter matrix related to the linear output layer, concat() is the concatenation operation, and Softmax() is the softmax function that return normalized probabilities over the seven self-disclosure score classes.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "C. Ablation Experiment Parameters",
      "text": "In our experiments we tested the influence of two different visual feature sets, two experimental framings, and four different loss functions to determine the best configuration for the problem.\n\n1) Visual Feature Sets: First, we wanted to test the efficacy of just the facial features output by the InceptionV1 ResNet architecture pretrained on VGGFace2 as our SVM experiments showed that these were likely to be the most informative visual features for the task. Next we wanted to test a combination of all visual features that we extracted as detailed in Section IV-A. To reduce the dimensionality of this feature space we concatenated all of the visual features together after the visual input has been passed through the ResNetV1 backbone in the visual stream and performed principal components analysis with parameters set such that 99% of the variance in the data was explained by the resulting feature matrix. This resulted in a dimensionality change in this feature space from a 555 dimensional feature vector to a 67 dimensional feature vector for each video frame.\n\n2) Classification Vs. Regression: In  [15]  we found that there was a nuance in the approach to classifying selfdisclosure scores. As we state in that study, participants rated the degree of self-disclosure in their interactions on a likert scale between 1 and 7. This means that each score falls into a discrete class meaning that one plausible way to frame the problem is as an n-class classification problem. However, loss functions related to n-class classification problems often treat incorrect guesses in the same manner i.e. there is no sense in which one guess can be numerically represented as being closer to a correct guess than any of the other possible guesses. The self-disclosure score data, however, is scaled in the sense that a model guess of 2 for ground truth selfdisclosure score of 1 should be treated as a better guess than 6 or 7. In this light an argument could be made that the problem is better represented as a regression problem. In  [15]  we found that framing the problem in both ways produced similar results and as such no clear empirically informed decision could be made about what approach worked best. In light of this we decided to test the effects of both approaches on our results.\n\n3) Loss Function: We wanted to study the effect of loss function on the problem. Standardly, regression based methods minimize a mean-squared error loss in order to optimize the parameters of a given model. Since we had no good reason to suspect that this particular problem required an alternative regression-based loss function we chose only to base our regression results on the mean squared error loss. For the classification version of the problem we chose a categorical cross-entropy loss function for our experiments. For this loss, research has shown that label smoothing, a technique whereby standard 'hard' labels are modified by a smoothing parameter α via y LS k = y k (1 -α) + α K where k indexes over the total number of classes (seven in the case of this study), can drastically improve results  [31] . As such we chose to include a cross entropy loss with label smoothing as part of ablation study. Last, we wanted to explore the possibility of designing a custom loss function that was able to strike a balance between the classification and regression versions of the task i.e. one that leveraged the fact that the data was categorical while also preserving the notion that certain guesses were better with respect to a ground truth label than others. Taking inspiration from  [29]  we designed a custom cross entropy loss function that penalises guesses with greater severity the further they are from the ground truth label. For example, for an input sample with labelled self-disclosure score of 7 a guess of 1 will result in a higher loss than a guess of 2, a guess of 2 will result in a higher loss than a guess of 3, and so on. To do this we amended the standard cross-entropy loss function which can be expressed as:\n\nwhere N is the number of input samples, C is the number of classes, 1 [c=yi] is an indicator variable that equals 1 when the predicted class is the same as the ground truth class and p i,c is the probability that the i th sample belongs to the c th class. We added a penalty term to 5 that formalizes the idea that guesses at a greater distance from the ground truth should be penalised more severely. This gives what we term the scale preserving cross-entropy loss:\n\nwhere λ and µ are hyperparamters that change the degree to which a incorrect guess is penalised with respect to how far away it is from the ground truth self-disclosure label.\n\nTaken together, the parameters of the ablation study lead to eight different training configurations for the multimodal attention network, the specifications of which are presented in Figure  5 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Model Training",
      "text": "Regression and classification models were trained over 100 epochs, while the SPCE models where trained on 150 since we found that they took longer to converge. All network version we trained using the Adam optimizer  [32] , an initial learning rate of 0.01, and mini-batch size of 35. Audio feature inputs were cropped to length l = 128 and divided in to s = 4 segments. Visual input features were cropped to l = 210 frames and divided into s = 7 segments. We prepared the training data as in  [15]  splitting the training and testing datasets into an 80/20 split and used weighted random sampling to account for imbalanced classes. Each model was trained five times and the average F1 score and standard deviation over all five training instances were computed to give a balanced assessment of the model's performance. We chose to validate the models using f1 scores so that our results were directly comparable to those produced by our SVM experiments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Results",
      "text": "The results of our study are displayed in 5. We found that all versions of the multimodal attention network scored significantly above the best SVM baseline. Interestingly, departing from  [15] , where regression and classification models performed about as well as each other, we found that a classification framing (treating self-disclosure scores as discrete classes) was significantly more effective at modelling the problem than a regression framing (treating the scores as being derived from the continuous number line). In all cases we found that, within each experimental framing, the features derived from principle components analysis outperformed models trained on just InceptionV1 facial features. This is perhaps unsurprising for two reasons. Firstly, because this feature set was comprised of three times the number of features than the pure InceptionV1 feature set before it was condensed to its principal components. Secondly, because significantly reducing the number of features (from 512 in the pure InceptionV1 case to 67 in the principal components case) would mean that our model was less susceptible to the curse of dimensionality i.e. that it would require much less data to effectively model that smaller set of features. Further, we found that label smoothing produced improvements in results when compared to the non-label smoothing variant of the cross entropy loss. Finally, we found that our scale preserving cross-entropy loss outperformed all but one version of the model (principal component features with label smoothing cross-entropy loss) to which it equalled in performance.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Discussion And Conclusion",
      "text": "Overall we report significant increases on performance in this task from previous attempts (e.g.,  [15] ). We hypothesise that this is due to a number of significant developments from that work. Firstly, we collected a much larger dataset meaning that the models had more examples to learn from. Second, in this case all interactions were recorded between the same interaction dyad (i.e. between a human and a Pepper robot). In  [15] , we collected interaction data between three different dyads: human-human, human-embodied robot, and human-voice agent. One reason that results may have been worse in that case is due to the possibility that vocal features particular to each self-disclosure class may have been modulated by the kind of agent the participant was interacting with. Since the interaction partner remained constant in our study, variability was reduced, which could have simplified the learning task. Moreover, we used significantly more advanced models, taking advantage of the representational power of large deep neural networks trained on extensive datasets. Further, our use of frame-wise attention mechanisms make use of deep learning techniques that have shown to be state-of-the art on video and language modelling tasks perhaps providing a straightforward upgrade of the 'off-theshelf' models that were used in the previous study. Lastly, and perhaps most obviously, in this study we modelled two different sensory modalities (audio and visual) as opposed to the single sensory modality that was considered in  [15] . It may well be the case that the auditory domain holds less discriminative information than the visual domain for self-disclosure modelling and thus, the previous study was automatically at a disadvantage in only considering the former.\n\nOne surprising observation from our baseline experiments was that the visual features were most effective at allowing the SVM models to predict a particular subjective selfdisclosure score. While much of the literature on selfdisclosure is varied with respects to its definitions one thing that is generally agreed upon is that self-disclosure is primarily a verbally communicated social phenomenon  [33] ,  [34] . In light of this it might be expected that the audio modality would produce the best results. It may well be the case that the way in which the audio features were averaged caused some of the information to be lost. Unfortunately, a thorough investigation of why the visual features were the most informative is outside of the scope of this paper.\n\nWhile this study shows significant improvement on previous work done on modelling self-disclosure with neural networks, it remains to be seen whether the advances that we detail here are significant enough for these models to be implemented in social robots. There is a significant risk associated with an incorrect self-disclosure scoring in a real world setting  [15] . Assuming that a person is sharing very little self-disclosure when in fact they believe themselves to be sharing a significant amount could lead to that person feeling as if they are being ignored or that the sensitive information that they are sharing is not worthy of the listener's consideration  [35] . Conversely, assigning a very high self-disclosure score in a situation where an interaction partner does not believe themselves to be sharing a significant amount of personal information could cause undue levels of attention to be paid to a situation which is not important  [36] . The issue described in both of these cases would be significantly confounded within the context of emotional well being interventions, where the risks associated with not picking up on a user's self-disclosure related signals could be very damaging. As such, considerably more work needs to be done before models like ours are considered for real world application. There are at least two ways that steps could be taken in this direction. Firstly, significantly more data should be collected to improve the performance of the models. Secondly, a study should be carried out to asses the differences between model performance and the performance on the same task by a trained professional. It is often the case that the quality of a machine learning model and it's viability as a real world application is measure with respect to its ability to achieve 'human-like' performance. It makes sense that a model that is effective at recognizing the degree to which a person is disclosing personal information should be able to do so at least as well as a trained professional (particularly if that model is to be implemented within the context of health care interventions).\n\nFurther, there are ways in which improvements on our approach might be made in the short term. Firstly, since we found that performance on the task was improved when visual features were combined using principal components analysis, it's likely to also be the case that performance improvements could be achieved by combining audio features. In particular, we did not experiment with ways to combine outputs from the transformer layers of wave2vec2.0 with the MFCC features. Additionally, more empirical work could be done to ascertain the best way to combine feature sets in both the audio and visual cases. For one such example,  [27]  used a denoising autoencoder to learn a compressed latent representation of the concatenated input features. A future study should empirically test the hypothesis that such a latent representation exists in a more effective input feature space than the one produced by principal components analysis. Further studies could also look into experimenting with other kinds of attention. In  [29]  the authors use channel-wise attention and spatial attention in the visual stream on top of the frame-wise attention that both of our methods share. One development along these lines could be to implement an attention mechanism that produces a descriptor over the features i.e. the columns of the input matrices. In this way the model would hold a representation of not only which frames of the input are important to its classification but also which features are important. Lastly, the model could be altered to leverage 3D ResNets to produce higher dimensional features over the input video frames. This approach however would require a 3DResNet trained on a very large video dataset focused on the modelling of human faces and, to our knowl-edge, no such pretrained model is publicly available. Taking from the modelling literature on self-disclosure  [37] , show that very good results on the task of (non-subjective) selfdisclosure modelling between two human interactants can be achieved multi-modally with the addition of lexical features. In that study, the authors use a pretrained BERT language model  [38]  to extract features related to the words used in each utterance. A significant part of self-disclosure (at least in the human-human case) is thought to be communicated verbally  [33] [34] . This a future study could look at including this modality in the HRI version of the task.\n\nWe believe that this study makes significant strides into the new field of subjective self-disclosure modelling by showing considerable improvements over results of any previous studies on the topic.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example output of the MTCNN used for facial feature extraction:",
      "page": 3
    },
    {
      "caption": "Figure 2: OpenFace 2.2 processing facial action units, and gaze from an",
      "page": 3
    },
    {
      "caption": "Figure 1: ). This was done",
      "page": 3
    },
    {
      "caption": "Figure 3: ) indicate that the facial features extracted using",
      "page": 3
    },
    {
      "caption": "Figure 3: Gaussian SVM baseline F1 scores for individual smoothed/filtered",
      "page": 4
    },
    {
      "caption": "Figure 4: 1) Audio Temporal Attention Subnetwork: Let xA",
      "page": 4
    },
    {
      "caption": "Figure 4: Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar",
      "page": 5
    },
    {
      "caption": "Figure 5: D. Model Training",
      "page": 6
    },
    {
      "caption": "Figure 5: F1 scores for our multimodal attention network trained on a",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Amazon, Edinburgh, UK.": "2School of Psychology and Neuroscience, University of Glasgow, Glasgow, UK."
        },
        {
          "1Amazon, Edinburgh, UK.": "3Department of\nIndustrial Engineering and Management, Ben-Gurion University of\nthe Negev, Beer Sheva,\nIsrael."
        },
        {
          "1Amazon, Edinburgh, UK.": "4Department of Computer Science and Technology, University of Cambridge, Cambridge, UK."
        },
        {
          "1Amazon, Edinburgh, UK.": "5Social Brain Sciences Group, Department of Humanities, Social and Political Sciences, ETH Zurich, Zurich, Switzerland."
        },
        {
          "1Amazon, Edinburgh, UK.": "Abstract— Subjective self-disclosure is an important\nfeature\nbe perceived to be particularly personal or\nsensitive. How-"
        },
        {
          "1Amazon, Edinburgh, UK.": "of human social\ninteraction. While much has been done in the"
        },
        {
          "1Amazon, Edinburgh, UK.": "ever, this information might well be meaningful to the person"
        },
        {
          "1Amazon, Edinburgh, UK.": "social and behavioural\nliterature\nto characterise\nthe\nfeatures"
        },
        {
          "1Amazon, Edinburgh, UK.": "disclosing it. Here the term subjective is supposed to clarify"
        },
        {
          "1Amazon, Edinburgh, UK.": "and consequences of\nsubjective self-disclosure,\nlittle work has"
        },
        {
          "1Amazon, Edinburgh, UK.": "that what\nis\nimportant\nin an act of\nself-disclosure\nis\nthat"
        },
        {
          "1Amazon, Edinburgh, UK.": "been done thus\nfar to develop computational\nsystems\nthat are"
        },
        {
          "1Amazon, Edinburgh, UK.": "the person performing it believes\nthemselves\nto be sharing\nable to accurately model\nit. Even less work has been done that"
        },
        {
          "1Amazon, Edinburgh, UK.": "attempts\nto model\nspecifically how human interactants\nself-\nmeaningful\nthoughts,\nfeelings, or personal\ninformation."
        },
        {
          "1Amazon, Edinburgh, UK.": "disclose with robotic partners.\nIt\nis becoming more pressing"
        },
        {
          "1Amazon, Edinburgh, UK.": "Considering the\nimportance of\nsubjective\nself-disclosure"
        },
        {
          "1Amazon, Edinburgh, UK.": "as we\nrequire\nsocial\nrobots\nto work in conjunction with and"
        },
        {
          "1Amazon, Edinburgh, UK.": "in developing meaningful personal\nrelationships, we\nargue"
        },
        {
          "1Amazon, Edinburgh, UK.": "establish relationships with humans in various social settings. In"
        },
        {
          "1Amazon, Edinburgh, UK.": "that\nsensitivity to these\nself-disclosures becomes\na\ncrucial"
        },
        {
          "1Amazon, Edinburgh, UK.": "this paper, our aim is\nto develop a custom multimodal atten-"
        },
        {
          "1Amazon, Edinburgh, UK.": "attribute\nof\nsocial\nrobots when\nestablishing\nrelationships\ntion network based on models\nfrom the\nemotion recognition"
        },
        {
          "1Amazon, Edinburgh, UK.": "literature,\ntraining\nthis model\non a\nlarge\nself-collected self-\nwith them. Social\nrobots are increasingly being studied in"
        },
        {
          "1Amazon, Edinburgh, UK.": "disclosure video corpus, and constructing a new loss function,"
        },
        {
          "1Amazon, Edinburgh, UK.": "social\ncontexts\n[4], with\na\nfocus\non\ntheir\naffective\nroles"
        },
        {
          "1Amazon, Edinburgh, UK.": "the\nscale preserving\ncross\nentropy\nloss,\nthat\nimproves upon"
        },
        {
          "1Amazon, Edinburgh, UK.": "[5],\n[6].Previous\nresearch has highlighted how individuals"
        },
        {
          "1Amazon, Edinburgh, UK.": "both\nclassification\nand\nregression\nversions\nof\nthis\nproblem."
        },
        {
          "1Amazon, Edinburgh, UK.": "develop\nsocial\nrelationships with\nthese\nagents\nover\ntime"
        },
        {
          "1Amazon, Edinburgh, UK.": "Our\nresults\nshow that\nthe\nbest\nperforming model,\ntrained"
        },
        {
          "1Amazon, Edinburgh, UK.": "[7],\n[8], why\nusers\nopen\nup\nto\nthem [9],\n[10],\nand\nthe\nwith our novel\nloss\nfunction, achieves an F1 score of 0.83, an"
        },
        {
          "1Amazon, Edinburgh, UK.": "improvement of 0.48 from the best baseline model. This result\ndiverse ways\nin which they can support human users\n[7],"
        },
        {
          "1Amazon, Edinburgh, UK.": "makes significant headway in the aim of allowing social robots"
        },
        {
          "1Amazon, Edinburgh, UK.": "[8],\n[11]–[13]. Therefore,\nthese robotic agents, designed to"
        },
        {
          "1Amazon, Edinburgh, UK.": "to pick up on an interaction partner’s self-disclosures, an ability"
        },
        {
          "1Amazon, Edinburgh, UK.": "operate among humans and communicate with them, must"
        },
        {
          "1Amazon, Edinburgh, UK.": "that will be essential\nin social robots with social cognition."
        },
        {
          "1Amazon, Edinburgh, UK.": "be equipped to handle such disclosures appropriately [10]."
        },
        {
          "1Amazon, Edinburgh, UK.": "However, despite their potential, social robots currently lack"
        },
        {
          "1Amazon, Edinburgh, UK.": "I.\nINTRODUCTION"
        },
        {
          "1Amazon, Edinburgh, UK.": "the nuanced cognitive\ncapacity to infer\nsuch social\ninfor-"
        },
        {
          "1Amazon, Edinburgh, UK.": "mation and understand subjective perceptions as effortlessly"
        },
        {
          "1Amazon, Edinburgh, UK.": "Self-disclosure is the sharing of one’s thoughts,\nfeelings,"
        },
        {
          "1Amazon, Edinburgh, UK.": "as humans do [14], which is critical when these robots take"
        },
        {
          "1Amazon, Edinburgh, UK.": "or personal\ninformation during a social\ninteraction [1].\nIt\nis"
        },
        {
          "1Amazon, Edinburgh, UK.": "social and affective roles. Moreover, there is very little, if any,"
        },
        {
          "1Amazon, Edinburgh, UK.": "an import facet of human social behaviour and can contribute"
        },
        {
          "1Amazon, Edinburgh, UK.": "work in the field of human–robot interaction (HRI) that seeks"
        },
        {
          "1Amazon, Edinburgh, UK.": "to many aspects of our lives. It can contribute to the extent to"
        },
        {
          "1Amazon, Edinburgh, UK.": "to model\nthe\nability to detect\nand measure\nself-disclosure"
        },
        {
          "1Amazon, Edinburgh, UK.": "which we form bonds with one another\n-\ni.e. how intimate"
        },
        {
          "1Amazon, Edinburgh, UK.": "with the\naim bestowing a\nsocially oriented artificial\nagent"
        },
        {
          "1Amazon, Edinburgh, UK.": "and important we\nconsider our\nrelationship with others\nto"
        },
        {
          "1Amazon, Edinburgh, UK.": "with\nthis\nability. One\nsuch\nexception\nis\na\nprevious\nstudy"
        },
        {
          "1Amazon, Edinburgh, UK.": "be - as well as contributing significantly to our mental and"
        },
        {
          "1Amazon, Edinburgh, UK.": "from our group, where we found that a number of standard"
        },
        {
          "1Amazon, Edinburgh, UK.": "physical health [1]–[3].\nIn this paper, we focus\nspecifically"
        },
        {
          "1Amazon, Edinburgh, UK.": "deep learning architectures were able to perform well above"
        },
        {
          "1Amazon, Edinburgh, UK.": "subjective\non\nself-disclosure, which\npicks\nout\nthe\ndegree"
        },
        {
          "1Amazon, Edinburgh, UK.": "average on the task of ranking the degree of users’ subjective"
        },
        {
          "1Amazon, Edinburgh, UK.": "to which one perceives\nthemselves\nto be\nsharing personal"
        },
        {
          "1Amazon, Edinburgh, UK.": "self-disclosure in recorded interactions [15]."
        },
        {
          "1Amazon, Edinburgh, UK.": "information with others. For example, it may be the case that"
        },
        {
          "1Amazon, Edinburgh, UK.": "In\nthis\nstudy, we\naim to\naddress\nthis\nproblem and\nto"
        },
        {
          "1Amazon, Edinburgh, UK.": "someone shares some information which may, in general, not"
        },
        {
          "1Amazon, Edinburgh, UK.": "improve previous results. We did this in a number of ways."
        },
        {
          "1Amazon, Edinburgh, UK.": "Firstly,\nby\ndeveloping\na\nsignificantly\nlarger\ndata\nset\nthat"
        },
        {
          "1Amazon, Edinburgh, UK.": "*Henry Powell and Guy Laban have contributed equally to this work and"
        },
        {
          "1Amazon, Edinburgh, UK.": "share first authorship. Corresponding author: guy.laban@cl.cam.ac.uk\nincluded an visual\nas well\nas\nan audio modality in order"
        },
        {
          "1Amazon, Edinburgh, UK.": "The authors gratefully acknowledge funding from the European Research"
        },
        {
          "1Amazon, Edinburgh, UK.": "to capture markers of subjective self-disclosure that may be"
        },
        {
          "1Amazon, Edinburgh, UK.": "Council\n(ERC) under\nthe European Union’s Horizon 2020 research and"
        },
        {
          "1Amazon, Edinburgh, UK.": "present\nin how facial\nfeatures\nevolve over\ntime. Secondly,"
        },
        {
          "1Amazon, Edinburgh, UK.": "innovation programme (Grant agreement 677270 to E.S.C.), the Leverhulme"
        },
        {
          "1Amazon, Edinburgh, UK.": "Trust\n(PLP-2018-152 to E.S.C.), and the European Union’s Horizon 2020\nby developing a more sophisticated deep learning model\nthat"
        },
        {
          "1Amazon, Edinburgh, UK.": "research\nand\ninnovation\nprogramme\nunder\nthe Marie Sklodowska-Curie"
        },
        {
          "1Amazon, Edinburgh, UK.": "was better suited to the task i.e. one that was developed using"
        },
        {
          "1Amazon, Edinburgh, UK.": "to ENTWINE,\nthe European Training Network on Informal Care\n(grant"
        },
        {
          "1Amazon, Edinburgh, UK.": "domain knowledge of\nthe problem and the data representa-"
        },
        {
          "1Amazon, Edinburgh, UK.": "agreement no. 814072 to E.S.C.). Our\nthanks also go to Hubert Ramsauer"
        },
        {
          "1Amazon, Edinburgh, UK.": "for helpful discussions on the use of Hopfield networks.\ntions we used as input\nto our model. Thirdly,\nto address the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "problem of experimental framing that we experienced in that": "study,\ni.e. how to model data that was both categorical and",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "two\nadditional\nquestions\nthat\ncorresponded\nto\none\nof\nthe"
        },
        {
          "problem of experimental framing that we experienced in that": "scaled.",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "10 randomly ordered topics\n(for\nthe topics, questions, and"
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "examples see [7]). The topic of each interaction was assigned"
        },
        {
          "problem of experimental framing that we experienced in that": "A. The Current Paper",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": ""
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "randomly, as was the order of the questions. After conversing"
        },
        {
          "problem of experimental framing that we experienced in that": "The remainder of\nthe paper\ntakes\nthe following form:\nin",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "with Pepper via\nthe\nzoom chat, participants filled a ques-"
        },
        {
          "problem of experimental framing that we experienced in that": "Section\nII we\ndetail\nthe\ndesign,\ndata\ncollection\nand\ndata",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "tionnaire reporting for\ntheir perceptions of\ntheir\nsubjective"
        },
        {
          "problem of experimental framing that we experienced in that": "pre-processing\nfor\nthe\nexperiment\nthat we\nconducted\nin",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "disclosure via an adapted version of\nJourad self-disclosure"
        },
        {
          "problem of experimental framing that we experienced in that": "order\nto form the dataset used to perform our deep learning",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "questionnaire [1]. The zoom chats were recorded for analysis"
        },
        {
          "problem of experimental framing that we experienced in that": "experiments. Next,\nin Section III we outline which features",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "purposes. Each\ninteraction with\nthe\nrobot\nlasted\nbetween"
        },
        {
          "problem of experimental framing that we experienced in that": "we extracted from the processed dataset and the means by",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "5\nto\n10 minutes,\nand\nanother\n10-20 minutes were\ntaken"
        },
        {
          "problem of experimental framing that we experienced in that": "which we\nextracted them.\nIn Section IV, we describe\nthe",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "up completing questionnaires. The study followed rigorous"
        },
        {
          "problem of experimental framing that we experienced in that": "architecture of our multi modal attention network in detail.",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "ethical standards and all study procedures were approved by"
        },
        {
          "problem of experimental framing that we experienced in that": "We then describe the experiments we conducted to produce",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "the research ethics committee of\nthe School of Psychology"
        },
        {
          "problem of experimental framing that we experienced in that": "baselines\nto which we\ncould compare\nthe performance of",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "and Neuroscience, University of Glasgow, UK."
        },
        {
          "problem of experimental framing that we experienced in that": "this model. Further, we detail\nthe parameters of our ablation",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "This lead to 39×10 = 390 interactions each comprising of"
        },
        {
          "problem of experimental framing that we experienced in that": "experiment\nto test\nthe effects of\nthe loss\nfunctions,\nfeature",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "at least 3 conversational segments that we were able to use to"
        },
        {
          "problem of experimental framing that we experienced in that": "sets,\nand experimental\nframings\nthat we used,\nand finally,",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "train our models. Once the dataset was collected the videos"
        },
        {
          "problem of experimental framing that we experienced in that": "the specific details of\nthe training implementation. Then,\nin",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "were segmented by hand to isolate the sections that contained"
        },
        {
          "problem of experimental framing that we experienced in that": "Section V we present\nthe results of the ablation study before",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "only the participants’\nspeech. Most videos contained three"
        },
        {
          "problem of experimental framing that we experienced in that": "finally,\nin\nSection VI,\ndiscussing\nsome\nareas\nfor\nfurther",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "speech\nsegments\ncomprised\nof\nthe\nparticipants’\nanswers"
        },
        {
          "problem of experimental framing that we experienced in that": "improvement\nto our approach and some issues with it.",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "to each of Pepper’s questions. However,\nsome participants"
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "followed up on Pepper’s responses to their answers resulting"
        },
        {
          "problem of experimental framing that we experienced in that": "B. Our Contribution",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": ""
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "in a number of additional speech segments that we were able"
        },
        {
          "problem of experimental framing that we experienced in that": "Our contributions to the field are as follows:",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "to add to the corpus. Each of the segments was then labelled"
        },
        {
          "problem of experimental framing that we experienced in that": "1) We present\nthe most extensive attempt\nto model sub-",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "by\nan\nexperimenter\nin\naccordance\nto\nthe\nself-disclosure"
        },
        {
          "problem of experimental framing that we experienced in that": "jective self-disclosure in HRI so far,",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "score that each participant had assigned to their\nrespective"
        },
        {
          "problem of experimental framing that we experienced in that": "2) A multi modal\nattention based architecture designed",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "interaction instances. This\nlead to a\ntotal of 1,248 speech"
        },
        {
          "problem of experimental framing that we experienced in that": "specifically for\nself-disclosure modelling from audio",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "and\nvideo\nsegments\nthat were\nused\nin\nour\ndeep\nlearning"
        },
        {
          "problem of experimental framing that we experienced in that": "and video data,",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "experiments."
        },
        {
          "problem of experimental framing that we experienced in that": "3) A novel\nloss\nfunction,\nthe scale preserving cross en-",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": ""
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "III. FEATURE EXTRACTION"
        },
        {
          "problem of experimental framing that we experienced in that": "tropy loss, that effectively deals with problems that fall",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": ""
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "A. Visual Features"
        },
        {
          "problem of experimental framing that we experienced in that": "between regression and classification and outperforms",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": ""
        },
        {
          "problem of experimental framing that we experienced in that": "both squared error\nand cross\nentropy approaches\nto",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "We\nextracted a number of visual\nfeature\ntypes using a"
        },
        {
          "problem of experimental framing that we experienced in that": "self-disclosure modelling.",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "combination\nof\nstate-of-the\nart\nfeature\nextraction models."
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "First, we\nextracted\nframe-by-frame\ngaze\nand\naction\nunit"
        },
        {
          "problem of experimental framing that we experienced in that": "II. DATA SET AND DATA COLLECTION",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": ""
        },
        {
          "problem of experimental framing that we experienced in that": "",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "features using the OpenFace 2.2 library [16] (see Figure 2 for"
        },
        {
          "problem of experimental framing that we experienced in that": "In\norder\nto\ngenerate\ndata\nfor\nthe models,\na\nlong-term",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "visual example). To account for missing frames in each time"
        },
        {
          "problem of experimental framing that we experienced in that": "mediated online experiment was conducted, as\nreported in",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "series that came about as a result of the OpenFace models not"
        },
        {
          "problem of experimental framing that we experienced in that": "[7]. We repeat\nthat protocol here verbatim for consistency:",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "registering the presence of a human face, we interpolated the"
        },
        {
          "problem of experimental framing that we experienced in that": "A 2\n(Discussion Theme: COVID-19\nrelated\nvs.\ngeneral)",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "missing frames with the recorded data using spline interpola-"
        },
        {
          "problem of experimental framing that we experienced in that": "by 10 (chat\nsessions across\ntime) between-groups\nrepeated",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "tion. We then filtered and smoothed the resulting multivariate"
        },
        {
          "problem of experimental framing that we experienced in that": "measures experimental design was followed. 39 Participants",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "time\nseries with\na\nSavitsky-Golay\nfilter\n(using\na\nsliding"
        },
        {
          "problem of experimental framing that we experienced in that": "were randomly assigned to one of\nthe two discussion topic",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "window of 11 frames and a polynomial order of 3). To test"
        },
        {
          "problem of experimental framing that we experienced in that": "groups,\naccording to which they conversed with the\nrobot",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "the the affects of smoothing and filtering on the results we"
        },
        {
          "problem of experimental framing that we experienced in that": "Pepper\n(SoftBank Robotics)\nvia Zoom video\nchats\nabout",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "treated smoothed/filtered and non-smoothed/filtered feature"
        },
        {
          "problem of experimental framing that we experienced in that": "general everyday topics (e.g., social\nrelationships, work-life",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "sets as separate in our\ninitial experiments."
        },
        {
          "problem of experimental framing that we experienced in that": "balance, health and well-being). One group’s\nconversation",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "Next, we extracted facial\nfeatures using an InceptionV1"
        },
        {
          "problem of experimental framing that we experienced in that": "topics were\nframed within\nthe\ncontext\nof\nthe COVID-19",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "ResNet\n[17]\n[18] architecture pretrained on the VGGFace2"
        },
        {
          "problem of experimental framing that we experienced in that": "pandemic\n(e.g.,\nsocial\nrelationships during the pandemic),",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "dataset\n[19]. VGGFace2 consists of 3.31 million images of"
        },
        {
          "problem of experimental framing that we experienced in that": "whereas the other group’s conversation topics were similar,",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "celebrity faces organized into 9131 subject categories with"
        },
        {
          "problem of experimental framing that we experienced in that": "except\nthat no explicit mention of\nthe COVID-19 pandemic",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "large variances in pose, age,\nillumination, and ethnicity. The"
        },
        {
          "problem of experimental framing that we experienced in that": "was ever made. Participants were scheduled to interact with",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "InceptionV1 ResNet\nthat we\nused\nscored\nan\naccuracy\nof"
        },
        {
          "problem of experimental framing that we experienced in that": "the\nrobot\ntwice\na week during prearranged times\nfor five",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "99.6% on this dataset. Pre-processing of\nthe video frames"
        },
        {
          "problem of experimental framing that we experienced in that": "weeks,\nresulting in 10 interactions in total. Each interaction",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "in this\ncase\nconsisted of\nextracting a 160x160 pixel\nsub-"
        },
        {
          "problem of experimental framing that we experienced in that": "consisted\nof\nthe\nrobot\nasking\nthe\nparticipant\n3\nquestions",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "region of each frame that contained pixel and feature-wise"
        },
        {
          "problem of experimental framing that we experienced in that": "(x3\nrepetitions),\nstarting with\na\ngeneric\nquestion\nto\nbuild",
          "rapport\n(e.g.,\nhow was\nyour week/weekend),\nfollowed\nby": "normalization\nof\nthe\nsubject’s\nface\n(an\nexample\nof\nthe"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "neural network based feature\nencoders\nand generates\ncon-": "textualised audio representations using a transformer model"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "[25]. We used a wav2vec2.0 model pretrained on 960 hours"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "of unlabelled audio data from the LibriSpeech dataset\n[26]."
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "To get\nthe feature sets for each wav file we took the outputs"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "from the models 12 transformer\nlayers which resulted in 12"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "t x 768 feature matrices where the value t was determined"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "by the number of\nframes in the audio file."
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "IV. DEEP LEARNING EXPERIMENTS"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "A.\nSupport Vector Machine Baselines"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "Since we were working with\na\nnovel\ndataset\ndesigned"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "specifically for our deep learning experiments we needed"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "some way of establishing a baseline that we were able to"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "compare our\nresults\nto. Following [27] we used Gaussian"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "kernel\nsupport vector machines\n(SVM)\ntrained on our\nex-"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "tracted audio and visual features separately to establish such"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "a baseline. For each feature type, a vector\nrepresenting the"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "mean over\nall\nframes\nin each example was\ncomputed and"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "the SVMs were\ntasked with classifying the\nself-disclosure"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "score for each interaction. Each model was trained using 3"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "fold cross validation and the average f1 score was used as a"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "means to measure the overall performance of each model."
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "The\nresults of\nthese baseline\nexperiments\n(illustrated in"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "Figure\n3)\nindicate\nthat\nthe\nfacial\nfeatures\nextracted\nusing"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "InceptionV1 pretrained on VGGFace2 were significantly the"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "most\ninformative for\nthe task while for\nthe audio features,"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "the MFCC representation was the most\ninformative. Overall"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "video features were the most useful feature sets in discrmiiti-"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "nating the self-disclosure score classes. The results also show"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "that\nthe problem is a difficult one given that\nthe best f1 score"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "measured was only 0.36. One surprising result was that\nthe"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "word2vec2.0 features performed so poorly. We hypothesised"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "that, given the strong relative performance of the InceptionV1"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "features, that word2vec2.0 would also perform relatively well"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "given\nthat\nboth models\nare\npretrained\non\nlarge\namounts"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "of\ntask\nrelevant\ndata. One\npossible\nexplanation\nof why"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "word2vec2.0 features performed so poorly in the baseline test"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "is with respect\nto how the mean vector\nfor each frame was"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "computed. The word2vec2.0 features for each frame were of"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "far higher dimension than both the MFCC features (12×tW ×"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "178 vs. 256 × tM ) and the visual\nfeature set of\nthe highest"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "dimensionality (InceptionV1 features\nat\ntI × 512)\n1. Thus"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "condensing the word2vec2.0 features\nacross both the\ntime"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "and attention-head dimensions into a single 178 dimensional"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "vector could have meant\nthat\ntoo much information was lost"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "leading to the feature dramatically losing its discriminative"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "ability with respect\nto the task."
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "B. Multimodal Attention Network"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "We\ndesigned\na multimodal\nattention\nnetwork\nthat\npro-"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "cesses the audio and visual features of each video in separate"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "streams\nand then combines\nthese\nrepresentations\nin a\nlate"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "fusion\nfashion\nbefore\nbeing\nclassified\nby\na\nlinear\nneural"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": ""
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "1Here tW , tM , and tI\nrefer\nto the time dimension of\nthe word2vec2.0"
        },
        {
          "neural network based feature\nencoders\nand generates\ncon-": "features,\nthe MFCC features, and the InceptionV1 features respectively."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "i"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "model\nthen receives a batch of size b of these tensors which"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "is then fed through the model’s audio stream."
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "The\nfirst\nstep\nof\nthe\naudio\nstream is\nto\nprocess\neach"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "of\nthe\nb × s feature\nsegments\nthrough a ResNet18 model"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "[18] pretrained on the\nImageNet dataset. This may sound"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "surprising given that we are using using an ImageNet\ntrained"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "model\non MFCC audio\nrepresentations\n(since\nImageNet"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "contains no MFCC examples) but\nresearch has\nshown that"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "using such ResNets on MFCC features matrices dependably"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "improves model scores [30] and indeed we also found this to"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "be the case in our experiments. We then take the output F A\nj"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "of\nthe fifth convolutional\nstack of\nthe pretrained ResNet18"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "model and perform spatial average pooling over\nthe feature"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "maps producing F A′\n(where j indexes over the feature matrix\nj"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "segments). This downsamples the output of the ResNet from"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "F A\n∈ Rs×h×w×c\nto F A′\n∈ Rs×c where s is\nthe number"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "j\nj"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "of\nsegments, h and w are\nthe\nheight\nand width\nof\nthe"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "feature maps respectively, and c is the number of channels,"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "creating a 1 × c length descriptor\nfor each of\nthe segments."
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "s × 1\nThe\ngoal\nis\nnow to\nlearn\nan\nlength\ndescriptor\nfor"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "the\naudio feature matrix segments where\nthe kth\nelement"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "of\nthe descriptor weights\nthe kth\nsegment according to its"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "importance in classifying the input\nsample. This descriptor"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "is learned using a convolutional stack that consists of a 1D"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "convolutional\nlayer,\na\nfully\nconnected\nlinear\nlayer,\nand\na"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "ReLU non-linearity such that:"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "H A = W A\n)T )T\n(1)\n1 (W A\n2 (F A′"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "s × s\n1 × c\nWhere W A\nand W A\nare\nand\nlearnable"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "1\n2"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "parameter matrices\nfor\nthe\nlinear\nand convolutional\nlayers"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "respectively. We\nthen compute\nthe\nactivation of\nthe\naudio"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "s × 1\nattention\nsubnetwork AA\ni.e.\nthe\nlength\nsegment"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "descriptor as:"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "AA = ReLU(H A)\n(2)"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "The\noutput\nembedding\nfor\nthe\naudio\nstream,\ni.e.\nthe"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "representation of which audio segments\nare most\nrelevant"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "to the classification of the input example to a particular self-"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "disclosure class,\nis computed via:"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "S(cid:88) j\nEA =\nF A′\n(3)\nj AA"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "=1"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "2) Visual Temporal Attention Subnetwork: The approach"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "to achieve the audio embedding EV\nfor\nthe visual\nfeatures"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "extracted from the videos\nfollows precisely the same steps"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": ""
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "as\nthe\naudio\ntemporal\nattention\nalgorithm. The\nprincipal"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "differences\nin\npractice\nare\nthat we\nuse\nthe\nInceptionV1"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "ResNet\narchitecture\ntrained\non VGGFace2\nthat we\nused"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "in our SVM baseline experiments\ninstead of\nthe ResNet18"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "model."
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "Given\nthe\noutput\nembeddings EA and EV\nfor\nthe\nau-"
        },
        {
          "∈ Rs× l\nthem on top of one another\nsuch that xA′\ns ×n. The": "dio and visual processing streams we\nthen summarize\nthe"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "streams. MFCC segments are fed through an ImageNet pretrained 2DResNet backbone before being average pooled, and cloned. One copy is then sent"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "through the attention subnetwork before being multiplied to the other ResNet output copy. This representation is then average pooled once again producing"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "the final audio embedding. The same process occurs with the frame input except\nthat\nthe backbone is a InceptionV1 ResNet architecture pretrained on"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "VGGFace2. The resulting audio and visual embeddings are then concatenated and fed through a linear classification layer. The network probabilities are"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "then used to compute the scale-preserving cross entropy loss by which the parameters of\nthe network are optimised."
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "change in this feature space from a 555 dimensional feature\nfeatures using average pooling by computing the mean of"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "vector\nto a\n67\ndimensional\nfeature\nvector\nfor\neach\nvideo\neach embedding vector along the time domain (i.e. across"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "frame.\nsegments) giving EA′\nand EV ′\n. These are then concatenated"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "before\nbeing\nfed\nto\na\nlinear\nlayer\ncontaining\n7\nneurons"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "2) Classification Vs. Regression:\nIn [15] we\nfound that"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "representing each of\nthe self-disclosure score classes. This"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "there was\na\nnuance\nin\nthe\napproach\nto\nclassifying\nself-"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "produces output ˆy:"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "disclosure scores. As we state in that study, participants rated"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "the degree of self-disclosure in their\ninteractions on a likert"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "scale between 1 and 7. This means that each score falls into"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "y = Softmax(W AV concat(EA, EV ))\n(4)"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "a discrete\nclass meaning that one plausible way to frame"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "the problem is as an n-class classification problem. However,\nwhere W AV\nis a learnable parameter matrix related to the"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "loss functions related to n-class classification problems often\nlinear output\nlayer, concat()\nis\nthe concatenation operation,"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "treat\nincorrect guesses\nin the same manner\ni.e.\nthere is no\nand Softmax() is the softmax function that return normalized"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "sense in which one guess can be numerically represented as\nprobabilities over\nthe seven self-disclosure score classes."
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "being closer to a correct guess than any of the other possible"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "C. Ablation Experiment Parameters\nguesses. The self-disclosure score data, however,\nis\nscaled"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "in the sense that a model guess of 2 for ground truth self-"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "In our experiments we tested the influence of two different"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "disclosure\nscore of 1 should be\ntreated as\na better guess"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "visual\nfeature\nsets,\ntwo\nexperimental\nframings,\nand\nfour"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "than 6 or 7. In this light an argument could be made that\nthe"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "different\nloss\nfunctions\nto determine the best configuration"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "problem is better represented as a regression problem. In [15]"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "for\nthe problem."
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "we found that\nframing the problem in both ways produced"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "1) Visual\nFeature\nSets:\nFirst, we wanted\nto\ntest\nthe"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "similar\nresults\nand as\nsuch no clear\nempirically informed"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "efficacy of just\nthe facial features output by the InceptionV1"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "decision could be made about what approach worked best. In"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "ResNet architecture pretrained on VGGFace2 as our SVM"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "light of this we decided to test the effects of both approaches"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "experiments\nshowed that\nthese were\nlikely to be\nthe most"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "on our\nresults."
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "informative visual\nfeatures for\nthe task. Next we wanted to"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "3) Loss Function: We wanted\nto\nstudy\nthe\neffect\nof\ntest\na\ncombination of\nall visual\nfeatures\nthat we\nextracted"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "loss\nfunction on the problem. Standardly,\nregression based\nas detailed in Section IV-A. To reduce the dimensionality of"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "methods minimize\na mean-squared\nerror\nloss\nin\norder\nto\nthis feature space we concatenated all of\nthe visual\nfeatures"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "optimize the parameters of a given model. Since we had no\ntogether after\nthe visual\ninput has been passed through the"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "good reason to suspect\nthat\nthis particular problem required\nResNetV1\nbackbone\nin\nthe\nvisual\nstream and\nperformed"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "principal components analysis with parameters set such that\nan alternative regression-based loss function we chose only"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "to\nbase\nour\nregression\nresults\non\nthe mean\nsquared\nerror\n99% of\nthe\nvariance\nin\nthe\ndata was\nexplained\nby\nthe"
        },
        {
          "Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nFig. 4:": "loss. For the classification version of the problem we chose a\nresulting feature matrix. This\nresulted in a dimensionality"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "categorical cross-entropy loss function for our experiments.": "For\nthis\nloss,\nresearch has\nshown that\nlabel\nsmoothing,\na",
          "prepared the training data as in [15] splitting the training and": "testing datasets into an 80/20 split and used weighted random"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "technique whereby standard ’hard’\nlabels are modified by a",
          "prepared the training data as in [15] splitting the training and": "sampling to account for imbalanced classes. Each model was"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "smoothing parameter α via yLS\n= yk(1 − α) + α",
          "prepared the training data as in [15] splitting the training and": "trained five\ntimes\nand the\naverage F1 score\nand standard"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "k\nK where k",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "indexes over the total number of classes (seven in the case of",
          "prepared the training data as in [15] splitting the training and": "deviation over all five training instances were computed to"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "this study), can drastically improve results [31]. As such we",
          "prepared the training data as in [15] splitting the training and": "give a balanced assessment of the model’s performance. We"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "chose to include a cross entropy loss with label\nsmoothing",
          "prepared the training data as in [15] splitting the training and": "chose\nto\nvalidate\nthe models\nusing\nf1\nscores\nso\nthat\nour"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "as part of\nablation study. Last, we wanted to explore\nthe",
          "prepared the training data as in [15] splitting the training and": "results were directly comparable to those produced by our"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "possibility of designing a custom loss function that was able",
          "prepared the training data as in [15] splitting the training and": "SVM experiments."
        },
        {
          "categorical cross-entropy loss function for our experiments.": "to strike a balance between the classification and regression",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "V. RESULTS"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "versions of\nthe task i.e. one that\nleveraged the fact\nthat\nthe",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "data was\ncategorical while\nalso preserving the notion that",
          "prepared the training data as in [15] splitting the training and": "The\nresults of our\nstudy are displayed in 5. We\nfound"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "certain guesses were better with respect\nto a ground truth",
          "prepared the training data as in [15] splitting the training and": "that all versions of\nthe multimodal attention network scored"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "label\nthan others. Taking inspiration from [29] we designed",
          "prepared the training data as in [15] splitting the training and": "significantly\nabove\nthe\nbest\nSVM baseline.\nInterestingly,"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "a custom cross entropy loss function that penalises guesses",
          "prepared the training data as in [15] splitting the training and": "departing\nfrom [15], where\nregression\nand\nclassification"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "with greater\nseverity the\nfurther\nthey are\nfrom the ground",
          "prepared the training data as in [15] splitting the training and": "models performed about as well as each other, we found that"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "truth label. For example,\nfor an input\nsample with labelled",
          "prepared the training data as in [15] splitting the training and": "a classification framing (treating self-disclosure scores as dis-"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "self-disclosure score of 7 a guess of 1 will result\nin a higher",
          "prepared the training data as in [15] splitting the training and": "crete classes) was\nsignificantly more effective at modelling"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "loss\nthan a guess of 2, a guess of 2 will\nresult\nin a higher",
          "prepared the training data as in [15] splitting the training and": "the problem than a regression framing (treating the scores as"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "loss than a guess of 3, and so on. To do this we amended the",
          "prepared the training data as in [15] splitting the training and": "being derived from the continuous number line). In all cases"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "standard cross-entropy loss function which can be expressed",
          "prepared the training data as in [15] splitting the training and": "we found that, within each experimental framing, the features"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "as:",
          "prepared the training data as in [15] splitting the training and": "derived\nfrom principle\ncomponents\nanalysis\noutperformed"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "models\ntrained\non\njust\nInceptionV1\nfacial\nfeatures. This"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "is\nperhaps\nunsurprising\nfor\ntwo\nreasons.\nFirstly,\nbecause"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "1 N\nN(cid:88) i\nC(cid:88) c\n(5)\nLCE = −\n1[c=yi] log pi,c",
          "prepared the training data as in [15] splitting the training and": "this\nfeature\nset was\ncomprised of\nthree\ntimes\nthe number"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "=1\n=1",
          "prepared the training data as in [15] splitting the training and": "of\nfeatures\nthan\nthe\npure\nInceptionV1\nfeature\nset\nbefore"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "it was\ncondensed\nto\nits\nprincipal\ncomponents.\nSecondly,"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "where N is the number of input samples, C is the number",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "because significantly reducing the number of\nfeatures (from"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "is an indicator variable that equals 1 when\nof classes, 1[c=yi]",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "512\nin\nthe\npure\nInceptionV1\ncase\nto\n67\nin\nthe\nprincipal"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "the\npredicted\nclass\nis\nthe\nsame\nas\nthe\nground\ntruth\nclass",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "components\ncase) would mean\nthat\nour model was\nless"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "ith\nis\nthe probability that\nthe\nsample belongs\nto\nand pi,c",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "susceptible to the curse of dimensionality i.e.\nthat\nit would"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "the cth class. We added a penalty term to 5 that\nformalizes",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "require much less data to effectively model\nthat smaller set"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "the idea that guesses at a greater distance from the ground",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "of features. Further, we found that\nlabel smoothing produced"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "truth should be penalised more severely. This gives what we",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "improvements\nin\nresults when\ncompared\nto\nthe\nnon-label"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "term the scale preserving cross-entropy loss:",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "smoothing variant of the cross entropy loss. Finally, we found"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "that our scale preserving cross-entropy loss outperformed all"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "but one version of\nthe model\n(principal component\nfeatures"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "1 N\nN(cid:88) i\nC(cid:88) c\n(1 + λ(|y − ˆy|)µ)\nLSP CE = −\n1[c=yi] log pi,c",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "with label smoothing cross-entropy loss) to which it equalled"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "=1\n=1",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "in performance."
        },
        {
          "categorical cross-entropy loss function for our experiments.": "(6)",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "VI. DISCUSSION AND CONCLUSION"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "where λ and µ are hyperparamters that change the degree",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "to which a incorrect guess is penalised with respect\nto how",
          "prepared the training data as in [15] splitting the training and": "Overall we report significant\nincreases on performance in"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "far away it\nis from the ground truth self-disclosure label.",
          "prepared the training data as in [15] splitting the training and": "this task from previous attempts (e.g., [15]). We hypothesise"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "Taken together,\nthe parameters of\nthe ablation study lead",
          "prepared the training data as in [15] splitting the training and": "that\nthis\nis\ndue\nto\na\nnumber\nof\nsignificant\ndevelopments"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "to eight different\ntraining configurations for\nthe multimodal",
          "prepared the training data as in [15] splitting the training and": "from that work. Firstly, we collected a much larger dataset"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "attention network,\nthe specifications of which are presented",
          "prepared the training data as in [15] splitting the training and": "meaning that\nthe models had more examples to learn from."
        },
        {
          "categorical cross-entropy loss function for our experiments.": "in Figure 5.",
          "prepared the training data as in [15] splitting the training and": "Second,\nin this case all\ninteractions were recorded between"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "the same interaction dyad (i.e. between a human and a Pepper"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "D. Model Training",
          "prepared the training data as in [15] splitting the training and": ""
        },
        {
          "categorical cross-entropy loss function for our experiments.": "",
          "prepared the training data as in [15] splitting the training and": "robot).\nIn [15], we collected interaction data between three"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "Regression and classification models were trained over 100",
          "prepared the training data as in [15] splitting the training and": "different dyads: human–human, human–embodied robot, and"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "epochs, while the SPCE models where trained on 150 since",
          "prepared the training data as in [15] splitting the training and": "human–voice agent. One reason that\nresults may have been"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "we\nfound that\nthey took longer\nto converge. All network",
          "prepared the training data as in [15] splitting the training and": "worse in that case is due to the possibility that vocal features"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "version we trained using the Adam optimizer [32], an initial",
          "prepared the training data as in [15] splitting the training and": "particular\nto each self-disclosure class may have been mod-"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "learning\nrate\nof\n0.01,\nand mini-batch\nsize\nof\n35. Audio",
          "prepared the training data as in [15] splitting the training and": "ulated by the kind of agent\nthe participant was\ninteracting"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "feature inputs were cropped to length l = 128 and divided",
          "prepared the training data as in [15] splitting the training and": "with. Since the interaction partner\nremained constant\nin our"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "in to s = 4 segments. Visual\ninput\nfeatures were cropped",
          "prepared the training data as in [15] splitting the training and": "study, variability was reduced, which could have simplified"
        },
        {
          "categorical cross-entropy loss function for our experiments.": "to l = 210 frames\nand divided into s = 7 segments. We",
          "prepared the training data as in [15] splitting the training and": "the\nlearning\ntask. Moreover, we\nused\nsignificantly more"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "information\nthat\nthey\nare\nsharing\nis\nnot worthy\nof\nthe"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "listener’s\nconsideration [35]. Conversely,\nassigning a very"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "high self-disclosure score in a situation where an interaction"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "partner does not believe themselves to be sharing a significant"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "amount of personal\ninformation could cause undue levels of"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "attention to be paid to a\nsituation which is not\nimportant"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "[36]. The issue described in both of\nthese cases would be"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "significantly\nconfounded within\nthe\ncontext\nof\nemotional"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "well being interventions, where the risks associated with not"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "picking up on a user’s\nself-disclosure related signals could"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "be very damaging. As such, considerably more work needs"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "to be done before models\nlike ours are considered for\nreal"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "world application. There\nare\nat\nleast\ntwo ways\nthat\nsteps"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "could be taken in this direction. Firstly,\nsignificantly more"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "data should be collected to improve the performance of\nthe"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "models. Secondly, a study should be carried out\nto asses the"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "differences between model performance and the performance"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "on the same task by a trained professional.\nIt\nis often the"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "case that\nthe quality of a machine learning model and it’s"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "viability as a real world application is measure with respect"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "to its ability to achieve ’human-like’ performance.\nIt makes"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "sense that a model\nthat\nis effective at recognizing the degree"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "to which a person is disclosing personal\ninformation should"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "be able to do so at\nleast as well as a trained professional"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "(particularly if\nthat model\nis\nto be implemented within the"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "context of health care interventions)."
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "Further,\nthere\nare ways\nin which improvements on our"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "approach might\nbe made\nin\nthe\nshort\nterm. Firstly,\nsince"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "we found that performance on the task was improved when"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "visual\nfeatures were combined using principal components"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "analysis,\nit’s likely to also be the case that performance im-"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "provements could be achieved by combining audio features."
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "In particular, we did not experiment with ways to combine"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "outputs from the transformer layers of wave2vec2.0 with the"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "MFCC features. Additionally, more empirical work could be"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "done\nto ascertain the best way to combine\nfeature\nsets\nin"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "both the audio and visual cases. For one such example, [27]"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "used a denoising autoencoder\nto learn a compressed latent"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "representation of\nthe concatenated input\nfeatures. A future"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "study should empirically test the hypothesis that such a latent"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "representation exists in a more effective input\nfeature space"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "than\nthe\none\nproduced\nby\nprincipal\ncomponents\nanalysis."
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "Further studies could also look into experimenting with other"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "kinds\nof\nattention.\nIn\n[29]\nthe\nauthors\nuse\nchannel-wise"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "attention and spatial\nattention in the visual\nstream on top"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "of\nthe frame-wise attention that both of our methods share."
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": ""
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "One development along these lines could be to implement"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "an attention mechanism that produces a descriptor over\nthe"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "features i.e. the columns of the input matrices. In this way the"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "model would hold a representation of not only which frames"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "of the input are important\nto its classification but also which"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "features are important. Lastly,\nthe model could be altered to"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "leverage 3D ResNets to produce higher dimensional features"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "over\nthe input video frames. This approach however would"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "require\na 3DResNet\ntrained on a very large video dataset"
        },
        {
          "feeling\nas\nif\nthey\nare\nbeing\nignored\nor\nthat\nthe\nsensitive": "focused on the modelling of human faces and,\nto our knowl-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "valid approach for inferring subjective self-disclosure in human-robot"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "from the modelling literature on self-disclosure [37],\nshow",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the 2022 ACM/IEEE International\ninteractions?,”\nin Proceedings of"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "that very good results on the task of\n(non-subjective)\nself-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Conference on Human-Robot\nInteraction, Hri\n’22, p. 991–996,\nIEEE"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "disclosure modelling between two human interactants can be",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Press, 2022."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[16]\nT. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “Openface"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "achieved multi-modally with the addition of lexical features.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "2.0: Facial behavior analysis toolkit,” in 2018 13th IEEE international"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "In that\nstudy,\nthe authors use a pretrained BERT language",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "conference\non\nautomatic\nface & gesture\nrecognition\n(FG 2018),"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "model\n[38]\nto extract\nfeatures\nrelated to the words used in",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 59–66,\nIeee, 2018."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[17] C. Szegedy, W. Liu, Y.\nJia, P. Sermanet, S. Reed, D. Anguelov,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "each utterance. A significant part of self-disclosure (at\nleast",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "D. Erhan, V. Vanhoucke,\nand A. Rabinovich,\n“Going\ndeeper with"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "in the human–human case)\nis\nthought\nto be communicated",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the\nIEEE conference on computer\nconvolutions,”\nin Proceedings of"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "verbally [33] [34]. This a future study could look at including",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "vision and pattern recognition, pp. 1–9, 2015."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "this modality in the HRI version of\nthe task.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "of\nthe\nIEEE conference\non\ncomputer\nrecognition,”\nin Proceedings"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "We believe that this study makes significant strides into the",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "vision and pattern recognition, pp. 770–778, 06 2016."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "new field of subjective self-disclosure modelling by showing",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[19] Q. Cao, L. Shen, W. Xie, O. M. Parkhi,\nand A. Zisserman,\n“Vg-"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "gface2: A dataset\nfor\nrecognising faces across pose and age,” CoRR,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "considerable\nimprovements\nover\nresults\nof\nany\nprevious",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "vol. abs/1710.08092, 2017."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "studies on the topic.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[20] K. Zhang, Z. Zhang, Z. Li,\nand Y. Qiao,\n“Joint\nface detection and"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "IEEE\nalignment\nusing multitask\ncascaded\nconvolutional\nnetworks,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "REFERENCES",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Signal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[21] N. Yang, N. Dey, R.\nS.\nSherratt,\nand\nF.\nShi,\n“Recognize\nbasic"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "the trans-\n[1]\nS. M. Jourard, Self-disclosure: An experimental analysis of",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "emotional statesin speech by machine learning techniques using mel-"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "parent self. Oxford, England: John Wiley, 1971.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "frequency cepstral coefficient features,” Journal of Intelligent & Fuzzy"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[2] H. Kreiner and Y. Levi-Belz, “Self-Disclosure Here and Now: Com-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Systems, vol. 39, no. 2, pp. 1925–1936, 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "bining Retrospective Perceived Assessment With Dynamic Behavioral",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[22] M. D. Pawar and R. D. Kokate, “Convolution neural network based"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Measures,” Frontiers in Psychology, vol. 10, p. 558, 2019.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "automatic speech emotion recognition using mel-frequency cepstrum"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[3]\nS. M. Jourard and P. Lasakow, “Some factors in self-disclosure,” The",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "coefficients,” Multimedia Tools\nand Applications,\nvol.\n80,\nno.\n10,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Journal of Abnormal and Social Psychology, vol. 56, no. 1, pp. 91–98,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 15563–15587, 2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "1958.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[23] U. Kumaran, S. Radha Rammohan, S. M. Nagarajan, and A. Prathik,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[4] A. Henschel, G. Laban,\nand E. S. Cross,\n“What Makes\na Robot",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "“Fusion of mel\nand gammatone\nfrequency cepstral\ncoefficients\nfor"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Social? A Review of Social Robots from Science Fiction to a Home",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "speech emotion recognition using deep c-rnn,” International Journal"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "or Hospital Near You,” Current Robotics Reports, no. 2, pp. 9–19,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "of Speech Technology, vol. 24, no. 2, pp. 303–314, 2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2021.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[24] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[5] N. Churamani, S. Kalkan,\nand H. Gunes,\n“Continual Learning for",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "framework for self-supervised learning of speech representations,” in"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Affective Robotics: Why, What and How?,” 29th IEEE International",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Advances\nin Neural\nInformation Processing Systems\n(H. Larochelle,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Conference on Robot and Human Interactive Communication, RO-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin,\neds.), vol. 33,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "MAN 2020, pp. 425–431, 8 2020.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 12449–12460, Curran Associates,\nInc., 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[6] M.\nSpitale, M. Axelsson,\nS.\nJeong,\nP.\nTuttosı, C. A.\nStamatis,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[25] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "G. Laban, A. Lim,\nand H. Gunes,\n“Past, Present,\nand Future: A",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Gomez, L. Kaiser,\nand\nI. Polosukhin,\n“Attention\nis\nall\nyou\nneed,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Survey of The Evolution of Affective Robotics For Well-being,” IEEE",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Proceedings\nof\nthe\n31st\nInternational\nConference\non\nNeural\nin"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Transactions in Affective Computing, 2025.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Information Processing\nSystems, Nips’17,\n(Red Hook, NY, USA),"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[7] G. Laban, A. Kappas, V. Morrison, and E. S. Cross, “Building Long-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "p. 6000–6010, Curran Associates Inc., 2017."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Term Human–Robot Relationships: Examining Disclosure, Percep-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "International\nJournal of Social\ntion and Well-Being Across Time,”",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "An asr corpus based on public domain audio books,” in 2015 IEEE"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Robotics, vol. 16, no. 5, pp. 1–27, 2024.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[8] G. Laban, V. Morrison, A. Kappas, and E. S. Cross, “Coping with",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "(ICASSP), pp. 5206–5210, 2015."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Emotional Distress\nvia Self-Disclosure\nto Robots: An\nIntervention",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[27] W. Lin,\nI. Orton, Q. Li, G. Pavarini, and M. Mahmoud, “Looking at"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "with Caregivers,” International Journal of Social Robotics, 2025.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the body: Automatic analysis of body gestures and self-adaptors\nin"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[9] G. Laban, A. Kappas, V. Morrison,\nand E. S. Cross,\n“Opening Up",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "psychological distress,”\nIEEE Transactions on Affective Computing,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "to Social Robots: How Emotions Drive Self-Disclosure Behavior,”",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "in 2023 32nd IEEE International Conference on Robot and Human",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[28]\nZ. Zhao, Q. Li, Z. Zhang, N. Cummins, H. Wang, J. Tao, and B. W."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Interactive Communication (RO-MAN),\n(Busan, Republic of Korea),",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Schuller, “Combining a parallel 2d cnn with a self-attention dilated"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "pp. 1697–1704,\nIEEE, 8 2023.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "residual network for ctc-based discrete speech emotion recognition,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[10] G. Laban and E. S. Cross, “Sharing our Emotions with Robots: Why",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Neural Networks, vol. 141, pp. 52–60, 2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "do we do it and how does\nit make us\nfeel?,” IEEE Transactions on",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[29]\nS. Zhao, Y. Ma, Y. Gu, J. Yang, T. Xing, P. Xu, R. Hu, H. Chai, and"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Affective Computing, 2024.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "K. Keutzer, “An end-to-end visual-audio attention network for emotion"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[11] N. L. Robinson, T. V. Cottier,\nand D.\nJ. Kavanagh,\n“Psychosocial",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "recognition\nin\nuser-generated\nvideos,” CoRR,\nvol.\nabs/2003.00832,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Health Interventions by Social Robots: Systematic Review of Random-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "ized Controlled Trials,” J Med Internet Res, vol. 21, no. 5, pp. 1–20,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[30] K. Palanisamy, D. Singhania, and A. Yao, “Rethinking cnn models for"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2019.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "audio classification,” arXiv preprint arXiv:2007.11154, 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[12] G. Laban, V. Morrison,\nand E. Cross,\n“Social\nrobots\nfor\nhealth",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[31]\nL. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, “Revisiting knowledge"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "psychology: A new frontier\nfor\nimproving human health and well-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the\ndistillation via label smoothing regularization,” in Proceedings of"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "being,” European Health Psychologist, vol. 23, pp. 1095–1102, 2 2024.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[13] G. Laban, S. Chiang, and H. Gunes, “What People Share With a Robot",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 3903–3911, 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "When Feeling Lonely and Stressed and How It Helps Over Time,” in",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[32] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2025 34th IEEE International Conference on Robot and Human In-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "International Conference on Learning Representations, 12 2014."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "teractive Communication (ROMAN), (Eindhoven, Netherlands), IEEE,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[33]\nP. C. Cozby,\n“Self-disclosure:\na\nliterature\nreview.,” Psychological"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2025.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "bulletin, vol. 79, no. 2, p. 73, 1973."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[14]\nE. S. Cross, R. Hortensius, and A. Wykowska, “From social brains to",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[34]\nJ. Omarzu,\n“A Disclosure Decision Model: Determining How and"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "social\nrobots: applying neurocognitive insights to human-robot\ninter-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "When Individuals Will Self-Disclose,” Pers Soc Psychol Rev, vol. 4,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "the Royal Society B: Biological\naction,” Philosophical Transactions of",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "no. 2, pp. 174–185, 2000."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Sciences, vol. 374, no. 1771, p. 20180024, 2019.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[35] N. L. Collins and L. C. Miller, “Self-disclosure and liking: a meta-"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "analytic review,” Psychological bulletin, vol. 116, pp. 457–475, 1994."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "valid approach for inferring subjective self-disclosure in human-robot"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "from the modelling literature on self-disclosure [37],\nshow",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the 2022 ACM/IEEE International\ninteractions?,”\nin Proceedings of"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "that very good results on the task of\n(non-subjective)\nself-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Conference on Human-Robot\nInteraction, Hri\n’22, p. 991–996,\nIEEE"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "disclosure modelling between two human interactants can be",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Press, 2022."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[16]\nT. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “Openface"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "achieved multi-modally with the addition of lexical features.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "2.0: Facial behavior analysis toolkit,” in 2018 13th IEEE international"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "In that\nstudy,\nthe authors use a pretrained BERT language",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "conference\non\nautomatic\nface & gesture\nrecognition\n(FG 2018),"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "model\n[38]\nto extract\nfeatures\nrelated to the words used in",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 59–66,\nIeee, 2018."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[17] C. Szegedy, W. Liu, Y.\nJia, P. Sermanet, S. Reed, D. Anguelov,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "each utterance. A significant part of self-disclosure (at\nleast",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "D. Erhan, V. Vanhoucke,\nand A. Rabinovich,\n“Going\ndeeper with"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "in the human–human case)\nis\nthought\nto be communicated",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the\nIEEE conference on computer\nconvolutions,”\nin Proceedings of"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "verbally [33] [34]. This a future study could look at including",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "vision and pattern recognition, pp. 1–9, 2015."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "this modality in the HRI version of\nthe task.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "of\nthe\nIEEE conference\non\ncomputer\nrecognition,”\nin Proceedings"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "We believe that this study makes significant strides into the",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "vision and pattern recognition, pp. 770–778, 06 2016."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "new field of subjective self-disclosure modelling by showing",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[19] Q. Cao, L. Shen, W. Xie, O. M. Parkhi,\nand A. Zisserman,\n“Vg-"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "gface2: A dataset\nfor\nrecognising faces across pose and age,” CoRR,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "considerable\nimprovements\nover\nresults\nof\nany\nprevious",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "vol. abs/1710.08092, 2017."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "studies on the topic.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[20] K. Zhang, Z. Zhang, Z. Li,\nand Y. Qiao,\n“Joint\nface detection and"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "IEEE\nalignment\nusing multitask\ncascaded\nconvolutional\nnetworks,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "REFERENCES",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Signal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[21] N. Yang, N. Dey, R.\nS.\nSherratt,\nand\nF.\nShi,\n“Recognize\nbasic"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "the trans-\n[1]\nS. M. Jourard, Self-disclosure: An experimental analysis of",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "emotional statesin speech by machine learning techniques using mel-"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "parent self. Oxford, England: John Wiley, 1971.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "frequency cepstral coefficient features,” Journal of Intelligent & Fuzzy"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[2] H. Kreiner and Y. Levi-Belz, “Self-Disclosure Here and Now: Com-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Systems, vol. 39, no. 2, pp. 1925–1936, 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "bining Retrospective Perceived Assessment With Dynamic Behavioral",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[22] M. D. Pawar and R. D. Kokate, “Convolution neural network based"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Measures,” Frontiers in Psychology, vol. 10, p. 558, 2019.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "automatic speech emotion recognition using mel-frequency cepstrum"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[3]\nS. M. Jourard and P. Lasakow, “Some factors in self-disclosure,” The",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "coefficients,” Multimedia Tools\nand Applications,\nvol.\n80,\nno.\n10,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Journal of Abnormal and Social Psychology, vol. 56, no. 1, pp. 91–98,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 15563–15587, 2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "1958.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[23] U. Kumaran, S. Radha Rammohan, S. M. Nagarajan, and A. Prathik,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[4] A. Henschel, G. Laban,\nand E. S. Cross,\n“What Makes\na Robot",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "“Fusion of mel\nand gammatone\nfrequency cepstral\ncoefficients\nfor"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Social? A Review of Social Robots from Science Fiction to a Home",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "speech emotion recognition using deep c-rnn,” International Journal"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "or Hospital Near You,” Current Robotics Reports, no. 2, pp. 9–19,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "of Speech Technology, vol. 24, no. 2, pp. 303–314, 2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2021.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[24] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[5] N. Churamani, S. Kalkan,\nand H. Gunes,\n“Continual Learning for",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "framework for self-supervised learning of speech representations,” in"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Affective Robotics: Why, What and How?,” 29th IEEE International",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Advances\nin Neural\nInformation Processing Systems\n(H. Larochelle,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Conference on Robot and Human Interactive Communication, RO-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin,\neds.), vol. 33,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "MAN 2020, pp. 425–431, 8 2020.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 12449–12460, Curran Associates,\nInc., 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[6] M.\nSpitale, M. Axelsson,\nS.\nJeong,\nP.\nTuttosı, C. A.\nStamatis,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[25] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "G. Laban, A. Lim,\nand H. Gunes,\n“Past, Present,\nand Future: A",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Gomez, L. Kaiser,\nand\nI. Polosukhin,\n“Attention\nis\nall\nyou\nneed,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Survey of The Evolution of Affective Robotics For Well-being,” IEEE",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Proceedings\nof\nthe\n31st\nInternational\nConference\non\nNeural\nin"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Transactions in Affective Computing, 2025.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Information Processing\nSystems, Nips’17,\n(Red Hook, NY, USA),"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[7] G. Laban, A. Kappas, V. Morrison, and E. S. Cross, “Building Long-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "p. 6000–6010, Curran Associates Inc., 2017."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Term Human–Robot Relationships: Examining Disclosure, Percep-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "International\nJournal of Social\ntion and Well-Being Across Time,”",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "An asr corpus based on public domain audio books,” in 2015 IEEE"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Robotics, vol. 16, no. 5, pp. 1–27, 2024.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[8] G. Laban, V. Morrison, A. Kappas, and E. S. Cross, “Coping with",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "(ICASSP), pp. 5206–5210, 2015."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Emotional Distress\nvia Self-Disclosure\nto Robots: An\nIntervention",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[27] W. Lin,\nI. Orton, Q. Li, G. Pavarini, and M. Mahmoud, “Looking at"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "with Caregivers,” International Journal of Social Robotics, 2025.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the body: Automatic analysis of body gestures and self-adaptors\nin"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[9] G. Laban, A. Kappas, V. Morrison,\nand E. S. Cross,\n“Opening Up",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "psychological distress,”\nIEEE Transactions on Affective Computing,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "to Social Robots: How Emotions Drive Self-Disclosure Behavior,”",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "in 2023 32nd IEEE International Conference on Robot and Human",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[28]\nZ. Zhao, Q. Li, Z. Zhang, N. Cummins, H. Wang, J. Tao, and B. W."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Interactive Communication (RO-MAN),\n(Busan, Republic of Korea),",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Schuller, “Combining a parallel 2d cnn with a self-attention dilated"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "pp. 1697–1704,\nIEEE, 8 2023.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "residual network for ctc-based discrete speech emotion recognition,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[10] G. Laban and E. S. Cross, “Sharing our Emotions with Robots: Why",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "Neural Networks, vol. 141, pp. 52–60, 2021."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "do we do it and how does\nit make us\nfeel?,” IEEE Transactions on",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[29]\nS. Zhao, Y. Ma, Y. Gu, J. Yang, T. Xing, P. Xu, R. Hu, H. Chai, and"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Affective Computing, 2024.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "K. Keutzer, “An end-to-end visual-audio attention network for emotion"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[11] N. L. Robinson, T. V. Cottier,\nand D.\nJ. Kavanagh,\n“Psychosocial",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "recognition\nin\nuser-generated\nvideos,” CoRR,\nvol.\nabs/2003.00832,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Health Interventions by Social Robots: Systematic Review of Random-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "ized Controlled Trials,” J Med Internet Res, vol. 21, no. 5, pp. 1–20,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[30] K. Palanisamy, D. Singhania, and A. Yao, “Rethinking cnn models for"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2019.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "audio classification,” arXiv preprint arXiv:2007.11154, 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[12] G. Laban, V. Morrison,\nand E. Cross,\n“Social\nrobots\nfor\nhealth",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[31]\nL. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, “Revisiting knowledge"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "psychology: A new frontier\nfor\nimproving human health and well-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "the\ndistillation via label smoothing regularization,” in Proceedings of"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "being,” European Health Psychologist, vol. 23, pp. 1095–1102, 2 2024.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[13] G. Laban, S. Chiang, and H. Gunes, “What People Share With a Robot",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "pp. 3903–3911, 2020."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "When Feeling Lonely and Stressed and How It Helps Over Time,” in",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[32] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2025 34th IEEE International Conference on Robot and Human In-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "International Conference on Learning Representations, 12 2014."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "teractive Communication (ROMAN), (Eindhoven, Netherlands), IEEE,",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[33]\nP. C. Cozby,\n“Self-disclosure:\na\nliterature\nreview.,” Psychological"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "2025.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "bulletin, vol. 79, no. 2, p. 73, 1973."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "[14]\nE. S. Cross, R. Hortensius, and A. Wykowska, “From social brains to",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[34]\nJ. Omarzu,\n“A Disclosure Decision Model: Determining How and"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "social\nrobots: applying neurocognitive insights to human-robot\ninter-",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "When Individuals Will Self-Disclose,” Pers Soc Psychol Rev, vol. 4,"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "the Royal Society B: Biological\naction,” Philosophical Transactions of",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "no. 2, pp. 174–185, 2000."
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "Sciences, vol. 374, no. 1771, p. 20180024, 2019.",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": ""
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "[35] N. L. Collins and L. C. Miller, “Self-disclosure and liking: a meta-"
        },
        {
          "edge, no such pretrained model\nis publicly available. Taking": "",
          "[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, “Is deep learning a": "analytic review,” Psychological bulletin, vol. 116, pp. 457–475, 1994."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "respondents are reluctant\nto steal\nthe spotlight, self-disclosers feel val-",
          "2019.": "J. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“BERT: Pre-"
        },
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "idated, understood, and cared for when respondents share comparable",
          "2019.": "training of deep bidirectional\ntransformers\nfor\nlanguage understand-"
        },
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "experiences,” Journal of Social and Personal Relationships, vol. 40,",
          "2019.": "the 2019 Conference of\nthe North American\ning,” in Proceedings of"
        },
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "pp. 3485–3514, 5 2023.",
          "2019.": "Chapter\nof\nthe Association\nfor Computational Linguistics: Human"
        },
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "[37] M. Soleymani, K. Stefanov, S.-H. Kang,\nJ. Ondras,\nand J. Gratch,",
          "2019.": "Language Technologies, Volume 1 (Long and Short Papers),\n(Min-"
        },
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "“Multimodal analysis and estimation of\nintimate self-disclosure,” in",
          "2019.": "neapolis, Minnesota), pp. 4171–4186, Association for Computational"
        },
        {
          "[36]\nZ. A. Reese\nand K. Orrach,\n“Reciprocal\nself-disclosure: Although": "2019 International Conference on Multimodal Interaction, pp. 59–68,",
          "2019.": "Linguistics, June 2019."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Self-disclosure: An experimental analysis of the transparent self",
      "authors": [
        "S Jourard"
      ],
      "year": "1971",
      "venue": "Self-disclosure: An experimental analysis of the transparent self"
    },
    {
      "citation_id": "2",
      "title": "Self-Disclosure Here and Now: Combining Retrospective Perceived Assessment With Dynamic Behavioral Measures",
      "authors": [
        "H Kreiner",
        "Y Levi-Belz"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "3",
      "title": "Some factors in self-disclosure",
      "authors": [
        "S Jourard",
        "P Lasakow"
      ],
      "year": "1958",
      "venue": "The Journal of Abnormal and Social Psychology"
    },
    {
      "citation_id": "4",
      "title": "What Makes a Robot Social? A Review of Social Robots from Science Fiction to a Home or Hospital Near You",
      "authors": [
        "A Henschel",
        "G Laban",
        "E Cross"
      ],
      "year": "2021",
      "venue": "Current Robotics Reports"
    },
    {
      "citation_id": "5",
      "title": "Continual Learning for Affective Robotics: Why, What and How?",
      "authors": [
        "N Churamani",
        "S Kalkan",
        "H Gunes"
      ],
      "venue": "29th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2020"
    },
    {
      "citation_id": "6",
      "title": "Past, Present, and Future: A Survey of The Evolution of Affective Robotics For Well-being",
      "authors": [
        "M Spitale",
        "M Axelsson",
        "S Jeong",
        "P Tuttosı",
        "C Stamatis",
        "G Laban",
        "A Lim",
        "H Gunes"
      ],
      "year": "2025",
      "venue": "IEEE Transactions in Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Building Long-Term Human-Robot Relationships: Examining Disclosure, Perception and Well-Being Across Time",
      "authors": [
        "G Laban",
        "A Kappas",
        "V Morrison",
        "E Cross"
      ],
      "year": "2024",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "8",
      "title": "Coping with Emotional Distress via Self-Disclosure to Robots: An Intervention with Caregivers",
      "authors": [
        "G Laban",
        "V Morrison",
        "A Kappas",
        "E Cross"
      ],
      "year": "2025",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "9",
      "title": "Opening Up to Social Robots: How Emotions Drive Self-Disclosure Behavior",
      "authors": [
        "G Laban",
        "A Kappas",
        "V Morrison",
        "E Cross"
      ],
      "venue": "2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)"
    },
    {
      "citation_id": "10",
      "title": "Sharing our Emotions with Robots: Why do we do it and how does it make us feel?",
      "authors": [
        "G Laban",
        "E Cross"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Psychosocial Health Interventions by Social Robots: Systematic Review of Randomized Controlled Trials",
      "authors": [
        "N Robinson",
        "T Cottier",
        "D Kavanagh"
      ],
      "year": "2019",
      "venue": "J Med Internet Res"
    },
    {
      "citation_id": "12",
      "title": "Social robots for health psychology: A new frontier for improving human health and wellbeing",
      "authors": [
        "G Laban",
        "V Morrison",
        "E Cross"
      ],
      "venue": "European Health Psychologist"
    },
    {
      "citation_id": "13",
      "title": "What People Share With a Robot When Feeling Lonely and Stressed and How It Helps Over Time",
      "authors": [
        "G Laban",
        "S Chiang",
        "H Gunes"
      ],
      "year": "2025",
      "venue": "2025 34th IEEE International Conference on Robot and Human Interactive Communication (ROMAN)"
    },
    {
      "citation_id": "14",
      "title": "From social brains to social robots: applying neurocognitive insights to human-robot interaction",
      "authors": [
        "E Cross",
        "R Hortensius",
        "A Wykowska"
      ],
      "year": "2019",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "15",
      "title": "Is deep learning a valid approach for inferring subjective self-disclosure in human-robot interactions?",
      "authors": [
        "H Powell",
        "G Laban",
        "J.-N George",
        "E Cross"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction, Hri '22"
    },
    {
      "citation_id": "16",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "17",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "20",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "21",
      "title": "Recognize basic emotional statesin speech by machine learning techniques using melfrequency cepstral coefficient features",
      "authors": [
        "N Yang",
        "N Dey",
        "R Sherratt",
        "F Shi"
      ],
      "year": "2020",
      "venue": "Journal of Intelligent & Fuzzy Systems"
    },
    {
      "citation_id": "22",
      "title": "Convolution neural network based automatic speech emotion recognition using mel-frequency cepstrum coefficients",
      "authors": [
        "M Pawar",
        "R Kokate"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "23",
      "title": "Fusion of mel and gammatone frequency cepstral coefficients for speech emotion recognition using deep c-rnn",
      "authors": [
        "U Kumaran",
        "S Radha Rammohan",
        "S Nagarajan",
        "A Prathik"
      ],
      "year": "2021",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "24",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, Nips'17"
    },
    {
      "citation_id": "26",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Looking at the body: Automatic analysis of body gestures and self-adaptors in psychological distress",
      "authors": [
        "W Lin",
        "I Orton",
        "Q Li",
        "G Pavarini",
        "M Mahmoud"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Combining a parallel 2d cnn with a self-attention dilated residual network for ctc-based discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "29",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "30",
      "title": "Rethinking cnn models for audio classification",
      "authors": [
        "K Palanisamy",
        "D Singhania",
        "A Yao"
      ],
      "year": "2020",
      "venue": "Rethinking cnn models for audio classification",
      "arxiv": "arXiv:2007.11154"
    },
    {
      "citation_id": "31",
      "title": "Revisiting knowledge distillation via label smoothing regularization",
      "authors": [
        "L Yuan",
        "F Tay",
        "G Li",
        "T Wang",
        "J Feng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "33",
      "title": "Self-disclosure: a literature review",
      "authors": [
        "P Cozby"
      ],
      "year": "1973",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "34",
      "title": "A Disclosure Decision Model: Determining How and When Individuals Will Self-Disclose",
      "authors": [
        "J Omarzu"
      ],
      "year": "2000",
      "venue": "Pers Soc Psychol Rev"
    },
    {
      "citation_id": "35",
      "title": "Self-disclosure and liking: a metaanalytic review",
      "authors": [
        "N Collins",
        "L Miller"
      ],
      "year": "1994",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "36",
      "title": "Reciprocal self-disclosure: Although respondents are reluctant to steal the spotlight, self-disclosers feel validated, understood, and cared for when respondents share comparable experiences",
      "authors": [
        "Z Reese",
        "K Orrach"
      ],
      "venue": "Journal of Social and Personal Relationships"
    },
    {
      "citation_id": "37",
      "title": "Multimodal analysis and estimation of intimate self-disclosure",
      "authors": [
        "M Soleymani",
        "K Stefanov",
        "S.-H Kang",
        "J Ondras",
        "J Gratch"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "38",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    }
  ]
}