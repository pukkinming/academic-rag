{
  "paper_id": "2411.10863v1",
  "title": "Improvement In Facial Emotion Recognition Using Synthetic Data Generated By Diffusion Model",
  "published": "2024-11-16T19:01:50Z",
  "authors": [
    "Arnab Kumar Roy",
    "Hemant Kumar Kathania",
    "Adhitiya Sharma"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Augmentation",
    "Stable Diffusion",
    "CNN",
    "SENets",
    "ResNets"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Emotion Recognition (FER) plays a crucial role in computer vision, with significant applications in humancomputer interaction, affective computing, and areas such as mental health monitoring and personalized learning environments. However, a major challenge in FER task is the class imbalance commonly found in available datasets, which can hinder both model performance and generalization. In this paper, we tackle the issue of data imbalance by incorporating synthetic data augmentation and leveraging the ResEmoteNet model to enhance the overall performance on facial emotion recognition task. We employed Stable Diffusion 2 and Stable Diffusion 3 Medium models to generate synthetic facial emotion data, augmenting the training sets of the FER2013 and RAF-DB benchmark datasets. Training ResEmoteNet with these augmented datasets resulted in substantial performance improvements, achieving accuracies of 96.47% on FER2013 and 99.23% on RAF-DB. These findings shows an absolute improvement of 16.68% in FER2013, 4.47% in RAF-DB and highlight the efficacy of synthetic data augmentation in strengthening FER models and underscore the potential of advanced generative models in FER research and applications. The source code for ResEmoteNet is available at https://github.com/ArnabKumarRoy02/ResEmoteNet",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial Emotion Recognition (FER) is a specialized image recognition task that focuses on identifying emotions from facial images or videos, which presents significant challenges due to the subtlety of facial expressions and the need for precise data collection and annotation. Despite these difficulties, FER plays a critical role in applications such as mental health monitoring, where it can detect conditions like depression and anxiety, as well as in human-computer interaction and educational systems.\n\nRecent advancements in FER have been driven by deep learning, particularly Convolutional Neural Networks (CNNs). For instance, in  [1]  an ensemble of networks with residual masking was introduced to improve FER accuracy, while in  [2]  MobileFaceNet was utilized  [3]  with a Dual Direction Attention Network (DDAN) to enhance feature extraction through attention maps. A dual-stream feature extraction method was proposed, combining an image backbone and facial landmark detector with a windowed cross-attention mechanism for improved computational efficiency  [4] . Similarly, Vision Transformers (ViTs) with Multi-View Complementary Prompters (MCPs) were utilized to integrate static and landmark-aware features, further enhancing system performance  [5] .\n\nAccurate emotion recognition in neural networks relies on well-annotated datasets, yet many freely available FER datasets face challenges such as class imbalance and small sample sizes, with classes like Happy and Neutral often being overrepresented. This issue is prevalent across various image recognition datasets and negatively impacts model performance. To mitigate this, synthetic data augmentation techniques have been introduced, with early popularity in GAN-based methods. GANs were used to enhance pneumonia classification by augmenting chest X-ray datasets with synthetic images, achieving a validation accuracy of 94.5%  [6] . A modified lightweight GAN model was similarly employed to generate high-quality synthetic images, effectively addressing class imbalance and leading to a classification accuracy of 99.06% for plastic bottle detection  [7] .\n\nRecently, diffusion models have become increasingly important for synthetic data generation. Stable Diffusion was utilized to create a skin disease dataset through controlled text prompts  [8] . Similarly, GLIDE, an open-source text-to-image diffusion model, was used to generate synthetic data, significantly improving classification across 17 diverse datasets  [9] . This approach demonstrated that combining synthetic data with minimal real data could achieve state-of-the-art performance and effectively pre-train large models. In FER, an augmentation method combining deep learning and genetic algorithms was introduced, focusing on enhancing the feature set from key-frame extractions rather than augmenting visual data, leading to improved performance  [10] .\n\nIn this paper, we utilize ResEmoteNet  [11] , a neural network architecture designed for facial emotion recognition (FER), incorporating Residual Connections and Squeeze-and-Excitation mechanisms to enhance feature extraction. Residual Connections help mitigate vanishing gradient issues, improving the training of deep models, while the Squeeze-and-Excitation mechanism refines feature maps by recalibrating channel-wise responses, allowing the network to focus on the most relevant facial image aspects. A significant challenge in FER is the class imbalance in datasets, where emotions like Happy and Neutral are overrepresented compared to Sad or Angry, leading to biased models. To address this, we introduce a method combining ResEmoteNet with diffusion-based data augmentation using Stable Diffusion  [12]  to generate synthetic images from text prompts. This approach creates a more balanced dataset, enhancing the model's generalization across emotional expressions. Our experiments show that this method not only resolves class imbalance but also significantly improves FER accuracy, setting a new performance benchmark.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Resemotenet Architecture",
      "text": "ResEmoteNet  [11]  has an extensive architecture consisting of Squeeze and Excitation blocks and Residual blocks. These blocks help in minimizing losses while training and are capable of learning complex features resulting in a model that helps in accurate classification of emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Loss Minimization",
      "text": "The model architecture for ResEmoteNet is shown in Fig.\n\nII, which consists of three parts, i.e., simple feature extraction from the CNN backbone, the Squeeze and Excitation Network (SENet) and the Residual Network for complex feature extraction. Given a sample X ∈ R H0×W0×3 with RGB facial image of size H 0 × W 0 (H 0 being the height of the facial image and W 0 being the width of the facial image), we utilize the CNN backbone to extract the simple features from the samples. The CNN backbone comprises of Convolution block accompanied by Batch Normalization to generate high-level feature maps of size H × W for each image. The spatial features f 0 ∈ R H×W ×C are then concatenated across channels to form a rich representation of the input image, which is further processed by the subsequent layers of the network. Subsequent to the extraction of high-level feature maps, a max-pooling layer is applied to further refine the feature representation. This layer reduces the spatial dimensions of the feature maps by retaining the most prominent features, thereby enhancing the model's efficiency and reducing the risk of overfitting.\n\nThe SE Block strengthens the network's ability to capture channel-wise features by applying a Global Average Pooling (GAP), condensing the spatial data which is following by a gating mechanism with Sigmoid activation to learn the attention weights. These weights modulate the spatial feature maps f 0 as described by: where w s represents the attention weights derived from the SE Block. This results in a new feature map f 1 ∈ R H×W ×C . It is noted that the temporal order of f 1 is in accordance with that of the input X.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Residual Feature Extraction",
      "text": "The Residual Network comprises three Residual Blocks, each with weight layers followed by ReLU activation and skip connections that iteratively learn residual functions. These functions model the differences between the block's input and output, rather than unreferenced mappings. The skip connections bypass layers within a block, helping to train deeper architectures by mitigating vanishing gradients. This design accelerates model convergence and enhances generalization by preserving the original input signal throughout the network. The operation within a Residual block can be defined as:\n\nwhere H(f 1 ) is the output of the stacked layers, and f 1 is the input. Additionally, the network incorporates Adaptive Average Pooling (AAP) to ensure consistent output dimensions, irrespective of input size, across different datasets. The final stage of the network produces a probability distribution over facial emotion classes, computed as:\n\nwhere Ỹfeats is the output from the AAP operation, f cls is the last layers of the architecture that gives the probability distribution over facial emotion classes and P out is the classification result.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Diffusion Based Data Augmentation",
      "text": "The class distribution in FER2013 and RAF-DB datasets varies significantly, leading to class imbalance. For instance, the Disgust class in FER2013 contains only 436 samples, while the Happy class has 7,215 samples. Similarly, in RAF-DB, the Fear class has just 281 samples compared to 4,772 samples in the Happy class. This imbalance causes the model to struggle with accurately classifying images from underrepresented classes, reducing the accuracy for those categories. To address this issue and balance the class distribution, we employ a generative diffusion model to generate additional facial images, as depicted in the pipeline shown in Fig.  2 .\n\nDiffusion Models: Diffusion models are generative models that create high-quality images from textual descriptions by iteratively denoising a random image, gradually evolving noise into realistic data distributions. In this experiment, we used two such models: Stable-Diffusion-2  [12]  and Stable-Diffusion-3-Medium  [13] . Stable-Diffusion-2 incorporates Progressive Distillation  [14]  for faster sampling, trained on the LAION dataset  [15]  for 550k steps at 256 × 256 resolution, and refined further to output 768 × 768 images. Stable-Diffusion-3-Medium, using the MM-DiT backbone built on Diffusion Transformers (DiTs)  [16] , also starts with lower-resolution images, fine-tuned on ImageNet  [17]  and CC12M  [18]  datasets for 500k steps with a batch size of 4096. Both models generate images from text prompts by tokenizing and emphasizing key terms, with synthetic data generated using a variety of prompts, some of which are listed in Table  I . Referring to the prompts listed in Table  I , we generate synthetic facial images as shown in Fig.  IV . These synthetic images were resized to 64 × 64 to align with the input requirements of the model and subsequently integrated into the training set of the original datasets: FER2013 and RAF-DB. The validation and testing sets remained unchanged. The model was then trained on the augmented datasets, and its performance was closely monitored.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Dataset And Experimental Setup",
      "text": "Datasets: We evaluate our proposed methodology using two datasets: FER2013  [19]  and RAF-DB  [20] . The FER2013 dataset was created for a facial expression recognition challenge at the ICML Workshop in Representation Learning and contains 35,887 grayscale images of size 48 × 48. It is divided into training (28,709 images), public test (3,589 images), and private test sets (3,589 images), with annotations for seven basic emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral. FER2013 includes diverse images, such as occluded faces, low contrast, and subjects with eyeglasses. Similarly, the RAF-DB dataset, created in 2017, includes around 30,000 facial images annotated by 40 crowdsourced labelers. This research focuses on the basic single-label subset, comprising 12,271 training samples and 3,068 testing samples, with 100 × 100 images in three color channels, categorized into the same seven emotions as FER2013. Data distributions for both datasets are shown in Table  II .\n\nData Processing: After the synthetic data were generated and added to the training sets of FER2013 and RAF-DB, each dataset was further divided into four augmentation subsets:  Training Configuration: We used the Stochastic Gradient Descent optimizer  [21]  with an initial learning rate of 10 -3 , along with a learning rate scheduler that reduced by a factor of 0.1 upon reaching a plateau, and a batch size of 16. Cross-Entropy Loss was employed as the cost function  [22] . ResEmoteNet was trained for up to 80 epochs with early stopping based on validation loss, with a patience of 5 epochs. The model converged before the maximum epochs on both datasets. Training with Augmentation 4 took 6.5 hours for FER2013 and 5 hours for RAF-DB, with inference times of 1.4 ms and 4.7 ms per image, respectively, using a batch size of 16. All training was conducted on a NVIDIA Tesla P100 GPU.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "Table  III  demonstrates the substantial improvements in model accuracy on both the FER2013 and RAF-DB datasets following various augmentations. On FER2013, the accuracy rose significantly from 79.79% to 96.47%, with notable gains in the Fear class (25% increase) and the Happy class (11% increase), leading to a perfect 100% accuracy in the latter. Similarly, the Sad and Neutral classes saw substantial improvements, further enhancing overall performance. On RAF-DB, the model's accuracy increased from an initial 94.76% to 99.23% after the final augmentation, with the Disgust class",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study highlights the importance of data augmentation in enhancing the performance of neural networks for facial emotion recognition. By employing advanced generative models like Stable Diffusion 2 and 3, we showed that synthetic data is instrumental in mitigating class imbalances and significantly boosting the overall robustness of neural networks. The significant improvements observed across both FER2013 and RAF-DB benchmarks validate the efficacy of our approach. Notably, ResEmoteNet's accuracy on the FER2013 dataset improved from 79.79% to 96.47% with augmentation, and on the RAF-DB dataset, the model's accuracy increased from 94.76% to 99.23%, marking substantial gains over the previous state-ofthe-art.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of ResEmoteNet for efficient facial emotion recognition.",
      "page": 2
    },
    {
      "caption": "Figure 2: Overall pipeline for data augmentation.",
      "page": 2
    },
    {
      "caption": "Figure 2: Diffusion Models: Diffusion models are generative models",
      "page": 2
    },
    {
      "caption": "Figure 3: Examples of synthetic images representing each emotion class in the",
      "page": 3
    },
    {
      "caption": "Figure 4: Confusion matrices for the performance of the model on (a) Original FER2013, (b) FER2013 with Augmentation 4 (15000 samples in each class),",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompts": "“A man’s face with surprise emotion”",
          "Keywords": "man, surprise"
        },
        {
          "Prompts": "“A face of\na woman in her 30’s\nexpressing disgust\nemotion”",
          "Keywords": "woman, 30’s, disgust"
        },
        {
          "Prompts": "“Happy expression on a kid,\nrealistic photo”",
          "Keywords": "happy, kid, realistic"
        },
        {
          "Prompts": "“An old man in his 80’s expressing neutral emotions\non his face”",
          "Keywords": "old man, 80’s, neutral"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classes": "",
          "Original": "FER2013",
          "Aug. 1": "FER2013",
          "Aug. 2": "Both",
          "Aug. 3": "Both",
          "Aug. 4": "Both"
        },
        {
          "Classes": "Anger",
          "Original": "3995",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        },
        {
          "Classes": "Disgust",
          "Original": "436",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        },
        {
          "Classes": "Fear",
          "Original": "4097",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        },
        {
          "Classes": "Happy",
          "Original": "7215",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        },
        {
          "Classes": "Neutral",
          "Original": "4965",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        },
        {
          "Classes": "Sad",
          "Original": "4830",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        },
        {
          "Classes": "Surprise",
          "Original": "3171",
          "Aug. 1": "7215",
          "Aug. 2": "10000",
          "Aug. 3": "12500",
          "Aug. 4": "15000"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "Accuracy in %": "FER2013"
        },
        {
          "Method": "LANMSFF [23]",
          "Accuracy in %": "70.44"
        },
        {
          "Method": "Local Learning Deep+BOW [24]",
          "Accuracy in %": "75.42"
        },
        {
          "Method": "Multi-Branch ViT [25]",
          "Accuracy in %": "77.85"
        },
        {
          "Method": "EmoNeXt\n[26]",
          "Accuracy in %": "76.12"
        },
        {
          "Method": "Ensemble ResMaskingNet\n[1]",
          "Accuracy in %": "76.82"
        },
        {
          "Method": "MixCut\n[27]",
          "Accuracy in %": "-"
        },
        {
          "Method": "DDAMFN++ [2]",
          "Accuracy in %": "-"
        },
        {
          "Method": "S2D [5]",
          "Accuracy in %": "-"
        },
        {
          "Method": "FMAE [28]",
          "Accuracy in %": "-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classes": "",
          "Dataset": "",
          "Accuracy (%)": "Original"
        },
        {
          "Classes": "Anger",
          "Dataset": "FER2013",
          "Accuracy (%)": "71"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "96"
        },
        {
          "Classes": "Disgust",
          "Dataset": "FER2013",
          "Accuracy (%)": "75"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "93"
        },
        {
          "Classes": "Fear",
          "Dataset": "FER2013",
          "Accuracy (%)": "70"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "90"
        },
        {
          "Classes": "Happy",
          "Dataset": "FER2013",
          "Accuracy (%)": "89"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "99"
        },
        {
          "Classes": "Neutral",
          "Dataset": "FER2013",
          "Accuracy (%)": "82"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "97"
        },
        {
          "Classes": "Sad",
          "Dataset": "FER2013",
          "Accuracy (%)": "71"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "96"
        },
        {
          "Classes": "Surprise",
          "Dataset": "FER2013",
          "Accuracy (%)": "81"
        },
        {
          "Classes": "",
          "Dataset": "RAF-DB",
          "Accuracy (%)": "98"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition using residual masking network",
      "authors": [
        "L Pham",
        "T Vu",
        "T Tran"
      ],
      "year": "2021",
      "venue": "2020 25Th international conference on pattern recognition (ICPR)"
    },
    {
      "citation_id": "2",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "S Zhang",
        "Y Zhang",
        "Y Zhang",
        "Y Wang",
        "Z Song"
      ],
      "year": "2023",
      "venue": "MDPI Electronics"
    },
    {
      "citation_id": "3",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition: 13th Chinese Conference"
    },
    {
      "citation_id": "4",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang"
      ],
      "year": "2023",
      "venue": "Poster++: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12149"
    },
    {
      "citation_id": "5",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Y Chen",
        "J Li",
        "S Shan",
        "M Wang",
        "R Hong"
      ],
      "year": "2023",
      "venue": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "arxiv": "arXiv:2312.05447"
    },
    {
      "citation_id": "6",
      "title": "Improved classification for pneumonia detection using transfer learning with gan based synthetic image augmentation",
      "authors": [
        "D Srivastav",
        "A Bajpai",
        "P Srivastava"
      ],
      "year": "2021",
      "venue": "2021 11th international conference on cloud computing, data science & engineering (confluence)"
    },
    {
      "citation_id": "7",
      "title": "Enhancement of image classification using transfer learning and gan-based synthetic data augmentation",
      "authors": [
        "S Chatterjee",
        "D Hazra",
        "Y.-C Byun",
        "Y.-W Kim"
      ],
      "year": "2022",
      "venue": "Mathematics"
    },
    {
      "citation_id": "8",
      "title": "Diffusion-based data augmentation for skin disease classification: Impact across original medical datasets to fully synthetic images",
      "authors": [
        "M Akrout",
        "B Gyepesi",
        "P Holló",
        "A Poór",
        "B Kincső",
        "S Solis",
        "K Cirone",
        "J Kawahara",
        "D Slade",
        "L Abid"
      ],
      "year": "2023",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention"
    },
    {
      "citation_id": "9",
      "title": "Is synthetic data from generative models ready for image recognition?",
      "authors": [
        "R He",
        "S Sun",
        "X Yu",
        "C Xue",
        "W Zhang",
        "P Torr",
        "S Bai",
        "X Qi"
      ],
      "year": "2022",
      "venue": "Is synthetic data from generative models ready for image recognition?",
      "arxiv": "arXiv:2210.07574"
    },
    {
      "citation_id": "10",
      "title": "Spatial deep feature augmentation technique for fer using genetic algorithm",
      "authors": [
        "N Nida",
        "M Yousaf",
        "A Irtaza",
        "S Javed",
        "S Velastin"
      ],
      "year": "2024",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "11",
      "title": "Resemotenet: Bridging accuracy and loss reduction in facial emotion recognition",
      "authors": [
        "A Roy",
        "H Kathania",
        "A Sharma",
        "A Dey",
        "M Ansari"
      ],
      "year": "2024",
      "venue": "Resemotenet: Bridging accuracy and loss reduction in facial emotion recognition",
      "arxiv": "arXiv:2409.10545"
    },
    {
      "citation_id": "12",
      "title": "Highresolution image synthesis with latent diffusion models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser",
        "B Ommer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "Scaling rectified flow transformers for high-resolution image synthesis",
      "authors": [
        "P Esser",
        "S Kulal",
        "A Blattmann",
        "R Entezari",
        "J Müller",
        "H Saini",
        "Y Levi",
        "D Lorenz",
        "A Sauer",
        "F Boesel"
      ],
      "year": "2024",
      "venue": "Forty-first International Conference on Machine Learning"
    },
    {
      "citation_id": "14",
      "title": "Progressive distillation for fast sampling of diffusion models",
      "authors": [
        "T Salimans",
        "J Ho"
      ],
      "year": "2022",
      "venue": "Progressive distillation for fast sampling of diffusion models",
      "arxiv": "arXiv:2202.00512"
    },
    {
      "citation_id": "15",
      "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
      "authors": [
        "C Schuhmann",
        "R Vencu",
        "R Beaumont",
        "R Kaczmarczyk",
        "C Mullis",
        "A Katta",
        "T Coombes",
        "J Jitsev",
        "A Komatsuzaki"
      ],
      "year": "2021",
      "venue": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
      "arxiv": "arXiv:2111.02114"
    },
    {
      "citation_id": "16",
      "title": "Scalable diffusion models with transformers",
      "authors": [
        "W Peebles",
        "S Xie"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "18",
      "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
      "authors": [
        "S Changpinyo",
        "P Sharma",
        "N Ding",
        "R Soricut"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Challenges in representation learning",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "20",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceed-ings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "An overview of gradient descent optimization algorithms",
      "authors": [
        "S Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms",
      "arxiv": "arXiv:1609.04747"
    },
    {
      "citation_id": "22",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "A lightweight attention-based deep network via multi-scale feature fusion for multi-view facial expression recognition",
      "authors": [
        "A Ezati",
        "M Dezyani",
        "R Rana",
        "R Rajabi",
        "A Ayatollahi"
      ],
      "year": "2024",
      "venue": "A lightweight attention-based deep network via multi-scale feature fusion for multi-view facial expression recognition",
      "arxiv": "arXiv:2403.14318"
    },
    {
      "citation_id": "24",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "Cross-task multi-branch vision transformer for facial expression and mask wearing classification",
      "authors": [
        "A Zhu",
        "K Li",
        "T Wu",
        "P Zhao",
        "W Zhou",
        "B Hong"
      ],
      "year": "2024",
      "venue": "Cross-task multi-branch vision transformer for facial expression and mask wearing classification",
      "arxiv": "arXiv:2404.14606"
    },
    {
      "citation_id": "26",
      "title": "Emonext: an adapted convnext for facial emotion recognition",
      "authors": [
        "Y Boudouri",
        "A Bohi"
      ],
      "year": "2023",
      "venue": "2023 IEEE 25th International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Mixcut: A data augmentation method for facial expression recognition",
      "authors": [
        "J Yu",
        "Y Liu",
        "R Fan",
        "G Sun"
      ],
      "year": "2024",
      "venue": "Mixcut: A data augmentation method for facial expression recognition",
      "arxiv": "arXiv:2405.10489"
    },
    {
      "citation_id": "28",
      "title": "Representation learning and identity adversarial training for facial behavior understanding",
      "authors": [
        "M Ning",
        "A Salah",
        "I Ertugrul"
      ],
      "year": "2024",
      "venue": "Representation learning and identity adversarial training for facial behavior understanding",
      "arxiv": "arXiv:2407.11243"
    }
  ]
}