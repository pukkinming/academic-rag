{
  "paper_id": "2105.12907v2",
  "title": "Directed Acyclic Graph Network For Conversational Emotion Recognition",
  "published": "2021-05-27T01:51:37Z",
  "authors": [
    "Weizhou Shen",
    "Siyue Wu",
    "Yunyi Yang",
    "Xiaojun Quan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC 1 , to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Utterance-level emotion recognition in conversation (ERC) is an emerging task that aims to identify the emotion of each utterance in a conversation. This task has been recently concerned by a considerable number of NLP researchers due to its potential applications in several areas, such as opinion mining in social media  (Chatterjee et al., 2019)  and building an emotional and empathetic dialog system  (Majumder et al., 2020) .\n\nThe emotion of a query utterance is likely to be influenced by many factors such as the utterances spoken by the same speaker and the surrounding conversation context. Indeed, how to model the conversational context lies at the heart of this task  (Poria et al., 2019a) . Empirical evidence also shows that a good representation of conversation context significantly contributes to the model performance, especially when the content of query utterance is too short to be identified alone  (Ghosal et al., 2019) .\n\nNumerous efforts have been devoted to the modeling of conversation context. Basically, they can be divided into two categories: graph-based methods  (Zhang et al., 2019a; Ghosal et al., 2019; Zhong et al., 2019; Ishiwatari et al., 2020; Shen et al., 2020)  and recurrence-based methods  (Hazarika et al., 2018a; Hazarika et al., 2018b; Majumder et al., 2019; Ghosal et al., 2020) . For the graphbased methods, they concurrently gather information of the surrounding utterances within a certain window, while neglecting the distant utterances and the sequential information. For the recurrencebased methods, they consider the distant utterances and sequential information by encoding the utterances temporally. However, they tend to update the query utterance's state with only relatively limited information from the nearest utterances, making them difficult to get a satisfying performance.\n\nAccording to the above analysis, an intuitively better way to solve ERC is to allow the advantages of graph-based methods and recurrence-based models to complement each other. This can be achieved by regarding each conversation as a directed acyclic graph (DAG). As illustrated in Figure  1 , each utterance in a conversation only receives information from some previous utterances and cannot propagate information backward to itself and its predecessors through any path. This characteristic indicates that a conversation can be regarded as a DAG. Moreover, by the information flow from predecessors to successors through edges, DAG can gather information for a query utterance from both the neighboring utterances and the remote utterances, which acts like a combination of graph structure and recurrence structure. Thus, we speculate that DAG is a more appropriate and reasonable way than graph-based structure and recurrence-based structure to model the conversation context in ERC.\n\nIn this paper, we propose a method to model the conversation context in the form of DAG. Firstly, rather than simply connecting each utterance with a fixed number of its surrounding utterances to build a graph, we propose a new way to build a DAG from the conversation with constraints on speaker identity and positional relations. Secondly, inspired by DAGNN  (Thost and Chen, 2021) , we propose a directed acyclic graph neural network for ERC, namely DAG-ERC. Unlike the traditional graph neural networks such as  GCN (Kipf and Welling, 2016)  and GAT  (Veličković et al., 2017 ) that aggregate information from the previous layer, DAG-ERC can recurrently gather information of predecessors for every utterance in a single layer, which enables the model to encode the remote context without having to stack too many layers. Besides, in order to be more applicable to the ERC task, our DAG-ERC has two improvements over DAGNN:\n\n(1) a relation-aware feature transformation to gather information based on speaker identity and (2) a contextual information unit to enhance the information of historical context. We conduct extensive experiments on four ERC benchmarks and the results show that the proposed DAG-ERC achieves comparable performance with the state-of-the-art models. Furthermore, several studies are conducted to explore the effect of the proposed DAG structure and the modules of DAG-ERC.\n\nThe contributions of this paper are threefold. First, we are the first to consider a conversation as a directed acyclic graph in the ERC task. Second, we propose a method to build a DAG from a conversation with constraints based on the speaker identity and positional relations. Third, we propose a directed acyclic graph neural network for ERC, which takes DAGNN as its backbone and has two main improvements designed specifically for ERC.\n\n2 Related work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Recently, several ERC datasets with textual data have been released  (Busso et al., 2008; Schuller et al., 2012; Zahiri and Choi, 2017; Li et al., 2017; Chen et al., 2018; Poria et al., 2019b) , arousing the widespread interest of NLP researchers. In the following paragraphs, we divide the related works into two categories according to the methods they use to model the conversation context. Graph-based Models DialogGCN  (Ghosal et al., 2019)  treats each dialog as a graph in which each utterance is connected with the surrounding utterances. RGAT  (Ishiwatari et al., 2020)  adds positional encodings to DialogGCN. ConGCN  (Zhang et al., 2019a)  regards both speakers and utterances as graph nodes and makes the whole ERC dataset a single graph. KET  (Zhong et al., 2019)  uses hierarchical Transformers  (Vaswani et al., 2017)  with external knowledge. DialogXL  (Shen et al., 2020)  improves XLNet  (Yang et al., 2019)  with enhanced memory and dialog-aware self-attention.  2 Recurrence-based Models In this category, ICON  (Hazarika et al., 2018a)  and CMN  (Hazarika et al., 2018b)  both utilize gated recurrent unit (GRU) and memory networks. HiGRU  (Jiao et al., 2019)  contains two GRUs, one for utterance encoder and the other for conversation encoder. DialogRNN  (Majumder et al., 2019 ) is a recurrence-based method that models dialog dynamics with several RNNs. COSMIC  (Ghosal et al., 2020)  is the latest model, which adopts a network structure very close to DialogRNN and adds external commonsense knowledge to improve performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Directed Acyclic Graph Neural Network",
      "text": "Directed acyclic graph is a special type of graph structure that can be seen in multiple areas, for example, the parsing results of source code  (Allamanis et al., 2018)  and logical formulas  (Crouse et al., 2019) . A number of neural networks that employ DAG architecture have been proposed, such as Tree-LSTM  (Tai et al., 2015) , DAG-RNN  (Shuai et al., 2016) , D-VAE  (Zhang et al., 2019b) , and DAGNN  (Thost and Chen, 2021) . DAGNN is different from the previous DAG models in the model structure. Specifically, DAGNN allows multiple layers to be stacked, while the others have only one single layer. Besides, instead of merely carrying out naive sum or element-wise product on the predecessors' representations, DAGNN conducts information aggregation using graph attention.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "In ERC, a conversation is defined as a sequence of utterances {u 1 , u 2 , ..., u N }, where N is the number of utterances. Each utterance u i consists of n i tokens, namely u i = {w i1 , w i2 , ..., w in i }. A discrete value y i ∈ S is used to denote the emotion label of u i , where S is the set of emotion labels. The speaker identity is denoted by a function p(•). For example, p(u i ) ∈ P denotes the speaker of u i and P is the collection of all speaker roles in an ERC dataset. The objective of this task is to predict the emotion label y t for a given query utterance u t based on dialog context {u 1 , u 2 , ..., u N } and the corresponding speaker identity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Building A Dag From A Conversation",
      "text": "We design a directed acyclic graph (DAG) to model the information propagation in a conversation. A DAG is denoted by G = (V, E, R). In this paper, the nodes in the DAG are the utterances in the conversation, i.e., V = {u 1 , u 2 , ..., u N }, and the edge (i, j, r ij ) ∈ E represents the information propagated from u i to u j , where r ij ∈ R is the relation type of the edge. The set of relation types of edges, R = {0, 1}, contains two types of relation: 1 for that the two connected utterances are spoken by the same speaker, and 0 for otherwise.\n\nWe impose three constraints to decide when an utterance would propagate information to another, i.e., when two utterances are connected in the DAG: Direction: ∀j > i, (j, i, r ji ) / ∈ E. A previous utterance can pass message to a future utterance, but a future utterance cannot pass message backwards. Remote information: ∃τ < i, p(u τ ) = p(u i ), (τ, i , r τ i ) ∈ E and ∀j < τ, (j, i, r ji ) / ∈ E. For each utterance u i except the first one, there is a previous utterance u τ that is spoken by the same speaker as Algorithm 1 Building a DAG from a Conversation\n\nwhile τ > 0 and c < ω do\n\nend while 16: end for 17: return G = (V, E, R) u i . The information generated before u τ is called remote information, which is relatively less important. We assume that when the speaker speaks u τ , she/he has been aware of the remote information before u τ . That means, u τ has included the remote information and it will be responsible for propagating the remote information to u i . Local information: ∀l, τ < l < i, (l, i, r li ) ∈ E. Usually, the information of the local context is important. Consider u τ and u i defined in the second constraint. We assume that every utterance u l in between u τ and u i contains local information, and they will propagate the local information to u i .\n\nThe first constraint ensures the conversation to be a DAG, and the second and third constraints indicate that u τ is the cut-off point of remote and local information. We regard u τ as the ω-th latest utterance spoken by p(u i ) before u i , where ω is a hyper-parameter. Then for each utterance u l in between u τ and u i , we make a directed edge from u l to u i . We show the above process of building a DAG in Algorithm 1.\n\nAn example of the DAG is shown in Figure  2 . In general, our DAG has two main advancements compared to the graph structures developed in previous works  (Ghosal et al., 2019; Ishiwatari et al., 2020) : First, our DAG doesn't have edges from future utterances to previous utterances, which we argue is more reasonable and realistic, as the emotion of a query utterance should not be influenced by the future utterances in practice. Second, our DAG seeks a more meaningful u τ for each utterance, rather than simply connecting each utterance with a fixed number of surrounding utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Directed Acyclic Graph Neural Network",
      "text": "In this section, we introduce the proposed Directed Acyclic Graph Neural Network for ERC (DAG-ERC). The framework is shown in Figure  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Utterance Feature Extraction",
      "text": "DAG-ERC regards each utterance as a graph node, the feature of which can be extracted by a pretrained Transformer-based language model. Following the convention, the pre-trained language model is firstly fine-tuned on each ERC dataset, and its parameters are then frozen while training DAG-ERC. Following  Ghosal et al. (2020) , we employ RoBERTa-Large  (Liu et al., 2019) , which has the same architecture as BERT-Large  (Devlin et al., 2018) , as our feature extractor. More specifically, for each utterance u i , we prepend a special token [CLS] to its tokens, making the input a form of {[CLS], w i1 , w i2 , ..., w in i }. Then, we use the [CLS]'s pooled embedding at the last layer as the feature representation of u i .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Gnn, Rnn And Dagnn",
      "text": "Before introducing the DAG-ERC layers in detail, we first briefly describe graph-based models, recurrence-based models and directed acyclic graph models to help understand their differences.\n\nFor each node at each layer, graph-based models (GNN) aggregate the information of its neighboring nodes at the previous layer as follows:\n\nwhere f (•) is the information processing function, Aggregate(•) is the information aggregation function to gather information from neighboring nodes, and N i denotes the neighbours of the i-th node.\n\nRecurrence-based models (RNN) allow information to propagate temporally at the same layer, while the i-th node only receives information from the (i-1)-th node:\n\nDirected acyclic graph models (DAGNN) work like a combination of GNN and RNN. They aggregate information for each node in temporal order, and allow all nodes to gather information from neighbors and update their states at the same layer:\n\nThe strength of applying DAGNN to ERC is relatively apparent: By allowing information to propagate temporally at the same layer, DAGNN can get access to distant utterances and model the information flow throughout the whole conversation, which is hardly possible for GNN. Besides, DAGNN gathers information from several neighboring utterances, which sounds more appealing than RNN as the latter only receives information from the (i-1)-th utterance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dag-Erc Layers",
      "text": "Our proposed DAG-ERC is primarily inspired by DAGNN  (Thost and Chen, 2021) , with novel improvements specially made for emotion recognition in conversation. At each layer l of DAG-ERC, due to the temporal information flow, the hidden state of utterances should be computed recurrently from the first utterance to the last one.\n\nFor each utterance u i , the attention weights between u i and its predecessors are calculated by using u i 's hidden state at the (l -1)-th layer to attend to the predecessors' hidden states at l-th layer:\n\nwhere W l α are trainable parameters and denotes the concatenation operation.\n\nThe information aggregation operation in DAG-ERC is different from that in DAGNN. Instead of merely gathering information according to the attention weights, inspired by R-GCN  (Schlichtkrull et al., 2018) , we apply a relation-aware feature transformation to make full use of the relational type of edges:\n\nwhere W l r ij ∈ {W l 0 , W l 1 } are trainable parameters for the relation-aware transformation.\n\nAfter the aggregated information M l i is calculated, we make it interact with u i 's hidden state at the previous layer H l-1 i to obtain the final hidden state of u i at the current layer. In DAGNN, the final hidden state is obtained by allowing M l i to control information propagation of H l-1 i to the l-th layer with a gated recurrent unit (GRU):\n\nwhere H l-1 i , M l i , and H l i are the input, hidden state and output of the GRU, respectively.\n\nWe refer to the process in Equation 6 as nodal information unit, because it focuses on the node information propagating from the past layer to the current layer. Nodal information unit may be suitable for the tasks that DAGNN is originally designed to solve. However, we find that only using nodal information unit is not enough for ERC, especially when the query utterance u i 's emotion should be derived from its context. The reason is that in DAGNN, the information of context M l i is only used to control the propagation of u i 's hidden state, and under this circumstance, the information of context is not fully leveraged. Therefore, we design another GRU called contextual information unit to model the information flow of historical context through a single layer. In the contextual information unit, the roles of H i-1 i and M l i in GRU are reversed, i.e., H i-1 i controls the propagation of M l i :\n\nThe representation of u i at the l-th layer is the sum of H l i and C l i :",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training And Prediction",
      "text": "We take the concatenation of u i 's hidden states at all DAG-ERC layers as the final representation of u i , and pass it through a feed-forward neural network to get the predicted emotion:\n\nFor the training of DAG-ERC, we employ the standard cross-entropy loss as objective function:\n\nwhere M is the number of training conversations, N i is the number of utterances in the i-th conversation, y i,t is the ground truth label, and θ is the collection of trainable parameters of DAG-ERC. We utilize only the textual modality of the above datasets for the experiments. For evaluation metrics, we follow  Ishiwatari et al. (2020)  and  Shen et al. (2020)  and choose micro-averaged F1 excluding the majority class (neutral) for DailyDialog and weighted-average F1 for the other datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Compared Methods",
      "text": "We compared our model with the following baselines in our experiments: Recurrence-based methods: DialogueRNN  (Majumder et al., 2019) , DialogRNN-RoBERTa  (Ghosal et al., 2020) , and COSMIC without external knowledge 3    (Ghosal et al., 2020) . Graph-based methods: DialogurGCN  (Ghosal et al., 2019) , KET  (Zhong et al., 2019) , DialogXL  (Shen et al., 2020)  and RGAT  (Ishiwatari et al., 2020) . Feature extractor: RoBERTa  (Liu et al., 2019) . Previous models with our extracted features: DialogueGCN-RoBERTa, RGAT-RoBERTa and DAGNN  (Thost and Chen, 2021)    4  . Ours: DAG-ERC.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Overall Performance",
      "text": "The overall results of all the compared methods on the four datasets are reported in Table  2 . We can note from the results that our proposed DAG-ERC achieves competitive performances across the four datasets and reaches a new state of the art on the IEMOCAP, DailyDialog and EmoryNLP datasets.\n\nAs shown in the table, when the feature extracting method is the same, graph-based models generally outperform recurrence-based models on IEMOCAP, DailyDialog, and EmoryNLP. This phenomenon indicates that recurrence-based models cannot encode the context as effectively as graphbased models, especially for the more important local context. What's more, we see a significant improvement of DAG-ERC over the graph-based models on IEMOCAP, which demonstrates DAG-ERC's superior ability to capture remote information given that the dialogs in IEMOCAP are much longer (almost 70 utterances per dialog).\n\nOn MELD, however, we observe that neither graph-based models nor our DAG-ERC outperforms the recurrence-based models. After going through the data, we find that due to the data collection method (collected from TV shows), sometimes two consecutive utterances in MELD are not coherent. Under this circumstance, graph-based models' advantage in encoding context is not that important.\n\nBesides, the graph-based models see considerable improvements when implemented with the powerful feature extractor RoBERTa. In spite of this, our DAG-ERC consistently outperforms these improved graph-based models and DAGNN, confirming the superiority of the DAG structure and the effectiveness of the improvements we make to build DAG-ERC upon DAGNN.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Variants Of Dag Structure",
      "text": "In this section, we investigate how the structure of DAG would affect our DAG-ERC's performance by applying different DAG structures to DAG-ERC. In addition to our proposed structure, we further define three kinds of DAG structure: (1) sequence, in which utterances are connected one by one; (2) DAG with single local information, in which each utterance only receives local information from its nearest neighbor, and the remote information remains the same as our DAG; (3) common DAG, in which each utterance is connected with κ previous utterances. Note that if there are only two speakers taking turns to speak in a dialog, then our DAG is equivalent to common DAG with κ = 2ω, making the comparison less meaningful. Therefore, we conduct the experiment on EmoryNLP, where there are usually multiple speakers in one dialog, and the speakers speak in arbitrary order. The test performances are reported in Table  3 , together with the average number of each utterance's predecessors.\n\nSeveral instructive observations can be made from the experimental results. Firstly, the performance of DAG-ERC drops significantly when equipped with the sequence structure. Secondly, our proposed DAG structure has the highest performance among the DAG structures. Considering our DAG with ω = 2 and common DAG with κ = 6, with very close numbers of predecessors, our DAG still outperforms the common DAG by a certain margin. This indicates that the constraints based on speaker identity and positional relation are effective inductive biases, and the structure of our DAG is more suitable for the ERC task than rigidly connecting each utterance with a fixed number of predecessors. Finally, we find that increasing the value of ω may not contribute to the performance of our DAG, and ω = 1 tends to be enough.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To study the impact of the modules in DAG-ERC, we evaluate DAG-ERC by removing relation-aware feature transformation, the nodal information unit, and the contextual information unit individually. The results are shown in Table  4 .\n\nAs shown in the  in IEMOCAP and DailyDialog, and there are usually more than two speakers in dialogs of MELD and EmoryNLP. Therefore, we can infer that the relation of whether two utterances have the same speaker is sufficient for two-speaker dialogs, while falls short in the multi-speaker setting.\n\nMoreover, we find that on each dataset, the performance drop caused by ablating nodal information unit is similar to contextual information unit, and all these drops are not that critical. This implies that either the nodal information unit or contextual information unit is effective for the ERC task, while combining the two of them can yield further performance improvement.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Number Of Dag-Erc Layers",
      "text": "According to the model structure introduced in Section 3.3.2, the only way for GNNs to receive information from a remote utterance is to stack many GNN layers. However, it is well known that stacking too many GNN layers might cause performance degradation due to over-smoothing  (Kipf and Welling, 2016) . We investigate whether the same phenomenon would happen when stacking many DAG-ERC layers. We conduct an experiment on IEMOCAP and plot the test result by different numbers of layers in Figure  4 , with RGAT-RoBERTa and DAGNN as baselines. As illustrated in the figure, RGAT suffers a significant performance degradation after the number of layers exceeds 6. While for DAGNN and DAG-ERC, with the number of layers changes, both of their performances fluctuate in a relatively narrow range, indicating that over-smoothing tends not to happen in the directed acyclic graph networks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Error Study",
      "text": "After going through the prediction results on the four datasets, we find that our DAG-ERC fails to distinguish between similar emotions very well, such as frustrated vs anger, happiness vs excited, scared vs mad, and joyful vs peaceful. This kind of mistake is also reported by  Ghosal et al. (2019) . Besides, we find that DAG-ERC tends to misclassify samples of other emotions to neutral on MELD, DailyDialog and EmoryNLP due to the majority proportion of neutral samples in these datasets.\n\nWe also look closely into the emotional shift issue, which means the emotions of two consecutive utterances from the same speaker are different. Existing ERC models generally work poorly in emotional shift. As shown in Table  5 , our DAG-ERC also fails to perform better on the samples with emotional shift than that without it, though the performance is still better than previous models. For example, the accuracy of DAG-ERC in the case of emotional shift is 57.98% on the IEMO-CAP dataset, which is higher than 52.5% achieved by DialogueRNN  (Majumder et al., 2019)  and 55% achieved by DialogXL  (Shen et al., 2020) .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a new idea of modeling conversation context with a directed acyclic graph (DAG) and proposed a directed acyclic graph neural network, namely DAG-ERC, for emotion recognition in conversation (ERC). Extensive experiments were conducted and the results show that the proposed DAG-ERC achieves comparable performance with the baselines. Moreover, by comprehensive evaluations and ablation study, we confirmed the superiority of our DAG-ERC and the impact of its modules. Several conclusions can be drawn from the empirical results. First, the DAG structures built from conversations do affect the performance of DAG-ERC, and with the constraints on speaker identity and positional relation, the proposed DAG structure outperforms its variants. Sec-ond, the widely utilized graph relation type of whether two utterances have the same speaker is insufficient for multi-speaker conversations. Third, the directed acyclic graph network does not suffer over-smoothing as easily as GNNs when the number of layers increases. Finally, many of the errors misjudged by DAG-ERC can be accounted for by similar emotions, neutral samples and emotional shift. These reasons have been partly mentioned in previous works but have yet to be solved, which are worth further investigation in future work.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Conversation as a directed acyclic graph,",
      "page": 1
    },
    {
      "caption": "Figure 1: , each ut-",
      "page": 2
    },
    {
      "caption": "Figure 2: In general, our DAG has two main advancements",
      "page": 3
    },
    {
      "caption": "Figure 2: An example DAG built from a three-party",
      "page": 4
    },
    {
      "caption": "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).",
      "page": 5
    },
    {
      "caption": "Figure 4: Test results of RGAT-RoBERTa, DAGNN,",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "new model and conﬁrm the motivation of the": ""
        },
        {
          "new model and conﬁrm the motivation of the": "directed acyclic graph architecture for ERC."
        },
        {
          "new model and conﬁrm the motivation of the": ""
        },
        {
          "new model and conﬁrm the motivation of the": ""
        },
        {
          "new model and conﬁrm the motivation of the": "1\nIntroduction"
        },
        {
          "new model and conﬁrm the motivation of the": ""
        },
        {
          "new model and conﬁrm the motivation of the": "Utterance-level emotion recognition in conversa-"
        },
        {
          "new model and conﬁrm the motivation of the": "tion (ERC) is an emerging task that aims to identify"
        },
        {
          "new model and conﬁrm the motivation of the": "the emotion of each utterance in a conversation."
        },
        {
          "new model and conﬁrm the motivation of the": "This task has been recently concerned by a con-"
        },
        {
          "new model and conﬁrm the motivation of the": "siderable number of NLP researchers due to its"
        },
        {
          "new model and conﬁrm the motivation of the": "potential applications in several areas, such as opin-"
        },
        {
          "new model and conﬁrm the motivation of the": "ion mining in social media (Chatterjee et al., 2019)"
        },
        {
          "new model and conﬁrm the motivation of the": "and building an emotional and empathetic dialog"
        },
        {
          "new model and conﬁrm the motivation of the": "system (Majumder et al., 2020)."
        },
        {
          "new model and conﬁrm the motivation of the": "The emotion of a query utterance is likely to be"
        },
        {
          "new model and conﬁrm the motivation of the": "inﬂuenced by many factors such as the utterances"
        },
        {
          "new model and conﬁrm the motivation of the": "spoken by the same speaker and the surrounding"
        },
        {
          "new model and conﬁrm the motivation of the": "conversation context.\nIndeed, how to model\nthe"
        },
        {
          "new model and conﬁrm the motivation of the": "conversational context lies at the heart of this task"
        },
        {
          "new model and conﬁrm the motivation of the": "(Poria et al., 2019a). Empirical evidence also shows"
        },
        {
          "new model and conﬁrm the motivation of the": ""
        },
        {
          "new model and conﬁrm the motivation of the": "∗ Corresponding author."
        },
        {
          "new model and conﬁrm the motivation of the": ""
        },
        {
          "new model and conﬁrm the motivation of the": "1The\ncode\nis\navailable\nat https://github.com/"
        },
        {
          "new model and conﬁrm the motivation of the": "shenwzh3/DAG-ERC"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "The modeling of conversational context plays"
        },
        {
          "Abstract": "a vital\nrole in emotion recognition from con-"
        },
        {
          "Abstract": "versation (ERC).\nIn this paper, we put\nfor-"
        },
        {
          "Abstract": "ward a novel\nidea of encoding the utterances"
        },
        {
          "Abstract": "with a directed acyclic graph (DAG) to better"
        },
        {
          "Abstract": "model the intrinsic structure within a conversa-"
        },
        {
          "Abstract": "tion, and design a directed acyclic neural net-"
        },
        {
          "Abstract": "work, namely DAG-ERC1,\nto implement\nthis"
        },
        {
          "Abstract": "idea. In an attempt to combine the strengths of"
        },
        {
          "Abstract": "conventional graph-based neural models and"
        },
        {
          "Abstract": "recurrence-based\nneural models, DAG-ERC"
        },
        {
          "Abstract": "provides a more intuitive way to model the in-"
        },
        {
          "Abstract": "formation ﬂow between long-distance conver-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "sation background and nearby context. Exten-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "sive experiments are conducted on four ERC"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "benchmarks with state-of-the-art models em-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ployed as baselines for comparison. The empir-"
        },
        {
          "Abstract": "ical results demonstrate the superiority of this"
        },
        {
          "Abstract": "new model and conﬁrm the motivation of the"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "directed acyclic graph architecture for ERC."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Utterance-level emotion recognition in conversa-"
        },
        {
          "Abstract": "tion (ERC) is an emerging task that aims to identify"
        },
        {
          "Abstract": "the emotion of each utterance in a conversation."
        },
        {
          "Abstract": "This task has been recently concerned by a con-"
        },
        {
          "Abstract": "siderable number of NLP researchers due to its"
        },
        {
          "Abstract": "potential applications in several areas, such as opin-"
        },
        {
          "Abstract": "ion mining in social media (Chatterjee et al., 2019)"
        },
        {
          "Abstract": "and building an emotional and empathetic dialog"
        },
        {
          "Abstract": "system (Majumder et al., 2020)."
        },
        {
          "Abstract": "The emotion of a query utterance is likely to be"
        },
        {
          "Abstract": "inﬂuenced by many factors such as the utterances"
        },
        {
          "Abstract": "spoken by the same speaker and the surrounding"
        },
        {
          "Abstract": "conversation context.\nIndeed, how to model\nthe"
        },
        {
          "Abstract": "conversational context lies at the heart of this task"
        },
        {
          "Abstract": "(Poria et al., 2019a). Empirical evidence also shows"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "∗ Corresponding author."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1The\ncode\nis\navailable\nat https://github.com/"
        },
        {
          "Abstract": "shenwzh3/DAG-ERC"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "According to the above analysis, an intuitively": "better way to solve ERC is to allow the advantages",
          "as a directed acyclic graph in the ERC task. Sec-": "ond, we propose a method to build a DAG from a"
        },
        {
          "According to the above analysis, an intuitively": "of graph-based methods and recurrence-based mod-",
          "as a directed acyclic graph in the ERC task. Sec-": "conversation with constraints based on the speaker"
        },
        {
          "According to the above analysis, an intuitively": "els to complement each other. This can be achieved",
          "as a directed acyclic graph in the ERC task. Sec-": "identity and positional relations. Third, we propose"
        },
        {
          "According to the above analysis, an intuitively": "by regarding each conversation as a directed acyclic",
          "as a directed acyclic graph in the ERC task. Sec-": "a directed acyclic graph neural network for ERC,"
        },
        {
          "According to the above analysis, an intuitively": "graph (DAG). As illustrated in Figure 1, each ut-",
          "as a directed acyclic graph in the ERC task. Sec-": "which takes DAGNN as its backbone and has two"
        },
        {
          "According to the above analysis, an intuitively": "terance in a conversation only receives information",
          "as a directed acyclic graph in the ERC task. Sec-": "main improvements designed speciﬁcally for ERC."
        },
        {
          "According to the above analysis, an intuitively": "from some previous utterances and cannot propa-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "2\nRelated work"
        },
        {
          "According to the above analysis, an intuitively": "gate information backward to itself and its prede-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "cessors through any path. This characteristic indi-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "2.1\nEmotion Recognition in Conversation"
        },
        {
          "According to the above analysis, an intuitively": "cates that a conversation can be regarded as a DAG.",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "Recently, several ERC datasets with textual data"
        },
        {
          "According to the above analysis, an intuitively": "Moreover, by the information ﬂow from predeces-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "have been released (Busso et al., 2008; Schuller"
        },
        {
          "According to the above analysis, an intuitively": "sors to successors through edges, DAG can gather",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "et al., 2012; Zahiri and Choi, 2017; Li et al., 2017;"
        },
        {
          "According to the above analysis, an intuitively": "information for a query utterance from both the",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "Chen et al., 2018; Poria et al., 2019b), arousing"
        },
        {
          "According to the above analysis, an intuitively": "neighboring utterances and the remote utterances,",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "the widespread interest of NLP researchers. In the"
        },
        {
          "According to the above analysis, an intuitively": "which acts like a combination of graph structure",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "following paragraphs, we divide the related works"
        },
        {
          "According to the above analysis, an intuitively": "and recurrence structure. Thus, we speculate that",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "into two categories according to the methods they"
        },
        {
          "According to the above analysis, an intuitively": "DAG is a more appropriate and reasonable way",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "use to model the conversation context."
        },
        {
          "According to the above analysis, an intuitively": "than graph-based structure and recurrence-based",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "Graph-based Models DialogGCN (Ghosal et al.,"
        },
        {
          "According to the above analysis, an intuitively": "structure to model the conversation context in ERC.",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "2019) treats each dialog as a graph in which each"
        },
        {
          "According to the above analysis, an intuitively": "In this paper, we propose a method to model the",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "utterance is connected with the surrounding utter-"
        },
        {
          "According to the above analysis, an intuitively": "conversation context in the form of DAG. Firstly,",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "ances. RGAT (Ishiwatari et al., 2020) adds posi-"
        },
        {
          "According to the above analysis, an intuitively": "rather than simply connecting each utterance with a",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "tional encodings to DialogGCN. ConGCN (Zhang"
        },
        {
          "According to the above analysis, an intuitively": "ﬁxed number of its surrounding utterances to build",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "et al., 2019a) regards both speakers and utterances"
        },
        {
          "According to the above analysis, an intuitively": "a graph, we propose a new way to build a DAG",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "as graph nodes and makes the whole ERC dataset"
        },
        {
          "According to the above analysis, an intuitively": "from the conversation with constraints on speaker",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "a single graph. KET (Zhong et al., 2019) uses hier-"
        },
        {
          "According to the above analysis, an intuitively": "identity and positional relations. Secondly, inspired",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "archical Transformers (Vaswani et al., 2017) with"
        },
        {
          "According to the above analysis, an intuitively": "by DAGNN (Thost and Chen, 2021), we propose",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "external knowledge. DialogXL (Shen et al., 2020)"
        },
        {
          "According to the above analysis, an intuitively": "a directed acyclic graph neural network for ERC,",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "improves XLNet (Yang et al., 2019) with enhanced"
        },
        {
          "According to the above analysis, an intuitively": "namely DAG-ERC. Unlike the traditional graph",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "memory and dialog-aware self-attention.2"
        },
        {
          "According to the above analysis, an intuitively": "neural networks such as GCN (Kipf and Welling,",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "Recurrence-based Models In this category, ICON"
        },
        {
          "According to the above analysis, an intuitively": "2016) and GAT (Veliˇckovi´c et al., 2017) that ag-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "(Hazarika et al., 2018a) and CMN (Hazarika et al.,"
        },
        {
          "According to the above analysis, an intuitively": "gregate information from the previous layer, DAG-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "2018b) both utilize gated recurrent unit (GRU) and"
        },
        {
          "According to the above analysis, an intuitively": "ERC can recurrently gather information of prede-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "memory networks. HiGRU (Jiao et al., 2019) con-"
        },
        {
          "According to the above analysis, an intuitively": "cessors for every utterance in a single layer, which",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "tains two GRUs, one for utterance encoder and"
        },
        {
          "According to the above analysis, an intuitively": "enables the model\nto encode the remote context",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "the other\nfor conversation encoder. DialogRNN"
        },
        {
          "According to the above analysis, an intuitively": "without having to stack too many layers. Besides,",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "(Majumder\net\nal., 2019)\nis\na\nrecurrence-based"
        },
        {
          "According to the above analysis, an intuitively": "in order to be more applicable to the ERC task, our",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "method that models dialog dynamics with several"
        },
        {
          "According to the above analysis, an intuitively": "DAG-ERC has two improvements over DAGNN:",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "RNNs. COSMIC (Ghosal et al., 2020) is the latest"
        },
        {
          "According to the above analysis, an intuitively": "(1) a relation-aware feature transformation to gather",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "model, which adopts a network structure very close"
        },
        {
          "According to the above analysis, an intuitively": "information based on speaker identity and (2) a con-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "to DialogRNN and adds external commonsense"
        },
        {
          "According to the above analysis, an intuitively": "textual information unit to enhance the information",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "knowledge to improve performance."
        },
        {
          "According to the above analysis, an intuitively": "of historical context. We conduct extensive exper-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "iments on four ERC benchmarks and the results",
          "as a directed acyclic graph in the ERC task. Sec-": "2.2\nDirected Acyclic Graph Neural Network"
        },
        {
          "According to the above analysis, an intuitively": "show that the proposed DAG-ERC achieves compa-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "Directed acyclic graph is a special\ntype of graph"
        },
        {
          "According to the above analysis, an intuitively": "rable performance with the state-of-the-art models.",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "structure that can be seen in multiple areas,\nfor"
        },
        {
          "According to the above analysis, an intuitively": "Furthermore, several studies are conducted to ex-",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "example, the parsing results of source code (Alla-"
        },
        {
          "According to the above analysis, an intuitively": "plore the effect of the proposed DAG structure and",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "manis et al., 2018) and logical formulas (Crouse"
        },
        {
          "According to the above analysis, an intuitively": "the modules of DAG-ERC.",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "2We regard KET and DialogXL as graph-based models"
        },
        {
          "According to the above analysis, an intuitively": "The contributions of\nthis paper are threefold.",
          "as a directed acyclic graph in the ERC task. Sec-": ""
        },
        {
          "According to the above analysis, an intuitively": "",
          "as a directed acyclic graph in the ERC task. Sec-": "because they both adopt Transformer in which self-attention"
        },
        {
          "According to the above analysis, an intuitively": "First, we are the ﬁrst\nto consider a conversation",
          "as a directed acyclic graph in the ERC task. Sec-": "can be viewed as a fully-connected graph in some sense."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "predecessors’ representations, DAGNN conducts"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "information aggregation using graph attention."
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "3"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "3.1"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "In ERC, a conversation is deﬁned as a sequence of"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "utterances {u1, u2, ..., uN }, where N is the num-"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "ber of utterances. Each utterance ui consists of ni"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "tokens, namely ui = {wi1, wi2, ..., wini}. A dis-"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "crete value yi ∈ S is used to denote the emotion"
        },
        {
          "ing out naive sum or element-wise product on the": ""
        },
        {
          "ing out naive sum or element-wise product on the": "label of ui, where S is the set of emotion labels."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "et al., 2019). A number of neural networks that em-": "ploy DAG architecture have been proposed, such",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "Input:\nthe dialog {u1, u2, ..., uN }, speaker iden-"
        },
        {
          "et al., 2019). A number of neural networks that em-": "as Tree-LSTM (Tai et al., 2015), DAG-RNN(Shuai",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "tity p(·), hyper-parameter ω"
        },
        {
          "et al., 2019). A number of neural networks that em-": "et al., 2016), D-VAE (Zhang et al., 2019b), and",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "Output: G = (V, E, R)"
        },
        {
          "et al., 2019). A number of neural networks that em-": "DAGNN (Thost and Chen, 2021). DAGNN is dif-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "1: V ← {u1, u2, ..., uN }"
        },
        {
          "et al., 2019). A number of neural networks that em-": "ferent from the previous DAG models in the model",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "2: E ← ∅"
        },
        {
          "et al., 2019). A number of neural networks that em-": "structure. Speciﬁcally, DAGNN allows multiple",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "3: R ← {0, 1}"
        },
        {
          "et al., 2019). A number of neural networks that em-": "layers to be stacked, while the others have only",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "4:\nfor all i ∈ {2, 3, ..., N } do"
        },
        {
          "et al., 2019). A number of neural networks that em-": "one single layer. Besides, instead of merely carry-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "5:\nc ← 0"
        },
        {
          "et al., 2019). A number of neural networks that em-": "ing out naive sum or element-wise product on the",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "6:\nτ ← i − 1"
        },
        {
          "et al., 2019). A number of neural networks that em-": "predecessors’ representations, DAGNN conducts",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "7:\nwhile τ > 0 and c < ω do"
        },
        {
          "et al., 2019). A number of neural networks that em-": "information aggregation using graph attention.",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "8:\nif p(uτ ) = p(ui) then"
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "9:\nE ← E ∪ {(τ, i, 1)}"
        },
        {
          "et al., 2019). A number of neural networks that em-": "3\nMethodology",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "10:\nc ← c + 1"
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "11:\nelse"
        },
        {
          "et al., 2019). A number of neural networks that em-": "3.1\nProblem Deﬁnition",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "12:\nE ← E ∪ {(τ, i, 0)}"
        },
        {
          "et al., 2019). A number of neural networks that em-": "In ERC, a conversation is deﬁned as a sequence of",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "13:\nend if"
        },
        {
          "et al., 2019). A number of neural networks that em-": "utterances {u1, u2, ..., uN }, where N is the num-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "14:\nτ ← τ − 1"
        },
        {
          "et al., 2019). A number of neural networks that em-": "ber of utterances. Each utterance ui consists of ni",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "15:\nend while"
        },
        {
          "et al., 2019). A number of neural networks that em-": "tokens, namely ui = {wi1, wi2, ..., wini}. A dis-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "16:\nend for"
        },
        {
          "et al., 2019). A number of neural networks that em-": "crete value yi ∈ S is used to denote the emotion",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "17:\nreturn G = (V, E, R)"
        },
        {
          "et al., 2019). A number of neural networks that em-": "label of ui, where S is the set of emotion labels.",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "The speaker identity is denoted by a function p(·).",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "For example, p(ui) ∈ P denotes the speaker of ui",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "is called\nui. The information generated before uτ"
        },
        {
          "et al., 2019). A number of neural networks that em-": "and P is the collection of all speaker roles in an",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "remote information, which is relatively less impor-"
        },
        {
          "et al., 2019). A number of neural networks that em-": "ERC dataset. The objective of this task is to predict",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "tant. We assume that when the speaker speaks uτ ,"
        },
        {
          "et al., 2019). A number of neural networks that em-": "the emotion label yt for a given query utterance ut",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "she/he has been aware of the remote information"
        },
        {
          "et al., 2019). A number of neural networks that em-": "based on dialog context {u1, u2, ..., uN } and the",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "before uτ . That means, uτ has included the remote"
        },
        {
          "et al., 2019). A number of neural networks that em-": "corresponding speaker identity.",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "information and it will be responsible for propagat-"
        },
        {
          "et al., 2019). A number of neural networks that em-": "3.2\nBuilding a DAG from a Conversation",
          "Algorithm 1 Building a DAG from a Conversation": "ing the remote information to ui."
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "Local\ninformation: ∀l, τ < l < i, (l, i, rli) ∈ E."
        },
        {
          "et al., 2019). A number of neural networks that em-": "We design a directed acyclic graph (DAG) to model",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "Usually, the information of the local context is im-"
        },
        {
          "et al., 2019). A number of neural networks that em-": "the information propagation in a conversation. A",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "portant. Consider uτ and ui deﬁned in the second"
        },
        {
          "et al., 2019). A number of neural networks that em-": "DAG is denoted by G = (V, E, R).\nIn this paper,",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "in\nconstraint. We assume that every utterance ul"
        },
        {
          "et al., 2019). A number of neural networks that em-": "the nodes in the DAG are the utterances in the con-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "between uτ and ui contains local information, and"
        },
        {
          "et al., 2019). A number of neural networks that em-": "versation, i.e., V = {u1, u2, ..., uN }, and the edge",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "they will propagate the local information to ui."
        },
        {
          "et al., 2019). A number of neural networks that em-": "(i, j, rij) ∈ E represents the information propa-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "The ﬁrst constraint ensures the conversation to"
        },
        {
          "et al., 2019). A number of neural networks that em-": "gated from ui\nto uj, where rij ∈ R is the relation",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "type of the edge. The set of relation types of edges,",
          "Algorithm 1 Building a DAG from a Conversation": "be a DAG, and the second and third constraints"
        },
        {
          "et al., 2019). A number of neural networks that em-": "R = {0, 1}, contains two types of relation: 1 for",
          "Algorithm 1 Building a DAG from a Conversation": "is the cut-off point of remote and\nindicate that uτ"
        },
        {
          "et al., 2019). A number of neural networks that em-": "that the two connected utterances are spoken by the",
          "Algorithm 1 Building a DAG from a Conversation": "local information. We regard uτ as the ω-th latest"
        },
        {
          "et al., 2019). A number of neural networks that em-": "same speaker, and 0 for otherwise.",
          "Algorithm 1 Building a DAG from a Conversation": "utterance spoken by p(ui) before ui, where ω is"
        },
        {
          "et al., 2019). A number of neural networks that em-": "We impose three constraints to decide when an",
          "Algorithm 1 Building a DAG from a Conversation": "in\na hyper-parameter. Then for each utterance ul"
        },
        {
          "et al., 2019). A number of neural networks that em-": "utterance would propagate information to another,",
          "Algorithm 1 Building a DAG from a Conversation": "between uτ and ui, we make a directed edge from"
        },
        {
          "et al., 2019). A number of neural networks that em-": "i.e., when two utterances are connected in the DAG:",
          "Algorithm 1 Building a DAG from a Conversation": "to ui. We show the above process of building a\nul"
        },
        {
          "et al., 2019). A number of neural networks that em-": "",
          "Algorithm 1 Building a DAG from a Conversation": "DAG in Algorithm 1."
        },
        {
          "et al., 2019). A number of neural networks that em-": "Direction: ∀j > i, (j, i, rji) /∈ E. A previous ut-",
          "Algorithm 1 Building a DAG from a Conversation": ""
        },
        {
          "et al., 2019). A number of neural networks that em-": "terance can pass message to a future utterance, but",
          "Algorithm 1 Building a DAG from a Conversation": "An example of the DAG is shown in Figure 2."
        },
        {
          "et al., 2019). A number of neural networks that em-": "a future utterance cannot pass message backwards.",
          "Algorithm 1 Building a DAG from a Conversation": "In general, our DAG has two main advancements"
        },
        {
          "et al., 2019). A number of neural networks that em-": "Remote information: ∃τ < i, p(uτ ) = p(ui), (τ, i",
          "Algorithm 1 Building a DAG from a Conversation": "compared to the graph structures developed in pre-"
        },
        {
          "et al., 2019). A number of neural networks that em-": ", rτ i) ∈ E and ∀j < τ,\n(j, i, rji) /∈ E. For each",
          "Algorithm 1 Building a DAG from a Conversation": "vious works (Ghosal et al., 2019; Ishiwatari et al.,"
        },
        {
          "et al., 2019). A number of neural networks that em-": "utterance ui except the ﬁrst one, there is a previous",
          "Algorithm 1 Building a DAG from a Conversation": "2020): First, our DAG doesn’t have edges from"
        },
        {
          "et al., 2019). A number of neural networks that em-": "that is spoken by the same speaker as\nutterance uτ",
          "Algorithm 1 Building a DAG from a Conversation": "future utterances to previous utterances, which we"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "Aggregate(·) is the information aggregation func-"
        },
        {
          "where f (·) is the information processing function,": "tion to gather information from neighboring nodes,"
        },
        {
          "where f (·) is the information processing function,": "and Ni denotes the neighbours of the i-th node."
        },
        {
          "where f (·) is the information processing function,": "Recurrence-based models (RNN) allow infor-"
        },
        {
          "where f (·) is the information processing function,": "mation to propagate temporally at the same layer,"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "while the i-th node only receives information from"
        },
        {
          "where f (·) is the information processing function,": "the (i−1)-th node:"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "H l"
        },
        {
          "where f (·) is the information processing function,": ").\n(2)\ni = f (H l\ni−1, H l−1"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "Directed acyclic graph models (DAGNN) work"
        },
        {
          "where f (·) is the information processing function,": "like a combination of GNN and RNN. They aggre-"
        },
        {
          "where f (·) is the information processing function,": "gate information for each node in temporal order,"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "and allow all nodes to gather\ninformation from"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "neighbors and update their states at the same layer:"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "H l"
        },
        {
          "where f (·) is the information processing function,": ").\n(3)\ni = f (Aggregate({H l\nj|j ∈ Ni}), H l−1"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "The strength of applying DAGNN to ERC is"
        },
        {
          "where f (·) is the information processing function,": "relatively apparent: By allowing information to"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "propagate temporally at the same layer, DAGNN"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "can get access to distant utterances and model the"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "information ﬂow throughout\nthe whole conversa-"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "tion, which is hardly possible for GNN. Besides,"
        },
        {
          "where f (·) is the information processing function,": "DAGNN gathers information from several neigh-"
        },
        {
          "where f (·) is the information processing function,": "boring utterances, which sounds more appealing"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "than RNN as the latter only receives information"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "from the (i−1)-th utterance."
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "3.3.3\nDAG-ERC Layers"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "Our proposed DAG-ERC is primarily inspired by"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "DAGNN (Thost and Chen, 2021), with novel im-"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "provements specially made for emotion recognition"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "in conversation. At each layer l of DAG-ERC, due"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "to the temporal information ﬂow, the hidden state"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "of utterances should be computed recurrently from"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "the ﬁrst utterance to the last one."
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "For each utterance ui, the attention weights be-"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "tween ui and its predecessors are calculated by"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "using ui’s hidden state at the (l − 1)-th layer to at-"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "tend to the predecessors’ hidden states at l-th layer:"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "αl"
        },
        {
          "where f (·) is the information processing function,": "])\n(4)\nij = Softmaxj∈Ni(W l\nα[H l\nj(cid:107)H l−1"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "where W l\nα are trainable parameters and (cid:107) denotes"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "the concatenation operation."
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "The information aggregation operation in DAG-"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "ERC is different from that in DAGNN. Instead of"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "merely gathering information according to the at-"
        },
        {
          "where f (·) is the information processing function,": ""
        },
        {
          "where f (·) is the information processing function,": "tention weights, inspired by R-GCN (Schlichtkrull"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "transformation to make full use of the relational"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "type of edges:"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "(cid:88)"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "M l"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "(5)\ni =\nj,\nrij H l"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "j∈Ni"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "0, W l\n1} are trainable parameters\nrij ∈ {W l"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "for the relation-aware transformation."
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "After the aggregated information M l\nis calcu-"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "lated, we make it interact with ui’s hidden state at"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "the previous layer H l−1\nto obtain the ﬁnal hidden"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "state of ui at the current layer. In DAGNN, the ﬁnal"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "hidden state is obtained by allowing M l\nto control"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "information propagation of H l−1\nto the l-th layer"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "with a gated recurrent unit (GRU):"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ", M l\nH l"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "(6)\ni = GRUl\nH (H l−1\ni ),"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "where H l−1"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i are the input, hidden state\ni , and (cid:101)H l"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "and output of the GRU, respectively."
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "We refer to the process in Equation 6 as nodal"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "information unit, because it focuses on the node"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "information propagating from the past layer to the"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "current layer. Nodal information unit may be suit-"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "able for\nthe tasks that DAGNN is originally de-"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "signed to solve. However, we ﬁnd that only using"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "nodal information unit is not enough for ERC, es-"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "pecially when the query utterance ui’s emotion"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "should be derived from its context. The reason is"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": ""
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "that in DAGNN, the information of context M l\nis"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "i"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "only used to control the propagation of ui’s hidden"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "state, and under this circumstance, the information"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "of context is not fully leveraged. Therefore, we de-"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "information\nsign another GRU called contextual"
        },
        {
          "Figure 3: The framework of Directed Acyclic Graph Neural Network for ERC (DAG-ERC).": "unit\nto model\nthe information ﬂow of historical"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "# Conversations\n# Uterrances": "Dataset",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "Train\nVal\nTest\nTrain\nVal\nTest",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "The emotion labels of this dataset include neutral,"
        },
        {
          "# Conversations\n# Uterrances": "IEMOCAP\n120\n31\n5810\n1623",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "sad, mad, scared, powerful, peaceful, and joyful."
        },
        {
          "# Conversations\n# Uterrances": "MELD\n1038\n114\n280\n9989\n1109\n2610",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "DailyDialog\n11118\n1000\n1000\n87170\n8069\n7740",
          "MELD in the choice of scenes and emotion labels.": "We utilize only the textual modality of the above"
        },
        {
          "# Conversations\n# Uterrances": "EmoryNLP\n713\n99\n85\n9934\n1344\n1328",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "datasets for the experiments. For evaluation met-"
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "rics, we follow Ishiwatari et al. (2020) and Shen"
        },
        {
          "# Conversations\n# Uterrances": "Table 1: The statistics of four datasets.",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "et al. (2020) and choose micro-averaged F1 exclud-"
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "ing the majority class (neutral) for DailyDialog and"
        },
        {
          "# Conversations\n# Uterrances": "4\nExperimental Settings",
          "MELD in the choice of scenes and emotion labels.": "weighted-average F1 for the other datasets."
        },
        {
          "# Conversations\n# Uterrances": "4.1\nImplementation Details",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "4.3\nCompared Methods"
        },
        {
          "# Conversations\n# Uterrances": "We conduct hyper-parameter search for our pro-",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "We compared our model with the following base-"
        },
        {
          "# Conversations\n# Uterrances": "posed DAG-ERC on each dataset by hold-out vali-",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "lines in our experiments:"
        },
        {
          "# Conversations\n# Uterrances": "dation with a validation set. The hyper-parameters",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "Recurrence-based methods: DialogueRNN (Ma-"
        },
        {
          "# Conversations\n# Uterrances": "to search include learning rate, batch size, dropout",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "jumder\net\nal.,\n2019),\nDialogRNN-RoBERTa"
        },
        {
          "# Conversations\n# Uterrances": "rate, and the number of DAG-ERC layers. For the",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "(Ghosal et al., 2020), and COSMIC without ex-"
        },
        {
          "# Conversations\n# Uterrances": "ω that\nis described in 3.2, we let ω = 1 for\nthe",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "ternal knowledge3 (Ghosal et al., 2020)."
        },
        {
          "# Conversations\n# Uterrances": "overall performance comparison by default, but we",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "Graph-based methods: DialogurGCN (Ghosal"
        },
        {
          "# Conversations\n# Uterrances": "report the results with ω varying from 1 to 3 in 5.2.",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "et al., 2019), KET (Zhong et al., 2019), DialogXL"
        },
        {
          "# Conversations\n# Uterrances": "For other hyper-parameters, the sizes of all hidden",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "(Shen et al., 2020) and RGAT (Ishiwatari et al.,"
        },
        {
          "# Conversations\n# Uterrances": "vectors are equal to 300, and the feature size for the",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "2020)."
        },
        {
          "# Conversations\n# Uterrances": "RoBERTa extractor is 1024. Each training and test-",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "Feature extractor: RoBERTa (Liu et al., 2019)."
        },
        {
          "# Conversations\n# Uterrances": "ing process is run on a single RTX 2080 Ti GPU.",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "Previous models with our\nextracted features:"
        },
        {
          "# Conversations\n# Uterrances": "Each training process contains 60 epochs and it",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "DialogueGCN-RoBERTa, RGAT-RoBERTa and"
        },
        {
          "# Conversations\n# Uterrances": "costs at most 50 seconds per epoch. The reported",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "DAGNN (Thost and Chen, 2021)4."
        },
        {
          "# Conversations\n# Uterrances": "results of our implemented models are all based on",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "Ours: DAG-ERC."
        },
        {
          "# Conversations\n# Uterrances": "the average score of 5 random runs on the test set.",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "4.2\nDatasets",
          "MELD in the choice of scenes and emotion labels.": "5\nResults and Analysis"
        },
        {
          "# Conversations\n# Uterrances": "We evaluate DAG-ERC on four ERC datasets. The",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "5.1\nOverall Performance"
        },
        {
          "# Conversations\n# Uterrances": "statistics of them are shown in Table 1.",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "The overall results of all the compared methods on"
        },
        {
          "# Conversations\n# Uterrances": "IEMOCAP (Busso et al., 2008): A multimodal",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "the four datasets are reported in Table 2. We can"
        },
        {
          "# Conversations\n# Uterrances": "ERC dataset. Each conversation in IEMOCAP",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "note from the results that our proposed DAG-ERC"
        },
        {
          "# Conversations\n# Uterrances": "comes from the performance based on script by",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "achieves competitive performances across the four"
        },
        {
          "# Conversations\n# Uterrances": "two actors. Models are evaluated on the samples",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "datasets and reaches a new state of the art on the"
        },
        {
          "# Conversations\n# Uterrances": "with 6 types of emotion, namely neutral, happiness,",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "IEMOCAP, DailyDialog and EmoryNLP datasets."
        },
        {
          "# Conversations\n# Uterrances": "sadness, anger, frustrated, and excited. Since this",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "dataset has no validation set, we follow Shen et al.",
          "MELD in the choice of scenes and emotion labels.": "As\nshown in the table, when the feature ex-"
        },
        {
          "# Conversations\n# Uterrances": "(2020) to use the last 20 dialogues in the training",
          "MELD in the choice of scenes and emotion labels.": "tracting method is the same, graph-based models"
        },
        {
          "# Conversations\n# Uterrances": "set for validation.",
          "MELD in the choice of scenes and emotion labels.": "generally outperform recurrence-based models on"
        },
        {
          "# Conversations\n# Uterrances": "MELD (Poria et al., 2019b): A multimodal ERC",
          "MELD in the choice of scenes and emotion labels.": "IEMOCAP, DailyDialog, and EmoryNLP. This phe-"
        },
        {
          "# Conversations\n# Uterrances": "dataset collected from the TV show Friends. There",
          "MELD in the choice of scenes and emotion labels.": "nomenon indicates that recurrence-based models"
        },
        {
          "# Conversations\n# Uterrances": "are 7 emotion labels including neutral, happiness,",
          "MELD in the choice of scenes and emotion labels.": "cannot encode the context as effectively as graph-"
        },
        {
          "# Conversations\n# Uterrances": "surprise, sadness, anger, disgust, and fear.",
          "MELD in the choice of scenes and emotion labels.": "based models, especially for the more important"
        },
        {
          "# Conversations\n# Uterrances": "DailyDialog (Li et al., 2017): Human-written di-",
          "MELD in the choice of scenes and emotion labels.": "local context. What’s more, we see a signiﬁcant"
        },
        {
          "# Conversations\n# Uterrances": "alogs collected from communications of English",
          "MELD in the choice of scenes and emotion labels.": "improvement of DAG-ERC over the graph-based"
        },
        {
          "# Conversations\n# Uterrances": "learners. 7 emotion labels are included: neutral,",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "3In this paper, we compare our DAG-ERC with COSMIC"
        },
        {
          "# Conversations\n# Uterrances": "happiness, surprise, sadness, anger, disgust, and",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "without external knowledge, rather than the complete COS-"
        },
        {
          "# Conversations\n# Uterrances": "fear. Since it has no speaker information, we con-",
          "MELD in the choice of scenes and emotion labels.": "MIC,\nin order\nto make a clearer comparison on the model"
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "architecture, even though our DAG-ERC outperforms the com-"
        },
        {
          "# Conversations\n# Uterrances": "sider utterance turns as speaker turns by default.",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "plete COSMIC on IEMOCAP, DailyDialog and EmoryNLP."
        },
        {
          "# Conversations\n# Uterrances": "EmoryNLP (Zahiri and Choi, 2017): TV show",
          "MELD in the choice of scenes and emotion labels.": ""
        },
        {
          "# Conversations\n# Uterrances": "",
          "MELD in the choice of scenes and emotion labels.": "4DAGNN is not originally designed for ERC, so we apply"
        },
        {
          "# Conversations\n# Uterrances": "scripts collected from Friends, but varies\nfrom",
          "MELD in the choice of scenes and emotion labels.": "our DAG building method and the extracted feature for it."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "DialogueRNN",
          "IEMOCAP": "62.75",
          "MELD": "57.03",
          "DailyDialog": "-",
          "EmoryNLP": "-"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "+RoBERTa",
          "IEMOCAP": "64.76",
          "MELD": "63.61",
          "DailyDialog": "57.32",
          "EmoryNLP": "37.44"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "COSMIC",
          "IEMOCAP": "63.05",
          "MELD": "64.28",
          "DailyDialog": "56.16",
          "EmoryNLP": "37.10"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "KET",
          "IEMOCAP": "59.56",
          "MELD": "58.18",
          "DailyDialog": "53.37",
          "EmoryNLP": "33.95"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "DialogXL",
          "IEMOCAP": "65.94",
          "MELD": "62.41",
          "DailyDialog": "54.93",
          "EmoryNLP": "34.73"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "DialogueGCN",
          "IEMOCAP": "64.18",
          "MELD": "58.10",
          "DailyDialog": "-",
          "EmoryNLP": "-"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "+RoBERTa",
          "IEMOCAP": "64.91",
          "MELD": "63.02",
          "DailyDialog": "57.52",
          "EmoryNLP": "38.10"
        },
        {
          "Model": "RGAT",
          "IEMOCAP": "65.22",
          "MELD": "60.91",
          "DailyDialog": "54.31",
          "EmoryNLP": "34.42"
        },
        {
          "Model": "+RoBERTa",
          "IEMOCAP": "66.36",
          "MELD": "62.80",
          "DailyDialog": "59.02",
          "EmoryNLP": "37.89"
        },
        {
          "Model": "RoBERTa",
          "IEMOCAP": "63.38",
          "MELD": "62.88",
          "DailyDialog": "58.08",
          "EmoryNLP": "37.78"
        },
        {
          "Model": "DAGNN",
          "IEMOCAP": "64.61",
          "MELD": "63.12",
          "DailyDialog": "58.36",
          "EmoryNLP": "37.89"
        },
        {
          "Model": "",
          "IEMOCAP": "",
          "MELD": "",
          "DailyDialog": "",
          "EmoryNLP": ""
        },
        {
          "Model": "DAG-ERC",
          "IEMOCAP": "68.03",
          "MELD": "63.65",
          "DailyDialog": "59.33",
          "EmoryNLP": "39.02"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "while a slight drop on MELD and EmoryNLP.": ""
        },
        {
          "while a slight drop on MELD and EmoryNLP.": ""
        },
        {
          "while a slight drop on MELD and EmoryNLP.": ""
        },
        {
          "while a slight drop on MELD and EmoryNLP.": ""
        },
        {
          "while a slight drop on MELD and EmoryNLP.": "IEMOCAP"
        },
        {
          "while a slight drop on MELD and EmoryNLP.": "68.03"
        },
        {
          "while a slight drop on MELD and EmoryNLP.": "64.12 (↓3.91)"
        },
        {
          "while a slight drop on MELD and EmoryNLP.": "66.19 (↓1.84)"
        },
        {
          "while a slight drop on MELD and EmoryNLP.": ""
        },
        {
          "while a slight drop on MELD and EmoryNLP.": "66.32 (↓1.71)"
        },
        {
          "while a slight drop on MELD and EmoryNLP.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "DAGNN\n64.61\n63.12\n58.36\n37.89"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "68.03\n59.33\n39.02\nDAG-ERC\n63.65"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "Table 2: Overall performance on the four datasets."
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "models on IEMOCAP, which demonstrates DAG-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "ERC’s superior ability to capture remote informa-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "tion given that the dialogs in IEMOCAP are much"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "longer (almost 70 utterances per dialog)."
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "On MELD, however, we observe that neither"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "graph-based models nor our DAG-ERC outper-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "forms the recurrence-based models. After going"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "through the data, we ﬁnd that due to the data collec-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "tion method (collected from TV shows), sometimes"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "two consecutive utterances in MELD are not coher-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "ent. Under this circumstance, graph-based models’"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "advantage in encoding context is not that important."
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "Besides,\nthe graph-based models see consider-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "able improvements when implemented with the"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "powerful feature extractor RoBERTa.\nIn spite of"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "this, our DAG-ERC consistently outperforms these"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "improved graph-based models and DAGNN, con-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "ﬁrming the superiority of the DAG structure and"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "the effectiveness of the improvements we make to"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "build DAG-ERC upon DAGNN."
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "5.2\nVariants of DAG Structure"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "In this section, we investigate how the structure of"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "DAG would affect our DAG-ERC’s performance"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "by applying different DAG structures to DAG-ERC."
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "In addition to our proposed structure, we further"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "deﬁne three kinds of DAG structure: (1) sequence,"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "in which utterances are connected one by one; (2)"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "DAG with single local information, in which each"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "utterance only receives local information from its"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "nearest neighbor, and the remote information re-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "mains the same as our DAG; (3) common DAG, in"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "which each utterance is connected with κ previous"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "utterances. Note that if there are only two speakers"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "taking turns to speak in a dialog, then our DAG is"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "equivalent\nto common DAG with κ = 2ω, mak-"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "ing the comparison less meaningful. Therefore, we"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "conduct the experiment on EmoryNLP, where there"
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": ""
        },
        {
          "RoBERTa\n63.38\n62.88\n58.08\n37.78": "are usually multiple speakers in one dialog, and the"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Test accuracy of DAG-ERC on samples with",
      "data": [
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "Dataset"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "# Samples\nAccuracy\n# Samples\nAccuracy"
        },
        {
          "68": "67",
          "Emotional shift\nw/o Emotional shift": "IEMOCAP\n576\n57.98%\n1002\n74.25%"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "MELD\n1003\n59.02%\n861\n69.45%"
        },
        {
          "68": "RGAT-RoBERTa\n66",
          "Emotional shift\nw/o Emotional shift": "DailyDialog\n670\n57.26%\n454\n59.25%"
        },
        {
          "68": "F1 score\nDAGNN",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "EmoryNLP\n673\n37.29%\n361\n42.10%"
        },
        {
          "68": "DAG-ERC",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "65",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "Table 5: Test accuracy of DAG-ERC on samples with"
        },
        {
          "68": "64",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "emotional shift and without it."
        },
        {
          "68": "63",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "# layers",
          "Emotional shift\nw/o Emotional shift": "5.5\nError Study"
        },
        {
          "68": "Figure 4: Test\nresults of RGAT-RoBERTa, DAGNN,",
          "Emotional shift\nw/o Emotional shift": "After going through the prediction results on the"
        },
        {
          "68": "and DAG-ERC on the IEMOCAP dataset by different",
          "Emotional shift\nw/o Emotional shift": "four datasets, we ﬁnd that our DAG-ERC fails to"
        },
        {
          "68": "numbers of network layers.",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "distinguish between similar emotions very well,"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "such as frustrated vs anger, happiness vs excited,"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "scared vs mad, and joyful vs peaceful. This kind of"
        },
        {
          "68": "in IEMOCAP and DailyDialog, and there are usu-",
          "Emotional shift\nw/o Emotional shift": "mistake is also reported by Ghosal et al. (2019). Be-"
        },
        {
          "68": "ally more than two speakers in dialogs of MELD",
          "Emotional shift\nw/o Emotional shift": "sides, we ﬁnd that DAG-ERC tends to misclassify"
        },
        {
          "68": "and EmoryNLP. Therefore, we can infer that\nthe",
          "Emotional shift\nw/o Emotional shift": "samples of other emotions to neutral on MELD,"
        },
        {
          "68": "relation of whether two utterances have the same",
          "Emotional shift\nw/o Emotional shift": "DailyDialog and EmoryNLP due to the majority"
        },
        {
          "68": "speaker is sufﬁcient for two-speaker dialogs, while",
          "Emotional shift\nw/o Emotional shift": "proportion of neutral samples in these datasets."
        },
        {
          "68": "falls short in the multi-speaker setting.",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "We also look closely into the emotional shift"
        },
        {
          "68": "Moreover, we ﬁnd that on each dataset, the per-",
          "Emotional shift\nw/o Emotional shift": "issue, which means the emotions of two consecu-"
        },
        {
          "68": "formance drop caused by ablating nodal informa-",
          "Emotional shift\nw/o Emotional shift": "tive utterances from the same speaker are different."
        },
        {
          "68": "tion unit is similar to contextual information unit,",
          "Emotional shift\nw/o Emotional shift": "Existing ERC models generally work poorly in"
        },
        {
          "68": "and all\nthese drops are not\nthat critical. This im-",
          "Emotional shift\nw/o Emotional shift": "emotional shift. As shown in Table 5, our DAG-"
        },
        {
          "68": "plies that either the nodal information unit or con-",
          "Emotional shift\nw/o Emotional shift": "ERC also fails to perform better on the samples"
        },
        {
          "68": "textual\ninformation unit\nis effective for the ERC",
          "Emotional shift\nw/o Emotional shift": "with emotional shift\nthan that without\nit,\nthough"
        },
        {
          "68": "task, while combining the two of them can yield",
          "Emotional shift\nw/o Emotional shift": "the performance is still better than previous mod-"
        },
        {
          "68": "further performance improvement.",
          "Emotional shift\nw/o Emotional shift": "els. For example, the accuracy of DAG-ERC in the"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "case of emotional shift\nis 57.98% on the IEMO-"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "CAP dataset, which is higher than 52.5% achieved"
        },
        {
          "68": "5.4\nNumber of DAG-ERC Layers",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "by DialogueRNN (Majumder et al., 2019) and 55%"
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "achieved by DialogXL (Shen et al., 2020)."
        },
        {
          "68": "According to the model\nstructure introduced in",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "Section 3.3.2,\nthe only way for GNNs to receive",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "",
          "Emotional shift\nw/o Emotional shift": "6\nConclusion"
        },
        {
          "68": "information from a remote utterance is to stack",
          "Emotional shift\nw/o Emotional shift": ""
        },
        {
          "68": "many GNN layers.\nHowever,\nit\nis well known",
          "Emotional shift\nw/o Emotional shift": "In this paper, we presented a new idea of mod-"
        },
        {
          "68": "that stacking too many GNN layers might cause",
          "Emotional shift\nw/o Emotional shift": "eling conversation context with a directed acyclic"
        },
        {
          "68": "performance degradation due to over-smoothing",
          "Emotional shift\nw/o Emotional shift": "graph (DAG) and proposed a directed acyclic graph"
        },
        {
          "68": "(Kipf and Welling, 2016). We investigate whether",
          "Emotional shift\nw/o Emotional shift": "neural network, namely DAG-ERC, for emotion"
        },
        {
          "68": "the same phenomenon would happen when stack-",
          "Emotional shift\nw/o Emotional shift": "recognition in conversation (ERC). Extensive ex-"
        },
        {
          "68": "ing many DAG-ERC layers. We conduct an ex-",
          "Emotional shift\nw/o Emotional shift": "periments were conducted and the results show"
        },
        {
          "68": "periment on IEMOCAP and plot\nthe test\nresult",
          "Emotional shift\nw/o Emotional shift": "that\nthe proposed DAG-ERC achieves compara-"
        },
        {
          "68": "by different numbers of layers in Figure 4, with",
          "Emotional shift\nw/o Emotional shift": "ble performance with the baselines. Moreover, by"
        },
        {
          "68": "RGAT-RoBERTa and DAGNN as baselines. As",
          "Emotional shift\nw/o Emotional shift": "comprehensive evaluations and ablation study, we"
        },
        {
          "68": "illustrated in the ﬁgure, RGAT suffers a signiﬁcant",
          "Emotional shift\nw/o Emotional shift": "conﬁrmed the superiority of our DAG-ERC and the"
        },
        {
          "68": "performance degradation after the number of lay-",
          "Emotional shift\nw/o Emotional shift": "impact of its modules. Several conclusions can be"
        },
        {
          "68": "ers exceeds 6. While for DAGNN and DAG-ERC,",
          "Emotional shift\nw/o Emotional shift": "drawn from the empirical results. First,\nthe DAG"
        },
        {
          "68": "with the number of layers changes, both of their",
          "Emotional shift\nw/o Emotional shift": "structures built from conversations do affect the per-"
        },
        {
          "68": "performances ﬂuctuate in a relatively narrow range,",
          "Emotional shift\nw/o Emotional shift": "formance of DAG-ERC, and with the constraints"
        },
        {
          "68": "indicating that over-smoothing tends not to happen",
          "Emotional shift\nw/o Emotional shift": "on speaker identity and positional relation, the pro-"
        },
        {
          "68": "in the directed acyclic graph networks.",
          "Emotional shift\nw/o Emotional shift": "posed DAG structure outperforms its variants. Sec-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: Test accuracy of DAG-ERC on samples with",
      "data": [
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": "5.5\nError Study"
        },
        {
          "emotional shift and without it.": "After going through the prediction results on the"
        },
        {
          "emotional shift and without it.": "four datasets, we ﬁnd that our DAG-ERC fails to"
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": "distinguish between similar emotions very well,"
        },
        {
          "emotional shift and without it.": "such as frustrated vs anger, happiness vs excited,"
        },
        {
          "emotional shift and without it.": "scared vs mad, and joyful vs peaceful. This kind of"
        },
        {
          "emotional shift and without it.": "mistake is also reported by Ghosal et al. (2019). Be-"
        },
        {
          "emotional shift and without it.": "sides, we ﬁnd that DAG-ERC tends to misclassify"
        },
        {
          "emotional shift and without it.": "samples of other emotions to neutral on MELD,"
        },
        {
          "emotional shift and without it.": "DailyDialog and EmoryNLP due to the majority"
        },
        {
          "emotional shift and without it.": "proportion of neutral samples in these datasets."
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": "We also look closely into the emotional shift"
        },
        {
          "emotional shift and without it.": "issue, which means the emotions of two consecu-"
        },
        {
          "emotional shift and without it.": "tive utterances from the same speaker are different."
        },
        {
          "emotional shift and without it.": "Existing ERC models generally work poorly in"
        },
        {
          "emotional shift and without it.": "emotional shift. As shown in Table 5, our DAG-"
        },
        {
          "emotional shift and without it.": "ERC also fails to perform better on the samples"
        },
        {
          "emotional shift and without it.": "with emotional shift\nthan that without\nit,\nthough"
        },
        {
          "emotional shift and without it.": "the performance is still better than previous mod-"
        },
        {
          "emotional shift and without it.": "els. For example, the accuracy of DAG-ERC in the"
        },
        {
          "emotional shift and without it.": "case of emotional shift\nis 57.98% on the IEMO-"
        },
        {
          "emotional shift and without it.": "CAP dataset, which is higher than 52.5% achieved"
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": "by DialogueRNN (Majumder et al., 2019) and 55%"
        },
        {
          "emotional shift and without it.": "achieved by DialogXL (Shen et al., 2020)."
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": "6\nConclusion"
        },
        {
          "emotional shift and without it.": ""
        },
        {
          "emotional shift and without it.": "In this paper, we presented a new idea of mod-"
        },
        {
          "emotional shift and without it.": "eling conversation context with a directed acyclic"
        },
        {
          "emotional shift and without it.": "graph (DAG) and proposed a directed acyclic graph"
        },
        {
          "emotional shift and without it.": "neural network, namely DAG-ERC, for emotion"
        },
        {
          "emotional shift and without it.": "recognition in conversation (ERC). Extensive ex-"
        },
        {
          "emotional shift and without it.": "periments were conducted and the results show"
        },
        {
          "emotional shift and without it.": "that\nthe proposed DAG-ERC achieves compara-"
        },
        {
          "emotional shift and without it.": "ble performance with the baselines. Moreover, by"
        },
        {
          "emotional shift and without it.": "comprehensive evaluations and ablation study, we"
        },
        {
          "emotional shift and without it.": "conﬁrmed the superiority of our DAG-ERC and the"
        },
        {
          "emotional shift and without it.": "impact of its modules. Several conclusions can be"
        },
        {
          "emotional shift and without it.": "drawn from the empirical results. First,\nthe DAG"
        },
        {
          "emotional shift and without it.": "structures built from conversations do affect the per-"
        },
        {
          "emotional shift and without it.": "formance of DAG-ERC, and with the constraints"
        },
        {
          "emotional shift and without it.": "on speaker identity and positional relation, the pro-"
        },
        {
          "emotional shift and without it.": "posed DAG structure outperforms its variants. Sec-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "whether two utterances have the same speaker is",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Cosmic:\nCommonsense\nknowledge\nfor\nemotion"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "insufﬁcient for multi-speaker conversations. Third,",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "identiﬁcation in conversations.\nIn Proceedings of"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "the directed acyclic graph network does not suffer",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "the 2020 Conference on Empirical Methods in Nat-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "over-smoothing as easily as GNNs when the num-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "ural Language Processing: Findings, pages 2470–"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "2481."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "ber of layers increases. Finally, many of the errors",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "misjudged by DAG-ERC can be accounted for by",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Deepanway Ghosal, Navonil Majumder, Soujanya Po-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "similar emotions, neutral samples and emotional",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "ria, Niyati Chhaya, and Alexander Gelbukh. 2019."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "shift. These reasons have been partly mentioned",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Dialoguegcn: A graph convolutional neural network"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "for emotion recognition in conversation.\nIn Proceed-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "in previous works but have yet to be solved, which",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "ings of\nthe 2019 Conference on Empirical Methods"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "are worth further investigation in future work.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "in Natural Language Processing and the 9th Inter-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "national Joint Conference on Natural Language Pro-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Acknowledgments",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "cessing (EMNLP-IJCNLP), pages 154–164."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "We thank the anonymous reviewers. This paper",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Devamanyu Hazarika,\nSoujanya Poria, Rada Mihal-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "cea, Erik Cambria, and Roger Zimmermann. 2018a."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "was supported by the Program for Guangdong In-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Icon:\nInteractive\nconversational memory network"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "troducing Innovative and Entrepreneurial Teams",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "for multimodal emotion detection.\nIn Proceedings"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "(No.2017ZT07X355).",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "of\nthe 2018 Conference on Empirical Methods\nin"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Natural Language Processing, pages 2594–2604."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "References",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Erik Cambria, Louis-Philippe Morency, and Roger"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Zimmermann. 2018b. Conversational memory net-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "and Charles Sutton. 2018.\nA survey of machine",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "work for\nemotion recognition in dyadic dialogue"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "ACM Com-\nlearning for big code and naturalness.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "the 2018 Conference of\nvideos.\nIn Proceedings of"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "puting Surveys (CSUR), 51(4):1–37.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "the North American Chapter of\nthe Association for"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Computational Linguistics: Human Language Tech-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "nologies, Volume 1 (Long Papers), volume 1, pages"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJean-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "2122–2132."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "nette N Chang,\nSungbok Lee,\nand\nShrikanth\nS",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Taichi\nIshiwatari, Yuki Yasuda, Taro Miyazaki,\nand"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Jun Goto. 2020. Relation-aware graph attention net-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Language\nre-\ndyadic motion\ncapture\ndatabase.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "works with relational position encodings\nfor emo-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "sources and evaluation, 42(4):335.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "tion recognition in conversations.\nIn Proceedings of"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "the 2020 Conference on Empirical Methods in Nat-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Ankush Chatterjee, Kedhar Nath Narahari, Meghana",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "ural Language Processing (EMNLP), pages 7360–"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Joshi,\nand Puneet Agrawal. 2019.\nSemeval-2019",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "7370."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "task 3: Emocontext contextual emotion detection in",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "text.\nIn Proceedings of the 13th International Work-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Wenxiang\nJiao,\nHaiqin\nYang,\nIrwin\nKing,\nand"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "shop on Semantic Evaluation, pages 39–48.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Michael R Lyu. 2019. Higru: Hierarchical gated re-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "current units for utterance-level emotion recognition."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "In Proceedings of the 2019 Conference of the North"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Ting-Hao, Huang,\nand Lun-Wei Ku. 2018.\nEmo-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "American Chapter of\nthe Association for Computa-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "tionlines: An emotion corpus of multi-party conver-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "tional Linguistics: Human Language Technologies,"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "sations.\nIn 11th International Conference on Lan-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Volume 1 (Long and Short Papers), pages 397–406."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "guage Resources and Evaluation, LREC 2018, pages",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "1597–1601.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Thomas N Kipf\nand Max Welling.\n2016.\nSemi-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "supervised classiﬁcation with graph convolutional"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Maxwell Crouse, Ibrahim Abdelaziz, Cristina Cornelio,",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "networks. arXiv preprint arXiv:1609.02907."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Veronika Thost, Lingfei Wu, Kenneth Forbus, and",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Achille Fokoue. 2019.\nImproving graph neural net-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "work representations of\nlogical\nformulae with sub-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Cao, and Shuzi Niu. 2017. Dailydialog: A manually"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "graph pooling. arXiv preprint arXiv:1911.06904.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "labelled multi-turn dialogue dataset.\nIn Proceedings"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "of the Eighth International Joint Conference on Nat-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "ural Language Processing (Volume 1: Long Papers),"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Kristina N. Toutanova. 2018. Bert: Pre-training of",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "volume 1, pages 986–995."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "deep bidirectional\ntransformers for language under-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": ""
        },
        {
          "ond,\nthe widely utilized graph relation type of": "standing.\nIn Proceedings of the 2019 Conference of",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "the North American Chapter of\nthe Association for",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "Computational Linguistics: Human Language Tech-",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Luke\nZettlemoyer,\nand Veselin\nStoyanov.\n2019."
        },
        {
          "ond,\nthe widely utilized graph relation type of": "nologies, Volume 1 (Long and Short Papers), pages",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "Roberta: A robustly optimized bert pretraining ap-"
        },
        {
          "ond,\nthe widely utilized graph relation type of": "4171–4186.",
          "Deepanway Ghosal, Navonil Majumder, Alexander": "proach. arXiv preprint arXiv:1907.11692."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Jiankun Lu, Deepanway Ghosal, Alexander Gel-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "bukh, Rada Mihalcea,\nand Soujanya Poria. 2020.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Mime:\nMimicking\nemotions\nfor\nempathetic\nre-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "the 31st International\nyou need.\nIn Proceedings of"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "sponse generation.\nIn Proceedings of the 2020 Con-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Conference on Neural\nInformation Processing Sys-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "ference on Empirical Methods in Natural Language",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "tems, pages 5998–6008."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Processing (EMNLP), pages 8968–8979.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Adriana Romero, Pietro Lio,\nand Yoshua Bengio."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "arXiv preprint\n2017.\nGraph attention networks."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "arika, Rada Mihalcea, Alexander Gelbukh, and Erik",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "arXiv:1710.10903."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Cambria. 2019. Dialoguernn: An attentive rnn for",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "emotion detection in conversations.\nIn Proceedings",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Zhilin Yang, Zihang Dai, Yiming Yang,\nJaime Car-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "of\nthe AAAI Conference on Artiﬁcial\nIntelligence,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "bonell, Russ R Salakhutdinov, and Quoc V Le. 2019."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "volume 33, pages 6818–6825.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Xlnet:\nGeneralized autoregressive pretraining for"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "language understanding.\nIn Advances in Neural In-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "formation Processing Systems, pages 5753–5763."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "jumder, Rada Mihalcea, Gautam Naik,\nand Erik",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Cambria. 2019b. Meld: A multimodal multi-party",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Sayyed M. Zahiri\nand Jinho D. Choi. 2017.\nEmo-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "dataset for emotion recognition in conversations.\nIn",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "tion detection on tv show transcripts with sequence-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "ACL 2019 : The 57th Annual Meeting of the Associa-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "based convolutional neural networks.\nIn AAAI Work-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "tion for Computational Linguistics, pages 527–536.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "shops, pages 44–52."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Soujanya Poria, Navonil Majumder, Rada Mihalcea,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Dong\nZhang,\nLiangqing Wu,\nChanglong\nSun,"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "and Eduard Hovy. 2019a.\nEmotion recognition in",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Shoushan Li, Qiaoming Zhu,\nand Guodong Zhou."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "conversation: Research challenges, datasets, and re-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "2019a.\nModeling\nboth\ncontext-\nand\nspeaker-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "cent advances.\nIEEE Access, 7:100943–100953.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "sensitive\ndependence\nfor\nemotion\ndetection\nin"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "of\nmulti-speaker\nconversations.\nIn Proceedings"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Michael Sejr Schlichtkrull, Thomas N. Kipf,\nPeter",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "the Twenty-Eighth International\nJoint Conference"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Bloem,\nRianne\nvan\nden Berg,\nIvan Titov,\nand",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "on Artiﬁcial\nIntelligence,\nIJCAI-19,\npages\n5415–"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Max Welling. 2018. Modeling relational data with",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "5421.\nInternational Joint Conferences on Artiﬁcial"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "graph convolutional networks.\nIn 15th International",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Intelligence Organization."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Conference on Extended Semantic Web Conference,",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Gar-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "ESWC 2018, pages 593–607.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "nett, and Yixin Chen. 2019b. D-vae: A variational"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "autoencoder\nfor\ndirected\nacyclic\ngraphs.\nIn Ad-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Bj¨orn Schuller, Michel Valster, Florian Eyben, Roddy",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "vances\nin Neural\nInformation Processing Systems,"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Cowie, and Maja Pantic. 2012. Avec 2012:\nthe con-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "pages 1588–1600."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "tinuous audio/visual emotion challenge.\nIn Proceed-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "ings of\nthe 14th ACM international conference on",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Peixiang Zhong, Di Wang, and Chunyan Miao. 2019."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Multimodal interaction, pages 449–456.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Knowledge-enriched transformer for emotion detec-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "tion in textual\nconversations.\nIn Proceedings of"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Weizhou Shen,\nJunqing Chen, Xiaojun Quan,\nand",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "the 2019 Conference on Empirical Methods in Nat-"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Zhixian Xie. 2020. Dialogxl: All-in-one xlnet\nfor",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "ural Language Processing and the 9th International"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "multi-party conversation emotion recognition. arXiv",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "Joint Conference on Natural Language Processing"
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "preprint arXiv:2012.08695.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": "(EMNLP-IJCNLP), pages 165–176."
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "2016. Dag-recurrent neural networks for scene la-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "of\nthe\nIEEE conference\nbeling.\nIn Proceedings",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "on computer vision and pattern recognition, pages",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "3620–3629.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Kai Sheng Tai, Richard Socher,\nand Christopher D",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Manning. 2015.\nImproved semantic representations",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "from tree-structured long short-term memory net-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "the 53rd Annual Meet-\nworks.\nIn Proceedings of",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "ing of the Association for Computational Linguistics",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "and the 7th International Joint Conference on Natu-",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "ral Language Processing (Volume 1: Long Papers),",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "pages 1556–1566.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "Veronika Thost and Jie Chen. 2021. Directed acyclic",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "graph neural networks.\nIn International Conference",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        },
        {
          "Navonil Majumder,\nPengfei Hong,\nShanshan Peng,": "on Learning Representations.",
          "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of machine learning for big code and naturalness",
      "authors": [
        "Miltiadis Allamanis",
        "T Earl",
        "Premkumar Barr",
        "Charles Devanbu",
        "Sutton"
      ],
      "year": "2018",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Semeval-2019 task 3: Emocontext contextual emotion detection in text",
      "authors": [
        "Ankush Chatterjee",
        "Kedhar Nath Narahari",
        "Meghana Joshi",
        "Puneet Agrawal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Sheng-Yeh Chen",
        "Chao-Chun Hsu",
        "Chuan-Chun Kuo",
        "Ting-Hao",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "11th International Conference on Language Resources and Evaluation, LREC 2018"
    },
    {
      "citation_id": "5",
      "title": "Improving graph neural network representations of logical formulae with subgraph pooling",
      "authors": [
        "Maxwell Crouse",
        "Ibrahim Abdelaziz",
        "Cristina Cornelio",
        "Veronika Thost",
        "Lingfei Wu",
        "Kenneth Forbus",
        "Achille Fokoue"
      ],
      "year": "2019",
      "venue": "Improving graph neural network representations of logical formulae with subgraph pooling",
      "arxiv": "arXiv:1911.06904"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "7",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings"
    },
    {
      "citation_id": "8",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "9",
      "title": "2018a. Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "11",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "12",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "13",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2016",
      "venue": "Semisupervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "14",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "16",
      "title": "Mime: Mimicking emotions for empathetic response generation",
      "authors": [
        "Navonil Majumder",
        "Pengfei Hong",
        "Shanshan Peng",
        "Jiankun Lu",
        "Deepanway Ghosal"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "17",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "Sejr Michael",
        "Thomas Schlichtkrull",
        "Peter Kipf",
        "Rianne Bloem",
        "Van Den",
        "Ivan Berg",
        "Max Titov",
        "Welling"
      ],
      "year": "2018",
      "venue": "15th International Conference on Extended Semantic Web Conference"
    },
    {
      "citation_id": "21",
      "title": "Avec 2012: the continuous audio/visual emotion challenge",
      "authors": [
        "Björn Schuller",
        "Michel Valster",
        "Florian Eyben",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "22",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "23",
      "title": "Dag-recurrent neural networks for scene labeling",
      "authors": [
        "Bing Shuai",
        "Zhen Zuo",
        "Bing Wang",
        "Gang Wang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Improved semantic representations from tree-structured long short-term memory networks",
      "authors": [
        "Kai Sheng",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Directed acyclic graph neural networks",
      "authors": [
        "Veronika Thost",
        "Jie Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "",
      "authors": [
        "Petar Veličković",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Lio",
        "Yoshua Bengio"
      ],
      "year": "2017",
      "venue": "",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "28",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "30",
      "title": "Modeling both context-and speakersensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
      "doi": "10.24963/ijcai.2019/752"
    },
    {
      "citation_id": "31",
      "title": "D-vae: A variational autoencoder for directed acyclic graphs",
      "authors": [
        "Muhan Zhang",
        "Shali Jiang",
        "Zhicheng Cui",
        "Roman Garnett",
        "Yixin Chen"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    }
  ]
}