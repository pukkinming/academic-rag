{
  "paper_id": "2007.12519v1",
  "title": "Heu Emotion: A Large-Scale Database For Multi-Modal Emotion Recognition In The Wild",
  "published": "2020-07-24T13:36:52Z",
  "authors": [
    "Jing Chen",
    "Chenhui Wang",
    "Kejun Wang",
    "Chaoqun Yin",
    "Cong Zhao",
    "Tao Xu",
    "Xinyi Zhang",
    "Ziqiang Huang",
    "Meichen Liu",
    "Tao Yang"
  ],
  "keywords": [
    "from search engines such as Tumblr",
    "Google",
    "and Giphy. Meanwhile",
    "the video clips were manually chosen from online videos such as movies",
    "TV shows",
    "graphics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The study of affective computing in the wild setting is underpinned by databases. Existing multimodal emotion databases in the real-world conditions are few and small, with a limited number of subjects and expressed in a single language. To meet this requirement, we collected, annotated, and prepared to release a new natural state video database (called HEU Emotion). HEU Emotion contains a total of 19,004 video clips, which is divided into two parts according to the data source. The first part contains videos downloaded from Tumblr, Google, and Giphy, including 10 emotions and two modalities (facial expression and body posture). The second part includes corpus taken manually from movies, TV series, and variety shows, consisting of 10 emotions and three modalities (facial expression, body posture, and emotional speech). HEU Emotion is by far the most extensive multi-modal emotional database with 9,951 subjects. In order to provide a benchmark for emotion recognition, we used many conventional machine learning and deep learning methods to evaluate HEU Emotion. We proposed a Multi-modal Attention module to fuse multi-modal features adaptively. After multi-modal fusion, the recognition accuracies for the two parts increased by 2.19% and 4.01% respectively over those of single-modal facial expression recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is intended to perceive a person's emotional state through various information channels such as face, speech, physiological signals, and the like. Both rhythm and substance are flourishing in parallel when a person expresses his or her emotional needs. It means that people arouse others' senses in a variety of ways. Current automatic emotion recognition systems, however, are still stuck in identifying a single feature. They lack all emotions and social skills that building intelligent human-computer interaction needs. There are two main reasons: on one hand, the operating environment is often uncontrollable, such as complex background, illumination change, change of camera angle, age, race, and gender of the user. On the other hand, complexity and diversity of emotions make it challenging to realize intelligent humancomputer interaction. Constructing an emotional database under the above uncontrollable environment and including as many expressive categories as possible is an essential step in promoting the practical application of emotion recognition.\n\nMany popular emotion recognition databases are collected in controlled laboratory environments. The subjects are called upon to make certain expressions in lab scenarios. These deliberately generated expressions are not tangible emotions. Moreover, there are some limitations, including a plain background, a single light, and few subjects. Therefore, the actual application effect of an emotion recognition system developed based on these databases is not satisfactory. With the rise of various social platforms and networks, millions of users upload and share their photos and video clips every day. Shooting scenes of these videos are the real application environment of an automatic emotion recognition system. Although some video clips are clipped from movies, they are closer to the real world than the previous databases from lab conditions.\n\nIn recent years, as more and more emotional databases  [37, 26, 9, 15]  in the wild have been collected and annotated, research on automatic algorithms for emotion recognition in the real world has gradually increased. These databases contain different backgrounds and a large number of testees. However, most of the databases are facial expressions and single static images. Consequently, it is difficult for researchers to utilize other emotional expression channels and to learn emotional changes. The process of emotional change is often continuous and time-correlated. Therefore, the information of dynamic expression sequence is more comprehensive and specific than that of a single static image in capturing emotions. Psychology professor Mehrabian pointed out that expression, speech, and language accounted for 55%, 38%, and 7% in emotional interaction, respectively  [36] . The expressions are divided into facial expressions and body postures. One or more movements of facial muscle produce facial expressions. At present, good classification accuracies of basic emotions in the natural environment have been achieved by using face images (more than 70%  [12, 14]  on the RAF-DB  [26]  dataset). Body postures are considered to be another nonverbal communication method of human emotions. They are often understood as changes in the movement of the head, limbs, and other parts of the body. Relative to collecting face, low-resolution collection facilities are enough to acquire postures. As a perfect complement of emotion recognition, the analysis of body movement data has recently become more common. Speech is another critical channel for emotion recognition. Human beings can capture the state of the other's emotions by listening to the speech and perceiving the speaker's modal particle and phonetic tone. According to reports, some emotions (such as sadness and fear) are more easily distinguished from audio signals than from visual appearance  [8] . Emotions are expressed through multiple channels. The emotions collected from the real world are difficult to classify through one channel. In the case of single-modality studies, information is often insufficient, and the classification results are susceptible to various external factors, such as face or body occlusion, and noise. The McGurk effect  [34]  reveals that different organs are automatically unintentionally combined to process the information when brain is sensing. The absent or inaccuracy information can lead to deviations of brain's understanding of external information. That is why multimodal technologies have become more prevalent in automatic emotion recognition recently.\n\nAs mentioned in Sect.2, over the past decades, many organizations have made great efforts to establish multimodal emotion databases. However, there are still three main problems:  (1)  The number of samples of the multimodal emotion database in the natural state is small. There is a big gap compared to the enormous demand for practical applications.  (2)  The number of subjects in these datasets is still small, as shown in Table  1 , with a maximum of 527 subjects. The limited number of subjects hinders the study of identityindependent emotion analysis and recognition.  (3)  The languages of the emotional databases in the natural state in Table  1  are single, and the cultural backgrounds of the subjects in one dataset are the same. Cultural divergences may lead to differences in emotional expression. The actual application effects of systems developed based on these databases can vary greatly depending on the attributes of the user.\n\nIn order to overcome the above problems in the multimodal emotion databases, we collected and annotated a sizeable multi-modal emotion database (HEU Emotion) in the wild environment. The advantages of HEU Emotion are as follows:\n\n-First of all, it is by far the largest multimodal emotion database collected in the natural state. It includes 16,569 video clips downloaded from different websites (Tumblr, Google, Giphy), as well as 2,435 corpora selected from movies, TV series, and live videos. -Secondly, there are 9,951 subjects in HEU Emotion. A massive increase in the number of subjects can significantly reduce the impact of identity information on emotion analysis and recognition. Compared to the existing multimodal emotional datasets shown in Table  1 , HEU Emotion has the largest number of subjects.\n\n-Finally, there are many speakers from different cultural backgrounds, such as Chinese, Americans, Thais, Koreans, etc. In most cases, they speak their native language. Therefore HEU Emotion is an emotional database with multiple languages. Besides, in order to enrich the emotional categories, we annotated three emotions (disappointed, confused, and bored) in addition to the basic seven emotions. In order to extend the research to the real environment, many research institutions have created emotion recognition datasets in the real environment. FER2013  [15]  is a database of the ICML2013 facial expression recognition Challenge. RAF-DB  [26]  dataset is a real-world facial expression database, which is widely used at present. Nevertheless, these datasets are static images that ignore dynamic emotional changes and focus only on facial expressions. They do not make use of the characteristics of multiple ways of emotional expression. This section focuses on datasets that contain dynamic emotional changes and contain multiple emotional modes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "FABO  [16]  database used two cameras to capture simultaneously facial expressions and gestures. The shooting angle was positive, and the background was a blue static setting. At the same time, in order to reduce the impact of illumination changes, the researchers built an artificial light source environment. The subjects were not directly exposed to the light source. At the time of the shooting, the researcher asked the subjects to make specific emotions and perform a combination of different facial expressions and upper body gestures. This bimodal database consists of 23 subjects, 11 males and 12 females, who are between 18 and 50 years old. They come from different countries and regions, such as Europe, the Middle East, Latin America, Asia, and Australia. The researchers filmed nine emotions: angry, scared, surprised, happy, disgusted, bored, worried, sad, and uncertain.\n\nRAVDESS  [31]  includes 60 speeches and 44 songs with emotions (including fear, sadness, surprise, happy, anger, disgust, and neutral), which were recorded by 24 professional actors. There were three forms of work recorded by each actor: audio-visual (AV), video-only (VO), and audio-only (AO). Recordings were recorded in professional studios, with only actors and green screens visible in the lens. To ensure that the camera was able to capture the actor's head and shoulder, the height of the camera was adjusted at any time. Full-spectrum illumination was provided in the studio, illuminated by ceiling fluorescent light and three 28W 5200k CRI 82 bulbs. These settings could minimize facial shadows.\n\nRAMAS  [40]  is the first Russian multimodal emotional database collected by Neurodata Lab LLC. Ten semi-professional actors (5 males and 5 females, aged 18-28, native Russian) were involved in the data collection. The collectors thought that professional drama actors might use stereotypes of action patterns. Accordingly, semi-professional actors were more suitable for performing actions in emotional situations. The semi-professional actor expressed one of the basic emotions (anger, sadness, disgust, happiness, fear, and surprise) in the set scene. Various data such as audio, motion capture, close-up and panoramic video, and physiological data were gathered during recording.\n\nSapinski et al.  [44]  published an emotional database in the Polish language, consisting of three forms: facial expressions, body movements and gestures, and speech. The recordings were recorded in the rehearsal room of T eatr N owy im by 16 professional actors (8 males and 8 females aged 25 to 64). Recordings were performed in a quiet, well-lit environment with a green background. In order to keep the actor's face in the picture and compensate for any movement in the emotional expression process, a medium shot was used. In the case of Kinect recording, the entire body was in the frame, including the legs.\n\nNNIME  [5]  is a recording of dyadic spoken interactions. Forty-four (24 females and 20 males) subjects from the National Taiwan University of the Arts Drama (NTUA) took part in the recording. Every two people were split into one group (7 female-female, 10 female-male, and 5 male-male pairs). Each pair was instructed to perform spontaneously a short scene of about 3 minutes. The overall performance was to provide evidence for one of six pre-specified emotions (anger, sadness, happiness, frustration, neutrality, and surprise).\n\nSEMAINE  [35]  is a multimodal database of emotional conversations between human subjects and computer conversational agents, which are collected by McKeown et al. High-quality recording was made by 5 high frame rate, high-resolution cameras, and 4 microphones. A total of 959 conversations with 150 participants and 4 Sensitive Artificial Listener (SAL) characters were recorded, each lasting approximately 5 minutes. Each clip was traced to 27 related categories by 6-8 raters. Furthermore, there were four main types of basic emotions, epistemic states, interaction process analysis, and validity.\n\nSAVEE  [23]  is a recording of four male subjects between the ages of 27 and 31, containing a total of 480 samples of 7 basic emotions. It used an inductive approach to collect data. Expressions and recordings were recorded when subjects watched video clips and text on the monitor. eNTERFACE'05  [33]  has audio-visual clips recorded in a laboratory environment. Forty-two subjects from different nationalities participated in the recording, and the recorded language was English. The dark gray background was used for the acquisition, and the captured images contained only heads.\n\nRML  [50]  was collected by Ryerson Laboratories and contained 720 samples of six basic expressions. Eight subjects who participated in the recordings spoke various languages. The recording was done in a no-noise atmosphere with a simple gray-green background and a digital camera to capture the videos.\n\nIEMOCAP  [3]  is an action, multi-modal, and multi-peak database collected by the University of Southern California's Sail Lab. It contains approximately 12 hours of audiovisual data, including video, voice, facial motion capture, and text transcription. Participants performed impromptu performances or scripting scenarios. IEMOCAP was annotated by many annotators into category tags such as anger, happiness, sadness, neutrality, and dimension labels such as valence, activation, and dominance.\n\nYu et al.  [52]  collected 721 phrases from Chinese movies and TV series, including 4 emotions (anger, happiness, sadness and neutral).\n\nSAFE  [6]  and Fiction  [49]  focuses on extreme emotions in abnormal situations. Unlike other databases, the emotions of these two databases fell into four categories: fear, negative emotions, neutral emotions, and positive emotions.\n\nMELD  [41]  was evolved from the EmotionLines dataset  [4] . EmotionLines only contains conversations from the TV series \"Friends\". MELD is a multimodal emotional conversation dataset, containing audio, visual, and text modalities. Since data were obtained from only one TV series, the number of participants was limited, and 84% of the sessions were obtained by 6 starring actors.\n\nCHEAVD  [27]  contains 140 minutes of emotional episodes from movies, TV shows, and talk shows. There are a total of 26 atypical emotional states, and a variety of emotional tags and fake/suppressed emotional tags. However, the emotional categories used in the final experiment were only six basic emo-tions. Moreover, the number is exceptionally uneven, neutral : surprised : disgust = 19.6 : 1.6 : 1.\n\nCHEAVD 2.0  [28]  is an extension of CHEAVD. In addition to the six basic emotions, it adds worried and anxious. As in  [27] , the database also has an extremely unbalanced number of sample, neutral : surprised : disgust = 10 : 1.2 : 1. Also, from the confusion matrix of the given baseline, it could be seen that other categories except for anger, happiness, and neutral were challenging to identify. Especially, the recognition accuracies of surprise and disgust were zero.\n\nAFEW  [10]  is an emotional database in the wild environment proposed by A.  Dhall   Table  1  summarizes the characteristics of the reviewed databases in categorical model.\n\nFrom the above review, it can be seen that many audio-visual databases were collected in a laboratory environment. The non-laboratory databases, SAFE and Fiction, focus on extreme expressions and are not conventional emotion recognition. CHEAVD and CHEAVD 2.0 are collected from movies and TV series, but their samples are only in the Chinese language. AFEW only provides English samples. Besides, these datasets have the most natural expressions and laughter expressions, and the amount of other expressions which are not easily distinguishable is even less. Due to the current small scale of emotional datasets in the natural environment, the single language, and the uneven distribution of various emotional samples, we established a large-scale, multiple languages and relatively balanced database HEU Emotion. Additionally, our HEU Emotion was recorded in the natural environment. Although it contains many noises, it is more in line with the real-world application environment and can help to further improve the generalization and robustness of the emotion recognition system.\n\n3 Heu emotion database HEU Emotion is currently the largest video-based multimodal emotion recognition database in the wild. In order to make the benchmark test closer to the actual application, we downloaded the relevant video clips by retrieving the emotion-related keywords from search engines such as Tumblr, Google, and Giphy. Meanwhile, the video clips were manually chosen from online videos such as movies, TV shows, and variety shows. Data of HEU Emotion part1 (HEU-part1) contained numerous non-personal information like drawing, graphics, or non-human objects which needed to be filtered out by an automatic al-gorithm. All clips extracted manually in HEU Emotion Part2 (HEU-part2) contained characters and therefore could be labeled directly. Figure  1  shows the process of database construction, and the information represented by each color is given on the right side of the graph. Details will be presented in the following sections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Data",
      "text": "Fig.  1  The process of building the HEU Emotion database",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Collecting Data",
      "text": "Firstly we created a query list of emotional keywords (e.g., nervous, disgusted, happy, sad, fear, anger, scared, surprised, bored, confused, disappointed, frustrated, etc). Three search engines (Tumblr, Google, Giphy) were queried with these emotion related tags, and URLs of videos were stored in a document. Then, 47,450 original video clips were obtained by batch download using the automatic downloader. Since a high percentage of the results returned by our query words already contained neutral expression videos, no separate query was made to obtain additional neutral expressions. The 3500+ original clips in HEU-part2 were picked out by 5 staff members. Manually edited videos follow the following rules: (1) in each video, the performer has a single expression. That is, only one expression appears, and the other expressions do not appear as far as possible.  (2)  the shots in the video should be kept on the actors who express their emotions as far as possible, to avoid switching back and forth.\n\n(3) A long video can be divided into several parts. However, each part presents a different stage of emotional expression. (4) multiple performers can be included in the same frame, but in most instances all try to express the same emotion.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Extracting Frames",
      "text": "After going to gain the video clips, we did the relevant processing work on the videos. The FFmpeg of OpenCV was used to get the JPG image from each video. The frame rate was given according to that of each video instead of setting a fixed one. Generally, humans can complete the entire process of emotional change and reach a peak within 10 seconds. Consequently, to reduce redundant information and computational burden, we re-edited the videos with lengths longer than 10 seconds.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Automatic Filtering By Pedestrian Detection",
      "text": "Considering that the video data which were obtained from Internet contained non-human objects, the video frames were filtered by YOLO V3  [42] . Among them, a frame was defined as a solid frame in which people can be detected. A clip containing of valid frames was defined as a sound clip, and invalid clips were directly deleted. We picked out characters from the original pictures under the bbox positioned by YOLO V3. When there was more than one character in the video, the emotion was often expressed by the one who took up a larger area. We calculated the area of the box where each character was located according to the position of the given bbox. Then, to make sure that only one character was in the image, the one with the largest area was presented after pedestrian detection. Since the sizes of images obtained directly were different, we performed a normalization process. In consequence of the irregular sizes of raw images, we could not find a suitable size to fit all the images, so we put the images into squares. Moreover, the background of the original image was generally complicated, and thus, we did not expand the cutting position on the original image. Instead, the difference between the length and width was calculated. After that, one-half of difference was expanded on both sides of the small side to obtain a rectangular picture. Results of normalization are shown in Fig.  2 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Obtaining Facial Expression Data",
      "text": "In the face detection section, we used two efficient face detection methods, MTCNN  [53]  and libface-detection (libface)  1  to achieve the effect of mutual complementation and to reduce missed detection. Figure  3  shows the images detected by libface, which only contained the faces. The length-width ratios of the images detected by MTCNN were not uniform. In order to facilitate the training of the model, the detected images were processed as follow. The values of the two sides were respectively calculated in the light of the given position of the box. The starting position of the smaller side was unchanged, and the ending position was calculated on the basis of the length of the longer side and the starting position.\n\n(1)\n\nwhere bbox is the coordinates of the detected face. Bbox[0][0] is the initial position of the x and y axes of the picture; Bbox[0]  [1]  is the initial position of the x-axis and the end position of the y-axis; Bbox  [1] [0] is the end position of the x-axis and the initial position of the y-axis; Bbox  [1]  [1] is the end position of x-axis and y-axis. Therefore, h is the length in the y-axis direction and w is the length in the x-axis direction. Images detected by MTCNN included not only face but also head. Samples are presented in Fig.  4 . Besides, the data of the HEU-part1 were more complicated, and face detections were based on the images detected by  [42] . The data in the HEU-part2 were ideal, and they were directly detected on the JPG images. In practical application, libface has better recognition ability for a variety of facial angles and occlusion. But many images are misdetected as local organs (such as nose, ears, hands, etc.). The final face image is mainly the face image detected by MTCNN. When MTCNN can not detect any face, the data detected by libface is used, and the bad image is deleted manually.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Database Annotation",
      "text": "The keywords used when downloading HEU-part1 depend on the information annotated by the user at the time of upload, which is not exactly consistent with the real emotional state. That means that all data needs to be re-tagged. Annotating nearly 45,000 videos is a challenging and time-consuming task. In the process of manual tailoring of HEU-part2, collectors were asked to cut videos according to the emotional state. However, the deviation of human emotional judgment and the different sensitivity of each person to emotion exists. In order to avoid the error caused by personal judgment, the annotation work was carried out jointly by multiple annotators. The annotation process is wholly blind and independent. There were a total of 15 tagging staff, and they were taught emotional psychology for a week before the tagging work began, including the definition of each emotional category, main features and some examples. Fifteen annotators tagged all the video clips independently and divided them into the most obvious of the 10 categories. We counted the annotated data, marked it by voting, and selected the one with the highest number of votes as its label. If the number of votes in several categories was very similar. One of them did not appear to be significantly higher than the other categories. We put the clip as a mixed emotion in another class. Because of limited time and workforce, there was not any further processing of the compound expression.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Statistics",
      "text": "The HEU Emotion database contains 19,004 video clips, which is divided into two parts: HEU-part1 and HEU-part2. HEU-part1 was divided into three parts, 80% for training, 10% for verification, and 10% for testing. The concrete number of each category is presented in Tables  2  and 3 . Table  2  shows all the data of HEU-part1, while Table  3  only shows the data that face can be detected in HEU-part1. From the comparison of Tables  2  and 3 , it can be observed that faces are difficult to be detected or too small to judge their emotions in some clips. However, the emotional state of the characters can still be judged by their body postures (such as waving his arms when angry, holding his head with his hands when he is afraid, dancing when he is happy, etc.). This illustrates the necessity of body posture emotional data. The characters in the video clips include Asians, Africans, and Caucasians. We used DEX  [43]  to estimate age and gender in facial data. According to the results, 67% of faces are male, and women account for 33%. Through age estimation, the average age of the male face is 34.67, and that of women is younger, 28.94 years old. In particular, the histogram of the number of faces in age ranges [0,20),  [20, 30) ,  [30, 40) ,  [40, 50) ,  [50, 60) ,[60,-]are depicted in Fig.  5a .\n\nWe used the same statistical method for HEU-part2. Drawing on the division of AFEW, HEU-part2 was divided into two parts, 65% as the training set and 35% as the validation set. Frames of HEU-part2 all contain facial expressions, the statistics of the categories are shown in Table  4 . Age and gender estimates were also performed on face data using DEX. Men account for 58%, and women account for 42%. The detailed age distribution is shown in Fig.  5b . Estimated by DEX, both datasets are male-majority, with ages concentrated at  [20, 50] . The result may be affected by datasets, which are used to train model weight. The actual gender and age distribution may be more balanced than that shown in Fig.  5 . In addition, we performed face matching using the face recognition program in dlib  [24] . The number of subjects included in HEU Emotion is 9,951, of which HEU-part1 is 8,984 and HEU-part2 is 967.\n\nWe also compared HEU-part1 with CHEAVD2.0 and RAF-DB, as shown in Fig.  6 . RAF is a facial expression dataset in the real environment which is widely used at present, and it has a relatively large amount of data. CHEAVD2.0 is a large multimodal emotion dataset. We compare these two datasets with HEU-part1. The amount of other emotions is similar or much higher than RAF-DB except for happy, anger and surprise. There are more clips of happy and neutral emotions in all three datasets. It shows that uneven distribution is a shared problem among most datasets. Category distribution of HEU-part1 is comparatively more balanced than others. The data sources of HEU-part2 are the same as those of AFEW. In Figure  7 , HEU-part2 is compared with AFEW and CHEAVD. We only know the emotional categories of the CHEAVD training set and test set, so Figure  7  shows the sum of the training set and test set of CHEAVD. It can see from Fig.  7  that the number of all categories of HEU-part2 is higher than that of AFEW. In CHEAVD, neutral emotion data account for about half of the total sample, and the number of other hard-to-identify emotions (disgust,fear, and so forth.) is minimal. Although the number of some categories is smaller than CHEAVD, HEU-part2 data is relatively more balanced, especially when only seven basic emotions are evaluated.\n\nFacial expressions in the HEU Emotion dataset contain multiple perspectives (top, bottom, and horizontal), multiple angles (front, side), partial occlusion, and multiple resolutions. The intensity of expression in some videos will be transforming; for example, the expression starts with no emotion and then gradually increases the emotion to the peak. Figure  8  displays some samples of the expression described above.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Baseline",
      "text": "In this section, four baselines are proposed for multi-modal emotion recognition for the HEU Emotion dataset. Video treatment flowsheet takes videos as input, extracts single-modal features and finally completes emotional recognition through a variety of fusion methods. The flowchart is shown in Fig.  9 .\n\nWe designed four challenging benchmark experiments. (1) To expedite the transformation of research from the laboratory condition to the real environment, we conducted comparative experiments on the small popular CK+ dataset  [32] .  (2)  In order to prove the validity of our database, the comparative experiments were done with AFEW dataset, and the baseline of facial expression recognition was given. (3) The HEU Emotion was trained and tested by",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Multi-Modal Attention Mechanism",
      "text": "Our dataset contains multiple modals of data. For one video, not every type of data plays a positive role in the final judgment. The judgment of human emotion is mainly based on facial expression, followed by the pronunciation and intonation of speech. The movement of the body can also assist the judgment to a certain extent. But it is worse than the other two kinds of information. All in all, the proportion of the data in the dataset to the judgment of the final emotion category is different. We propose a multimodal attention module (MMA) to adaptively adjust the proportion of different modal features according to their contribution to the classification results. The specific structure of the multimodal attention module is shown in Fig.  10 .\n\nChannel-Wise Multiplication First of all, we spliced the video-level features of each modality to get the input attention of the multimodal attention module X ∈ R D×C , where D represented the dimension of the video-level feature, and C represented the number of input modes, which was called the number of channels. We used the global average pooling operation to compress along the dimensions of video-level features, turning each one-dimensional feature into a real number, which to some extent had a global receptive field and represented the global distribution of responses on each channel. Notice that the number of channels C is constant.\n\nwhere y c represents the real number obtained by global average pooling on the c-th channel. x i c represents the i th element on the c-th channel. Then we use two full connection layers (FCs) to model the correlation between channels. In the first full connection layer, we increase the dimension of the channel to 7 times of the input. After being activated by the activation function ReLU, we reduce the channel to the original dimension through the second full connection layer. The use of two FC and ReLU makes the nonlinearity better and can better fit the complex correlation between channels. Generally speaking, the number of modes in multimodal fusion is not very large, and the channel is upgraded in order to improve the learning ability and information processing ability of the network. Furthermore, it will not bring too many parameters and calculations. Then a Sigmoid function is used to normalize the resulting attention weight to between 0 and 1. Therefore, the output of the Sigmoid function represents the importance of each channel after feature selection.\n\nwhere W 1 ∈ R c×7c represents the parameter of the first fully connected layer, where 7 is the scaling ratio. W 2 ∈ R 7c×c represents the parameters of the second fully connected layer. The correlation between modeling channels is displayed by learning the parameter W of the network. Then the original feature was re-calibrated in the channel dimension by weighting the previous feature channel by multiplication.\n\nwhere u c represents the feature on the c-th channel after recalibration. Finally, the feature map after feature selection was compressed into a vector and fed to the classifier.\n\nDiscussion MMA is inspired by the channel attention module SENet  [20] . The main difference is that SENet captures the importance of each channel in the feature extraction process. The input here comes from the convolution results of different convolution kernels. The inputs of MMA are features from different modalities, and each feature is spliced as a channel. Then the problem of different effects of each modality on the final result in multi-modalities is transformed into modeling the correlation between each channel. Through the automatic learning mode obtain the importance of each feature channel. Then according to this importance the useful features are enhanced and the features that are not useful are suppressed for the current task. Another difference is that SENet is averagely pooled on a two-dimensional feature graph, while our input features are multiple one-dimensional vectors, so our global average pooling is carried out in one-dimensional vectors.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Comparison With Ck+ Dataset",
      "text": "In the experiments from the laboratory environment to the real world, the two parts of HEU-Emotion were made. We conducted comparative experiments using the most popular dataset CK+. Firstly, the facial information of CK+ dataset was extracted. Then the dataset was split into training set and test set in a ratio of 4:1 for each type of expression. Furthermore, the last five pictures of each folder were chosen as samples. The validation set and test set of HEU-part1 were combined together to be used as the test set, so the proportion of training and test sets was 4:1. The ratio of HEU-part2 remained unchanged at 0.65:0.35. In the same way, five pictures of each folder were supposed to form an experimental sample. Taking into account of the small dissimilarity between adjacent frames, the following sampling method was adopted: if the total number was less than 5, all were selected; if less than 10, the first 5 were taken; if less than 14, one picture was taken every interval; If there were less than 18, one picture was taken at intervals of two frames; the remaining interval was taken as 3 frames.\n\nIn the baseline experiment, hand-crafted features extracted by conventional machine learning methods were used. For Local Binary Pattern (LBP)  [1] , the images were resized to 128 × 128 pixels as input and then divided into 8 × 8 pixel blocks. The LBP descriptor applied the Uniform Pattern and generated 3,776-dimensional feature vectors for each image. The Histogram of Oriented Gradient (HOG) features  [7]  also used images of 128 × 128 pixels as input. The HOG features utilized shape-based segmentation dividing the image into 16 × 16 pixel blocks of four 8 × 8 pixel cells with no overlapping. We got 8,100-dimensional HOG feature vectors for each image by setting nine bins. For Gabor wavelet  [30] , we adjusted face images to the size of 16 × 16 pixels and implemented 128 Gabor filters in 16 spatial scales and eight directions. Each image was divided into 10 × 10 blocks, each block was filtered with 128 filters, and finally, the feature vectors of 12,800 dimensions were obtained. After feature extraction using LBP, HOG and Gabor wavelets, Support Vector Machines (SVM)  [48]  was used as classifiers for classification. It can be seen from the second column of the data in Table  5  that the extracted LBP, HOG and Gabor features are valid on the face data. When they were used on the HEU Emotion dataset, the accuracy rate is only over 20%, with a maximum of 35.88%. The main reason why HEU Emotion performs poorly is that the actual conditions are more challenging than the laboratory environment. The illumination change and partial occlusion problems increase the difficulty of recognition. What is more, the result also indicates that the effectiveness of transferring the method developed in the laboratory environment to the real condition is not guaranteed. In order to further develop the actual application of affective computing, it is necessary to establish a large-scale emotional dataset in the real environment. As the most massive dynamic, temporal multimodal emotion recognition database, HEU Emotion will undoubtedly promote the advancement of emotion recognition.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Cross-Dataset Experiment",
      "text": "Deep learning has made breakthroughs in many application areas of artificial intelligence such as machine vision, speech recognition, and natural language processing. In recent years, the methods proposed in the emotion recognition task are also based on deep learning. From the best papers of the EmotiW Challenge  [13, 21, 29] , it can be seen that the deep learning methods are very advantageous to solve emotion recognition problems in the real environment. To demonstrate the validity of the HEU Emotion dataset, we compared it with the AFEW dataset. For deep learning methods, we took classic convolutional neural network models such as VGG  [47] , Resnet  [19] , Densenet  [22] , and SE-inception  [20]  as feature extraction networks, where VGG was the best model in the paper of the EmotiW 2016 Challenge winner  [13] . The HEU Emotion is an emotion recognition dataset of sequencial task. After performing feature extraction of a single frame, we used the GRU to process the features further and finally sent them to the classifier to get the emotional category of the clip. Before training, face image was resized to 229 × 229 pixels. In the training phase, image data enhancement operations such as random cropping and horizontal flipping were performed, and the input image was randomly cropped to 224 × 224 pixels. Sixteen pictures were randomly selected from a video sequence as inputs in the training process. If the entire video sequence was less than 16, the last picture was copied until 16 pictures were satisfied. In the validation and test phase, 16 consecutive frames in the video were taken. If the total number of frames was more than 16, it was taken as the second group, and so on until the entire pictures were taken, and 8 frames in the adjacent group were overlapped. When multiple sets of input were taken, the average of their outputs was fed into softmax to give the final result. As a result, a video was only available in one emotional tag.\n\nConvergence was particularly slow when using HEU Emotion to train the model directly. Considering that using pre-training models can speed up the convergence  [25, 38, 39] , we trained the model of the convolutional neural network in the popular non-laboratory static expression database FER2013  [15]  to obtain the parameters. When training the AFEW and HEU Emotion datasets, all parameters except that of the classifier were loaded and then globally finetuned. The network was trained with 500 epochs and used the batch size of 16. Since a pre-training model was used, the initial learning rate was set to 0.0001. The optimizer used a stochastic gradient descent (SGD) with a fixed learning rate.\n\nThe partition of the HEU Emotion dataset is described in Sect. 4. According to the distribution of EmotiW2019 dataset, AFEW dataset is divided into training set (773) and verification set (383). The second column of Table 6 is the result of the verification set trained on the AFEW dataset. It can be seen that the classical network is effective for facial expression feature extraction. In particular, the features extracted by VGG are fed into GRU to achieve the highest accuracy of 50.28%. The same algorithm has different results on our HEU Emotion dataset. For HEU-part1, it reached 47.53% on  Table  7  shows the experimental results of cross-dataset testing. We use VGG+GRU, the most effective method shown in Table  6 , to experiment across datasets. The first two columns in Table  7  show the datasets used for training and the corresponding number of categories, the third column shows the test sets used, and the last column shows the accuracy on the corresponding test sets. Firstly, the parameter model trained on the two-part dataset of HEU Emotion is tested on the training and verification dataset of AFEW. Emotion categorie of HEU Emotion is 10. Consequently, the model parameter can not be directly applied to AFEW. We replaced 10 classifiers with classifiers trained by AFEW. From the first and second rows of Table  7 , we can see that the parameters trained with all 10 types of data from HEU-part2 have poor results on the AFEW dataset. After that, we used the basic seven types of emotion data in HEU-part2 to train and got the accuracy of 53.17%. When the param-   6 , that of HEU-part2 (7 class) is slightly lower. But it reveals that the sample distribution of HEU-part2 and AFEW is very similar to the annotation standard. We used the same authentication method for HEU-part1. From the six or seven rows in Table  7 , we can see that the feature distribution obtained by 10 classification is different from that of 7 classification, and the result is not ideal. In order to further explain that the number of categories of HEU-part1 in the training process will affect the final feature distribution. Finally, we chose the seven basic categories of the HEU-part1 training set for training. The results containing only seven categories are 58.40% in verification set and 51.46% in test set respectively. Then the parameters were applied to the AFEW. The accuracy of AFEW training set is 56.26%, and that of verification set is 42.33%.\n\nAs can be seen from the analysis of Tables  6  and 7 , when there are only 7 types of expressions in the HEU Emotion, the results on the validation and test sets are better. The addition of the other three types of expressions reduces the overall accuracy. To verify the influence of the other three emotions, the diagonal values of the normalized confusion matrix on the HEU Emotion are listed in Table  8 .\n\nIt can be observed from Table  8  that except for bored, confused, and disappointed emotions, the accuracy of most other categories is slightly higher than that of 10, especially anger and neutral. The results indicates that the addition of categories does affect the distribution of final features, especially bored, which is highly similar to neutral data.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "The Baselines Of Other Modalities",
      "text": "In addition to the baseline for facial expression recognition, we also used the method of deep convolutional neural network (CNN) to give baselines for another two modalities. Body postures emotion recognition and human behavior recognition were both judged by human body movements, but the target domains were different. Therefore, we applied the methods of human behavior recognition research  [17, 46, 54]  in recent years to posture emotion recognition. To accelerate the convergence speed, we used the weight of these models on action recognition as the pre-training model. Many networks were given in  [17] . We chose the best-performing model ResNeXt-101. Each image was resized to 112 × 112 pixels before entering the network. In the training phase, we took 16 consecutive frames as one input, and the index of the starting image was random. When the index of the ending image exceeded the range, the last image was reproduced to 16 frames. In the test phase, 16 consecutive frames were input, besides here all the pictures were taken in sequence. The batch size was 3, the initial learning rate of HEU-part1 was 0.01, and HEU-part2 was 0.1. The learning rate was reduced to one-tenth of the previous one every 30 times. In addition, we chose ResNet101 as the spatial model in  [46] . First, images were resized into 224 × 224 pixels. During the training process, three frames were randomly selected from each video, and all frames were used in the testing stage. The initial learning rate was set at 0.0005, and the learning rate decreased by 0.1 times for every 50 batches. The optimizer used the Stochastic Gradient Descent method (SGD) with momentum. For  [54] , we also used the spatial model Resenet 152 to train with a random frame image in each video, and verified with a single-frame image in the middle of the video. The batch size was set to 25. The initial learning rate was 0.01, and the learning rate per 100 batches was attenuated by 0.1 times. Table  9  shows the baseline of body posture emotion recognition of HEU Emotion database. ResNeXt-101 uses 3D convolution kernels to process sequencial data, and another two methods are based on single-frame images. Compared with the baseline of the facial expressions given in Table  6 , when the accuracy of facial expression recognition in the HEU-part1 test dataset achieves 41.19%, the accuracy of the posture emotion recognition can reach 33.02%. It proves that posture emotion recognition is also a meaningful way to judge emotions and also shows the effectiveness of posture emotion data of HEU Emotion dataset. Accuracy of HEU-part1 is higher than that of HEU-part2, but the opposite is exact in facial expression recognition. Considering that HEU-part1 has more videos of posture emotions, in other words, it contains the whole body or movements of a broader range. It can also be seen from the quantitative comparison in Table  2  and Table  3 . HEU-part2 is mostly close-shot, which contains slightly worse body information. Although HEU-part2 includes less posture information, it has more emotional speech information.\n\nIn order to get the baseline of emotional speech, we first calculated 16 low level descriptors (LLDs), which consisted of zero-crossing rate, energy square root, pitch frequency (F0), signal-to-noise ratio (HNR), MFCC1-12. Then, we got 32 LLDs by calculating the first-order difference of these 16 LLDs. These audio features can be extracted by openSMILE toolkit  [11]  based on INTERSPEECH 2009 audio template  [45] . Each video clip was computed to obtain 384-dimensional features, and then classified by SVM.\n\nWe also gave another baseline using raw speech information and a deep convolutional neural network. The sampling rate of speech was 16K, that was, the speech in one second was a one-dimensional vector with a length of 16,000. The length of each speech sample was inconsistent.When the length was more than 20,000, the vector was randomly truncated to the length of 20,000, and when the length was less than 20,000, it was supplemented with 0. The vector of length 20,000 was sent to the one-dimensional convolution network (ResNet10) for training. During the test, each sample was cut into 20,000-dimensional vectors by a step size of 160, and multiple one-dimensional vectors can be gained. The test results were put to the vote to obtain the final prediction result of each sample.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Model",
      "text": "Accuracy(%) Opensmile+SVM  [11]  37.15 ResNet10  [19]  28.64\n\nThe accuracy of the speech emotion recognition on the HEU-part2 validation set is shown in Table  10 . It reveals that the audio features extracted by openSMILE can get a better classification result of 37.15%. Compared with another two modalities, the speech emotion also achieves competitive results. Hence, the emotional speech data of HEU-part2 is also effective.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "From the baselines of the three single modalities given in Sect. 5.3 and Sect.5.4, single-modal emotion recognition can achieve excellent results. Compared with single modality, multi-modality can get higher accuracy by complementing each other. Our database contains faces, body postures, and speeches, which all play roles in the judgment of emotions. When a face is not available, the body posture can help to recognize emotion. In some cases, some emotional expressions do not have corresponding postures. Therefore, we need to perform a comprehensive analysis of the three modalities. We used the facial expression recognition model  [47] , the posture emotion recognition model  [18]  and the speech emotion recognition model  [11]  to perform fusion in various ways. We used three fusion methods: MMA, fusion proposed by  [2]  and late fusion  [51] . During the forecasting phase of late fusion, each model predicted the probability that the sample belonged to the relevant emotion. Assign appropriate weights to each model to get the following probability representation:\n\nWhere 4 i=1 w i = 1, w i is the weight of each model. The subscript of p represents the corresponding modality, and the weight is w i = 0 when the corresponding modality is not available. We used a grid search strategy to get the weight.  From the results in Tables  11  and 12 , we get the following observations. Our proposed multimodal fusion module MMA achieves better results than other methods. That is mainly due to the fact that MMA can adaptively adjust each mode on the classification results. Compared with the late-fusion fixed model weight, the way is more flexible and accurate. The HEU-part1 verification set increased by 1.69% from 47.53% to 49.22%, and the test set increased from 41.19% to 43.38%, an increase of 2.19%. The verification set of HEU-part2 has an accuracy of 51.03% on the facial expression dataset, and the overall accuracy is improved by 4.01% after being fused with speech and posture.\n\nIt proves that multimodal information is essential for video-based emotion recognition. From Tables 6, 9 and 10, we can see that facial expressions have more advantages over other modes. The multi-modalities can make up for the impact of occlusion or other noise on facial expression recognition and achieve better recognition accuracy.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Conclusion",
      "text": "Human emotion recognition is a challenging but meaningful research work. However, the existing multimodal emotional databases in the wild are small in scale, with a limited number of subjects, expressed in a single language, or few samples containing certain emotions. In this article, we collected and annotated a new multi-language multimodal video emotion database HEU Emotion (the number of subjects is 9,951). It consists of two parts: the bimodal database HEU-part1 (including facial expressions and body posture emotions) and multimodal database HEU-part2 (including facial expressions, body postures, and speeches). HEU Emotion was recorded in a multimodal synchronous way and can be directly used for multi-modal emotion recognition experiments. The videos of HEU Emotion were taken under uncontrollable natural conditions and included a variety of the change of multi-view face and body posture, local occlusion, illumination change, and changes in expression intensity. Our experiments proved that the algorithm trained on the lab-controlled databases were no longer suitable for emotion recognition tasks in the wild. Cross-dataset experiments with AFEW demonstrated that our database was superior to the AFEW. Finally, we performed multi-modal emotion recognition experiments using facial expressions, body postures, and emotional speeches. The evaluation indicators showed that it was beneficial to use multi-modality to process real-world video. To date, HEU Emotion has been the largest multimodal emotional recognition database in the natural environment. We hope that HEU Emotion can promote the development of multimodal emotion recognition and improve the performance of automatic affective computing systems in real-world applications.",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The process of building the HEU Emotion database",
      "page": 8
    },
    {
      "caption": "Figure 2: Fig. 2 The normalization samples of body posture",
      "page": 9
    },
    {
      "caption": "Figure 3: shows the images",
      "page": 10
    },
    {
      "caption": "Figure 3: Samples of Face Images Detected by Libface-detection",
      "page": 11
    },
    {
      "caption": "Figure 4: Samples of Face Images Detected by MTCNN",
      "page": 11
    },
    {
      "caption": "Figure 5: In addition, we performed face matching using the",
      "page": 12
    },
    {
      "caption": "Figure 7: , HEU-part2 is compared with AFEW",
      "page": 12
    },
    {
      "caption": "Figure 7: shows the sum of the training set and test",
      "page": 12
    },
    {
      "caption": "Figure 8: displays some samples",
      "page": 13
    },
    {
      "caption": "Figure 5: The age distribution of HEU Emotion. (a) HEU-part1 (b) HEU-part2",
      "page": 13
    },
    {
      "caption": "Figure 6: The comparison between HEU-part1, RAF-DB and CHEAVD 2.0",
      "page": 14
    },
    {
      "caption": "Figure 7: The comparison between HEU-part2, AFEW and CHEAVD",
      "page": 14
    },
    {
      "caption": "Figure 9: We designed four challenging benchmark experiments. (1) To expedite the",
      "page": 14
    },
    {
      "caption": "Figure 8: The facial expressions of HEU Emotion in many diﬀerent situations. (a) Look at",
      "page": 15
    },
    {
      "caption": "Figure 9: The ﬂowchart of our multi-modal emotion recognition method. It begins with ex-",
      "page": 15
    },
    {
      "caption": "Figure 10: Multi-modal attention module",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "mapping",
          "Column_2": ""
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Face recognition with local binary patterns",
      "authors": [
        "T Ahonen",
        "A Hadid",
        "M Pietikäinen"
      ],
      "year": "2004",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "2",
      "title": "Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection",
      "authors": [
        "H Ben-Younes",
        "R Cadene",
        "N Thome",
        "M Cord"
      ],
      "year": "2019",
      "venue": "Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection",
      "arxiv": "arXiv:1902.00038"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "S Chen",
        "C Hsu",
        "C Kuo",
        "L Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "5",
      "title": "Nnime: The nthuntua chinese interactive multimodal emotion corpus",
      "authors": [
        "H Chou",
        "W Lin",
        "L Chang",
        "C Li",
        "H Ma",
        "C Lee"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "The safe corpus: illustrating extreme emotions in dynamic situations",
      "authors": [
        "C Clavel",
        "I Vasilescu",
        "L Devillers",
        "G Richard",
        "T Ehrette",
        "C Sedogbo"
      ],
      "year": "2006",
      "venue": "First International Workshop on Emotion: Corpora for Research on Emotion and Affect (International conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "7",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "IEEE computer society conference on computer vision and pattern recognition (CVPR'05)"
    },
    {
      "citation_id": "8",
      "title": "Facial emotion recognition using multi-modal information",
      "authors": [
        "L De Silva",
        "T Miyasato",
        "R Nakatsu"
      ],
      "year": "1997",
      "venue": "Proceedings of ICICS, 1997 International Conference on Information, Communications and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
    },
    {
      "citation_id": "10",
      "title": "Collecting large, richly annotated facialexpression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE MultiMedia",
      "doi": "10.1109/MMUL.2012.26"
    },
    {
      "citation_id": "11",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Multi-region ensemble convolutional neural network for facial expression recognition",
      "authors": [
        "Y Fan",
        "J Lam",
        "V Li"
      ],
      "year": "2018",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "13",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "14",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "16",
      "title": "A bimodal face and body gesture database for automatic analysis of human nonverbal affective behavior",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2006",
      "venue": "18th International Conference on Pattern Recognition (ICPR'06)"
    },
    {
      "citation_id": "17",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Learning supervised scoring ensemble for emotion recognition in the wild",
      "authors": [
        "P Hu",
        "D Cai",
        "S Wang",
        "A Yao",
        "Y Chen"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "22",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "23",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "24",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "V Kazemi",
        "J Sullivan"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in the wild via convolutional neural networks and mapped binary patterns",
      "authors": [
        "G Levi",
        "T Hassner"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "26",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.277"
    },
    {
      "citation_id": "27",
      "title": "Cheavd: a chinese natural emotional audiovisual database",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao",
        "W Bao",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "28",
      "title": "Mec 2017: Multimodal emotion recognition challenge",
      "authors": [
        "Y Li",
        "J Tao",
        "B Schuller",
        "S Shan",
        "D Jiang",
        "J Jia"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia)"
    },
    {
      "citation_id": "29",
      "title": "Multi-feature based emotion recognition for video clips",
      "authors": [
        "C Liu",
        "T Tang",
        "K Lv",
        "M Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "30",
      "title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition",
      "authors": [
        "C Liu",
        "H Wechsler"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Image processing"
    },
    {
      "citation_id": "31",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "32",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops"
    },
    {
      "citation_id": "33",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "34",
      "title": "Hearing lips and seeing voices",
      "authors": [
        "H Mcgurk",
        "J Macdonald"
      ],
      "year": "1976",
      "venue": "Nature"
    },
    {
      "citation_id": "35",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Communication without words. Communication theory",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2008",
      "venue": "Communication without words. Communication theory"
    },
    {
      "citation_id": "37",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "38",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "39",
      "title": "Towards facial expression recognition in the wild: A new database and deep recognition system",
      "authors": [
        "X Peng",
        "Z Xia",
        "L Li",
        "X Feng"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "40",
      "title": "Ramas: Russian multimodal corpus of dyadic interaction for affective computing",
      "authors": [
        "O Perepelkina",
        "E Kazimirova",
        "M Konstantinova"
      ],
      "year": "2018",
      "venue": "International Conference on Speech and Computer"
    },
    {
      "citation_id": "41",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "42",
      "title": "Yolov3: An incremental improvement",
      "authors": [
        "J Redmon",
        "A Farhadi"
      ],
      "year": "2018",
      "venue": "Yolov3: An incremental improvement",
      "arxiv": "arXiv:1804.02767"
    },
    {
      "citation_id": "43",
      "title": "Dex: Deep expectation of apparent age from a single image",
      "authors": [
        "R Rothe",
        "R Timofte",
        "L Van Gool"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "44",
      "title": "Multimodal database of emotional speech, video and gestures",
      "authors": [
        "T Sapiński",
        "D Kamińska",
        "A Pelikant",
        "C Ozcinar",
        "E Avots",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "46",
      "title": "Two-stream convolutional networks for action recognition in videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "48",
      "title": "Support vector machines: a nonlinear modelling and control perspective",
      "authors": [
        "J Suykens"
      ],
      "year": "2001",
      "venue": "European Journal of Control"
    },
    {
      "citation_id": "49",
      "title": "Fiction database for emotion detection in abnormal situations",
      "authors": [
        "I Vasilescu",
        "L Devillers",
        "C Clavel",
        "T Ehrette"
      ],
      "year": "2004",
      "venue": "Eighth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "50",
      "title": "Ryerson multimedia research laboratory (rml)",
      "authors": [
        "Z Xie"
      ],
      "year": "2010",
      "venue": "Ryerson multimedia research laboratory (rml)"
    },
    {
      "citation_id": "51",
      "title": "Multi-cue fusion for emotion recognition in the wild",
      "authors": [
        "J Yan",
        "W Zheng",
        "Z Cui",
        "C Tang",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "52",
      "title": "Emotion detection from speech to enrich multimedia content",
      "authors": [
        "F Yu",
        "E Chang",
        "Y Xu",
        "H Shum"
      ],
      "year": "2001",
      "venue": "Pacific-Rim Conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "54",
      "title": "Hidden two-stream convolutional networks for action recognition",
      "authors": [
        "Y Zhu",
        "Z Lan",
        "S Newsam",
        "A Hauptmann"
      ],
      "year": "2018",
      "venue": "Asian Conference on Computer Vision"
    }
  ]
}