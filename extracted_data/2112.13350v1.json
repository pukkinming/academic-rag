{
  "paper_id": "2112.13350v1",
  "title": "Novel Dual-Channel Long Short-Term Memory Compressed Capsule Networks For Emotion Recognition",
  "published": "2021-12-26T10:37:35Z",
  "authors": [
    "Ismail Shahin",
    "Noor Hindawi",
    "Ali Bou Nassif",
    "Adi Alhudhaif",
    "Kemal Polat"
  ],
  "keywords": [
    "Capsule networks",
    "convolutional neural network",
    "deep neural network",
    "dual-channel",
    "emotion recognition",
    "LSTM"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent analysis on speech emotion recognition (SER) has made considerable advances with the use of MFCC's spectrogram features and the implementation of neural network approaches such as convolutional neural networks (CNNs). The fundamental issue of CNNs is that the spatial information is not recorded in spectrograms. Capsule networks (CapsNet) have gained gratitude as alternatives to CNNs with their larger capacities for hierarchical representation. However, the concealed issue of CapsNet is the compression method that is employed in CNNs cannot be directly utilized in CapsNet. To address these issues, this research introduces a text-independent and speaker-independent SER novel architecture, where a dual-channel long short-term memory compressed-CapsNet (DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features of CapsNet. Our proposed novel classifier can ensure the energy efficiency of the model and adequate compression method in speech emotion recognition, which is not delivered through the original structure of a CapsNet. Moreover, the grid search (GS) approach is used to attain optimal solutions. Results witnessed an improved performance and reduction in the training and testing running time. The speech datasets used to evaluate our algorithm are: Arabic Emiratiaccented corpus, English \"speech under simulated and actual stress (SUSAS)\" corpus, English Ryerson audio-visual database of emotional speech and song (RAVDESS) corpus, and crowdsourced emotional multimodal actors dataset (CREMA-D). This work reveals that the optimum feature extraction method compared to other known methods is MFCCs delta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM COMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers, CNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our results demonstrate that the proposed work yields average emotion recognition accuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and 31.9% based on CapsNet, CNN, support vector machine (SVM), multi-layer perceptron (MLP), k-nearest neighbor (KNN), radial basis function (RBF), and naïve Bayes (NB), respectively.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) can be expressed as the extraction of the emotional talking condition of the speaker from his/her speech signal. Essential loudness, pitch, speech intensity, and glottal parameters, and frequency are the prosodic features utilized to model the various emotions  (Zhou et al., 2009) . Emotional robots and human-robot emotional communication have been extensively created and employed in multiple areas (Z.  Liu et al., 2016) . One of the crucial capabilities of the emotional robot is emotion recognition, which primarily involves body language emotion recognition  (Rattanyu & Mizukawa, 2011) , facial expression recognition  (Sun et al., 2017) , and speech emotion recognition  (Song et al., 2016) . For human communication with robots effortlessly and harmoniously, it is required to identify human emotion with significant accuracy for the robot. As speech signal is simple to understand, it is broadly utilized for emotion recognition in human-robot communication (Z.-T.  Liu et al., 2018) .\n\nWith recurrent neural network (RNN), hidden Markov model (HMM), Gaussian mixture model (GMM), and convolutional neural networks (CNNs), recent experiments in automatic speech emotion recognition (SER) have been integrated. Neural networks in SER systems accept features developed from the deep learning classifiers and anticipate targets at frame-level. The two challenging issues in SER are the extraction of high-level frame-based feature representations and the creation of utterance-level features. In small frames, the voice signals are essentially stationary  (Schuller et al., 2011) ,  (El Ayadi et al., 2011) . Several acoustic features derived from short frames (e.g., pitch), are thought to be affected by emotions and may give precise local information that is emotionally important  (Fernández-Diaz & Gallardo-Antolin, 2020; Uddin & Nilsson, 2020) .\n\nThese frame-based characteristics are often be described as low-level functions  (Mirsamadi et al., 2017) . Focused solely on the low-level frame-based features, neural networks are used to generate frame-by-frame neural representations, which are described as high-level frame-based representations of features  (J. & I.Tashev, 2015) . Nevertheless, identification of emotions at the utterance level involves a global representation of features, which includes both specific local knowledge and emotion-related global characteristics.\n\nAdditionally, in the field of deep learning, CNNs have been extremely efficient but have weaknesses in their basic design, making them perform less than expected for certain tasks  (Kwabena Patrick et al., 2019) . CNN is implemented to low-level features such as pitch and energy for learning high-level features, i.e., outputs of the neural network. Layers near the beginning detect simple features (low-level), and deeper layers will detect other complicated features. To make a final decision, CNN utilizes all these features that have been analyzed. This is where the system shortcomings lay -there is no spatial information utilized elsewhere in a CNN, and the pooling function utilized to link layers is ineffective (J.  Bae & Kim, 2018a) . It has lately been suggested by  Sabour et al. that capsule networks (CapsNet)  conquer the limitation of CNNs in collecting spatial information  (Sabour et al., 2017) .\n\nCapsNet is the outcome of almost ten years of research by  Sabour et al. (Sabour et al., 2017) .\n\nCapsNet, unlike CNNs, is fabricated of a network of neurons that inputs and outputs are vectors instead of two scalar values. A capsule contains a community of neurons that carry vectors for action. The vector length determines the probability of existence of the activity described by the capsule, and the orientation assembles the precise instantiation parameters of the activity, (e.g., translation, pitch, energy, rotation information, etc.). A routing algorithm is utilized to pair the related activity vectors to enable the respective capsules in the upper layer, depending on the activity vectors provided by the lower layer of the capsules.\n\nToo much essential information is lost in the pooling phase of CNNs since only the most sufficiently active neurons are selected for promotion to the next layer. This process is the reason why useful spatial information among the layers gets lost. Therefore, Sabour et al. presented a CapsNet whose fundamental architecture is displayed in Fig.  1    (Sabour et al., 2017) . In their algorithm, they assumed that the individual's brain could attain flip and translation invariance in a further sophisticated way than pooling. \"Capsule units\" are parts in the brain known as neurons, whereas the capsule layer is invented of several ''capsule'' units, which sequentially create a capsule network. Furthermore, Sabour and her colleagues suggested utilizing methods known as \"dynamic routing\" to resolve this problem. Dynamic routing technique is employed only between vectorized features to substitute pooling tasks. Geoffrey Hinton, one of the most influential people in the field of deep learning, proposed this solution, which encodes spatial details into features while still utilizing dynamic routing (by agreement)  (Sabour et al., 2017) , (G. E.  Hinton et al., 2011) .\n\nFig.  1  Basic architecture of CapsNet  (Sabour et al., 2017)  In the CapsNet architecture, six main phases are to be completed. In the first phase, the input of the spectrogram or MFCC's representation should be introduced after the preprocessing step. The input is to be inserted into the second phase, where the CNN takes place. Moving to the third phase in the primary capsules, the output of CNNs is the input to these capsules. After that, a dynamic routing mechanism is the fourth phase to allow each capsule at one level to attend to some active capsules at the level below and to ignore others. The fifth phase is called the detection capsule phase, where the main idea of this phase is to detect the correct emotion and organize them each in a row. The final phase is the decoder or Euclidean norm algorithm.\n\nThe detailed process of how CapsNet work can be explained as follows:\n\n A capsule, as mentioned before, is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity, e.g., an emotion.\n\n In a capsule network, after having the CNNs, the capsules ordered in the layer are called primary capsules, and each of these capsules in this layer corresponds to a particular entity in the input, e.g., a particular emotion. For example, if the input is a voice, each capsule corresponds to one feature of the voice such as pitch (high, low), frequency, energy, etc.\n\n Each of these capsules output is a vector, where the length of the vector represents the probability that this feature exists in the voice \"P (feature ∈ voice).\"\n\n The next layer is the digital/detection capsules layer, where each capsule in this layer takes information from each capsule in the lower layer and integrates all information.  In the last step, the correct emotion is recognized.\n\nThe main limitation, recently, in CapsNet is the compression method that is employed to CNN, which cannot be directly utilized in the CapsNet. Moreover, CapsNet is a slow algorithm which is due to the inner loop of a dynamic routing algorithm. As the size of the dataset increases, the number of iterations increases. Consequently, CapsNet has higher complexity in implementation compared to  CNNs (Jain et al., 2018) . Therefore, this study concentrates on utilizing four separate speech datasets to manipulate the capsule nets model for improving SER in emotional talking environments",
      "page_start": 2,
      "page_end": 5
    },
    {
      "section_name": "Prior Work",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Capsnet Systems In Speech Applications",
      "text": "The CapsNet has been applied to several tasks and demonstrated its effectiveness (J. Bae & Kim, 2018a),  (Zhao et al., 2018) ,  (Turan & Erzin, 2018) . Nevertheless, the CapsNet utilized in earlier work does not consider the model compression approach that is applied to CapsNet nor the implementation complexity. The used compression method in the conventional neural network cannot be directly utilized in the CapsNet. There have been also attempts to implement CapsNet to diverse areas beyond image classification, such as user intent detection  ((Xia et al., 2018 ), (C. Zhang et al., 2019) ), self-driving  (Kim & Suyoung, 2019) , sound event detection  (Iqbal et al., 2018) , speech command classification (J. Bae & Kim, 2018b), and speech emotion recognition  (Wu et al., 2019a) . Databases used in any neural network system should be huge and complex to arise with adequate system performance. Therefore, the work performed by  Xi et al. (Xi et al., 2017 ) evaluated the CapsNet performance on complex data. The best validation accuracy that was reported is 71.55% trained across 50 iterations.\n\nOne of the decent applications in speech command recognition has been studied by Bae and Kim (J.  Bae & Kim, 2018) . The study encountered CapsNet in their system to find the spatial relationship and pose information of speech spectrogram features. The dataset that was utilized is a global single-word English dataset  (Warden, 2017) . The results demonstrated that the presented end-to-end speech recognition system accomplished superior results on both clean and noise-added evaluation than baseline CNN frameworks. The error rate (ER) shown in the outcomes reached down to 10.5% in a clean environment, whereas ER reached 44.7% in noisy environments.\n\nAnother study by  Turin and Erzin (Turan & Erzin, 2018 ) revealed a remarkable application of CapsNet in observing child emotional cry in local environments that can help in remote infants monitoring system. The dataset utilized is the INTERSPEECH 2018 computational paralinguistic challenge (ComParE), crying sub-challenge, which is a three-class (neutral, fussing, and crying) classification mission by utilizing an interpreted database (CRIED).\n\nIn prior work, there are four known techniques of CapsNet applications: transforming autoencoders (G. E.  Hinton et al., 2011) , vector capsules based on dynamic routing  (Sabour et al., 2017) , matrix capsules based on expectation maximization (EM) routings  (G. Hinton et al., 2018) , and sequential routing framework  (Lee et al., 2020) . In transforming auto-encoders, any property of a picture demonstrated by a transforming auto-encoder can be manipulated. This demonstration occurs by forcing the outputs of the capsule found in a transforming auto-encoder. This idea is easily implemented, for instance, to enhance the intensity of all pixels. The vector capsules based on the dynamic routing process are explained in Section 4.1. The objective of the matrix capsules based on EM is to group several capsules to provide a part-whole relationship. The sequential routing framework is clarified in work achieved by  Lee et al. (Lee et al., 2020) .  Lee et al. (Lee et al., 2020)  introduced the sequential routing framework (SRF), which is the first technique to adjust a CapsNet-only formation to sequence-to-sequence recognition. By presenting speech sequence recognition tasks on the TIMIT dataset, results showed an 82.6% recognition rate. This outcome is 0.8% more precise than that of CNN-based using connectionist temporal classification (CTC) networks.  Wu et al. (Wu et al., 2019b)  studied speech emotion recognition based on CapsNet, where the system can take into consideration the spatial association of phoneme features in spectrograms and deliver an efficient pooling technique for achieving utterance-level features. They also presented a recurrent connection to CapsNet to enhance the system time sensitivity. The evaluation has been performed on the benchmark database IEMOCAP over four emotions, i.e., sad, happy, angry, and neutral. Comparing to previous results based on combined CNN-long short-term memory (CNN-LSTM) models, results showed that the model attained better outcomes than the baseline system, where the average accuracy reached 72.73%. In another study by  Tereikovska et al. (Tereikovska et al., 2019) , they recognized the basic emotions by the aid of face geometry using CapsNet with an average accuracy of 85.3%.  Liu et al. (Liu et al., 2020)   In the DEAP corpus, the average reported accuracies are 98.32%, 98.31%, and 97.97% under, respectively, dominance, arousal, and valence. In the DREAMER corpus, 95.26%, 95.13%, and 94.59% are the average accuracies under arousal, dominance, and valence, respectively.  GUO et al. (GUO et al., 2018)  proposed a classification model based on the CapsNet neural network by obtaining the granger connection feature of original EEG signals. EEG is a sufficient modality that aids in obtaining brain signals related to several states from the scalp surface area  (Kumar & Bhuvaneswari, 2012) . Therefore, this study uses EEG signals as a support system to help in recognizing the correct emotion. The experimental results obtained 87.37% and 88.09% average accuracy based on valence and arousal emotion dimensions, in contrast with CNN and SVM classifier.  Zhong et al. (Zhong et al., 2020)  proposed a novel approach to enhance emotion recognition performance, which is called the sentiment polarity algorithm (SPT-CapsNet). Using facial expression, the accuracy of this approach reached 93.6%. The results showed that the running speed was improved, and the balance between classification accuracy and computational efficiency was maintained.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Classical Classifiers Systems In Speech Applications",
      "text": "Currently, a growing interest has been targeted to the examination of the emotional content of speech signals. Thus, several systems have been presented to identify the emotional content of the uttered sentences. The paper proposed by El Ayadi  (El Ayadi et al., 2011)  is a survey of speech emotion recognition focusing on three crucial facets of a speech emotion recognition model design.\n\nThe first aspect is the selection of appropriate features for speech representation. The second concern is the design of a suitable classification system. The third problem is the adequate planning of an emotional speech corpus for assessing system performance. Shahin and Ba-Hutair  (Shahin & Ba-Hutair, 2015)  studied emotional and stressful talking condition recognition based on secondorder circular suprasegmental hidden Markov models (CSPHMM2s) as a classifier. The emotional talking environment database utilized is the \"emotional prosody speech and transcripts (EPST)\".\n\nResults demonstrated that CSPHMM2s surpass each of the hidden Markov models (HMMs), second-order circular hidden Markov models (CHMM2s), and suprasegmental hidden Markov models (SPHMMs) with an accuracy of 76.25%.\n\nAnother work by Shahin  (Shahin, 2019)  proposed the utilization of \"third-order circular suprasegmental hidden Markov models (CSPHMM3s)\" in emotion recognition. The reported results yielded average emotion recognition accuracy of 77.8%, which was tested on the EPST dataset. Shahin  (Shahin, 2016)  studied the cascaded phases approach that used the speaker emotion cues endorsed by SPHMMs and HMMs as classifiers. The proposed framework contains a twostage approach that integrates and combines the emotion recognizer complemented by a speaker recognizer into a single recognizer. The study showed that his approach gave better results with a remarkable alteration over previous studies along with other approaches such as \"emotionindependent speaker verification method\" and \"emotion-dependent speaker verification method based on HMMs\". Shahin also studied the improvement of talking condition recognition in both emotional and stressful environments based on HMMs, CHMM2s, and SPHMMs  (Shahin, 2012) .\n\nThe attained outcomes proved that SPHMMs surpassed each of CHMM2s and HMMs using an English dataset. Besides that, outcomes also revealed that emotional talking condition recognition is much less than that in stressful talking environments by 3.3%, 2.7%, and 1.8% based on SPHMMs, HMMs, and CHMM2s, respectively.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Neural Network Systems In Speech Applications",
      "text": "Nassif et al. concluded in their study that prior work implemented 75% of standalone deep neural networks (DNNs) models, where only 25% of the models used hybrid models  (Nassif et al., 2019) .\n\nThe work accomplished by  Shahin et al. (Shahin et al., 2019 ) used a hybrid model, where they focused on recognizing emotion by utilizing \"hybrid Gaussian mixture model and deep neural network (GMM-DNN)\". GMM-DNN resulted in an accuracy of 83.97% using six different emotions, which are taken from the Arabic Emirati speech dataset. Nassif et al.  (Nassif et al., 2021)  aimed in their research on enhancing \"text-independent speaker identification performance under both emotional and noisy talking environments\". Computational Auditory Scene Analysis (CASA) and cascaded Gaussian Mixture Model -Convolutional Neural Network (GMMCNN) classifier have been employed in their study. The CASA-based module is used for the pre-processing of noise reduction, while the GMM-CNN classifier is utilized for speaker identification followed by emotion recognition. This work has achieved 83.7% accuracy using an \"Arabic Emirati accented database\".  Mirsamadi et al. (Mirsamadi et al., 2017)  studied automatic speech emotion recognition using recurrent neural networks (RNNs) with local attention. The intended solution assessed on the interactive emotional dyadic motion capture (IEMOCAP) corpus and proved to deliver more precise predictions compared to most of the emotion recognition systems.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations Of Prior Work",
      "text": "The limitations of similar work addressed previously were derived from a limited number of studies that focused on the application of an emotion recognition model with the employment of dual-channel LSTM compressed-CapsNet (DC-LSTM COMP-CapsNet) along with MFCCs deltadelta in harsh talking environments and the complex implementation of the model. Prior work did not solve the issues of the compression method nor the energy deficiency in emotion recognition because CapsNet structure is inefficient energy. To define the unknown emotion, an appropriate model must be constructed and discussed. Therefore, the method that is being suggested in this research to solve the previously mentioned issues poses a speech emotion recognition model based on the DC-LSTM COMP-CapsNet framework.\n\nMost of the research works did not experience multiple real emotion states in their systems nor multiple databases for a sufficient evaluation. Moreover, the CapsNet model used in most of the prior work was not modified. There are some studies that utilized the modified CapsNet, such as the study by  Zhong et al. (Zhong et al., 2020)  that proposed SPT-CapsNet and the study by  Liu et al. (Liu et al., 2020)  that utilized MLF-CapsNet. However, SPT-CapsNet system relies on facial expression, and the MLF-CapsNet system depends on the EEG and ECG signals that are extracted from a wireless device. Our proposed system does not depend on any extra device nor on the aid of any facial expression systems.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Main Contributions Of This Work",
      "text": " Developing a novel DC-LSTM COMP-CapsNet classifier to ensure the model energy efficiency and adequate compression method in speech emotion recognition, which is not delivered through the original structure of a CapsNet.\n\n Using hyperparameter optimization based on grid search (GS) to attain optimal solutions.\n\nResults witnessed an improved performance and reduction in the training and testing running time over the trial-and-error approach.\n\n Creating supervised text-independent and speaker-independent DC-LSTM COMP-CapsNet model to improve the performance of emotion recognition utilizing MFCC delta-delta as feature extraction technique in abnormal talking environments.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Proposed Dc-Lstm Comp-Capsnet Framework And Emotion Recognition",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Algorithm",
      "text": "Although CNNs have a significant breakthrough in the emotion recognition field; however, CNNs need a huge training data size and are incompetent in recognizing the deformation and pose of objects guiding the establishment of capsule networks. Capsule networks are the latest phenomenon in deep learning  (Punjabi et al., 2020) . CapsNet has resided to this belief as its performance corresponding to the mentioned issues has surpassed CNNs (Kwabena  Patrick et al., 2019) .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "The Principle Of Capsule Networks Model",
      "text": "The concept is to incorporate \"capsules\" structures to CNN then re-utilize output from some of those capsules to make representations for higher capsules relatively robust (concerning different perturbations) (G. E.  Hinton et al., 2011) . The input to a capsule is the output features from the last CNN layer. These features are handled depending on the type of the used capsule. The output of a capsule is comprised of the probability that the feature encoded by the capsule is present and a set of vector values are commonly called \"instantiation parameters\". The output vector involves an observation likelihood and a position for that observation (G. E.  Hinton et al., 2011) . A capsule is a group of neurons that trigger independently for different forms of voice properties such as energy, pitch, etc. Technically, a group of neurons generates a capsule that is an action vector containing one part for each neuron to carry the instantiation property of that neuron (e.g., pitch)  (Sabour et al., 2017) . The probability of the existence of the object in a given input is the length of the vector, whereas the direction of the vector quantifies the properties of the capsule  (Sabour et al., 2017 ), (Srihari, 2017) .\n\nSince capsules are self-sufficient, the possibility of successful identification is even greater when several capsules consent. Just once in a million trials will two capsules (a small cluster) by chance considered a six-dimensional object accepts around 10%. If the number of dimensions increases, the probability of a chance agreement exponentially decreases over a bigger cluster of higher dimensions. The outputs of capsules at lower layers are collected by higher-layer capsules and embrace those with output scalar. A cluster allows the higher capsule to create a high-level detection likelihood of an object's existence and a three-dimensional pose as well  (Sabour et al., 2017) . The overall block diagram is shown in Fig.  2    (Vesperini et al., 2019) , where the input features activity vector is represented by a voice spectrogram. The next step remains in inserting the input vector to the convolutional layers to be inserted after that to the primary capsules. Each capsule contains an independent feature vector that has multiple neurons-one part for each neuron to carry the instantiation property of that neuron. Dynamic routing takes place next to replace the pooling step, and the last step is handled by the detection capsules.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Routing By Agreement",
      "text": "The one capsule (child) outputs are redirected in the parent's next layer (next layer) to capsules as per the child's capability to anticipate the parent's outputs. Throughout a few simulations, the outcomes of each parent might overlap with certain children's anticipations and vary from those of others, indicating that the parent is attended or missing from the event  (Sabour et al., 2017) . A child evaluates a predictive vector for each potential parent by multiplying a matrix of weight (trained by backpropagation) by the parent's output  (Srihari, 2017) . Then, the parent's output is measured as a prediction scalar product with a coefficient reflecting the probability that this child goes to that parent.\n\nA child whose expectations are comparatively similar to the subsequent performance raises the differential between the parent and child progressively and lowers it to balance it less well for parents. This improves the contribution the child makes to the parent. Thus, the correlation of the capsule's scalar value increases with the performance of the parent. After a small number of epochs, the coefficients firmly connect a parent to his highly probable children, suggesting that the existence of children indicates the parent's participation in the scene  (Sabour et al., 2017) . The more children having expectations that are similar to the performance of a parent, the more the coefficients will rise and accelerate convergence. The parent's pose (represented in its output) increasingly becomes consistent with that of its children  (Srihari, 2017) .\n\nThe priors, together with the weights, can be educated discriminatively. The priors rely on the parent and child capsules' position and type but not on the present data. The coefficients are modified by a \"routing\" Softmax for each iteration such that they sum to 1 (to represent the chance that a provided capsule is the parent of a provided child). The Softmax amplifies the bigger value and decreases lower values further than their proportion of the sum. Likewise, the likelihood of a feature is present in the input is exaggerated by a nonlinear \"squash\" function, which decreases values and normalizes them (dramatically smaller ones and bigger ones, but they are less than 1)  (Srihari, 2017) . The squash equation can be described as follows  (Lee et al., 2020) : Fig.  2  The overall architecture of CapsNet  (Vesperini et al., 2019)\n\nwhere s j is the un-normalized instantiation parameter vector for the j-th capsule of the output capsule group. The sum of the predictions from all capsules in the lower layer is fed to capsules s j in the subsequent higher level, each with a coupling coefficient c ij  (Sabour et al., 2017) :\n\nThe pose vector u i is translated and rotated by a matrix W i,j into a vector u ̂j|i that anticipates the parent capsule output as provided in the given equation  (Sabour et al., 2017) :",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Eliminating Irrelevant Instantiation Parameters By Compression Of The Model",
      "text": "In contrast to the traditional CNNs, the number of nodes that take part in the computation of the CapsNet is precipitously rising. The issue of expanding will turn out to be a challenging task to be ignored as the number of neural network layers is expanded. The \"high-dimensional stereo\" composition of the CapsNet obstructs the lower network structure of the capsule network. The capsule units of CapsNet are vector carriers, including a variety of characteristic neurons, each of which is utilized to identify a particular attribute (instantiation parameters). In addition to the strength of the target object, instantiation parameters can also signify the probability of the target object in the detection zone and the spatial position information. Scheming as various parameters as possible with hyper-parameter selection in the CapsNet leads to an incredibly conventional system; however, this design may generate excessive or irrelevant instantiation parameters.\n\nThe major concept of model compression for CapsNet is acquiring these excessive or irrelevant instantiation parameters and later \"branching\" these instantiation parameters. Nevertheless, by applying the CapsNet as a classifier, the number of instantiation parameters that must be in a capsule is defined by the developers that are based on application scenarios and experience. There are situations of over-design or inadequate design that can lead to insufficient results. Frequently, the insufficient design will cause an inadequate performance of the CapsNet, which is regularly altered throughout the growth phase of the network. By summarizing as various entity features as possible and design numerous instantiation parameters, the greatest potential accuracy can be obtained. Screening and compressing these additional instantiation parameters are ideas of CapsNet model compression  (Zhong et al., 2020) . Regarding this \"overdesign\" issue, this work introduces an approach like the dropout mechanism for each instantiated parameter. In the dropout mechanism, zeroing the weights of instantiation parameters is the main procedure. On the other hand, the weight will also take part in the feedback method, fine-tuning, and weight update.\n\nTherefore, throughout the network adjustment, the process is still not completed.\n\nAs an alternative to zeroing the weight of the instantiated parameter, the proposed algorithm closes the instantiation parameter itself. CNN utilizes the layer output loss function before and after modifying the weight to define if the modification has a value or the modification has to be withdrawn in order to prevent the accuracy loss, which is not appropriate. Therefore, we introduce a CapsNet framework based on the compression of instantiation parameters and energy efficiency with the optimization of the model complexity \"DC-LSTM COMP-CapsNet\" as described in Section 3.1.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "The Proposed Dc-Lstm Comp-Capsnet Algorithm",
      "text": "A dual-channel LSTM compressed-CapsNet (DC-LSTM COMP-CapsNet) algorithm has been proposed and implemented in our current work based on the structural features of the CapsNet to solve the previously mentioned issues. The model can compress the scale of system computation based on preserving the accuracy of system recognition, consequently, decreases the computational complexity. Dual-Channel LSTM layers intend to extract and compress sequencecorrelated characteristics of Amplitude/Phase signal components and In-phase/Quadrature signal components. The distinction between our proposed novel technique and the work in  (Zhong et al., 2020) , is that we utilize a dual-channel LSTM approach, as well as MFCCs feature extraction is applied to compare the feature efficiency and then concatenate DC-LSTM with MFCCs output.\n\nThis approach is followed by removing the outliers before inserting the feature vector into the COMP-CapsNet (compressed CapsNet). By doing so, CapsNet would result in a sufficient performance with ideal compressed input features to CapsNet.\n\nAlthough nowadays the transformer techniques achieve state-of-the-art in several applications, the transformers are more effective in Natural Language Processing (NLP) problems. On the other hand, Capsule Nets that are based on dynamic routing, are more powerful in Speech Recognition (SR). The transformer techniques are the first sequence transduction models based fully on attention, substituting the recurrent layers commonly utilized in encoder-decoder architectures with multi-headed self-attention  (Vaswani et al., 2017) . In contrast, the idea of CapsNets is to add structures known as \"capsules\" to CNN, then to reuse output from various of those capsules to produce more stable (with respect to various perturbations) representations for higher capsules  (Sabour et al., 2017) . Our method is considered better than state-of-the-art techniques in improving the original CapsNet, where the proposed model ensures the model energy efficiency and adequate compression method in SER.\n\nOur   ----------------------------------------------------------------  for each capsule layer in the solid capsule layer: for variance i in an instantiation parameter vector for a capsule with a vector length of n:\n\nfor the dropout matrix is set to 1 except for the value of i:",
      "page_start": 15,
      "page_end": 18
    },
    {
      "section_name": "D Is Multiplied With Each Instantiated Parameter",
      "text": "If an instantiation parameter is trimmed out, the value of dropout is small, thus the instantiation parameter becomes meaningless (unimportant) due to its small impact.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Return All The Compressed Instantiated Parameters Based On The Compression Rate (Output)",
      "text": "Two consecutive convolutional layers following by primary Caps, routing Caps, and decoder To illustrate more, first, the spoofed voices and their corresponding speaker identity are inserted into the system. After that, a pre-processing step is applied, where the dataset is divided into the training phase and testing phase, as well as organizing the voice with a label that corresponds to the speaker identity. In order to extract the important features and to reduce the complexity of the system, a feature extraction method is applied, which is a concatenation of the \"MFCCs, MFCCs delta, and MFCCs delta-delta\". At this point, the data is ready to be inserted into the system explained above. There is an important step between DC-LSTM and the COMP-CapsNet to remove the outliers. DC-LSTM aims to extract and compress sequence-correlated characteristics of Amplitude/Phase signal components and In-phase/Quadrature signal components. Each channel in the dual channel LSTM is comprised of two layers of LSTM each of which is made up of the same number of expanded nodes (pitch, energy) according to the input data. Corresponding to each training accuracy, the dual channel LSTM reduces the loss to attain the best biases and weights.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "End",
      "text": "The DC-LSTM model is divided into two channels where one channel takes energy data and the other one takes pitch data as input in batches. Each channel determines different number of LSTM cells in terms of the energy and pitch variation of the input (M and N). By conducting synchronous training, the output feature of the final cell from the second layer of LSTM (OM, ON) is obtained to connect into a feature vector that can foresee each class's probability. The proposed DC-LSTM is exemplified in Fig.  4 .",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Fig. 4 Dual Channel Lstm Model",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Model Parameters",
      "text": "Our work utilizes the powerful framework named \"Keras\" that is based on \"Tensorflow\". Based on the grid search (GS) approach that optimizes hyperparameters, we reached optimal solutions, where the number of hidden layers, input neurons, and output neurons were accurately selected.\n\nGrid search is a tuning methodology that attempts to compute the optimal values of hyperparameters in a pre-defined search grid space. It is a massive search that is functioned on all parameter values in the grid of a specific model. In GS, the domain of the hyperparameters is divided into a discrete grid. After that, by computing some performance metrics using crossvalidation, every combination of values of this grid has been tried. The optimal combination of values for the hyperparameters is the goal of the grid that maximizes the average value in crossvalidation. Grid search is a thorough method that traverses all the combinations to find the best point in the domain  (Bergstra & Bengio, 2012 ). An incorrect selection of the hyperparameter values may yield incorrect results and a model with inferior performance. Therefore, by using GS algorithm in this work, our model performance has been enhanced.\n\nThe CNN model utilizes two layers. After that primary capsule with 'squash' activation also has one layer. In the third layer, the rotation algorithm capsule is occupied. In the end, a decoder network is built to achieve the final system predictions. The number of input neurons is 1,024 in the first stage and eight-dimensions capsule length in the second stage. The number of output neurons is 512 in the first output layer and 1,024 in the second output layer. After categorizing the testing labels, the output neurons represent the final classes. A normalized exponential function, \"Softmax\" also known as \"softargmax\" is an activation function that facilitates the anticipation of probability scores throughout classification tasks  (Goodfellow et al., 2016) . In our represented system, \"Softargmax\" is used and employed in the output layer. Firstly, \"Softargmax\" takes K real numbers (vector input), then normalizes it into a probability distribution encircling of K probabilities linked to the exponentials of the input numbers. The standard unit \"Softmax\" function σ∶ R^K→ R^K is given as the following (Wikipedia, 2014):\n\nThe addition of every input vector value applied by the exponential function is then divided and normalized.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "The Unique Parameters Used In The Classical Classifiers",
      "text": "Designing the classical classifiers depends on the application that is being used. The classical classifiers contain parameters that can be altered based on the system's requirements. This section explicitly explains the parameters that have been utilized. The following are the parameters used in each classical classifier:\n\n SVM: C=1, kernel='rbf', gamma=0.001, cache_size=200, \"penalty='l2', loss='squared_hinge', max_iter=1000\", decision_function_shape='ovr'.\n\nwhere 'C=1.0' is the regularization parameter, where the strength of the regularization is inversely proportional to C and must be strictly positive. 'Kernel' parameter specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or 'callable'. Moreover, 'gamma' is the kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Whereas 'cache_size' identifies the size of the kernel cache. 'Penalty' parameter defines the tradе-off between maximizing thе classification margin and minimizing the training error rate. Furthermore, the 'loss' parameter can be indicated by the loss formula as follows:\n\nThe better classifier model will have a lower loss. The 'max_iteration' exemplifies the hard limit on iterations within the solver. Additionally, 'decision_function_shape' specifies whether to return a one-vs-rest ('ovr') decision function as all other classifiers or the original one-vs-one ('ovo') decision function 1    (Lin et al., 2008) .\n\n MLP: \"hiddеn_layеr_sizеs=(100,), activation='rеlu', solvеr='adam', alpha=1, max_iter=200\", batch_sizе='auto', learning_ratе='constant', learning_rate_init=0.001, tol=0.001, early_stopping=True, validation_fraction=0.1, epsilon=1e-08.\n\nwhere, 'hidden_layer_sizes' is a tuple parameter, where the i th element signifies the number of neurons in the i th hidden layer. The activation function for the hidden layer is defined as 'activation='relu''. The 'relu' is the rеctified linеar unit function, and it rеturns f(x) = max(0, x).\n\nThe 'solvеr' is used for weight optimization. In our work, the 'adam' solver was utilized, which refers to the stochastic gradient-based optimizer. where it represents the proportion of training data to sеt aside as validation sеt for еarly stopping.\n\nThe 'epsilon' is the valuе for numеrical stability in the 'adam' solver 2  .\n\n KNN: n_neighbors=10, weights='uniform', algorithm='auto', metric='euclidean', n_jobs=-1.\n\nwhere 'n_neighbors' is the number of neighbors to be used. A uniform weight is applied, where all points in each neighborhood are equally weighted. In order to compute the nearest neighbors, 'algorithm' parameter is set to auto to dеcidе the most appropriatе algorithm basеd on the valuеs passеd to fit the model. The distance 'mеtric' usеd for thе trеe is chosеn to bе the 'Euclidеan' distance. Using 'n_jobs' parametеr definеs the number of parallеl jobs to run for nеighbors sеarch,\n\nwhere -1 mеans to use all processors 3  . The main advantage of the KNN method is that it is easy to interpret and does not consume time in terms of calculations  (Zhai et al., 2016) . The distance between all the training samples and the test sample is the Euclidean distance that can be expressed as (X.  Wang et al., 2016) ,\n\nwhere Y is the training sample and X is the testing sample.\n\n RBF: C=1000.0, cache_size=200, gamma='auto', kernel='rbf', shrinking=True, tol=0.001.\n\nwhere 'C=1.0' is the regularization parameter and 'cache_size' is the parameter that identifies the size of the kernel cache. The 'gamma' is the kernel coefficient that is specified as 'auto', where 'auto' uses 1/number of features 4  .\n\n NB: \"GaussianNB, priors=Nonе, var_smoothing=1е-09\".\n\nwhere NB implements the 'GaussianNB' algorithm for classification. A framework for integrating prior distribution with observational data is delivered by the NB classifier. Assuming the prior distribution about an indeterminate event (A), which is P(A), the likelihood of gaining an experimental outcome (B), assuming that event A happened, is P(B│A), and the probability of detecting experimental outcome B, without knowing A has happened, is P(B). NB classifier is utilized to define the following belief about event A after detecting the experiment results, P(A│B) is expressed as  (Karandikar et al., 2015) :\n\n'priors' are the prior probabilities of the classes. If priors are determined, the priors are not modified according to the data. Additionally, the 'var_smoothing' parameter is the portion of the largest variance of all features that are added to variances for stability calculation 5  .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "The Common Parameters Used In The Classical Classifiers",
      "text": " (fit parameter): 'fit (X,y)' fits the modеl to data X and labеls y, whеrе X is an array for the training vectors of shape (n_samples, n_features). The number of samples is signified by 'n_samples' and the number of features is represented by 'n_features'. The target values are labeled in the y array.\n\n (score parameter): 'score (X,y)' gives back the mеan accuracy on the providеd tеst data and labеls for data X and labels y.\n\n (predict parameter): 'Prеdict (X)' pеrform classification on samplеs in X to prеdict using the targeted classifier the labels y.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Speech Databases And Feature Extraction",
      "text": "",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Arabic Emirati-Accented Speech Corpus",
      "text": "The Arabic corpus utilized in this work is an Emirati-accented dialect consisting of 50 Emirati speakers (25 male and 25 female) between the age range of 20 to 55 years. Moreover, besides the neutral state, the corpus retains 5 different emotions, which are fear, happiness, sadness, disgust, and anger. Each emotion is pronounced in eight distinct utterances. Each utterance is repeated 9\n\ntimes. These utterances are frequently used in the \"United Arab Emirates\". Skilled engineers recorded the corpus with the co-operation of \"College of Communication at the University of Sharjah in the UAE\". Table  1  displays the corpus utilized in this analysis, where the left column displays the Emirati accent utterances, while the right column demonstrates the corresponding English translation  (Shahin, 2018b) .\n\nTable  1  \"Arabic Emirati database and it's English translation\"",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Susas Corpus",
      "text": "Hansen captured speech under simulated and actual stress (SUSAS) database, funded by the air force research laboratory (AFRL), at the University of Colorado-Boulder. The primary purpose of the SUSAS was initially to investigate the comprehension of speech in difficult contexts (J. H.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Crema-D Corpus",
      "text": "CREMA-D stands for crowd-sourced emotional multimodal actors dataset. This database is global English that consists of 7,442 audios from 91 speakers. These audios were collected from 48 male and 43 female speakers between the ages of 20 and 74 coming from different races (African American, Asian, Caucasian, Hispanic, and Unspecified). Speakers uttered a collection of 12 utterances. The utterances were spoken in six distinct emotions (disgust, anger, happy, sad, fear, and neutral)  (Cao et al., 2014) .",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Trained/Tested The Model",
      "text": "Databases are divided into training and testing files. Therefore, there are two main phases: the training phase and the testing phase since the proposed model is text-independent and speakerindependent (the speakers used in the testing phase are different from those utilized in the training stage).\n\nUsing the Arabic Emirati-accented corpus, the training phase includes one reference model per emotion that was created using only 37 speakers out of 50 speakers producing the first 4 sentences, with nine times repetition per sentence. The overall number of utterances that have been employed in this phase is 7,992 (37 speakers × 4 utterances × 9 times/utterance × 6 emotion states). When the model was trained, the system was tested on a different number of speakers  (35, 36, 37, 38, and 39) , where 37 speakers were the best number that results in higher accuracy. In the test stage, each one of the 13 remaining speakers uses the second 4 utterances with a repetition of 9 times per utterance under each of the six emotions. The total number of utterances that have been utilized in this phase is 2,808 (13 speakers × 4 utterances × 9 times/utterance × 6 emotion states).\n\nThe same procedure is applied to the SUSAS database, where in the training phase, the size of the dataset is 6,600 (22 speakers × 5 utterances × 10 times/utterance × 6 emotion states). In the testing phase, the total number of utterances is 3,000 (10 speakers × 5 utterances × 10 times/utterance × 6 emotion states).\n\nThe training and testing phases in RAVDESS also follow the same procedure. \"The total number of utterances in the training phase is 1,176 (14 speakers × 2 utterances × 7 times/utterance × 6 emotion states), while the total number of utterances in the testing phase is 840 (10 speakers × 2 utterances × 7 times/utterance × 6 emotion states)\".\n\nCREMA-D corpus is divided into training and testing files. \"Total number of utterances used in the training phase is 2,160 (60 speakers × 6 utterances × 6 emotions), whereas the total number of utterances used in the testing phase is 1,116 (31 speakers × 6 utterances × 6 emotions)\".\n\nThe training and testing phases of shallow classifiers and the proposed classifier were achieved.\n\n\"In the training phase, the proposed classifier and the shallow classifiers employ the features of training files to fit them with their targets. Whereas in the testing phase, the features of testing files are utilized. The features of testing files are used to fit them with their targets\".",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Feature Extraction",
      "text": "Linear predictive coding (LPC)  (O'Shaughnessy, 1988) ,  (Dave, 2013) , Mel-Frequency Cepstral\n\nCoefficients delta-delta (MFCCs delta-delta)  (Davis & Mermelstein, 1980) , Hybrid Algorithm DWPD  (Sunny et al., 2013) , discrete cosine transforms (DCT)  (Sahidullah & Saha, 2009) , and probabilistic linear discriminate analysis (PLDA)  (Ioffe, 2006) ,  (Narang & Gupta, 2015)  are the five techniques of feature extraction that are used to determine the best output of dimensionality in our work. Neither approach surpasses the other; however, the best approach should be chosen based on the result of the model performance. MFCC acknowledges the best output feature in the proposed work, where various enhancements to the MFCC method were introduced to make them less sensitive to noise, more stable, and quicker. Further details are provided in Experiment 9 of Section 5.\n\nThe extraction of features in this study utilizes a concatenation of \"delta-delta, MFCCs-delta, and\n\nMFCCs\". The MFCC is a representation of an audio signal short-term power spectrogram function (with other tune-able parameters defined for nested core functions), all parameters related to wave signal segmentation (such as the overlapped values and frames) are mainly used.\n\nTo choose functions, the command is: \"compute-mfcc-feats. Therefore, the MFCC system was utilized for selecting functions. The output is, by default, 20-dimensional selected features. The 20-dimensional is inadequate to represent a very broad set of data, the dimensionality to 40dimensional chosen features has been expanded. MFCCs extracted feature are fine, but other MFCCs can work more skillfully, such as adding coefficients of dynamic features such as deltadelta and delta (first order and second order frame-to-frame difference). Typically, an improvement in efficiency or output performance levels of 20.0% arises when MFCC delta-delta features are utilized relative to MFCC features as they have a wealthier frame context  (Ahmad et al., 2015) ,  (Godino-llorente et al., 2006) . A concatenation of MFCC, MFCC delta, and MFCC delta-delta were utilized in the proposed work\".\n\nThe rate of sampling is Sr, \"the log-power Mel spectrogram is S, n_mfcc is the number of MFCC in return, and y denotes the wave file. There are n_fft=512 samples (physical duration: 23 ms at sr= 22,050 Hz sample rate), and hop length= 256 samples to get 50% overlap for each frame. By default, the Dct_type is the discrete cosine transform (DCT), and the DCT type-2 is usually used.\n\nIf dct_type is 2 or 3, making norm='ortho' uses an ortho-normal DCT basis. **kwargs is utilized for further keyword arguments. MFCCs-delta-delta holds richer background information that contributes to better precision than each of MFCC and MFCC-delta  (Hanson & Applebaum, 1990) ,  (Furui, 1986) . MFCC-delta-delta has a lower number of coefficients than each of MFCC and MFCC-delta\".",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Results And Discussions",
      "text": "Our novel work proposes a DC-LSTM COMP-CapsNet framework that enhances text-independent and speaker-independent emotion recognition performance under six distinct emotions using four diverse databases. The emotional talking conditions are: happy, sad, disgust, angry, fearful, and",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "1) Experiment 1: Emotion Recognition Model Assessment Based On Various Classifiers A) Model Accuracy",
      "text": "Table  3  validates average emotion recognition performance based on various classifiers using the Arabic Emirati-accented corpus. The highest accuracy amongst the classical classifiers is reported for SVM, where the accuracy reaches 69.8%, followed by MLP that achieves 69.2%. On the other hand, RBF and NB remark the lowest accuracy. Nevertheless, the proposed framework attains an accuracy of 89.3%, which shows that the proposed work is superior to each of the classical classifiers, CNN and CapsNet.\n\nIt is well known that classifiers work best in neutral talking conditions than in any harsh talking environment such as emotional talking environments. Fig.  5  shows that the greatest performance occurs in a neutral state and oppositely in the rest of the talking states using the Emirati-accented corpus as listed in Table  2 . The average of each classifier has been computed, and results indicate that the proposed model proves its superiority based on each of CNN, CapsNet, MLP, KNN, NB, RBF, and SVM in all talking conditions. Consequently, our proposed architecture reports the optimum performance, to date, due to its capability to adequately accomplish the compression step and simultaneously maintain sufficient energy with low complexity. Furthermore, the results of emotion recognition performance using the Emirati-accented database demonstrate that the proposed framework has homogenous (close performances) outcomes across different emotional talking conditions, whereas non-homogenous results appear for the other classifiers.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "B) Roc Curve",
      "text": "A substantial comparison among shallow classifiers, CNN, CapsNet, and DC-LSTM COMP-CapsNet model using the ROC (receiver operating characteristic) curve of the traditional classifiers is demonstrated in Fig.  6 . ROC curve signifies the graphical structure that illustrates the analytical ability of a classifier composition as its discernment threshold is altered. As shown in Fig.  6 , SVM has the maximum area under the curve, followed by the MLP, whereas RBF has the least accuracy as it has the minimum area under the curve.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "C) Precision, Recall, And Confusion Matrix",
      "text": "Precision (correctness) and recall (completeness) are two essential parameters in defining the performance of the proposed system. The recall is the number of related records recovered by an examination divided by the total number of current related records, whereas precision is the number of related records recovered by an examination divided by the total number of records recovered by that examination  (Buckland & Gey, 1994) . Precision is the portion given by correctly predicted positive clarifications to the predicted positive interpretations, meanwhile, recall is the portion of truly predicted positive clarifications to the overall negative interpretations as demonstrated in Fig.  7 . Precision and recall functions are, respectively, demonstrated as the following  (Goutte & Gaussier, 2005) : when the real class is incorrect, but the predicted class is correct. On the contrary, FN (false negative) where the real class is incorrect, but the predicted class is incorrect  (Goutte & Gaussier, 2005) .\n\nThe outcomes of recall for the proposed framework, the original CapsNet, CNN, and the shallow classifiers using the Emirati-accented database, are given in Table  3 . Furthermore, the outcomes of precision for the proposed system, CapsNet, CNN, and the shallow classifiers are demonstrated in Table  4  utilizing the same dataset. The findings in Table  3     4 . SVM has the highest precision of the shallow classifiers, while NB has the lowest.\n\nAll classifiers reach the highest precision in the neutral state competed with the other states.\n\nA confusion matrix that capitulates a confusion percentage of a test emotion with the other emotions using the Arabic Emirati-accented dataset based on the proposed model is given in Table  5 . This table determines the following:\n\n(a) Neutral emotion is the easiest recognizable emotion among other emotions (97.8%). This is in agreement with the prior work  (Shahin & Ba-Hutair, 2015) ,  (Shahin et al., 2018) ,  (Shahin, 2012) .\n\nTherefore, the highest performance of emotional talking condition recognition is neutral.\n\n(b) Angry emotion is the least easily recognizable emotional talking condition (86.2%).\n\nAccordingly, the least talking state recognition performance in such a talking environment is angry. This result agrees with that in previous studies  (Shahin & Ba-Hutair, 2015) ,  (Shahin et al., 2018) ,  (Shahin, 2012) .\n\n(c) In the fear emotion column, e.g., 87.9% (in bold) of the sentences that were uttered in a fear state were recognized correctly. 0% represents that the neutral state is the least state identified as fear. Therefore, fear emotion is not confusable at all with neutral emotion. This column displays that the fear state has the highest confusion percentage with the angry state (5%). Therefore, fear emotion is highly confusable with angry emotion. This column also exemplifies that 2% of the sentences that were generated in a fear emotion were recognized as disgust emotion. The inferential statistic is used to identify whether there is a significant variance between the means of two groups signified by the \"t-statistical distribution test\" or not. Testing of the hypothesis is considered as the major utilization of the t-statistical test  (Investopedia, n.d.) .\n\nConsequently, the t-test is conducted to verify if performance variances of emotion recognition are true or only due to statistical variabilities. The t-test is carried utilizing the Emirati-accented database to evaluate our proposed model, CapsNet, CNN, and the classical classifiers. The equation below yields the t-test  (Hogg et al., 2005) ,\n\nGiven 1\n\nx , 2 x , as the mean of the 1 st and 2 nd sample, respectively, where both have the same size 'n'. The pooled standard deviation (SDpooled) of the two samples is given as  (Hogg et al., 2005) ,\n\nGiven the standard deviation of the first sample (SD1) of size 'n', and the standard deviation of the second sample (SD2) of equal size 'n'  (Hogg et al., 2005) . the \"tabulated critical value t0.01=3.17 at 0.01 significant level\". Each measured t-value is larger than t0.01=3.17, so the assessment confirms that the proposed framework generates a substantial performance enhancement for emotion recognition.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "A) Comparison Between The Proposed Work And Prior Work",
      "text": "Emotion recognition results based on the proposed framework are shown in Table  7  along with those based on distinct classifiers employed in preceding studies using the Arabic Emirati-accented database. As demonstrated in Table  8 , our results are significantly better than those reported in prior work.\n\nThe study by  Wu et al. (Wu et al., 2019)  used CapsNets, which showed that the average improvement rate between our proposed system and their work is 22.78% which proves the superiority of our model over their model.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "B) Comparison Between The Proposed Work And Each Of Cnn And The Original Capsnet",
      "text": "In this experiment, the comparison between the proposed model and each of CNN and the original CapsNet will be discussed based on their performance, average running time, and code complexity.\n\nIt is shown in Tables  9, 10 , and 11 that the average running time of CNN and original CapsNet is approximately similar to the proposed work; however, the code complexity of the proposed model is higher than each of CNN and the original CapsNet. On the other hand, the proposed classifier reports a superior performance to each of those based on CNN and the original CapsNet. The comparison has been done on the same PC with the following specifications: \"Intel® Core™ i7-9750H with a CPU @2.60GHZ (12 CPUs) ~2.6GHZ\".",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "4) Experiment 4: Training And Testing Running Time Comparison",
      "text": "This work also accounts for an essential aspect called running time to test the proposed model against each of the CapsNet model, CNN classifier, and the classical classifiers. The running time engaged in testing/train the mentioned models using the Emirati-accented database is shown in Table  13 . As mentioned earlier, the same PC has been used with the following specifications for all the experiments: \"Intel® Core™ i7-9750H with a CPU @2.60GHZ (12 CPUs) ~2.6GHZ\".\n\nTable  13  presents that the running time of the proposed model is roughly the same as all other models except for RBF and NB classifiers. Although the proposed model is not the fastest in the training phase, it is the fastest in the testing phase, which is more critical since training can be performed offline, but testing is performed online. This proves that the least emotion that has been recognized is angry with the lowest recognition performance.    The four databases that have been utilized to evaluate our proposed model report different emotion recognition accuracies. In SUSAS and Arabic Emirati corpora, the results are homogeneous except for the neutral state, whereas the results of the RAVDESS corpus are homogeneous except for the neutral and angry states. On the other hand, the results of the CREMA-D dataset are heterogeneous.\n\nAll the datasets prove the superiority of the proposed model, the insufficient results of the NB classifier. Furthermore, the datasets remark the highest accuracies in neutral talking conditions and the lowest accuracies in angry talking conditions.",
      "page_start": 38,
      "page_end": 43
    },
    {
      "section_name": "Experiment 8: Human Listeners",
      "text": "In order to complete the subjective evaluation of the proposed model utilizing the Emirati-accented database for the assessment of human listeners, the assistance of ten Arabic listeners is required.\n\nIn this evaluation, a total of 2,160 audio files (10 speakers × 4 sentences × 6 emotional states counting the neutral condition × 9 repetitions/utterance) were utilized. Initially, the ten Arabic listeners were prepared and then were asked to recognize, from the test samples, the right emotion.\n\nAn illustrative graph on the subjective interpretation of the output measurement by individual listeners is demonstrated in Fig.  11 . The graph validates that the performance of human listeners is quite similar to the proposed model performance utilizing the Emirati-accented database.    (Hibare & Vibhute, 2014; Kaur & Kaur, 2013; Lu & Renals, 2014; Magre et al., 2013; Micallef, 2013; J. Wang et al., 2014) . MFCCs delta-delta  (Magre et al., 2013)  The human system response can be estimated more tightly than any other system, as the frequency bands are placed logarithmically in MFCC.\n\nIn the event of additive noise, MFCC values suffer from low robustness, thus, the values in speech recognition models should be normalized to lessen the impact of noise.\n\nLPC  (Magre et al., 2013)  LPC can decrease the sum of the squared differences between the predicted speech signal and the original speech signal over a finite period.\n\nSince human perceptiveness has fluctuating frequency perceptiveness, the speech recognition system using LPC that approximates constant weighing for the entire spectrum results in hidden outcomes.\n\nHybrid Algorithm DWPD  (Hibare & Vibhute, 2014) ,  (Sunny et al., 2013)   Connects the features of both high-frequency components (WPD) and low-frequency components (DWT).  It can not only decrease the high-frequency band to additional segments but also prevent complications in the calculation.\n\nIn DWPD, the high frequencies are reduced to get rid of the noise. Yet, from time to time the high-frequency elements may include valuable features of the signal.",
      "page_start": 44,
      "page_end": 45
    },
    {
      "section_name": "Plda",
      "text": "It is an adaptable acoustic method that gets use of the variable number of the noncorrelated input frames without any constraints of covariance modeling  (Lu & Renals, 2014) .\n\nThe Gaussian supposition that is on the class conditional distributions, is just a supposition and is not real (J.  Wang et al., 2014) . emotions is the same, which makes it impossible for a classifier to distinguish among such emotions.\n\nOur future work aims to study the novel proposed model for both speaker identification and verification under emotional and stressful talking conditions. Furthermore, we will take advantage of class ties among labels, specifically on multi-label learning issues such as ours. Moreover, we are planning to enhance the performance of the system in an angry state with decreased computational complexity and less running time in the training and testing phases. Ultimately, these outcomes in the capture of emotion-related data can trigger the development of a more robust framework to deal with in-emotion variation simultaneously. Finally, yet importantly, distributed DCT based on MFCC will be utilized and employed rather than only MFCCs delta-delta, resulting in a more robust, faster, and less complex model.",
      "page_start": 46,
      "page_end": 46
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (Sabour et al., 2017). In their algorithm, they",
      "page": 4
    },
    {
      "caption": "Figure 2: (Vesperini et al., 2019), where the input",
      "page": 12
    },
    {
      "caption": "Figure 2: The overall architecture of CapsNet (Vesperini et al., 2019)",
      "page": 13
    },
    {
      "caption": "Figure 3: In the proposed model, there are pre-steps for compression and DC-",
      "page": 16
    },
    {
      "caption": "Figure 3: Proposed DC-LSTM COMP-CapsNet algorithm",
      "page": 17
    },
    {
      "caption": "Figure 4: Fig. 4 Dual channel LSTM model",
      "page": 19
    },
    {
      "caption": "Figure 5: shows that the greatest performance",
      "page": 28
    },
    {
      "caption": "Figure 6: ROC curve signifies the graphical structure that illustrates the",
      "page": 29
    },
    {
      "caption": "Figure 6: , SVM has the maximum area under the curve, followed by the MLP, whereas RBF has the",
      "page": 29
    },
    {
      "caption": "Figure 5: Emotion recognition performance assessment using Arabic Emirati-accented corpus",
      "page": 29
    },
    {
      "caption": "Figure 7: Precision and recall functions are, respectively, demonstrated as the",
      "page": 30
    },
    {
      "caption": "Figure 6: ROC curve representation of the five classical classifiers",
      "page": 30
    },
    {
      "caption": "Figure 7: Precision and recall",
      "page": 31
    },
    {
      "caption": "Figure 8: The proposed model records the highest",
      "page": 39
    },
    {
      "caption": "Figure 8: Average emotion recognition performance based on DC-LSTM COMP-CapsNet",
      "page": 40
    },
    {
      "caption": "Figure 8: This figure indicates that the proposed model is the best for all specified",
      "page": 41
    },
    {
      "caption": "Figure 9: that CNN and CapsNet classifiers noted the next",
      "page": 41
    },
    {
      "caption": "Figure 9: Average emotion recognition performance based on the proposed model, CapsNet,",
      "page": 41
    },
    {
      "caption": "Figure 10: The proposed model remarks the highest performance amongst all the classical",
      "page": 42
    },
    {
      "caption": "Figure 10: Average emotion recognition performance based on the proposed model,",
      "page": 43
    },
    {
      "caption": "Figure 11: The graph validates that the performance of human listeners",
      "page": 44
    },
    {
      "caption": "Figure 11: Average emotion recognition performance based on human listeners and DC-LSTM",
      "page": 44
    }
  ],
  "tables": [
    {
      "caption": "Table 1: displays the corpus utilized in this analysis, where the left column",
      "data": [
        {
          "Utterance \nNumber": "1",
          "“Emirati Sentences": "ةعاس بقع كاّيو ىقلاتنب",
          "English Translation": "We will meet with you in an hour"
        },
        {
          "Utterance \nNumber": "2",
          "“Emirati Sentences": "كابي ايوبأ دنع ريس",
          "English Translation": "Go to my father he wants you"
        },
        {
          "Utterance \nNumber": "3",
          "“Emirati Sentences": "ةرجحلا نم ينوفلت تاه",
          "English Translation": "Bring my cell phone from the room"
        },
        {
          "Utterance \nNumber": "4",
          "“Emirati Sentences": "بقع كسّمرب نيحلا ةلوغشم/لوغشم",
          "English Translation": "I am busy now I will talk to you later"
        },
        {
          "Utterance \nNumber": "5",
          "“Emirati Sentences": "هقوس حدمي عاّيب لك",
          "English Translation": "Every seller praises his market"
        },
        {
          "Utterance \nNumber": "6",
          "“Emirati Sentences": "بيطت ام هتضعو بيذ بيرغلا",
          "English Translation": "A stranger is a wolf whose bite wound will not heal"
        },
        {
          "Utterance \nNumber": "7",
          "“Emirati Sentences": "مهنع كسفن مشحا سانو\n مهمشحا سان",
          "English Translation": "Show respect around some people and show self-\nrespect around other people"
        },
        {
          "Utterance \nNumber": "8",
          "“Emirati Sentences": "ام يللاو هبيعت لا هبييت تردق ام يللا\n \nهلوح موحت لا هلوطت",
          "English Translation": "Don't criticize what you can't get and don't swirl \naround something you can't obtain”"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 2: Average emotion recognition accuracy using Arabic Emirati-accented corpus based on",
      "data": [
        {
          "Proposed Model": "89.3",
          "CapsNet": "84.7",
          "CNN": "82.2",
          "NB": "31.9",
          "RBF": "42.6",
          "SVM": "69.8",
          "KNN": "53.8",
          "MLP": "69.2",
          "Classifiers": "Performance (%)"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table 3: Computed recall for shallow classifiers, CNN classifier, CapsNet model, and the proposed",
      "data": [
        {
          "Model/State": "Proposed model",
          "Neutral": "0.99",
          "Fear": "0.76",
          "Sad": "0.77",
          "Happy": "0.80",
          "Disgust": "0.77",
          "Angry": "0.71",
          "Average": "0.80"
        },
        {
          "Model/State": "CapsNet",
          "Neutral": "0.95",
          "Fear": "0.75",
          "Sad": "0.76",
          "Happy": "0.77",
          "Disgust": "0.75",
          "Angry": "0.70",
          "Average": "0.78"
        },
        {
          "Model/State": "CNN",
          "Neutral": "0.90",
          "Fear": "0.74",
          "Sad": "0.76",
          "Happy": "0.76",
          "Disgust": "0.73",
          "Angry": "0.68",
          "Average": "0.76"
        },
        {
          "Model/State": "SVM",
          "Neutral": "0.85",
          "Fear": "0.68",
          "Sad": "0.70",
          "Happy": "0.70",
          "Disgust": "0.65",
          "Angry": "0.66",
          "Average": "0.71"
        },
        {
          "Model/State": "MLP",
          "Neutral": "0.83",
          "Fear": "0.61",
          "Sad": "0.60",
          "Happy": "0.71",
          "Disgust": "0.56",
          "Angry": "0.53",
          "Average": "0.64"
        },
        {
          "Model/State": "KNN",
          "Neutral": "0.83",
          "Fear": "0.46",
          "Sad": "0.48",
          "Happy": "0.70",
          "Disgust": "0.45",
          "Angry": "0.42",
          "Average": "0.56"
        },
        {
          "Model/State": "RBF",
          "Neutral": "0.59",
          "Fear": "0.31",
          "Sad": "0.33",
          "Happy": "0.35",
          "Disgust": "0.30",
          "Angry": "0.27",
          "Average": "0.36"
        },
        {
          "Model/State": "NB",
          "Neutral": "0.47",
          "Fear": "0.25",
          "Sad": "0.26",
          "Happy": "0.28",
          "Disgust": "0.24",
          "Angry": "0.20",
          "Average": "0.28"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 3: Computed recall for shallow classifiers, CNN classifier, CapsNet model, and the proposed",
      "data": [
        {
          "Model/State": "Proposed model",
          "Neutral": "0.98",
          "Fear": "0.76",
          "Sad": "0.78",
          "Happy": "0.85",
          "Disgust": "0.76",
          "Angry": "0.74",
          "Average": "0.81"
        },
        {
          "Model/State": "CapsNet",
          "Neutral": "0.92",
          "Fear": "0.73",
          "Sad": "0.77",
          "Happy": "0.83",
          "Disgust": "0.74",
          "Angry": "0.73",
          "Average": "0.79"
        },
        {
          "Model/State": "CNN",
          "Neutral": "0.89",
          "Fear": "0.70",
          "Sad": "0.79",
          "Happy": "0.81",
          "Disgust": "0.72",
          "Angry": "0.71",
          "Average": "0.77"
        },
        {
          "Model/State": "SVM",
          "Neutral": "0.71",
          "Fear": "0.51",
          "Sad": "0.52",
          "Happy": "0.57",
          "Disgust": "0.50",
          "Angry": "0.48",
          "Average": "0.55"
        },
        {
          "Model/State": "MLP",
          "Neutral": "0.72",
          "Fear": "0.53",
          "Sad": "0.54",
          "Happy": "0.55",
          "Disgust": "0.49",
          "Angry": "0.47",
          "Average": "0.55"
        },
        {
          "Model/State": "KNN",
          "Neutral": "0.63",
          "Fear": "0.51",
          "Sad": "0.52",
          "Happy": "0.54",
          "Disgust": "0.50",
          "Angry": "0.41",
          "Average": "0.52"
        },
        {
          "Model/State": "RBF",
          "Neutral": "0.45",
          "Fear": "0.31",
          "Sad": "0.33",
          "Happy": "0.35",
          "Disgust": "0.34",
          "Angry": "0.27",
          "Average": "0.34"
        },
        {
          "Model/State": "NB",
          "Neutral": "0.37",
          "Fear": "0.25",
          "Sad": "0.26",
          "Happy": "0.28",
          "Disgust": "0.23",
          "Angry": "0.21",
          "Average": "0.27"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 5: Confusion matrix in emotional talking conditions using the Arabic Emirati-accented database",
      "data": [
        {
          "Emotion": "Neutral",
          "Neutral": "97.8",
          "Happy": "1.5",
          "Sad": "1",
          "Fear": "0",
          "Disgust": "0",
          "Angry": "1"
        },
        {
          "Emotion": "Happy",
          "Neutral": "1.2",
          "Happy": "88.7",
          "Sad": "2.2",
          "Fear": "2.1",
          "Disgust": "2.5",
          "Angry": "1.3"
        },
        {
          "Emotion": "Sad",
          "Neutral": "0",
          "Happy": "1",
          "Sad": "87.6",
          "Fear": "3",
          "Disgust": "2",
          "Angry": "2"
        },
        {
          "Emotion": "Fear",
          "Neutral": "0.4",
          "Happy": "2.3",
          "Sad": "4.5",
          "Fear": "87.9",
          "Disgust": "1.1",
          "Angry": "3.1"
        },
        {
          "Emotion": "Disgust",
          "Neutral": "0.6",
          "Happy": "2",
          "Sad": "1.4",
          "Fear": "2",
          "Disgust": "87.4",
          "Angry": "6.4"
        },
        {
          "Emotion": "Angry",
          "Neutral": "0",
          "Happy": "4.5",
          "Sad": "3.3",
          "Fear": "5",
          "Disgust": "7",
          "Angry": "86.2"
        }
      ],
      "page": 33
    },
    {
      "caption": "Table 8: , our results are significantly better than those reported in prior work.",
      "data": [
        {
          "Prior work": "Shahin et al. \n(Shahin et al., \n2019)",
          "Classifier": "GMM-DNN",
          "Emotion recognition \naccuracy (%)": "83.97",
          "*Average relative improvement rate of \nthe proposed model over prior work (%)": "6.35"
        },
        {
          "Prior work": "Shahin (Shahin, \n2019)",
          "Classifier": "CSPHMM3s",
          "Emotion recognition \naccuracy (%)": "77.80",
          "*Average relative improvement rate of \nthe proposed model over prior work (%)": "14.78"
        }
      ],
      "page": 35
    },
    {
      "caption": "Table 8: , our results are significantly better than those reported in prior work.",
      "data": [
        {
          "Prior work": "Wu et al. (Wu et al., 2019)",
          "Classifier": "CapsNet",
          "Emotion recognition \naccuracy (%)": "72.73",
          "*Average relative improvement \nrate of the proposed model over \nprior work (%)": "22.78"
        },
        {
          "Prior work": "Xi et al. (Xi et al., 2017)",
          "Classifier": "CapsNet",
          "Emotion recognition \naccuracy (%)": "71.55",
          "*Average relative improvement \nrate of the proposed model over \nprior work (%)": "24.81"
        },
        {
          "Prior work": "Shahin and Ba-Hutair (Shahin & \nBa-Hutair, 2015)",
          "Classifier": "CSPHMM2s",
          "Emotion recognition \naccuracy (%)": "76.25",
          "*Average relative improvement \nrate of the proposed model over \nprior work (%)": "17.11"
        },
        {
          "Prior work": "Shahin (Shahin, 2012)",
          "Classifier": "HMMs",
          "Emotion recognition \naccuracy (%)": "62.08",
          "*Average relative improvement \nrate of the proposed model over \nprior work (%)": "43.85"
        },
        {
          "Prior work": "",
          "Classifier": "CHMM2s",
          "Emotion recognition \naccuracy (%)": "66.92",
          "*Average relative improvement \nrate of the proposed model over \nprior work (%)": "33.44"
        },
        {
          "Prior work": "",
          "Classifier": "SPHMMs",
          "Emotion recognition \naccuracy (%)": "72.75",
          "*Average relative improvement \nrate of the proposed model over \nprior work (%)": "22.75"
        }
      ],
      "page": 35
    },
    {
      "caption": "Table 9: Comparison between the proposed work and each of CNN and CapsNet based on different",
      "data": [
        {
          "Classifier": "CNN",
          "Emotion recognition \naccuracy (%)": "82.2",
          "Average running \ntime (sec)": "2.31",
          "Code complexity": "Less complex"
        },
        {
          "Classifier": "CapsNet",
          "Emotion recognition \naccuracy (%)": "84.7",
          "Average running \ntime (sec)": "2.37",
          "Code complexity": "Less complex"
        },
        {
          "Classifier": "DC-LSTM COMP-CapsNet",
          "Emotion recognition \naccuracy (%)": "89.3",
          "Average running \ntime (sec)": "2.39",
          "Code complexity": "More complex"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table 9: Comparison between the proposed work and each of CNN and CapsNet based on different",
      "data": [
        {
          "Classifier": "CNN",
          "Emotion recognition \naccuracy (%)": "79.5",
          "Average running time \n(sec)": "1.84",
          "Code complexity": "Less complex"
        },
        {
          "Classifier": "CapsNet",
          "Emotion recognition \naccuracy (%)": "80.8",
          "Average running time \n(sec)": "1.89",
          "Code complexity": "Less complex"
        },
        {
          "Classifier": "DC-LSTM COMP-CapsNet",
          "Emotion recognition \naccuracy (%)": "82.9",
          "Average running time \n(sec)": "1.91",
          "Code complexity": "More complex"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table 9: Comparison between the proposed work and each of CNN and CapsNet based on different",
      "data": [
        {
          "Classifier": "CNN",
          "Emotion recognition \naccuracy (%)": "75.3",
          "Average running \ntime (sec)": "1.75",
          "Code complexity": "Less complex"
        },
        {
          "Classifier": "CapsNet",
          "Emotion recognition \naccuracy (%)": "77.9",
          "Average running \ntime (sec)": "1.80",
          "Code complexity": "Less complex"
        },
        {
          "Classifier": "DC-LSTM COMP-CapsNet",
          "Emotion recognition \naccuracy (%)": "82.1",
          "Average running \ntime (sec)": "1.83",
          "Code complexity": "More complex"
        }
      ],
      "page": 36
    },
    {
      "caption": "Table 12: Time complexity according to Big O notation for all the presented classifiers using the Arabic",
      "data": [
        {
          "Models/\nCase": "Best \ncase",
          "Proposed \nmodel": "Ω (n log(n))",
          "CapsNet": "Ω (n log(n))",
          "CNN": "Ω (log(n))",
          "SVM": "Ω (log(n))",
          "MLP": "Ω (log(n))",
          "KNN": "Ω \n(log(n))",
          "RBF": "Ω (m2)  Ω",
          "NB": "(log(n))"
        },
        {
          "Models/\nCase": "Average \ncase",
          "Proposed \nmodel": "Θ (n log(n))",
          "CapsNet": "Θ (n log(n))",
          "CNN": "Θ (log(n))",
          "SVM": "Θ (log(n))",
          "MLP": "Θ (n log(n))",
          "KNN": "Θ \n(log(n))",
          "RBF": "Θ (m2)",
          "NB": "Θ \n(log(n))"
        },
        {
          "Models/\nCase": "Worst \ncase",
          "Proposed \nmodel": "O (n^2)",
          "CapsNet": "O(n^2)",
          "CNN": "O(n)",
          "SVM": "O(n)",
          "MLP": "O(n)",
          "KNN": "O(n)",
          "RBF": "O(m)",
          "NB": "O(n)"
        }
      ],
      "page": 38
    },
    {
      "caption": "Table 12: Time complexity according to Big O notation for all the presented classifiers using the Arabic",
      "data": [
        {
          "Model": "DC-LSTM COMP-CapsNet",
          "Training time (sec.)": "2.8",
          "Testing time (sec.)": "1.8"
        },
        {
          "Model": "CapsNet",
          "Training time (sec.)": "2.3",
          "Testing time (sec.)": "1.6"
        },
        {
          "Model": "CNN",
          "Training time (sec.)": "2.2",
          "Testing time (sec.)": "1.5"
        },
        {
          "Model": "SVM",
          "Training time (sec.)": "1.9",
          "Testing time (sec.)": "1.3"
        },
        {
          "Model": "MLP",
          "Training time (sec.)": "1.9",
          "Testing time (sec.)": "1.3"
        },
        {
          "Model": "KNN",
          "Training time (sec.)": "1.8",
          "Testing time (sec.)": "1.2"
        },
        {
          "Model": "RBF",
          "Training time (sec.)": "1.7",
          "Testing time (sec.)": "1.1"
        },
        {
          "Model": "NB",
          "Training time (sec.)": "1.6",
          "Testing time (sec.)": "1.0"
        }
      ],
      "page": 38
    },
    {
      "caption": "Table 14: Confusion matrix using the SUSAS database based on the proposed model (%)",
      "data": [
        {
          "Emotion": "Neutral",
          "Neutral": "96.0",
          "Happy": "8",
          "Sad": "1.3",
          "Fear": "1",
          "Disgust": "2",
          "Angry": "3.5"
        },
        {
          "Emotion": "Happy",
          "Neutral": "1.5",
          "Happy": "81.9",
          "Sad": "1.5",
          "Fear": "2",
          "Disgust": "1.5",
          "Angry": "5"
        },
        {
          "Emotion": "Sad",
          "Neutral": "1",
          "Happy": "2.5",
          "Sad": "81.7",
          "Fear": "6.5",
          "Disgust": "6",
          "Angry": "6.5"
        },
        {
          "Emotion": "Fear",
          "Neutral": "0.5",
          "Happy": "2",
          "Sad": "7",
          "Fear": "80.2",
          "Disgust": "8",
          "Angry": "4"
        },
        {
          "Emotion": "Disgust",
          "Neutral": "1",
          "Happy": "2.5",
          "Sad": "3.5",
          "Fear": "4.3",
          "Disgust": "78.1",
          "Angry": "6"
        },
        {
          "Emotion": "Angry",
          "Neutral": "0",
          "Happy": "3.1",
          "Sad": "5",
          "Fear": "6",
          "Disgust": "4.4",
          "Angry": "75.0"
        }
      ],
      "page": 40
    },
    {
      "caption": "Table 15: Confusion matrix of emotional talking conditions using the RAVDESS database based on the",
      "data": [
        {
          "Emotion": "Neutral",
          "Neutral": "95.8",
          "Happy": "8",
          "Sad": "1",
          "Fear": "2",
          "Disgust": "2",
          "Angry": "3"
        },
        {
          "Emotion": "Happy",
          "Neutral": "1.5",
          "Happy": "81.9",
          "Sad": "2.3",
          "Fear": "4.3",
          "Disgust": "1.4",
          "Angry": "4.5"
        },
        {
          "Emotion": "Sad",
          "Neutral": "1",
          "Happy": "1.5",
          "Sad": "81.7",
          "Fear": "6",
          "Disgust": "6",
          "Angry": "7"
        },
        {
          "Emotion": "Fear",
          "Neutral": "0",
          "Happy": "2.5",
          "Sad": "6.5",
          "Fear": "80.2",
          "Disgust": "8",
          "Angry": "2.5"
        },
        {
          "Emotion": "Disgust",
          "Neutral": "0.5",
          "Happy": "2",
          "Sad": "3",
          "Fear": "3",
          "Disgust": "78.1",
          "Angry": "8"
        },
        {
          "Emotion": "Angry",
          "Neutral": "1.2",
          "Happy": "4.1",
          "Sad": "5.5",
          "Fear": "4.5",
          "Disgust": "4.5",
          "Angry": "75.0"
        }
      ],
      "page": 42
    },
    {
      "caption": "Table 16: Confusion matrix of emotional talking conditions using the CREMA-D database based on the",
      "data": [
        {
          "Emotion": "Neutral",
          "Neutral": "95.6",
          "Happy": "6",
          "Sad": "3",
          "Fear": "4",
          "Disgust": "2",
          "Angry": "3.7"
        },
        {
          "Emotion": "Happy",
          "Neutral": "1.7",
          "Happy": "84.8",
          "Sad": "2",
          "Fear": "2.5",
          "Disgust": "1.5",
          "Angry": "4.1"
        },
        {
          "Emotion": "Sad",
          "Neutral": "1",
          "Happy": "1.5",
          "Sad": "79.8",
          "Fear": "4.3",
          "Disgust": "6.1",
          "Angry": "6.5"
        },
        {
          "Emotion": "Fear",
          "Neutral": "0.5",
          "Happy": "3",
          "Sad": "6",
          "Fear": "78.2",
          "Disgust": "7",
          "Angry": "4.2"
        },
        {
          "Emotion": "Disgust",
          "Neutral": "1",
          "Happy": "2",
          "Sad": "4",
          "Fear": "6",
          "Disgust": "78",
          "Angry": "5.9"
        },
        {
          "Emotion": "Angry",
          "Neutral": "0.2",
          "Happy": "2.7",
          "Sad": "5.2",
          "Fear": "5",
          "Disgust": "5.4",
          "Angry": "75.6"
        }
      ],
      "page": 43
    },
    {
      "caption": "Table 17: The performance of the proposed work based on five different feature extraction techniques",
      "data": [
        {
          "Technique": "MFCCs delta-delta",
          "Average Emotion Recognition Performance": "89.3%"
        },
        {
          "Technique": "LPC",
          "Average Emotion Recognition Performance": "80.4%"
        },
        {
          "Technique": "Hybrid Algorithm DWPD",
          "Average Emotion Recognition Performance": "79.2%"
        },
        {
          "Technique": "PLDA",
          "Average Emotion Recognition Performance": "75.9%"
        },
        {
          "Technique": "DCT",
          "Average Emotion Recognition Performance": "66.8%"
        }
      ],
      "page": 45
    },
    {
      "caption": "Table 17: The performance of the proposed work based on five different feature extraction techniques",
      "data": [
        {
          "Method": "MFCCs delta-delta \n(Magre et al., 2013)",
          "Pros": "The human system response can be \nestimated  more \ntightly \nthan  any \nother \nsystem, \nas \nthe \nfrequency \nbands are placed logarithmically in \nMFCC.",
          "Cons": "In  the  event  of  additive  noise, \nMFCC  values  suffer  from  low \nrobustness,  thus,  the  values  in \nspeech \nrecognition \nmodels \nshould  be  normalized  to  lessen \nthe impact of noise."
        },
        {
          "Method": "LPC (Magre et al., \n2013)",
          "Pros": "LPC  can  decrease  the  sum  of  the \nsquared  differences  between \nthe \npredicted \nspeech \nsignal \nand \nthe \noriginal  speech  signal  over  a  finite \nperiod.",
          "Cons": "Since human perceptiveness has \nfluctuating \nfrequency \nperceptiveness, \nthe \nspeech \nrecognition  system  using  LPC \nthat \napproximates \nconstant \nweighing for the entire spectrum \nresults in hidden outcomes."
        },
        {
          "Method": "Hybrid Algorithm \nDWPD (Hibare & \nVibhute, 2014), \n(Sunny et al., 2013)",
          "Pros": " \nConnects \nthe  features  of  both \nhigh-frequency \ncomponents \n(WPD) \nand \nlow-frequency \ncomponents (DWT). \n \nIt  can  not  only  decrease \nthe \nhigh-frequency \nband \nto \nadditional \nsegments  but \nalso \nprevent \ncomplications \nin \nthe \ncalculation.",
          "Cons": "In DWPD, the high frequencies \nare  reduced \nto  get  rid  of \nthe \nnoise. Yet, from time to time the \nhigh-frequency \nelements  may \ninclude valuable  features of the \nsignal."
        },
        {
          "Method": "PLDA",
          "Pros": "It  is  an  adaptable  acoustic  method \nthat gets use of the variable number \nof  the  noncorrelated  input  frames \nwithout \nany \nconstraints \nof \ncovariance modeling (Lu & Renals, \n2014).",
          "Cons": "The Gaussian supposition that is \non \nthe \nclass \nconditional \ndistributions, \nis \njust \na \nsupposition  and  is  not  real  (J. \nWang et al., 2014)."
        }
      ],
      "page": 45
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "2",
      "title": "Feature Extraction Methods LPC , PLP and MFCC In Speech Recognition",
      "authors": [
        "N Dave"
      ],
      "year": "2013",
      "venue": "International Journal for Advance Research in Engineering and Technology"
    },
    {
      "citation_id": "3",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/TASSP.1980.1163420"
    },
    {
      "citation_id": "4",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "5",
      "title": "An attention Long Short-Term Memory based system for automatic classification of speech intelligibility",
      "authors": [
        "M Fernández-Diaz",
        "A Gallardo-Antolin"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation",
      "authors": [
        "C Goutte",
        "E Gaussier"
      ],
      "year": "2005",
      "venue": "Advances in Information Retrieval"
    },
    {
      "citation_id": "7",
      "title": "Feature Extraction Techniques in Speech Processing : A Survey",
      "authors": [
        "R Hibare",
        "A Vibhute"
      ],
      "year": "2014",
      "venue": "International Journal of Computer Applications",
      "doi": "10.5120/18744-9997"
    },
    {
      "citation_id": "8",
      "title": "T-Test Definition",
      "authors": [
        "R Hogg",
        "J Mckean",
        "A Craig"
      ],
      "year": "2005",
      "venue": "Introduction to Mathematical Statistics. Investopedia"
    },
    {
      "citation_id": "9",
      "title": "Probabilistic Linear Discriminant Analysis",
      "authors": [
        "S Ioffe"
      ],
      "year": "2006",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "11",
      "title": "Tool wear monitoring using naïve Bayes classifiers",
      "authors": [
        "J Karandikar",
        "T Mcleay",
        "S Turner",
        "T Schmitz"
      ],
      "year": "2015",
      "venue": "The International Journal of Advanced Manufacturing Technology",
      "doi": "10.1007/s00170-014-6560-6"
    },
    {
      "citation_id": "12",
      "title": "Enhancement of Speech Recognition Algorithm Using DCT and Inverse Wave Transformation",
      "authors": [
        "S Kaur",
        "E Kaur"
      ],
      "year": "2013",
      "venue": "Journal of Engineering Research and Applications"
    },
    {
      "citation_id": "13",
      "title": "Capsule Networks -A survey",
      "authors": [
        "Kwabena Patrick",
        "M Felix Adekoya",
        "A Abra Mighty",
        "A Edward"
      ],
      "year": "2019",
      "venue": "Capsule Networks -A survey",
      "doi": "10.1016/j.jksuci.2019.09.014"
    },
    {
      "citation_id": "14",
      "title": "Particle swarm optimization for parameter determination and feature selection of support vector machines",
      "authors": [
        "S.-W Lin",
        "K.-C Ying",
        "S.-C Chen",
        "Z.-J Lee"
      ],
      "year": "2008",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2007.08.088"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "Y Mei",
        "J.-W Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2018.05.005"
    },
    {
      "citation_id": "16",
      "title": "A multimodal emotional communication based humans-robots interaction system",
      "authors": [
        "Z Liu",
        "F Pan",
        "M Wu",
        "W Cao",
        "L Chen",
        "J Xu",
        "R Zhang",
        "M Zhou"
      ],
      "year": "2016",
      "venue": "35th Chinese Control Conference (CCC)",
      "doi": "10.1109/ChiCC.2016.7554357"
    },
    {
      "citation_id": "17",
      "title": "Probabilistic Linear Discriminant Analysis for Acoustic Modeling",
      "authors": [
        "L Lu",
        "S Renals"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2014.2313410"
    },
    {
      "citation_id": "18",
      "title": "A comparative study on feature extraction techniques in speech recognition",
      "authors": [
        "S Magre",
        "R Deshmukh",
        "P Shrishrimal"
      ],
      "year": "2013",
      "venue": "International Conference on Recent Advances in Statistics and Their Application"
    },
    {
      "citation_id": "19",
      "title": "Comparative study of automatic speech recognition techniques",
      "authors": [
        "J Micallef"
      ],
      "year": "2013",
      "venue": "IET Signal Processing",
      "doi": "10.1049/iet-spr.2012.0151"
    },
    {
      "citation_id": "20",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2017.7952552"
    },
    {
      "citation_id": "21",
      "title": "Speech Feature Extraction Techniques: A Review",
      "authors": [
        "S Narang",
        "D Gupta"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Science and Mobile Computing"
    },
    {
      "citation_id": "22",
      "title": "Speech Recognition Using Deep Neural Networks: A Systematic Review",
      "authors": [
        "A Nassif",
        "I Shahin",
        "I Attili",
        "M Azzeh",
        "K Shaalan"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2896880"
    },
    {
      "citation_id": "23",
      "title": "CASA-based speaker identification using cascaded GMM-CNN classifier in noisy and emotional talking conditions",
      "authors": [
        "A Nassif",
        "I Shahin",
        "S Hamsa",
        "N Nemmour",
        "K Hirose"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2021.107141"
    },
    {
      "citation_id": "24",
      "title": "Linear predictive coding",
      "authors": [
        "D O'shaughnessy"
      ],
      "year": "1988",
      "venue": "IEEE Potentials",
      "doi": "10.1109/45.1890"
    },
    {
      "citation_id": "25",
      "title": "Examining the Benefits of Capsule Neural Networks",
      "authors": [
        "A Punjabi",
        "J Schmid",
        "A Katsaggelos"
      ],
      "year": "2020",
      "venue": "Examining the Benefits of Capsule Neural Networks"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition based on ECG signals for service robots in the intelligent space during daily life",
      "authors": [
        "K Rattanyu",
        "M Mizukawa"
      ],
      "year": "2011",
      "venue": "Ournal of Advanced Computational Intelligence and Intelligent Informatics"
    },
    {
      "citation_id": "27",
      "title": "Dynamic Routing Between Capsules",
      "authors": [
        "S Sabour",
        "N Frosst",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "",
      "authors": [
        "Inc Associates"
      ],
      "venue": ""
    },
    {
      "citation_id": "29",
      "title": "On the Use of Distributed DCT in Speaker Identification",
      "authors": [
        "M Sahidullah",
        "G Saha"
      ],
      "year": "2009",
      "venue": "Annual IEEE India Conference",
      "doi": "10.1109/INDCON.2009.5409408"
    },
    {
      "citation_id": "30",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2011.01.011"
    },
    {
      "citation_id": "31",
      "title": "Studying and enhancing talking condition recognition in stressful and emotional talking environments based on HMMs, CHMM2s and SPHMMs",
      "authors": [
        "I Shahin"
      ],
      "year": "2012",
      "venue": "Journal on Multimodal User Interfaces",
      "doi": "10.1007/s12193-011-0082-4"
    },
    {
      "citation_id": "32",
      "title": "Employing Emotion Cues to Verify Speakers in Emotional Talking Environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2016",
      "venue": "Journal of Intelligent Systems",
      "doi": "10.1515/jisys-2014-0118"
    },
    {
      "citation_id": "33",
      "title": "Novel Third-Order Hidden Markov Models for Speaker Identification in Shouted Talking Environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2018",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2014.07.006"
    },
    {
      "citation_id": "34",
      "title": "Emotion Recognition based on Third-Order Circular Suprasegmental Hidden Markov Model",
      "authors": [
        "I Shahin"
      ],
      "year": "2019",
      "venue": "IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT)",
      "doi": "10.1109/JEEIT.2019.8717396"
    },
    {
      "citation_id": "35",
      "title": "Text-Independent Emirati-Accented Speaker Identification in Emotional Talking Environment",
      "authors": [
        "I Shahin"
      ],
      "year": "2018",
      "venue": "Fifth HCT Information Technology Trends (ITT)",
      "doi": "10.1109/CTIT.2018.8649514"
    },
    {
      "citation_id": "36",
      "title": "Talking condition recognition in stressful and emotional talking environments based on CSPHMM2s",
      "authors": [
        "I Shahin",
        "M Ba-Hutair"
      ],
      "year": "2015",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-014-9251-7"
    },
    {
      "citation_id": "37",
      "title": "Emotion Recognition Using Hybrid Gaussian Mixture Model and Deep Neural Network",
      "authors": [
        "I Shahin",
        "A Nassif",
        "S Hamsa"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2901352"
    },
    {
      "citation_id": "38",
      "title": "Cross-corpus speech emotion recognition based on transfer non-negative matrix factorization",
      "authors": [
        "P Song",
        "W Zheng",
        "S Ou",
        "X Jin",
        "Y Liu",
        "J Yu"
      ],
      "year": "2016",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2016.07.010"
    },
    {
      "citation_id": "39",
      "title": "An efficient unconstrained facial expression recognition algorithm based on Stack Binarized Auto-encoders and Binarized Neural Networks",
      "authors": [
        "W Sun",
        "H Zhao",
        "Z Jin"
      ],
      "year": "2017",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2017.06.050"
    },
    {
      "citation_id": "40",
      "title": "Design of a Novel Hybrid Algorithm for Improved Speech Recognition with Support Vector Machines Classifier",
      "authors": [
        "S Sunny",
        "S David Peter",
        "K Jacob"
      ],
      "year": "2013",
      "venue": "International Journal of Emerging Technology and Advanced Engineering"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition using speech and neural structured learning to facilitate edge intelligence",
      "authors": [
        "M Uddin",
        "E Nilsson"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": "10.1109/2943.974352"
    },
    {
      "citation_id": "43",
      "title": "Polyphonic Sound Event Detection by Using Capsule Neural Networks",
      "authors": [
        "F Vesperini",
        "L Gabrielli",
        "E Principi",
        "S Squartini"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Discriminative scoring for speaker recognition based on I-vectors",
      "authors": [
        "J Wang",
        "D Wang",
        "Z Zhu",
        "T Zheng",
        "F Soong"
      ],
      "year": "2014",
      "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA)"
    },
    {
      "citation_id": "45",
      "title": "Asia-Pacific",
      "venue": "Asia-Pacific",
      "doi": "10.1109/APSIPA.2014.7041619"
    },
    {
      "citation_id": "46",
      "title": "Predicting Subcellular Localization of Apoptosis Proteins Combining GO Features of Homologous Proteins and Distance Weighted KNN Classifier",
      "authors": [
        "X Wang",
        "H Li",
        "Q Zhang",
        "R Wang"
      ],
      "year": "2016",
      "venue": "BioMed Research International",
      "doi": "10.1155/2016/1793272"
    },
    {
      "citation_id": "47",
      "title": "Speech Emotion Recognition Using Capsule Networks. ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Using Capsule Networks. ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2019.8683163"
    },
    {
      "citation_id": "48",
      "title": "Capsule Network Performance on Complex Data",
      "authors": [
        "E Xi",
        "S Bing",
        "Y Jin"
      ],
      "year": "2017",
      "venue": "Capsule Network Performance on Complex Data"
    },
    {
      "citation_id": "49",
      "title": "MLP Neural Network Based Gas Classification System on Zynq SoC",
      "authors": [
        "X Zhai",
        "A Ali",
        "A Amira",
        "F Bensaali"
      ],
      "year": "2016",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2016.2619181"
    },
    {
      "citation_id": "50",
      "title": "An emotion classification algorithm based on SPT-CapsNet",
      "authors": [
        "X Zhong",
        "J Liu",
        "L Li",
        "S Chen",
        "W Lu",
        "Y Dong",
        "B Wu",
        "L Zhong"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-019-04621-y"
    },
    {
      "citation_id": "51",
      "title": "Speech Emotion Recognition Using Both Spectral and Prosodic Features. 2009 International Conference on Information Engineering and Computer Science",
      "authors": [
        "Y Zhou",
        "Y Sun",
        "J Zhang",
        "Y Yan"
      ],
      "year": "2009",
      "venue": "Speech Emotion Recognition Using Both Spectral and Prosodic Features. 2009 International Conference on Information Engineering and Computer Science",
      "doi": "10.1109/ICIECS.2009.5362730"
    }
  ]
}