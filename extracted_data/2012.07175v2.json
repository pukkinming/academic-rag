{
  "paper_id": "2012.07175v2",
  "title": "Msaf: Multimodal Split Attention Fusion",
  "published": "2020-12-13T22:42:41Z",
  "authors": [
    "Lang Su",
    "Chuqing Hu",
    "Guofa Li",
    "Dongpu Cao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal learning mimics the reasoning process of the human multi-sensory system, which is used to perceive the surrounding world. While making a prediction, the human brain tends to relate crucial cues from multiple sources of information. In this work, we propose a novel lightweight multimodal fusion module that learns to emphasize more contributive features across all modalities. Specifically, the proposed Multimodal Split Attention Fusion (MSAF) module splits each modality into channel-wise equal feature blocks and creates a joint representation that is used to generate soft attention for each channel across the feature blocks. Further, the MSAF module is designed to be compatible with features of various spatial dimensions and sequence lengths, suitable for both CNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal networks and utilize existing pretrained unimodal model weights. To demonstrate the effectiveness of our fusion module, we design three multimodal networks with MSAF for emotion recognition, sentiment analysis, and action recognition tasks. Our approach achieves competitive results in each task and outperforms other applicationspecific networks and multimodal fusion benchmarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal learning has been explored in numerous machine learning applications such as audio-visual speech recognition  [34] , action recognition  [2] , and video question answering  [22] , where each modality contains useful information from a different perspective. Although these tasks can benefit from the complementary relationship in multimodal data, different modalities are represented in diverse fashions, making it challenging to grasp their complex correlations.\n\nStudies in multimodal machine learning are mainly categorized into three fusion strategies: early fusion, intermediate fusion, and late fusion. Early fusion explicitly * Equal Contribution. exploits the cross-modal correlation by joining the representation of the features from each modality at the feature level, which is then used to predict the final outcome. The fusion is typically operated after the feature extractor for each modality, where techniques such as Compact Bilinear Pooling (CBP)  [12, 36]  and Canonical Correlation Analysis (CCA)  [28, 31]  are used to exploit the covariation between modalities. Unfortunately, modalities usually have different natures causing unaligned spatial and temporal dimensions. This creates obstacles in capturing the latent interrelationships in the low-level feature space  [4] . On the other hand, late fusion fuses the decision from each modality into a final decision using a simple mechanism such as voting  [33]  and averaging  [41] . Since little training is required, a multimodal system can be promptly deployed by utilizing pretrained unimodal weights, unlike early fusion methods. However, decision-level fusion neglects the crossmodal correlation between the low-level features in modalities, resulting in limited improvement compared to the unimodal models. The intermediate fusion method joins features in the middle of the network, where some feature processing is done for the raw features from the feature extractors. Recent intermediate multimodal fusion networks  [20, 45, 17]  exploit the modality-wise relationships at different stages of the network, which has shown impressive results. However, there are still a limited number of works that can effectively capture cross-modal dynamics in an efficient way by using pretrained weights while introducing minimal parameters.\n\nTo overcome the aforementioned shortcomings of intermediate fusion methods, we propose a lightweight fusion module, MSAF, taking inspiration from the split-attention block in ResNeSt  [53] . The split-attention mechanism explores cross-channel relationships by dividing the featuremap into several groups and applying attention across the groups based on the global contextual information. We extend split-attention for multimodal applications in the proposed MSAF module to explore inter-and intra-modality relationships while maintaining a compact multimodal context. The MSAF module splits the features of each modality channel-wise into equal-sized feature blocks, which are globally summarized by a channel descriptor. The descrip-tor then learns to emphasize the important feature blocks by generating attention values. Subsequently, the enhanced feature blocks are rejoined for each modality, resulting in an optimized feature space with an understanding of the multimodal context. Our MSAF module is compatible with features of any shape as it operates only on the channel dimension. Thus, MSAF can be added between layers of any CNN or RNN architecture. Furthermore, we boost performance on sequential features by splitting modalities timewise and applying an MSAF module to each time segment. This allows emphasis of different feature blocks in each time segment. To our knowledge, our work is the first independent fusion module that can be used in both CNN-and RNN-based multimodal learning applications.\n\nWe comprehensively evaluate the effectiveness of MSAF in three multimodal learning tasks, namely audiovisual emotion recognition, sentiment analysis, and action recognition. We design a neural network with integrated MSAF modules for each task to demonstrate the ease of applying MSAF to existing unimodal configurations. Our experiments show that MSAF-powered networks outperform other fusion methods and state-of-the-art models designed for each application. Empirically, we observe that MSAF achieves better results while using a similar number of parameters as simple late fusion methods. Our module learns to pinpoint the important features regardless of the modality's complexity.\n\nIn summary, our work provides the following contributions: 1) MSAF -A novel lightweight multimodal fusion module for CNN and RNN networks that effectively fuses intermediate and high-level modality features to leverage the advantages of each modality. 2) We design three multimodal fusion networks corresponding to three applications: emotion recognition, sentiment analysis, and action recognition. Our experiments demonstrate the capabilities of MSAF through competitive results in all applications while utilizing fewer parameters.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Early Fusion. The majority of works in early fusion integrate features immediately after they are extracted from each modality, whereas occasionally studies perform fusion at the input level, such as  [32] . A simple solution for early fusion is feature concatenation after they are transformed to the same length, followed by fully connected layers. Many early fusion works use CCA to exploit cross-modality correlations.  [39]  applies CCA to improve the performance in speaker identification using visual and audio modalities.  [1]  proposes deep CCA to learn complex nonlinear transformations between modalities, which inspired multimodal applications such as  [28] . Bilinear pooling is another early fusion method that fuses modalities by calculating their outer product. However, the generated high dimensional feature vectors are very computationally expensive for subsequent analysis. Compact bilinear pooling  [13]  significantly mitigates the curse of dimensionality problem  [17]  through a novel kernelized analysis while keeping the same discriminative power as the full bilinear representation.\n\nLate Fusion. Late fusion merges the decision values from each unimodal model into an unified decision using fusion mechanisms such as averaging  [41] , voting  [33]  and weighted sum  [46] . In contrast to early fusion, late fusion embraces the end-to-end learning between each modality and the given task. It allows for more flexibility as it can still train or make predictions when one or more modalities are missing. Nevertheless, late fusion lacks the exploration of lower-level correlations between the modalities. Therefore, when it comes to a disagreement between modalities, a simple mechanism acting only on decisions might be too simplified. There are also more complex late fusion approaches that exploit modality-wise synergies. For example,  [26]  proposes a multiplicative combination layer that promotes the training of strong modalities per sample and tolerates mistakes made by other modalities.\n\nIntermediate Fusion. Intermediate fusion exploits feature correlations after some level of processing, therefore the fusion takes place in the middle between the feature extractor and the decision layer. For instance,  [48]  applies principle component analysis on the extracted features for each modality, and further processes them respectively before feature concatenation. Recent works continue to improve modality feature alignment to give stronger joint features. CentralNet  [45]  coordinates features of each modality by performing a weighted sum of modalities in a central branch at different levels of the network. EmbraceNet  [7]  prevents dependency on data of specific modalities and increases robustness to missing data through learning crossmodal correlations by combining selected features from each modality using a multinomial distribution.  [20]  utilizes the squeeze and excitation module from SENet  [18]  to enable slow modality fusion by channel-wise feature recalibration at different stages of the network. Our work aims to effectively fuse features of modalities while maintaining efficiency. We adopt the split attention  [53]  concept for multimodal fusion where modalities are broken down into feature map groups with hidden complement relationships.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "We first formulate the multimodal fusion problem in an MSAF module. Let M be the number of modalities and the feature map of modality m ∈ {1, 2,\n\nHere, K is the number of spatial dimensions of modality m and C m is the number of Split. We start by splitting each feature map channelwise into equal-channel feature blocks where the number of channels in each block is C. We denote the set of the feature blocks that belong to modality m as B m , where\n\nthe last block is padded with zeros in the missing channels.\n\nJoin. The join operation is a crucial step that learns the multimodal global context which is used to generate per channel block-wise attention. We join the blocks that belong to modality m into a shared representation D m , by calculating the element-wise sum S m over B m , followed by global average pooling on the spatial dimensions:\n\n) Each channel descriptor is now a feature vector of the common length C that summarizes the feature blocks within a modality. To obtain multimodal contextual information, we calculate the element-wise sum of the per modality descriptors {D 1 , • • • , D M } to form a multimodal channel descriptor G. We capture the channel-wise dependencies by a fully connected layer with a reduction factor r followed by a batch normalization layer and a ReLU activation function. The transformation maps G to the joint representation Z ∈ IR C , C = C/r which helps with generalization for complex models.\n\nwhere\n\nAs advised in  [20]  and evident in our experiments, a reduction factor of 4 is ideal for two modalities. As the number of modalities increase, we recommend reducing the reduction factor to accommodate mores features in the joint representation. For example, when fusing 3 modalities, a reduction factor of 2 was optimal in our results for sentiment analysis.\n\nHighlight. The multimodal channel descriptor contains generalized but rich knowledge of the global context. In this step, for a block B i m , we generate the corresponding logits U i m by applying a linear transformation on Z and obtain the block-wise attention weights A i m using the softmax activation.\n\nwhere W i m ∈ IR C×C and b i m ∈ IR C are weights and bias of the corresponding fully connected layer. Since soft attention values are dependent on the total number of feature blocks, features may be over-suppressed. The effect is more apparent in complex tasks which results in insufficient information for accurate predictions. Thus, we present a hyperparameter λ ∈ [0, 1] that controls the suppression power of MSAF. Intuitively, λ can be understood as a regularizer for the lowest attention of a split. We obtain an optimized feature block Bi m using attention signals A i m and λ:\n\nFinally, the feature blocks belonging to modality m are merged by channel-wise concatenation to produce Fm .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Blockdropout",
      "text": "To lessen the dependencies on certain strong feature blocks and ease overfitting, we propose a dropout method for the feature blocks called BlockDropout. BlockDropout generates a binary mask that randomly drops feature blocks from the set of all feature blocks from each modality B, and applies the same mask on the block's attention. Let the dropout probability p ∈ [0, 1), we draw |B| samples from a Bernoulli distribution with the probability of success (1 -p), resulting in a binary mask for dropping out the feature blocks. Subsequently, the mask is scaled by 1  1-p and is applied to the generated attention vectors. This is not to be confused with DropBlock  [14]  which is used in ResNeSt to regularize convolutional layers by randomly masking out local block regions in the feature map. Whereas Block-Dropout is applied to feature blocks after the first step of MSAF which are split in the channel dimension.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Enhancing Msaf For Rnns",
      "text": "As features pass through a CNN, the number of channels gradually increases while other dimensions shrink from convolving filters and pooling layers. In a multilayer RNN, the length of the feature sequence remains the same between layers. When applying split attention to features of longer sequence length, as is the case for RNNs, the original MSAF module does not have the flexibility to adjust the attention vector of each block over the sequence. For example, in audiovisual emotion recognition, there may be a segment where the audio modality should be highlighted over the video modality and vice versa in another segment of the same sequence.\n\nWe achieve this by splitting each modality into q segments. We define the sequence length of a modality as S then the length of each segment for modality m is S m /q . The modalities of each segment are passed into separate MSAF blocks. Optimally the sequence lengths are the same or evenly divisible by q. Otherwise, the extra segment is combined with the second last segment. This process is visualized in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Applications",
      "text": "In this section, we apply the MSAF module to fuse unimodal networks in three applications. We describe each unimodal network and our configuration for the MSAF modules.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Multimodal emotion recognition (MER) is a classification task that categorizes human emotions using multiple interacting signals. Although numerous works have utilized more complex modalities such as EEG  [54]  and body gesture  [9] , video and audio remain as dominant modalities used for this task. Thus, we design a multimodal network that fuses a 3D CNN for video and a 1D CNN for audio using MSAF. Video data has dependencies in both spatial and temporal dimensions, therefore requires a network with 3D kernels to learn both the facial expression and its movement. Considering both network performance and training efficiency, we choose the 3D ResNeXt50 network  [49]  as suggested by  [15]  with cardinality set to 32. For the audio modality, recent works  [35, 47]  have demonstrated the effectiveness of deep learning based methods built on Melfrequency cepstral coefficients (MFCC) features. We design a simple 1D CNN for the MFCC features and fuse the two modalities via two MSAF modules as shown in Figure  3 . We configured the two MSAF modules with 16 and 32 channels per block respectively and BlockDropout with p = 0.2. Finally, we sum the logits of both networks followed by a softmax function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "Sentiment analysis is the use of Natural Language Processing (NLP) to interpret and classify people's opinions from text. In recent years, the multimodal nature of human language has led to the incorporation of other modalities such as visual and audio data in NLP tasks. Similar to works such as  [42, 16] , we apply our module on audio, visual and text modalities. Our architecture uses two LSTM layers for each modality with an MSAF module in between to fuse features from the first LSTM layer. We use 32, 64 and 128 hidden units for both LSTM layers in the visual, audio and text modality respectively. For the MSAF module, each modality is split into 5 segments sequence-wise (q = 5) and each segment is passed into a separate MSAF block with 16 channels per feature block and BlockDropout with p = 0.2. Lastly, we concatenate the final output from each LSTM and pass that to a fully connected layer to generate the sentiment value.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Action Recognition",
      "text": "With the development of depth cameras, depth and skeleton data became crucial modalities in the action recognition task along with RGB videos. Multiple works such as  [20, 24, 27]  have achieved competitive performance using RGB videos associated with skeleton sequences. We follow  [20]  which utilizes I3D  [6]  for the video data, and HCN  [23]  for the skeleton stream. As illustrated in Figure  4 , we deploy two MSAF modules, one at an intermediate-level in both networks, and the other one for high-level feature recalibration. The HCN framework proposes two strategies to be scalable to multi-person scenarios. The first type stacks the joints from all persons and feeds it as the input of the network in an early fusion style. The second type adapts late fusion that passes the inputs of multiple persons through the same subnetwork, whose Conv6 channel-wise concatenates or element-wise maximizes the group of features of persons. The latter generalizes better to various numbers of persons than the other, which needs a predefined maximum number of persons.  [20]  follows the multi-person late fusion strategy and utilizes their first fusion module on one of the two persons universally. We take a different approach by considering all available individuals in a sample because either can send important signals during a multi-person interaction. Our first MSAF module has 64 channels per block and is inserted between the second last Inception layer in I3D and the Conv5 outputs of each person. The second MSAF has 256 channels per block and is positioned between the last Inception layer in I3D and the FC7 layer in HCN. A suppression power of λ = 0.5 is used for both modules. Finally, we average the logits of both networks followed by a softmax function.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we discuss our dataset choice, data preprocessing and training details for each application. Further, we evaluate our method and compare with other stateof-the-art works. Validation set accuracy was used to select the optimal hyperparameters for benchmarks and our proposed method in our tables. To verify the effectiveness of MSAF and the proposed hyperparameters, we conduct an ablation study for each task and analyze the module through its complexity, computation cost and visualization of attention signals. All of our experiments were conducted using a single Nvidia 2080 Ti GPU in Ubuntu 20.04 with Python 3.6 and PyTorch 1.7.1. To ensure the reproducibility of our results, our code is made publicly available on GitHub.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Data Preparation. There are many emotion recognition datasets that contain both facial expression and audio signals including  [29, 25] . We chose the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [29]  dataset due to its high quality in both video and audio recording and the sufficient number of video clips. RAVDESS contains 1440 videos of short speech from 24 actors (12 males, 12 females), performed under the emotion they are told to act upon. Eight emotions are included in the dataset: neutral, calm, happy, sad, angry, fearful, disgust and surprised. We extracted 30 consecutive images from each video. For each image, we used the 2D facial landmarks provided to crop the face area and then resized to (224, 224). Random crop, horizontal flip, and normalization were used for data augmentation. For the audio modality, since the first 0.5 seconds usually contains no sound, we cropped the first 0.5 seconds and took the next 2.45 seconds for consistency. As suggested by  [19] , we extracted the first 13 MFCC features for each cropped audio clip. We proceed with a 6 fold cross-validation based on the actors for the RAVDESS dataset. We split the 24 actors in a 5:1 ratio for the training and testing sets. Since the gender of the actors is indicated by even or odd actor IDs, we keep the genders evenly distributed by rotating through 4 consecutive actor IDs as the testing set for each fold.\n\nTraining. We first fine-tuned the unimodal models for each fold on RAVDESS. For both unimodal and multimodal training, we used the Adam optimizer  [21]  with a constant learning rate of 10 -3 . The final accuracy reported is the average accuracy over the 6 folds.\n\nBenchmarks. For this task, we implemented multiple recent multimodal fusion algorithms as our benchmarks. We categorize them into the following: 1) simple feature concatenation followed by fully connected layers based on  [37]  and MCBP  [12]  as two early fusion methods, 2) MMTM  [20]  as the state-of-the-art intermediate fusion method, 3) averaging, multiplication are two standard late fusion methods; multiplicative layer  [26]  is a late fusion method that adds a down-weighting factor to CE loss to suppress weaker modalities.\n\nResults. Table  1  presents the accuracy of the proposed method in comparison with the implemented benchmarks. Our network surpasses unimodal baselines by over 10% verifying the importance of multimodal fusion. Early fusion methods did not exceed standard late fusion benchmarks by a significant number, indicating the challenge of finding cross-modal correlations between the complex video network and the 1D audio model in early stages. As expected, intermediate fusion methods out performed late and early methods as they are able to highlight features while they are developed to identify areas of focus in each modality. Our model outperforms the top performer MMTM by 1.74% while using 19% less parameters. Compared to the unimodal models, we only introduced 30K parameters in the fusion module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "Data Preparation. CMU-MOSI  [52]  and CMU-MOSEI  [3]  are commonly used datasets for multimodal sentiment analysis. We chose to evaluate our method using CMU-MOSEI since it is the next generation of CMU-MOSI provided by the same authors. The CMU-MOSEI dataset contains 22852 annotated video segments (utterances) from 1000 distinct speakers and 250 topics gathered from online video sharing websites. Each utterance is annotated with a sentiment intensity from  [-3, 3]  test set contain 16322, 1871, and 4659 samples respectively. Following recent works, we evaluate: 1) mean absolute error (MAE) and Pearson correlation (Corr) for regression, 2) binary accuracy (Acc-2) and F-score, 3) 7 class accuracy (Acc-7) from -3 to 3. For binary classification, we consider [-3, 0) labels as negative and (0, 3] as positive.\n\nIn order to draw comparisons with recent works  [42, 16] . COVAREP  [10] , FACET  1  and BERT  [11]  features were selected for audio, visual and text modalities respectively. We acquired aligned COVAREP, FACET features and raw text from the public CMU-MultimodalSDK v1.2.0 2  with a sequence length of 50 words for all modalities. The raw text of individual utterances was passed into a pretrained uncased BERT model (not fine-tuned on CMU-MOSEI) and the final encoder layer output was used for text features. Each utterance was passed in separate epochs to avoid adding additional context.\n\nTraining. Since the network is relatively small, the unimodal models were not pretrained. The multimodal network was trained using mean squared error (MSE) loss and the Adam optimizer was used with a constant learning rate of 10 -3 . For Acc-7, we rounded the output to the nearest integer and clipped to -3 and 3.\n\nBenchmarks. We summarize various multimodal fusion methods proposed for sentiment analysis as follows: 1) DCCA  [43]  and ICCN  [42]  use Deep CCA to correlate text with audio-visual features, 2) TFN  [51]  and LMF  [30]  perform outer-products on modalities to create a joint representation, 3) MFM  [44]  and MISA  [16]  separates features into modality-specific and modality-invariant. While MFM optimizes a joint generative-discriminative objective allowing for classification and reconstruction of missing modalities, MISA trains modality-specific and multimodal encoders which are then fused using multi-headed self-attention before prediction.   2 . Comparison between multimodal fusion benchmarks and ours for sentiment analysis on CMU-MOSEI. * from original papers and from  [42] . The standard error over 5 runs are in italics.\n\nResults. Table  2  shows the results of our experiments in comparison with the state-of-the-art and recent works using BERT. Our multimodal model outperforms all unimodal models confirming that audio-visual features improve sentiment analysis. MSAF achieves better or similar performance on all metrics compared to state-of-the-art multimodal methods while using a simpler network architecture. Compared to MISA and others, our training method is simpler, allowing MSAF to be easily applied to existing unimodal networks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Action Recognition",
      "text": "Data Preparation. NTU RGB+D  [40]  is a large-scale human action recognition dataset. It contains 60 action classes and 56,880 video samples associated with 3D skeleton data. Cross-Subject (CS) and Cross-View (CV) are two recommended protocols. CS splits the training set and testing set by the subject IDs, whereas CV splits the samples based on different camera views. Recent methods  [50, 27, 8]  have achieved decent CV accuracies; however, CS still remains a more challenging evaluation method based on the reported performance compared to the CV counterpart. We adopt the CS evaluation and split the 40 subjects based on the specified rule. For data preprocessing, video frames are extracted at 32 FPS and we adopt the same data augmentation approach as  [20] .\n\nTraining. The Adam optimizer with a base learning rate of 10 -3 and a weight decay of 10 -4 is used. The learning rate is reduced to 10 -4 at epoch 5, where the loss is near saturation in our experiment.\n\nBenchmarks. We summarize the multimodal fusion benchmarks for action recognition based on RGB videos and skeletons as follows: 1) SGM-Net  [24]  proposed a skeleton guidance block to enhance RGB features, 2) Cen-tralNet  [45]     3 . Comparison between multimodal fusion benchmarks and ours on the NTU RGB+D Cross-Subject protocol. * from original papers and from  [20] . The standard error for Inflated ResNet50 and I3D over 5 runs is 0.04 and 0.03 respectively. sum of skeleton and RGB features at various locations, 3) MFAS  [38]  is a generic search algorithm that finds an optimal architecture for a given dataset, 4) PoseMap  [27]  uses CNNs to process pose estimation maps and skeletons independently with late fusion for final prediction, 5) MMTM  [20]  recalibrates features at different stages achieving stateof-the-art in RGB and skeleton fusion.\n\nResults. Table  3  reports the accuracy of the proposed MSAF network in comparison with other action recognition models using RGB videos and skeletons. To compare with state-of-the-art intermediate fusion methods, we also evaluate the performance of MSAF applied to Inflated ResNet50  [5]  and HCN. Our model outperforms all intermediate fusion methods and application-specific models, achieving state-of-the-art performance in RGB+pose action recognition in the NTU RGB+D CS protocol.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To obtain the configurations used for each application, we conduct the ablation study on all three datasets with the following hyperparameters: the number of channels in a block C, attention regularizer λ (default value is 0), Block-Dropout (with p = 0.2), and the number of segments q for RNNs (default value is 1). Table  4  reports the accuracy of the configurations building up to the best configuration. We observe the optimal number of channels in a block, C, for each dataset can be derived from min {C 1 , • • • , C M }/2 which serves as a good starting point when tuning C for other applications. Hyperparameter λ plays an important role in NTU by avoiding over-suppression of features for more complex tasks. BlockDropout is essential to the performance in RAVDESS and MOSEI but not NTU as dropout tends to be more effective on smaller datasets to prevent overfitting. The importance of q is shown in MO-SEI, where a sequence length of 5 words was optimal for a sequence length of 50 words. As q increases, the number of MSAF modules increases which increases the number of parameters and thus results in overfitting.\n\nThe location of a MSAF module in a multimodal network architecture is an important factor for effective feature fusion. On one hand, placing a MSAF module at an earlier part of the network can help unimodal models learn to correlate raw features of each other. On the other hand, using MSAF to fuse high-level features generates more apparent bias towards specific unimodal patterns as the highlevel features are more tailored to the task. To analyze the effect of MSAF in different fusion locations on model performance, we define three positions to place MSAF in our action recognition network (I3D + HCN). In the early location, a MSAF receives the concatenated Conv4 features from the two actors in HCN and the third last Inception layer of I3D. The intermediate location is set to be between the Conv5 layer of HCN and the second last Inception layer of I3D. Finally, the late location is at the last I3D Inception layer and the FC7 layer of HCN. We follow C = min {C 1 , • • • , C M }/2 while keeping other parameters the same. We train the multimodal network with different combinations of the above fusion locations and report our results in Table  5 .\n\nWe observe that the combination of intermediate and late fusion achieves the best result among all seven experiments. Interestingly, all experiments that involve early fusion yield similar performance at around 91.9%. Further, deploying MSAF in all three locations does not achieve better performance than using only intermediate and late fusion. We believe this is because the low-level features at the early position are still underdeveloped to show enough correlation for effective fusion, which results in sub-optimal performance. In summary, we find that multimodal fusion using MSAF is the most effective when applied to a combination of intermediate and high-level features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parameter And Attention Analysis",
      "text": "Reflecting on our objective to design an effective fusion module that is also lightweight, we analyze the number of parameters of the MSAF module. Ideally, the fusion module should introduce minimal parameters to the unimodal networks combined despite the feature map size of the modalities. The split and join steps in MSAF ensure the joint feature space depends on the channel number of the feature blocks instead of the channel of the modalities. Therefore, the number of parameters is significantly reduced. In Figure  5 , we compare the number of parameters of MSAF to MMTM  [20] . For both methods, we use two example modalities with shape (4, #Channels, 3, 128, 128) where #Channels is indicated on the x-axis. The reduction factor is set to 4 for both modules and we set C = min {C 1 , • • • , C M }/2 for MSAF. As shown, MSAF utilizes parameters more efficiently, reaching a maximum of 330K parameters. In terms of computational cost, the number of FLOPs for MSAF has a similar trend as the number of parameters. For instance, when #Channels is 64 and 1024, MSAF has 10.4K and 2.6M FLOPs, whereas MMTM has 131.6K and 33.6M FLOPs respectively. To further understand the MSAF module and its effectiveness, we analyze the attention signals produced on the RAVDESS dataset. We first compare the attention signals averaged per emotion. Figure  6  shows the attention signals from the second MSAF module and sum the attention values for the blocks of the same modality. The video modality has higher attention weights when summed together since it has more blocks and is the stronger modality. However, we observe that for some emotions such as happy, a number of channels in the audio modality have similar weights as the video modality. This shows that the MSAF module is able to optimize how the modalities are used together depending on the emotion.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we present a lightweight multimodal fusion module, MSAF, that learns to exploit the complementary relationships between the modalities and highlight features for optimal multimodal learning. MSAF enables easy deployment of high-performance multimodal models due to its compatibility with CNNs and RNNs. We implement three multimodal networks with MSAF for emotion recognition, sentiment analysis, and action recognition. Our experiments demonstrate the module's ability to coordinate various types of modalities through competitive evaluation results in all three tasks.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Breakdown of the MSAF module with steps, split, join",
      "page": 3
    },
    {
      "caption": "Figure 1: We explicate the three",
      "page": 3
    },
    {
      "caption": "Figure 2: Enhancing the MSAF module for RNNs. For easy visu-",
      "page": 4
    },
    {
      "caption": "Figure 2: 4. Applications",
      "page": 4
    },
    {
      "caption": "Figure 3: Proposed architecture for emotion recognition",
      "page": 4
    },
    {
      "caption": "Figure 4: Proposed architecture for action recognition. “Inc.” de-",
      "page": 5
    },
    {
      "caption": "Figure 5: , we compare the number of param-",
      "page": 8
    },
    {
      "caption": "Figure 5: Number of parameters comparison between an MSAF",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the attention signals",
      "page": 9
    },
    {
      "caption": "Figure 6: Visualization of attention values from the second MSAF",
      "page": 9
    },
    {
      "caption": "Figure 7: Comparison between attention values of 2 MSAF mod-",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 University of Waterloo, 2 Shenzhen University": "cq2hu,\n{l26su,\ndongpu}@uwaterloo.ca,"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "Abstract"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "Multimodal learning mimics the reasoning process of the"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "human multi-sensory system, which is used to perceive the"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "surrounding world. While making a prediction,\nthe human"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "brain tends to relate crucial cues from multiple sources of"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "information.\nIn this work, we propose a novel\nlightweight"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "multimodal\nfusion module that\nlearns\nto emphasize more"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "contributive features across all modalities. Speciﬁcally, the"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "proposed Multimodal Split Attention Fusion (MSAF) mod-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "ule\nsplits\neach modality\ninto channel-wise\nequal\nfeature"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "blocks and creates a joint\nrepresentation that\nis used to"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "generate soft attention for each channel across the feature"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "blocks. Further,\nthe MSAF module is designed to be com-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "patible with features of various spatial dimensions and se-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "quence lengths, suitable for both CNNs and RNNs.\nThus,"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "MSAF can be\neasily added to fuse\nfeatures of any uni-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "modal networks and utilize existing pretrained unimodal"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "model weights.\nTo demonstrate\nthe\neffectiveness of our"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "fusion module, we design three multimodal networks with"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "MSAF for emotion recognition, sentiment analysis, and ac-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "tion recognition tasks.\nOur approach achieves competi-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "tive results in each task and outperforms other application-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "speciﬁc networks and multimodal fusion benchmarks."
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "1. Introduction"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "Multimodal\nlearning\nhas\nbeen\nexplored\nin\nnumerous"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "machine learning applications such as audio-visual speech"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "recognition [34], action recognition [2], and video question"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "answering [22], where each modality contains useful infor-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "mation from a different perspective. Although these tasks"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "can beneﬁt from the complementary relationship in multi-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "modal data, different modalities are represented in diverse"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "fashions, making it challenging to grasp their complex cor-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "relations."
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "Studies in multimodal machine learning are mainly cat-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "egorized into three fusion strategies:\nearly fusion,\ninter-"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "mediate\nfusion,\nand late\nfusion.\nEarly fusion explicitly"
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": ""
        },
        {
          "1 University of Waterloo, 2 Shenzhen University": "*Equal Contribution."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "by generating attention values. Subsequently, the enhanced",
          "vectors are very computationally expensive for subsequent": "analysis. Compact bilinear pooling [13] signiﬁcantly mit-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "feature blocks are rejoined for each modality,\nresulting in",
          "vectors are very computationally expensive for subsequent": "igates the curse of dimensionality problem [17]\nthrough a"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "an optimized feature space with an understanding of\nthe",
          "vectors are very computationally expensive for subsequent": "novel kernelized analysis while keeping the same discrimi-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "multimodal context. Our MSAF module is compatible with",
          "vectors are very computationally expensive for subsequent": "native power as the full bilinear representation."
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "features of any shape as it operates only on the channel di-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "mension. Thus, MSAF can be added between layers of any",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "Late Fusion.\nLate fusion merges the decision values from"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "CNN or RNN architecture. Furthermore, we boost perfor-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "each unimodal model\ninto an uniﬁed decision using fu-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "mance on sequential features by splitting modalities time-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "sion mechanisms such as averaging [41], voting [33] and"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "wise and applying an MSAF module to each time segment.",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "weighted sum [46].\nIn contrast\nto early fusion,\nlate fusion"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "This allows emphasis of different\nfeature blocks\nin each",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "embraces\nthe end-to-end learning between each modality"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "time segment. To our knowledge, our work is the ﬁrst inde-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "and the given task.\nIt allows for more ﬂexibility as it can"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "pendent fusion module that can be used in both CNN- and",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "still train or make predictions when one or more modalities"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "RNN-based multimodal learning applications.",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "are missing. Nevertheless, late fusion lacks the exploration"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "We comprehensively evaluate the effectiveness of MSAF",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "of lower-level correlations between the modalities. There-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "in\nthree multimodal\nlearning\ntasks,\nnamely\naudiovisual",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "fore, when it comes to a disagreement between modalities,"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "emotion recognition, sentiment analysis, and action recog-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "a simple mechanism acting only on decisions might be too"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "nition. We design a neural network with integrated MSAF",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "simpliﬁed.\nThere are also more complex late fusion ap-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "modules\nfor each task to demonstrate the ease of apply-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "proaches that exploit modality-wise synergies.\nFor exam-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "ing MSAF to existing unimodal conﬁgurations.\nOur ex-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "ple,\n[26] proposes a multiplicative combination layer\nthat"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "periments show that MSAF-powered networks outperform",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "promotes the training of strong modalities per sample and"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "other fusion methods and state-of-the-art models designed",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "tolerates mistakes made by other modalities."
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "for each application. Empirically, we observe that MSAF",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "achieves better results while using a similar number of pa-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "rameters as simple late fusion methods. Our module learns",
          "vectors are very computationally expensive for subsequent": "Intermediate Fusion.\nIntermediate fusion exploits\nfea-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "to pinpoint\nthe important features regardless of the modal-",
          "vectors are very computationally expensive for subsequent": "ture correlations after some level of processing,\ntherefore"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "ity’s complexity.",
          "vectors are very computationally expensive for subsequent": "the fusion takes place in the middle between the feature ex-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "In summary, our work provides the following contribu-",
          "vectors are very computationally expensive for subsequent": "tractor and the decision layer.\nFor\ninstance,\n[48] applies"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "tions: 1) MSAF – A novel\nlightweight multimodal\nfusion",
          "vectors are very computationally expensive for subsequent": "principle component analysis on the extracted features for"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "module for CNN and RNN networks that effectively fuses",
          "vectors are very computationally expensive for subsequent": "each modality, and further processes them respectively be-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "intermediate and high-level modality features\nto leverage",
          "vectors are very computationally expensive for subsequent": "fore feature concatenation. Recent works continue to im-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "the advantages of each modality. 2) We design three multi-",
          "vectors are very computationally expensive for subsequent": "prove modality feature alignment to give stronger joint fea-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "modal fusion networks corresponding to three applications:",
          "vectors are very computationally expensive for subsequent": "tures. CentralNet [45] coordinates features of each modal-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "emotion recognition, sentiment analysis, and action recog-",
          "vectors are very computationally expensive for subsequent": "ity by performing a weighted sum of modalities in a central"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "nition.\nOur\nexperiments demonstrate\nthe\ncapabilities of",
          "vectors are very computationally expensive for subsequent": "branch at different\nlevels of the network. EmbraceNet [7]"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "MSAF through competitive results in all applications while",
          "vectors are very computationally expensive for subsequent": "prevents dependency on data of speciﬁc modalities and in-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "utilizing fewer parameters.",
          "vectors are very computationally expensive for subsequent": "creases robustness to missing data through learning cross-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "modal\ncorrelations by combining selected features\nfrom"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "each modality using a multinomial distribution.\n[20] uti-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "2. Related Work",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "lizes the squeeze and excitation module from SENet [18] to"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "Early Fusion.\nThe majority of works in early fusion in-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "enable slow modality fusion by channel-wise feature recali-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "tegrate features immediately after\nthey are extracted from",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "bration at different stages of the network. Our work aims to"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "each modality, whereas occasionally studies perform fusion",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "effectively fuse features of modalities while maintaining ef-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "at\nthe input\nlevel, such as [32]. A simple solution for early",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "ﬁciency. We adopt the split attention [53] concept for multi-"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "fusion is feature concatenation after they are transformed to",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "modal fusion where modalities are broken down into feature"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "the same length, followed by fully connected layers. Many",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "map groups with hidden complement relationships."
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "early fusion works use CCA to exploit cross-modality cor-",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "relations.\n[39] applies CCA to improve the performance in",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "",
          "vectors are very computationally expensive for subsequent": "3. Proposed Method"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "speaker identiﬁcation using visual and audio modalities. [1]",
          "vectors are very computationally expensive for subsequent": ""
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "proposes deep CCA to learn complex nonlinear transforma-",
          "vectors are very computationally expensive for subsequent": "We ﬁrst\nformulate\nthe multimodal\nfusion problem in"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "tions between modalities, which inspired multimodal appli-",
          "vectors are very computationally expensive for subsequent": "an MSAF module.\nLet M be the number of modalities"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "cations such as [28]. Bilinear pooling is another early fu-",
          "vectors are very computationally expensive for subsequent": "and the feature map of modality m ∈ {1, 2, · · ·\n, M } be"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "sion method that fuses modalities by calculating their outer",
          "vectors are very computationally expensive for subsequent": "Fm ∈ IRN1×N2×···×NK ×Cm. Here, K is the number of"
        },
        {
          "tor\nthen learns to emphasize the important\nfeature blocks": "product. However,\nthe generated high dimensional feature",
          "vectors are very computationally expensive for subsequent": "spatial dimensions of modality m and Cm is the number of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "we calculate the element-wise sum of the per modality de-": "scriptors {D1, · · ·\n, DM } to form a multimodal channel de-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "scriptor G. We capture the channel-wise dependencies by"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "a fully connected layer with a reduction factor r followed"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "by a batch normalization layer and a ReLU activation func-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "tion. The transformation maps G to the joint representation"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "Z ∈ IRC(cid:48)\n, C (cid:48) = (cid:98)C/r(cid:99) which helps with generalization for"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "complex models."
        },
        {
          "we calculate the element-wise sum of the per modality de-": "(2)\nZ = WZG + bZ"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ". As advised in [20] and\nwhere WZ ∈ IRC(cid:48)×C, bZ ∈ IRC(cid:48)"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "evident\nin our experiments, a reduction factor of 4 is ideal"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "for two modalities. As the number of modalities increase,"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "we recommend reducing the reduction factor to accommo-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "date mores features in the joint representation. For example,"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "when fusing 3 modalities, a reduction factor of 2 was opti-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "mal in our results for sentiment analysis."
        },
        {
          "we calculate the element-wise sum of the per modality de-": "Highlight.\nThe multimodal\nchannel descriptor\ncontains"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "generalized but\nrich knowledge of\nthe global context.\nIn"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "this step,\nfor a block Bi\nm, we generate the corresponding"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "logits U i\nm by applying a linear transformation on Z and ob-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "tain the block-wise attention weights Ai"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "m using the softmax"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "activation."
        },
        {
          "we calculate the element-wise sum of the per modality de-": "U i"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "(3)\nm = W i\nmZ + bi\nm"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "exp(U i\nm)"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "Ai"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "(4)\nm ="
        },
        {
          "we calculate the element-wise sum of the per modality de-": "(cid:80)M\n(cid:80)|Bk|"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "exp(U j"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "k\nj\nk )"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "and bi\nwhere W i\nm ∈ IRC are weights and bias\nm ∈ IRC×C(cid:48)"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "of\nthe corresponding fully connected layer.\nSince soft at-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "tention values are dependent on the total number of feature"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "blocks, features may be over-suppressed. The effect is more"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "apparent\nin complex tasks which results in insufﬁcient\nin-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "formation for accurate predictions. Thus, we present a hy-"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "perparameter λ ∈ [0, 1] that controls the suppression power"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "of MSAF. Intuitively, λ can be understood as a regularizer"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "for the lowest attention of a split. We obtain an optimized"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "ˆBi\nfeature block\nm using attention signals Ai\nm and λ:"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "ˆBi\n(5)\nm = [λ + (1 − λ) × Ai\nm] (cid:12) Bi\nm"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "Finally,\nthe\nfeature blocks belonging to modality m are"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "merged by channel-wise concatenation to produce\nˆFm."
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "|Bm|"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "ˆB1\nˆB2\n,\n]\n(6)\nˆBm\nˆFm = [\nm,\nm, · · ·"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "3.1. BlockDropout"
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": ""
        },
        {
          "we calculate the element-wise sum of the per modality de-": "To lessen the dependencies on certain strong feature"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "blocks and ease overﬁtting, we propose a dropout method"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "for the feature blocks called BlockDropout. BlockDropout"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "generates a binary mask that randomly drops feature blocks"
        },
        {
          "we calculate the element-wise sum of the per modality de-": "from the set of all\nfeature blocks from each modality B,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4. Applications": "In this section, we apply the MSAF module to fuse uni-"
        },
        {
          "4. Applications": "modal networks\nin three applications. We describe each"
        },
        {
          "4. Applications": "unimodal network and our\nconﬁguration for\nthe MSAF"
        },
        {
          "4. Applications": "modules."
        },
        {
          "4. Applications": "4.1. Emotion Recognition"
        },
        {
          "4. Applications": "Multimodal emotion recognition (MER)\nis a classiﬁca-"
        },
        {
          "4. Applications": "tion task that categorizes human emotions using multiple"
        },
        {
          "4. Applications": "interacting signals. Although numerous works have utilized"
        },
        {
          "4. Applications": "more complex modalities such as EEG [54] and body ges-"
        },
        {
          "4. Applications": "ture [9], video and audio remain as dominant modalities"
        },
        {
          "4. Applications": "used for this task. Thus, we design a multimodal network"
        },
        {
          "4. Applications": "that\nfuses a 3D CNN for video and a 1D CNN for audio"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "using MSAF. Video data has dependencies in both spatial"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "and temporal dimensions, therefore requires a network with"
        },
        {
          "4. Applications": "3D kernels to learn both the facial expression and its move-"
        },
        {
          "4. Applications": "ment. Considering both network performance and training"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "efﬁciency, we choose the 3D ResNeXt50 network [49] as"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "suggested by [15] with cardinality set\nto 32.\nFor\nthe au-"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "dio modality, recent works [35, 47] have demonstrated the"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "effectiveness of deep learning based methods built on Mel-"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "frequency cepstral coefﬁcients (MFCC)\nfeatures. We de-"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "sign a simple 1D CNN for the MFCC features and fuse the"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "two modalities via two MSAF modules as shown in Fig-"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "ure 3. We conﬁgured the two MSAF modules with 16 and"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "32 channels per block respectively and BlockDropout with"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "p = 0.2. Finally, we sum the logits of both networks fol-"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "lowed by a softmax function."
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": "Figure 3. Proposed architecture for emotion recognition"
        },
        {
          "4. Applications": ""
        },
        {
          "4. Applications": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.2. Sentiment Analysis": "Sentiment analysis is the use of Natural Language Pro-"
        },
        {
          "4.2. Sentiment Analysis": "cessing (NLP)\nto interpret and classify people’s opinions"
        },
        {
          "4.2. Sentiment Analysis": "from text.\nIn recent years,\nthe multimodal nature of hu-"
        },
        {
          "4.2. Sentiment Analysis": "man language has led to the incorporation of other modal-"
        },
        {
          "4.2. Sentiment Analysis": "ities such as visual and audio data in NLP tasks.\nSimilar"
        },
        {
          "4.2. Sentiment Analysis": "to works such as [42, 16], we apply our module on audio,"
        },
        {
          "4.2. Sentiment Analysis": "visual and text modalities. Our architecture uses two LSTM"
        },
        {
          "4.2. Sentiment Analysis": "layers for each modality with an MSAF module in between"
        },
        {
          "4.2. Sentiment Analysis": "to fuse features from the ﬁrst LSTM layer. We use 32, 64"
        },
        {
          "4.2. Sentiment Analysis": "and 128 hidden units for both LSTM layers in the visual,"
        },
        {
          "4.2. Sentiment Analysis": "audio and text modality respectively. For the MSAF mod-"
        },
        {
          "4.2. Sentiment Analysis": "ule, each modality is split\ninto 5 segments sequence-wise"
        },
        {
          "4.2. Sentiment Analysis": "(q = 5) and each segment\nis passed into a separate MSAF"
        },
        {
          "4.2. Sentiment Analysis": "block with 16 channels per feature block and BlockDropout"
        },
        {
          "4.2. Sentiment Analysis": "with p = 0.2. Lastly, we concatenate the ﬁnal output from"
        },
        {
          "4.2. Sentiment Analysis": "each LSTM and pass that to a fully connected layer to gen-"
        },
        {
          "4.2. Sentiment Analysis": "erate the sentiment value."
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "4.3. Action Recognition"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "With the development of depth cameras, depth and skele-"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "ton data became crucial modalities in the action recogni-"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "tion task along with RGB videos. Multiple works such as"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "[20, 24, 27] have achieved competitive performance using"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "RGB videos associated with skeleton sequences. We fol-"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "low [20] which utilizes I3D [6] for the video data, and HCN"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "[23] for the skeleton stream. As illustrated in Figure 4, we"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "deploy two MSAF modules, one at an intermediate-level in"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "both networks, and the other one for high-level feature re-"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "calibration. The HCN framework proposes two strategies to"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "be scalable to multi-person scenarios. The ﬁrst\ntype stacks"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "the joints from all persons and feeds it as the input of\nthe"
        },
        {
          "4.2. Sentiment Analysis": "network in an early fusion style. The second type adapts late"
        },
        {
          "4.2. Sentiment Analysis": ""
        },
        {
          "4.2. Sentiment Analysis": "fusion that passes the inputs of multiple persons through the"
        },
        {
          "4.2. Sentiment Analysis": "same subnetwork, whose Conv6 channel-wise concatenates"
        },
        {
          "4.2. Sentiment Analysis": "or element-wise maximizes the group of features of persons."
        },
        {
          "4.2. Sentiment Analysis": "The latter generalizes better to various numbers of persons"
        },
        {
          "4.2. Sentiment Analysis": "than the other, which needs a predeﬁned maximum number"
        },
        {
          "4.2. Sentiment Analysis": "of persons.\n[20] follows the multi-person late fusion strat-"
        },
        {
          "4.2. Sentiment Analysis": "egy and utilizes their ﬁrst fusion module on one of the two"
        },
        {
          "4.2. Sentiment Analysis": "persons universally. We take a different approach by con-"
        },
        {
          "4.2. Sentiment Analysis": "sidering all available individuals in a sample because either"
        },
        {
          "4.2. Sentiment Analysis": "can send important signals during a multi-person interac-"
        },
        {
          "4.2. Sentiment Analysis": "tion. Our ﬁrst MSAF module has 64 channels per block and"
        },
        {
          "4.2. Sentiment Analysis": "is inserted between the second last\nInception layer\nin I3D"
        },
        {
          "4.2. Sentiment Analysis": "and the Conv5 outputs of each person. The second MSAF"
        },
        {
          "4.2. Sentiment Analysis": "has 256 channels per block and is positioned between the"
        },
        {
          "4.2. Sentiment Analysis": "last\nInception layer\nin I3D and the FC7 layer\nin HCN. A"
        },
        {
          "4.2. Sentiment Analysis": "suppression power of λ = 0.5 is used for both modules. Fi-"
        },
        {
          "4.2. Sentiment Analysis": "nally, we average the logits of both networks followed by a"
        },
        {
          "4.2. Sentiment Analysis": "softmax function."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: presents the accuracy of the proposed",
      "data": [
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "Concat + FC\nEarly\n71.04\n26.87 M"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "MCBP\nEarly\n71.32\n51.03 M"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "MMTM\nInter.\n73.12\n31.97 M"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "74.86\n25.94 M\nMSAF\nInter."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "Table 1. Comparison between multimodal fusion benchmarks and"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "ours for emotion recognition on RAVDESS."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "test set contain 16322, 1871, and 4659 samples respectively."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "Following recent works, we evaluate: 1) mean absolute er-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "ror (MAE) and Pearson correlation (Corr) for regression, 2)"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "binary accuracy (Acc-2) and F-score, 3) 7 class accuracy"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "(Acc-7) from -3 to 3. For binary classiﬁcation, we consider"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "[-3, 0) labels as negative and (0, 3] as positive."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "In order to draw comparisons with recent works [42, 16]."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "COVAREP [10], FACET 1 and BERT [11]\nfeatures were"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "selected for audio, visual and text modalities respectively."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "We acquired aligned COVAREP, FACET features and raw"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "text\nfrom the public CMU-MultimodalSDK v1.2.0 2 with"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "a sequence length of 50 words for all modalities. The raw"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "text of\nindividual utterances was passed into a pretrained"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "uncased BERT model\n(not ﬁne-tuned on CMU-MOSEI)"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "and the ﬁnal encoder\nlayer output was used for\ntext\nfea-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "tures. Each utterance was passed in separate epochs to avoid"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "adding additional context."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "Training.\nSince the network is relatively small,\nthe uni-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "modal models were not pretrained.\nThe multimodal net-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "work was trained using mean squared error (MSE) loss and"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "the Adam optimizer was used with a constant\nlearning rate"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "of 10−3. For Acc-7, we rounded the output\nto the nearest"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "integer and clipped to -3 and 3."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "Benchmarks.\nWe summarize various multimodal\nfusion"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "methods proposed for\nsentiment\nanalysis\nas\nfollows:\n1)"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "DCCA [43] and ICCN [42] use Deep CCA to correlate text"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "with audio-visual features, 2) TFN [51] and LMF [30] per-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "form outer-products on modalities to create a joint represen-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "tation, 3) MFM [44] and MISA [16] separates features into"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "modality-speciﬁc and modality-invariant. While MFM op-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "timizes a joint generative-discriminative objective allowing"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "for classiﬁcation and reconstruction of missing modalities,"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "MISA trains modality-speciﬁc\nand multimodal\nencoders"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "which are then fused using multi-headed self-attention be-"
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": "fore prediction."
        },
        {
          "Multiplication\nLate\n70.56\n25.92 M": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: presents the accuracy of the proposed",
      "data": [
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "3D ResNeXt50 (Vis.)\n-\n62.99\n25.88 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "1D CNN (Aud.)\n-\n56.53\n0.03 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Averaging\nLate\n68.82\n25.92 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Multiplicative β=0.3\nLate\n70.35\n25.92 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Multiplication\nLate\n70.56\n25.92 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Concat + FC\nEarly\n71.04\n26.87 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "MCBP\nEarly\n71.32\n51.03 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "MMTM\nInter.\n73.12\n31.97 M"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "74.86\n25.94 M\nMSAF\nInter."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Table 1. Comparison between multimodal fusion benchmarks and"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "ours for emotion recognition on RAVDESS."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "test set contain 16322, 1871, and 4659 samples respectively."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Following recent works, we evaluate: 1) mean absolute er-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "ror (MAE) and Pearson correlation (Corr) for regression, 2)"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "binary accuracy (Acc-2) and F-score, 3) 7 class accuracy"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "(Acc-7) from -3 to 3. For binary classiﬁcation, we consider"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "[-3, 0) labels as negative and (0, 3] as positive."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "In order to draw comparisons with recent works [42, 16]."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "COVAREP [10], FACET 1 and BERT [11]\nfeatures were"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "selected for audio, visual and text modalities respectively."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "We acquired aligned COVAREP, FACET features and raw"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "text\nfrom the public CMU-MultimodalSDK v1.2.0 2 with"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "a sequence length of 50 words for all modalities. The raw"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "text of\nindividual utterances was passed into a pretrained"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "uncased BERT model\n(not ﬁne-tuned on CMU-MOSEI)"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "and the ﬁnal encoder\nlayer output was used for\ntext\nfea-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "tures. Each utterance was passed in separate epochs to avoid"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "adding additional context."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Training.\nSince the network is relatively small,\nthe uni-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "modal models were not pretrained.\nThe multimodal net-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "work was trained using mean squared error (MSE) loss and"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "the Adam optimizer was used with a constant\nlearning rate"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "of 10−3. For Acc-7, we rounded the output\nto the nearest"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "integer and clipped to -3 and 3."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "Benchmarks.\nWe summarize various multimodal\nfusion"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "methods proposed for\nsentiment\nanalysis\nas\nfollows:\n1)"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "DCCA [43] and ICCN [42] use Deep CCA to correlate text"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "with audio-visual features, 2) TFN [51] and LMF [30] per-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "form outer-products on modalities to create a joint represen-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "tation, 3) MFM [44] and MISA [16] separates features into"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "modality-speciﬁc and modality-invariant. While MFM op-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "timizes a joint generative-discriminative objective allowing"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "for classiﬁcation and reconstruction of missing modalities,"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "MISA trains modality-speciﬁc\nand multimodal\nencoders"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "which are then fused using multi-headed self-attention be-"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "fore prediction."
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": ""
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "1https://imotions.com/platform/"
        },
        {
          "Model\nFusion Stage\nAccuracy\n#Params": "2https://github.com/A2Zadeh/CMU-MultimodalSDK"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: shows the results of our experiments in",
      "data": [
        {
          "Model": "Inf. ResNet50 (RGB)",
          "Acc-2": "65.9",
          "F-score": "64.7",
          "MAE": "0.829",
          "Acc-7": "41.3",
          "Corr": "0.248",
          "RGB Model": "-",
          "Acc. (CS)": "83.91"
        },
        {
          "Model": "I3D (RGB)",
          "Acc-2": "65.7",
          "F-score": "62.7",
          "MAE": "0.821",
          "Acc-7": "41.2",
          "Corr": "0.263",
          "RGB Model": "-",
          "Acc. (CS)": "85.63"
        },
        {
          "Model": "HCN (Skeleton)",
          "Acc-2": "83.5",
          "F-score": "83.5",
          "MAE": "0.572",
          "Acc-7": "51.0",
          "Corr": "0.727",
          "RGB Model": "-",
          "Acc. (CS)": "85.24"
        },
        {
          "Model": "SGM-Net*",
          "Acc-2": "83.6",
          "F-score": "83.8",
          "MAE": "0.579",
          "Acc-7": "50.1",
          "Corr": "0.707",
          "RGB Model": "-",
          "Acc. (CS)": "89.10"
        },
        {
          "Model": "CentralNet(cid:5)",
          "Acc-2": "82.6",
          "F-score": "82.1",
          "MAE": "0.593",
          "Acc-7": "50.2",
          "Corr": "0.700",
          "RGB Model": "Inf. ResNet50",
          "Acc. (CS)": "89.36"
        },
        {
          "Model": "MFAS*",
          "Acc-2": "82.0",
          "F-score": "82.2",
          "MAE": "0.623",
          "Acc-7": "48.0",
          "Corr": "0.677",
          "RGB Model": "Inf. ResNet50",
          "Acc. (CS)": "90.04"
        },
        {
          "Model": "MMTM*",
          "Acc-2": "84.4",
          "F-score": "84.4",
          "MAE": "0.568",
          "Acc-7": "51.4",
          "Corr": "0.717",
          "RGB Model": "Inf. ResNet50",
          "Acc. (CS)": "90.11"
        },
        {
          "Model": "PoseMap*",
          "Acc-2": "84.2",
          "F-score": "84.2",
          "MAE": "0.565",
          "Acc-7": "51.6",
          "Corr": "0.713",
          "RGB Model": "-",
          "Acc. (CS)": "91.71"
        },
        {
          "Model": "MMTM*",
          "Acc-2": "85.5",
          "F-score": "85.3",
          "MAE": "0.555",
          "Acc-7": "52.2",
          "Corr": "0.756",
          "RGB Model": "I3D",
          "Acc. (CS)": "91.99"
        },
        {
          "Model": "MSAF",
          "Acc-2": "85.5",
          "F-score": "85.5",
          "MAE": "0.559",
          "Acc-7": "52.4",
          "Corr": "0.738",
          "RGB Model": "Inf. ResNet50",
          "Acc. (CS)": "90.63"
        },
        {
          "Model": "",
          "Acc-2": "",
          "F-score": "",
          "MAE": "",
          "Acc-7": "",
          "Corr": "",
          "RGB Model": "",
          "Acc. (CS)": ""
        },
        {
          "Model": "MSAF",
          "Acc-2": "0.3",
          "F-score": "0.3",
          "MAE": "0.003",
          "Acc-7": "0.1",
          "Corr": "0.002",
          "RGB Model": "I3D",
          "Acc. (CS)": "92.24"
        },
        {
          "Model": "",
          "Acc-2": "",
          "F-score": "",
          "MAE": "",
          "Acc-7": "",
          "Corr": "",
          "RGB Model": "Table 3. Comparison between multimodal fusion benchmarks and",
          "Acc. (CS)": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: shows the results of our experiments in",
      "data": [
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "85.5\n0.555\n0.756\nMISA*\n85.3\n52.2",
          "PoseMap*\n-\n91.71": "MMTM*\nI3D\n91.99"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "85.5\n85.5\n52.4\n0.559\n0.738",
          "PoseMap*\n-\n91.71": "90.63\nMSAF\nInf. ResNet50"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "MSAF",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "0.3\n0.3\n0.003\n0.1\n0.002",
          "PoseMap*\n-\n91.71": "92.24\nMSAF\nI3D"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Table 2. Comparison between multimodal fusion benchmarks and",
          "PoseMap*\n-\n91.71": "Table 3. Comparison between multimodal fusion benchmarks and"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "ours for sentiment analysis on CMU-MOSEI. * from original pa-",
          "PoseMap*\n-\n91.71": "ours on the NTU RGB+D Cross-Subject protocol. * from original"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "pers and (cid:5) from [42]. The standard error over 5 runs are in italics.",
          "PoseMap*\n-\n91.71": "papers and (cid:5) from [20]. The standard error for Inﬂated ResNet50"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "and I3D over 5 runs is 0.04 and 0.03 respectively."
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Results.\nTable 2 shows the results of our experiments in",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "sum of skeleton and RGB features at various locations, 3)"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "comparison with the state-of-the-art and recent works us-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "MFAS [38] is a generic search algorithm that ﬁnds an opti-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "ing BERT. Our multimodal model outperforms all unimodal",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "mal architecture for a given dataset, 4) PoseMap [27] uses"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "models conﬁrming that audio-visual features improve sen-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "CNNs to process pose estimation maps and skeletons inde-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "timent analysis. MSAF achieves better or similar perfor-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "pendently with late fusion for ﬁnal prediction, 5) MMTM"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "mance on all metrics compared to state-of-the-art multi-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "[20] recalibrates features at different stages achieving state-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "modal methods while using a simpler network architecture.",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "of-the-art in RGB and skeleton fusion."
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Compared to MISA and others, our training method is sim-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "pler, allowing MSAF to be easily applied to existing uni-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "modal networks.",
          "PoseMap*\n-\n91.71": "Results.\nTable 3 reports\nthe\naccuracy of\nthe proposed"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "MSAF network in comparison with other action recognition"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "5.3. Action Recognition",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "models using RGB videos and skeletons. To compare with"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "state-of-the-art intermediate fusion methods, we also evalu-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Data Preparation.\nNTU RGB+D [40] is a large-scale hu-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "ate the performance of MSAF applied to Inﬂated ResNet50"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "man action recognition dataset. It contains 60 action classes",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "[5] and HCN. Our model outperforms all\nintermediate fu-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "and 56,880 video samples associated with 3D skeleton data.",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "sion methods\nand application-speciﬁc models,\nachieving"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Cross-Subject\n(CS) and Cross-View (CV) are two recom-",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "state-of-the-art performance in RGB+pose action recogni-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "mended protocols. CS splits the training set and testing set",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "tion in the NTU RGB+D CS protocol."
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "by the subject IDs, whereas CV splits the samples based on",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "different camera views. Recent methods [50, 27, 8] have",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "5.4. Ablation Study"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "achieved decent CV accuracies; however, CS still remains a",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "more challenging evaluation method based on the reported",
          "PoseMap*\n-\n91.71": "To obtain the conﬁgurations used for each application,"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "performance compared to the CV counterpart. We adopt the",
          "PoseMap*\n-\n91.71": "we conduct the ablation study on all three datasets with the"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "CS evaluation and split\nthe 40 subjects based on the speci-",
          "PoseMap*\n-\n91.71": "following hyperparameters:\nthe number of channels\nin a"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "ﬁed rule. For data preprocessing, video frames are extracted",
          "PoseMap*\n-\n91.71": "block C, attention regularizer λ (default value is 0), Block-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "at 32 FPS and we adopt\nthe same data augmentation ap-",
          "PoseMap*\n-\n91.71": "Dropout\n(with p = 0.2), and the number of\nsegments q"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "proach as [20].",
          "PoseMap*\n-\n91.71": "for RNNs (default value is 1). Table 4 reports the accuracy"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "of the conﬁgurations building up to the best conﬁguration."
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "We observe the optimal number of channels in a block, C,"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Training.\nThe Adam optimizer with a base learning rate",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "for each dataset can be derived from min {C1, · · ·\n, CM }/2"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "of 10−3 and a weight decay of 10−4 is used. The learning",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "which serves as a good starting point when tuning C for"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "rate is reduced to 10−4 at epoch 5, where the loss is near",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "other applications. Hyperparameter λ plays an important"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "saturation in our experiment.",
          "PoseMap*\n-\n91.71": ""
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "role in NTU by avoiding over-suppression of\nfeatures for"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "",
          "PoseMap*\n-\n91.71": "more\ncomplex\ntasks.\nBlockDropout\nis\nessential\nto\nthe"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "Benchmarks.\nWe\nsummarize\nthe multimodal\nfusion",
          "PoseMap*\n-\n91.71": "performance in RAVDESS and MOSEI but not NTU as"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "benchmarks\nfor action recognition based on RGB videos",
          "PoseMap*\n-\n91.71": "dropout\ntends to be more effective on smaller datasets to"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "and skeletons\nas\nfollows:\n1) SGM-Net\n[24] proposed a",
          "PoseMap*\n-\n91.71": "prevent overﬁtting. The importance of q is shown in MO-"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "skeleton guidance block to enhance RGB features, 2) Cen-",
          "PoseMap*\n-\n91.71": "SEI, where a sequence length of 5 words was optimal for a"
        },
        {
          "ICCN*\n84.2\n84.2\n0.565\n51.6\n0.713": "tralNet [45] adds a central branch that\nlearns the weighted",
          "PoseMap*\n-\n91.71": "sequence length of 50 words. As q increases,\nthe number"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Ablation study of the placement of MSAF modules in",
      "data": [
        {
          "32": "16",
          "51.1": "52.3",
          "early, intermediate and late feature levels on NTU RGB+D.": ""
        },
        {
          "32": "16",
          "51.1": "52.4",
          "early, intermediate and late feature levels on NTU RGB+D.": ""
        },
        {
          "32": "16",
          "51.1": "52.2",
          "early, intermediate and late feature levels on NTU RGB+D.": "In summary, we ﬁnd that multimodal fusion using MSAF is"
        },
        {
          "32": "32, 128",
          "51.1": "91.04",
          "early, intermediate and late feature levels on NTU RGB+D.": "the most effective when applied to a combination of inter-"
        },
        {
          "32": "64, 256",
          "51.1": "91.56",
          "early, intermediate and late feature levels on NTU RGB+D.": "mediate and high-level features."
        },
        {
          "32": "126, 512",
          "51.1": "91.05",
          "early, intermediate and late feature levels on NTU RGB+D.": ""
        },
        {
          "32": "64, 256",
          "51.1": "92.00",
          "early, intermediate and late feature levels on NTU RGB+D.": "5.5. Parameter and Attention Analysis"
        },
        {
          "32": "64, 256",
          "51.1": "92.24",
          "early, intermediate and late feature levels on NTU RGB+D.": ""
        },
        {
          "32": "",
          "51.1": "",
          "early, intermediate and late feature levels on NTU RGB+D.": "Reﬂecting on our objective to design an effective fu-"
        },
        {
          "32": "64, 256",
          "51.1": "92.12",
          "early, intermediate and late feature levels on NTU RGB+D.": ""
        },
        {
          "32": "",
          "51.1": "",
          "early, intermediate and late feature levels on NTU RGB+D.": "sion module that"
        },
        {
          "32": "Table 4. Ablation study of MSAF module hyperparameters.",
          "51.1": "For",
          "early, intermediate and late feature levels on NTU RGB+D.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: Ablation study of the placement of MSAF modules in",
      "data": [
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "8, 16\n71.01",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n91.93"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "72.99\n16, 32",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n92.08"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "73.40\n32, 64",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n92.11"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "RAVDESS",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)\n32, 64\n72.29",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n(cid:51)\n91.81"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)\n74.86\n16, 32",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n(cid:51)\n91.88"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)\n16, 32\n0.25\n74.37",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n(cid:51)\n92.24"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "(cid:51)\n(cid:51)\n(cid:51)\n91.88"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "8\n51.6",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "51.7\n16",
          "Early\nIntermediate\nLate\nAcc. (CS)": "Table 5. Ablation study of\nthe placement of MSAF modules in"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "32\n51.1",
          "Early\nIntermediate\nLate\nAcc. (CS)": "early, intermediate and late feature levels on NTU RGB+D."
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "MOSEI",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)\n52.3\n16",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)\n52.4\n16\n5",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)\n16\n10\n52.2",
          "Early\nIntermediate\nLate\nAcc. (CS)": "In summary, we ﬁnd that multimodal fusion using MSAF is"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "32, 128\n91.04",
          "Early\nIntermediate\nLate\nAcc. (CS)": "the most effective when applied to a combination of inter-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "91.56\n64, 256",
          "Early\nIntermediate\nLate\nAcc. (CS)": "mediate and high-level features."
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "126, 512\n91.05",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "NTU",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "64, 256\n0.25\n92.00",
          "Early\nIntermediate\nLate\nAcc. (CS)": "5.5. Parameter and Attention Analysis"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "92.24\n64, 256\n0.5",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "(cid:51)",
          "Early\nIntermediate\nLate\nAcc. (CS)": "Reﬂecting on our objective to design an effective fu-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "64, 256\n0.5\n92.12",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "sion module that\nis also lightweight, we analyze the num-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "Table 4. Ablation study of MSAF module hyperparameters.\nFor",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "CMU-MOSEI, Acc-7 is shown for Acc.",
          "Early\nIntermediate\nLate\nAcc. (CS)": "ber of parameters of\nthe MSAF module.\nIdeally,\nthe fu-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "sion module should introduce minimal parameters\nto the"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "unimodal networks combined despite the feature map size"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "of MSAF modules increases which increases the number of",
          "Early\nIntermediate\nLate\nAcc. (CS)": "of\nthe modalities.\nThe split and join steps in MSAF en-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "parameters and thus results in overﬁtting.",
          "Early\nIntermediate\nLate\nAcc. (CS)": "sure the joint feature space depends on the channel number"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "of\nthe feature blocks instead of\nthe channel of\nthe modal-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "The location of a MSAF module in a multimodal net-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "ities. Therefore,\nthe number of parameters is signiﬁcantly"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "work architecture is an important\nfactor\nfor effective fea-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "reduced.\nIn Figure 5, we compare the number of param-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "ture fusion. On one hand, placing a MSAF module at an",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "eters of MSAF to MMTM [20].\nFor both methods, we"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "earlier part of the network can help unimodal models learn",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "use two example modalities with shape (4, #Channels,"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "to correlate raw features of each other. On the other hand,",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "3, 128, 128) where #Channels is indicated on the x-axis."
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "using MSAF to fuse high-level features generates more ap-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "The reduction factor is set to 4 for both modules and we set"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "parent bias towards speciﬁc unimodal patterns as the high-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "C = min {C1, · · ·\n, CM }/2 for MSAF. As shown, MSAF"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "level features are more tailored to the task. To analyze the",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "utilizes parameters more efﬁciently, reaching a maximum of"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "effect of MSAF in different fusion locations on model per-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "330K parameters. In terms of computational cost, the num-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "formance, we deﬁne three positions to place MSAF in our",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "ber of FLOPs for MSAF has a similar trend as the number"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "action recognition network (I3D + HCN).\nIn the early lo-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "of parameters. For instance, when #Channels is 64 and"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "cation, a MSAF receives the concatenated Conv4 features",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "1024, MSAF has 10.4K and 2.6M FLOPs, whereas MMTM"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "from the two actors\nin HCN and the third last\nInception",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "",
          "Early\nIntermediate\nLate\nAcc. (CS)": "has 131.6K and 33.6M FLOPs respectively."
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "layer of\nI3D. The\nintermediate\nlocation is\nset\nto be be-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "tween the Conv5 layer of HCN and the second last Incep-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "tion layer of\nI3D. Finally,\nthe late location is at\nthe last",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "I3D Inception layer and the FC7 layer of HCN. We follow",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "C = min {C1, · · ·\n, CM }/2 while keeping other parameters",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "the same. We train the multimodal network with different",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "combinations of the above fusion locations and report our",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "results in Table 5.",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "We observe that the combination of intermediate and late",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "fusion achieves the best result among all seven experiments.",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "Interestingly, all experiments that involve early fusion yield",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "similar performance at around 91.9%.\nFurther, deploying",
          "Early\nIntermediate\nLate\nAcc. (CS)": "Figure 5. Number of parameters comparison between an MSAF"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "MSAF in all three locations does not achieve better perfor-",
          "Early\nIntermediate\nLate\nAcc. (CS)": "module and an MMTM [20] module. Each module receives two"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "mance than using only intermediate and late fusion. We be-",
          "Early\nIntermediate\nLate\nAcc. (CS)": "modalities with the same channel number indicated by the x-axis."
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "lieve this is because the low-level features at the early posi-",
          "Early\nIntermediate\nLate\nAcc. (CS)": ""
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "tion are still underdeveloped to show enough correlation for",
          "Early\nIntermediate\nLate\nAcc. (CS)": "To further understand the MSAF module and its effec-"
        },
        {
          "C\nλ\nq\nDataset\nBlockDropout\nAcc.": "effective fusion, which results in sub-optimal performance.",
          "Early\nIntermediate\nLate\nAcc. (CS)": "tiveness, we analyze the attention signals produced on the"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "averaged per emotion. Figure 6 shows the attention signals",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "from the second MSAF module and sum the attention val-",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": "module, MSAF,\nthat"
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "ues for the blocks of the same modality. The video modality",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "has higher attention weights when summed together since it",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": "for optimal multimodal"
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "has more blocks and is the stronger modality. However, we",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "observe that for some emotions such as happy, a number of",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "channels in the audio modality have similar weights as the",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "video modality. This shows that\nthe MSAF module is able",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "to optimize how the modalities are used together depending",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "on the emotion.",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": ""
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": "results in all three tasks."
        },
        {
          "RAVDESS dataset. We ﬁrst compare the attention signals": "",
          "6. Conclusion": "References"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] Galen Andrew,\nRaman Arora,\nJeff Bilmes,\nand Karen"
        },
        {
          "References": "Livescu. Deep canonical correlation analysis.\nIn Proceed-"
        },
        {
          "References": "ings of the 30th International Conference on Machine Learn-"
        },
        {
          "References": "ing (ICML), volume 28,\npages 1247–1255, Atlanta, GA,"
        },
        {
          "References": "2013. PMLR."
        },
        {
          "References": "[2] Moez\nBaccouche,\nFranck Mamalet,\nChristian Wolf,"
        },
        {
          "References": "Christophe Garcia,\nand Atilla Baskurt.\nSequential deep"
        },
        {
          "References": "learning for human action recognition.\nIn Human Behav-"
        },
        {
          "References": "ior Understanding, pages 29–39, Berlin, Heidelberg, 2011."
        },
        {
          "References": "Springer."
        },
        {
          "References": "[3] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik"
        },
        {
          "References": "Cambria,\nand Louis-Philippe Morency.\nMultimodal\nlan-"
        },
        {
          "References": "guage analysis in the wild: CMU-MOSEI dataset and inter-"
        },
        {
          "References": ""
        },
        {
          "References": "the 56th\npretable dynamic fusion graph.\nIn Proceedings of"
        },
        {
          "References": ""
        },
        {
          "References": "Annual Meeting of\nthe Association for Computational Lin-"
        },
        {
          "References": ""
        },
        {
          "References": "guistics, volume 1, pages 2236–2246, Melbourne, Australia,"
        },
        {
          "References": "2018. Association for Computational Linguistics."
        },
        {
          "References": "[4] Tadas Baltruˇsaitis, Chaitanya Ahuja,\nand Louis-Philippe"
        },
        {
          "References": ""
        },
        {
          "References": "Morency. Multimodal machine learning: A survey and tax-"
        },
        {
          "References": ""
        },
        {
          "References": "IEEE transactions on pattern analysis and machine\nonomy."
        },
        {
          "References": ""
        },
        {
          "References": "intelligence, 41(2):423–443, 2018."
        },
        {
          "References": ""
        },
        {
          "References": "[5]\nFabien Baradel, Christian Wolf,\nJulien Mille, and Graham"
        },
        {
          "References": ""
        },
        {
          "References": "Taylor. Glimpse clouds: Human activity recognition from"
        },
        {
          "References": ""
        },
        {
          "References": "unstructured feature points.\nIn The IEEE Conference on"
        },
        {
          "References": "Computer Vision and Pattern Recognition (CVPR), pages"
        },
        {
          "References": "469–478, 2018."
        },
        {
          "References": "[6]\nJoao Carreira and Andrew Zisserman.\nQuo vadis,\naction"
        },
        {
          "References": "recognition? a new model and the kinetics dataset.\nIn 2017"
        },
        {
          "References": "IEEE Conference on Computer Vision and Pattern Recogni-"
        },
        {
          "References": "tion (CVPR), pages 4724–4733, 2017."
        },
        {
          "References": "[7]\nJun-Ho Choi and Jong-Seok Lee. Embracenet: A robust deep"
        },
        {
          "References": "Informa-\nlearning architecture for multimodal classiﬁcation."
        },
        {
          "References": "tion Fusion, 51:259–270, 2019."
        },
        {
          "References": "[8] Alban Main de Boissiere and Rita Noumeir.\nInfrared and 3d"
        },
        {
          "References": "skeleton feature fusion for\nrgb-d action recognition, 2020."
        },
        {
          "References": "arXiv preprint. arXiv:2002.12886 [cs.CV]."
        },
        {
          "References": "[9]\nP Ravindra De Silva, Minetada Osano, Ashu Marasinghe,"
        },
        {
          "References": ""
        },
        {
          "References": "and Ajith P Madurapperuma.\nTowards\nrecognizing emo-"
        },
        {
          "References": ""
        },
        {
          "References": "tion with affective dimensions\nthrough body gestures.\nIn"
        },
        {
          "References": ""
        },
        {
          "References": "7th International Conference on Automatic Face and Ges-"
        },
        {
          "References": "ture Recognition (FGR06), pages 269–274, 2006."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Covarep — a\ncollaborative\nvoice\nanalysis\nrepository\nfor",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Belgium, 2018. Association for Computational Linguistics."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "speech technologies.\nIn 2014 IEEE International Confer-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[23] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu.\nCo-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "ence on Acoustics, Speech and Signal Processing (ICASSP),",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "occurrence\nfeature\nlearning from skeleton data\nfor\naction"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "pages 960–964, 2014.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "recognition and detection with hierarchical aggregation.\nIn"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[11]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Proceedings of\nthe 27th International Joint Conference on"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Toutanova.\nBert:\nPre-training of deep bidirectional\ntrans-",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Artiﬁcial Intelligence (IJCAI), pages 786–792. AAAI Press,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "formers for\nlanguage understanding, 2018.\narXiv preprint.",
          "Natural Language Processing, pages 1369–1379, Brussels,": "2018."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "arXiv:1810.04805 [cs.CL].",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[24]\nJianan Li, Xuemei Xie, Qingzhe Pan, Yuhan Cao, Zhifu"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[12] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Zhao, and Guangming Shi. Sgm-net: Skeleton-guided mul-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Trevor Darrell,\nand Marcus Rohrbach.\nMultimodal com-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "timodal network for action recognition. Pattern Recognition,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "pact bilinear pooling for visual question answering and vi-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "104:107356, 2020."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "the 2016 Conference on\nsual grounding.\nIn Proceedings of",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[25] Wenbo Li, Yaodong Cui, Yintao Ma, Xingxin Chen, Guofa"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Empirical Methods in Natural Language Processing, pages",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Li, Gang Guo, and Dongpu Cao. A spontaneous driver emo-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "457–468, Austin, TX, 2016. Association for Computational",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "tion facial expression (defe) dataset for intelligent vehicles,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Linguistics.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "2020. arXiv preprint. arXiv:2005.08626 [cs.CV]."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[13] Yang Gao, Oscar Beijbom, Ning Zhang,\nand Trevor Dar-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[26] Kuan Liu, Yanen Li, Ning Xu, and Prem Natarajan. Learn"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "rell. Compact bilinear pooling.\nIn 2016 IEEE Conference",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "to combine modalities in multimodal deep learning, 2018."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "on Computer Vision and Pattern Recognition (CVPR), pages",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "arXiv preprint. arXiv:1805.11730 [stat.ML]."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "317–326, 2016.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[27] Mengyuan Liu and Junsong Yuan. Recognizing human ac-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[14] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "tions as\nthe evolution of pose estimation maps.\nIn 2018"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "A regularization method for convolutional networks.\nIn S.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "IEEE/CVF Conference\non Computer Vision\nand Pattern"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Recognition (CVPR), pages 1159–1168, 2018."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Bianchi, and R. Garnett, editors, Advances in Neural Infor-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "mation Processing Systems, volume 31. Curran Associates,",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[28] Wei Liu, Jie-Lin Qiu, Wei-Long Zheng, and Bao-Liang Lu."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Inc., 2018.",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Multimodal emotion recognition using deep canonical cor-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[15] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.\nCan",
          "Natural Language Processing, pages 1369–1379, Brussels,": "relation analysis, 2019.\narXiv preprint. arXiv:1908.05349"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "spatiotemporal 3d cnns retrace the history of 2d cnns and im-",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[cs.LG]."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "agenet? In 2018 IEEE/CVF Conference on Computer Vision",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[29]\nS. Livingstone\nand F. Russo.\nThe\nryerson\naudio-visual"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "and Pattern Recognition (CVPR), pages 6546–6555, 2018.",
          "Natural Language Processing, pages 1369–1379, Brussels,": "database of\nemotional\nspeech and song (ravdess): A dy-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[16] Devamanyu Hazarika, Roger Zimmermann,\nand Soujanya",
          "Natural Language Processing, pages 1369–1379, Brussels,": "namic, multimodal\nset of\nfacial\nand vocal\nexpressions\nin"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Poria. Misa: Modality-invariant and -speciﬁc representations",
          "Natural Language Processing, pages 1369–1379, Brussels,": "north american english. PLOS ONE, 13(5):1–35, 2018."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "for multimodal\nsentiment analysis, 2020.\narXiv preprint.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[30]\nS. Mai, S. Xing, and H. Hu. Locally conﬁned modality fu-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "arXiv:2005.03545 [cs.CL].",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "sion network with a global perspective for multimodal hu-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[17] Di Hu, Chengze Wang, Feiping Nie, and Xuelong Li. Dense",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "man affective computing. IEEE Transactions on Multimedia,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "multimodal\nfusion\nfor\nhierarchically\njoint\nrepresentation.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "22(1):122–137, 2020."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "In ICASSP 2019-2019 IEEE International Conference on",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[31] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Acoustics, Speech and Signal Processing (ICASSP), pages",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Bera,\nand Dinesh Manocha. M3er: Multiplicative multi-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "3941–3945, 2019.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "modal emotion recognition using facial,\ntextual, and speech"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[18]\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "the AAAI Conference on Artiﬁcial\ncues.\nIn Proceedings of"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "works.\nIn 2018 IEEE/CVF Conference on Computer Vision",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Intelligence, volume 34, pages 1359–1367, Palo Alto, CA,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "and Pattern Recognition (CVPR), pages 7132–7141, 2018.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "2020. AAAI Press."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[19] Qin Jin, Chengxin Li, Shizhe Chen, and Huimin Wu. Speech",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[32]\nFrancisco J. Morales and Daniel Roggen.\nDeep convolu-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "emotion recognition with acoustic and lexical\nfeatures.\nIn",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "tional\nand lstm recurrent neural networks\nfor multimodal"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "2015 IEEE international\nconference on acoustics,\nspeech",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "wearable activity recognition. Sensors, 16(1):115, 2016."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "and signal processing (ICASSP), pages 4749–4753, 2015.",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[33] Emilie Morvant, Amaury Habrard,\nand St´ephane Ayache."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[20] Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L Iuz-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Majority vote of diverse classiﬁers for late fusion.\nIn Struc-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "zolino, and Kazuhito Koishida. Mmtm: Multimodal\ntrans-",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "tural, Syntactic, and Statistical Pattern Recognition, pages"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "fer module for cnn fusion.\nIn 2020 IEEE/CVF Conference",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "",
          "Natural Language Processing, pages 1369–1379, Brussels,": "153–162, Berlin, Heidelberg, 2014. Springer."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "on Computer Vision and Pattern Recognition (CVPR), pages",
          "Natural Language Processing, pages 1369–1379, Brussels,": ""
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "13289–13299, 2020.",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[34] Chalapathy Neti, Gerasimos Potamianos,\nJuergen Luettin,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[21] Diederik P Kingma and Jimmy Ba. Adam: A method for",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Iain Matthews, Herve Glotin, Dimitra Vergyri, June Sison,"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "the 3rd Inter-\nstochastic optimization.\nIn Proceedings of",
          "Natural Language Processing, pages 1369–1379, Brussels,": "and Azad Mashari. Audio visual speech recognition. Tech-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "national Conference on Learning Representations\n(ICLR),",
          "Natural Language Processing, pages 1369–1379, Brussels,": "nical report, IDIAP, 2000."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "2015.",
          "Natural Language Processing, pages 1369–1379, Brussels,": "[35] Natalia Neverova, Christian Wolf, Graham Taylor, and Flo-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "[22]\nJie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa:",
          "Natural Language Processing, pages 1369–1379, Brussels,": "rian Nebout. Moddrop: Adaptive multi-modal gesture recog-"
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "Localized, compositional video question answering.\nIn Pro-",
          "Natural Language Processing, pages 1369–1379, Brussels,": "IEEE Transactions on Pattern Analysis and Machine\nnition."
        },
        {
          "[10] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer.": "ceedings of\nthe 2018 Conference on Empirical Methods in",
          "Natural Language Processing, pages 1369–1379, Brussels,": "Intelligence, 38(8):1692–1706, 2016."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "and Clinton\nFookes.\nDeep\nspatio-temporal\nfeature\nfu-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "IEEE International Conference on Acoustics, Speech and"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "sion with compact bilinear pooling for multimodal emotion",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Signal Processing (ICASSP), pages 6474–6478, 2020."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "recognition.\nComputer Vision and Image Understanding,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[48]\nJennifer Williams, Ramona Comanescu, Oana Radu,\nand"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "174:33–42, 2018.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Leimin Tian.\nDnn multimodal\nfusion techniques\nfor pre-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[37]\nJuan DS Ortega, Mohammed Senoussaoui, Eric Granger,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "dicting video sentiment. In Proceedings of Grand Challenge"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Marco Pedersoli, Patrick Cardinal,\nand Alessandro L Ko-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "and Workshop on Human Multimodal Language (Challenge-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "erich.\nMultimodal\nfusion with deep neural networks\nfor",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "HML), pages 64–72, Melbourne, Australia, 2018. Associa-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "audio-video\nemotion\nrecognition,\n2019.\narXiv\npreprint.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "tion for Computational Linguistics."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "arXiv:1907.03196 [cs.CV].",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[49]\nSaining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[38]\nJuan-Manuel Perez-Rua, Valentin Vielzeuf, Stephane Pa-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Kaiming He. Aggregated residual\ntransformations for deep"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "teux, Moez Baccouche,\nand Frederic Jurie.\nMfas: Mul-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "neural networks.\nIn 2017 IEEE Conference on Computer"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "2019\nIEEE/CVF\ntimodal\nfusion\narchitecture\nsearch.\nIn",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Vision and Pattern Recognition (CVPR), pages 5987–5995,"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Conference on Computer Vision and Pattern Recognition",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "2017."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "(CVPR), pages 6966–6975, 2019.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[50] Hao Yang, Dan Yan, Li Zhang, Dong Li, YunDa Sun, ShaoDi"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[39] Mehmet Emre\nSargin,\nEngin Erzin, Y¨ucel Yemez,\nand",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "You, and Stephen J Maybank. Feedback graph convolutional"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "A Murat Tekalp. Multimodal\nspeaker\nidentiﬁcation using",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "network for skeleton-based action recognition, 2020.\narXiv"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "canonical correlation analysis.\nIn 2006 IEEE International",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "preprint. arXiv:2003.07564 [cs.CV]."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Conference on Acoustics Speech and Signal Processing Pro-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[51] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria,"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "ceedings, volume 1, pages 613–616, 2006.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "and Louis-Philippe Morency. Tensor fusion network for mul-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[40] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "timodal sentiment analysis. In Proceedings of the 2017 Con-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Ntu rgb+d: A large scale dataset for 3d human activity anal-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "ference on Empirical Methods in Natural Language Process-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "ysis. In 2016 IEEE Conference on Computer Vision and Pat-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "ing, pages 1103–1114, Copenhagen, Denmark, 2017. Asso-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "tern Recognition (CVPR), pages 1010–1019, 2016.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "ciation for Computational Linguistics."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[41] Ekaterina Shutova, Douwe Kiela, and Jean Maillard. Black",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[52] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "holes and white rabbits: Metaphor identiﬁcation with visual",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Morency.\nMosi: Multimodal\ncorpus of\nsentiment\ninten-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "features. In Proceedings of the 2016 Conference of the North",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "sity and subjectivity analysis in online opinion videos, 2016."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "American Chapter of the Association for Computational Lin-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "arXiv preprint. arXiv:1606.06259 [cs.CL]."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "guistics: Human Language Technologies, pages 160–170,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[53] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "San Diego, CA, 2016. Association for Computational Lin-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Zhang, Haibin Lin, Yue Sun, Tong He,\nJonas Mueller, R."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "guistics.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Manmatha, Mu Li, and Alexander Smola.\nResnest: Split-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "attention networks, 2020. arXiv preprint. arXiv:2004.08955"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[42] Zhongkai Sun,\nPrathusha Sarma, William Sethares,\nand",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[cs.CV]."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Yingyu Liang.\nLearning relationships between text, audio,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "and video via deep canonical correlation for multimodal lan-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "[54] Wei-Long Zheng, Bo-Nan Dong, and Bao-Liang Lu. Multi-"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "the AAAI Conference on\nguage analysis.\nIn Proceedings of",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "modal emotion recognition using eeg and eye tracking data."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Artiﬁcial\nIntelligence, volume 34, pages 8992–8999, Palo",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "the IEEE\nIn 2014 36th Annual International Conference of"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Alto, CA, 2020. AAAI Press.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "Engineering in Medicine and Biology Society, pages 5040–"
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": "5043, 2014."
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[43] Zhongkai Sun, Prathusha K. Sarma, William Sethares, and",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Erik P. Bucy. Multi-modal\nsentiment analysis using deep",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "canonical correlation analysis.\nIn Proc.\nInterspeech 2019,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "pages 1323–1327, 2019.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[44] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Philippe Morency, and Ruslan Salakhutdinov. Learning fac-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "the\ntorized multimodal\nrepresentations.\nIn Proceedings of",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "7th International Conference on Learning Representations",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "(ICLR 2019), 2019.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[45] Valentin Vielzeuf, Alexis Lechervy, St´ephane Pateux, and",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Fr´ed´eric Jurie. Centralnet: a multilayer approach for multi-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "modal fusion. In Computer Vision – ECCV 2018 Workshops,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "pages 575–589, Cham, Switzerland, 2019. Springer.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[46] Aarohi Vora, Chirag N Paunwala,\nand Mita\nPaunwala.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Improved weight assignment approach for multimodal\nfu-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "sion.\nIn 2014 International Conference on Circuits, Sys-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "tems, Communication and Information Technology Applica-",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "tions (CSCITA), pages 70–74, 2014.",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "[47]\nJianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao,",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        },
        {
          "[36] Dung Nguyen, Kien Nguyen, Sridha Sridharan, David Dean,": "Jie Ding, and Vahid Tarokh.\nSpeech emotion recognition",
          "with dual-sequence lstm architecture.\nIn ICASSP 2020-2020": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "2",
      "title": "Sequential deep learning for human action recognition",
      "authors": [
        "Moez Baccouche",
        "Franck Mamalet",
        "Christian Wolf",
        "Christophe Garcia",
        "Atilla Baskurt"
      ],
      "year": "2011",
      "venue": "Human Behavior Understanding"
    },
    {
      "citation_id": "3",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "5",
      "title": "Glimpse clouds: Human activity recognition from unstructured feature points",
      "authors": [
        "Fabien Baradel",
        "Christian Wolf",
        "Julien Mille",
        "Graham Taylor"
      ],
      "year": "2018",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "6",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "7",
      "title": "Embracenet: A robust deep learning architecture for multimodal classification",
      "authors": [
        "Jun-Ho Choi",
        "Jong-Seok Lee"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "Infrared and 3d skeleton feature fusion for rgb-d action recognition",
      "authors": [
        "Alban Main De Boissiere",
        "Rita Noumeir"
      ],
      "year": "2020",
      "venue": "Infrared and 3d skeleton feature fusion for rgb-d action recognition",
      "arxiv": "arXiv:2002.12886[cs.CV"
    },
    {
      "citation_id": "9",
      "title": "Towards recognizing emotion with affective dimensions through body gestures",
      "authors": [
        "P Ravindra",
        "De Silva",
        "Minetada Osano",
        "Ashu Marasinghe",
        "Ajith Madurapperuma"
      ],
      "year": "2006",
      "venue": "7th International Conference on Automatic Face and Gesture Recognition (FGR06)"
    },
    {
      "citation_id": "10",
      "title": "Covarep -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805[cs.CL"
    },
    {
      "citation_id": "12",
      "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "authors": [
        "Akira Fukui",
        "Dong Park",
        "Daylen Yang",
        "Anna Rohrbach",
        "Trevor Darrell",
        "Marcus Rohrbach"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Compact bilinear pooling",
      "authors": [
        "Yang Gao",
        "Oscar Beijbom",
        "Ning Zhang",
        "Trevor Darrell"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "14",
      "title": "A regularization method for convolutional networks",
      "authors": [
        "Golnaz Ghiasi",
        "Tsung-Yi Lin",
        "Quoc V Le",
        "Dropblock"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
      "authors": [
        "Kensho Hara",
        "Hirokatsu Kataoka",
        "Yutaka Satoh"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "arxiv": "arXiv:2005.03545[cs.CL"
    },
    {
      "citation_id": "17",
      "title": "Dense multimodal fusion for hierarchically joint representation",
      "authors": [
        "Di Hu",
        "Chengze Wang",
        "Feiping Nie",
        "Xuelong Li"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Qin Jin",
        "Chengxin Li",
        "Shizhe Chen",
        "Huimin Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Mmtm: Multimodal transfer module for cnn fusion",
      "authors": [
        "Hamid Reza",
        "Vaezi Joze",
        "Amirreza Shaban",
        "Kazuhito Michael L Iuzzolino",
        "Koishida"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "22",
      "title": "Tvqa: Localized, compositional video question answering",
      "authors": [
        "Jie Lei",
        "Licheng Yu",
        "Mohit Bansal",
        "Tamara Berg"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation",
      "authors": [
        "Chao Li",
        "Qiaoyong Zhong",
        "Di Xie",
        "Shiliang Pu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "24",
      "title": "Sgm-net: Skeleton-guided multimodal network for action recognition",
      "authors": [
        "Jianan Li",
        "Xuemei Xie",
        "Qingzhe Pan",
        "Yuhan Cao",
        "Zhifu Zhao",
        "Guangming Shi"
      ],
      "year": "2020",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Gang Guo, and Dongpu Cao. A spontaneous driver emotion facial expression (defe) dataset for intelligent vehicles",
      "authors": [
        "Wenbo Li",
        "Yaodong Cui",
        "Yintao Ma",
        "Xingxin Chen",
        "Guofa Li"
      ],
      "year": "2020",
      "venue": "Gang Guo, and Dongpu Cao. A spontaneous driver emotion facial expression (defe) dataset for intelligent vehicles",
      "arxiv": "arXiv:2005.08626[cs.CV"
    },
    {
      "citation_id": "26",
      "title": "Learn to combine modalities in multimodal deep learning",
      "authors": [
        "Kuan Liu",
        "Yanen Li",
        "Ning Xu",
        "Prem Natarajan"
      ],
      "year": "2018",
      "venue": "Learn to combine modalities in multimodal deep learning",
      "arxiv": "arXiv:1805.11730"
    },
    {
      "citation_id": "27",
      "title": "Recognizing human actions as the evolution of pose estimation maps",
      "authors": [
        "Mengyuan Liu",
        "Junsong Yuan"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "28",
      "title": "Multimodal emotion recognition using deep canonical correlation analysis",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition using deep canonical correlation analysis",
      "arxiv": "arXiv:1908.05349[cs.LG]"
    },
    {
      "citation_id": "29",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "30",
      "title": "Locally confined modality fusion network with a global perspective for multimodal human affective computing",
      "authors": [
        "S Mai",
        "S Xing",
        "H Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition",
      "authors": [
        "Francisco Morales",
        "Daniel Roggen"
      ],
      "year": "2016",
      "venue": "Sensors"
    },
    {
      "citation_id": "33",
      "title": "Majority vote of diverse classifiers for late fusion",
      "authors": [
        "Emilie Morvant",
        "Amaury Habrard",
        "Stéphane Ayache"
      ],
      "year": "2014",
      "venue": "Structural, Syntactic, and Statistical Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Audio visual speech recognition",
      "authors": [
        "Chalapathy Neti",
        "Gerasimos Potamianos",
        "Juergen Luettin",
        "Iain Matthews",
        "Herve Glotin",
        "Dimitra Vergyri",
        "June Sison",
        "Azad Mashari"
      ],
      "year": "2000",
      "venue": "IDIAP"
    },
    {
      "citation_id": "35",
      "title": "Adaptive multi-modal gesture recognition",
      "authors": [
        "Natalia Neverova",
        "Christian Wolf",
        "Graham Taylor",
        "Florian Nebout",
        "Moddrop"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Deep spatio-temporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "Dung Nguyen",
        "Kien Nguyen",
        "Sridha Sridharan",
        "David Dean",
        "Clinton Fookes"
      ],
      "year": "2018",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "37",
      "title": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "authors": [
        "Juan Ds Ortega",
        "Mohammed Senoussaoui",
        "Eric Granger",
        "Marco Pedersoli",
        "Patrick Cardinal",
        "Alessandro Koerich"
      ],
      "year": "2019",
      "venue": "Multimodal fusion with deep neural networks for audio-video emotion recognition",
      "arxiv": "arXiv:1907.03196[cs.CV"
    },
    {
      "citation_id": "38",
      "title": "Mfas: Multimodal fusion architecture search",
      "authors": [
        "Juan-Manuel Perez-Rua",
        "Valentin Vielzeuf",
        "Stephane Pateux",
        "Moez Baccouche",
        "Frederic Jurie"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "39",
      "title": "Multimodal speaker identification using canonical correlation analysis",
      "authors": [
        "Engin Mehmet Emre Sargin",
        "Yücel Erzin",
        "Yemez",
        "Tekalp Murat"
      ],
      "year": "2006",
      "venue": "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings"
    },
    {
      "citation_id": "40",
      "title": "Ntu rgb+d: A large scale dataset for 3d human activity analysis",
      "authors": [
        "Amir Shahroudy",
        "Jun Liu",
        "Tian-Tsong Ng",
        "Gang Wang"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "Black holes and white rabbits: Metaphor identification with visual features",
      "authors": [
        "Ekaterina Shutova",
        "Douwe Kiela",
        "Jean Maillard"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "42",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Multi-modal sentiment analysis using deep canonical correlation analysis",
      "authors": [
        "Zhongkai Sun",
        "K Prathusha",
        "William Sarma",
        "Erik Sethares",
        "Bucy"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "44",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Learning Representations"
    },
    {
      "citation_id": "45",
      "title": "Centralnet: a multilayer approach for multimodal fusion",
      "authors": [
        "Valentin Vielzeuf",
        "Alexis Lechervy",
        "Stéphane Pateux",
        "Frédéric Jurie"
      ],
      "year": "2019",
      "venue": "Computer Vision -ECCV 2018 Workshops"
    },
    {
      "citation_id": "46",
      "title": "Improved weight assignment approach for multimodal fusion",
      "authors": [
        "Aarohi Vora",
        "Chirag Paunwala",
        "Mita Paunwala"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)"
    },
    {
      "citation_id": "47",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "48",
      "title": "Dnn multimodal fusion techniques for predicting video sentiment",
      "authors": [
        "Jennifer Williams",
        "Ramona Comanescu",
        "Oana Radu",
        "Leimin Tian"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "49",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "Saining Xie",
        "Ross Girshick",
        "Piotr Dollár",
        "Zhuowen Tu",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "50",
      "title": "Feedback graph convolutional network for skeleton-based action recognition",
      "authors": [
        "Hao Yang",
        "Dan Yan",
        "Li Zhang",
        "Dong Li",
        "Yunda Sun",
        "Shaodi You",
        "Stephen Maybank"
      ],
      "year": "2020",
      "venue": "Feedback graph convolutional network for skeleton-based action recognition",
      "arxiv": "arXiv:2003.07564[cs.CV"
    },
    {
      "citation_id": "51",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency",
        "Mosi"
      ],
      "year": "2016",
      "venue": "Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259[cs.CL"
    },
    {
      "citation_id": "53",
      "title": "Resnest: Splitattention networks",
      "authors": [
        "Hang Zhang",
        "Chongruo Wu",
        "Zhongyue Zhang",
        "Yi Zhu",
        "Zhi Zhang",
        "Haibin Lin",
        "Yue Sun",
        "Tong He",
        "Jonas Mueller",
        "R Manmatha",
        "Mu Li",
        "Alexander Smola"
      ],
      "year": "2020",
      "venue": "Resnest: Splitattention networks",
      "arxiv": "arXiv:2004.08955[cs.CV"
    },
    {
      "citation_id": "54",
      "title": "Multimodal emotion recognition using eeg and eye tracking data",
      "authors": [
        "Wei-Long Zheng",
        "Bo-Nan Dong",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    }
  ]
}