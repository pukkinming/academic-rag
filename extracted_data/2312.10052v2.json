{
  "paper_id": "2312.10052v2",
  "title": "Estformer: Transformer Utilizing Spatiotemporal Dependencies For Electroencephalogram Super-Resolution",
  "published": "2023-12-03T12:26:32Z",
  "authors": [
    "Dongdong Li",
    "Zhongliang Zeng",
    "Zhe Wang",
    "Hai Yang"
  ],
  "keywords": [
    "Electroencephalogram (EEG)",
    "masked autoencoders (MAEs)",
    "super-resolution (SR)",
    "Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Towards practical applications of Electroencephalography (EEG), lightweight acquisition devices garner significant attention. However, EEG channel selection methods are commonly data-sensitive and cannot establish a unified sound paradigm for EEG acquisition devices. Through reverse conceptualisation, we formulated EEG applications in an EEG super-resolution (SR) manner, but suffered from high computation costs, extra interpolation bias, and few insights into spatiotemporal dependency modelling. To this end, we propose ESTformer, an EEG SR framework that utilises spatiotemporal dependencies based on the transformer. ESTformer applies positional encoding methods and a multihead self-attention mechanism to the space and time dimensions, which can learn spatial structural correlations and temporal functional variations. ESTformer, with the fixed mask strategy, adopts a mask token to upsample low-resolution (LR) EEG data in the case of disturbance from mathematical interpolation methods. On this basis, we designed various transformer blocks to construct a spatial interpolation module (SIM) and a temporal reconstruction module (TRM). Finally, ESTformer cascades the SIM and TRM to capture and model the spatiotemporal dependencies for EEG SR with fidelity. Extensive experimental results on two EEG datasets show the effectiveness of ESTformer against previous state-of-the-art methods, demonstrating the versatility of the Transformer for EEG SR tasks. The superiority of the SR data was verified in an EEG-based person identification and emotion recognition task, achieving a 2% to 38% improvement compared with the LR data at different sampling scales.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "W ITH the rapid advancements in artificial intelligence and neuroscience, electroencephalography (EEG)based brain-computer interface technology has enabled a wide range of applications, including biometrics  [1] , healthcare  [2] ,  [3] , and intelligent control systems  [4] ,  [5] , offering users novel and transformative life experiences. For practical applications, EEG acquisition devices must meet key requirements such as user comfort and affordability  [6] . Consequently, design paradigms for lightweight and portable EEG devices have attracted considerable attention. Researchers have explored various design strategies to meet these demands, such as enhancing and reducing the number of electrodes. A common approach is to employ EEG channel selection techniques to eliminate redundant electrodes  [7] . However, these methods DongDong Li, Zhongliang Zeng, Zhe Wang, and Hai Yang are with the East China University of Science and Technology.\n\n* Corresponding author. E-mail: wangzhe@ecust.edu.cn(Z.  Wang) .\n\nare highly data-sensitive and often yield varying channel combinations depending on the manual features extracted from the data acquired using different protocols and devices  [8] . In addition to channel selection, EEG applications face a similar challenge: the need for diverse methods tailored to different protocols and devices  [9] -  [11] . This complexity arises from intrinsic difficulties in processing data from heterogeneous devices and protocols  [12] . Thus, although a unified paradigm for EEG devices is crucial for advancing EEG applications, it remains difficult to achieve this using current channel-selection approaches alone.\n\nIn general, diverse techniques can be applied to process data acquired through lightweight EEG devices to enhance the performance of subsequent tasks, such as data augmentation to expand dataset samples  [13] , recover poor segments in some electrodes based on effective electrodes  [14] , and extract better EEG representations by self-supervised learning with a random mask strategy  [15] . Nevertheless, these studies mainly focus on the question of how to better represent EEG in the given data dimensions and can hardly reconcile into a unified electrode system design. In addition, the implicit neural information in the uncovered electrode areas remains to be explored for advanced applications, especially for low-spatialresolution EEG data.\n\nIn computer vision, deep-learning methods  [16] ,  [17]  are commonly applied to preprocessed data using mathematical techniques such as artefact removal  [18] , image segmentation  [19] , and noise reduction  [20] ,  [21] . Consequently, researchers have emphasised these upstream preprocessing techniques to assess their impact on downstream performance. A notable area within these techniques is image reconstruction, which investigates the inherent characteristics of data in low-level tasks, such as super-resolution (SR)  [22] , to address illposed problems and establish a more robust foundation for higher-level tasks  [23] . Advancements in SR technology have provided valuable insights into recovering EEG data from unobserved channels. In general, there are two methodological approaches for integrating the SR reconstruction technology into EEG. One approach involves an end-to-end strategy that employs deep-learning methods directly to upsample lowspatial-resolution EEG data  [24] . An alternative approach adopts a stage-to-stage methodology that combines mathematical interpolation techniques with deep-learning methods for data refinement  [25] .\n\nDespite their demonstrated feasibility, existing EEG SR methods face three significant limitations that hinder their effectiveness:\n\n1) High computational cost: Both end-to-end and stageto-stage approaches frequently rely on convolutional neural network (CNN)-based generative adversarial networks (GANs) or other complex frameworks, which results in excessive computational overhead and limits real-time applications  [24] .\n\n2) Extra-interpolation bias: In stage-to-stage frameworks, traditional mathematical interpolation techniques, often used as preprocessing steps, can introduce substantial bias rather than capture the true underlying characteristics of complex EEG data. This bias can negatively affect the generalisation performance of subsequent deep-learning models, particularly when handling challenging or noisy EEG signals  [26] .\n\n3) Lack of spatiotemporal dependencies: From a spatiotemporal perspective, end-to-end methods typically do not account for the physical distances between electrodes during interpolation, which limits their ability to capture spatial relationships. In addition, effectively modelling the temporal dependencies inherent in EEG data remains an open challenge that has not been adequately addressed in current models  [27] ,  [28] .\n\nTo overcome these challenges, our emphasis is on end-toend frameworks to obviate the need for mathematical interpolation methods. The backbone of the framework should proficiently model data in both spatial and temporal dimensions. Therefore, we propose ESTformer, which is a framework based on a transformer  [29]  with a fixed-mask strategy.\n\nIn the development of end-to-end frameworks for EEG SR, an intuitive approach is to employ a fixed-mask strategy, in which the model is trained to reconstruct the data in masked channels based on the information available from other channels. However, only a few studies have explored fixedmask strategies for EEG applications. Instead, a random mask strategy is commonly adopted, particularly in self-supervised pretraining paradigms  [30] . Although the random mask strategy offers marginally better performance by presenting a model with diverse views of the same data, it may conflict with the specific requirements of the EEG spatial SR task. By generating random masks across epochs, the random mask strategy risks exposing the model to nearly all channels, which could lead to unintended leakage of unseen channel representations, particularly in the EEG SR task described in  [28] . Based on this, we hypothesised that the fixed-mask strategy holds significant potential for EEG SR reconstruction, guiding our framework design.\n\nIn recent studies, the mask strategy has commonly been accompanied by an encoder-decoder framework based on a transformer, which is compact and easy to train. The Transformer is mainly implemented using a multihead self-attention (MSA) mechanism with positional encoding, which can explore spatial structural correlations between different channels of EEG data, conforming to the SR task. The application of the transformer in the field of long-term sequence data also demonstrated its potential for analysing and modelling temporal variations in EEG data with high temporal resolution  [31] . The Transformer, which is primarily composed of MSA and linear layers, can establish long-term dependency relationships and support parallel computing, which can alleviate the timeconsuming issues associated with previous methods.\n\nIn summary, the transformer-based framework has the potential to effectively model the functional connectivity of different brain regions at a low time cost. Therefore, we propose a spatial interpolation module (SIM) and temporal reconstruction module (TRM) based on a transformer to capture spatial structural correlations and temporal functional variations in EEG, respectively. The proposed ESTformer, composed of SIM and TRM, can utilise spatial and temporal dependencies to efficiently achieve high-quality signal spatial dimension expansion, thereby improving the performance of downstream tasks in EEG, such as person identification and emotion recognition.\n\nThe main contributions of this study are as follows.\n\n1) This study proposes ESTformer, a transformer-based EEG SR framework with less computation. We apply the fixed-mask strategy for the EEG SR task to upsample the EEG data without introducing extra bias from mathematical interpolation methods. ESTformer, establishing the mapping between low-resolution (LR) and SR data, can be adapted to diverse lightweight acquisition devices.\n\n2) To better model data in space and time dimensions, we apply the MSA to introduce space-wise MSA (SSA) and time-wise MSA (TSA). Leveraging the SSA and TSA, we introduced the SSA block (SSAB) and TSA block (TSAB). Considering the spatiotemporal dependency, we propose the cross-attention block (CAB) and further build SIM and TRM with 3D spatial and 1D temporal positional encoding methods.\n\n3) Extensive experiments were conducted to verify the EEG SR performance of the proposed ESTformer. The downstream tasks of person identification and emotion recognition were studied to discuss the effectiveness of EEG SR data in terms of common handcrafted features in different frequency bands, compared to the LR data and ground truth (GT) data.\n\nThe remainder of this paper is organised as follows: Sec.II discusses related work, Sec.III presents the proposed ES-Tformer framework, Sec.IV demonstrates the experimental results including the EEG SR reconstruction task and the EEG downstream tasks, and Sec.V concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Eeg Interpolation And Sr",
      "text": "The EEG source is subject to strong volume conduction distortion owing to the conduction characteristics of the scalp and the \"many-to-many\" relationship between scalp electrodes and brain sources. To model the human brain, a volume conduction model can be established using electromagnetics with a spatial mathematical foundation  [32] . Thus, traditional interpolation methods for EEG are based on the strength of spatial mathematical interpolation analysis. Nouira et al.  [33]  proposed barycentric and spline interpolation methods for EEG. Khouaja et al.  [34]  combined the Kalman filter with spherical spline interpolation (SI)  [35] . These mathematical interpolation methods are highly dependent on the position of the electrodes and are sensitive to the calculation of distances between electrodes  [36] . However, only the type of electrode system can be accessed instead of the precise position of the electrodes in many EEG datasets and practical applications, introducing some deviations caused by different human head shapes and wearable modes.\n\nConsequently, researchers have begun to use deep-learning methods in an end-to-end manner for EEG interpolation and SR by establishing encoder-decoder architectures and GAN primarily based on CNN. Corley et al.  [24]  applied a Wasserstein GAN (WGAN) to an EEG SR with better stability. Saba-Sadiya et al.  [14]  proposed a deep encoderdecoder network to interpolate artefacts in EEG in general and transfer learning settings. Svantesson et al.  [27]  proposed three encoder-decoder networks to reconstruct EEG data under different SR settings. Sun et al.  [37]  simply applied the Informer  [31]  to various EEG channel interpolation tasks.\n\nRecent studies have mainly focused on a stage-to-stage manner, tending to use mathematical interpolation methods for interpolation, and then using other deep-learning models to improve the interpolation results. Han et al.  [25]  interpolated unseen channels by averaging their neighbouring channels and proposed a deep CNN-based model for data augmented by adding noise. Panwar et al.  [38]  proposed a conditional WGAN with a gradient penalty to interpolate single-channel EEG data based on bicubic interpolation and bilinear weight initialisation. Tang et al.  [28]  studied brain structure and functional connectivity and adopted suitable graph neural networks and CNNs to obtain EEG SR data based on preliminary interpolated EEG data using the SI method  [35] , and then compared the SR EEG data with the original LR EEG data in case studies. Although these stage-to-stage methods that combine mathematical interpolation methods with deeplearning models have achieved better performance, they rely on the modelling ability of mathematical interpolation methods, thereby introducing significant bias in challenging data and weakening the generalisation ability of subsequent deeplearning models  [39] .\n\nIn recent years, there have been studies on brain magnetic resonance image (MRI) SR based on transformers, demonstrating the modelling and generation capabilities of brain image data  [40] . Although the feasibility of the selfattention mechanism has been verified in EEG  [41] , it mainly serves EEG classification-related tasks  [42] . When using a transformer, there are still few insights into electrode spatial position encoding specifically for EEG  [43]  and how MSA represents the implicit information of brain region connectivity  [44] .\n\nIn summary, there is still room for exploration to address the ill-posed problem of transformers in EEG-related generative tasks, as little research can be found in the literature  [37] . Therefore, we attempted to explore the transformer capability of EEG SR to alleviate the limitations of the existing methods explored in this context.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Mask Strategy For Eeg",
      "text": "In 2019, Devlin et al.  [45]  proposed BERT, a bidirectional masked language model based on a transformer  [29] , which demonstrated remarkable performance in language understanding and suggested a random mask strategy for selfsupervised learning to explore the intrinsic representations of data. Sequentially, wav2vec 2.0  [46]  was proposed for audio signal learning tasks. He et al.  [30]  proposed MAEs based on a vision transformer (ViT)  [47]  and discussed three mask strategies: block-wise sampling, grid-wise sampling, and random sampling. Among these, the random sampling mask strategy achieved remarkable image reconstruction quality, even at a high masking rate of 75%. MAEs, which serve as a strong motivation for researchers, have been applied to other domains such as graph data  [48] .\n\nThese studies provide evidence for the feasibility of leveraging the transformer and random mask strategy for selfsupervised learning in different data modalities; thus, existing studies on EEG pay more attention to the random mask strategy. Inspired by wav2vec 2.0  [46] , Kostas et al.  [15]  proposed the pretraining model BENDR for EEG. Similarly, recent methods reconstructed EEG data for sleep-detection tasks  [49]  and reconstructed handcrafted features extracted from EEG data for emotion-recognition tasks  [50] .\n\nIn summary, transformer-based frameworks with a random mask strategy have shown promising experimental results for exploring the latent representations of EEG. However, despite the image reconstruction ability of the grid-wise sampling mask strategy in MAEs  [30] , few studies have discussed a transformer-based framework with a fixed-mask strategy for EEG tasks. Therefore, we adopted a fixed-mask strategy conforming to the requirements of the EEG SR task, which may provide a way to overcome the challenges in EEG SR and support lightweight devices for EEG applications.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In the EEG SR task (specific to spatial SR), given LR EEG data denoted as X LR ∈ R CLR×T , the objective is to reconstruct EEG SR data denoted as X SR ∈ R CSR×T by interpolating the missing channels of EEG data denoted as X Mask ∈ R C Mask ×T . C SR and C LR denote the numbers of electrode channels in X SR and X LR respectively. C Mask denotes the number of missing channels that must be interpolated (i.e. channels masked in the original data). T denotes the number of time-sampling points for X SR and X LR , indicating the same temporal resolution.\n\nIn this study, we propose ESTformer, an EEG SR framework utilising spatial and temporal dependencies based on a transformer, to alleviate the high time cost and inadequate capacity of modelling challenging EEG data in existing methods. The proposed ESTformer is shown in Fig.  1 , composing the SIM and TRM. In SIM, we adopt CAB based on two types of transformer blocks. One is the space-wise self-attention block (SSAB), which captures spatial dependency using space-wise self-attention (SSA). The other is the time-wise self-attention block (TSAB) to capture the temporal dependency by timewise self-attention (TSA), which is also adopted to construct the TRM.\n\nIn summary, given the hypothesis that exploring temporal variations and spatial correlations in EEG data can help utilise the inherent spatiotemporal dependencies for EEG SR, we applied MSA to both the spatial and temporal dimensions, introducing space-wise MSA (SSA) and time-wise MSA (TSA)   for EEG data in III-A. We then construct SSAB and TSAB, which are combined in the CAB in III-B. Based on CAB, we built the SIM in III-C. Regarding the high temporal resolution characteristics of the EEG data, we built the TRM based on the TSAB in III-D. Finally, in III-E, because stacking blocks mainly focus on high-frequency features  [51] , we establish an ESTformer framework with dense residual connections to learn multilevel features based on SIM and TRM.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Space-And Time-Wise Self-Attention",
      "text": "Inspired by Crossformer  [52] , MSA  [29]  was applied to EEG data in both spatial and temporal dimensions.\n\nAs shown in Fig.  2 , given the spatiotemporal latent variable Z in , the output variable Z s , representing spatial structural correlations, can be obtained by Eq. (  1 ). Similarly, as shown in Fig.  3 , given the Z in , the output variable Z t , representing temporal functional variations, can be obtained by Eq. (  2 ). In Eq. (  1 ) and Eq. (  2 ), Z in , Z s , Z t ∈ R ds×dt , where d s and d t denote the sizes of the spatial and temporal dimensions of the features respectively.\n\nBy estimating the computational complexity using floating point operations (FLOPs), the FLOPs calculation formulas for the SSA and TSA can be expressed as Eq. (  3 ) and Eq. (  4 ) respectively.\n\nExisting deep-learning-based EEG interpolation methods primarily use 2D convolution layers, and the FLOPs calculation formula for this approach can be expressed as  (5) .\n\nIn Eq. (  5 ), C in and C out denote the number of convolution channels for the input data and the output data respectively. k s and k t denote the spatial and temporal sizes of the convolution kernels, respectively.\n\nAccording to EEGSR-GAN  [24] , assuming that d s = 64, d t = 1600, k s = 33, k t = 1, C in = 128, C out = 128, the calculations can be obtained that FLOPs SSA ≈ 0.67G, FLOPs TSA ≈ 0.35G, FLOPs Conv2d ≈ 55.36G. Hence, EEG interpolation models constructed based on the MSA can significantly reduce the high time cost in existing EEG interpolation models based on two-dimensional convolutional neural networks (2D-CNNs)  [24] ,  [25] .\n\nIn the implementation, considering the 3D spatial positional encoding of the EEG electrodes, the SSA sets the number of heads for the MSA to three. However, considering the 1D temporal positional encoding of EEG sampling points, the TSA sets the number of heads for the MSA to one (not using the multihead mechanism).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Transformer Block Design",
      "text": "The key to the EEG spatial SR task lies in modelling the spatial dependencies of EEG and capturing spatial structural correlations. Inspired by the transformer  [29]  and ViT  [47] , the proposed SSA is combined with a multilayer perceptron (MLP) to form the SSAB, as shown in Eq. (  6 ) -  (7) , where LN represents Layer Normalisation, denoted as Layer Norm in Fig.  1 .\n\nThe high temporal resolution characteristics of EEG data require models that utilise temporal dependencies to capture temporal functional variations and improve the EEG SR. performance. Similarly, the TSAB can be formed by combining the MLP with the TSA, as shown in Eq. (  8 ) -(  9 ).\n\nBecause the MLP is based on linear layers, its parameter size and computational complexity are positively correlated with the feature dimensions. Specifically, the SSAB regards the time dimension in EEG data as a feature, whereas the TSAB regards the spatial dimension in EEG data as a feature. Consequently, for EEG data with low spatial resolution but high temporal resolution, the TSAB owns a smaller parameter size and requires a lower computational cost than SSAB.\n\nInspired by CAT  [53] , we leverage the TSAB as an auxiliary module for the SSAB. On the one hand, we suppose that combining TSAB to explore temporal variations can strengthen the representation ability of the SSAB to capture better spatial structural correlations. Thus, the CAB, composed of the SSAB and TSAB through residual connections, is proposed to construct the SIM. After integrating the temporaldimension features, the spatial dependencies of the EEG are modelled to preliminarily reconstruct X SR , denoted as XMask ∈ R C Mask ×T . On the other hand, we directly constructed the TRM based on the TSAB by connecting X LR and XMask according to the electrode channels to obtain X temp ∈ R CSR×T . The temporal dependencies of X temp are modelled to rearrange XMask in X temp , thereby enhancing the reconstruction effect of the EEG spatial SR and obtaining the final X SR .\n\nThe implementations of SSAB, TSAB, and CAB are depicted in the right part of Fig.  1 . The details of the SIM and TRM can be found in Sec.III-C and Sec.III-D.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Spatial Interpolation Module",
      "text": "The MAEs  [30]  demonstrated powerful image reconstruction capabilities. We propose a SIM with a similar architecture.\n\nThe approximate 3D spatial positions of electrodes can be obtained according to the international 10-20 system. Based on the classical Sincos absolute positional encoding method  [54] , we adopt a 3D approach to encode the position information on the x-, y-, and z-axes separately, and integrate them to obtain the final 3D spatial positional encoding, as shown in Eq. (  10 ) -  (13) .\n\nPE (pos (j) ,2i) = sin pos (j)\n\nPE (pos (j) ,2i+1) = cos pos (j)\n\nWhere pos (j) denotes the coordinate position on the j-axis in 3D space, i denotes the dimension, d model denotes the dimension of the positional encoding P E (j) on the j-axis, and the concatenated 3D spatial position encoding is obtained as PE 3D .\n\nThe CAB-based SIM can alternately learn and fuse the temporal and spatial dependencies of EEG data. Because multilevel feature concatenation helps model convergence and alleviates the problem of focusing too much on detailed representations, residual connections are applied to multilevel CAB. A fixed-mask strategy is used to satisfy the requirements of the EEG SR task.\n\nThe main process of SIM is illustrated on the left side of Fig.  1 . First, the given X LR is embedded in the electrode channel representations through linear layers combined with PE 3D . Leveraging the proposed CAB with residual connections, multilevel feature representation Z SIM ∈ R CLR×dt can be learned from X LR . Z SIM is then concatenated with the initialised mask token Z Mask ∈ R C Mask ×dt as the upsampling manner, and combined with PE 3D again. Using the same number of layers L S , the multilevel CAB, with the assistance of PE 3D , enables Z Mask to learn the spatial structural correlations of Z SIM in a high-dimensional feature space, resulting in the preliminary reconstruction XMask .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Temporal Reconstruction Module",
      "text": "We assume that SIM disrupts the temporal information of the original data during the reconstruction process because of the embedding layer, resulting in interpolated data that primarily focus on the data content rather than maintaining an aligned temporal relationship with the real data. Therefore, we propose the TRM.\n\nBecause modelling the temporal information is necessary, the spatial dimension of the electrode channels is regarded as the feature dimension. Similar to BERT  [45] , the classic sinecos absolute positional encoding method is used to encode the 1D temporal position of the time series P E 1D (i.e. PE (t) , where t denotes the time axis).\n\nThe TSAB-based TRM serves as an auxiliary module for the final temporal adjustment, avoiding excessive parameter growth. Similar to SIM, residual connections are applied to handle multilevel EEG features from the TSAB. Unlike SIM, because the input and output data are of the same dimension in TRM, no additional mask strategy, not to mention the mask token, is required.\n\nThe main process of TRM is depicted in the middle of Fig.  1 . Firstly, based on the connection of X LR and XMask according to the order of electrode channels, the obtained X temp ∈ R CSR×T is embedded for temporal variations through linear layers and combined with PE 1D . Leveraging the proposed TSAB with residual connections, the multilevel feature representation Z TRM ∈ R ds×T can be learned from X temp . Z TRM is then combined with PE 1D again. Using the same number of layers L T , the multilevel TSAB, with the assistance of PE 1D , enables XMask to learn the temporal variations of X LR in the high-dimensional feature space Z TRM , reconstructing XMask in the time domain.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Estformer Framework",
      "text": "The overall ESTformer framework is illustrated in Fig.  1 . Given the low-spatial-resolution EEG data X LR , SIM is used to capture the spatial dependencies and model the structural correlations of the EEG, resulting in a preliminary reconstruction XMask for the unseen/masked electrode channels. To learn fully from the original X LR , we concatenate XMask and X LR based on the positions of the electrode channels. The TRM then learns the temporal dependencies and reconstructs the XMask concerning temporal variations. Similarly, the corresponding part of the output from the TRM is replaced with the original X LR to obtain the final EEG SR data X SR .\n\nReconstruction losses are commonly used for EEG SR tasks, such as the mean squared error (MSE) and mean absolute error (MAE) calculated in the time domain. In addition, EEG research often employs frequency-domain analysis  [50] . However, measuring the distance between the reconstructed and real data in the frequency domain helps alleviate the issue of missing high-frequency information in image reconstruction tasks  [55] . In this study, the discrete Fourier Transform (DFT) is used to calculate the MSE (FMSE) between X SR and the real EEG dataX GT ∈ R CSR×T in the frequency domain, as shown in Eq.  (14) .\n\nwhere M denotes the number of unseen electrode channels, X m GT and X m SR respectively denote the data of the m-th unseen channel in X GT and X SR , and F (•) represents the complex number form of the data by the DFT.\n\nWe combine the loss functions in both the time and frequency domains in an auto-weighted manner  [56]  thus, the weights between different loss functions are learned by the deep-learning model. The final loss function is given in Eq.  (15) .\n\nwhere σ 1 and σ 2 are learnable weight-related parameters for FMSE and MAE, respectively, and are updated according to the model-training process. The selected optimiser iteratively minimises L until the gradients converge to an optimal or a suboptimal point.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments",
      "text": "We conducted experiments on the EEG SR task to verify the effectiveness of ESTformer on two datasets, where we compared hyperparameter settings, and analysed the effectiveness of the main modules in ESTformer. Because the proposed ESTformer is a general approach for downstream tasks, we conducted person identification experiments on the EEG motor movement/imagery (MI/MM) dataset  [57]  and emotion-recognition experiments on the SEED dataset  [58]  to further explore the superiority of EEG SR data. The experiments were performed on a PC equipped with a CPU (Intel(R) Core(TM) i7-10700K CPU @ 3.80 GHz), RAM(32 GB), and GPU(NVIDIA GeForce RTX 3080).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Preprocessing",
      "text": "The EEG Motor Movement/Imagery Dataset (MI/MM)  [57]  records EEG signals from 64 channels while the subjects perform different motor tasks of movement and imagery. The electrode positions were obtained using the international 10-20 system. The signal-sampling frequency was set to 160 Hz. The dataset includes over 1500 EEG records, each lasting 1-2 min, obtained from 109 subjects. Each subject participated in 14 experiments: two baseline movements (one minute with eyes open and one minute with eyes closed) and four types of real or imagined hand or foot movements (three two-minute trials for each type). The SEED dataset  [58]  selected 15 movie clips as emotional stimuli, each lasting approximately four minutes, to evoke the corresponding emotions continuously and prominently. This dataset includes EEG and eye-tracking data from 12 subjects and EEG data from an additional three subjects. The movie clips were used to evoke positive, neutral, and negative emotions. After watching each clip, participants were immediately asked to complete a survey reporting their emotional responses. During data collection, a 62-channel international 10-20 system was used for electrode placement, with a sampling frequency of 1000 Hz. In the preprocessed version, the EEG data were downsampled to 200 Hz and filtered using a 0-75 Hz bandpass filter.\n\nIn this study, the EEG SR experimental setup followed the Deep-EEGSR  [28] . The preprocessed EEG signals were divided into continuous non-overlapping 10-second samples for the MI dataset and continuous non-overlapping 4-second samples for the SEED dataset. For each dataset, 80% of the samples were used as the training set and the remaining 20% as the test set. Different visible-channel combinations (four cases) were selected for MI/MM  [57]  and SEED  [58]  using different SR scale factors (two, four, and eight).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Experiment Design",
      "text": "First, we compared the proposed ESTformer with different hyperparameters in a preliminary study of the EEG SR task. Subsequently, the main modules in ESTformer were compared to verify their effectiveness and efficiency in the EEG SR task. Finally, experiments on two downstream tasks were conducted to explore the performance of the SR EEG data.\n\nThe details of these experiments are as follows.\n\nExperiment 1 -Hyperparameter comparison. The experiments were conducted under an SR scale factor of four and an LR channel combination setting for Case 1. The experiments were repeated ten times using the MI/MM and SEED datasets. We compared the main hyperparameter settings of ESTformer from two perspectives: model depth and model width. Comparing each hyperparameter with the three settings, we trade-off efficiency and effectiveness to determine an appropriate combination of hyperparameters. Additionally, the computational efficiency of ESTformer was compared with two EEG SR methods based on 2D-CNN to provide a broader assessment of its performance.\n\nExperiment 2 -SR Scaling Capability. The experiments were conducted across four SR scale factors  (2, 4, 6, and 8) , each paired with a specific LR channel combination setting for Case 1. To ensure robustness, each experiment was repeated ten times on the MI/MM and SEED datasets. In a preliminary study, the ESTformer model was trained under these conditions to assess its reconstruction capability at various SR scales. We analyse the results in this part to examine the relationship between performance and SR scale factor adjustments.\n\nExperiment 3 -Ablation study. The experiments were conducted under all three SR scale factors and four LR channel combination settings. The experiments were repeated ten times using the MI/MM and SEED datasets. We compared the modified MAEs  [30] , proposed SIM, and proposed ESTformer to analyse the effectiveness of each module. In addition, we adopted an easy-to-implement DeepCNN  [25]  as the enhancement module for the interpolated EEG data to validate whether ESTformer can replace traditional mathematical interpolation methods in a two-stage EEG SR manner.\n\nExperiment 4 -Downstream tasks. The experiments were conducted under all three SR scale factors and four LR channel combination settings. The experiments were repeated five times using MI/MM and SEED datasets. A simple 2-block MLP was chosen as the classifier, with the first block learning the feature dimension information and the second block learning the channel dimension representation. Power spectral density (PSD) and differential entropy (DE) features were extracted for person identification and emotion-recognition tasks using MI/MM and SEED, respectively. These features were extracted from the EEG LR, EEG GT, and EEG SR data interpolated by ESTformer. We compared five individual bands and all the frequency bands of the extracted features to analyse the performance of the SR data in the frequency domain for downstream tasks.\n\nWe used the normalised mean square error (NMSE), signalto-noise ratio (SNR), and Pearson correlation coefficient (PCC) as metrics for the EEG SR task. The runtime in  minutes is another important consideration in Ex. 1 and Ex. 2. We used the classification accuracy (Acc) as a metric for the downstream tasks of EEG-based person identification and emotion recognition. All these metrics can be calculated by averaging the results of the repeated experiments for all cases.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Hyperparameter Comparison",
      "text": "The setting of the main hyperparameters of the proposed model was based on two aspects: model depth and model width. In the model width settings, α t represents the proportion of embedded feature length in the SIM module, α s represents the proportion of embedded feature length in the TRM module, and r mlp represents the amplification ratio of the hidden layer in the MLP. Specifically in model usage,  The underlined font represents the counterparts of specific hyperparameter. When one parameter was adjusted, the other parameters used their default values. It can be observed that although deepening the model depth can significantly improve the model performance, it also noticeably increases the computational time. Therefore, the selection of hyperparameter values is based on the SR performance and computational time, considering the effects on both datasets. AdamW was used as the optimiser. The other main hyperparameter settings were a batch size of 36, dropout rate of 0.5, weight decay rate of 0.5, β 1 = 0.9, β 2 = 0.95, and learning rate of 5 × 10 -5 .\n\nAs outlined in Sec.III-A, the use of self-attention mechanisms in the SIM and TRM modules is anticipated to improve computational efficiency over 2D-CNN-based architectures. To evaluate this, we compared ESTformer with two 2D-CNNbased methods: the DEEP-CNN approach from  [25]  and the DenseNet-like  [59]  generator used in EEGSR-GAN  [24] . We assessed the computational efficiency using three metrics: FLOPs per sample, total model parameters, and training time. As shown in Table  II , ESTformer achieved the lowest FLOPs and the shortest training time, aligned with our calculations in Sec.III-A, despite having a higher parameter count. This demonstrates that the self-attention mechanisms in ESTformer significantly contribute to its computational advantages over traditional 2D-CNN structures.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "D. Sr Scaling Capability",
      "text": "To assess the scaling capability of ESTformer preliminarily, we evaluated its performance across four scale factors (2, 4, 6, and 8) using two datasets.\n\nAs shown in Fig.  7 , ESTformer effectively reconstructs EEG data across varying scale factors. As anticipated, the performances of the three key metrics-NMSE, PCC, and SNR-decreases as the scale factor increases in both the MI/MM and SEED datasets. In addition, performance drops become more pronounced at higher scale factors, reflecting the challenge of accurately reconstructing EEG data when a larger proportion of channels are missing. This trend highlights the increased difficulty in maintaining reconstruction accuracy with fewer available data points at higher scaling levels. Based on these observations, we focused on subsequent experiments with scale factors of 2, 4, and 8, aligned with the experimental settings in  [28] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Ablation Study",
      "text": "Based on the set of hyperparameters obtained using Eqs. 1, The following models were selected for the ablation study: MAEs  [30] , SIM, ESTformer ( SIM and TRM), and ESTformer+Deep-CNN (denoted as EST+CNN in the figures), which replaced the mathematical interpolation method of aver-  aging neighbour channels (AN) with the proposed ESTformer. We modified MAEs  [30]  based on the characteristics of the EEG data and the requirements of the EEG SR task. Specifically, the modified MAEs are composed of an SSAB with 1D positional encoding using a fixed-mask sampling strategy.\n\nThe comparative results are shown in Fig.  4 . Compared with modified MAEs, the proposed SIM uses 3D positional encoding and CAB, resulting in a significant improvement. ES-Tformer, built on SIM and TRM, outperforms SIM, indicating that additional temporal reconstruction is needed for modelling long-term time dependency through SIM. ESTformer+Deep-CNN (EST+CNN) applies a Deep-CNN  [25]  to the interpolated EEG data obtained from ESTformer, resulting in improved but significant time consumption, as mentioned in III-A.\n\nAccordingly, Fig.  5  and Fig.  6  show the reconstructed EEG time series and scalp potentials, respectively. In Fig.  5 , EEG time series (averaged between channels) by ESTformer (in orange) better fits the GT (in black) than the SIM (in red) in time-axis, which demonstrates TRM is effective in modelling the long-term time dependency. In Fig.  6 , we calculate PSD features in MI/MM and DE features in SEED, aligning to downstream task settings in Sec.IV-G. The distributions of scalp potentials (averaged between frequency bands) obtained by ESTformer and ESTformer+Deep-CNN (EST+CNN) were also closer to the GT than their counterparts, especially for the outline of the critical location.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "F. Overall Comparison With Existing Methods",
      "text": "Following the ablation study, ESTformer is considered a one-stage approach and ESTformer+Deep-CNN is considered a two-stage approach. From this perspective, we compared existing state-of-the-art EEG SR methods. ESTformer was compared with SI and EEGSR-GAN, whereas ESTformer+Deep-CNN was compared with AN+Deep-CNN and SI+Deep-EEGSR  [28] .\n\nTable  III  and Table  IV  present the results in the MI/MM and the SEED, respectively. In these two datasets, the proposed ESTformer shows a significant improvement in one-stage approaches, whereas ESTformer+Deep-CNN can also outperform the state-of-the-art method in two-stage approaches. AN+Deep-CNN  [25]  outperforms EEGSR-GAN in MI/MM, whereas EEGSR-GAN achieves competitive results with AN+Deep-CNN  [25]  in SEED. This finding indicates that modelling challenging data in an end-to-end manner may be a better choice, conforming to the notable performance of ESTformer compared with SI+Deep-EEGSR  [28]",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "G. Downstream Tasks",
      "text": "We conducted experiments on downstream tasks of person identification and emotion recognition. We compared the ESTformer-derived full-channel SR, visible-channel LR, and original full-channel GT data. According to common EEG classification tasks, PSD features are extracted for person identification and DE features are extracted for emotion recognition. PSD and DE features were divided into five frequency bands (Delta, Theta, Alpha, Beta, and Gamma) with dimensions of R C×1 . The effect of using features from all five frequency bands with dimensions of R C×5 was also considered.\n\nTable  V  and Table  VI  represent the results of the person identification task and the emotion recognition task, respectively. Bold font represents the best performance for the same SR scale factor setting. Overall, the SR data consistently outperformed the LR data and, in some cases, achieved competitive performance relative to the GT data, particularly when the scale factor was two. This suggests that the ESTformerderived interpolated data may introduce features that are more \"learnable\" for downstream models, providing benefits over the original full-channel data in certain scenarios.\n\nFor the person identification task, the SR data demonstrated superior performance, particularly in high-frequency bands. This is likely because of the nature of motor MI tasks, which are known to elicit neural activity in the high-frequency range, making SR data more effective at capturing relevant discriminative features in this context. The improvements ranged significantly from 2% to 38%, highlighting the potential of SR data to enhance person identification performance, particularly with larger SR scale factors.\n\nBy contrast, the emotion-recognition task showed modest improvements with the SR data, with gains ranging from 3% to 8% across the different frequency bands. Interestingly, the best performance was achieved when using features from all five frequency bands, emphasising the complexity of affective computing tasks. The smaller improvements in this task underscore the challenge of capturing the nuanced neural signals associated with emotional states and suggest that while SR data can be beneficial, the task requires leveraging the full spectrum of EEG signals for optimal performance. These findings have broad implications for EEG-based applications. The ability of ESTformer to supplement information from missing channels indicates that lightweight EEG devices with fewer channels can potentially be used for real-time applications, with SR methods compensating for missing data. This is particularly relevant for EEG-based biometric systems, where SR data show significant promise for enhancing person identification. In this context, a multichannel device can be used to collect high-quality data for training (as GT data), whereas a more practical, fewer-channel device can be employed in real-time settings (with LR data interpolated to SR data for testing), enabling more accessible and efficient EEG data collection without compromising performance.\n\nIn conclusion, ESTformer offers a powerful solution for improving the usability of low-channel EEG data across a range of downstream tasks, facilitating the development of lightweight EEG acquisition systems while maintaining robust performance.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "In this study, we introduced ESTformer, an end-to-end transformer-based framework designed to efficiently reconstruct high-resolution (HR) EEG data by leveraging the spatiotemporal dependencies inherent in EEG signals. The ESTformer architecture integrates a SIM and TRM that employ SSA and TSA, respectively, along with 3D spatial and 1D temporal positional encoding techniques.\n\nOur experimental results indicate that the following: (1) ES-Tformer achieves state-of-the-art performance in EEG signal reconstruction; (2) the reconstructed (SR) EEG data significantly enhance performance in downstream EEG-based tasks, including person identification and emotion recognition.\n\nHowever, some limitations remain: (1) As a transformerbased framework, ESTformer remains computationally intensive, which could limit its applicability in real-time scenarios;\n\n(2) the ESTformer architecture uses a straightforward cascade of SIM and TRM modules, which may benefit from alternative designs that facilitate more complex interactions between modules to improve learning efficiency.\n\nIn addition, several directions warrant further investigation.\n\n(1) Performance under Complex Scenarios: Although ES-Tformer exhibits robust performance, its effectiveness under challenging conditions-such as cross-subject variations, task diversity, and long-term data acquisition-requires further generalisation validation. Thus, more adaptive and resilient modelling strategies should be explored.\n\n(2) Comparative Analyses with Self-Supervised Representations and Privacy Considerations: The advantages of representations generated by SR relative to those learned through selfsupervised approaches remain underexplored. Furthermore, ensuring privacy in the reconstructed EEG data is crucial, especially given the sensitivity of personal neurophysiological data.\n\n(3) Exploration of multimodal synergies: Given the unique strengths of different modalities across various resolutions, a multimodal approach holds the potential to enhance EEG reconstruction quality. By leveraging modality-specific advantages, future research can provide an enriched synergistic framework for EEG analysis with fidelity.\n\n(4) Application of Implicit Neural Representations: The adoption of implicit neural representation techniques may further enable bidirectional mapping between EEG source activity and scalp electrode signals, facilitating flexible EEG signal reconstruction. The incorporation of these methods may provide a more versatile solution that accommodates diverse reconstruction demands.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the ESTformer framework. ESTformer is composed of SIM and TRM. LR EEG data first pass through SIM to reconstruct missing",
      "page": 4
    },
    {
      "caption": "Figure 2: SSA pipeline in SSAB, modelling spatial relationships. Unfolding the",
      "page": 4
    },
    {
      "caption": "Figure 3: TSA pipeline in TSAB, modelling temporal relationships. Unfolding",
      "page": 4
    },
    {
      "caption": "Figure 4: Comparisons between ESTformer and counterparts for ablation study.",
      "page": 8
    },
    {
      "caption": "Figure 5: Time-axis visualisation results of EEG reconstruction in two datasets.",
      "page": 8
    },
    {
      "caption": "Figure 6: Channel-axis visualisation results of EEG reconstruction in two datasets.",
      "page": 9
    },
    {
      "caption": "Figure 7: ESTformer performance at four SR scale settings in two datasets.",
      "page": 9
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Personalized Federated Continual Learning for Task-Incremental Biometrics",
      "authors": [
        "D Li",
        "N Huang",
        "Z Wang",
        "H Yang"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "2",
      "title": "Performance enhancement of p300 detection by multiscale-cnn",
      "authors": [
        "H Wang",
        "Z Pei",
        "L Xu",
        "T Xu",
        "A Bezerianos",
        "Y Sun",
        "J Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "3",
      "title": "Adazd-Net: Automated adaptive and explainable Alzheimer's disease detection system using EEG signals",
      "authors": [
        "S Khare",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "4",
      "title": "Linking attention-based multiscale cnn with dynamical gcn for driving fatigue detection",
      "authors": [
        "H Wang",
        "L Xu",
        "A Bezerianos",
        "C Chen",
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "5",
      "title": "Recognising situation awareness associated with different workloads using EEG and eyetracking features in air traffic control tasks",
      "authors": [
        "Q Li",
        "K Ng",
        "S Yu",
        "C Yiu",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "6",
      "title": "DSCNN-CAU: Deep-Learning-Based Mental Activity Classification for IoT Implementation Toward Portable BCI",
      "authors": [
        "M Saini",
        "U Satija",
        "M Upadhayay"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "7",
      "title": "Person identification using EEG channel selection with hybrid flower pollination algorithm",
      "authors": [
        "Z Alyasseri",
        "A Khader",
        "M Al-Betar",
        "O Alomari"
      ],
      "year": "2020",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Stable EEG-Based biometric system using functional connectivity based on Time-Frequency features with optimal channels",
      "authors": [
        "R Ashenaei",
        "A Beheshti",
        "T Rezaii"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "9",
      "title": "An approach of one-vs-rest filter bank common spatial pattern and spiking neural networks for multiple motor imagery decoding",
      "authors": [
        "H Wang",
        "C Tang",
        "T Xu",
        "T Li",
        "L Xu",
        "H Yue",
        "P Chen",
        "J Li",
        "A Bezerianos"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Diverse feature blend based on filter-bank common spatial pattern and brain functional connectivity for multiple motor imagery detection",
      "authors": [
        "H Wang",
        "T Xu",
        "C Tang",
        "H Yue",
        "C Chen",
        "L Xu",
        "Z Pei",
        "J Dong",
        "A Bezerianos",
        "J Li"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Motor imagery decoding enhancement based on hybrid eeg-fnirs signals",
      "authors": [
        "T Xu",
        "Z Zhou",
        "Y Yang",
        "Y Li",
        "J Li",
        "A Bezerianos",
        "H Wang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Hybrid convolutional neural network and flexible dwarf mongoose optimization algorithm for strong kidney stone diagnosis",
      "authors": [
        "H Liu",
        "N Ghadimi"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "13",
      "title": "Data Augmentation for EEG-Based Emotion Recognition Using Generative Adversarial Networks",
      "authors": [
        "G Bao",
        "B Yan",
        "L Tong",
        "J Shu",
        "L Wang",
        "K Yang",
        "Y Zeng"
      ],
      "year": "2021",
      "venue": "Frontiers in Computational Neuroscience"
    },
    {
      "citation_id": "14",
      "title": "EEG Channel Interpolation Using Deep Encoder-decoder Networks",
      "authors": [
        "S Saba-Sadiya",
        "T Alhanai",
        "T Liu",
        "M Ghassemi"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "15",
      "title": "BENDR: Using Transformers and a Contrastive Self-Supervised Learning Task to Learn From Massive Amounts of EEG Data",
      "authors": [
        "D Kostas",
        "S Aroca-Ouellette",
        "F Rudzicz"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Timely detection of skin cancer: An ai-based approach on the basis of the integration of echo state network and adapted seasons optimization algorithm",
      "authors": [
        "M Han",
        "S Zhao",
        "H Yin",
        "G Hu",
        "N Ghadimi"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "17",
      "title": "A deep learning outline aimed at prompt skin cancer detection utilizing gated recurrent unit networks and improved orca predation algorithm",
      "authors": [
        "L Zhang",
        "J Zhang",
        "W Gao",
        "F Bai",
        "N Li",
        "N Ghadimi"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "18",
      "title": "Imperialist competitive algorithm-based optimization of neuro-fuzzy system parameters for automatic red-eye removal",
      "authors": [
        "N Razmjooy",
        "M Ramezani",
        "N Ghadimi"
      ],
      "year": "2017",
      "venue": "International Journal of Fuzzy Systems"
    },
    {
      "citation_id": "19",
      "title": "A hybrid neural network -world cup optimization algorithm for melanoma detection",
      "authors": [
        "N Razmjooy",
        "F Sheykhahmad",
        "N Ghadimi"
      ],
      "year": "2018",
      "venue": "Open Medicine"
    },
    {
      "citation_id": "20",
      "title": "Computeraided diagnosis of skin cancer based on soft computing techniques",
      "authors": [
        "Z Xu",
        "F Sheykhahmad",
        "N Ghadimi",
        "N Razmjooy"
      ],
      "year": "2020",
      "venue": "Open Medicine"
    },
    {
      "citation_id": "21",
      "title": "Breast cancer diagnosis by convolutional neural network and advanced thermal exchange optimization algorithm",
      "authors": [
        "X Cai",
        "X Li",
        "N Razmjooy",
        "N Ghadimi"
      ],
      "year": "2021",
      "venue": "Computational and Mathematical Methods in Medicine"
    },
    {
      "citation_id": "22",
      "title": "Two-Stage Deep Single-Image Super-Resolution With Multiple Blur Kernels for Internet of Things",
      "authors": [
        "X Sun",
        "S Wang",
        "J Yang",
        "F Wei",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "23",
      "title": "Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy",
      "authors": [
        "J Yoo",
        "N Ahn",
        "K.-A Sohn"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "24",
      "title": "Deep EEG super-resolution: Upsampling EEG spatial resolution with Generative Adversarial Networks",
      "authors": [
        "I Corley",
        "Y Huang"
      ],
      "year": "2018",
      "venue": "2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)"
    },
    {
      "citation_id": "25",
      "title": "Feasibility Study of EEG Super-Resolution Using Deep Convolutional Networks",
      "authors": [
        "S Han",
        "M Kwon",
        "S Lee",
        "S Jun"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "26",
      "title": "Distortions in EEG interregional phase synchrony by spherical spline interpolation: Causes and remedies",
      "authors": [
        "S Kang",
        "T Lano",
        "S Sponheim"
      ],
      "year": "2015",
      "venue": "Neuropsychiatric Electrophysiology"
    },
    {
      "citation_id": "27",
      "title": "Virtual EEG-electrodes: Convolutional neural networks as a method for upsampling or restoring channels",
      "authors": [
        "M Svantesson",
        "H Olausson",
        "A Eklund",
        "M Thordstein"
      ],
      "year": "2021",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "28",
      "title": "Deep EEG Superresolution via Correlating Brain Structural and Functional Connectivities",
      "authors": [
        "Y Tang",
        "D Chen",
        "H Liu",
        "C Cai",
        "X Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17"
    },
    {
      "citation_id": "30",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollar",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
      "authors": [
        "H Zhou",
        "S Zhang",
        "J Peng",
        "S Zhang",
        "J Li",
        "H Xiong",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Electroencephalography Source Connectivity: Aiming for High Resolution of Brain Networks in Time and Space",
      "authors": [
        "M Hassan",
        "F Wendling"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "33",
      "title": "EEG potential mapping by 3D interpolation methods",
      "authors": [
        "I Nouira",
        "A Ben Abdallah",
        "M Bedoui"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Multimedia Computing and Systems (ICMCS)"
    },
    {
      "citation_id": "34",
      "title": "Enhancing EEG Surface Resolution by Using a Combination of Kalman Filter and Interpolation Method",
      "authors": [
        "I Khouaja",
        "I Nouira",
        "M Bedoui",
        "M Akil"
      ],
      "year": "2016",
      "venue": "2016 13th International Conference on Computer Graphics, Imaging and Visualization (CGiV)"
    },
    {
      "citation_id": "35",
      "title": "Spherical spline interpolation-basic theory and computational aspects",
      "authors": [
        "W Freeden"
      ],
      "year": "1984",
      "venue": "Journal of Computational and Applied Mathematics"
    },
    {
      "citation_id": "36",
      "title": "EEG channel interpolation using ellipsoid geodesic length",
      "authors": [
        "H Courellis",
        "J Iversen",
        "H Poizner",
        "G Cauwenberghs"
      ],
      "year": "2016",
      "venue": "2016 IEEE Biomedical Circuits and Systems Conference (BioCAS)"
    },
    {
      "citation_id": "37",
      "title": "Design of virtual BCI channels based on informer",
      "authors": [
        "H Sun",
        "C Li",
        "H Zhang"
      ],
      "year": "2023",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "38",
      "title": "Generating EEG signals of an RSVP Experiment by a Class Conditioned Wasserstein Generative Adversarial Network",
      "authors": [
        "S Panwar",
        "P Rad",
        "J Quarles",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)"
    },
    {
      "citation_id": "39",
      "title": "Spatial Graph Signal Interpolation with an Application for Merging BCI Datasets with Various Dimensionalities",
      "authors": [
        "Y Ouahidi",
        "L Drumetz",
        "G Lioi",
        "N Farrugia",
        "B Pasdeloup",
        "V Gripon"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "3d Cross-Scale Feature Transformer Network for Brain Mr Image Super-Resolution",
      "authors": [
        "W Zhang",
        "L Wang",
        "W Chen",
        "Y Jia",
        "Z He",
        "J Du"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "41",
      "title": "Spatial-frequency convolutional self-attention network for EEG emotion recognition",
      "authors": [
        "D Li",
        "L Xie",
        "B Chai",
        "Z Wang",
        "H Yang"
      ],
      "year": "2022",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "42",
      "title": "A Dual-Branch Spatio-Temporal-Spectral Transformer Feature Fusion Network for EEG-Based Visual Recognition",
      "authors": [
        "J Luo",
        "W Cui",
        "S Xu",
        "L Wang",
        "X Li",
        "X Liao",
        "Y Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "43",
      "title": "Introducing Attention Mechanism for EEG Signals: Emotion Recognition with Vision Transformers",
      "authors": [
        "A Arjun",
        "A Rajpoot",
        "M Panicker"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "44",
      "title": "Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "45",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North"
    },
    {
      "citation_id": "46",
      "title": "Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "48",
      "title": "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner",
      "authors": [
        "Z Hou",
        "Y He",
        "Y Cen",
        "X Liu",
        "Y Dong",
        "E Kharlamov",
        "J Tang"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "49",
      "title": "MAEEG: Masked Auto-encoder for EEG Representation Learning",
      "authors": [
        "H.-Y Chien",
        "H Goh",
        "C Sandino",
        "J Cheng"
      ],
      "year": "2022",
      "venue": "NeurIPS 2022 Workshop on Learning from Time Series for Health"
    },
    {
      "citation_id": "50",
      "title": "A Multi-view Spectral-Spatial-Temporal Masked Autoencoder for Decoding Emotions with Self-supervised Learning",
      "authors": [
        "R Li",
        "Y Wang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Improving Pixelbased MIM by Reducing Wasted Modeling Capability",
      "authors": [
        "Y Liu",
        "S Zhang",
        "J Chen",
        "Z Yu",
        "K Chen",
        "D Lin"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting",
      "authors": [
        "Y Zhang",
        "J Yan"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "53",
      "title": "CAT: Cross Attention in Vision Transformer",
      "authors": [
        "H Lin",
        "X Cheng",
        "X Wu",
        "D Shen"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "54",
      "title": "Accurate medium-range global weather forecasting with 3D neural networks",
      "authors": [
        "K Bi",
        "L Xie",
        "H Zhang",
        "X Chen",
        "X Gu",
        "Q Tian"
      ],
      "year": "2023",
      "venue": "Nature"
    },
    {
      "citation_id": "55",
      "title": "Focal Frequency Loss for Image Reconstruction and Synthesis",
      "authors": [
        "L Jiang",
        "B Dai",
        "W Wu",
        "C Loy"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "56",
      "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
      "authors": [
        "R Cipolla",
        "Y Gal",
        "A Kendall"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "BCI2000: A general-purpose brain-computer interface (BCI) system",
      "authors": [
        "G Schalk",
        "D Mcfarland",
        "T Hinterberger",
        "N Birbaumer",
        "J Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "58",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "59",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}