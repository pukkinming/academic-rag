{
  "paper_id": "2109.06112v1",
  "title": "Beyond Isolated Utterances: Conversational Emotion Recognition",
  "published": "2021-09-13T16:40:35Z",
  "authors": [
    "Raghavendra Pappagari",
    "Piotr Żelasko",
    "Jesús Villalba",
    "Laureano Moro-Velazquez",
    "Najim Dehak"
  ],
  "keywords": [
    "conversational emotion recognition",
    "isolated utterances",
    "Transformer",
    "DiverseCatAugment augmentation",
    "sequence labeling",
    "interlocutor-aware"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is the task of recognizing the speaker's emotional state given a recording of their utterance. While most of the current approaches focus on inferring emotion from isolated utterances, we argue that this is not sufficient to achieve conversational emotion recognition (CER) which deals with recognizing emotions in conversations. In this work, we propose several approaches for CER by treating it as a sequence labeling task. We investigated transformer architecture for CER and, compared it with ResNet-34 and BiLSTM architectures in both contextual and contextless scenarios using IEMOCAP corpus. Based on the inner workings of the self-attention mechanism, we proposed DiverseCatAugment (DCA), an augmentation scheme, which improved the transformer model performance by an absolute 3.3% micro-f1 on conversations and 3.6% on isolated utterances. We further enhanced the performance by introducing an interlocutor-aware transformer model where we learn a dictionary of interlocutor index embeddings to exploit diarized conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is the task of recognizing the speaker's emotional state from their speech. The speaker's emotion might be affected by several factors such as demographics  [1] , age  [2] , and conversational context. Recognizing the speaker's emotion is usually performed on the utterance level, i.e., the model predicts one emotion class for the whole utterance  [3] [4] [5] [6] [7] [8] [9] . The core assumption of this approach is that isolated utterances, generally less than 10s in duration, contain only one speaker and one emotion throughout the utterance. These approaches can be applied to conversational speech, provided that an utterance-level segmentation is available either from another system or a human annotator and assuming the utterance contains only one emotion.\n\nRecently focus has shifted towards conversational emotion recognition (CER)  [10] [11] [12] [13] [14] [15] [16] [17]  to predict emotion more ac-curately using context. One study explores a fixed context (4 recent utterances) and speaker-specific modeling using gated recurrent unit (GRU) architecture  [10] . Their model, referred to as conversational memory network, uses attention mechanism and memory hopping to combine information from multiple streams of representations and to attend to history. The main limitation of this approach is its fixed context and a lack of extensibility to multi-party conversations. Model proposed in  [11] , referred to as DialogueRNN, overcomes limitations of the fixed context and also proposes to use separate GRUs to model speaker, emotion and global context. Authors in  [13]  propose to use graph based neural net by defining utterances and speakers as nodes to exploit context and speaker dependencies. A transformer model with pairwise speaker verification as auxiliary task is proposed in  [12]  to encode context and speaker information into the model hidden representations. Even though the above approaches provide good CER performance, all of them are fundamentally limited by their reliance on the availability of a segmentation of the recording/transcript, and their strong assumptions about each speaker turn consisting of just a single emotion.\n\nCER without requiring segmented recordings is explored in  [14] [15] [16] [17]  by predicting emotional attributes on a framelevel. Authors in  [14]  use a fuzzy logic estimator while  [15]  propose an optimal statistical mapping between audiovisual features and emotion attributes using Gaussian mixture model (GMM). Deep learning models such as CNN and LSTM are used in  [16, 17]  for frame-level prediction.\n\nIn this work, we present transformer-based models for CER by treating it as a sequence labeling task, where short duration frames of the speech signal are assigned emotion labels by a model that looks at the broader context. Based on self-attention operation, we proposed DiverseCatAugment (DCA), an augmentation scheme to improve transformer model performance. We quantified the effect of context by comparing models trained on isolated utterances and conversations. We compared transformer architecture with several neural architectures: ResNet-34, which models context locally in each layer and globally with stacked layers and; BiLSTM, which captures context sequential manner. To leverage both the local and global context modeling strengths of ResNet-34 and transformer architectures, we explore their The rest of the paper is organized as follows. First, we present our models in Section 2 and the proposed DCA augmentation scheme in Section 3. Then, the experimental setup and results are detailed in Section 4 and 5 respectively. Finally, conclusions of our work and future directions are discussed in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Our Models",
      "text": "We present transformer-based models that predict emotion on a frame-level. Our experiments include the use of a basic transformer architecture and also a combination of transformer and a CNN architecture. For all the approaches in this work, we employ MFCC features as input, with 25 ms frame length and 10 ms shift.\n\nBaseline models: The proposed transformer models are compared with two baseline models employed in previous studies: one using a BiLSTM architecture  [18]  and the other employing CNN  [7] . We use BiLSTM and CNN models as the mechanism of exploiting context is different in them compared to the transformer. BiLSTM learns context information in a sequential manner; CNN exploits local context in each layer and global context with a stack of layers; in contrast, the transformer has access to the entire conversational context in every self-attention layer. Also, self-attention operation in the transformer model allows attending other frames with the same emotion in the sequence while convolutional operation treats all frames inside a receptive field in a similar manner disregarding their class label. Our BiLSTM architecture contains a sequence of 6 bi-directional LSTM layers followed by a dropout layer and 2 fully connected layers to obtain logits. For CNN, we use ResNet-34 model architecture reported in  [19]  without pooling layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transformer Model",
      "text": "As the employed frame length is very small to predict an emotion, the context plays a crucial role in deciding the emotion of a frame. Hence, an architecture that can use context efficiently is crucial. Recently, transformer architecture has shown to outperform other neural architectures in several speech and NLP tasks  [20] [21] [22] [23] . It contains a sequence of selfattention operations which are designed to exploit long-range dependencies in the input sequence. Our architecture contains a sequence of 12 self-attention layers as in the standard BERT base model  [21] . A schematic of the transformer model is shown in Fig.  1 . For this model, we use MFCC features as input. As the entire input sequence is processed simultaneously in self-attention layers, the order of the input sequence does not matter. However, the sequence information could be useful for the CER task. We encode position information by learning a set of positional embeddings during training and adding them to the input sequence. For training, we use an input sequence length of 2048 hence we learn 2048 positional embeddings during training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Resnet+Transformer Model",
      "text": "Several studies suggest that down-sampling input representation using convolutional layers before processing with transformer layers provides better results for ASR  [24, 25] . Intuitively, convolutional layers use local context to produce bet-ter contextual features. In this work, we used a pre-trained ResNet-34 to process input MFCC features and fed its output to the transformer layers. ResNet-34 is pre-trained on the speaker classification task. We jointly trained ResNet-34 and transformer to exploit the benefits of both transfer learning and transformer model capabilities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Interlocutor-Aware Resnet+Transformer Model",
      "text": "A conversation is structured as a sequence of turns by all participating speakers. The emotion of a speaker in each turn could depend on that speaker's emotions in previous turns and also on the interlocutor's emotions  [26, 27] . Hence, we expect the model to perform better when the model knows who is speaking when in the conversation.\n\nWe propose to infuse interlocutor information by learning an interlocutor index embedding and adding it to the ResNet embeddings along with positional embeddings. This method assumes the availability of speaker segmented conversations. Our model schematic with interlocutor index embeddings is shown in Fig.  1 . During training, we indexed the speakers in a conversation and represented them with one-hot encoding. Indices are assigned following the order in which the interlocutors appear in the conversation. We passed the one-hot encoding through an embedding layer to get interlocutor index embedding. The embedding layer learns a dictionary of embeddings which acts as a lookup table. The dictionary size is set to the maximum number of speakers that can appear in a training sample. Then, we added interlocutor index embeddings to the ResNet output to incorporate interlocutor information into the transformer layers. With this additional information, we expect the model learns to distinguish the speakers in the conversation and make accurate emotion predictions. Note that this approach does not assume known speakers at the test time, it only assumes speaker diarized recording. This model can deal with multi-party conversations but during test time it is limited by the maximum number of speakers seen in a training conversation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Diverse Category Augment Scheme",
      "text": "In this section, we present a data augmentation scheme, named as DiverseCatAugment (DCA), motivated by the inner workings of the self-attention operation. Given an input sequence of vectors X = [x 1 , x 2 , ..., x N ], we perform self-attention operation and obtain a sequence of vectors Y = [y 1 , y 2 , ..., y N ]. Self-attention operation (dot-product variation) as shown in (1) includes finding dot-product between every vector in the sequence i.e., X • X T . On the dot-product matrix, the softmax operation is employed to obtain normalized similarities for each vector with other vectors in the sequence. Then, the dot product matrix is multiplied with the input sequence X to obtain Y. In essence, every vector in Y is a weighted sum of vectors in X, as shown in  (2)  with weights being the normalized similarities with other vectors in the sequence as shown in  (3) .\n\n[w i1 , w i2 , ..., w iN ] = softmax(\n\n(3) Attention operation allows us to attend relevant vectors in the sequence by assigning higher weights and discard irrelevant vectors using lower weights. For a given vector, say x i , if all the weights/similarities ([w i1 , w i2 , ..., w iN ]) are in a narrow range, it implies that all the vectors in X are equally relevant to x i . This could happen if all the vectors in the sequence belong to the same class. In this case, the attention operation acts as, effectively, an averaging operation instead of a weighted average. Consequently, we may not be exploiting transformer abilities to the maximum level. Based on this insight, we hypothesize that input sequences with less categorical variety hinder transformer model performance. Equivalently, training data with input sequences containing diverse emotion classes provide better performance compared to sequences with less emotional diversity.\n\nWe validate our hypothesis by proposing a data augmentation scheme, referred to as DiverseCatAugment (DCA), which improves the diversity of categories/emotions in the input sequences. We apply DCA on conversations as well as isolated utterances. When applying DCA to conversations, we choose two conversations and concatenate them for model training. For example, assume one conversation is filled with angry for most of the time and another with happy category. Concatenation of the two conversations results in a sequence with both angry and happy. It is easy to see that the concatenated conversations have a more diverse composition of emotions. According to the DCA hypothesis, proposed transformer models perform better if input sequences have diverse categories.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We performed CER on the widely used IEMOCAP dataset, which contains 150 dyadic conversations between 5 female and 5 male speakers. Each conversation is set up between one male and one female, and are approximately 5 min long.\n\nThe scripts and topics for spontaneous conversations were selected to elicit emotions. Even though only 5 emotions -Angry, Frustration, Happy, Neutral, and Sad -are targeted for elicitation, more emotions albeit less frequently are found in the annotation process. In this work, we used only the most frequent emotions, -Angry, Frustration, Happy, Neutral, and Sad -for classification. We merged Excitation emotion with Happy as is commonly done for this dataset. To facilitate comparison between models trained with isolated utterances and conversations, we discarded segments in the conversations which have labels other than the considered emotions. As the dataset contains only 5 sessions, we use 3 sessions for training, 1 for development, and 1 for testing. We performed a 5-fold cross-validation (leave-one-out-session) and report the weighted average f1-score (micro-f1) score for our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dca Implementation",
      "text": "We implement DCA augmentation during the formation of the batch for model training. We first choose a batch of 6 conversations and pick a sequence of length 1024 from each of the conversations. Then, we randomly pair each conversation with one of the other 5 conversations to form a sequence of length 2048 for model training. We note that to maximize DCA utility, conversations with distinct emotions should be selected for concatenation but as we train the model for 100 epochs, the model sees a fairly high number of sequences with diverse emotions. DCA on conversations produces sequences with conversational context preserved for most of the sequence and adds a bit of random context. When applying DCA on isolated utterances, we concatenate multiple isolated utterances until we obtain 2048 length sequences. We choose isolated utterances for concatenation randomly to result in a sequence with diverse emotions expressed by multiple speakers. DCA on isolated utterances results in sequences similar to conversations but without conversational context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Impact Of The Context",
      "text": "To gain insights into the model capabilities and importance of context, we compare the transformer model with ResNet-34 and BiLSTM using 4 types of training data: We evaluate all the models on conversations. We compare models trained with Isolated utterances (no context) and Conversations to understand the impact of conversational con-text on the CER performance. To further improve the performance, we employ DCA on Isolated utterances and Conversations.\n\nBased on the DCA method hypothesis, we expect DCA Conversations and DCA Isolated utterances to perform better than Conversations and Isolated utterances respectively. Also, as context could help to disambiguate emotions, we expect models trained on conversational data (2 nd and 4 th types) to perform better than models trained on isolated utterances data (1 st and 3 rd types). The performance of models trained with isolated utterances enables us to answer the question of \"how well can we perform CER without access to the conversational data?\". The answer to this question is important because most of the current datasets have only isolated utterances and a lot of past research efforts focused on them.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results With Dca Augmentation And Context",
      "text": "Table  1  shows the results with DCA augmentation and context. The first and second rows, denoted with Isolated and Conversations, show the results of models trained with isolated utterances and conversations. We can observe that models trained on isolated utterances perform worse than the models trained on conversations suggesting the importance of context. BiLSTM seems to predict just a little better than chance when trained on isolated utterances. The impact of conversational context on the BiLSTM model is comparatively higher than ResNet and transformer. Among the architectures, the transformer model outperformed ResNet and BiLSTM in every case with the best performance of 42% when trained on conversations.\n\nModels trained with DCA augmentation are denoted with DCA Isolated utterances and DCA Conversations. We can observe that along with the transformer model, ResNet and BiLSTM also perform better with DCA augmentation on isolated utterances suggesting that emotional variety in the training sequences helps to discriminate emotions well. On isolated utterances, ResNet and BiLSTM models perform 3.9% and 13.1% absolute better with DCA augmentation. However, they perform worse in comparison to Conversations suggesting that original conversational context is more important than categorical/emotional variety in the training sequences. Interestingly, the transformer model trained with DCA Isolated utterances performs better than Conversations. Upon further investigation into the conversations, we found that many conversations are dominated by a single emotion. Fig.  2  shows proportions of emotions for a subset of 38 conversations (25% of the dataset) in the IEMOCAP dataset. Each bar represents the proportion of emotions in a single conversation. We can observe that these conversations have only one emotion dominating for more than 75% of the conversation time. In literature, this phenomenon is referred to as emotional inertia  [28]   which states humans naturally tend to resist changing emotions.\n\nEmotional inertia in the conversations explains the better performance with DCA Isolated utterances compared to Conversations even though the latter has conversational context. It also implies that emotional variety in the training sequences is important for the transformer model confirming the DCA augmentation hypothesis. Better (3.3% absolute) performance with DCA Conversations over Conversations further strengthens the DCA augmentation hypothesis.\n\nOverall, we observed that training the models with random context is better than no context. Access to the conversational context further improved our models' performance. Transformer model trained with conversations and DCA augmentation performed best with a micro-f1 of 45.3%.   show an analysis of our model's row-normalized confusion matrix in Fig.  3 . We can observe that our model is confusing Angry with Frustration 37.6% of the frames and Neutral with other emotions 67.9% of the frames. Angry and Frustration seem much more similar to each other than to any other emotion in the label set, hence we wondered whether there could be some confusion between them for annotators too. Looking at inter-annotator agreement, we found that when the annotation of each crowd-sourced worker is matched against their majority-voted annotation, Angry is found to be confused with Frustration 17% and Frustration with Angry 11% of total segments  [29]  which are significantly high compared to any other emotion. These confusion rates in the ground-truth annotations would explain our model's confusion to some extent.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fig. 3: Confusion Matrix Of Resnet+Transformer Model",
      "text": "To understand the confusion of Neutral emotion with others, we investigated the trigram probabilities of emotions in the conversations. Fig.  4  shows dataset statistics for a subset of trigrams of the form (neighbor-emotion, central-emotion, neighbor-emotion). These statistics are computed from sequences of turn/segment emotions. Each row is normalized for analysis purposes. If central-emotion is equal to neighboremotion then we call the trigram as homogeneous, and heterogeneous otherwise. From Fig.  4 , we can observe that the majority of trigrams are homogeneous (diagonal values) except when the central-emotion is Neutral. Approximately 54.3% (100%-45.7%) of the trigrams are heterogeneous for Neutral emotion compared to 38.5%, 37.5%, 7.5%, and 13.1% for Angry, Frustration, Happy, and Sad respectively. We speculate that the heterogeneous nature of Neutral in this dataset could be one reason why our model confuses with other emotions more often -it simply \"prefers\" to recognize longer contiguous segments with a single emotion, mislabeling Neutral in the process. This observation is consistent with the hypothesis posed by  [7]  that neutral utterances are perceived as emotional when presented in the context of another emotional utterance. However, whether this behavior is because of the dataset characteristics or the acoustic characteristics of Neutral emotion warrants further analysis which we plan to address in future work.\n\nFig.  4 : Probabilities of trigrams of the form (neighboremotion, central-emotion, neighbor-emotion). The labels ang, fru, hap, neu, and sad stand for Angry, Frustration, Happy, Neutral, and Sad respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Results With Interlocutor-Aware Resnet+Transformer",
      "text": "Table  3  presents the results of models trained with and without interlocutor information. We can observe a 3.2% absolute improvement in micro-f1 with the addition of interlocutor information to the model training. With the interlocutor information, the model could be able to distinguish the emotions of interlocutors from the past and future to predict the emotion of the current location. The class-wise analysis further revealed that a significant contribution to the overall gain is from Neutral and Angry emotions with 20% and 15% relative improvements. We suspect the higher improvements specifically for Neutral and Angry emotions are because of their higher percentage of heterogeneous trigrams (refer to the discussion in Section 5.2). As heterogeneous trigrams are formed from a sequence of three turns, by definition, the central turn has different emotion and speaker compared to neighbor turns. We think the interlocutor-aware model can better distinguish central turn's emotion from neighbor turns' emotions as it has access to the speaker diarization information.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this work, we presented transformer-based models for conversational emotion recognition (CER). Our analysis on the impact of context showed that models trained with random conversational context perform better on conversations than those trained without context from other speakers. We found that less diversity of emotions/categories in the input sequences limits the transformer model performance. Our proposed data augmentation scheme which aims to improve diversity has helped to discriminate the emotions better. Conversational context and diversity of emotions provided the best results when using transformers. The proposed transformer-based approaches always outperformed the baseline architectures ResNet-34 and BiLSTM. We presented a model combining ResNet-34 and transformer architecture to exploit local and global context, that provides better results than the model based on transformer only. We have shown that a model with interlocutor information improves the CER performance.\n\nExcept for the interlocutor-aware transformer model, models presented in this work do not assume a single emotion per turn and can deal with multi-party conversations without requiring speaker-diarized recordings. The interlocutoraware transformer model requires speaker-diarized recording and the number of speakers in a conversation at test time can not be more than the maximum number of speakers seen in a training conversation. In the future, we plan to propose models to overcome this limitation. In this work, we evaluated the proposed methods on the IEMOCAP corpus. Some of the shortcomings of this corpus are its limited number of speakers and its collection in controlled settings. We plan to evaluate our models on more spontaneous conversations data with more speakers such as MELD  [30] .",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Transformer block diagram.",
      "page": 2
    },
    {
      "caption": "Figure 1: For this model, we use MFCC features as",
      "page": 2
    },
    {
      "caption": "Figure 1: During training, we indexed the speakers in",
      "page": 3
    },
    {
      "caption": "Figure 2: Proportion of emotions in a subset of 38 IEMOCAP",
      "page": 5
    },
    {
      "caption": "Figure 3: We can observe that our model is confusing",
      "page": 5
    },
    {
      "caption": "Figure 3: Confusion matrix of ResNet+Transformer model",
      "page": 5
    },
    {
      "caption": "Figure 4: shows dataset statistics for a subset",
      "page": 5
    },
    {
      "caption": "Figure 4: , we can observe that the ma-",
      "page": 5
    },
    {
      "caption": "Figure 4: Probabilities of trigrams of the form (neighbor-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the results with DCA augmentation and con-",
      "page": 4
    },
    {
      "caption": "Table 1: Effect of context on the CER performance (micro-",
      "page": 5
    },
    {
      "caption": "Table 2: compares the results of the ResNet+Tranformer",
      "page": 5
    },
    {
      "caption": "Table 2: Results of joint ResNet and transformer training.",
      "page": 5
    },
    {
      "caption": "Table 3: presents the results of models trained with and with-",
      "page": 6
    },
    {
      "caption": "Table 3: Inﬂuence of interlocutor information on the perfor-",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "3",
      "title": "How aging affects the recognition of emotional speech",
      "authors": [
        "S Paulmann",
        "M Pell",
        "S Kotz"
      ],
      "year": "2008",
      "venue": "Brain and language"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "6",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "8",
      "title": "CopyPaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Żelasko",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "End-to-end speech emotion recognition combined with acoustic-to-word asr model",
      "authors": [
        "H Feng",
        "S Ueno",
        "T Kawahara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "10",
      "title": "Speech sentiment analysis via pre-trained features from end-toend asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C.-C Chiu",
        "J Fan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "North American Chapter. Meeting"
    },
    {
      "citation_id": "12",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "15",
      "title": "Primitives-based evaluation and estimation of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "16",
      "title": "Tracking continuous emotional trends of participants during affective dyadic interactions using body language and speech information",
      "authors": [
        "A Metallinou",
        "A Katsamanis",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "A Graves",
        "B Schuller",
        "E Douglas-Cowie",
        "R Cowie"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "18",
      "title": "Continuous emotion recognition in speech-do we need recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Sixteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "20",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "arxiv": "arXiv:2002.05039"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "23",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "J Chaumond",
        "L Debut",
        "V Sanh",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "M Funtowicz",
        "J Davison",
        "S Shleifer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "24",
      "title": "A comparative study on transformer vs rnn in speech applications",
      "authors": [
        "S Karita",
        "N Chen",
        "T Hayashi",
        "T Hori",
        "H Inaguma",
        "Z Jiang",
        "M Someki",
        "N Soplin",
        "R Yamamoto",
        "X Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "25",
      "title": "Exploring transformers for large-scale speech recognition",
      "authors": [
        "L Lu",
        "C Liu",
        "J Li",
        "Y Gong"
      ],
      "year": "2020",
      "venue": "Exploring transformers for large-scale speech recognition",
      "arxiv": "arXiv:2005.09684"
    },
    {
      "citation_id": "26",
      "title": "Transformers with convolutional context for asr",
      "authors": [
        "A Mohamed",
        "D Okhonko",
        "L Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Transformers with convolutional context for asr",
      "arxiv": "arXiv:1904.11660"
    },
    {
      "citation_id": "27",
      "title": "Emotional contagion",
      "authors": [
        "E Hatfield",
        "J Cacioppo",
        "R Rapson"
      ],
      "year": "1993",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "28",
      "title": "Emotions amplify speaker-listener neural alignment",
      "authors": [
        "D Smirnov",
        "H Saarimäki",
        "E Glerean",
        "R Hari",
        "M Sams",
        "L Nummenmaa"
      ],
      "year": "2019",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "29",
      "title": "Emotional inertia and psychological maladjustment",
      "authors": [
        "P Kuppens",
        "N Allen",
        "L Sheeber"
      ],
      "year": "2010",
      "venue": "Psychological science"
    },
    {
      "citation_id": "30",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "31",
      "title": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}