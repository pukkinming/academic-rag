{
  "paper_id": "2406.08931v2",
  "title": "Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues In Multitask Learning",
  "published": "2024-06-13T09:00:14Z",
  "authors": [
    "Arnav Goel",
    "Medha Hira",
    "Anubha Gupta"
  ],
  "keywords": [
    "Speaker emotion recognition",
    "co-attention",
    "multitask learning",
    "new dataset"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In her seminal work on affective computing, Picard asserts that computers can achieve genuine intelligence and natural interactivity if we empower them with the ability to recognize and understand emotions  [1] . Besides working with vision-based cues such as facial expressions and hand movements, humans excel in discerning emotions even when only auditory information is available  [2] . This ability highlights the nuanced and adaptable nature of human emotional understanding, capable of discerning and processing emotions across a wide spectrum of speakers and voice types. Building on this premise, it is imperative to extend the capabilities of Speaker Emotion Recognition (SER) systems beyond speakers on which they have been trained.\n\nTraditional SER systems rely on features including pitch, energy, MFCCs, and spectrograms for emotion recognition  [3, 4] . With the emergence of deep learning methods, systems employing CNNs, Bi-Directional RNNs, and LSTMs are able to learn discernible features  [5, 6, 7] . Transformer-based models, a more recent development, marked a significant advancement with the introduction of large pre-trained models (PTMs) trained under a self-supervised learning framework  [8] . Recent advancements in weakly-supervised models, such as Whisper  [9] , which are trained on extensive corpora, have demonstrated superior performance on a diverse array of downstream tasks  [10, 11, 12] . Attention mechanisms, including cross-attention  [13] , windowed-attention  [14] , and self-attention  [15]  along with multitask training have been explored to enhance the performance of SER  [16, 17, 18] . Despite these advances, a critical † Equal contribution; # Corresponding Author challenge remains: the inability of modern SER systems to effectively adapt to unseen scenarios and speakers, resulting in performance that is inferior to human capabilities  [19, 20] .\n\nThis study contributes to the field by benchmarking various PTM embeddings in a transfer learning framework, specifically addressing unseen speaker recognition on five existing benchmark datasets and the 6th newly released dataset with this work. Although co-attention based fusion mechanisms have been used previously on the speaker emotion recognition downstream task for fusing features from multiple modalities  [21]  and multi-level acoustic information  [22] , their use on unseen speaker emotion recognition tasks along with multitask learning is yet to be thoroughly explored. This study addresses this gap by proposing an architecture that fuses features from the frequency domain and PTM embeddings.\n\nMoreover, the variation in emotional expression across languages poses a distinctive challenge in multilingual SER, which is compounded by the scarcity of comprehensive datasets. To address this gap, we introduce a novel Hindi SER dataset, designed to enhance model training and benchmarking in Indian linguistic contexts. To the best of our knowledge, this is the first open-source Hindi SER dataset. Extending our efforts, we apply our methodology to French and German datasets, positioning our work as the first to benchmark Whisper's encoder in multilingual SER settings. The codes and dataset can be found on GitHub  1  .\n\nThe key contributions of this work are three-fold: 1. We introduce BhavVani, the first-ever Hindi Speech Emotion Recognition dataset with over 9,000 utterances. 2. Our research uniquely benchmarks pre-trained model embeddings across six datasets in four languages (English, German, French, Hindi) for unseen speaker recognition, marking the first study of its kind to explore these embeddings, including Whisper, on this downstream task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "We Introduce Camulenet, A Co-Attention Based Multitask",
      "text": "Learning Network architecture that fuses frequency domain features with PTM features in a multitask framework of emotion and gender recognition, aiming to derive generalized representations for enhanced speaker emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methodology",
      "text": "The proposed CAMuLeNet architecture described in Figure1, aims to fuse traditional frequency domain features of the spectrogram and MFCCs, with the features extracted from a pretrained Whisper encoder using the co-attention mechanism described next. We train this architecture through a multitask setup to improve performance on unseen speakers' emotion recognition. pooling. An audio clip x is preprocessed through padding and filtering for consistent sequence length, followed by feature extraction via an AlexNet encoder  [23] , treating the spectrogram as an image. The latent embeddings from AlexNet is a one-dimensional feature vector x ′ s of size 4096. Concurrently, MFCCs undergo processing through a Bidirectional Gated Recurrent Unit (Bi-GRU) with two 256-sized hidden layers and a 0.2 dropout, producing a one-dimensional embedding x ′ m sized (seq len ×512).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Extracting Features Through Transfer Learning",
      "text": "OpenAI's Whisper 2  , a multilingual encoder-decoder model, exhibits state-of-the-art performance across various speech-to-text benchmarks. Trained on a vast and diverse audio dataset, it significantly improves speech recognition and translation tasks. We hypothesize that Whisper's encoder produces rich latent representations of audio samples. We pass the preprocessed audio clip x through the Whisper Encoder (which converts it into a mel spectrogram for processing), obtaining a 2D latent representation xw ∈ R L×W . We refrain from using pooling on this 2D representation to maintain the time-frequency information of these embeddings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Co-Attention Based Fusion",
      "text": "The three embeddings hold vital knowledge representation that we aim to fuse for improved performance. First, the derived embeddings from the spectrogram x ′ s and MFCC features x ′ m are passed through a fully connected (FC) layer with the same output dimension:\n\nwhere xs att ∈ R 1×T , xm att ∈ R 1×T , and T (= 1024) is a hyperparameter. The two transformed one-dimensional embeddings are concatenated to create a one-dimensional vector and passed through another FC layer:\n\nwhere xsm att ∈ R 1×L . The activated concatenated features from (  2 ) are sent through a FC layer activated by ReLU and sent through a Layer Norm. This output is multiplied with Whisper features xw to generate attention-weighted Whisper features\n\nwhere xw att ∈ R 1×W . Attention-weighted features are sent through a 3-layer network with 0.15 dropout, an FC layer with ReLU activation, and a terminal Layer Norm. These processed features, along with activated features from (2) sent via a skipconnection, are concatenated to form the network's final embedding for downstream classification. This co-attention mechanism attends frequency domain information with frame-level features from the Whisper encoder.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multitask Learning",
      "text": "Utilizing multitask learning, our network concurrently trains on emotion and gender recognition, leveraging the significant influence of speaker gender on emotion. This approach aims to develop nuanced latent representations that captures intricate emotion-speaker correlations. We use categorical cross entropy loss Lcce for multiclass emotion recognition and binary cross entropy loss L bce for gender recognition, reflecting the prevalent binary gender annotation. The combined multitask objective is defined as\n\nwith α and β as tunable weights for each task and γ as a tunable parameter providing stability while training. Experimentally, setting α three times higher than β effectively balances training across tasks, attributed to the class count ratio and the relative simplicity of emotion recognition, thus imposing a higher penalty for better learning from the latter.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Benchmark Datasets",
      "text": "This study utilizes five widely-used multilingual datasets (English, German, French) to assess pre-trained model embeddings and our methodology for unseen speaker emotion recognition.\n\nThey are summarised next.\n\nThe CREMA-D dataset  [24] , featuring 7,442 clips from 91 voice actors (48 males, 43 females), offers recordings of 12 sentences expressed in 6 emotions in English. The IEMOCAP dataset  [25]  includes 10,039 clips from 10 actors (5 males, 5 females) in both scripted and spontaneous conversations, annotated with 4 emotion classes. The RAVDESS  [26]  dataset comprises 1,440 recordings from 24 actors (12 males, 12 females), each articulating two sentences across 8 emotional states. Each recording features one speaker expressing a single emotion, with equal distribution of male and female voices across the dataset. Among these, CREMA-D stands out with the largest variety of unique speakers from various ethnicity and is the largest corpus of speech samples among the existing datasets.\n\nWe utilize EmoDB  [27]  and CaFE  [28]  datasets for German and French languages, respectively. EmoDB includes 535 recordings from 10 actors (5 males, 5 females), featuring 10 sentences in 7 emotional states. CaFE provides a collection of 936 clips from 12 actors (6 males, 6 females), each expressing 6 sentences across 7 emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Our Novel Hindi Ser Dataset: Bhavvani",
      "text": "In response to the under-representation of Indic languages in Speech Emotion Recognition (SER) and their absence in multilingual benchmarks such as SERAB  [29] , we release BhavVani, a novel dataset tailored for Hindi SER. This initiative aims to enrich the SER research landscape by incorporating the linguistic and cultural diversity of Indian languages, leveraging their morphological richness and unique emotional expressiveness.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Collection",
      "text": "The BhavVani dataset comprises approximately 13 hours of audio across 8734 utterances, with an average clip length of 5.08 seconds. The audio clips are curated from the popular Indian sitcom \"Sarabhai vs Sarabhai\", sourced from prior work that used the show's text for sarcasm detection tasks  [30] . To our knowledge, BhavVani is the first open-source Hindi dataset tailored for speaker emotion recognition. The dataset will be released to the community for further work. Each utterance is a single-speaker dialogue, annotated into one of seven categories: Neutral, Surprise, Enjoyment, Disgust and Anger, Fear, Sadness based off Ekman's seven basic human emotions  [31] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Annotation And Validation",
      "text": "The BhavVani dataset was annotated by 18 native Hindi speakers, who received preliminary training to ensure annotation consistency and accuracy. Each annotator evaluated approximately 750 audio clips, identifying the speaker's gender, name, and perceived emotion while excluding clips with excessive background noise or multiple speakers. To uphold the integrity of the dataset, a rigorous validation procedure was followed involving three independent annotators reviewing each annotation. Annotations that were agreed by all three annotators, were marked as confirmed, while those with disagreements were re-evaluated, if necessary, and an alternative emotion was suggested. This meticulous annotation and validation method, evidenced by a Fleiss Kappa score of 0.637, highlights substantial annotator agreement, ensuring the reliability and quality of the dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Statistics",
      "text": "The pie-charts in Figure  2  provide insights of the thoroughly annotated and curated dataset that had no missing labels. The dataset is balanced with male and female speakers. The gender has been annotated for all speakers, while the speaker name has been annotated primarily for the main characters. None of the other characters had an individual occurrence greater than 41, hence they were positioned under the umbrella of 'Others'. We employed encoders from pre-trained models (PTMs) such as Wav2Vec2.0  [32] , WavLM  [33] , HuBERT  [34] , and Whisper  [9]  as baselines via transfer learning. Initially trained for ASR tasks, Wav2Vec2.0, WavLM, and HuBERT are transformerbased models with 95M parameters each trained through selfsupervised learning on English datasets. We fine-tuned these models for multilingual SER using CTC loss. Whisper, distinguished as an encoder-decoder model, is trained with weak supervision on a diverse 680000-hour multilingual dataset, with our focus on its Base (74M) and Medium (769M) variants. We adapt Whisper models for SER by converting their 2D timefrequency outputs into 1D embeddings via average pool. Chosen for their excellence in speech processing, their potential with unseen speakers is yet to be fully explored.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Experiment Setup",
      "text": "In the baseline experiment, we provided 1D-embedding obtained in Section 4.1 as input to a CNN based feature extractor comprising of a 1D convolutional layer, batch normalization, ReLU activation, dropout (0.3), and max pool followed by flattening and two fully connected (FC) layers for classification. We utilized cross-entropy loss and Adam optimizer with a learning rate of 10 -4 as higher rates led to model overshoot, rapid loss increase, and early underfitting within a few epochs. We trained the models on a NVIDIA A5000 GPU. Training was capped at 20 epochs with early stopping based on validation loss to prevent overfitting. For evaluating performance on unseen speakers, we followed 10-fold leave-speaker-out cross validation, wherein each dataset was segmented into 10 folds with each fold containing unique speakers. Thus, CREMA-D had around 9 new speakers at the time of validation that the model had not seen at the training time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Camulenet Training Setup",
      "text": "We extracted MFCC and spectrogram features from the preprocessed audio waveform. The spectrogram is derived by applying a Hamming window based short-time Fourier transform, with a window length of 40, a hop length of 10, setting the size of the FFT window to 800. The calculation of MFCCs involved generating 40 MFCC values with a hop length of 160, ensuring that these coefficients were compatible with the Hidden Markov Model Toolkit (HTK). The training of our model architecture was conducted using a NVIDIA A5000 GPU using batches of 64, leveraging the Adam optimizer with a learning rate of 5 × 10 -5 . Dropout was maintained at 0.15 throughout the network. In the context of multitask learning, we determined experimentally that the model achieved optimal stability across different datasets when the weighting factors were set to α = 0.4, β = 0.1 and γ = 0.2. These values, however, may require tuning to accommodate variations in dataset characteristics and training configurations. Remaining conditions are as described in Section-4.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "This Section details our comparative analysis of baseline and proposed methods from 4.2, quantified by Weighted Accuracy (WA) and Weighted F1 score (WF1) across six datasets, presented in Table  1 , complemented by an ablation study discussed later. We report mean metric values averaged over ten folds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analysis Of Baseline Transfer Learning Results",
      "text": "As per Table-1, Whisper-Medium shows a huge performance jump over self-supervised based PTMs and its base architecture. The increase is noteworthy on the IEMOCAP (13% over WavLM), CaFE (18% over Whisper-Base) and BhavVani (14% over WavLM). The increase is higher on French language dataset due to Whisper being trained on a lot of audio chunks from the French language. Objective performance, however, remains sub-optimal on our Hindi SER dataset indicating the need for interventions to create more robust models and resources for Indic languages.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analysis Of Results On Camulenet",
      "text": "The baseline analysis indicated Whisper-Medium as an optimal pre-trained embedding for our co-attention fusion method. As detailed in Section 4. Furthermore, the results on multilingual benchmarks, such as CaFE, mirrored the performance gains observed in baseline experiments, underscoring the generalizability of our approach. The multi-task training paradigm notably amplified model's performance on gender recognition tasks, achieving over 95% accuracy on all benchmarks. These findings confirm the effectiveness of our multi-task and co-attention strategy to improve the model's performance on unseen speakers from various linguistic backgrounds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "Our proposed methods utilise features from the frequency domain and latent representations generated by a Whisper encoder. Table  1  additionally shows our ablation study by removing the multi-task training setup (1) and then additionally removing co-attention to fuse the features (2). We observed that removing multitask training and replacing it with a single-task training setup reduces performance by an average of approximately 4% over CAMuLeNet. Replacing co-attention based feature fusion with normal concatenation reduced performance across all benchmark datasets below the best-performing baseline methods by an average of around 10% over CAMuLeNet. This could be attributed largely to performing concatenation without accounting for the underlying correlations, emphasising the importance of using co-attention based fusion methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Limitations And Conclusion",
      "text": "This study exposes a critical limitation in SER for low-resource languages through our novel BhavVani benchmark, where gains were modest and overall performance was subdued, emphasizing the challenges in unseen speaker recognition. Future work will prioritize these languages and investigate alternative fusion mechanisms for robust generalizations. Our contributions include the introduction of BhavVani dataset, comprehensive benchmark of pretrained embeddings across six datasets using a rigorous 10-fold leave-speaker-out cross-validation strategy, and the novel CAMuLeNet architecture, which synergizes frequency domain features with PTM Whisper embeddings through co-attention based feature fusion and multitask training. These efforts spotlight the issue of emotion recognition of unseen speakers for advancing research in this field.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: CAMuLeNet: Co-Attention based Multitask Learning Network",
      "page": 2
    },
    {
      "caption": "Figure 2: provide insights of the thoroughly",
      "page": 3
    },
    {
      "caption": "Figure 2: BhavVani Dataset Statistics",
      "page": 3
    },
    {
      "caption": "Figure 3: t-SNE visualisation of Feature Distribution. (a) Dis-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PTM": "",
          "CREMA-D": "WA\n(↑)",
          "IEMOCAP": "WA\n(↑)",
          "RAVDESS": "WA\n(↑)",
          "EmoDB": "WA\n(↑)",
          "CaFE": "WA\n(↑)",
          "BhavVani": "WA\n(↑)"
        },
        {
          "PTM": "Wav2Vec2.0\nHuBERT\nWavLM\nWhisper-Base\nWhisper-Medium",
          "CREMA-D": "0.451\n0.529\n0.587\n0.603\n0.666",
          "IEMOCAP": "0.427\n0.461\n0.512\n0.573\n0.648",
          "RAVDESS": "0.396\n0.587\n0.654\n0.593\n0.713",
          "EmoDB": "0.469\n0.486\n0.806\n0.743\n0.831",
          "CaFE": "0.417\n0.424\n0.444\n0.452\n0.623",
          "BhavVani": "0.251\n0.253\n0.303\n0.261\n0.412"
        },
        {
          "PTM": "Architecture",
          "CREMA-D": "Experiments on Co-Attention and Multitask Learning",
          "IEMOCAP": "",
          "RAVDESS": "",
          "EmoDB": "",
          "CaFE": "",
          "BhavVani": ""
        },
        {
          "PTM": "CAMuLeNet (Ours)\nOurs (w/o Multitask)\nOurs (w/o Co-Att & Multitask)",
          "CREMA-D": "0.762\n0.768\n0.734\n0.728\n0.823\n0.826\n0.862\n0.847\n0.709\n0.691\n0.453\n0.441",
          "IEMOCAP": "",
          "RAVDESS": "",
          "EmoDB": "",
          "CaFE": "",
          "BhavVani": ""
        },
        {
          "PTM": "",
          "CREMA-D": "Ablation Study",
          "IEMOCAP": "",
          "RAVDESS": "",
          "EmoDB": "",
          "CaFE": "",
          "BhavVani": ""
        },
        {
          "PTM": "",
          "CREMA-D": "0.719\n0.721\n0.703\n0.697\n0.782\n0.784\n0.853\n0.844\n0.672\n0.683\n0.431\n0.429\n0.671\n0.653\n0.659\n0.662\n0.721\n0.728\n0.817\n0.793\n0.605\n0.593\n0.407\n0.398",
          "IEMOCAP": "",
          "RAVDESS": "",
          "EmoDB": "",
          "CaFE": "",
          "BhavVani": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "3",
      "title": "Human abilities: Emotional intelligence",
      "authors": [
        "J Mayer",
        "R Roberts",
        "S Barsade"
      ],
      "year": "2008",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "4",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Speech Emotion Recognition Using Spectrogram & Phoneme Embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "7",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "8",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "11",
      "title": "Multilingual prosody transfer: Comparing supervised & transfer learning",
      "authors": [
        "A Goel",
        "M Hira",
        "A Gupta"
      ],
      "venue": "Multilingual prosody transfer: Comparing supervised & transfer learning"
    },
    {
      "citation_id": "12",
      "title": "End-to-end speech recognition: A survey",
      "authors": [
        "R Prabhavalkar",
        "T Hori",
        "T Sainath",
        "R Schlüter",
        "S Watanabe"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Crossvoice: Crosslingual prosody preserving cascade-s2st using transfer learning",
      "authors": [
        "M Hira",
        "A Goel",
        "A Gupta"
      ],
      "venue": "Crossvoice: Crosslingual prosody preserving cascade-s2st using transfer learning"
    },
    {
      "citation_id": "14",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Y He",
        "N Minematsu",
        "D Saito"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Dwformer: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Self-Attention for Speech Emotion Recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Speech Emotion Recognition with Multi-Task Learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Speaker attentive speech emotion recognition",
      "authors": [
        "C Moine",
        "N Obin",
        "A Roebel"
      ],
      "year": "2021",
      "venue": "Speaker attentive speech emotion recognition",
      "arxiv": "arXiv:2104.07288"
    },
    {
      "citation_id": "21",
      "title": "Designing and evaluating speech emotion recognition systems: A reality check case study with iemocap",
      "authors": [
        "N Antoniou",
        "A Katsamanis",
        "T Giannakopoulos",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Knowledge-aware bayesian coattention for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "26",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "27",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "28",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "29",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM multimedia systems conference"
    },
    {
      "citation_id": "30",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "When did you become so smart, oh wise one?! sarcasm explanation in multi-modal multi-party dialogues",
      "authors": [
        "S Kumar",
        "A Kulkarni",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2022",
      "venue": "When did you become so smart, oh wise one?! sarcasm explanation in multi-modal multi-party dialogues",
      "arxiv": "arXiv:2203.06419"
    },
    {
      "citation_id": "32",
      "title": "Basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "33",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    }
  ]
}