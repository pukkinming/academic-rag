{
  "paper_id": "2210.06701v1",
  "title": "Empirical Evaluation Of Data Augmentations For Biobehavioral Time Series Data With Deep Learning",
  "published": "2022-10-13T03:40:12Z",
  "authors": [
    "Huiyuan Yang",
    "Han Yu",
    "Akane Sano"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning has performed remarkably well on many tasks recently. However, the superior performance of deep models relies heavily on the availability of a large number of training data, which limits the wide adaptation of deep models on various clinical and affective computing tasks, as the labeled data are usually very limited. As an effective technique to increase the data variability and thus train deep models with better generalization, data augmentation (DA) is a critical step for the success of deep learning models on biobehavioral time series data. However, the effectiveness of various DAs for different datasets with different tasks and deep models is understudied for biobehavioral time series data. In this paper, we first systematically review eight basic DA methods for biobehavioral time series data, and evaluate the effects on seven datasets with three backbones. Next, we explore adapting more recent DA techniques (i.e., automatic augmentation, random augmentation) to biobehavioral time series data by designing a new policy architecture applicable to time series data. Last, we try to answer the question of why a DA is effective (or not) by first summarizing two desired attributes for augmentations (challenging and faithful), and then utilizing two metrics to quantitatively measure the corresponding attributes, which can guide us in the search for more effective DA for biobehavioral time series data by designing more challenging but still faithful transformations. Our code and results are available at Link.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning performs remarkably well in many fields, including computer vision (CV), natural language processing (NLP), and recently time series-related tasks  [6, 10, 32] . Those successful applications increasingly inspire researchers to embrace deep learning for solving issues in human-centered applications that use physiological and be-havioral time series data. However, the superior performance of deep models relies heavily on the availability of a large number of training data, but unfortunately, many human centered applications (i.e., healthcare tasks) usually do not have enough labeled samples, which may limit the wide adaptation of deep models to various computing tasks.\n\nAs an effective technique to increase the data variability and thus train deep models with better generalization, data augmentation (DA) is a critical step for the successful applications of deep learning models. While DA can yield considerable performance improvements, they do require domain knowledge and are task-and domain-dependent. For example, image rotation, a likely class-preserving behavior, is designed to rotate the input by some number of degrees. The image's class can still be recognized by humans, thus allowing the model to generalize in a way humans expect it to generalize. However, such an effective random anglebased rotation operation may not be applicable to other domains, i.e., wearable data. In addition, searching for the most effective DA methods for a new dataset is very timeconsuming, and this motivated the proposal of several automatic DA search algorithms  [4, 5, [14] [15] [16] .\n\nThe existing DA literature mainly focuses on computer vision, but its application to other domains, i.e, biobehavioral time series data, is understudied. A few works investigated the effectiveness of basic DA methods for time series and wearable data  [1, 11, 30, 32] . However, those works only investigated the very basic DAs, leaving the more recent DA techniques (i.e., automatic DA) unexplored. More importantly, it is still an open question of why a DA method works, and how to quantify its effectiveness. Therefore, in this paper, we first systematically review various basic DA methods for biobehavioral time series data, evaluating the effects on different datasets with varied backbones and tasks. Next, we validate the effectiveness of adapting more recent DA techniques (i.e., automatic DA) to biobehavioral time series data. Following the DADA  [14] , we designed a different policy architecture where the operations are differentiable with respect to different time series DA methods. Therefore, the model can be applied to biobehavioral time series data, and the DA parameters and deep model weights can be jointly optimized. Lastly, we try to answer the open question of why a DA works(or not), by first summarizing two desired attributes (challenging and faithful) for an effective DA, and then utilizing two metrics to quantitatively measure the two attributes. We find that an effective DA needs to generate challenging but still faithful transformations, which can guide us for the search of more effective DA for biobehavioral time series data. The contributions of this work are summarized as follows:\n\n• A comprehensive and systematic evaluation of eight data augmentation methods on seven biomedical time series datasets with three backbones for different tasks.\n\n• We revisit the automatic DA methods to make the operations are differentiable with respect to different time series DA methods, therefore can be applied to biobehavioral time series data. Besides, random DA is also investigated to boost efficiency.\n\n• We summarize two desired attributes for an effective DA, and adopt two metrics to quantitatively measure the two attributes respectively. Recommendations are summarized for the search of more effective data augmentation methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Augmentations For Biobehavioral Time Series Data",
      "text": "Most of the basic DA methods are borrowed or inspired from image or time series data augmentation, such as flipping, cropping and noise addition. These augmentation methods rely on adding random transformations to the training data. Um et al.  [30]  systematically evaluated six DA methods for wearable sensor data based Parkinson's disease monitoring, and found that the combination of rotational and permutational data augmentation methods improve the baseline performance the most. Ohashi et al.  [20]  proposed a rotation based data augmentation method for wearable data, which can take the physical constraint into account. Alawneh et al.  [1]  investigated the benefits of adopting time series data augmentation methods to biomedical time series data, and demonstrated the improved accuracy of several deep learning models for human activity recognition. Eyobu and Han  [27]  proposed an ensemble of feature space augmentation methods, which was used for human activity classification based on wearable inertial measurement unit (IMU) sensors. Besides, DA methods have been also used to balance the dataset. For example, Cao et al.  [3]  used DA methods to balance the number of samples among different categories for automated heart disease detection. However, those related works only investigated the very basic DAs, and the effectiveness of adapting more advanced DAs is not explored yet for biobehavioral time series data. More importantly, the previous works did not explore the question why a DA is effective, and vice versa.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Automatic Data Augmentation",
      "text": "DA can be very useful for the training of deep models, but the success relies heavily on domain knowledge and also the extensive experiments to select the effective DA policies for a target dataset. Otherwise, a model may be negatively impacted by some DA policies  [11, 32] . Therefore, it is nontrivial and desired to select the effective DA policy for a new dataset automatically. The goal of automatic data augmentation is to search for effective data augmentation policies that, when applied during the model training, will minimize its validation loss, therefore better generalization ability. The pioneering work, AutoAugment  [4] , formats the process of searching DA as an optimization problem to search for the parameters of augmentation, and follwing work  [5, 9, [14] [15] [16]  were later proposed to improve the efficiency. Despite the success of automatic DA for computer vision tasks, the adaption to biobehavioral time series data is understudied. The only work we know is  [24] , which investigated the automatic differentiable data augmentation for EEG signals. However, our work is different with  [24] , as we target more diverse types of data and DA methods, and more importantly, we explore to explain why a DA works or not. Figure  2 . Examples of time series data augmented by two consecutive operations. The first column is the original input signal, with two random operations sequentially applied to the input signal, the final augmented signal is shown in the third column (as green line). Note that, not only the name of operations and magnitude matter, but also the order of operation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Quantitative Measurement Of Effectiveness For Data Augmentation",
      "text": "Although the effectiveness of DA is well acknowledged, a quantitative evaluation of the effectiveness of DA is still an open question. Currently, the most well-known hypothesis is that effective DA can produce samples from an \"overlapping but different\" distribution  [2, 17] , therefore improving generalization by training with the diverse samples. However, the role of distribution shift in training remains unclear. A more recent work  [8]  studied to quantify how DA improves model generalization, and introduced two measures: Affinity and Diversity, to predict the performance of an augmentation method. During our experiments, we adopt those two metrics to jointly evaluate the two attributes of different augmentations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "We performed extensive experiments with various DA methods on different datasets with different tasks and back-bones. After that, an automatic DA search strategy is adapted to jointly optimize the deep models and also the best DA policies for wearable data. To avoid the complicated searching procedure of automatic DA while keeping the effectiveness, a random augmentation procedure was then adapted in our experiments. Lastly, We also explored to answer the open questions of why some DA methods are more effective than others by quantitatively measuring the effectiveness.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Basic Data Augmentation Methods",
      "text": "DA methods rely on adding random transformations to the training data, and the transformations can be generally classified into three categories: magnitude-based, timebased, and frequency-based transformations. Magnitudebased transformations are applied to the wearable data along the variate or value axes. Time-based transformations change the time steps, and frequency-based transformations warp the frequencies, respectively. During our experiments, we will mainly focus on the magnitude and timebased transformations.\n\nLet T α denote an augmentation operations parameterized by α, given input data x, this procedure outputs augmented data x = T α (x), where α controls the magnitude of the operation T . We consider a set of DA methods drawn from the traditional time series processing literature. Specifically, our augmentation set consists of eight operations: Jittering, Scaling, Rotation, Permutation, Magnitude Warping, Time Warping, Window Slicing, and Window Warping.\n\nGiven an input data x = [x 1 , x 2 , . . . , x T ] with T , the number of time steps, and each element x t can be univariate or multivariate. Jittering. A random noise is added to the input data:\n\nwhere x is the augmented data, is a random noise (i.e., Gaussian noise) added to each time step t. Assume ∼ N (0, σ 2 ), and the standard deviation α = {σ} of the added noise is a hyper-parameter that needs to be pre-determined. Scaling. The magnitude of the data in a window is changed by multiplying a random scalar.\n\nwhere can be determined by a Gaussian distribution ∼ N (1, σ 2 ) with α = {σ} as a hyperparameter.\n\nRotation. Rotate each element by a random rotation matrix.\n\nAlthough rotating data by a random angle can create plausible patterns for images, it might not be suitable for time series data. A widely used alternative is flipping, which is defined as:\n\nPermutation. Perturb the location of the data in a single window. It should be noted that permutation operation does not preserve time dependencies. Permutation can be performed in two ways: equal sized segments and variable sized segments. First, the data x is split into N segments, each of the segment has a length of T N (assuming equal sized segments); then the location of those segments are randomly permuted.\n\nwhere segment i * is the i * -th segment of the input data x,\n\nThe number of segments α = {N } is a hyperparameter to be pre-determined. Magnitude Warping. Change the magnitude of each sample by convolving the data window with a smooth curve.\n\nwhere γ 1 , γ 2 , . . . γ T is a sequence created by interpolating a cubic spline function S(u) with knots u = u 1 , u 2 , . . . u I . I is the number of knots and each knot u i is sampled from a Gaussian distribution N (1, σ 2 ), therefore, the operation parameters α = {σ, I}. Time Warping. Perturb the temporal location by smoothly distorting the time intervals between samples, which is similar to the magnitude warping operation and defined as:\n\nwhere γ(•) : i → j, and i, j ∈ [1, T ] is a warping function that warps the time steps based on a smooth curve. The smooth curve is defined by a cubic spline S(u), which is exactly the same as used in the magnitude warping operation, and the operation parameters α = {σ, I}. Window Slicing. Slice time steps off the ends of the pattern, which is equivalent to cropping for image data augmentation. The operation is defined as:\n\nwhere 0 ≤ W ≤ T is the size of a window that needs to be pre-determined, and δ is a random integer such that 0 ≤ δ ≤ T -W , and the operation parameters α = {W }. Window Warping. Randomly select a window with the length of W from the time series and stretch it by K or contract it by 1 K . Linear interpolation is used for other part of the time series, so that the output will have equal length to the input. Note that, in this paper we only consider K = 2, but other ratios could be used as well. The length of window is a hyper-parameter to be pre-determined and α = {W } With different operations and their corresponding magnitude parameters α, the generated augmented data are also different. As shown in Fig.  1 , the two rows represent the examples of the augmented data where the eight basic DA methods are applied to the same input with two different magnitude parameters α for each operation. Besides, different DAs can be combined together to generate more diverse augmented data, as shown in Fig.  2 . As a result, manually searching for the optimal DA(s) can be very challenging and time consuming.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Advanced Data Augmentation Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Data Augmentation",
      "text": "To alleviate the computation burden, differentiable automatic DA  [14]  was proposed to greatly improve the optimization efficiency through relaxing the optimization process as differentiable and jointly optimized DA parameters with deep model weights. In this paper, we revisit the idea of automatic DA  [14, 24]  to make the operations differentiable with respect to different time series DA methods. Therefore, the model can be applied to biobehavioral time series data, and the weights of deep models and augmentation parameters can be jointly optimized.\n\nA collection of DA policies contains K sub-policies P = [p 1 , p 2 , . . . p K ], and each sub-policy p k includes J basic DA operations (i.e., jittering, scaling, rotation) that are applied to input signal sequentially. Each operation can be represented as\n\nwhere K is the total number of sub-policies, J is the operations included in each sub-policy. p i k is the probability of applying the operation and m i k represents the magnitude of the corresponding operation. The objective is to jointly optimize a model's parameter θ and also the augmentation parameters α, where α represents both the probability p i k and magnitude m i k . Therefore, we can not only train a deep model that generalizes well on the testing dataset, but also learn the optimal augmentation policies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Practical Data Augmentation",
      "text": "In spite of the benefits of learned DA policies from the target dataset, the computational requirements as well as the added complexity of the jointed optimization procedures can be prohibitive. Therefore, we also deployed random augmentation, which eliminate the searching phase but still keep the benefits of different augmentations. More specifically, we follow RandAugment  [5]  to replace the differentiable automatic DA module with a parameter-free procedure for biomedical time series data, which always selects an operation with uniform probability 1 K , where K is the total number of operations. The algorithm can be defined as: randaugment(J, M ), where J is the number of augmentation transformations to be selected from the K operation pool, M is the magnitude for all the transformations, then the J operations with magnitude M will be sequentially applied to the input data. With this method, the DA procedure can be easily plugged in the training of a deep model without requiring special attention or designation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quantitative Measurement For Data Augmentation",
      "text": "We explore to answer the open questions of why some DA methods are more effective, and vice versa. Following previous work  [8, 29] , we first summarize the desired attributes (challenging and faithful)for effective DA, and then we adopt the metrics to quantitatively measure the two attributes respectively. Specifically, challenging refers to the attribute that the augmented data should be challenging for a deep model to over-fit, which is defined as the ratio of final training loss of a model trained with a given augmentation, and the loss of the model trained on original data. Faithful attribute requires the augmented data should not be so strong that make the learning task impossible, which is defined as the ratio between the validation accuracy of a model trained on clean data and tested on an augmented validation set, and the accuracy of the same model tested on clean data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "Through the experiments, we aim to answer the following research questions:\n\nR1: What is the most effective DA method for a given backbone and dataset?\n\nR2: what are the factors that impact the selection of DA? R3: What are the general conclusion we could make for DA methods in biobehavioral time series data? R4: Why are some DA methods more effective than the others?",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets And Implementation Details",
      "text": "Datasets. We conduct extensive experiments on seven biomedical time series datasets, including electrocardiogram (ECG) data: PTB-XL  [31]  and Apnea-ECG  [21] ; electroencephalogram (EEG) data: Sleep-EDF (expanded)  [12]  and MMIDB-EEG  [26] ; electrodermal activity (EDA) data: CLAS  [18] ; inertial measurement unit (IMU) data: PAMAP2  [22]  and UCI-HAR  [23] . A detailed description of those datasets can be find in Appendix A.\n\nImplementation Details. We use three deep learning architectures, including MLP, Conv-1d, and ResNet-1d. These networks were chosen due to being effective in time series data and also being used in a wide range of biomedical applications.\n\nWe use an Adam optimizer with initial learning rate of 1 × 10 -3 , and the learning rate is decayed by 0.9 after every 5 epoches. The batch size is 100, and we train the model for 50 epochs. Our model is implemented in the Py-Torch deep learning framework, and is trained and tested on the NVIDIA GeForce 3090Ti GPU. To evaluate the performance, average accuracy of three runs is reported. For fair comparison, we use the default value as magnitude for different basic augmentations on different datasets, therefore we can guarantee that the varied performance is caused by augmentation rather than fine-tuning. For random DA, we set J = 2, K = 8 and magnitude M = 12 during our experiments. For automatic DA, K = 14 sub-policies are randomly generated from the 8 basic DA methods, and each sub-policy contains J = 2 operations.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Of Eight Basic Da Methods.",
      "text": "Experiments were first conducted on seven datasets with three different (MLP, Conv-1D and ResNet18-1D) back- I). All the Tasks can benefit from DAs. (R1,R3) Comparing with the models trained w/o augmentation, training with proper DA usually can achieve higher performance, and the improvement ranges from 0.5% to 15% over different datasets, demonstrating the effectiveness of DA methods for biobehavioral signals. It is worth to note that although augmentations are generally beneficial, the effectiveness varies over different datasets and backbones. II). The effectiveness of DA depends on many factors. (R2) From the grey colored areas, our first impression is that there is no such a single augmentation method works equally well for all the different datasets. For example, the permutation shows improved or comparable performance on most of the datasets, but decreased performance is also observed for the MMIDB-EEG dataset. The dataset varies in terms of data type (e.g., ECG, EEG, EDA and IMU) and tasks (sleep quality, heart disease, human activities, etc). We can find that the effectiveness of an augmentation not only depends on the dataset itself (data type, task), but also the selection of backbones. For example, with the same backbone (i.e., ResNet-1D) and data type (ECG), the top three augmentations for PTB-XL and Apnea-ECG are totally different. scaling and jittering work the best for PTB-XL dataset, while rotation and permutation show the highest improvement for the Apnea-ECG dataset. In addition to how different tasks impact the effectiveness of different augmentations, the backbone also effect the choice of DA methods. For example, in the MMIDB-EEG dataset, Window warp reports the highest performance for the MLP backbone, while Jittering and Scaling achieve the best performance for Conv-1D and ResNet-1D backbone respectively.\n\nBesides, we also observe that permutation appears 13 times in the top three augmentations, which counts for around half of the rows. Therefore, it is suggested to try permutation first for the related applications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Evaluation Of Automatic And Practi-",
      "text": "cal DA methods.\n\nThe performance evaluation of automatic and random DA is displayed in Table .2, where both the baseline (No Aug) We can conclude that I). Random DA is both effective and efficient (R3). As we can see from Table .1, the performance trained with random DA generally outperforms the baselines (No Aug), and are comparable with the best performance across different datasets and backbones. Therefore, random DA is an efficient, effective, and practical automated data augmentation method for biobehavioral time series data. II). Automatic DA outperforms most comparable baselines (R1). From the various possible DA combinations and their corresponding magnitudes, the automatic DA method helps us search for the more effective DA policies (operations and magnitudes). As we may find that the automatic DA method (Auto) generally outperforms the baselines across different datasets and backbones, and achieves comparable or higher performance than the best performance, which is achieved by manually selected DAs. We tried to answer the question of why some DA methods are more effective than others, and how can we quantitatively measure the effectiveness of different DA meth-ods? First, we summarized the desired attributes for effective DAs: 1) challenging: the augmented data should be complex and strong enough that a deep model must learn useful representations to perform the task.\n\n2) faithful: the DA must not make the task impossible, being so strong that they destroy all features of the input. For example, the random noise added to jittering should not be so strong that makes learning impossible from the augmented data.\n\nNext, we adopted the metrics in  [8]  to quantify how DA improves model's performance by quantitatively measuring challenging and faithful for different DAs,. As shown in Fig.  3 (c ), affinity is used to measure how faithful of a DA, while diversity measures the level of challenge resulting from applying an augmentation. Fig.  3  (a) and (b) measure both affinity and diversity across 65 different augmentations for PTB-XL and PAMAP2 dataset. We find that many augmentations that dramatically decrease the performance have low affinity and high diversity (upper left). On the other hand, many successful augmentations (blue points) lay in the area with high affinity, and for fixed (high) value of affinity, test accuracy generally increases with the increase of diversity.\n\nIn summary, affinity and diversity together provide an explanation of an augmentation policy's benefit to a model's performance. In other words, effective DA is a trade-off between generating more diverse data that should be more difficult for a model to fit, while remaining faithful that the learning task is still possible from the augmented data. The random DA method achieves improved or comparable performances across different tasks and datasets using the fixed number of transformations and fixed value of magnitude. To further study the sensitivity of random DA to the selection of transformations and magnitude, we run experiments of random DA with different number of operations and magnitude on the PTB-XL dataset. The results in Fig.  4  (a) (fixed M) suggest that the random DA improves performance as the number of operations is increased, even with only 1 operation (J = 1). However, the performance is dropped once J > 4, which is potentially caused by the fact that the augmented data is too challenging for the model to learn anything meaningful after J consecutive augmentations, therefore, leading to dropped performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "A constant magnitude M sets the distortion magnitude to a constant number during training. To validate the impact of different magnitudes, we run experiments with fixed J = 2 and varied numbers in terms of magnitude. The results in Fig.  4 (b ) suggest that the model is not very sensitive to the  change of magnitude (except MLP, as it is a relatively weak backbone). That is because first, the magnitude is limited in a small reasonable range for all the operations (details in supplementary materials); and second, the composition of J random consecutive operations could potentially alleviate the sensitivity to the different magnitudes.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization Of Policies In Automatic Da",
      "text": "An illustration of the development of probability (normalized) for individual DA policy over training epochs is shown in Fig.  5 , where all the 14 sub-policies start from the same probability, and change with the training epochs. For example, the probability is gradually increased for sub-policy 1, sub-policy 5 and sub-policy 14, indicting those policies are more effective than others. Although automatic differentiable DA shows improved performance, it comes at a cost in terms of computational complexity, which is roughly four times than the corresponding baseline models.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we first conducted a comprehensive and systematic evaluation of various basic DAs on different biobehavioral datasets, finding that all the datasets can benefit from DA if using them carefully. The effectiveness of an augmentation depended on both the dataset itself (data type, task) and the used backbone. Further, we explored to adopt more recent DA techniques for biobehavioral signals by designing a different policy architecture. The results demonstrated that automatic DA can help learn effective policies but at a cost of high computational complexity, while random DA was demonstrated to be both effective and efficient during our experiments. At last, we attempted to answer the question of why DA is effective (or not), by first summarizing two desired attributes (challenging and faithful), and then two we used two quantitative metrics (diversity and affinity) to measure the corresponding attributes of DA. This can help understand why a DA works (or not), and guide us for the search of more effective DA methods in the future.\n\nWe hope our experimental results could shed some light on DA for biobehavioral time series data, as a carefully selected DA could outperform many claimed improvements in the literature. Our next plan is to investigate domain knowledge inspired DA, and also develop more efficient way to quantitatively measure the effectiveness of a DA method.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines",
      "page": 2
    },
    {
      "caption": "Figure 2: Examples of time series data augmented by two consec-",
      "page": 3
    },
    {
      "caption": "Figure 3: Augmentation performance is determined by both afﬁnity and diversity. Test accuracy plotted against each of Afﬁnity and",
      "page": 5
    },
    {
      "caption": "Figure 4: Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.",
      "page": 8
    },
    {
      "caption": "Figure 5: The development of probability parameters for each DA",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rice University, Houston TX 77005, USA": "hy29,\n{hy48,"
        },
        {
          "Rice University, Houston TX 77005, USA": "Abstract"
        },
        {
          "Rice University, Houston TX 77005, USA": ""
        },
        {
          "Rice University, Houston TX 77005, USA": "Deep learning has performed remarkably well on many"
        },
        {
          "Rice University, Houston TX 77005, USA": "tasks recently. However,\nthe superior performance of deep"
        },
        {
          "Rice University, Houston TX 77005, USA": "models relies heavily on the availability of a large number"
        },
        {
          "Rice University, Houston TX 77005, USA": "of\ntraining data, which limits the wide adaptation of deep"
        },
        {
          "Rice University, Houston TX 77005, USA": "models on various clinical and affective computing tasks,"
        },
        {
          "Rice University, Houston TX 77005, USA": "as the labeled data are usually very limited. As an effec-"
        },
        {
          "Rice University, Houston TX 77005, USA": "tive technique to increase the data variability and thus train"
        },
        {
          "Rice University, Houston TX 77005, USA": "deep models with better generalization, data augmentation"
        },
        {
          "Rice University, Houston TX 77005, USA": "(DA) is a critical step for the success of deep learning mod-"
        },
        {
          "Rice University, Houston TX 77005, USA": "els on biobehavioral\ntime series data. However,\nthe effec-"
        },
        {
          "Rice University, Houston TX 77005, USA": "tiveness of various DAs\nfor different datasets with differ-"
        },
        {
          "Rice University, Houston TX 77005, USA": "ent\ntasks and deep models\nis understudied for biobehav-"
        },
        {
          "Rice University, Houston TX 77005, USA": "ioral\ntime series data.\nIn this paper, we ﬁrst systematically"
        },
        {
          "Rice University, Houston TX 77005, USA": "review eight basic DA methods for biobehavioral\ntime se-"
        },
        {
          "Rice University, Houston TX 77005, USA": "ries data, and evaluate the effects on seven datasets with"
        },
        {
          "Rice University, Houston TX 77005, USA": "three backbones. Next, we explore adapting more recent DA"
        },
        {
          "Rice University, Houston TX 77005, USA": "techniques (i.e., automatic augmentation, random augmen-"
        },
        {
          "Rice University, Houston TX 77005, USA": "tation) to biobehavioral time series data by designing a new"
        },
        {
          "Rice University, Houston TX 77005, USA": "policy architecture applicable to time series data. Last, we"
        },
        {
          "Rice University, Houston TX 77005, USA": "try to answer the question of why a DA is effective (or not)"
        },
        {
          "Rice University, Houston TX 77005, USA": "by ﬁrst summarizing two desired attributes for augmenta-"
        },
        {
          "Rice University, Houston TX 77005, USA": "tions (challenging and faithful), and then utilizing two met-"
        },
        {
          "Rice University, Houston TX 77005, USA": "rics to quantitatively measure the corresponding attributes,"
        },
        {
          "Rice University, Houston TX 77005, USA": "which can guide us in the search for more effective DA for"
        },
        {
          "Rice University, Houston TX 77005, USA": "biobehavioral time series data by designing more challeng-"
        },
        {
          "Rice University, Houston TX 77005, USA": "ing but still\nfaithful\ntransformations. Our code and results"
        },
        {
          "Rice University, Houston TX 77005, USA": "are available at Link."
        },
        {
          "Rice University, Houston TX 77005, USA": ""
        },
        {
          "Rice University, Houston TX 77005, USA": ""
        },
        {
          "Rice University, Houston TX 77005, USA": ""
        },
        {
          "Rice University, Houston TX 77005, USA": "1. Introduction"
        },
        {
          "Rice University, Houston TX 77005, USA": ""
        },
        {
          "Rice University, Houston TX 77005, USA": "Deep learning performs remarkably well in many ﬁelds,"
        },
        {
          "Rice University, Houston TX 77005, USA": "including computer vision (CV), natural\nlanguage process-"
        },
        {
          "Rice University, Houston TX 77005, USA": "ing (NLP),\nand recently time\nseries-related tasks\n[6, 10,"
        },
        {
          "Rice University, Houston TX 77005, USA": "32]. Those successful applications increasingly inspire re-"
        },
        {
          "Rice University, Houston TX 77005, USA": "searchers\nto embrace deep learning for\nsolving issues\nin"
        },
        {
          "Rice University, Houston TX 77005, USA": "human-centered applications that use physiological and be-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tions, which can guide us for the search of more effective": "DA for biobehavioral time series data. The contributions of",
          "data, and demonstrated the improved accuracy of\nseveral": "deep learning models for human activity recognition. Ey-"
        },
        {
          "tions, which can guide us for the search of more effective": "this work are summarized as follows:",
          "data, and demonstrated the improved accuracy of\nseveral": "obu and Han [27] proposed an ensemble of\nfeature space"
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "augmentation methods, which was used for human activity"
        },
        {
          "tions, which can guide us for the search of more effective": "• A comprehensive and systematic evaluation of eight",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "classiﬁcation based on wearable inertial measurement unit"
        },
        {
          "tions, which can guide us for the search of more effective": "data augmentation methods on seven biomedical\ntime",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "(IMU) sensors. Besides, DA methods have been also used"
        },
        {
          "tions, which can guide us for the search of more effective": "three\nseries\ndatasets with\nbackbones\nfor\ndifferent",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "to balance the dataset. For example, Cao et al. [3] used DA"
        },
        {
          "tions, which can guide us for the search of more effective": "tasks.",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "methods to balance the number of samples among different"
        },
        {
          "tions, which can guide us for the search of more effective": "• We revisit the automatic DA methods to make the op-",
          "data, and demonstrated the improved accuracy of\nseveral": "categories for automated heart disease detection. However,"
        },
        {
          "tions, which can guide us for the search of more effective": "erations are differentiable with respect to different time",
          "data, and demonstrated the improved accuracy of\nseveral": "those related works only investigated the very basic DAs,"
        },
        {
          "tions, which can guide us for the search of more effective": "series DA methods, therefore can be applied to biobe-",
          "data, and demonstrated the improved accuracy of\nseveral": "and the effectiveness of adapting more advanced DAs is not"
        },
        {
          "tions, which can guide us for the search of more effective": "havioral\ntime series data. Besides, random DA is also",
          "data, and demonstrated the improved accuracy of\nseveral": "explored yet for biobehavioral\ntime series data. More im-"
        },
        {
          "tions, which can guide us for the search of more effective": "investigated to boost efﬁciency.",
          "data, and demonstrated the improved accuracy of\nseveral": "portantly,\nthe previous works did not explore the question"
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "why a DA is effective, and vice versa."
        },
        {
          "tions, which can guide us for the search of more effective": "• We summarize two desired attributes for an effective",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "DA, and adopt\ntwo metrics to quantitatively measure",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "2.2. Automatic Data Augmentation"
        },
        {
          "tions, which can guide us for the search of more effective": "the two attributes respectively. Recommendations are",
          "data, and demonstrated the improved accuracy of\nseveral": ""
        },
        {
          "tions, which can guide us for the search of more effective": "summarized for the search of more effective data aug-",
          "data, and demonstrated the improved accuracy of\nseveral": "DA can be very useful for the training of deep models,"
        },
        {
          "tions, which can guide us for the search of more effective": "mentation methods.",
          "data, and demonstrated the improved accuracy of\nseveral": "but the success relies heavily on domain knowledge and also"
        },
        {
          "tions, which can guide us for the search of more effective": "",
          "data, and demonstrated the improved accuracy of\nseveral": "the extensive experiments to select\nthe effective DA poli-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "are the augmented data based on the eight data augmentation operations including Jittering, Scaling, Rotation, Permutation, Magnitude"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "Warping, Time Warping, and Window Warping with two different magnitudes."
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "Therefore,\nthe model can be applied to biobehavioral\ntime"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "series data, and the DA parameters and deep model weights"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "can be jointly optimized. Lastly, we try to answer the open"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "question of why a DA works(or not), by ﬁrst summarizing"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "two desired attributes (challenging and faithful)\nfor an ef-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "fective DA, and then utilizing two metrics to quantitatively"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "measure the two attributes. We ﬁnd that an effective DA"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "needs to generate challenging but still faithful\ntransforma-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "tions, which can guide us for the search of more effective"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "DA for biobehavioral time series data. The contributions of"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "this work are summarized as follows:"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "• A comprehensive and systematic evaluation of eight"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "data augmentation methods on seven biomedical\ntime"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "three\nseries\ndatasets with\nbackbones\nfor\ndifferent"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "tasks."
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "• We revisit the automatic DA methods to make the op-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "erations are differentiable with respect to different time"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "series DA methods, therefore can be applied to biobe-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "havioral\ntime series data. Besides, random DA is also"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "investigated to boost efﬁciency."
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "• We summarize two desired attributes for an effective"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "DA, and adopt\ntwo metrics to quantitatively measure"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "the two attributes respectively. Recommendations are"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "summarized for the search of more effective data aug-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "mentation methods."
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "2. Related Works"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "2.1. Augmentations for Biobehavioral Time Series"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "Data"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": ""
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "Most of the basic DA methods are borrowed or inspired"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "from image or time series data augmentation, such as ﬂip-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "ping,\ncropping and noise\naddition.\nThese\naugmentation"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "methods rely on adding random transformations to the train-"
        },
        {
          "Figure 1. Examples of different data augmentation methods used in the experiments. The red lines indicate the input data and the green lines": "ing data. Um et al.\n[30] systematically evaluated six DA"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to search for the parameters of augmentation, and follwing": "work [5, 9, 14–16] were later proposed to improve the efﬁ-",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "adapted to jointly optimize the deep models and also the"
        },
        {
          "to search for the parameters of augmentation, and follwing": "ciency. Despite the success of automatic DA for computer",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "best DA policies for wearable data. To avoid the compli-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "vision tasks,\nthe adaption to biobehavioral\ntime series data",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "cated searching procedure of automatic DA while keeping"
        },
        {
          "to search for the parameters of augmentation, and follwing": "is understudied.\nThe only work we know is [24], which",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "the effectiveness,\na random augmentation procedure was"
        },
        {
          "to search for the parameters of augmentation, and follwing": "investigated the automatic differentiable data augmentation",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "then adapted in our experiments. Lastly, We also explored"
        },
        {
          "to search for the parameters of augmentation, and follwing": "for EEG signals. However, our work is different with [24],",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "to answer the open questions of why some DA methods are"
        },
        {
          "to search for the parameters of augmentation, and follwing": "as we target more diverse types of data and DA methods,",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "more effective than others by quantitatively measuring the"
        },
        {
          "to search for the parameters of augmentation, and follwing": "and more importantly, we explore to explain why a DA",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "effectiveness."
        },
        {
          "to search for the parameters of augmentation, and follwing": "works or not.",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "3.1. Basic Data Augmentation Methods"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "DA methods rely on adding random transformations to"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "the\ntraining data,\nand the\ntransformations\ncan be gener-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "ally classiﬁed into three categories: magnitude-based, time-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "based, and frequency-based transformations. Magnitude-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "based\ntransformations\nare\napplied\nto\nthe wearable\ndata"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "along the variate or value axes.\nTime-based transforma-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "tions change the time steps, and frequency-based transfor-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "mations warp the frequencies, respectively. During our ex-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "periments, we will mainly focus on the magnitude and time-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "based transformations."
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Let Tα denote an augmentation operations parameterized"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "by α, given input data x, this procedure outputs augmented"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "the\ndata ˆx = Tα(x), where α controls the magnitude of"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "operation T . We consider a set of DA methods drawn from"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "the traditional time series processing literature. Speciﬁcally,"
        },
        {
          "to search for the parameters of augmentation, and follwing": "Figure 2. Examples of time series data augmented by two consec-",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "our augmentation set consists of eight operations: Jittering,"
        },
        {
          "to search for the parameters of augmentation, and follwing": "utive operations. The ﬁrst column is the original input signal, with",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Scaling, Rotation, Permutation, Magnitude Warping, Time"
        },
        {
          "to search for the parameters of augmentation, and follwing": "two random operations sequentially applied to the input signal, the",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Warping, Window Slicing, and Window Warping."
        },
        {
          "to search for the parameters of augmentation, and follwing": "ﬁnal augmented signal is shown in the third column (as green line).",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "Note that, not only the name of operations and magnitude matter,",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "the\nGiven an input data x = [x1, x2, . . . , xT ] with T ,"
        },
        {
          "to search for the parameters of augmentation, and follwing": "but also the order of operation.",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "number of time steps, and each element xt can be univariate"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "or multivariate."
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Jittering. A random noise is added to the input data:"
        },
        {
          "to search for the parameters of augmentation, and follwing": "2.3. Quantitative Measurement of Effectiveness for",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "(1)\nx = Tα(x) = [x1 + (cid:15)1, x2 + (cid:15)2, . . . , xT + (cid:15)T ]"
        },
        {
          "to search for the parameters of augmentation, and follwing": "Data Augmentation",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "Although the effectiveness of DA is well acknowledged,",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "where ˆx is the augmented data, (cid:15) is a random noise (i.e.,"
        },
        {
          "to search for the parameters of augmentation, and follwing": "a quantitative evaluation of the effectiveness of DA is still an",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Gaussian noise) added to each time step t. Assume (cid:15) ∼"
        },
        {
          "to search for the parameters of augmentation, and follwing": "open question. Currently,\nthe most well-known hypothesis",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "N (0, σ2), and the standard deviation α = {σ} of the added"
        },
        {
          "to search for the parameters of augmentation, and follwing": "is that effective DA can produce samples from an ”overlap-",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "noise is a hyper-parameter that needs to be pre-determined."
        },
        {
          "to search for the parameters of augmentation, and follwing": "ping but different” distribution [2, 17],\ntherefore improving",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Scaling. The magnitude of the data in a window is changed"
        },
        {
          "to search for the parameters of augmentation, and follwing": "generalization by training with the diverse samples. How-",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "by multiplying a random scalar."
        },
        {
          "to search for the parameters of augmentation, and follwing": "ever,\nthe role of distribution shift\nin training remains un-",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "clear. A more recent work [8] studied to quantify how DA",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "(2)\nx = Tα(x) = [(cid:15)x1, (cid:15)x2, . . . , (cid:15)xT ]"
        },
        {
          "to search for the parameters of augmentation, and follwing": "improves model generalization, and introduced two mea-",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "where (cid:15) can be determined by a Gaussian distribution (cid:15) ∼"
        },
        {
          "to search for the parameters of augmentation, and follwing": "sures:\nAfﬁnity and Diversity,\nto predict\nthe performance",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "N (1, σ2) with α = {σ} as a hyperparameter."
        },
        {
          "to search for the parameters of augmentation, and follwing": "of an augmentation method. During our experiments, we",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "adopt those two metrics to jointly evaluate the two attributes",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Rotation. Rotate each element by a random rotation matrix."
        },
        {
          "to search for the parameters of augmentation, and follwing": "of different augmentations.",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "Although rotating data by a random angle can create plau-"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "sible patterns for images,\nit might not be suitable for time"
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "series data. A widely used alternative is ﬂipping, which is"
        },
        {
          "to search for the parameters of augmentation, and follwing": "3. Methods",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        },
        {
          "to search for the parameters of augmentation, and follwing": "",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": "deﬁned as:"
        },
        {
          "to search for the parameters of augmentation, and follwing": "We performed extensive experiments with various DA",
          "bones.\nAfter\nthat,\nan automatic DA search strategy is": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "window. It should be noted that permutation operation does",
          "methods are applied to the same input with two different": "magnitude parameters α for each operation. Besides, differ-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "not preserve time dependencies.\nPermutation can be per-",
          "methods are applied to the same input with two different": "ent DAs can be combined together to generate more diverse"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "formed in two ways:\nequal\nsized segments and variable",
          "methods are applied to the same input with two different": "augmented data, as shown in Fig.2. As a result, manually"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "sized segments. First,\nthe data x is split\ninto N segments,",
          "methods are applied to the same input with two different": "searching for the optimal DA(s) can be very challenging and"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "each of the segment has a length of T",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "N (assuming equal sized",
          "methods are applied to the same input with two different": "time consuming."
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "segments); then the location of those segments are randomly",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "permuted.",
          "methods are applied to the same input with two different": "3.2. Advanced Data Augmentation Methods"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "3.2.1\nAutomatic Data Augmentation"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "(4)\nx = Tα(x) = [segment1∗ , . . . , segmentN ∗ ]",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "To alleviate the computation burden, differentiable auto-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "is the i*-th segment of the input data x,\nwhere segmenti∗",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "matic DA [14] was proposed to greatly improve the opti-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "and i∗ ∈ [1, N ]. The number of segments α = {N } is a",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "mization efﬁciency through relaxing the optimization pro-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "hyperparameter to be pre-determined.",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "cess as differentiable and jointly optimized DA parameters"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "Magnitude Warping. Change the magnitude of each sam-",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "with deep model weights.\nIn this paper, we revisit\nthe idea"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "ple by convolving the data window with a smooth curve.",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "of automatic DA [14, 24]\nto make the operations differ-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "entiable with respect\nto different\ntime series DA methods."
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "(5)\nx = Tα(x) = [γ1x1, γ2x2, . . . , γT xT ]",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "Therefore,\nthe model can be applied to biobehavioral\ntime"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "where γ1, γ2, . . . γT is a sequence created by interpolating",
          "methods are applied to the same input with two different": "series data, and the weights of deep models and augmenta-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "a cubic spline function S(u) with knots u = u1, u2, . . . uI .",
          "methods are applied to the same input with two different": "tion parameters can be jointly optimized."
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "is sampled from\nI is the number of knots and each knot ui",
          "methods are applied to the same input with two different": "A collection of DA policies\ncontains K sub-policies"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "a Gaussian distribution N (1, σ2),\ntherefore,\nthe operation",
          "methods are applied to the same input with two different": "includes J\nP = [p1, p2, . . . pK], and each sub-policy pk"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "parameters α = {σ, I}.",
          "methods are applied to the same input with two different": "jittering,\nscaling,\nbasic DA operations (i.e.,\nrotation)\nthat"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "Time Warping. Perturb the temporal location by smoothly",
          "methods are applied to the same input with two different": "are applied to input signal sequentially. Each operation can"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "distorting the time intervals between samples, which is sim-",
          "methods are applied to the same input with two different": "(cid:0)x; pi\n(cid:1), k ∈ [1, K],"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "i ∈ [0, J],\nk\nk, mi\nk"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "ilar to the magnitude warping operation and deﬁned as:",
          "methods are applied to the same input with two different": "where K is the total number of sub-policies, J is the op-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "pi"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "k is the probability"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "(6)\nx = Tα(x) = [xγ(1), xγ(2), . . . , xγ(T )]",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "of applying the operation and mi"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "k represents the magnitude"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "of the corresponding operation. The objective is to jointly"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": ":\ni → j, and i, j ∈ [1, T ]\nwhere γ(·)\nis a warping func-",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "optimize a model’s parameter θ and also the augmentation"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "tion that warps the time steps based on a smooth curve. The",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "parameters α, where α represents both the probability pi"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "k"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "smooth curve is deﬁned by a cubic spline S(u), which is ex-",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "and magnitude mi"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "k. Therefore, we can not only train a deep"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "actly the same as used in the magnitude warping operation,",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "model\nthat generalizes well on the testing dataset, but also"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "and the operation parameters α = {σ, I}.",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "learn the optimal augmentation policies."
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "Window Slicing. Slice time steps off the ends of the pat-",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "tern, which is equivalent\nto cropping for\nimage data aug-",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "mentation. The operation is deﬁned as:",
          "methods are applied to the same input with two different": "3.2.2\nPractical Data Augmentation"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "(7)\nx = Tα(x) = [xδ, . . . , xt, . . . , xW +δ]",
          "methods are applied to the same input with two different": "In spite of the beneﬁts of learned DA policies from the tar-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "get dataset,\nthe computational\nrequirements as well as the"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "where 0 ≤ W ≤ T is\nthe size of a window that needs",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "added complexity of\nthe jointed optimization procedures"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "to be pre-determined, and δ is a random integer such that",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "can be prohibitive.\nTherefore, we also deployed random"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "0 ≤ δ ≤ T − W , and the operation parameters α = {W }.",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "augmentation, which eliminate the searching phase but still"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "Window Warping.\nRandomly select a window with the",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "keep the beneﬁts of different augmentations. More specif-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "length of W from the time series and stretch it by K or",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "ically, we follow RandAugment [5] to replace the differen-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "1K\n. Linear interpolation is used for other part\ncontract it by",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "tiable automatic DA module with a parameter-free proce-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "of the time series, so that the output will have equal length to",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "dure for biomedical\ntime series data, which always selects"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "the input. Note that, in this paper we only consider K = 2,",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "an operation with uniform probability 1"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "K , where K is the to-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "but other ratios could be used as well. The length of window",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "tal number of operations. The algorithm can be deﬁned as:"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "is a hyper-parameter to be pre-determined and α = {W }",
          "methods are applied to the same input with two different": ""
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "",
          "methods are applied to the same input with two different": "randaugment(J, M ), where J is the number of augmen-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "With different operations and their corresponding mag-",
          "methods are applied to the same input with two different": "tation transformations to be selected from the K operation"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "nitude parameters α, the generated augmented data are also",
          "methods are applied to the same input with two different": "pool, M is the magnitude for all\nthe transformations,\nthen"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "different. As shown in Fig.1,\nthe two rows represent\nthe",
          "methods are applied to the same input with two different": "the J operations with magnitude M will be sequentially ap-"
        },
        {
          "Permutation.\nPerturb the location of\nthe data in a single": "examples of the augmented data where the eight basic DA",
          "methods are applied to the same input with two different": "plied to the input data. With this method, the DA procedure"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0": ""
        },
        {
          "0": ""
        },
        {
          "0": "-1"
        },
        {
          "0": ""
        },
        {
          "0": ""
        },
        {
          "0": "-2"
        },
        {
          "0": "Relative Test Accuracy"
        },
        {
          "0": "-3"
        },
        {
          "0": ""
        },
        {
          "0": "-4"
        },
        {
          "0": ""
        },
        {
          "0": ""
        },
        {
          "0": ""
        },
        {
          "0": "<-5"
        },
        {
          "0": ""
        },
        {
          "0": ""
        },
        {
          "0": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0\n0.0\n<-5": "0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.2\n0.4\n0.6\n0.8\n1.0"
        },
        {
          "0\n0.0\n<-5": "Low Affinity          High Affinity\nAffinity\nAffinity"
        },
        {
          "0\n0.0\n<-5": "(a) PTB-XL\n(b) PAMAP2\n(c) Illustration"
        },
        {
          "0\n0.0\n<-5": "Figure 3. Augmentation performance is determined by both afﬁnity and diversity.\nTest accuracy plotted against each of Afﬁnity and"
        },
        {
          "0\n0.0\n<-5": "Diversity in the PTB-XL dataset\n(a) and PAMAP2 dataset\n(b), where each point\nrepresents a different DA (65 augmentations in total)."
        },
        {
          "0\n0.0\n<-5": "Color shows the ﬁnal\ntest accuracy relative to the baseline model\ntrained without augmentation.\n(c)\nIllustration of how raw data and"
        },
        {
          "0\n0.0\n<-5": "augmented data are associated in terms of the two metrics. A larger circle represents higher diversity while the distributional similarity is"
        },
        {
          "0\n0.0\n<-5": "depicted trough the overlap of circles. Test accuracy generally improves with both high afﬁnity and high diversity (upper right space)."
        },
        {
          "0\n0.0\n<-5": "can be easily plugged in the training of a deep model with-\n4.1. Datasets and Implementation Details"
        },
        {
          "0\n0.0\n<-5": "out requiring special attention or designation."
        },
        {
          "0\n0.0\n<-5": "Datasets. We conduct extensive experiments on seven"
        },
        {
          "0\n0.0\n<-5": "biomedical\ntime\nseries datasets,\nincluding electrocardio-"
        },
        {
          "0\n0.0\n<-5": "gram (ECG) data:\nPTB-XL [31]\nand Apnea-ECG [21];\n3.3. Quantitative Measurement for Data Augmen-"
        },
        {
          "0\n0.0\n<-5": "electroencephalogram (EEG) data: Sleep-EDF (expanded)\ntation"
        },
        {
          "0\n0.0\n<-5": "[12] and MMIDB-EEG [26]; electrodermal activity (EDA)"
        },
        {
          "0\n0.0\n<-5": "We explore to answer\nthe open questions of why some"
        },
        {
          "0\n0.0\n<-5": "data: CLAS [18];\ninertial measurement unit\n(IMU) data:"
        },
        {
          "0\n0.0\n<-5": "DA methods are more effective, and vice versa. Following"
        },
        {
          "0\n0.0\n<-5": "PAMAP2 [22] and UCI-HAR [23]. A detailed description"
        },
        {
          "0\n0.0\n<-5": "previous work [8, 29], we ﬁrst summarize the desired at-"
        },
        {
          "0\n0.0\n<-5": "of those datasets can be ﬁnd in Appendix A."
        },
        {
          "0\n0.0\n<-5": "tributes (challenging and faithful)for effective DA, and then"
        },
        {
          "0\n0.0\n<-5": "Implementation Details.\nWe\nuse\nthree\ndeep\nlearn-"
        },
        {
          "0\n0.0\n<-5": "we adopt\nthe metrics to quantitatively measure the two at-"
        },
        {
          "0\n0.0\n<-5": "ing architectures, including MLP, Conv-1d, and ResNet-1d."
        },
        {
          "0\n0.0\n<-5": "tributes respectively. Speciﬁcally, challenging refers to the"
        },
        {
          "0\n0.0\n<-5": "These networks were chosen due to being effective in time"
        },
        {
          "0\n0.0\n<-5": "attribute that the augmented data should be challenging for"
        },
        {
          "0\n0.0\n<-5": "series data and also being used in a wide range of biomedi-"
        },
        {
          "0\n0.0\n<-5": "a deep model to over-ﬁt, which is deﬁned as the ratio of ﬁnal"
        },
        {
          "0\n0.0\n<-5": "cal applications."
        },
        {
          "0\n0.0\n<-5": "training loss of a model trained with a given augmentation,"
        },
        {
          "0\n0.0\n<-5": "We use an Adam optimizer with initial\nlearning rate of"
        },
        {
          "0\n0.0\n<-5": "and the loss of\nthe model\ntrained on original data. Faith-"
        },
        {
          "0\n0.0\n<-5": "1 × 10−3,\nand the learning rate is decayed by 0.9 after"
        },
        {
          "0\n0.0\n<-5": "ful attribute requires the augmented data should not be so"
        },
        {
          "0\n0.0\n<-5": "every 5 epoches.\nThe batch size is 100, and we train the"
        },
        {
          "0\n0.0\n<-5": "strong that make the learning task impossible, which is de-"
        },
        {
          "0\n0.0\n<-5": "model for 50 epochs. Our model is implemented in the Py-"
        },
        {
          "0\n0.0\n<-5": "ﬁned as the ratio between the validation accuracy of a model"
        },
        {
          "0\n0.0\n<-5": "Torch deep learning framework, and is trained and tested on"
        },
        {
          "0\n0.0\n<-5": "trained on clean data and tested on an augmented validation"
        },
        {
          "0\n0.0\n<-5": "the NVIDIA GeForce 3090Ti GPU. To evaluate the perfor-"
        },
        {
          "0\n0.0\n<-5": "set, and the accuracy of the same model tested on clean data."
        },
        {
          "0\n0.0\n<-5": "mance, average accuracy of three runs is reported. For fair"
        },
        {
          "0\n0.0\n<-5": "comparison, we use the default value as magnitude for dif-"
        },
        {
          "0\n0.0\n<-5": "ferent basic augmentations on different datasets,\ntherefore"
        },
        {
          "0\n0.0\n<-5": "4. Experiments"
        },
        {
          "0\n0.0\n<-5": "we can guarantee that\nthe varied performance is caused by"
        },
        {
          "0\n0.0\n<-5": "augmentation rather than ﬁne-tuning. For random DA, we"
        },
        {
          "0\n0.0\n<-5": "Through the experiments, we aim to answer the follow-"
        },
        {
          "0\n0.0\n<-5": "set J = 2, K = 8 and magnitude M = 12 during our"
        },
        {
          "0\n0.0\n<-5": "ing research questions:"
        },
        {
          "0\n0.0\n<-5": "experiments. For automatic DA, K = 14 sub-policies are"
        },
        {
          "0\n0.0\n<-5": "is the most effective DA method for a given\nR1: What"
        },
        {
          "0\n0.0\n<-5": "randomly generated from the 8 basic DA methods, and each"
        },
        {
          "0\n0.0\n<-5": "backbone and dataset?"
        },
        {
          "0\n0.0\n<-5": "sub-policy contains J = 2 operations."
        },
        {
          "0\n0.0\n<-5": "R2: what are the factors that impact the selection of DA?"
        },
        {
          "0\n0.0\n<-5": "4.2. Experimental Results"
        },
        {
          "0\n0.0\n<-5": "R3: What are the general conclusion we could make for"
        },
        {
          "0\n0.0\n<-5": "4.2.1\nPerformance of eight basic DA methods."
        },
        {
          "0\n0.0\n<-5": "DA methods in biobehavioral time series data?"
        },
        {
          "0\n0.0\n<-5": "Experiments were ﬁrst conducted on seven datasets with\nR4: Why are some DA methods more effective than the"
        },
        {
          "0\n0.0\n<-5": "others?\nthree different\n(MLP, Conv-1D and ResNet18-1D) back-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Performance in terms of 8 different DA methods and three backbones (MLP, Conv-1D and ResNet-1D) are reported on night",
      "data": [
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "different datasets and backbones are colored in gray, and the grayscale represents the corresponding improvements.",
          "improve the baseline for": ""
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "Window"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "Dataset",
          "improve the baseline for": ""
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "Slice"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "66.34"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "PTB-XL",
          "improve the baseline for": "75.91"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "75.48"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "56.19"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "Apnea-ECG",
          "improve the baseline for": "75.94"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "74.32"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "53.78"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "Sleep-EDFE",
          "improve the baseline for": "81.12"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "82.80"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "74.43"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "MMIDB-EEG",
          "improve the baseline for": "74.97"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "70.98"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "77.88"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "CLAS",
          "improve the baseline for": "75.37"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "66.59"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "62.92"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "PAMAP2",
          "improve the baseline for": "91.29"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "88.33"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "89.96"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "UCI-HAR",
          "improve the baseline for": "91.75"
        },
        {
          "datasets. Bold numbers indicate the best performance. The top three most effective DA methods (if exists) that": "",
          "improve the baseline for": "90.91"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Performance in terms of 8 different DA methods and three backbones (MLP, Conv-1D and ResNet-1D) are reported on night",
      "data": [
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "ResNet-1D\n89.79\n87.95\n88.63",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "92.94\n65.12\n91.89\n89.62\n88.39\n90.91"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "bones and eight basic DA methods, (Jittering, Scaling, Ro-",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "the selection of backbones.\nFor example, with the same"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "tation, Permutation, Magnitude Warp, Time Warp, Window",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "backbone (i.e., ResNet-1D) and data type (ECG),\nthe top"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "Slice, Window Warp). From the results in Table.1, We may",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "three augmentations for PTB-XL and Apnea-ECG are to-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "ﬁnd that:",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "tally different. scaling and jittering work the best for PTB-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "XL dataset, while rotation and permutation show the high-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "I). All the Tasks can beneﬁt from DAs. (R1,R3) Compar-",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "est\nimprovement\nfor\nthe Apnea-ECG dataset.\nIn addition"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "ing with the models trained w/o augmentation, training with",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "to how different\ntasks\nimpact\nthe effectiveness of differ-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "proper DA usually can achieve higher performance,\nand",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "ent augmentations,\nthe backbone also effect\nthe choice of"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "the improvement\nranges from 0.5% to 15% over different",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "DA methods.\nFor example,\nin the MMIDB-EEG dataset,"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "datasets, demonstrating the effectiveness of DA methods for",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "Window warp reports the highest performance for the MLP"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "biobehavioral signals. It is worth to note that although aug-",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "backbone, while Jittering and Scaling achieve the best per-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "mentations are generally beneﬁcial, the effectiveness varies",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "formance for Conv-1D and ResNet-1D backbone respec-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "over different datasets and backbones.",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "tively."
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "II). The effectiveness of DA depends on many factors.",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "Besides, we also observe that permutation appears 13"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "(R2) From the grey colored areas, our ﬁrst\nimpression is",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "times\nin the\ntop three\naugmentations, which counts\nfor"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "that\nthere is no such a single augmentation method works",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "around half of\nthe rows.\nTherefore,\nit\nis suggested to try"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "equally well for all the different datasets. For example,\nthe",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "permutation ﬁrst for the related applications."
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "permutation shows\nimproved or comparable performance",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "on most of the datasets, but decreased performance is also",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "observed for the MMIDB-EEG dataset. The dataset varies",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "4.2.2\nPerformance evaluation of automatic and practi-"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "in terms of data type (e.g., ECG, EEG, EDA and IMU) and",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "cal DA methods."
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "tasks (sleep quality, heart disease, human activities, etc).",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": ""
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "We can ﬁnd that\nthe effectiveness of an augmentation not",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "The performance evaluation of automatic and random DA"
        },
        {
          "UCI-HAR\nConv-1D\n91.92\n89.11\n91.79": "only depends on the dataset itself (data type, task), but also",
          "93.55\n73.91\n90.19\n89.41\n91.75\n92.20": "is displayed in Table.2, where both the baseline (No Aug)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Performance evaluation of different backbones trained ods? First,wesummarizedthedesiredattributesforeffec-",
      "data": [
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "Best\nw/o DA on various datasets.\nrepresents the highest perfor-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "mance achieved by different DA methods. Auto indicts automatic"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "DA method, and Rand means random DA."
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "Best\nDataSet\nBackbone\nNo Aug\nAuto\nRand"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n60.29\n66.34\n62.77\n61.86"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "PTB-XL\nConv-1D\n75.00\n77.91\n78.75\n77.91"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n77.24\n78.03\n78.21\n79.60"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n53.57\n57.90\n57.42\n57.84"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "Apnea-ECG\nConv-1D\n80.14\n82.67\n79.1\n81.33"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n76.64\n77.74\n73.52\n77.64"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n49.85\n56.27\n53.68\n55.38"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "Sleep-EDF\nConv-1D\n83.62\n85.00\n84.44\n85.30"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n83.61\n85.01\n85.26\n85.16"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n77.24\n78.68\n79.29\n78.53"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MMIDB-EEG\nConv-1D\n79.29\n79.72\n81.23\n81.66"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n76.16\n80.15\n79.61\n78.53"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n76.71\n79.06\n74.93\n74.91"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "CLAS\nConv-1D\n62.83\n77.96\n73.26\n68.73"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n76.54\n76.54\n74.65\n69.36"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n61.15\n66.17\n60.71\n54.51"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "PAMAP2\nConv-1D\n89.22\n92.02\n91.88\n88.18"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n89.81\n92.02\n91.14\n87.00"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "MLP\n87.89\n90.80\n86.77\n87.48"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "UCI-HAR\nConv-1D\n91.92\n93.55\n92.5\n93.86"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ResNet-1D\n89.79\n92.94\n91.72\n93.38"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "and best performance achieved by different augmentations"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "(Best) from Table.1 are also included for easy comparison."
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "We can conclude that"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "I). Random DA is both effective and efﬁcient\n(R3). As"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "we can see from Table.1, the performance trained with ran-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "dom DA generally outperforms the baselines (No Aug), and"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "are comparable with the best performance across different"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "datasets and backbones. Therefore, random DA is an efﬁ-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "cient, effective, and practical automated data augmentation"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "method for biobehavioral time series data."
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "II). Automatic DA outperforms most comparable base-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "lines (R1). From the various possible DA combinations and"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "their corresponding magnitudes,\nthe automatic DA method"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "helps us search for\nthe more effective DA policies (oper-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ations and magnitudes).\nAs we may ﬁnd that\nthe auto-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "matic DA method (Auto) generally outperforms the base-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "lines across different datasets and backbones, and achieves"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "comparable or higher performance\nthan the best perfor-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "mance, which is achieved by manually selected DAs."
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "4.2.3\nQuantitative measurement\nof\nthe\neffectiveness"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "for different DA methods. (R4)"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": ""
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "We tried to answer\nthe question of why some DA meth-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "ods are more effective than others, and how can we quan-"
        },
        {
          "Table 2.\nPerformance evaluation of different backbones trained": "titatively measure the effectiveness of different DA meth-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "80": ""
        },
        {
          "80": "78"
        },
        {
          "80": "76"
        },
        {
          "80": "74"
        },
        {
          "80": ""
        },
        {
          "80": "72"
        },
        {
          "80": ""
        },
        {
          "80": "61"
        },
        {
          "80": "Accuracy"
        },
        {
          "80": "60"
        },
        {
          "80": ""
        },
        {
          "80": "59"
        },
        {
          "80": ""
        },
        {
          "80": "58"
        },
        {
          "80": ""
        },
        {
          "80": "57"
        },
        {
          "80": "56"
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": ""
        },
        {
          "80": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "5. Conclusion"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "In this work, we ﬁrst conducted a comprehensive and"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "systematic\nevaluation of various basic DAs on different"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "biobehavioral datasets, ﬁnding that all the datasets can ben-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "eﬁt from DA if using them carefully. The effectiveness of an"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "augmentation depended on both the dataset itself (data type,"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "task) and the used backbone. Further, we explored to adopt"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "more recent DA techniques for biobehavioral signals by de-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "signing a different policy architecture. The results demon-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "strated that automatic DA can help learn effective policies"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "but at a cost of high computational complexity, while ran-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "dom DA was demonstrated to be both effective and efﬁcient"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "during our experiments. At last, we attempted to answer the"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "question of why DA is effective (or not), by ﬁrst summa-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "rizing two desired attributes (challenging and faithful), and"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "then two we used two quantitative metrics\n(diversity and"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "afﬁnity) to measure the corresponding attributes of DA. This"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "can help understand why a DA works (or not), and guide us"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "for the search of more effective DA methods in the future."
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "We hope our experimental results could shed some light"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "on DA for biobehavioral time series data, as a carefully se-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "lected DA could outperform many claimed improvements in"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "the literature. Our next plan is to investigate domain knowl-"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "edge inspired DA, and also develop more efﬁcient way to"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "quantitatively measure the effectiveness of a DA method."
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": "References"
        },
        {
          "Figure 4. Performance when number of operation (a) and magnitude (b) is changed on the PTB-XL dataset.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "2"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "teenth"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "ference Proceedings, 2011. 3",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Computer Vision, pages 12219–12228, 2021. 1, 3"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[3]\nPing Cao, Xinyi Li, Kedong Mao, Fei Lu, Gangmin Ning,",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[17] Zirui Liu, Haifeng Jin, Ting-Hsiang Wang, Kaixiong Zhou,"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Luping Fang,\nand Qing Pan.\nA novel data augmentation",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "and Xia Hu.\nDivaug:\nPlug-in automated data augmenta-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "method to enhance deep neural networks\nfor detection of",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "tion with explicit diversity maximization.\nIn Proceedings"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Biomedical Signal Processing and Con-\natrial ﬁbrillation.",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "of the IEEE/CVF International Conference on Computer Vi-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "trol, 56:101675, 2020. 2",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "sion, pages 4762–4770, 2021. 3"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[4] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[18] Valentina Markova, Todor Ganchev,\nand Kalin Kalinkov."
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "van, and Quoc V Le. Autoaugment: Learning augmentation",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Clas: A database for cognitive load, affect and stress recog-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "strategies from data.\nIn Proceedings of the IEEE/CVF Con-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "nition.\nIn 2019 International Conference on Biomedical In-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "ference on Computer Vision and Pattern Recognition, pages",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "novations and Applications (BIA), pages 1–4. IEEE, 2019. 5,"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "113–123, 2019. 1, 2",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "10, 11"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[19]\nFernando Moya Rueda, Ren´e Grzeszick, Gernot A Fink,"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Sascha Feldhorst, and Michael Ten Hompel. Convolutional"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Le.\nRandaugment:\nPractical\nautomated data\naugmenta-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "neural networks for human activity recognition using body-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "the\ntion with a reduced search space.\nIn Proceedings of",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "worn sensors.\nIn Informatics, volume 5, page 26. Multidis-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "IEEE/CVF Conference\non Computer Vision\nand Pattern",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "ciplinary Digital Publishing Institute, 2018. 11"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Recognition Workshops, pages 702–703, 2020. 1, 3, 4",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[20] Hiroki Ohashi, M Al-Nasser,\nSheraz Ahmed,\nTakayuki"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[6]\nJohn Cristian Borges Gamboa. Deep learning for time-series",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Akiyama, Takuto Sato, Phong Nguyen, Katsuyuki Naka-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "analysis. arXiv preprint arXiv:1701.01887, 2017. 1",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "mura, and Andreas Dengel.\nAugmenting wearable sensor"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[7] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "data with physical constraint\nfor dnn-based human-action"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mi-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "recognition.\nIn ICML 2017 times\nseries workshop, pages"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "etus, George B Moody, Chung-Kang Peng, and H Eugene",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "6–11, 2017. 2"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Stanley. Physiobank, physiotoolkit, and physionet: compo-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[21] Thomas Penzel, George B Moody, Roger G Mark, Ary L"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "nents of a new research resource for complex physiologic",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Goldberger,\nand\nJ\nHermann\nPeter.\nThe\napnea-ecg"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "signals. circulation, 101(23):e215–e220, 2000. 10",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "database.\nIn Computers in Cardiology 2000. Vol. 27 (Cat."
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[8] Raphael Gontijo-Lopes, Sylvia Smullin, Ekin Dogus Cubuk,",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "00CH37163), pages 255–258. IEEE, 2000. 5, 10, 11"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "and Ethan Dyer. Tradeoffs in data augmentation: An empir-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[22] Attila Reiss and Didier Stricker.\nIntroducing a new bench-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "ical study.\nIn International Conference on Learning Repre-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "marked dataset for activity monitoring.\nIn 2012 16th inter-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "sentations, 2021. 3, 5, 7, 11",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "national symposium on wearable computers, pages 108–109."
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[9] Daniel Ho, Eric Liang, Xi Chen,\nIon Stoica,\nand Pieter",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "IEEE, 2012. 5, 10, 11"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Abbeel.\nPopulation based augmentation: Efﬁcient\nlearning",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[23]\nJorge-L Reyes-Ortiz,\nLuca Oneto, Albert Sam`a, Xavier"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "of augmentation policy schedules.\nIn International Confer-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Parra, and Davide Anguita. Transition-aware human activity"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "ence on Machine Learning, pages 2731–2741. PMLR, 2019.",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "recognition using smartphones. Neurocomputing, 171:754–"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "3",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "767, 2016. 5, 11"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[10] Hassan Ismail Fawaz, Germain Forestier,\nJonathan Weber,",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[24] C´edric Rommel,\nThomas Moreau,\nJoseph\nPaillard,\nand"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Lhassane Idoumghar, and Pierre-Alain Muller. Deep learn-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Alexandre Gramfort.\nCadda: Class-wise automatic differ-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "ing for time series classiﬁcation: a review. Data mining and",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "arXiv preprint\nentiable data augmentation for eeg signals."
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "knowledge discovery, 33(4):917–963, 2019. 1",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "arXiv:2106.13695, 2021. 3, 4"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[11] Brian Kenji\nIwana and Seiichi Uchida.\nAn empirical sur-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[25] Karel Roots, Yar Muhammad, and Naveed Muhammad. Fu-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "vey of data augmentation for time series classiﬁcation with",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "sion convolutional neural network for cross-subject eeg mo-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "neural networks. Plos one, 16(7):e0254841, 2021. 1, 2",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "tor imagery classiﬁcation. Computers, 9(3):72, 2020. 10"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[12] Bob Kemp, A Zwinderman, B Tuk, H Kamphuisen, and J",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[26] G. Schalk, D.J. McFarland, T. Hinterberger, N. Birbaumer,"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Obery´e. Sleep-edf database expanded. 2018. 5, 10",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "and\nJ.R. Wolpaw.\nBci2000:\na\ngeneral-purpose\nbrain-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[13] Bob Kemp, Aeilko H Zwinderman, Bert Tuk, Hilbert AC",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "IEEE Transactions on\ncomputer\ninterface\n(bci)\nsystem."
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Kamphuisen, and Joseﬁen JL Oberye. Analysis of a sleep-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "Biomedical Engineering, 51(6):1034–1043, 2004. 5, 10, 11"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "dependent neuronal feedback loop:\nthe slow-wave microcon-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[27] Odongo Steven Eyobu and Dong Seog Han. Feature repre-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "IEEE Transactions on Biomedical Engi-\ntinuity of the eeg.",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "sentation and data augmentation for human activity classiﬁ-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "neering, 47(9):1185–1194, 2000. 11",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "cation based on wearable imu sensor data using a deep lstm"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[14] Yonggang\nLi,\nGuosheng Hu,\nYongtao Wang,\nTimothy",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "neural network. Sensors, 18(9):2892, 2018. 2"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Hospedales, Neil M Robertson, and Yongxin Yang. Dada:",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[28] Akara Supratak and Yike Guo.\nTinysleepnet: An efﬁcient"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "arXiv preprint\ndifferentiable automatic data augmentation.",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": ""
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "deep learning model\nfor sleep stage scoring based on raw"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "arXiv:2003.03780, 2020. 1, 3, 4",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "single-channel eeg. In 2020 42nd Annual International Con-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[15]\nSungbin Lim,\nIldoo Kim, Taesup Kim, Chiheon Kim, and",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "ference of the IEEE Engineering in Medicine & Biology So-"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Advances in Neural\nSungwoong Kim.\nFast autoaugment.",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "ciety (EMBC), pages 641–644. IEEE, 2020. 10"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Information Processing Systems, 32, 2019. 1, 3",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "[29] Alex Tamkin, Mike Wu, and Noah Goodman. Viewmaker"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "[16] Aoming Liu, Zehao Huang, Zhiwu Huang,\nand Naiyan",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "networks: Learning views\nfor unsupervised representation"
        },
        {
          "and Statistics, pages 164–172. JMLR Workshop and Con-": "Wang. Direct differentiable augmentation search.\nIn Pro-",
          "ceedings\nof\nthe\nIEEE/CVF International Conference\non": "learning. arXiv preprint arXiv:2010.07432, 2020. 5, 11"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Muriel Lang, Sandra Hirche, Urban Fietzek, and Dana Kuli´c.",
          "with physiological signals and sleep stages that were anno-": "tated manually by well-trained technicians.\nIn this dataset,"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Data augmentation of wearable sensor data for parkinson’s",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "the\nphysiological\nsignals,\nincluding Fpz-Cz/Pz-Oz\nelec-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "disease monitoring using convolutional neural networks.\nIn",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "troencephalogram (EEG),\nelectrooculogram (EOG),\nand"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Proceedings of\nthe 19th ACM international conference on",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "chin electromyogram (EMG), were sampled at 100 Hz. We"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "multimodal interaction, pages 216–220, 2017. 1, 2",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "targeted to detect 5 sleep stages.\nincluding wakefulness,"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "[31]\nPatrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Di-",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "stage N1, N2, N3, and REM [?]. To model the relationship"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "eter Kreiseler, Fatima I Lunze, Wojciech Samek, and Tobias",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "between the sleep patterns and physiological data, we split"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Schaeffter. Ptb-xl, a large publicly available electrocardiog-",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "the whole-night\nrecordings\ninto 30-second Fpz-Cz ECG"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "raphy dataset. Scientiﬁc data, 7(1):1–15, 2020. 5, 10, 11",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "segments as in [28], which resulted in a total of 42308 ECG"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "[32] Qingsong Wen,\nLiang\nSun,\nFan Yang, Xiaomin\nSong,",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "and sleep pattern pairs. We divided 25% of the samples into"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Jingkun Gao, Xue Wang, and Huan Xu.\nTime series data",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "a testing set according to the order of the subject IDs."
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "arXiv preprint\naugmentation for deep learning: A survey.",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "arXiv:2002.12478, 2020. 1, 2",
          "with physiological signals and sleep stages that were anno-": "MMIDB-EEG: The MMIDB-EEG dataset\n[26]\nstud-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "ies the relationship between physiological EEG and human"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Appendix",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "body physical/imaginary movement. This dataset contains"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "over 1,500 EEG recordings in 1-2 minutes with a sampling"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "A. Datasets",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "rate of 160Hz from 109 subjects. Each subject performed"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "baseline (eyes open and close) and four\ntasks,\nincluding"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "PTB-XL:\nThe\nPTB-XL\n[31]\ndataset\nis\na\nlarge",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "open and close left or right ﬁst,\nimagine opening and clos-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "dataset\ncontaining 21,837 clinical 12-lead electrocardio-",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "ing left or\nright ﬁst, open and close both ﬁsts or\nfeet, and"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "gram (ECG)\nrecords\nfrom 18,885 patients of 10 second",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "imagine opening and closing both ﬁsts or both feet.\nFol-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "length, where 52% are male and 48% are female with ages",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "lowing [25], we omit\nthe data from 6 subjects due to the"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "range from 0 to 95 years (median 62 and interquantile range",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "incorrect annotations and split\nthe remaining data into 4s"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "of 22). There are two sampling rates: 100 Hz and 500 Hz,",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "segments, which results in 4635 segments in total. Our task"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "available in the dataset, but\nin our experiments, only data",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "focuses on the classiﬁcation between baseline and physical"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "sampled at 100 Hz are used.\nThe raw ECG data are an-",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "hand movement. Further, data from 22 subjects, based on"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "notated by two cardiologists into ﬁve major categories,\nin-",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "the order of subject ids, is split into the test set, and the rest"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "cluding normal ECG (NORM), myocardial infarction (MI),",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "samples are employed as the training set."
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "ST/T Change (STTC), Conduction Disturbance (CD) and",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Hypertrophy (HYP). The dataset contains a comprehensive",
          "with physiological signals and sleep stages that were anno-": "CLAS: The CLAS dataset [18] aims to support research"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "collection of various co-occurring pathologies and a large",
          "with physiological signals and sleep stages that were anno-": "on the automated assessment of certain states of mind and"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "proportion of healthy control\nsamples. We experimented",
          "with physiological signals and sleep stages that were anno-": "emotional conditions using physiological data. The dataset"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "classifying all 5 cardiac conditions as learning tasks. Fur-",
          "with physiological signals and sleep stages that were anno-": "consists of synchronized recordings of ECG, photopletys-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "ther,\nto ensure a fair comparison of machine learning algo-",
          "with physiological signals and sleep stages that were anno-": "mogram (PPG), electrodermal activity (EDA), and accel-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "rithms trained on the dataset, we follow the recommended",
          "with physiological signals and sleep stages that were anno-": "eration (ACC) signals. There are 62 healthy subjects who"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "splits of\ntraining and test\nsets, which results\nin a\ntrain-",
          "with physiological signals and sleep stages that were anno-": "participated and were involved in three interactive tasks and"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "ing/testing ratio of 8/1.",
          "with physiological signals and sleep stages that were anno-": "two perceptive tasks.\nThe perceptive tasks, which lever-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "aged the images and audio-video stimuli, were purposely"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Apnea-ECG: The Apnea-ECG [21] dataset studies the",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "selected to evoke emotions in the four quadrants of arousal-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "relationship between human sleep apnea\nsymptoms\nand",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "valence space.\nIn this study, our goal was to use the EDA"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "heart activities (monitored by ECG). This database can be",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "signal\nto detect binary high/low stress\nstates\nthat are an-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "accessed through Physionet\n[7].\nThis dataset contains 70",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "notated in arousal-valence space. We processed the raw"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "records with a sampling rate of 100 Hz,\nfrom where 35",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "EDA data with a\nlowpass Butterworth ﬁlter with a\ncut-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "records were divided into training, and the other 35 were",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "off\nfrequency of 0.2 Hz,\nthen split\nthe sequences into 10-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "divided into the test set. The duration of the records varies",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "second segments. We divided the train/test set in a subject-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "from slightly less than 7 hours to nearly 10 hours. The la-",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "independent manner and utilized the data from 17 subjects"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "bels were the annotation of each minute of each recording",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "",
          "with physiological signals and sleep stages that were anno-": "as the test set according to subject ids (> 45)."
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "indicating the presence or absence of sleep apnea. Thus, we",
          "with physiological signals and sleep stages that were anno-": ""
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "split the ECG recording into each minute, which was a total",
          "with physiological signals and sleep stages that were anno-": "PAMAP2: The PAMAP2 [22] physical activity moni-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "of 6000 data points for each separation. We extracted 17233",
          "with physiological signals and sleep stages that were anno-": "toring dataset consists data of 18 different physical activi-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "samples for the training set and 17010 samples for the test",
          "with physiological signals and sleep stages that were anno-": "ties, including household activities (sitting, walking, stand-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "set. And the ratio of non-apnea and apnea samples in the",
          "with physiological signals and sleep stages that were anno-": "ing, vacuum cleaning,\nironing, etc) and a variety of exer-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "training set was 61.49% to 38.51%.",
          "with physiological signals and sleep stages that were anno-": "cise activities (Nordic walking, playing soccer, rope jump-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "Sleep-EDFE: The Sleep-EDF (expanded)\n[12] dataset",
          "with physiological signals and sleep stages that were anno-": "ing,\netc),\nperformed by 9 participants wearing three\nin-"
        },
        {
          "[30] Terry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo,": "contains whole-night\nsleep recordings\nfrom 822 subjects",
          "with physiological signals and sleep stages that were anno-": "ertial measurement units (IMU) and a heart\nrate monitor."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3. Details of datasets used in the experiments.": "# Channels"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "1"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "1"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "1"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "64"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "1"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "52"
        },
        {
          "Table 3. Details of datasets used in the experiments.": "9"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "UCI-HAR [23]\nIMU\n30\n9\n128",
          "4775\n677\n12\nhuman activity recognition": "7352\n2947\n6\nhuman activity recognition"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "Accelerometer, gyroscope, magnetometer and temperature",
          "4775\n677\n12\nhuman activity recognition": "model\ntested on clean data. More formally,\nlet Dtrain and"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "data are recorded from the 3 IMUs placed on three differ-",
          "4775\n677\n12\nhuman activity recognition": "(cid:48) v\nDval be the training and validation datasets, and let D"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "al be"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "ent\nlocations (1 IMU over the wrist on the dominat arm, 1",
          "4775\n677\n12\nhuman activity recognition": "let F(·)\nan augmented dataset derived from Dval. Further,"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "IMU on the chest and 1 IMU on the dominant side’s an-",
          "4775\n677\n12\nhuman activity recognition": "be a model\ntrained on Dtrain, and Acc(F, D) denote the"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "kle) with sampling frequency 100Hz. Heart\nrate data are",
          "4775\n677\n12\nhuman activity recognition": "accuracy of the model when evaluated on dataset D. Then,"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "recorded from the heart\nrate monitor with sampling fre-",
          "4775\n677\n12\nhuman activity recognition": "for an augmentation τ , the Afﬁnity is deﬁned as:"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "quency 9Hz.\nThe resulting dataset has 52 dimensions (3",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "x 17 (IMU) + 1 (heart rate) = 52). Following the same set-",
          "4775\n677\n12\nhuman activity recognition": "(cid:48) v\nAcc(F, D\nal)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": ",\nAf f inity(τ ) =\n(8)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "ting as used in [19, 29], we linearly interpolated the miss-",
          "4775\n677\n12\nhuman activity recognition": "Acc(F, Dval)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "ing data (upsampling the sampling frequency from 9Hz to",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "(cid:48) v\nD\n(9)\nal = (cid:8)(τ (x), y)|∀(x, y) ∈ Dval"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "100Hz for heart rate), then took random 10s windows from",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "subject\nrecordings with an overlap of 7s, using the same",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "Diversity is deﬁned as the ratio of ﬁnal training loss of a"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "train/validation/test splits. As mentioned in [19, 29], 12 of",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "model trained with a given augmentation, and the loss of the"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "the total 18 different physical activities are used in the ex-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "model\ntrained on original data. Formally,\nlet τ be an aug-"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "periments.",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "(cid:48) t\nmentation and Dtrain and D\nrain be the training data and"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "UCI-HAR: The UCI-HAR [23] dataset was collected",
          "4775\n677\n12\nhuman activity recognition": "augmented training data respectively.\nFurther,\nlet L(θ|D)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "from a group of 30 volunteers with an age range from 19 to",
          "4775\n677\n12\nhuman activity recognition": "be the training loss of a model with parameter θ on the train-"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "48 years. During the data collection, all\nthe subjects wore",
          "4775\n677\n12\nhuman activity recognition": "ing data D. Then, we can deﬁne the Diversity as:"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "a smartphone (Samsung Galaxy S II) with embedded iner-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "tial sensors around their waist and were instructed to follow",
          "4775\n677\n12\nhuman activity recognition": "(cid:48) t"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "rain)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "Diversity(τ ) =\n,\n(10)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "an activity protocol performing six basic activities,\ninclud-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "L(θ|Dtrain)"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "ing three static postures (standing, sitting,\nlying) and three",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "(cid:48) t\nD\n(11)\nrain = (cid:8)(τ (x), y)|∀(x, y) ∈ Dtrain"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "dynamic activities (walking, walking downstairs and walk-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "ing upstairs). The dataset also included postural transitions",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "",
          "4775\n677\n12\nhuman activity recognition": "C. Model Architecture"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "that occurred between the static postures, but they were dis-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "carded in our experiments (also in related works), due to a",
          "4775\n677\n12\nhuman activity recognition": "A list of basic DA methods used in the paper is included"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "much smaller size for those transitions. 3-axial linear accel-",
          "4775\n677\n12\nhuman activity recognition": "in Table.4, where the range of magnitude, and default value"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "eration and 3-axial angular velocity were captured using the",
          "4775\n677\n12\nhuman activity recognition": "for individual DA is illustrated. The detailed information of"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "embedded accelerometer and gyroscope of the smartphone",
          "4775\n677\n12\nhuman activity recognition": "the Conv-1D and ResNet-1D are illustrated in Table. 6 and"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "at a constant rate of 50Hz. Following the authors’ sugges-",
          "4775\n677\n12\nhuman activity recognition": "Table. 7."
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "tion,\nthe sensor signals were sampled in a ﬁxed-width slid-",
          "4775\n677\n12\nhuman activity recognition": "The\ndetailed\ninformation\nof\nthe MLP, Conv-1D and"
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "ing windows of 2.56 second and 50% overlap, resulting in",
          "4775\n677\n12\nhuman activity recognition": "ResNet-1D are illustrated in Table.5, Table.\n6 and Table."
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "128 readings per window.",
          "4775\n677\n12\nhuman activity recognition": "7."
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "B. Quantitative Measurement\nfor Data Aug-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "mentation",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "In [8], Afﬁnity is deﬁned as the ratio between the valida-",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "tion accuracy of a model trained on clean data and tested on",
          "4775\n677\n12\nhuman activity recognition": ""
        },
        {
          "PAMAP2 [22]\nIMU\n9\n52\n1000": "an augmented validation set, and the accuracy of the same",
          "4775\n677\n12\nhuman activity recognition": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 7: Structure of the ResNet-1D model. B: batch size; L:",
      "data": [
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Some operations do not use the magnitude information (e.g. Rotation)."
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Operation Name"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Jittering"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Scaling"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Rotation"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Permutation"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Magnitude Warping"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Time Warping"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Window Slicing"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": "Window Warping"
        },
        {
          "Table 4. List of basic DA methods discussed in this paper. Additionally, the range of magnitude for individual operations is also reported.": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 7: Structure of the ResNet-1D model. B: batch size; L:",
      "data": [
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "Input Shape",
          "length of": "Parameter"
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "[B, L, C]",
          "length of": "-"
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "[B, L*C]",
          "length of": "[L*C, 500]"
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "[B, 500]",
          "length of": "[500, 256]"
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "[B, 256]",
          "length of": "[256, Classes]"
        },
        {
          "Table 5. Structure of the MLP model. B: batch size; L:": "",
          "length of": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 7: Structure of the ResNet-1D model. B: batch size; L:",
      "data": [
        {
          "Reshape": "Conv1d",
          "[B, L, C]": "[B, C, L]",
          "[B, C, L]": "[B, 64, L]",
          "-": "1x3, 64, max pool"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "(cid:21)"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "(cid:20)1 × 3,\n64"
        },
        {
          "Reshape": "Layer1 x",
          "[B, L, C]": "[B, 64, L/2]",
          "[B, C, L]": "[B, 64, L/2]",
          "-": "× 2"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "1 × 3,\n64"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "(cid:20)1 × 3,\n128"
        },
        {
          "Reshape": "Layer2 x",
          "[B, L, C]": "[B, 64, L/2]",
          "[B, C, L]": "[B, 128, L/4]",
          "-": "× 2"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "1 × 3,\n128"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "(cid:20)1 × 3,\n256"
        },
        {
          "Reshape": "Layer3 x",
          "[B, L, C]": "[B, 128, L/4]",
          "[B, C, L]": "[B, 256, L/8]",
          "-": "× 2"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "1 × 3,\n256"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "(cid:20)1 × 3,\n512"
        },
        {
          "Reshape": "Layer4 x",
          "[B, L, C]": "[B, 256, L/8]",
          "[B, C, L]": "[B, 512, L/16]",
          "-": "× 2"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": "1 × 3,\n512"
        },
        {
          "Reshape": "Average pool",
          "[B, L, C]": "[B, 512, L/16]",
          "[B, C, L]": "[B, 512, 1]",
          "-": "-"
        },
        {
          "Reshape": "FC",
          "[B, L, C]": "[B, 512]",
          "[B, C, L]": "[B, Classes]",
          "-": "[512, Classes]"
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        },
        {
          "Reshape": "",
          "[B, L, C]": "",
          "[B, C, L]": "",
          "-": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Enhancing human activity recognition using deep learning and time series augmented data",
      "authors": [
        "Luay Alawneh",
        "Tamam Alsarhan",
        "Mohammad Al-Zinati",
        "Mahmoud Al-Ayyoub",
        "Yaser Jararweh",
        "Hongtao Lu"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "2",
      "title": "Deep learners benefit more from out-of-distribution examples",
      "authors": [
        "Yoshua Bengio",
        "Frédéric Bastien",
        "Arnaud Bergeron",
        "Nicolas Boulanger-Lewandowski",
        "Thomas Breuel",
        "Youssouf Chherawala",
        "Moustapha Cisse",
        "Myriam Côté",
        "Dumitru Erhan",
        "Jeremy Eustache"
      ],
      "year": "2011",
      "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "3",
      "title": "Luping Fang, and Qing Pan. A novel data augmentation method to enhance deep neural networks for detection of atrial fibrillation",
      "authors": [
        "Ping Cao",
        "Xinyi Li",
        "Kedong Mao",
        "Fei Lu",
        "Gangmin Ning"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "4",
      "title": "Learning augmentation strategies from data",
      "authors": [
        "Barret Ekin D Cubuk",
        "Dandelion Zoph",
        "Vijay Mane",
        "Quoc V Vasudevan",
        "Le",
        "Autoaugment"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "authors": [
        "Barret Ekin D Cubuk",
        "Jonathon Zoph",
        "Quoc V Shlens",
        "Le"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "6",
      "title": "Deep learning for time-series analysis",
      "authors": [
        "John Cristian",
        "Borges Gamboa"
      ],
      "year": "2017",
      "venue": "Deep learning for time-series analysis",
      "arxiv": "arXiv:1701.01887"
    },
    {
      "citation_id": "7",
      "title": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals",
      "authors": [
        "Ary L Goldberger",
        "A Luis",
        "Leon Amaral",
        "Jeffrey Glass",
        "Plamen Hausdorff",
        "Roger Ch Ivanov",
        "Joseph Mark",
        "George Mietus",
        "Chung-Kang Moody",
        "H Eugene Peng",
        "Stanley"
      ],
      "year": "2000",
      "venue": "Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals"
    },
    {
      "citation_id": "8",
      "title": "Tradeoffs in data augmentation: An empirical study",
      "authors": [
        "Raphael Gontijo-Lopes",
        "Sylvia Smullin",
        "Ekin Dogus Cubuk",
        "Ethan Dyer"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "9",
      "title": "Population based augmentation: Efficient learning of augmentation policy schedules",
      "authors": [
        "Daniel Ho",
        "Eric Liang",
        "Xi Chen"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "10",
      "title": "Deep learning for time series classification: a review",
      "authors": [
        "Hassan Ismail Fawaz",
        "Germain Forestier",
        "Jonathan Weber",
        "Lhassane Idoumghar",
        "Pierre-Alain Muller"
      ],
      "year": "2019",
      "venue": "Data mining and knowledge discovery"
    },
    {
      "citation_id": "11",
      "title": "An empirical survey of data augmentation for time series classification with neural networks",
      "authors": [
        "Brian Kenji",
        "Seiichi Uchida"
      ],
      "year": "2021",
      "venue": "Plos one"
    },
    {
      "citation_id": "12",
      "title": "Sleep-edf database expanded",
      "authors": [
        "Bob Kemp",
        "B Zwinderman",
        "H Tuk",
        "Kamphuisen",
        "Oberyé"
      ],
      "year": "2018",
      "venue": "Sleep-edf database expanded"
    },
    {
      "citation_id": "13",
      "title": "Analysis of a sleepdependent neuronal feedback loop: the slow-wave microcontinuity of the eeg",
      "authors": [
        "Bob Kemp",
        "Bert Aeilko H Zwinderman",
        "Hilbert Tuk",
        "Josefien Jl Ac Kamphuisen",
        "Oberye"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "14",
      "title": "Dada: differentiable automatic data augmentation",
      "authors": [
        "Yonggang Li",
        "Guosheng Hu",
        "Yongtao Wang",
        "Timothy Hospedales",
        "Yongxin Neil M Robertson",
        "Yang"
      ],
      "year": "2004",
      "venue": "Dada: differentiable automatic data augmentation",
      "arxiv": "arXiv:2003.03780"
    },
    {
      "citation_id": "15",
      "title": "Fast autoaugment",
      "authors": [
        "Sungbin Lim",
        "Ildoo Kim",
        "Taesup Kim",
        "Chiheon Kim",
        "Sungwoong Kim"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Direct differentiable augmentation search",
      "authors": [
        "Aoming Liu",
        "Zehao Huang",
        "Zhiwu Huang",
        "Naiyan Wang"
      ],
      "year": "2021",
      "venue": "Pro-ceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Divaug: Plug-in automated data augmentation with explicit diversity maximization",
      "authors": [
        "Zirui Liu",
        "Haifeng Jin",
        "Ting-Hsiang Wang",
        "Kaixiong Zhou",
        "Xia Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Clas: A database for cognitive load, affect and stress recognition",
      "authors": [
        "Valentina Markova",
        "Todor Ganchev",
        "Kalin Kalinkov"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Biomedical Innovations and Applications (BIA)"
    },
    {
      "citation_id": "19",
      "title": "Convolutional neural networks for human activity recognition using bodyworn sensors",
      "authors": [
        "Fernando Moya Rueda",
        "René Grzeszick",
        "A Gernot",
        "Sascha Fink",
        "Michael Feldhorst",
        "Hompel"
      ],
      "year": "2011",
      "venue": "Informatics"
    },
    {
      "citation_id": "20",
      "title": "Augmenting wearable sensor data with physical constraint for dnn-based human-action recognition",
      "authors": [
        "Hiroki Ohashi",
        "M Al-Nasser",
        "Sheraz Ahmed",
        "Takayuki Akiyama",
        "Takuto Sato",
        "Phong Nguyen"
      ],
      "year": "2017",
      "venue": "ICML 2017 times series workshop"
    },
    {
      "citation_id": "21",
      "title": "The apnea-ecg database",
      "authors": [
        "Thomas Penzel",
        "George Moody",
        "Roger Mark",
        "Ary Goldberger",
        "J Hermann"
      ],
      "year": "2000",
      "venue": "Computers in Cardiology"
    },
    {
      "citation_id": "22",
      "title": "Introducing a new benchmarked dataset for activity monitoring",
      "authors": [
        "Attila Reiss",
        "Didier Stricker"
      ],
      "year": "2012",
      "venue": "2012 16th international symposium on wearable computers"
    },
    {
      "citation_id": "23",
      "title": "Transition-aware human activity recognition using smartphones",
      "authors": [
        "Jorge-L Reyes-Ortiz",
        "Luca Oneto",
        "Albert Samà",
        "Xavier Parra",
        "Davide Anguita"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "24",
      "title": "Class-wise automatic differentiable data augmentation for eeg signals",
      "authors": [
        "Cédric Rommel",
        "Thomas Moreau",
        "Joseph Paillard",
        "Alexandre Gramfort",
        "Cadda"
      ],
      "year": "2021",
      "venue": "Class-wise automatic differentiable data augmentation for eeg signals",
      "arxiv": "arXiv:2106.13695"
    },
    {
      "citation_id": "25",
      "title": "Fusion convolutional neural network for cross-subject eeg motor imagery classification",
      "authors": [
        "Karel Roots",
        "Yar Muhammad",
        "Naveed Muhammad"
      ],
      "year": "2020",
      "venue": "Computers"
    },
    {
      "citation_id": "26",
      "title": "Bci2000: a general-purpose braincomputer interface (bci) system",
      "authors": [
        "G Schalk",
        "D Mcfarland",
        "T Hinterberger",
        "N Birbaumer",
        "J Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "27",
      "title": "Feature representation and data augmentation for human activity classification based on wearable imu sensor data using a deep lstm neural network",
      "authors": [
        "Odongo Steven",
        "Dong Seog"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "28",
      "title": "Tinysleepnet: An efficient deep learning model for sleep stage scoring based on raw single-channel eeg",
      "authors": [
        "Akara Supratak",
        "Yike Guo"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "29",
      "title": "Viewmaker networks: Learning views for unsupervised representation learning",
      "authors": [
        "Alex Tamkin",
        "Mike Wu",
        "Noah Goodman"
      ],
      "year": "2020",
      "venue": "Viewmaker networks: Learning views for unsupervised representation learning",
      "arxiv": "arXiv:2010.07432"
    },
    {
      "citation_id": "30",
      "title": "Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks",
      "authors": [
        "Terry T Um",
        "M Franz",
        "Daniel Pfister",
        "Satoshi Pichler",
        "Muriel Endo",
        "Sandra Lang",
        "Urban Hirche",
        "Dana Fietzek",
        "Kulić"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "31",
      "title": "Ptb-xl, a large publicly available electrocardiography dataset. Scientific data",
      "authors": [
        "Patrick Wagner",
        "Nils Strodthoff",
        "Ralf-Dieter Bousseljot",
        "Dieter Kreiseler",
        "Fatima Lunze",
        "Wojciech Samek",
        "Tobias Schaeffter"
      ],
      "year": "2020",
      "venue": "Ptb-xl, a large publicly available electrocardiography dataset. Scientific data"
    },
    {
      "citation_id": "32",
      "title": "Time series data augmentation for deep learning: A survey",
      "authors": [
        "Qingsong Wen",
        "Liang Sun",
        "Fan Yang",
        "Xiaomin Song",
        "Jingkun Gao",
        "Xue Wang",
        "Huan Xu"
      ],
      "year": "2020",
      "venue": "Time series data augmentation for deep learning: A survey",
      "arxiv": "arXiv:2002.12478"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "L *c] [b"
      ],
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "",
      "authors": [
        "Relu Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "35",
      "title": "",
      "authors": [
        "Relu Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "36",
      "title": "B: batch size; L: length of sequence; C: number of channels. Layer Name Input Shape Output Shape Parameter Reshape",
      "venue": "Structure of the Conv-1D model"
    },
    {
      "citation_id": "37",
      "title": "Max Pooling Conv1d-2",
      "authors": [
        "Relu Batchnorm"
      ],
      "venue": "Max Pooling Conv1d-2"
    },
    {
      "citation_id": "38",
      "title": "Max Pooling Conv1d-3",
      "authors": [
        "Relu Batchnorm"
      ],
      "venue": "Max Pooling Conv1d-3"
    },
    {
      "citation_id": "39",
      "title": "Max Pooling Conv1d-4",
      "authors": [
        "Relu Batchnorm"
      ],
      "venue": "Max Pooling Conv1d-4"
    },
    {
      "citation_id": "40",
      "title": "ReLU Average Pool [B, 256, L/27",
      "authors": [
        "Batchnorm"
      ],
      "venue": "ReLU Average Pool [B, 256, L/27"
    }
  ]
}