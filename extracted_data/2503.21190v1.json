{
  "paper_id": "2503.21190v1",
  "title": "Leveraging Llms With Iterative Loop Structure For Enhanced Social Intelligence In Video Question Answering",
  "published": "2025-03-27T06:14:21Z",
  "authors": [
    "Erika Mori",
    "Yue Qiu",
    "Hirokatsu Kataoka",
    "Yoshimitsu Aoki"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Social intelligence, the ability to interpret emotions, intentions, and behaviors, is essential for effective communication and adaptive responses. As robots and AI systems become more prevalent in caregiving, healthcare, and education, the demand for AI that can interact naturally with humans grows. However, creating AI that seamlessly integrates multiple modalities, such as vision and speech, remains a challenge. Current video-based methods for social intelligence rely on general video recognition or emotion recognition techniques, often overlook the unique elements inherent in human interactions. To address this, we propose the Looped Video Debating (LVD) framework, which integrates Large Language Models (LLMs) with visual information, such as facial expressions and body movements, to enhance the transparency and reliability of question-answering tasks involving human interaction videos. Our results on the Social-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance without finetuning. Furthermore, supplementary human annotations on existing datasets provide insights into the model's accuracy, guiding future improvements in AI-driven social intelligence. The proposed dataset can be accessed via the following link (https://github.com/edrkr96/Social-IQ-2.0-Sub). Q3: Why does the man on the right interrupt the man on the left? A0: He doesn't care what he has to say. A1: He is too distracted by his phone to listen. A2: The man on the right was bored and wanted to change the subject. A3: The man on the right was trying to impress the young man on the left. Q1: How does the woman react when she sees the man? A0: The woman is happy to see the man and runs to hug him. A1: The woman barely acknowledges the man and quickly walks away, clearly upset. A2: The woman greets the man and says hi to him. A3: The woman is devastated to see the man and doesn't want to talk to him.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Social intelligence is a distinct human ability encompassing a broad range of skills necessary for successful social interactions and appropriate behavior across various contexts. It allows individuals to recognize and interpret others' emotions, intentions, and behavioral patterns, enabling nuanced and adaptive responses in everyday situations. For robotics to effectively integrate into human society and interact in daily life, understanding social intelligence will be crucial. While social intelligence has been extensively studied in fields such as sociology and psychology, where its role in human cognition and behavior is well-documented  [1] ,  [2] ,  [3] ,  [4] ,  [5] ,  [6] ,  [7] , it remains underexplored in the domains of AI and robotics.\n\nThe recent integration of robots into caregiving, healthcare, and education, coupled with the widespread adoption of conversational AI tools like ChatGPT  [8] , has created a growing demand for robots and AI systems capable of natural human-like interactions. To fulfill this demand, these systems need to be equipped with social intelligence to understand human emotions, interpret intentions, and respond appropriately. However, the development of AI that effectively utilizes multiple modalities, such as vision and speech, for seamless human communication remains a significant challenge  [9] .\n\nSeveral key challenges exist in developing socially intelligent robots and AI tools. While conversational AI models Fig.  1 . Two samples from the Social-IQ 2.0 dataset  [13] . Each video, depicting human interactions, is accompanied by approximately six questions that require advanced reasoning. For each question, one correct answer (green) and three incorrect answers (red) are provided.\n\nlike ChatGPT, trained on vast datasets, have demonstrated some degree of social intelligence by generating responses that account for cultural contexts, manners, and user intentions, there are few examples where large language models (LLMs) are explicitly used to understand multimodal human interactions. Most existing approaches to understanding human interactions in videos  [10] ,  [11]  are extensions of general video comprehension or emotion recognition techniques and do not explicitly extract diverse information critical for social understanding, such as dialogue content, facial expressions, voice tone, and scene context. Furthermore, many of these approaches  [10] ,  [11] ,  [12]  are end-to-end models, which often lack transparency and reliability, particularly for complex tasks like question-answering based on human interaction videos.\n\nTo address these challenges, we propose a novel framework called Looped Video Debating (LVD), which leverages the social intelligence capabilities of LLMs while explicitly incorporating detailed visual information, such as facial expressions and body movements. LVD is designed as a looped framework for question-answering tasks related to human interaction videos, combining an LLM with a visual question answering (VQA) model. This design improves transparency and reliability by enabling the model to produce not only answers but also rationale and additional information required.\n\nOur experimental results on the Social-IQ 2.0 benchmark  [13]  show that LVD achieves state-of-the-art performance without fine-tuning. Furthermore, to assess the accuracy of the generated rationale and additional information, we conduct supplementary annotations on existing datasets and compare the performance between humans and LLMs in detail. This analysis offers valuable insights for future enhancements of the model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Social Intelligence Recognition In Ai",
      "text": "Social intelligence is the ability to understand others' intentions and the atmosphere of a situation through verbal and non-verbal communication and to behave appropriately in social interactions. This ability is essential for natural interactions between robots and humans, and it has become a focus of active research in the fields of robotics and AI.\n\nIn robotics, social intelligence is often evaluated through simulations. For example, Lee et al.  [14]  measure a robot's social intelligence using counseling simulations. They assess how well the robot-generated responses match human responses in categories such as speech, action, facial expression, and emotion to evaluate how accurately the robot can replicate non-verbal empathy styles. Similarly, Zhou et al.  [15]  evaluate the interactions of artificial agents in various social scenarios from multiple perspectives, such as goal achievement and relationship maintenance. Their study defines agent actions in each turn, including facial expressions and movements, enabling the evaluation of nonverbal information. However, these simulations are limited to predefined textual information and do not use images or videos, lacking detailed visual information.\n\nIn the field of computer vision, social intelligence is often evaluated through question answering (QA) tasks involving images or videos, as seen in benchmarks like VCR  [16] , MovieQA  [17] , and TVQA  [18] . VCR is limited to imagebased inference, while MovieQA and TVQA include questions about people or actions in videos, typically beginning with \"what\" or \"who\". In contrast, Social-IQ  [19]  and its updated version, Social-IQ 2.0  [13] , offer more challenging benchmarks by incorporating diverse modalities, such as videos, images, dialogues and audio, and presenting advanced questions that explore the causes behind specific emotions and the intentions behind behaviors. We use Social-IQ 2.0 as our evaluation benchmark due to its richness of modalities and the complexity of its questions. However, to evaluate aspects not covered in the existing Social-IQ 2.0 dataset, such as answer validity and rationale, we added additional annotations to a subset of the data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Social Intelligence Recognition Method",
      "text": "Just Ask Plus is state-of-the-art method in Social-IQ 2.0 benchmark, one of the most widely used social intelligence datasets. This method focuses on acquiring multimodal representations and selecting feature extractors. It relies on choosing the generated option feature most similar to the question feature, making it challenging to answer atypical questions or those that depend heavily on video context. The most accurate method with training is the Multi-Modal Temporal Correlated Network with Emotional Social Cues (MMTC-ESC)  [11] . This approach surpasses existing methods by utilizing multimodal inputs and contrastive learning with emotional features of question-option pairs. However, since it concentrates exclusively on emotions, it often fails to select the correct answer when the type of emotion alone is insufficient to justify the answer. Analysis in  [11]  indicates that there is only about a 25% chance that the correct option's emotion is not present in the other three options.\n\nWith the recent remarkable improvements, LLMs have developed the ability to recognize human interactions within text. Current research has assessed their social intelligence using text-based QA  [20]  and simulations with artificial agents  [15] . However, these studies are limited to textual modalities, treating non-verbal cues, such as facial expressions and gestures, as simple text descriptions.\n\nAt the same time, the application of LLMs to videorelated tasks has been expanding  [21] ,  [22] ,  [23] ,  [24] . Despite this growth, recent studies show that video-based LLMs face significant challenges, including the integration of multiple modalities and the comprehension of long-duration videos  [25] ,  [26] ,  [27] ,  [28] . These limitations make them less effective for advanced content understanding tasks like Social-IQ 2.0 dataset.\n\nTo address these challenges, this study explores the potential for improving accuracy in an end-to-end framework for the Social-IQ 2.0 dataset by integrating an LLM with a VQA model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Approach",
      "text": "This study aims to evaluate social intelligence of LLMs by conducting QA using images and dialogue information from the Social-IQ 2.0 dataset under various settings. Additionally, to improve QA accuracy without additional training of LLM, we propose a framework called Looped Video Debating (LVD), which combines LLM with a VQA model. The overview of the proposed method is shown in Figure  2 . As a novel setting, we included an \"unanswerable\" option in addition to the originally provided options. When LLM thought the question is unanswerable, it infers the additional information needed to answer, retrieves this information through the VQA model, and attempts to answer the question again. Section III-A describes the method for inferring additional information, and Section III-B explains the process for retrieving this information. Finally, to analyze the QA results from LLM, we constructed a sub-dataset by adding annotations to a portion of the videos in Social-IQ 2.0, the details of which are discussed in Section III-C.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Inferring Additional Information With Llms",
      "text": "To enhance the reliability and interpretability of responses in QA tasks, we first allow LLM to determine whether it is possible to answer a question based solely on the input information related to the video such as retrived frames and automatic transcription. Specifically, we instruct LLM with prompts that outline how to respond in cases where it can answer the question and in cases where it cannot. When the question is judged answerable, LLM outputs the option it believes to be correct. If judged unanswerable, LLM outputs the additional information needed for answering, as well as the relevant time segments of the video. The additional information is selected from predefined options, including In this method, the model first determines whether the question is answerable based on 10 images (or captions, in the of GPT-4 and Llama) and the dialogue information (blue). If the question is considered answerable, the option inferred to be correct is output (the green arrow). If the question is deemed unanswerable, a loop structure is employed to obtain additional information (red dashed arrows). This additional information is then added to the original input, and the QA process is repeated (red solid arrows).\n\nthe \"scene context\", \"appearance of people\", \"facial expressions\", \"motion\", \"tone of voice\", and \"accurate dialogue\". The option \"accurate dialogue\" was included due to concerns that the accuracy might be insufficient based on our review of the actual transcript data. LLM infers the relevant time segments of the video based on the timestamps included in the dialogue information within the dataset, outputting the start and end times as integers.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Retrieving Additional Information Through The Loop Structure",
      "text": "Only when the additional information predicted in Section III-A involves visual modalities, an image corresponding to the video reference time predicted by the LLM is retrieved from the dataset. Among the available options for additional information, the following four are classified as visual information: \"scene context\", \"appearance of people\", \"facial expressions\", and \"motion\". The timing for image retrieval is determined by rounding the average of the predicted start and end times to the nearest integer.\n\nThe retrieved image is then input into the VQA model, along with prompts specifically defined for each modality, to generate detailed descriptions of the respective additional information. The visual information output by the VQA model is subsequently incorporated into the default prompt for the LLM (as explained in Section III-A), and the model attempts to answer the question again. Notably, the \"unanswerable\" option is still retained in the second attempt, allowing for more reliable results compared to conventional methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Construction Of The Social-Iq Sub",
      "text": "As mentioned earlier, the proposed method differs from the standard Social-IQ 2.0 approach in the following ways:\n\n(1) it introduces an \"unanswerable\" option, allowing for the retrieval of additional information when a question is deemed unanswerable; (2) it leverages tailored prompts to output the reasoning and justification behind answers, enabling an analysis of the model's response tendencies. To demonstrate the effectiveness of the proposed method and identify potential areas for improvement, we conducted an analysis that includes a comparison with human responses. To this end, we constructed a manually annotated dataset Social-IQ Sub.\n\nOverview of the Annotation Process. In the Social-IQ Sub, annotations were added to 200 videos randomly sampled from the Social-IQ 2.0 dataset: 174 from the training set and 26 from the validation set (This sampling ratio reflects the proportion of videos with full annotations in the downloaded Social-IQ 2.0 dataset). To address concerns that inaccurate transcripts might affect the precise evaluation of human response accuracy and the identification of necessary additional information (as discussed in III-A), we generated more accurate transcripts using Whisper (large)  [29] , a stateof-the-art speech recognition model.\n\nThe annotation process was divided into two tasks: (1) answering questions based solely on the videos, and (2) answering questions based solely on the transcripts. Six annotators, three assigned to each task, completed the annotations over approximately one month. Quality control was performed in two stages: initially, a quality assurance reviewer checked the grammar and content, followed by a project manager who verified the number and format of the questions.\n\nAnnotation Items. The primary aspects we aim to analyze with the newly constructed sub-dataset are as follows: (1) comparison of accuracy with human responses, (2) accuracy in determining answerability, (3) accuracy of the rationale behind responses, (4) accuracy of additional information retrieved when a question is deemed unanswerable, and (5) accuracy of the referenced video timestamps. To enable the analysis of these aspects, we defined specific annotation criteria for answers derived from both the video and the transcript.\n\nFor the video-only task, annotations were made for each question regarding answerability, the chosen option, the reasoning behind the answer, the information used as the basis for the answer (or additional information required if the question was deemed unanswerable), and the video time segment referenced for the answer. To facilitate analysis, predefined options were provided for the information used as the basis for the answer, with free-form responses allowed only when none of the predefined options applied. The predefined options included \"contents of conversation\", \"scene context\", \"appearance of people\", \"facial expression\", \"motion\", \"tone of voice\", and \"other information\".\n\nFor the transcript-only task, annotations covered answerability, the chosen option, the reasoning behind the answer, additional information required (only if the question was deemed unanswerable), and the video time segment referenced. The additional information options in this case included all items from the video-only task except for \"contents of conversation\".",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "As previously mentioned, we used the Social-IQ 2.0 dataset  [13]  for our experiments. Social-IQ 2.0 consists of 934 valid videos and 6,020 related questions. The number of videos is slightly reduced from the original dataset due to changes in availability and region-based restrictions on YouTube. Each video is approximately 60 seconds long, with around six annotated questions per video. For each question, one correct answer and three incorrect answers are provided. The dataset also includes dialogue information (transcripts obtained via YouTube's automatic transcription feature), around 180 images extracted from each video at 3 frames per second, and an audio file.\n\nThe LLMs used in this study were Llama 3 (8B-Instruct)  [30] , GPT-4  [31] , and GPT-4o  [32] . To accommodate context window limitations, we used GPT-4 Turbo (hereafter referred to as GPT-4). BLIP-2  [33]  was employed as the caption generation model, and GPT-4o served as the VQA model.\n\nIn this experiment, we conducted QA on the validation data (120 videos, 807 questions) using the dialogue information (transcripts) and images. Since Llama 3 and GPT-4 cannot directly process images, we input the generated captions instead. The evaluation metrics were accuracy across all questions and accuracy across answerable questions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Accuracy Comparison Between Existing Methods And Llms",
      "text": "We evaluated the social intelligence of LLMs by comparing their accuracy to existing methods. Specifically, we performed QA on 120 validation videos from the Social-IQ",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Modality Accuracy",
      "text": "Just Ask Plus  [10]  video-based 53.4% MMTC-ESC  [11]  video-based 74.35% DeSIQ  [12]  video-based 62.28% Llama 3  [30]  image-based 52.66% GPT-4  [31]  image-based 67.04% GPT-4o  [32]  image-based 75.22%\n\n2.0 dataset using Llama 3, GPT-4, and GPT-4o. To ensure a fair comparison with existing methods, we provided prompts that required the LLMs to produce an answer regardless of their certainty. Due to GPT-4o's limitation of processing only 10 input images, we used 10 evenly spaced frames extracted from each video. For GPT-4 and Llama 3, we used captions generated by BLIP-2 from the same 10 images. The results are presented in Table  I .\n\nFor existing methods, we selected Just Ask Plus  [10] , which has the highest accuracy among non-learning methods, and the Multi-Modal Temporal Correlated Network with Emotional Social Cues (MMTC-ESC)  [11] , which achieves the highest accuracy among learning methods. Additionally, we included the baseline framework (referred to as DeSIQ) developed in a study that focused on evaluating the Social-IQ dataset and creating a new dataset  [12] . DeSIQ processes features from each modality (question, correct answer, incorrect answers, transcript, or video) using a simple MLP. The accuracy of these existing methods is based on the values reported in their respective papers. Although each method utilized different modalities, to enable a more direct comparison, we excluded audio information and focused on visual information (images, captions, videos) and dialogue information (transcripts). Please note that the term \"video\" here refers to either the entire video or short clips segmented from the video.\n\nAs shown in Table  I , the QA accuracy based on GPT-4o significantly outperformed the non-learning method (Just Ask Plus) and even surpassed the accuracy of the learning methods (MMTC-ESC, DeSIQ). Similarly, GPT-4 achieved far higher accuracy than the non-learning method, while Llama 3 produced results comparable to the non-learning method. However, it should be noted that Llama 3 could not be prompted to answer all questions, potentially lowering its accuracy score (94 out of 807 questions were left unanswered and counted as incorrect). These results suggest that LLMs have already acquired a certain degree of social intelligence. Additionally, GPT-4o's accuracy is 8.2% higher than GPT-4, likely due to its multimodal capabilities and the information disparity between images and captions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Checking The Validity Of The Loop Structure",
      "text": "In this section, we assess the reliability of output answers by introducing an \"unanswerable\" option, a concept not previously explored in existing research. As shown in Table  II , the inclusion of this option reduced accuracy by about",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model (Setting)",
      "text": "# Correct # Wrong # Unanswerable Accuracy (overall) Accuracy (answered questions) Llama 3 (original)  [30]  425 382 -52.66% 52.66% Llama 3 (w/ \"unanswerable\")  [30]  330 213 264 40.89% 60.77% Llama 3 (LVD)  [30]  421 260 126 52.17% 61.82% GPT-4 (original)  [31]  541 266 -67.04% 67.04% GPT-4 (w/ \"unanswerable\")  [31]  334 104 369 41.39% 76.26% GPT-4 (LVD)  [31]  372 124 311 46.09% 75.00% GPT-4o (original)  [32]  607 200 -75.22% 75.22% GPT-4o (w/ \"unanswerable\")  [32]  581 182 44 72.00% 76.15% GPT-4o (LVD)  [32]  601 194 12 74.47% 75.60%",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "First Attempt:",
      "text": "[Response] Unanswerable; The provided transcript does not include any mention or indication of the girls screaming or the context around such an event occurring at 0:15. Therefore, based on the transcript alone, it is not possible to determine why the girls would have screamed. The transcript only discusses someone's body giving out and their legs having no feeling.\n\n[Additional Information] \"tone of voice\", \"facial expression\", \"accurate dialog\"\n\n[Timestamp] start: 12, end:\n\nNewly retrieved frame (14 s)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Retrieved Additional Information:",
      "text": "Visual Information",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "[Facial Expression]",
      "text": "In the image, the three people seated in the car exhibit different facial expressions. The person on the left appears to be looking to her left with a curious or slightly worried expression. The person on the right has a more neutral or slightly surprised expression, looking straight ahead or to the right. The person in the back seat has their face obscured, making it difficult to determine their expression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Second Attempt Using Additional Information:",
      "text": "[Response] A3; The transcript and the sequence of events suggest that the girls had an intense reaction, which likely stems from a sudden realization of something they had done wrong or thought they had made a mistake. This is supported by the conversation about physical reactions and the expressions on their faces in the images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Question And Options:",
      "text": "Question: Why did the girls suddenly scream at 0:15? A0: They were confused and didn't know what to do.\n\nA1: They hit an animal crossing the road. A2: They laughed it off and made jokes about the mistake.\n\nA3: They thought they made a mistake.\n\nFig.  3 . Result example. In the \"Question and options:\" section, red options indicate incorrect answers, while green options indicate correct ones. In this case, the first attempt resulted in \"Unanswerable\", but the correct answer was later produced using additional information from the VQA model. Under \"First attempt:\", the response, the additional information inferred by the LLM, and the corresponding video timestamps are recorded. Under \"Acquired additional information:\", the frame retrieved based on the predicted timestamps and the visual details obtained by the VQA from the frame is documented.\n\n16% for GPT-4 and 12% for Llama 3, highlighting the impact of uncertain responses. In contrast, GPT-4o showed only a 3.2% decrease, as it rarely chose \"unanswerable\". Additionally, Table  2  demonstrates that incorporating the loop structure improved QA accuracy by 2.5% to 11.3% compared to the non-loop scenario. This suggests that acquiring additional visual information inferred by the LLM helps answer complex questions related to social intelligence. For qualitative results, actual examples of responses are provided in Figure  3 . As illustrated, when the LLM accurately infers the necessary information and the relevant time segments of the video, it can utilize the retrieved information to reach the correct answer.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Comparison Of Human And Llm Responses",
      "text": "As an initial step in understanding the aspects humans focus on when interpreting interpersonal interactions and to enhance AI-based methods, we analyzed human responses and compared them with AI responses using the newly created Social-IQ Sub dataset.\n\nThe upper part of Table  III  shows the QA accuracy achieved by humans. To ensure a fair comparison between AI and human performance, we applied the proposed method to the Social-IQ Sub dataset and performed VideoQA, with the results displayed in the lower part of Table  III . As shown, human accuracy was not 100%, and in some \"unanswerable\" cases, the semantic distinctions between the options were unclear, or none of the options were appropriate. This highlights the inherent difficulty of designing suitable questions and choices for high-context benchmarks like Social-IQ. Therefore, it is essential for AI to reason like humans and help identify errors within the dataset.\n\nMoreover, the fact that humans labeled more options as \"unanswerable\" than GPT-4o indicates that determining \"unanswerable\" with clear reasoning is challenging, underscoring a significant difference between LLMs and AI. Next, we evaluated the accuracy of acquiring additional information within the loop structure and the correctness of the rationale provided for answers. Results related to the rationale for answers are shown in Figure  4 , while those related to additional information are presented in Figure  5 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Gpt-4O",
      "text": "Figure  4  shows the frequency of information types used to answer questions based solely on video content. As indicated, Llama 3 and GPT-4o produced results relatively close to those of humans, suggesting a reasonable degree of accuracy in the rationale predictions by LLMs. However, these models tend to rely on easily available information, such as \"contents of conversation\" and \"scene context\", while providing less of the information like \"motion\". This suggests that the proposed method, which limits additional information to a single image, may not fully capture the information available in the video.\n\nFigure  5  illustrates the frequency of additional information needed to answer questions deemed unanswerable using only transcripts. As shown, Llama 3 and GPT-4o produced results close to those of humans, indicating high accuracy in LLM predictions of additional information. However, Figures  4  and 5  show that GPT-4 deviates more from human judgment in both rationale and additional information compared to the other two LLMs. This may relate to the finding in Table  2  that, despite many unanswerable samples, GPT-4's accuracy did not improve significantly with the loop structure. We calculated the IoU (Intersection over Union) between the reference times provided by LLMs and humans, with the results shown in Table  IV . Note that the prediction of reference timestamps is limited to samples that can be answered using either the transcript or the video. As indicated in Table  IV , the IoU values range from 0.2 to 0.5, suggesting there is room for improvement. Among the three LLMs, GPT-4o achieved the highest accuracy. In the annotated data, the average reference time for all samples was 11.50 seconds when using the transcript and 24.96 seconds when using the video. The average reference times shown in the table are longer than those for all samples, regardless of modality or LLM. This implies that while the proposed method is effective at capturing key information throughout the entire video, it has difficulty with questions requiring focus on specific timestamps.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we evaluated the social intelligence of LLMs and improved accuracy on the Social-IQ 2.0 benchmark without fine-tuning by combining LLM capabilities with detailed visual information using a looped structure. Additionally, we created a new dataset by annotating human responses and their rationale (or additional information required) on a subset of Social-IQ 2.0 data, enabling a direct comparison between human and AI performance and contributing to enhancing AI's social intelligence capabilities. In the future, we plan to strengthen the framework by using Video LLMs as VQA models and incorporating audio modalities, based on insights gained from evaluating the newly created Social-IQ Sub dataset.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two samples from the Social-IQ 2.0 dataset [13]. Each video,",
      "page": 1
    },
    {
      "caption": "Figure 2: As a novel setting, we included an “unanswerable” option",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed method (LVD). In this method, the model first determines whether the question is answerable based on 10 images (or captions, in the",
      "page": 3
    },
    {
      "caption": "Figure 3: Result example. In the “Question and options:” section, red options indicate incorrect answers, while green options indicate correct ones. In this",
      "page": 5
    },
    {
      "caption": "Figure 3: As illustrated, when the LLM accurately infers",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparison of rationale for answers between humans and LLMs.",
      "page": 6
    },
    {
      "caption": "Figure 5: Comparison of additional information required by humans and",
      "page": 6
    },
    {
      "caption": "Figure 4: , while those",
      "page": 6
    },
    {
      "caption": "Figure 5: Figure 4 shows the frequency of information types used to",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrates the frequency of additional information",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Frames": "Instruction:\nPlease choose the correct answer from the options based on \ncaptions and transcript below. …\nTranscript:\n00:00:00.853 --> 00:00:02.943\nin your normal life what's your job well\ncurrently I'm focusing on living a life …\nQuestion & Options:\nQ2: Is the woman calm as she delivers her message?\nA0: Yes because she keeps a slight grin as she talks.\nA1: The woman is angry as she delivers her message.\nA2: No, because her voice sounds natural.\nA3: The woman is sad as she delivers her message.",
          "Unanswerable\nStart time: 56 s \nFrame retrieval \nfrom dataset\nRetrieved frame\n(58 s)": ""
        },
        {
          "Frames": "",
          "Unanswerable\nStart time: 56 s \nFrame retrieval \nfrom dataset\nRetrieved frame\n(58 s)": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Intelligence and its uses",
      "authors": [
        "E Thorndike"
      ],
      "year": "1920",
      "venue": "Harper's Magazine"
    },
    {
      "citation_id": "2",
      "title": "The measurement of social intelligence",
      "authors": [
        "T Hunt"
      ],
      "year": "1928",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "3",
      "title": "An evaluation of the attempts to measure social intelligence",
      "authors": [
        "R Thorndike",
        "S Stein"
      ],
      "year": "1937",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "4",
      "title": "The Measurement of Social Intelligence (Book style)",
      "authors": [
        "M O'sullivan",
        "R De Mille",
        "J Guilford"
      ],
      "year": "1965",
      "venue": "The Measurement of Social Intelligence (Book style)"
    },
    {
      "citation_id": "5",
      "title": "Social intelligence: Its history and measurement",
      "authors": [
        "R Walker",
        "J Foley"
      ],
      "year": "1973",
      "venue": "Psychological Reports"
    },
    {
      "citation_id": "6",
      "title": "Guilford's concept of social intelligence revisited",
      "authors": [
        "D Romney",
        "M Pyryt"
      ],
      "year": "1999",
      "venue": "High Ability Studies"
    },
    {
      "citation_id": "7",
      "title": "Reviving the search for social intelligence-a multitrait-multimethod study of its structure and construct validity",
      "authors": [
        "S Weis",
        "H.-M Süß"
      ],
      "year": "2007",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "8",
      "title": "Introducing ChatGPT",
      "authors": [
        "Openai"
      ],
      "year": "2022",
      "venue": "Introducing ChatGPT"
    },
    {
      "citation_id": "9",
      "title": "Artificial intelligence in HRM: role of emotional-social intelligence and future work skill",
      "authors": [
        "A Singh",
        "T Chouhan"
      ],
      "year": "2023",
      "venue": "*The Adoption and Effect of Artificial Intelligence on Human Resources Management, Part A*"
    },
    {
      "citation_id": "10",
      "title": "Just Ask Plus: Using Transcripts for VideoQA",
      "authors": [
        "M Pirhadi",
        "M Mirzaei",
        "S Eetemadi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "11",
      "title": "Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering",
      "authors": [
        "B Xie",
        "C Park"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "12",
      "title": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",
      "authors": [
        "X.-Y Guo"
      ],
      "year": "2023",
      "venue": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",
      "arxiv": "arXiv:2310.18359"
    },
    {
      "citation_id": "13",
      "title": "Social-IQ 2.0 Challenge: Benchmarking Multimodal Social Understanding",
      "authors": [
        "A Wilf",
        "L Mathur",
        "S Mathew",
        "C Ko",
        "Y Kebe",
        "P Liang",
        "L.-P Morency"
      ],
      "venue": "GitHub repository, 2023"
    },
    {
      "citation_id": "14",
      "title": "Developing social robots with empathetic non-verbal cues using large language models",
      "authors": [
        "Y Lee",
        "Y Jung",
        "G Kang",
        "S Hahn"
      ],
      "year": "2023",
      "venue": "*arXiv preprint*"
    },
    {
      "citation_id": "15",
      "title": "Sotopia: Interactive evaluation for social intelligence in language agents",
      "authors": [
        "X Zhou"
      ],
      "year": "2023",
      "venue": "Sotopia: Interactive evaluation for social intelligence in language agents",
      "arxiv": "arXiv:2310.11667"
    },
    {
      "citation_id": "16",
      "title": "From recognition to cognition: Visual commonsense reasoning",
      "authors": [
        "R Zellers"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Movieqa: Understanding stories in movies through question-answering",
      "authors": [
        "M Tapaswi"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Tvqa: Localized, compositional video question answering",
      "authors": [
        "J Lei"
      ],
      "year": "2018",
      "venue": "Tvqa: Localized, compositional video question answering",
      "arxiv": "arXiv:1809.01696"
    },
    {
      "citation_id": "19",
      "title": "Socialiq: A question answering benchmark for artificial social intelligence",
      "authors": [
        "A Zadeh",
        "M Chan",
        "P Liang",
        "E Tong",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark",
      "authors": [
        "M Choi"
      ],
      "year": "2023",
      "venue": "Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark",
      "arxiv": "arXiv:2305.14938"
    },
    {
      "citation_id": "21",
      "title": "TempCompass: Do Video LLMs Really Understand Videos?",
      "authors": [
        "Y Liu"
      ],
      "year": "2024",
      "venue": "TempCompass: Do Video LLMs Really Understand Videos?",
      "arxiv": "arXiv:2403.00476"
    },
    {
      "citation_id": "22",
      "title": "Retrieving-to-answer: Zero-shot video question answering with frozen large language models",
      "authors": [
        "J Pan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator",
      "authors": [
        "H Huang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning",
      "authors": [
        "H Lin"
      ],
      "year": "2023",
      "venue": "Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning",
      "arxiv": "arXiv:2309.15091"
    },
    {
      "citation_id": "25",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "M Maaz"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "26",
      "title": "Video-llama: An instruction-tuned audiovisual language model for video understanding",
      "authors": [
        "H Zhang"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audiovisual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "27",
      "title": "VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding",
      "authors": [
        "M Maaz"
      ],
      "year": "2024",
      "venue": "VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding",
      "arxiv": "arXiv:2406.09418"
    },
    {
      "citation_id": "28",
      "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "authors": [
        "Z Cheng"
      ],
      "year": "2024",
      "venue": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "29",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "30",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "31",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "32",
      "title": "Hello GPT-4o",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "Hello GPT-4o"
    },
    {
      "citation_id": "33",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "arxiv": "arXiv:2301.12597"
    }
  ]
}