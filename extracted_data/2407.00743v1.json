{
  "paper_id": "2407.00743v1",
  "title": "Aimdit: Modality Augmentation And Interaction Via Multimodal Dimension Transformation For Emotion Recognition In Conversations",
  "published": "2024-04-12T11:31:18Z",
  "authors": [
    "Sheng Wu",
    "Jiaxing Liu",
    "Longbiao Wang",
    "Dongxiao He",
    "Xiaobao Wang",
    "Jianwu Dang"
  ],
  "keywords": [
    "emotion recognition in conversations",
    "multimodal fusion",
    "dimension transformation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT to solve the problem of multimodal fusion of deep features. Specifically, we design a Modality Augmentation Network which performs rich representation learning through dimension transformation of different modalities and parameter-efficient inception block. On the other hand, the Modality Interaction Network performs interaction fusion of extracted inter-modal features and intra-modal features. Experiments conducted using our AIMDiT framework on the public benchmark dataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1 metrics compared to the state-of-the-art (SOTA) models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversations (ERC) is a novel task within the realm of natural language processing, holding a pivotal role in human-computer interaction  [1]  and social media analysis  [2] . There is an increasing focus on multimodal ERC, which involves leveraging text, audio, and visual information to achieve more accurate emotion recognition.\n\nMultimodal data tends to be asynchronous, with distributional gaps in text, audio, and visual modal data, which tends to introduce information redundancy  [3, 4] . In ERC, most of the work focuses on context modeling based on sequence models or graph models and often ignores the work on multimodal fusion  [5] [6] [7] . Although these early works can realize multimodal data fusion by simple early-late fusion  [8] [9] [10] [11] , they cannot extract the inter-modal interaction information and the simple concatenation operation makes it easy to introduce redundant information. In recent years, some ERC works based on multimodal fusion of depth graphs have been proposed  [12] , but due to the characteristics of depth graphs, redundant information gradually accumulates in the vector space of each layer and is prone to over-smoothing problem that makes node differentiation insufficient  [13] .\n\nWe have observed subtle distinctions in the distribution of text, audio, and visual information in emotional expressions, but they tend to be concentrated over a brief time span. For the purpose of discussion, we standardize the embedding dimensions of various modalities, treating vector length as the temporal dimension. Consequently, modal vectors can be conceptualized as 1D time series. Drawing inspiration from  [14] [15] [16] , we refer to changes in adjacent time intervals within a single modality and the same time interval across different modalities as intra-modal variation and inter-modal variation, respectively. To represent these characteristics, we transform 1D vectors into 2D, where the length axis signifies intra-modal information and the dimensional axis signifies inter-modal information. As a result, we successfully incorporate intra-and inter-modal features into 2D tensors.\n\nBased on the above motivation, we propose the Modality Augmentation and Interaction Network via Dimension Transformation (AIMDiT). We first design the Modality Augmentation Network (MAN) based on dimension change and convolution by Inception. Specifically, the MAN consists of multiple layers of MABlock with residual links, which captures intra-and inter-modal information in 2D space. In order to perform an effective fusion of modalities, inspired by  [17] , we propose the Interaction Network (MIN) where inter-modal feature and intra-modal feature guide the fusion of each other. Finally, emotion classifier predicts the emotion label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "The proposed model AIMDiT in this paper is shown in Fig.  1 . A primary utterance involves three modalities: U t (text), U a (audio), and U v (visual). Initially, we extract features from these modalities, yielding X t ,X a ,X v . To capture inter-and intra-modal features, we devise the Modality Augmentation Network, elevating X t , X a , X v to higher dimensions for richer information. Following this, the Modality Interaction Network interacts with the MAN's output, guided by a specific direction. This leads to subsequent emotion classification. Further details are outlined below.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality Augmentation Network (Man)",
      "text": "For the same modality, each time point involves both adjacent regions inside a modality and in-phase changes between different modalities, that is, changes within modalities and changes between modals. To capture both features at the same time, we build a module Modality Augmentation Block (MABlock), which explores rich information between different modalitys through dimensional changes. Specifically, ignoring the dimension d, we consider t, a and v, where each modal corresponds to having 1D sequences\n\nand with lengths T t , T a , T v , respectively. We can then convert the three tensors into a 2D tensor by using the following formula:\n\nwhere P adding(â€¢) is the process of padding each of the three tensors into a new tensor of uniform length, and use the length of the longest tensor of the three as the length of the new tensor T Î´ = max(T t , T a , T v ). Eventually we obtain a 2D tensor X t,a,v 2D . After the transformation, we process the 2D tensor by inception block with multiple 2D kernels of different scales to obtain a 2D tensor X t,a,v 2D with global information:\n\nSince the 2D kernel of the inception block is multiscale, it can capture information at adjacent time points within each modality and at the same time point for different modalities, and is able to aggregate intra-and inter-modal variations at different scales. Subsequently, we reconvert the learned X t,a,v 2D tensor into 1D space, and truncate the three tensors with length T Î´ back to their original lengths:\n\nAs shown in Fig.  1 , we connect the Modality Augmentation Block (MABlock) with residuals to form a MAN in order to prevent the problem of gradient vanishing in deep networks and to improve the expressive power of the network. Specifically, given three X t 1D , X a 1D , and X v 1D with lengths T t , T a , T v , respectively, which are represented by a one-dimensional tensor\n\n} with length T Î´ , and for the kth-layer MAN, the inputs are X k-1 1D , and its computation procedure is represented by Equation  4 :\n\nAccording to Eqs. 1, 2, and 3 the whole process of the kth MAB can be summarized as follows:\n\nAfter the processing of the k-layer MAB, we get three modal outputs\n\n, and finally we need to fuse these three outputs, where we use the average fusion. Through the above operation, we can extract to the intermodal Feature X\n\nIn order to capture the intra-modal feature X p Î± within the modality Î± as well as to maintain the consistency of the MAN structure, we take the text, audio and visual feature inputs as the inputs to the MAN respectively, i.e., three MANs are used to process each of the three modalities, which are denoted by\n\n), Î± = {t, a, v}.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Modality Interaction Network (Min)",
      "text": "In order to fully utilize the consistent and complementary information in the acquired inter-modal and intra-modal features, we propose modality interaction network, which consists of cross-modal Transformer and self-modal Transformer  [18] , with the intra-modal features guiding the reinforcement of the inter-modal feature. We apply the cross-modal attention proposed by  [17] . The main idea is to use modality Î² to reconstruct modality Î±, so as to achieve the fusion of the two modalities. Specifically, the modal Î² tensor X Î² passes through as the inputs of K and V in the attention, and the modal Î± as the input of Q. Then, the cross-modal attention Y Î± is defined as follows:\n\nBased on Cross-modal Attention, we constitute Crossmodal Transformer by stacking this structure. CMT consists of k layers of Cross-modal Transformer, which takes the outputs X i-1 Î²â†’Î± and X Î² of the previous layer as inputs, and the outputs are the enhanced features X i Î²â†’Î± :\n\nwhere Î² âˆˆ {t, a, v}, X i Î²â†’Î± âˆˆ R T Î´ Ã—d . In addition, in order to fuse the inter-modal and intra-modal information, we also performed the traditional self-attention based Selfmodal Transformer processing for each modality, and finally averaged the two to obtain a comprehensive representation of each modality. It is expressed as follows:\n\nwhere Y c Î± is the output obtained by CMT using the intermodal feature X c and the intra-modal feature of the modal Î±. Y p Î± is the output obtained by SMT for the modal Î±, where Î± âˆˆ {t, a, v}.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Classifier",
      "text": "The Emotion classifier uses connected multivariables as inputs to perform sentiment prediction. Finally we input Y tav to the softmax layer to obtain the sentiment category:\n\nwhere W 1 , W 2 , b 1 and b 2 are trainable parameters. We apply the standard cross-entropy loss function to train the model, for N b utterances in a batch, these are calculated as:\n\n3. EXPERIMENTS",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We conducted our experiments using the MELD dataset  [19] . MELD contains 13708 utterances out of 1433 dialogues of the TV series Old Friends. The dataset contains seven emotions, namely Anger, Disgust, Fear, Happy, Neutral, Sadness and Surprise. For a fair comparison, we conducted experiments using the MELD dataset pre-set with training, validation and test splits.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison Methods",
      "text": "TFN  [8]  utilizes an early fusion approach for modal fusion. LMF  [9]  low ranks the tensor for fusion. DialogueGCN  [7]  utilizes the dependency of graph structural context to model conversational data. DialogueCRN  [5]  designs multi-turn reasoning modules to understand conversational context. MMGCN  [12]  utilizes the structural features of graphs to capture intra-and inter-modal features. MM-DFN  [13]  designs graph-based dynamic fusion modules for multimodal context fusion in conversations, and reduces redundant information by capturing context dynamics. Note that  [7]  and  [5]  are designed for single-modality, thus introducing early fusion as a multimodal fusion method. Implementation Details. For text, we used RoBERTa  [20]  to pre-train the model. For audio, we extracted Melspectrogram audio features via librosa. For visual, we used effecientNet  [21]  pre-trained on the VGGface and AFEW datasets  [22, 23]  to extract visual features. For the dataset MELD, the batch size is 96, the initial learning rate is 1e-4.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overall Results",
      "text": "We compare our model with various state-of-the-art methods and the overall results are shown in Table  1 . It can be seen that AIMDiT outperforms the previous methods in terms of accuracy and F1 score on the MELD dataset. Compared to Table  1 . We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the Disgust and Fear categories, where the sample is smaller. Bold marks indicate best performance. tract inter-modal and intra-modal features, and MIN has the ability to fuse the two features, in order to explore the importance of these two modules, we conducted ablation experiments on both modules and the results are shown in the Table  3 . It can be seen that after removing MAN and MIN, compared with the complete model, there is a significant decrease in the performance of both, especially after removing MAN there is a decrease of more than 3 %, which reflects the ability of MAN to extract features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Meld",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Impact Of Man Layers",
      "text": "In order to study the effect of different layers of MAN, we keep the number of MIN layers as 4 and change only the number of MAN layers. The experimental results are shown in Table  4 . We find that although the Acc-7 index is highest when the MAN has only 1 layer, the boost is small. Interestingly, the overall results are best when the MAN has 2 layers. However, increasing to 3 layers significantly reduces the effect. This indicates that redundant information between modes accumulates after a certain point in the number of layers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a novel multimodal fusion framework (AIMDiT) composed of Modality Augmentation Network (MAN) and Modality Interaction Network (MIN) for ERC tasks. Specifically, we designed a inter-and intra-modal features learning network MAN through dimension transformation and Inception convolution, and proposed MIN effectively integrates inter-and intra-modal features for interaction. Extensive experiments on the benchmark dataset MELD demonstrate the effectiveness and superiority of AIMDiT.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework illustration of the AIMDiT based emotion recognition in conversations, which consists of four key compo-",
      "page": 2
    },
    {
      "caption": "Figure 1: A primary utterance involves three modalities: Ut (text), Ua",
      "page": 2
    },
    {
      "caption": "Figure 1: , we connect the Modality Augmen-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "College of Intelligence and Computing, Tianjin University, Tianjin, China"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "works based on multimodal fusion of depth graphs have been"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "proposed [12], but due to the characteristics of depth graphs,"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "redundant\ninformation gradually accumulates\nin the vector"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "space of each layer and is prone to over-smoothing problem"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "that makes node differentiation insufficient [13]."
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "We have observed subtle distinctions in the distribution"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "of\ntext, audio, and visual\ninformation in emotional expres-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "sions, but they tend to be concentrated over a brief time span."
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "For the purpose of discussion, we standardize the embedding"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "dimensions of various modalities,\ntreating vector\nlength as"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "the temporal dimension.\nConsequently, modal vectors can"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "be conceptualized as 1D time series.\nDrawing inspiration"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "from [14â€“16], we refer to changes in adjacent\ntime intervals"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "within a single modality and the same time interval across"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "different modalities as intra-modal variation and inter-modal"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "variation, respectively. To represent\nthese characteristics, we"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "transform 1D vectors into 2D, where the length axis signi-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "fies intra-modal\ninformation and the dimensional axis signi-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "fies inter-modal information. As a result, we successfully in-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "corporate intra- and inter-modal features into 2D tensors."
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "Based on the above motivation, we propose the Modality"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "Augmentation and Interaction Network via Dimension Trans-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "formation (AIMDiT). We first design the Modality Augmen-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "tation Network (MAN) based on dimension change and con-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "volution by Inception. Specifically, the MAN consists of mul-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "tiple layers of MABlock with residual\nlinks, which captures"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "intra- and inter-modal\ninformation in 2D space.\nIn order\nto"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "perform an effective fusion of modalities,\ninspired by [17],"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "we propose the Interaction Network (MIN) where inter-modal"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "feature and intra-modal feature guide the fusion of each other."
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "Finally, emotion classifier predicts the emotion label."
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "The work of this paper can be summarized as follows: 1)"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "We propose a multimodal fusion model named AIMDiT to fa-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "cilitate understanding in ERC. 2) We design a network called"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "MAN with dimension transformation and Inception convolu-"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "tion, which can effectively learn intra-modal and inter-modal"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "features.\n3) We introduce a fusion network called MIN that"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "utilizes different types of modality features to fuse modalities."
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": ""
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "4) Extensive experiments on benchmark datasets demonstrate"
        },
        {
          "2 Tianjin Key Laboratory of Cognitive Computing and Application,": "the effectiveness and superiority of the proposed model."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "Fig. 1. Framework illustration of the AIMDiT based emotion recognition in conversations, which consists of four key compo-"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "X t,a,v\n. After the transformation, we process the 2D tensor by"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "2D"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "inception block with multiple 2D kernels of different scales to"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "t,a,v"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "obtain a 2D tensor X\nwith global information:\n2D"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "t,a,v"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "= Inception(X t,a,v\n).\nX\n(2)\n2D"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "2D"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "Since the 2D kernel of the inception block is multiscale, it"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "can capture information at adjacent\ntime points within each"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "modality and at\nthe same time point\nfor different modali-"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "ties, and is able to aggregate intra- and inter-modal variations"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "at different scales.\nSubsequently, we reconvert\nthe learned"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "t,a,v"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "X\ntensor\ninto 1D space, and truncate the three tensors"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "2D"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "with length TÎ´ back to their original lengths:"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "t,a,v"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "t 1\na 1\nv 1\nX\n(3)\nD, X\nD, X\nD = Debulk(Reshape(X\n2D )),"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "t 1\na 1\nv 1"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "where X\nD âˆˆ RTÎ´Ã—d, X\nD âˆˆ RTÎ´Ã—d, X\nD âˆˆ RTÎ´Ã—d."
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "As shown in Fig.\n1, we connect\nthe Modality Augmen-"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "tation Block (MABlock) with residuals\nto form a MAN in"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "order\nto prevent\nthe problem of gradient vanishing in deep"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "networks and to improve the expressive power of\nthe net-"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "work.\n1D, X a\n1D,\n1D with"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "respectively, which\nare\nrepresented by\nlengths Tt, Ta, Tv,"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "tensor X1D = {X t"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "a one-dimensional\n1D, X v\n1D} with\n1D, X a"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "the inputs are X kâˆ’1\nlength TÎ´, and for the kth-layer MAN,"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "1D ,"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "and its computation procedure is represented by Equation 4:"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "Xk"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "(4)\n1D ) + Xkâˆ’1"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "According to Eqs. 1, 2, and 3 the whole process of the kth"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "MAB can be summarized as follows:"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "Xk\n2D = Reshape(P adding(Xkâˆ’1"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "1D )),"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": "k 2\n(5)\nX\nD = Inception(Xk\n2D),"
        },
        {
          "ğ‘‹(cid:3030)\nğ‘‹(cid:3049)\nğ‘‹(cid:3028)\nğ‘‹(cid:3047)": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: It can be seen",
      "data": [
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "t 1\na 1\nv 1\nmodal\noutputs X\nand finally we\nneed\nto\nD, X\nD, X\nD,",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "The Emotion classifier uses connected multivariables as in-"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "fuse these three outputs, where we use the average fusion.",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "puts to perform sentiment prediction. Finally we input Ytav"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "Through the above operation, we can extract\nto the inter-",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "to the softmax layer to obtain the sentiment category:"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "t 1\na 1\nv 1\nmodal Feature Xc = mean(X\nD, X\nD, X\nD).",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "In order to capture the intra-modal feature X p\nÎ± within the",
          "2.3. Emotion Classifier": "Ptav = GELU (W1(Yt âŠ• Ya âŠ• Yv)) + b1,"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "(9)"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "modality Î± as well as to maintain the consistency of the MAN",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "y = sof tmax(W2Ptav + b2),"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "structure, we take the text, audio and visual feature inputs as",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "the inputs to the MAN respectively, i.e., three MANs are used",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "where W1, W2, b1 and b2 are trainable parameters. We apply"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "to process each of the three modalities, which are denoted by",
          "2.3. Emotion Classifier": "the standard cross-entropy loss function to train the model,"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "Î± 1\nÎ± 1\nÎ± 1\nX p\nD, X\nD, X\nD), Î± = {t, a, v}.\nÎ± = M AN (X",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "for Nb utterances in a batch, these are calculated as:"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "2.2. Modality Interaction Network (MIN)",
          "2.3. Emotion Classifier": "Nb(cid:88)"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "1 N\nL = âˆ’\n(10)\nyi Â· log Ë†yi."
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "b"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "i=0"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "In order to fully utilize the consistent and complementary in-",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "formation in the acquired inter-modal and intra-modal\nfea-",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "tures, we propose modality interaction network, which con-",
          "2.3. Emotion Classifier": "3. EXPERIMENTS"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "sists of cross-modal Transformer and self-modal Transformer",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "3.1. Datasets"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "[18], with the intra-modal features guiding the reinforcement",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "of the inter-modal feature.",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "We conducted our experiments using the MELD dataset [19]."
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "We apply the cross-modal attention proposed by [17]. The",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "MELD contains 13708 utterances out of 1433 dialogues of the"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "main idea is to use modality Î² to reconstruct modality Î±, so",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "TV series Old Friends. The dataset contains seven emotions,"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "as to achieve the fusion of\nthe two modalities.\nSpecifically,",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "namely Anger, Disgust, Fear, Happy, Neutral, Sadness and"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "the modal Î² tensor XÎ² passes through as the inputs of K and",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "Surprise.\nFor a fair comparison, we conducted experiments"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "V in the attention, and the modal Î± as the input of Q. Then,",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "using the MELD dataset pre-set with training, validation and"
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "the cross-modal attention YÎ± is defined as follows:",
          "2.3. Emotion Classifier": ""
        },
        {
          "After\nthe processing of\nthe k-layer MAB, we get\nthree": "",
          "2.3. Emotion Classifier": "test splits."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: It can be seen that after removing MAN and MIN,",
      "data": [
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": ""
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": ""
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "Methods"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": ""
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "TFN [8]"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "LMF [9]"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "DialogueCRN [5]"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "DialogueGCN [7]"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "MMGCN [12]"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "MM-DFN [13]"
        },
        {
          "Table 1. We present the overall performance of Acc-7 and w-F1. We also present w-F1 scores for each category except for the": "AIMDiT"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: It can be seen that after removing MAN and MIN,",
      "data": [
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "ability of MAN to extract features."
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "Impact of MAN layers In order\nto study the effect of dif-"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "ferent layers of MAN, we keep the number of MIN layers as"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "4 and change only the number of MAN layers. The experi-"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "mental\nresults are shown in Table 4. We find that although"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "the Acc-7 index is highest when the MAN has only 1 layer,"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "the boost\nis small.\nInterestingly,\nthe overall\nresults are best"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "when the MAN has 2 layers. However, increasing to 3 layers"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": "significantly reduces the effect. This indicates that redundant"
        },
        {
          "MAN there is a decrease of more than 3 %, which reflects the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: It can be seen that after removing MAN and MIN,",
      "data": [
        {
          "Table 2.\nThe results under different combinations of modes": "are compared in terms of Acc-7 and w-F1 metrics.",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "in different MAN layers."
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Modality\nAcc-7\nw-F1",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "MAN\nMIN\nAcc-7\nw-F1"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Text\n62.83\n60.15",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "64.86\n1\n4\n61.64"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Audio\n48.46\n32.39",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "62.33\n2\n4\n64.83"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "3\n4\n63.14\n59.79"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Visual\n48.12\n31.26",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Text + Audio\n63.12\n60.38",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Text + Visual\n62.37\n60.71",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Audio + Visual\n49.54\n35.46",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "tract\ninter-modal and intra-modal features, and MIN has the"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "64.83\n62.33\nText + Audio + Visual",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "ability to fuse the two features,\nin order\nto explore the im-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "portance of\nthese two modules, we conducted ablation ex-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "periments on both modules and the results are shown in the"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Table 3. Ablation Study of the MAN and MIN module.",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "Table 3.\nIt can be seen that after removing MAN and MIN,"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "AIMDiT\nAcc-7\nw-F1",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "compared with the complete model,\nthere is a significant de-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "crease in the performance of both, especially after removing"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "-w/o MAN\n61.72\n59.54",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "MAN there is a decrease of more than 3 %, which reflects the"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "-w/o MIN\n62.91\n59.62",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "ability of MAN to extract features."
        },
        {
          "Table 2.\nThe results under different combinations of modes": "AIMDiT (Full)\n64.83\n62.33",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "Impact of MAN layers In order\nto study the effect of dif-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "ferent layers of MAN, we keep the number of MIN layers as"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "4 and change only the number of MAN layers. The experi-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "the previous SOTA, AIMDiT improves Acc-7 and w-F1 of",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "mental\nresults are shown in Table 4. We find that although"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "MELD by 2.35 % and 2.87 % respectively. As can be seen",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "the Acc-7 index is highest when the MAN has only 1 layer,"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "from the results, our model outperforms a number of sophis-",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "the boost\nis small.\nInterestingly,\nthe overall\nresults are best"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "ticated fusion mechanisms such as TFN and LFN, as well as",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "when the MAN has 2 layers. However, increasing to 3 layers"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "a range of graph-based fusion models, which illustrates the",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "significantly reduces the effect. This indicates that redundant"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "importance of multimodal fusion.",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "information between modes accumulates after a certain point"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "in the number of layers."
        },
        {
          "Table 2.\nThe results under different combinations of modes": "3.4. Ablation Study",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Comparison under Different Modality Settings Table 2",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "4. CONCLUSION"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "shows a comparison of performance under different modal",
          "Table 4.\nFixed MIN and different performance of AIMDiT": ""
        },
        {
          "Table 2.\nThe results under different combinations of modes": "combinations.\nIt can be seen that\nthe bimodal and trimodal",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "In this paper, we proposed a novel multimodal fusion frame-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "models are generally better than the unimodal model.\nIn the",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "work (AIMDiT) composed of Modality Augmentation Net-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "bimodal model,\nthe\ncombination of\ntext\nand audio (T+A)",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "work (MAN) and Modality Interaction Network (MIN)\nfor"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "shows\nthe best performance,\nindicating that\ntext and audio",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "ERC tasks. Specifically, we designed a inter- and intra- modal"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "features\nare more\ncomplementary.\nIn contrast,\naudio and",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "features learning network MAN through dimension transfor-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "visual (A+V) have the worst effect,\nreflecting a high degree",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "mation and Inception convolution, and proposed MIN effec-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "of distribution difference and redundancy in audio and visual",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "tively integrates inter- and intra-modal\nfeatures for\ninterac-"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "features, which is also worth improving in the future.",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "tion. Extensive experiments on the benchmark dataset MELD"
        },
        {
          "Table 2.\nThe results under different combinations of modes": "Impact of modules. The MAN module has the ability to ex-",
          "Table 4.\nFixed MIN and different performance of AIMDiT": "demonstrate the effectiveness and superiority of AIMDiT."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "â€œMMGCN: multimodal fusion via deep graph convolu-"
        },
        {
          "5. REFERENCES": "[1] Scott Brave\nand Cliff Nass,\nâ€œEmotion\nin\nhuman-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "tion network for emotion recognition in conversation,â€"
        },
        {
          "5. REFERENCES": "computer\ninteraction,â€\nin The human-computer inter-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "in ACL/IJCNLP, 2021, pp. 5666â€“5675."
        },
        {
          "5. REFERENCES": "action handbook, 2007, pp. 103â€”118.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[13] Dou Hu, Xiaolong Hou, Lingwei Wei, et al.,\nâ€˜â€˜MM-"
        },
        {
          "5. REFERENCES": "[2] Ankush Chatterjee, Kedhar Nath Narahari, Megh na",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "DFN: Multimodal Dynamic Fusion Network for Emo-"
        },
        {
          "5. REFERENCES": "Joshi, et al.,\nâ€œSemEval-2019 task 3: EmoContext con-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "tion Recognition in Conversations,â€\nin ICASSP, 2022,"
        },
        {
          "5. REFERENCES": "textual emotion detection in text,â€ in Proceedings of the",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "pp. 7037â€“7041."
        },
        {
          "5. REFERENCES": "13th International Workshop on Semantic Evaluation,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[14] Devamanyu Hazarika, Roger Zimmermann, Soujanya"
        },
        {
          "5. REFERENCES": "2019, pp. 39â€“48.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "Poria, â€œMISA: Modality-Invariant and -Specific Repre-"
        },
        {
          "5. REFERENCES": "[3] Dingkang Yang, Haopeng Kuang, Shuai Huang, et al.,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "sentations for Multimodal Sentiment Analysis,â€ in Pro-"
        },
        {
          "5. REFERENCES": "â€œLearning Modality-Specific and -Agnostic Represen-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "ceedings of\nthe 28th ACM International Conference on"
        },
        {
          "5. REFERENCES": "tations\nfor Asynchronous Multimodal Language Se-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "Multimedia, 2020, pp. 1122â€“1131."
        },
        {
          "5. REFERENCES": "quences,â€ in Proceedings of the 30th ACM International",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[15] Dingkang Yang, Shuai Huang, Haopeng Kuang, et al.,"
        },
        {
          "5. REFERENCES": "Conference on Multimedia, 2022, pp. 1708â€”1717.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "â€œDisentangled Representation Learning for Multimodal"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "of\nthe\n30th\nEmotion Recognition,â€\nin Proceedings"
        },
        {
          "5. REFERENCES": "[4] Fengmao Lv, Xiang Chen, Yanyong Huang,\net\nal.,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "ACM International Conference on Multimedia, 2022,"
        },
        {
          "5. REFERENCES": "â€œProgressive Modality Reinforcement for Human Mul-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "pp. 1642â€“1651."
        },
        {
          "5. REFERENCES": "timodal Emotion Recognition from Unaligned Multi-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "modal Sequences,â€ in CVPR, 2021, pp. 2554-2562.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[16] Haixu Wu, Tengge Hu, Yong Liu, et al.,\nâ€œTimesNet:"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "Temporal 2D-Variation Modeling for General Time Se-"
        },
        {
          "5. REFERENCES": "[5] Dou Hu, Lingwei Wei, and Xiaoyong Huai, â€œDialogue-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "ries Analysis,â€ in ICLR, 2023."
        },
        {
          "5. REFERENCES": "crn: Contextual reasoning networks for emotion recog-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "nition in conversations,â€\nin ACL/IJCNLP, 2021, pp.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[17] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,"
        },
        {
          "5. REFERENCES": "7042â€“7052.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "et al.,\nâ€œMultimodal Transformer\nfor Unaligned Multi-"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "modal Language Sequences,â€\nin ACL, 2019, pp. 6558â€“"
        },
        {
          "5. REFERENCES": "[6] Navonil Majumder, Soujanya Poria, Devamanyu Haz-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "6569."
        },
        {
          "5. REFERENCES": "arika, et al., â€œDialoguernn: An attentive RNN for emo-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "tion detection in conversations,â€\nin AAAI, 2019, pp.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[18] Ashish Vaswani, Noam Shazeer, Niki Parmar,\net\nal.,"
        },
        {
          "5. REFERENCES": "6818â€“6825.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "â€œAttention Is All You Need,â€ in Proceedings of the 31st"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "International Conference on Neural\nInformation Pro-"
        },
        {
          "5. REFERENCES": "[7] Deepanway Ghosal, Navonil Majumder, Soujanya Po-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "cessing Systems, 2017, pp. 6000â€”6010."
        },
        {
          "5. REFERENCES": "ria, et al.,\nâ€œDialoguegcn: A graph convolutional neural",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "network for emotion recognition in conversation,â€\nin",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[19] Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "5. REFERENCES": "EMNLP/IJCNLP, 2019, pp. 154â€“164.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "jumder,\net\nal.,\nâ€œMELD: A multimodal multi-party"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "dataset\nfor emotion recognition in conversations,â€\nin"
        },
        {
          "5. REFERENCES": "[8] Amir Zadeh, Minghai Chen,\nSoujanya Poria,\net\nal.,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "ACL, 2019, pp. 527â€“536."
        },
        {
          "5. REFERENCES": "â€œTensor fusion network for multimodal sentiment anal-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "ysis,â€ in EMNLP, 2017, pp. 1103â€“1114.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[20] Yinhan Liu, Myle Ott, Naman Goyal, et al.,\nâ€œRoberta:"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "A robustly optimized BERT pretraining approach,â€\nin"
        },
        {
          "5. REFERENCES": "[9] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "CoRR, abs/1907.11692, 2019."
        },
        {
          "5. REFERENCES": "narasimhan, et al.,\nâ€œEfficient\nlow-rank multimodal fu-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "sion with modality-specific factors,â€\nin ACL (1). 2018,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[21] Mingxing Tan and Quoc V. Le,\nâ€œEfficientnet: Rethink-"
        },
        {
          "5. REFERENCES": "pp. 2247â€“2256, ACL.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "ing model scaling for convolutional neural networks,â€"
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "in Proceedings of the 36th International Conference on"
        },
        {
          "5. REFERENCES": "[10] Yahui\nFu,\nShogo Okada,\nLongbiao Wang,\net\nal.,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "Machine Learning, 2019, pp. 6105â€”6114."
        },
        {
          "5. REFERENCES": "â€œCONSK-GCN:\nconversational\nsemantic-\nand",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        },
        {
          "5. REFERENCES": "knowledge-oriented\ngraph\nconvolutional\nnetwork",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[22]\nJ. Kossaifi, G. Tzimiropoulos,\nS. Todorovic,\net\nal.,"
        },
        {
          "5. REFERENCES": "for multimodal emotion recognition,â€\nin ICME. 2021,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "â€œAFEW-VA database for valence and arousal estimation"
        },
        {
          "5. REFERENCES": "pp. 1â€“6, IEEE.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "in-the-wild,â€ in Image and Vision Computing, 2017."
        },
        {
          "5. REFERENCES": "[11] Zaijing Li, Fengxiao Tang, Ming Zhao, et al.,\nâ€œEmo-",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "[23] A. Dhall, R. Goecke, S. Lucey,\net\nal.,\nâ€œCollecting"
        },
        {
          "5. REFERENCES": "Caps: Emotion Capsule based Model for Conversational",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "Large, Richly Annotated Facial-Expression Databases"
        },
        {
          "5. REFERENCES": "Emotion Recognition,â€\nin ACL. 2022, pp. 1610â€“1618,",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": "from Movies,â€ in IEEE MultiMedia, 2012."
        },
        {
          "5. REFERENCES": "ACL.",
          "[12]\nJingwen Hu,\nYuchen\nLiu,\nJinming\nZhao,\net\nal.,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion in humancomputer interaction",
      "authors": [
        "Scott Brave",
        "Cliff Nass"
      ],
      "year": "2007",
      "venue": "The human-computer interaction handbook"
    },
    {
      "citation_id": "3",
      "title": "SemEval-2019 task 3: EmoContext contextual emotion detection in text",
      "authors": [
        "Ankush Chatterjee",
        "Kedhar Nath Narahari",
        "Megh Na Joshi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences",
      "authors": [
        "Dingkang Yang",
        "Haopeng Kuang",
        "Shuai Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Progressive Modality Reinforcement for Human Multimodal Emotion Recognition from Unaligned Multimodal Sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "6",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "7",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "8",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "EMNLP/IJCNLP"
    },
    {
      "citation_id": "9",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "10",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "11",
      "title": "CONSK-GCN: conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Yahui Fu",
        "Shogo Okada",
        "Longbiao Wang"
      ],
      "year": "2021",
      "venue": "ICME"
    },
    {
      "citation_id": "12",
      "title": "Emo-Caps: Emotion Capsule based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao"
      ],
      "year": "2022",
      "venue": "ACL"
    },
    {
      "citation_id": "13",
      "title": "MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "14",
      "title": "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Disentangled Representation Learning for Multimodal Emotion Recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
      "authors": [
        "Haixu Wu",
        "Tengge Hu",
        "Yong Liu"
      ],
      "year": "2023",
      "venue": "ICLR"
    },
    {
      "citation_id": "18",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "19",
      "title": "Attention Is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "21",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "22",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "AFEW-VA database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "24",
      "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey"
      ],
      "year": "2012",
      "venue": "IEEE MultiMedia"
    }
  ]
}