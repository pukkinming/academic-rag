{
  "paper_id": "2412.01171v1",
  "title": "Cross-Task Inconsistency Based Active Learning (Ctial) For Emotion Recognition",
  "published": "2024-12-02T06:23:46Z",
  "authors": [
    "Yifan Xu",
    "Xue Jiang",
    "Dongrui Wu"
  ],
  "keywords": [
    "Active learning",
    "transfer learning",
    "emotion classification",
    "emotion estimation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a critical component of affective computing. Training accurate machine learning models for emotion recognition typically requires a large amount of labeled data. Due to the subtleness and complexity of emotions, multiple evaluators are usually needed for each affective sample to obtain its ground-truth label, which is expensive. To save the labeling cost, this paper proposes an inconsistency-based active learning approach for cross-task transfer between emotion classification and estimation. Affective norms are utilized as prior knowledge to connect the label spaces of categorical and dimensional emotions. Then, the prediction inconsistency on the two tasks for the unlabeled samples is used to guide sample selection in active learning for the target task. Experiments on within-corpus and cross-corpus transfers demonstrated that cross-task inconsistency could be a very valuable metric in active learning. To our knowledge, this is the first work that utilizes prior knowledge on affective norms and data in a different task to facilitate active learning for a new task, even the two tasks are from different datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E Motion recognition is an important part of affective comput- ing, which focuses on identifying and understanding human emotions from facial expressions  [1] , body gestures  [2] , speech  [3] , physiological signals  [4] , etc. It has potential applications in healthcare and human-machine interactions, e.g., emotion health surveillance  [5]  and emotion-based music recommendation  [6] .\n\nEmotions can be represented categorically (discretely) or dimensionally (continuously). A typical example of the former is Ekman's six basic emotions  [7] . Typical dimensional emotion representations include the pleasure-arousal-dominance model  [8]  (pleasure is often replaced by its opposite, valence), and the circumplex model  [9]  that only considers valence and arousal. Both categorical emotion classification (CEC) and dimensional emotion estimation (DEE) are considered in this paper.\n\nAccurate emotion recognition usually requires a large amount of labeled training data. However, labeling affective data is expensive because emotion is subtle and has large individual differences. Each affective sample needs to be evaluated by multiple annotators to obtain its 'ground-truth' label. Active learning (AL)  [10]  and transfer learning (TL)  [11]  are promising solutions to alleviate the labeling effort.\n\nAL selects the most useful samples to query for their labels, improving the model performance for a limited annotation budget. The key is to define an appropriate measure of sample usefulness. For CEC from facial images, Muhammad and Alhamid  [12]  adopted entropy as the uncertainty measure and selected samples with high entropy to annotate. Zhang et al.  [13]  used the distance to decision boundary as the uncertainty measure in AL for speech emotion classification. They selected samples with medium certainty to avoid noisy ones and allocated different numbers of • Y. Xu, X.  Jiang   annotators for each sample adaptively. For AL in DEE, Wu et al.  [14]  considered greedy sampling in both the feature space and the label space. Their approach was later extended to multi-task DEE  [3] . Abdelwahab and Busso  [15]  also verified the effectiveness of greedy sampling based AL approaches in valence and arousal estimation using deep neural networks. TL utilizes knowledge from relevant (source) tasks to facilitate the learning for a new (target) task  [16] ,  [17] . In taskhomogeneous TL, i.e., the source and target domains have the same label space, the key is to reduce their distribution discrepancies  [18] . For example, Zhou and Chen  [19]  employed classwise adversarial domain adaptation to transfer from spontaneous emotional speeches of children to acted corpus of adults. A more challenging scenario is task-heterogenous TL, where the source and target domains have different label spaces. Pre-training on large datasets of relevant (but may not be exactly the same) tasks and then fine-tuning on task-specific data was used in facial emotion recognition  [20] -  [22] . Zhao et al.  [23]  adopted age and gender prediction as source tasks, and supplemented the extracted features to target tasks of speech emotion classification and estimation. Park et al.  [24]  trained models on text corpora with categorical emotion labels to estimate their dimensional emotions. They ordered the discrete emotions along valence, arousal, and dominance dimensions according to domain knowledge  [25]  and reduced the earth mover's distance to optimize the model. This paper proposes cross-task inconsistency based active transfer learning (CTIAL) between CEC and DEE, which has not been explored before. We aim to reduce the labeling efforts in the task-heterogeneous TL scenario where a homogenous source dataset suitable for transfer is hard to obtain, but a heterogeneous one is available. CTIAL enhances the efficiency of sample selection in AL for the target task, by exploiting the source task knowledge. Fig.  1  illustrates the difference between cross-task AL and the traditional within-task AL.\n\nTo transfer knowledge between CEC and DEE tasks, we first train models for CEC and DEE separately using the corresponding labeled data and obtain their predictions on the unlabeled samples. Affective norms, normative emotional ratings for words  [26] , are utilized as domain knowledge to map the estimated categorical emotion probabilities into the dimensional emotion space. A cross-task inconsistency (CTI) is computed in this common label space and used as an informativeness measure in AL. By further integrating the CTI with other metrics, e.g., uncertainty in CEC or diversity in DEE, we can identify the most useful unlabeled samples to annotate for the target task and merge them into the labeled dataset to update the model.\n\nOur contributions are:\n\n1) We propose CTI to measure the prediction inconsistency between CEC and DEE tasks for cross-task AL. 2) We integrate CTI with other metrics in within-task AL to further improve the sample query efficiency. 3) Within-and cross-corpus experiments demonstrated the effectiveness of CTIAL in cross-task transfers.\n\nTo our knowledge, this is the first work that utilizes prior knowledge on affective norms and data in a different task to facilitate AL for a new task, even though the two tasks are from different datasets.\n\nThe remainder of the paper is organized as follows: Section 2 introduces the proposed CTIAL approach. Section 3 describes the datasets and the experimental setup. Sections 4 and 5 present experiment results on cross-task transfers from DEE to CEC, and from CEC to DEE, respectively. Section 6 draws conclusions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "This section introduces the CTI measure and its application in cross-task AL.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Setting",
      "text": "Consider the transfer between CEC and DEE, i.e., one is the source task, and the other is the target task. The source task has a large amount of labeled samples, whereas the target task has only a few. AL selects the most useful target samples from the unlabeled data pool and queries for their labels.\n\nDenote the dataset with categorical and dimensional emotion annotations as",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Expert Knowledge",
      "text": "Affective norms, i.e., dimensional emotion ratings for words  [25] ,  [26] , can be utilized as domain knowledge to establish the connection between the label spaces of categorical and dimensional emotions  [24] ,  [27] . This paper uses the NRC Valence-Arousal-Dominance Lexicon  [25] , where 20,000 English words were manually annotated with valence, arousal and dominance scores via crowd-sourcing. Specifically, the annotators were presented with a four-word tuple each time, and asked to select the word with the highest and lowest valence/arousal/dominance, respectively. The best-worst scaling technique was then used to aggregate the annotations: the score for a word is the proportion of times it was chosen as the highest valence/arousal/dominance minus that as the lowest valence/arousal/dominance. The scores for each emotion dimension were then linearly mapped into the interval [0, 1].\n\nWith the help of affective norms, we can exploit datasets of different emotion representations for TL, by converting categorical emotion labels into dimensional ones. The affective norms of different languages demonstrate relatively high correlations  [28] , suggesting the feasibility for cross-linguistic transfer. Table  1  presents the dimensional emotion scores of some emotion categories in the NRC Lexicon.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Task Inconsistency (Cti)",
      "text": "Fig.  2  shows the flowchart for computing the CTI, an informativeness measure of the prediction inconsistency between two tasks. First, we construct the emotion classification and estimation models, f Cat and f Dim , using D Cat and D Dim respectively. For an unlabeled sample x i ∈ P , the prediction probabilities for the categorical emotions are:\n\nwhere each element ŷe i ∈ ŷCat i is the probability of Emotion e ∈ E.\n\nThe estimated dimensional emotion values are:\n\nwhere\n\nis the score of Dimension dim ∈ D. Utilizing prior knowledge of affective norms, we map the predicted categorical emotion probabilities to dimensional emotion values:\n\nwhere NRC[e] denotes the scores of Emotion e from the NRC Lexicon.\n\nThe CTI is then computed as:\n\nA high CTI indicates that the two models have a high disagreement, i.e., the corresponding unlabeled sample has high uncertainty (informativeness). Those samples with high CTIs may have inaccurate probability predictions on emotion categories or imprecise estimations of emotion primitives. Labeling these informative samples can expedite the model learning. Besides, the affective norms only represent the most typical (or average) emotion primitives of each category. In practice, a categorical emotion's dimensional primitives may have a large variance. Its corresponding samples are also likely to have high CTIs, and annotating them can increase the sample diversity.\n\nThus, the sample x q selected for labeling for the target task is:\n\nCTI can be integrated with other useful within-task AL indicators for better performance, as introduced next.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ctial For Cross-Task Transfer From Dee To Cec",
      "text": "Uncertainty is a frequently used informativeness metric for AL in classification  [10] . We consider two typical uncertainty measures in CEC: information entropy and prediction confidence.\n\nThe information entropy, also called Shannon entropy  [29] , is:\n\nA large entropy indicates high uncertainty  [30] .\n\nThe prediction confidence directly reflects how certain the classifier is about its prediction:\n\nA low confidence indicates high uncertainty  [31] .\n\nConsidering the uncertainty and CTI simultaneously, we select the sample x q by: q = arg max\n\nor by: q = arg max\n\nand query for its class.\n\nThe pseudo-code of CTIAL for cross-task transfer from DEE to CEC is given in Algorithm 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ctial For Cross-Task Transfer From Cec To Dee",
      "text": "Wu et al.  [3] ,  [14]  employed greedy sampling in both feature and label spaces for sample selection and verified the importance of diversity in AL for regression. Their multi-task improved greedy sampling (MTiGS)  [3]  computes the distance between an unlabeled sample x i and the labeled sample set D Dim as:\n\nTo weight the feature and label spaces equally, we slightly modify MTiGS to:\n\nConsidering CTI (informativeness) and MTiGS (diversity) together, we select the sample x q by: q = arg max\n\nand query for its dimensional emotion values. Compute entropy {H i } NP i=1 using (6); Select sample x q using (8); else // the uncertainty measure is confidence Compute confidence {Conf i } NP i=1 using (7); Select sample x q using (9); end Query for the emotion category y Cat q of x q ; D Cat ← D Cat ∪ (x q , y Cat q ); P ← P \\x q ; N P ← N P -1; Update f Cat on D Cat ; end",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Domain Adaptation In Cross-Corpus Transfer",
      "text": "CTIAL assumes the source model can make reliable predictions for the target dataset. However, speech emotion recognition corpora vary according to whether they are acted or spontaneous, collected in lab or in the wild, and so on. These discrepancies may cause a model trained on one corpus to perform poorly on another, violating the underlying assumption in CTIAL.\n\nTo enable cross-corpus transfer, we adopt two classical domain adaptation approaches, transfer component analysis (TCA)  [32]  and balanced distribution adaptation (BDA)  [33] .\n\nWhen the source task is CEC, BDA jointly aligns both the marginal distributions and the class-conditional distributions with a balance factor that adjusts the weights of the two corresponding terms in the objective function. Specifically, the source model assigns pseudo-labels for the unlabeled target dataset. The source and target datasets' average features and their corresponding class' average features are aligned. The adapted features of the source dataset are then used to update the source model. This process iterates till convergence.\n\nWhen the source task is DEE, TCA adapts the marginal distributions of the source and target datasets by reducing the distance between their average features. BDA is not used here, since the class-conditional probabilities are difficult to compute in regression problems. i } NP i=1 between samples in P and D Dim using  (11) ; Select sample x q using (12); Query for the dimensional emotion values y Dim q of x q ; D Dim ← D Dim ∪ (x q , y Dim q ); P ← P \\x q ; N P ← N P -1; Update f Dim on D Dim ; end After domain adaptation, the model trained on the source dataset is applied to P to obtain the predictions for the source task. More details on the implementations will be introduced in Section 3.1.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "This section describes the datasets and the experimental setup.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Feature Extraction",
      "text": "Three public speech emotion datasets, IEMOCAP (Interactive Emotional Dyadic Motion Capture Database)  [34]  of semiauthentic emotions, MELD (Multimodal EmotionLines Dataset)  [35]  of semi-authentic emotions, and VAM (Vera am Mittag; Vera at Noon in English)  [36]  of authentic emotions, were used to verify the proposed CTIAL. Specifically, we performed experiments of within-corpus transfer on IEMOCAP, and cross-corpus transfer from VAM and MELD to IEMOCAP.\n\nIEMOCAP is a multi-modal dataset annotated with both categorical and dimensional emotion labels. Audio data from five emotion categories (angry, happy 1 , sad, frustrated and neutral with 289, 947, 608, 971 and 1099 samples, respectively) in spontaneous sessions were used in the within-corpus experiments and crosscorpus transfer from DEE on VAM. In cross-corpus transfer from CEC on MELD, only four overlapping classes (angry, happy, sad and neutral) were used. The valence, arousal and dominance annotations are in  [1, 5] , so we linearly rescaled the scores in the NRC Lexicon from [0, 1] to  [1, 5] .\n\nMELD is a multi-modal dataset collected from the TV series Friends. Each utterance was labeled by five annotators from seven 1. The 'happy' class contains data of 'happy' and 'excited' in the original annotation as in multiple previous works.\n\nclasses (anger, disgust, sadness, joy, surprise, fear, and neutral), and majority vote was applied to generate the ground-truth labels. We only used the four overlapping categories with IEMOCAP in the training set (angry, happy, sad and neutral with 1109, 1743, 683 and 4709 samples, respectively).\n\nVAM consists of 947 utterances collected from 47 guests (11m/36f) in a German TV talk-show Vera am Mittag. Each sentence was annotated by 6 or 17 evaluators for valence, arousal and dominance values, and the weighted average was used to obtain the ground-truth labels. We linearly rescaled their values from [-1, 1] to  [1, 5]  to match the range of IEMOCAP.\n\nThe wav2vec 2.0 model  [37] , pre-trained on 960 hours of unlabeled audio from the LibriSpeech dataset  [38]  and fine-tuned for automatic speech recognition on the same audio with transcripts, was used for feature extraction. For each audio segment, we took the average output of 12 transformer encoder layers, and averaged these features again along the time axis, obtaining a 768dimensional feature for each utterance.\n\nIn cross-task AL, we combined the source and target datasets and used principal component analysis to reduce the feature dimensionality (maintaining 90% variance). In cross-corpus transfer, an additional feature adaptation step was performed using TCA or BDA to obtain more accurate source task predictions for samples in P .\n\nBefore applying AL to P , we applied principal component analysis to the original 768-dimensional features of the target dataset in both cross-task AL and within-task AL baselines.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We used logistic regression (LR) and ridge regression (RR) as the base model for CEC and DEE, respectively. The weight of the regularization term in each model, i.e., 1/C in LR and α in RR, was chosen from {1, 5, 10, 50, 1e2, 5e2, 1e3, 5e3} by three-fold cross-validation on the corresponding training data.\n\nIn cross-corpus transfer experiments, feature dimensionality in TCA and BDA was set to 30 and 40, respectively. BDA used 10 iterations to estimate the class labels of the target dataset, align the marginal and conditional distributions, and update the classifier. The balance factor was selected from {0.1, 0.2, ..., 0.9} to minimize the sum of the maximum mean discrepancy metrics  [39]  on all data and in each class.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Evaluation",
      "text": "We aimed to classify all samples in the target dataset, some by human annotators and the remaining by the trained classifier. Therefore, we evaluated the classification performance on the entire target dataset, including the manually labeled samples and P . Specifically, we concatenated the ground-truth labels {y Cat i } NCat i=1 of D Cat and the predictions { ŷCat i } NP i=1 of P to compute the performance metrics.\n\nIn within-corpus transfer, either from DEE to CEC or from CEC to DEE on IEMOCAP, each time we used two sessions as the source dataset and the rest three sessions as the target dataset, resulting in 10 source-target dataset partitions. Experiments on each dataset partition were repeated three times with different initial labeled samples.\n\nCross-corpus experiments considered transfer from the source DEE task on VAM to the target CEC task on IEMOCAP and from the source CEC task on MELD to the target DEE task on IEMOCAP. Experiments were repeated 10 times with different initial labeled sample sets.\n\nIn each run of the experiment, the initial 20 labeled samples were randomly selected. In the subsequent 200 iterations of sample selection, one sample was chosen for annotation at a time.\n\nBalanced classification accuracy (BCA), i.e., the average perclass accuracies, was used as the performance measure in CEC since IEMOCAP has significant class-imbalance. Root mean squared error (RMSE) and correlation coefficient (CC) were used as performance measures in DEE.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments On Cross-Task Transfer From Dee To Cec",
      "text": "This section presents the results in cross-task transfer from DEE to CEC. The DEE and CEC tasks could be from the same corpus, or different ones.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Algorithms",
      "text": "We compared the performance of the following sample selection strategies in cross-task transfer from DEE to CEC: 1) Random sampling (Rand), which randomly selects K samples to annotate. 2) Entropy (Ent)  [30] , an uncertainty-based within-task AL approach that selects samples with maximum entropy computed by  (6) . 3) Least confidence (LC)  [31] , an uncertainty-based withintask AL approach that selects samples with minimum prediction confidence computed by  (7) . 4) Multi-task iGS on the source task (Source MTiGS), which performs AL according to the informativeness metric in the source DEE task. Specifically, we compute the distance between samples x i ∈ P and x j ∈ D Cat by  (10)  and select samples with the maximum distance.\n\nWithout any dimensional emotion labels of D Cat , we replace the true label y dim j with ŷdim j estimated from the source model. This simple cross-task AL baseline directly uses the knowledge from the source task. 5) CTIAL, which selects samples with the maximum CTI by (5). 6) Ent-CTIAL, which integrates entropy with CTI as introduced in Algorithm 1. 7) LC-CTIAL, which integrates the prediction confidence with CTI as introduced in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Effectiveness Of Ctial",
      "text": "Fig.  3  shows the average BCAs in within-and cross-corpus cross-task transfers from DEE to CEC. To examine if the performance improvements of the integration of the uncertainty-based approaches and CTIAL were statistically significant, Wilcoxon signed-rank tests with Holm's p-value adjustment  [40]  were performed on the results in each AL iteration. The test results are shown in Fig.  Rand, suggesting that only considering the uncertainty may not be enough. 3) Source MTiGS that performed AL according to the source DEE task achieved higher BCAs than Rand and the two within-task AL approaches, because MTiGS increased the feature diversity, which was also useful for the target CEC task. Additionally, MTiGS increased the diversity of the dimensional emotion labels, which in turn increased the diversity of the categorical emotion labels. 4) CTIAL outperformed Rand, the two within-task AL approaches (Ent and LC), and the cross-task AL baseline Source MTiGS when K was small, demonstrating that the proposed CTI measure properly exploited the relationship between the CEC and DEE tasks and effectively utilized knowledge from the source task. However, in cross-corpus transfer, CTIAL and Rand had similar performance when K was large. The reason may be that domain shift limited the performance of the source DEE models on the target dataset, which further resulted in inaccurate CTI calculation and degraded AL performance. 5) Integrating CTI with uncertainty can further enhance the classification accuracies by simultaneously considering within-and cross-task informativeness: both LC-CTIAL and Ent-CTIAL statistically significantly outperformed the other baselines. 6) LC-CTIAL generally achieved the best performance among all seven approaches.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effectiveness Of Tca",
      "text": "CTIAL assumes that the model trained on the source dataset is reliable for the target dataset. We performed within-corpus and cross-corpus experiments to verify this assumption. Fig.  5  shows the average RMSEs and CCs in valence, arousal and dominance estimation on IEMOCAP in within-and crosscorpus transfers. In within-corpus transfer, the average RMSE and CC were 0.6667 and 0.5659, respectively. Although models trained on VAM were inferior to those on IEMOCAP, TCA still achieved lower RMSEs and higher CCs than direct transfer. Generally, the assumption that the source model is reliable was satisfied in both within-corpus transfer and more challenging cross-corpus transfer, with the help of TCA.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments On Cross-Task Transfer From Cec To Dee",
      "text": "This section presents the experiment results in cross-task transfer from CEC to DEE, where the two tasks may be from the same or different datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Algorithms",
      "text": "The following approaches were compared in transferring from CEC to DEE: 1) Direct mapping by NRC Lexicon (NRC Mapping),\n\nwhere the dimensional emotion estimates of all the samples are obtained by  (3) . This non-AL approach only utilizes the information of the source CEC task and domain knowledge. 2) Random sampling (Rand), which randomly selects K samples to annotate. 3) Multi-task iGS (MTiGS)  [3] , a diversity-based withintask AL approach that selects samples with the furthest distance to labeled data computed by  (10) . 4) Least confidence on the source task (Source LC), which selects samples with the minimum source model prediction confidence defined by  (7) . This cross-task AL baseline depends solely on the source CEC model. 5) Cross-task iGS (CTiGS), a variant of MTiGS that further considers the information in the source CEC task. Specifically, we first obtain the predicted emotion categories for the target data using the source model. The distance calculation is only conducted between unlabeled and labeled samples predicted with the same emotion category.\n\nFor an emotion category with only unlabeled samples, we calculate the distance between them and all the labeled samples using  (10) , as in MTiGS. The subsequent sample selection process is the same as MTiGS. 6) CTIAL, which selects samples with the maximum CTI by (5). 7) MTiGS-CTIAL, which integrates MTiGS and CTI as introduced in Algorithm 2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effectiveness Of Ctial",
      "text": "Fig.  6  shows the average performance in DEE on IEMOCAP using different sample selection approaches. Wilcoxon signed-rank tests with Holm's p-value adjustment  [40]  were again used to examine if the performance improvements of MTiGS-CTIAL over other approaches were statistically significant in each AL iteration. The test results are shown in Fig.  7 . Figs.  6  and 7  shows that:\n\n1) The regression models in all approaches performed better as the number of labeled samples increased, except the non-AL baseline NRC Mapping.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "Human emotions can be described by both categorical and dimensional representations. Usually, training accurate emotion classification and estimation models requires large labeled datasets. However, manual labeling of affective samples is expensive, due to the subtleness and complexity of emotions. This paper integrates AL and TL to reduce the labeling effort. We proposed CTI to measure the prediction inconsistency between the CEC and DEE tasks.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the difference between cross-task AL",
      "page": 1
    },
    {
      "caption": "Figure 1: Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.",
      "page": 2
    },
    {
      "caption": "Figure 2: Flowchart for computing the CTI.",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the ﬂowchart for computing the CTI, an informative-",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the average BCAs in within- and cross-corpus",
      "page": 5
    },
    {
      "caption": "Figure 4: Figs. 3 and 4 demonstrate that:",
      "page": 5
    },
    {
      "caption": "Figure 3: Average BCAs of different sample selection approaches in cross-",
      "page": 6
    },
    {
      "caption": "Figure 4: Statistical signiﬁcance of the performance improvements of",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the average RMSEs and CCs in valence, arousal",
      "page": 6
    },
    {
      "caption": "Figure 5: Average RMSEs and CCs in valence, arousal and dominance",
      "page": 6
    },
    {
      "caption": "Figure 6: shows the average performance in DEE on IEMOCAP using",
      "page": 7
    },
    {
      "caption": "Figure 7: Figs. 6 and 7 shows that:",
      "page": 7
    },
    {
      "caption": "Figure 8: shows the classiﬁcation results in CEC on IEMOCAP using",
      "page": 7
    },
    {
      "caption": "Figure 6: Average RMSEs and CCs of different sample selection approaches in cross-task transfer from CEC to DEE. (a) Within-corpus transfer from",
      "page": 8
    },
    {
      "caption": "Figure 7: Statistical signiﬁcance of the performance improvements of",
      "page": 9
    },
    {
      "caption": "Figure 8: BCAs in CEC on IEMOCAP in within-corpus transfer (CEC on",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "Abstract—Emotion recognition is a critical component of affective computing. Training accurate machine learning models for emotion"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "recognition typically requires a large amount of labeled data. Due to the subtleness and complexity of emotions, multiple evaluators are"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "usually needed for each affective sample to obtain its ground-truth label, which is expensive. To save the labeling cost, this paper"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "proposes an inconsistency-based active learning approach for cross-task transfer between emotion classiﬁcation and estimation."
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "Affective norms are utilized as prior knowledge to connect\nthe label spaces of categorical and dimensional emotions. Then, the"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "prediction inconsistency on the two tasks for the unlabeled samples is used to guide sample selection in active learning for the target"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "task. Experiments on within-corpus and cross-corpus transfers demonstrated that cross-task inconsistency could be a very valuable"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "metric in active learning. To our knowledge,\nthis is the ﬁrst work that utilizes prior knowledge on affective norms and data in a different"
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "task to facilitate active learning for a new task, even the two tasks are from different datasets."
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "Index Terms—Active learning, transfer learning, emotion classiﬁcation, emotion estimation."
        },
        {
          "Yifan Xu, Xue Jiang, and Dongrui Wu": "✦"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "dataset suitable for transfer is hard to obtain, but a heterogeneous"
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "one is\navailable. CTIAL enhances"
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "lection in AL for\nthe target"
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "knowledge. Fig. 1 illustrates the difference between cross-task AL"
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "and the traditional within-task AL."
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "To transfer knowledge between CEC and DEE tasks, we ﬁrst"
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "train models for CEC and DEE separately using the corresponding"
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": ""
        },
        {
          "the task-heterogeneous TL scenario where a homogenous source": "labeled data and obtain their predictions on the unlabeled samples."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: presents the dimensional emotion scores of some emotion cate-",
      "data": [
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "samples to annotate for\nthe target\ntask and merge them into the"
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": ""
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "labeled dataset to update the model."
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": ""
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "Our contributions are:"
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": ""
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "1)\nWe propose CTI to measure the prediction inconsistency"
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "between CEC and DEE tasks for cross-task AL."
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "2)\nWe integrate CTI with other metrics in within-task AL to"
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "further improve the sample query efﬁciency."
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "3)\nWithin- and cross-corpus experiments demonstrated the"
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": "effectiveness of CTIAL in cross-task transfers."
        },
        {
          "or diversity in DEE, we can identify the most useful unlabeled": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: presents the dimensional emotion scores of some emotion cate-",
      "data": [
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "Affective norms, normative emotional ratings for words [26], are"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "utilized as domain knowledge to map the estimated categorical"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "emotion\nprobabilities\ninto\nthe\ndimensional\nemotion\nspace. A"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "cross-task inconsistency (CTI) is computed in this common label"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "space and used as an informativeness measure in AL. By further"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "integrating the CTI with other metrics, e.g., uncertainty in CEC"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "or diversity in DEE, we can identify the most useful unlabeled"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "samples to annotate for\nthe target\ntask and merge them into the"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "labeled dataset to update the model."
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "Our contributions are:"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "1)\nWe propose CTI to measure the prediction inconsistency"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "between CEC and DEE tasks for cross-task AL."
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "2)\nWe integrate CTI with other metrics in within-task AL to"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "further improve the sample query efﬁciency."
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "3)\nWithin- and cross-corpus experiments demonstrated the"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "effectiveness of CTIAL in cross-task transfers."
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "To our knowledge,\nthis is the ﬁrst work that utilizes prior knowl-"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "edge on affective norms and data in a different\ntask to facilitate"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "AL for a new task, even though the two tasks are from different"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "datasets."
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "The remainder of the paper is organized as follows: Section 2"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "introduces\nthe proposed CTIAL approach. Section 3 describes"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "the datasets and the experimental setup. Sections 4 and 5 present"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "experiment results on cross-task transfers from DEE to CEC, and"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "from CEC to DEE, respectively. Section 6 draws conclusions."
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": ""
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "2\nMETHODOLOGY"
        },
        {
          "Fig. 1. Within-task AL for DEE, and CTIAL for cross-task transfer from CEC to DEE.": "This\nsection introduces\nthe CTI measure and its\napplication in"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: presents the dimensional emotion scores of some emotion cate-",
      "data": [
        {
          "2\nMETHODOLOGY": "This\nsection introduces\nthe CTI measure and its\napplication in",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "cross-task AL.",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": "Valence, Arousal and Dominance scores of eight typical emotions in"
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "2.1\nProblem Setting",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "Consider\nthe\ntransfer between CEC and DEE,\ni.e., one\nis\nthe",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "source task, and the other\nis the target\ntask. The source task has",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": "Angry"
        },
        {
          "2\nMETHODOLOGY": "a large amount of\nlabeled samples, whereas\nthe target\ntask has",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": "Happy"
        },
        {
          "2\nMETHODOLOGY": "only a few. AL selects\nthe most useful\ntarget\nsamples\nfrom the",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": "Sad"
        },
        {
          "2\nMETHODOLOGY": "unlabeled data pool and queries for their labels.",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "Denote\nthe\ndataset with\ncategorical\nand\ndimensional\nemo-",
          "gories in the NRC Lexicon.": "Disgusted"
        },
        {
          "2\nMETHODOLOGY": "tion\nannotations\nand DDim\n=\nas DCat\n=\n)}NCat\n{(xi, yCat",
          "gories in the NRC Lexicon.": "Fearful"
        },
        {
          "2\nMETHODOLOGY": ")}NDim\nin DCat\nand DDim are\n{(xi, yDim\nrespectively, where xi\ni=1 ,",
          "gories in the NRC Lexicon.": "Surprised"
        },
        {
          "2\nMETHODOLOGY": "from the same feature space, yCat\nis the one-hot encoding\n∈ R|E|\ni",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": "Frustrated"
        },
        {
          "2\nMETHODOLOGY": "label of\nis the dimen-\nthe emotion category set E, yDim\n∈ R|D|\ni",
          "gories in the NRC Lexicon.": ""
        },
        {
          "2\nMETHODOLOGY": "",
          "gories in the NRC Lexicon.": "Neutral"
        },
        {
          "2\nMETHODOLOGY": "sional emotion label of\nthe\nthe dimension set D. An example of",
          "gories in the NRC Lexicon.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. Flowchart for computing the CTI.": "2.3\nCross-Task Inconsistency (CTI)"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "Fig. 2 shows the ﬂowchart for computing the CTI, an informative-"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "ness measure of the prediction inconsistency between two tasks."
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "First, we construct\nthe emotion classiﬁcation and estimation"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "models, f Cat and f Dim, using DCat and DDim respectively. For an"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "the prediction probabilities\nfor\nthe\nunlabeled sample xi ∈ P ,"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "categorical emotions are:"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "yCat\n(1)\n= f Cat(xi),\ni"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "where each element ˆye"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "is the probability of Emotion e ∈\ni ∈ ˆyCat"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "E."
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "The estimated dimensional emotion values are:"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "yDim\n(2)\n= f Dim(xi),"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "i"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "is the score of Dimension dim ∈ D.\n∈ ˆyDim\nwhere ˆydim\ni\ni"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "Utilizing prior knowledge of affective norms, we map the pre-"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "dicted categorical emotion probabilities\nto dimensional emotion"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "values:"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "yDim\n(3)\nye\n· NRC[e],"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "i\n= X"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "e∈E"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "where NRC[e] denotes\nthe scores of Emotion e from the NRC"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "Lexicon."
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "The CTI is then computed as:"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "yDim\n(4)\n− ˜yDim\n2 ."
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "i\ni\nIi = (cid:13)"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "(cid:13)(cid:13)"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "(cid:13)"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "A high CTI\nindicates\nthat\nthe\ntwo models\nhave\na\nhigh"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "disagreement,\ni.e.,\nthe corresponding unlabeled sample has high"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "uncertainty (informativeness). Those samples with high CTIs may"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "have\ninaccurate\nprobability\npredictions\non\nemotion\ncategories"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "or\nimprecise\nestimations of\nemotion primitives. Labeling these"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "informative\nsamples\ncan expedite\nthe model\nlearning. Besides,"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "the affective norms only represent\nthe most\ntypical\n(or average)"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "emotion primitives of\neach category.\nIn practice,\na\ncategorical"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "emotion’s dimensional primitives may have a large variance.\nIts"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "corresponding samples\nare\nalso likely to have high CTIs,\nand"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "annotating them can increase the sample diversity."
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "Thus, the sample xq selected for labeling for the target task is:"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "(5)\nq = arg max\nIi."
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "xi∈P"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": ""
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "CTI can be integrated with other useful within-task AL indi-"
        },
        {
          "Fig. 2. Flowchart for computing the CTI.": "cators for better performance, as introduced next."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "",
          "Algorithm 2: CTIAL for cross-task transfer": "to DEE.",
          "from CEC": ""
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "Source dataset with dimensional emotion values",
          "Algorithm 2: CTIAL for cross-task transfer": "Input:",
          "from CEC": "Source dataset with categorical emotion labels"
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": ")}NDim\nDDim = {(xi, yDim\ni=1 ;",
          "Algorithm 2: CTIAL for cross-task transfer": ")}NCat\nDCat = {(xi, yCat\ni=1 ;",
          "from CEC": ""
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "Target dataset,\nincluding data with categorical",
          "Algorithm 2: CTIAL for cross-task transfer": "Target dataset,",
          "from CEC": "including data with dimensional"
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": ")}NCat\nemotion labels DCat = {(xi, yCat",
          "Algorithm 2: CTIAL for cross-task transfer": ")}NDim\nemotion values DDim = {(xi, yDim",
          "from CEC": "and"
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "i=1 and",
          "Algorithm 2: CTIAL for cross-task transfer": "i=1",
          "from CEC": ""
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "unlabeled data pool P = {xi}NP\ni=1;",
          "Algorithm 2: CTIAL for cross-task transfer": "unlabeled data pool P = {xi}NP\ni=1;",
          "from CEC": ""
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "K, number of samples to be queried;",
          "Algorithm 2: CTIAL for cross-task transfer": "K, number of samples to be queried.",
          "from CEC": ""
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "Uncertainty measure, entropy or conﬁdence.",
          "Algorithm 2: CTIAL for cross-task transfer": "Output: Emotion estimation model f Dim.",
          "from CEC": ""
        },
        {
          "Algorithm 1: CTIAL for cross-task transfer\nfrom DEE": "Output: Emotion classiﬁcation model f Cat.",
          "Algorithm 2: CTIAL for cross-task transfer": "Train the model f Cat on DCat and f Dim on DDim;",
          "from CEC": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "",
          "Estimate category probabilities { ˆyCat\n}NP": "i\ni=1 of P by (1);"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "Estimate dimensional emotion values { ˆyDim\n}NP\ni=1 of P",
          "Estimate category probabilities { ˆyCat\n}NP": "Map { ˆyCat\n}NP"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "i",
          "Estimate category probabilities { ˆyCat\n}NP": "i\ni=1 into the dimensional emotion space and"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "using (2);",
          "Estimate category probabilities { ˆyCat\n}NP": "obtain { ˜yDim\n}NP"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "",
          "Estimate category probabilities { ˆyCat\n}NP": "i\ni=1 using (3);"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "for k = 1 : K do",
          "Estimate category probabilities { ˆyCat\n}NP": "for k = 1 : K do"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "}NP\nEstimate emotion category probabilities { ˆyCat\ni=1 of",
          "Estimate category probabilities { ˆyCat\n}NP": "Estimate dimensional emotion values { ˆyDim\n}NP"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "i",
          "Estimate category probabilities { ˆyCat\n}NP": "i\ni=1 of P"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "P using (1);",
          "Estimate category probabilities { ˆyCat\n}NP": "using (2);"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "}NP\nMap { ˆyCat\ni=1 into the dimensional emotion space",
          "Estimate category probabilities { ˆyCat\n}NP": "Calculate CTI {Ii}NP"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "i",
          "Estimate category probabilities { ˆyCat\n}NP": "i=1 using (4);"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "and obtain { ˜yDim\n}NP",
          "Estimate category probabilities { ˆyCat\n}NP": ""
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "i\ni=1 using (3);",
          "Estimate category probabilities { ˆyCat\n}NP": "Compute the distance {d′\ni}NP\ni=1 between samples in P"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "Calculate CTI {Ii}NP\ni=1 using (4);",
          "Estimate category probabilities { ˆyCat\n}NP": "and DDim using (11);"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "if the uncertainty measure is entropy then",
          "Estimate category probabilities { ˆyCat\n}NP": "Select sample xq using (12);"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "Compute entropy {Hi}NP\ni=1 using (6);",
          "Estimate category probabilities { ˆyCat\n}NP": "Query for the dimensional emotion values yDim\nof xq;\nq"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "Select sample xq using (8);",
          "Estimate category probabilities { ˆyCat\n}NP": ");\nDDim ← DDim ∪ (xq, yDim"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "else",
          "Estimate category probabilities { ˆyCat\n}NP": "P ← P \\xq;\nNP ← NP − 1;"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "// the\nuncertainty\nmeasure\nis",
          "Estimate category probabilities { ˆyCat\n}NP": "Update f Dim on DDim;"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "confidence",
          "Estimate category probabilities { ˆyCat\n}NP": "end"
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "Compute conﬁdence {Confi}NP\ni=1 using (7);",
          "Estimate category probabilities { ˆyCat\n}NP": ""
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "Select sample xq using (9);",
          "Estimate category probabilities { ˆyCat\n}NP": ""
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "end",
          "Estimate category probabilities { ˆyCat\n}NP": ""
        },
        {
          "Train the model f Cat on DCat and f Dim on DDim;": "",
          "Estimate category probabilities { ˆyCat\n}NP": "After\ndomain\nadaptation,\nthe model\ntrained\non\nthe\nsource"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "confidence": "Compute conﬁdence {Confi}NP\ni=1 using (7);",
          "end": ""
        },
        {
          "confidence": "Select sample xq using (9);",
          "end": ""
        },
        {
          "confidence": "end",
          "end": ""
        },
        {
          "confidence": "",
          "end": "After\ndomain\nadaptation,\nthe model"
        },
        {
          "confidence": "Query for the emotion category yCat\nof xq;",
          "end": ""
        },
        {
          "confidence": "q",
          "end": "dataset\nis applied to P to obtain the predictions"
        },
        {
          "confidence": ");\nDCat ← DCat ∪ (xq, yCat",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "P ← P \\xq;\nNP ← NP − 1;",
          "end": ""
        },
        {
          "confidence": "",
          "end": "Section 3.1."
        },
        {
          "confidence": "Update f Cat on DCat;",
          "end": ""
        },
        {
          "confidence": "end",
          "end": ""
        },
        {
          "confidence": "",
          "end": "3\nEXPERIMENTAL SETUP"
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "2.6\nDomain Adaptation in Cross-Corpus Transfer",
          "end": ""
        },
        {
          "confidence": "",
          "end": "3.1\nDatasets and Feature Extraction"
        },
        {
          "confidence": "CTIAL assumes the source model can make reliable predictions",
          "end": ""
        },
        {
          "confidence": "",
          "end": "Three\npublic\nspeech\nemotion\ndatasets,"
        },
        {
          "confidence": "for\nthe target dataset. However, speech emotion recognition cor-",
          "end": ""
        },
        {
          "confidence": "",
          "end": "Emotional Dyadic Motion Capture Database)"
        },
        {
          "confidence": "pora vary according to whether\nthey are\nacted or\nspontaneous,",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "collected in lab or in the wild, and so on. These discrepancies may",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "cause a model trained on one corpus to perform poorly on another,",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "violating the underlying assumption in CTIAL.",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "To enable cross-corpus transfer, we adopt two classical domain",
          "end": ""
        },
        {
          "confidence": "",
          "end": "within-corpus\ntransfer on IEMOCAP,"
        },
        {
          "confidence": "adaptation approaches,\ntransfer component analysis\n(TCA)\n[32]",
          "end": ""
        },
        {
          "confidence": "",
          "end": "from VAM and MELD to IEMOCAP."
        },
        {
          "confidence": "and balanced distribution adaptation (BDA) [33].",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "When the source task is CEC, BDA jointly aligns both the",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "marginal distributions and the class-conditional distributions with",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "a balance factor that adjusts the weights of the two corresponding",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "terms\nin the objective\nfunction. Speciﬁcally,\nthe\nsource model",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "assigns pseudo-labels for the unlabeled target dataset. The source",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "and target datasets’ average features and their corresponding class’",
          "end": ""
        },
        {
          "confidence": "",
          "end": "CEC on MELD,\nonly\nfour\noverlapping classes"
        },
        {
          "confidence": "average features are aligned. The adapted features of\nthe source",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "dataset are then used to update the source model. This process",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "iterates till convergence.",
          "end": ""
        },
        {
          "confidence": "",
          "end": "NRC Lexicon from [0, 1] to [1, 5]."
        },
        {
          "confidence": "When\nthe\nsource\ntask\nis DEE, TCA adapts\nthe marginal",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "distributions of\nthe\nsource\nand target datasets by reducing the",
          "end": ""
        },
        {
          "confidence": "",
          "end": ""
        },
        {
          "confidence": "distance between their average features. BDA is not used here,",
          "end": ""
        },
        {
          "confidence": "since the class-conditional probabilities are difﬁcult to compute in",
          "end": ""
        },
        {
          "confidence": "",
          "end": "1. The ‘happy’ class contains data of"
        },
        {
          "confidence": "regression problems.",
          "end": "annotation as in multiple previous works."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In cross-task AL, we combined the source and target datasets": "and used principal component analysis to reduce the feature di-",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "We compared the performance of\nthe following sample selection"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "mensionality (maintaining 90% variance). In cross-corpus transfer,",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "strategies in cross-task transfer from DEE to CEC:"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "an additional feature adaptation step was performed using TCA or",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "BDA to obtain more accurate source task predictions for samples",
          "4.1\nAlgorithms": "1)\nRandom sampling (Rand), which randomly selects K"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "in P .",
          "4.1\nAlgorithms": "samples to annotate."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "2)\nEntropy (Ent) [30], an uncertainty-based within-task AL"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "Before applying AL to P , we applied principal component",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "approach that\nselects\nsamples with maximum entropy"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "analysis\nto the original 768-dimensional\nfeatures\nof\nthe\ntarget",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "computed by (6)."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "dataset in both cross-task AL and within-task AL baselines.",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "3)\nLeast conﬁdence (LC) [31], an uncertainty-based within-"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "task AL approach that\nselects\nsamples with minimum"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "3.2\nExperimental Setup",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "prediction conﬁdence computed by (7)."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "We used logistic regression (LR) and ridge regression (RR) as the",
          "4.1\nAlgorithms": "4)\nMulti-task iGS on the\nsource task (Source\nMTiGS),"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "base model\nfor CEC and DEE,\nrespectively. The weight of\nthe",
          "4.1\nAlgorithms": "which\nperforms AL according\nto\nthe\ninformativeness"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "regularization term in each model,\ni.e., 1/C in LR and α in RR,",
          "4.1\nAlgorithms": "metric in the source DEE task. Speciﬁcally, we compute"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "was chosen from {1, 5, 10, 50, 1e2, 5e2, 1e3, 5e3} by three-fold",
          "4.1\nAlgorithms": "the distance between samples xi ∈ P and xj ∈ DCat"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "cross-validation on the corresponding training data.",
          "4.1\nAlgorithms": "by (10) and select samples with the maximum distance."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "In cross-corpus transfer experiments, feature dimensionality in",
          "4.1\nAlgorithms": "Without\nany\ndimensional\nemotion\nlabels\nof DCat, we"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "TCA and BDA was set\nto 30 and 40, respectively. BDA used 10",
          "4.1\nAlgorithms": "estimated from the\nwith ˆydim\nreplace the true label ydim\nj\nj"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "iterations to estimate the class labels of\nthe target dataset, align",
          "4.1\nAlgorithms": "source model. This simple cross-task AL baseline directly"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "the marginal and conditional distributions, and update the classi-",
          "4.1\nAlgorithms": "uses the knowledge from the source task."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "ﬁer. The balance factor was\nselected from {0.1, 0.2, ..., 0.9} to",
          "4.1\nAlgorithms": "5)\nCTIAL, which selects\nsamples with the maximum CTI"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "minimize the sum of the maximum mean discrepancy metrics [39]",
          "4.1\nAlgorithms": "by (5)."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "on all data and in each class.",
          "4.1\nAlgorithms": "6)\nEnt-CTIAL, which integrates entropy with CTI as in-"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "troduced in Algorithm 1."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "7)\nLC-CTIAL, which integrates\nthe prediction conﬁdence"
        },
        {
          "In cross-task AL, we combined the source and target datasets": "3.3\nPerformance Evaluation",
          "4.1\nAlgorithms": ""
        },
        {
          "In cross-task AL, we combined the source and target datasets": "",
          "4.1\nAlgorithms": "with CTI as introduced in Algorithm 1."
        },
        {
          "In cross-task AL, we combined the source and target datasets": "We aimed to classify all\nsamples in the target dataset,\nsome by",
          "4.1\nAlgorithms": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "IEMOCAP. Experiments were\nrepeated 10 times with different"
        },
        {
          "5": "initial labeled sample sets."
        },
        {
          "5": "In each run of\nthe experiment,\nthe initial 20 labeled samples"
        },
        {
          "5": "were randomly selected. In the subsequent 200 iterations of sample"
        },
        {
          "5": "selection, one sample was chosen for annotation at a time."
        },
        {
          "5": "Balanced classiﬁcation accuracy (BCA),\ni.e.,\nthe average per-"
        },
        {
          "5": "class accuracies, was used as\nthe performance measure in CEC"
        },
        {
          "5": "since\nIEMOCAP\nhas\nsigniﬁcant\nclass-imbalance. Root mean"
        },
        {
          "5": "squared error (RMSE) and correlation coefﬁcient (CC) were used"
        },
        {
          "5": "as performance measures in DEE."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "ON\n4\nEXPERIMENTS\nCROSS-TASK\nTRANSFER"
        },
        {
          "5": ""
        },
        {
          "5": "FROM DEE TO CEC"
        },
        {
          "5": ""
        },
        {
          "5": "This section presents the results in cross-task transfer\nfrom DEE"
        },
        {
          "5": "to CEC. The DEE and CEC tasks could be from the same corpus,"
        },
        {
          "5": "or different ones."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "4.1\nAlgorithms"
        },
        {
          "5": ""
        },
        {
          "5": "We compared the performance of\nthe following sample selection"
        },
        {
          "5": ""
        },
        {
          "5": "strategies in cross-task transfer from DEE to CEC:"
        },
        {
          "5": ""
        },
        {
          "5": "1)\nRandom sampling (Rand), which randomly selects K"
        },
        {
          "5": "samples to annotate."
        },
        {
          "5": "2)\nEntropy (Ent) [30], an uncertainty-based within-task AL"
        },
        {
          "5": ""
        },
        {
          "5": "approach that\nselects\nsamples with maximum entropy"
        },
        {
          "5": ""
        },
        {
          "5": "computed by (6)."
        },
        {
          "5": ""
        },
        {
          "5": "3)\nLeast conﬁdence (LC) [31], an uncertainty-based within-"
        },
        {
          "5": "task AL approach that\nselects\nsamples with minimum"
        },
        {
          "5": ""
        },
        {
          "5": "prediction conﬁdence computed by (7)."
        },
        {
          "5": "4)\nMulti-task iGS on the\nsource task (Source\nMTiGS),"
        },
        {
          "5": "which\nperforms AL according\nto\nthe\ninformativeness"
        },
        {
          "5": "metric in the source DEE task. Speciﬁcally, we compute"
        },
        {
          "5": "the distance between samples xi ∈ P and xj ∈ DCat"
        },
        {
          "5": "by (10) and select samples with the maximum distance."
        },
        {
          "5": "Without\nany\ndimensional\nemotion\nlabels\nof DCat, we"
        },
        {
          "5": "estimated from the\nwith ˆydim\nreplace the true label ydim\nj\nj"
        },
        {
          "5": "source model. This simple cross-task AL baseline directly"
        },
        {
          "5": "uses the knowledge from the source task."
        },
        {
          "5": "5)\nCTIAL, which selects\nsamples with the maximum CTI"
        },
        {
          "5": "by (5)."
        },
        {
          "5": "6)\nEnt-CTIAL, which integrates entropy with CTI as in-"
        },
        {
          "5": "troduced in Algorithm 1."
        },
        {
          "5": "7)\nLC-CTIAL, which integrates\nthe prediction conﬁdence"
        },
        {
          "5": ""
        },
        {
          "5": "with CTI as introduced in Algorithm 1."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "4.2\nEffectiveness of CTIAL"
        },
        {
          "5": ""
        },
        {
          "5": "Fig.\n3\nshows\nthe\naverage BCAs\nin within-\nand\ncross-corpus"
        },
        {
          "5": ""
        },
        {
          "5": "cross-task transfers from DEE to CEC. To examine if the perfor-"
        },
        {
          "5": ""
        },
        {
          "5": "mance improvements of\nthe integration of\nthe uncertainty-based"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "approaches\nand CTIAL were\nstatistically signiﬁcant, Wilcoxon"
        },
        {
          "5": ""
        },
        {
          "5": "signed-rank tests with Holm’s p-value adjustment [40] were per-"
        },
        {
          "5": ""
        },
        {
          "5": "formed on the results\nin each AL iteration. The test\nresults are"
        },
        {
          "5": ""
        },
        {
          "5": "shown in Fig. 4."
        },
        {
          "5": ""
        },
        {
          "5": "Figs. 3 and 4 demonstrate that:"
        },
        {
          "5": ""
        },
        {
          "5": "1)\nAs the number of labeled samples increased, classiﬁers in"
        },
        {
          "5": "all approaches became more accurate, achieving higher"
        },
        {
          "5": "BCAs."
        },
        {
          "5": "2)\nBetween the two uncertainty-based AL approaches, LC"
        },
        {
          "5": "outperformed Ent. However, both may not outperform"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rand": ""
        },
        {
          "Rand": "Ent"
        },
        {
          "Rand": ""
        },
        {
          "Rand": "LC"
        },
        {
          "Rand": ""
        },
        {
          "Rand": "Source MTiGS"
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        },
        {
          "Rand": "CTIAL"
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        },
        {
          "Rand": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "Source\nsmall,\ndemonstrating\nMTiGS when K was"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": ""
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "that\nthe proposed CTI measure properly exploited the"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "relationship between the CEC and DEE tasks and effec-"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": ""
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "tively utilized knowledge from the source task. However,"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": ""
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "in cross-corpus\ntransfer, CTIAL and Rand had similar"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "performance when K was large. The reason may be that"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": "domain shift\nlimited the performance of the source DEE"
        },
        {
          "approaches (Ent and LC), and the cross-task AL baseline": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "K": ""
        },
        {
          "K": "(b)"
        },
        {
          "K": ""
        },
        {
          "K": ""
        },
        {
          "K": "Fig. 3. Average BCAs of different sample selection approaches in cross-"
        },
        {
          "K": ""
        },
        {
          "K": "task transfer from DEE to CEC. (a) Within-corpus transfer from DEE on"
        },
        {
          "K": ""
        },
        {
          "K": "IEMOCAP to CEC on IEMOCAP; and,\n(b) cross-corpus transfer\nfrom"
        },
        {
          "K": ""
        },
        {
          "K": "DEE on VAM to CEC on IEMOCAP. K is the number of samples to be"
        },
        {
          "K": ""
        },
        {
          "K": "queried in addition to the initial\nlabeled ones."
        },
        {
          "K": ""
        },
        {
          "K": ""
        },
        {
          "K": ""
        },
        {
          "K": "Rand,\nsuggesting that only considering the uncertainty"
        },
        {
          "K": ""
        },
        {
          "K": "may not be enough."
        },
        {
          "K": ""
        },
        {
          "K": "Source\n3)\nMTiGS that performed AL according to the"
        },
        {
          "K": ""
        },
        {
          "K": "source DEE task achieved higher BCAs than Rand and"
        },
        {
          "K": ""
        },
        {
          "K": "the\ntwo within-task AL approaches,\nbecause MTiGS"
        },
        {
          "K": ""
        },
        {
          "K": "increased the feature diversity, which was also useful for"
        },
        {
          "K": ""
        },
        {
          "K": "the target CEC task. Additionally, MTiGS increased the"
        },
        {
          "K": ""
        },
        {
          "K": "diversity of the dimensional emotion labels, which in turn"
        },
        {
          "K": ""
        },
        {
          "K": "increased the diversity of the categorical emotion labels."
        },
        {
          "K": ""
        },
        {
          "K": "4)\nCTIAL outperformed Rand,\nthe\ntwo within-task AL"
        },
        {
          "K": "approaches (Ent and LC), and the cross-task AL baseline"
        },
        {
          "K": "Source\nsmall,\ndemonstrating\nMTiGS when K was"
        },
        {
          "K": ""
        },
        {
          "K": "that\nthe proposed CTI measure properly exploited the"
        },
        {
          "K": "relationship between the CEC and DEE tasks and effec-"
        },
        {
          "K": ""
        },
        {
          "K": "tively utilized knowledge from the source task. However,"
        },
        {
          "K": ""
        },
        {
          "K": "in cross-corpus\ntransfer, CTIAL and Rand had similar"
        },
        {
          "K": "performance when K was large. The reason may be that"
        },
        {
          "K": "domain shift\nlimited the performance of the source DEE"
        },
        {
          "K": ""
        },
        {
          "K": "models on the target dataset, which further resulted in in-"
        },
        {
          "K": "accurate CTI calculation and degraded AL performance."
        },
        {
          "K": "5)\nIntegrating CTI with uncertainty can further enhance the"
        },
        {
          "K": ""
        },
        {
          "K": "classiﬁcation accuracies by simultaneously considering"
        },
        {
          "K": "within- and cross-task informativeness: both LC-CTIAL"
        },
        {
          "K": ""
        },
        {
          "K": "and Ent-CTIAL statistically signiﬁcantly outperformed"
        },
        {
          "K": ""
        },
        {
          "K": "the other baselines."
        },
        {
          "K": ""
        },
        {
          "K": "LC-CTIAL\n6)\ngenerally\nachieved\nthe\nbest\nperformance"
        },
        {
          "K": ""
        },
        {
          "K": "among all seven approaches."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "single tuple of dimensional emotion values according to"
        },
        {
          "7": "the affective norms. In practice, samples belonging to the"
        },
        {
          "7": "same emotion category may have diverse emotion prim-"
        },
        {
          "7": "itives. Direct mapping\noversimpliﬁed\nthe\nrelationship"
        },
        {
          "7": "between categorical emotions and dimensional emotions."
        },
        {
          "7": ""
        },
        {
          "7": "3)\nMTiGS on average achieved lower RMSEs\nand higher"
        },
        {
          "7": ""
        },
        {
          "7": "CCs\nthan Rand. However,\nit\nperformed\nsimilarly\nto"
        },
        {
          "7": "Rand for some emotion primitives."
        },
        {
          "7": "Source\n4)\nLC performed much worse than Rand because"
        },
        {
          "7": "it only considered the classiﬁcation information in the"
        },
        {
          "7": "source task but\nignored the characteristics of\nthe target"
        },
        {
          "7": "task. Samples with low classiﬁcation conﬁdence may not"
        },
        {
          "7": ""
        },
        {
          "7": "be very useful to the target regression tasks."
        },
        {
          "7": ""
        },
        {
          "7": "5)\nCTiGS initially performed worse than MTiGS, but grad-"
        },
        {
          "7": ""
        },
        {
          "7": "ually outperformed on some dimensions as\nthe number"
        },
        {
          "7": "of labeled samples increased. The reason is that CTiGS"
        },
        {
          "7": "emphasizes\nthe within-class\nsample\ndiversity, whereas"
        },
        {
          "7": "MTiGS focuses on the global sample diversity. The latter"
        },
        {
          "7": "is more\nbeneﬁcial\nfor\nthe\nregression models\nto\nlearn"
        },
        {
          "7": "global patterns, which is important when labeled samples"
        },
        {
          "7": "are\nenriching\nthe within-class\nrare. As K increases,"
        },
        {
          "7": "sample diversity facilitates\nthe global models\nto learn"
        },
        {
          "7": "local patterns."
        },
        {
          "7": "CTIAL\n6)\nIn within-corpus\ntransfer,\noutperformed\nboth"
        },
        {
          "7": "Rand and MTiGS initially, but gradually became inferior"
        },
        {
          "7": "to Rand.\nIn\ncross-corpus\ntransfer, CTIAL performed"
        },
        {
          "7": "better\nthan Rand on arousal but worse on valence and"
        },
        {
          "7": "dominance. The\npossible\nreason\nis\nsimilar\nto\nthat\nin"
        },
        {
          "7": "transfer\nfrom DEE to CEC:\nthe\nsource\nclassiﬁcation"
        },
        {
          "7": "models were not accurate enough, resulting in inaccurate"
        },
        {
          "7": "CTI calculation and unsatisfactory AL performance."
        },
        {
          "7": "7)\nMTiGS-CTIAL achieved the best overall performance"
        },
        {
          "7": "in both within- and cross-corpus transfers,\nindicating that"
        },
        {
          "7": "considering both CTI and diversity in AL helped improve"
        },
        {
          "7": "the regression performance even when the CTI was not"
        },
        {
          "7": "very accurate."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "5.3\nEffectiveness of BDA"
        },
        {
          "7": ""
        },
        {
          "7": "Fig. 8 shows the classiﬁcation results in CEC on IEMOCAP using"
        },
        {
          "7": ""
        },
        {
          "7": "different\ntraining data and projection dimensionality for BDA in"
        },
        {
          "7": ""
        },
        {
          "7": "cross-corpus\ntransfer.\nIn within-corpus validation, we used two"
        },
        {
          "7": ""
        },
        {
          "7": "sessions as the training data and the rest\nthree sessions as the test"
        },
        {
          "7": ""
        },
        {
          "7": "data. The average BCA in 10 training-test partitions for four-class"
        },
        {
          "7": "emotion recognition reached\n(E={angry, happy,\nsad, neutral})"
        },
        {
          "7": ""
        },
        {
          "7": "0.6477.\nIn cross-corpus\ntransfer, directly transferring the model"
        },
        {
          "7": "trained on MELD to IEMOCAP only achieved a BCA of 0.3904."
        },
        {
          "7": "BDA boosted\nthe BCA to\n0.5378 when\nsetting\nthe\nprojected"
        },
        {
          "7": "feature dimensionality to 45."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "6\nCONCLUSIONS"
        },
        {
          "7": ""
        },
        {
          "7": "Human emotions can be described by both categorical and dimen-"
        },
        {
          "7": ""
        },
        {
          "7": "sional representations. Usually,\ntraining accurate emotion classiﬁ-"
        },
        {
          "7": "cation and estimation models requires large labeled datasets. How-"
        },
        {
          "7": "ever, manual labeling of affective samples is expensive, due to the"
        },
        {
          "7": "subtleness and complexity of emotions. This paper integrates AL"
        },
        {
          "7": "and TL to reduce the labeling effort. We proposed CTI to measure"
        },
        {
          "7": "the prediction inconsistency between the CEC and DEE tasks. To"
        },
        {
          "7": "further consider other useful AL metrics, CTI was integrated with"
        },
        {
          "7": "uncertainty in CEC and diversity in DEE for enhanced reliability."
        },
        {
          "7": "Experiments on three speech emotion datasets demonstrated the"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "100",
          "Arousal": "100",
          "Dominance": "100",
          "Avg": "100"
        },
        {
          "Valence": "K",
          "Arousal": "K",
          "Dominance": "K",
          "Avg": "K"
        },
        {
          "Valence": "Valence",
          "Arousal": "Arousal",
          "Dominance": "Dominance",
          "Avg": "Avg"
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "100",
          "Arousal": "100",
          "Dominance": "100",
          "Avg": "100"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "100",
          "Arousal": "100",
          "Dominance": "100",
          "Avg": "100"
        },
        {
          "Valence": "K",
          "Arousal": "K",
          "Dominance": "K",
          "Avg": "K"
        },
        {
          "Valence": "Valence",
          "Arousal": "Arousal",
          "Dominance": "Dominance",
          "Avg": "Avg"
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        },
        {
          "Valence": "",
          "Arousal": "",
          "Dominance": "",
          "Avg": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b)": "Fig. 6. Average RMSEs and CCs of different sample selection approaches in cross-task transfer from CEC to DEE. (a) Within-corpus transfer from"
        },
        {
          "(b)": "CEC on IEMOCAP to DEE on IEMOCAP; and, (b) cross-corpus transfer from CEC on MELD to DEE on IEMOCAP. K is the number of samples to"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "[6]"
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "[7]"
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "[8]"
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "[9]"
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "[11]"
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "Int’l Conf. on Acoustics, Speech and Signal Processing, Brighton, UK,"
        },
        {
          "9": "May 2019, pp. 3732–3736."
        },
        {
          "9": "[20] H. Kaya, F. G¨urpınar, and A. A. Salah, “Video-based emotion recognition"
        },
        {
          "9": "in the wild using deep transfer\nlearning and score fusion,” Image and"
        },
        {
          "9": "Vision Computing, vol. 65, pp. 66–75, 2017."
        },
        {
          "9": "[21]\nT. Q. Ngo and S. Yoon, “Facial expression recognition on static images,”"
        },
        {
          "9": "in Proc. Future Data and Security Engineering, Nha Trang City, Viet-"
        },
        {
          "9": "nam, Nov. 2019, pp. 640–647."
        },
        {
          "9": "[22] N. Sugianto and D. Tjondronegoro, “Cross-domain knowledge transfer"
        },
        {
          "9": "for incremental deep learning in facial expression recognition,” in Proc."
        },
        {
          "9": "Int’l Conf. on Robot Intelligence Technology and Applications, Daejeon,"
        },
        {
          "9": "South Korea, Nov. 2019, pp. 205–209."
        },
        {
          "9": "[23] H. Zhao, N. Ye,\nand R. Wang,\n“Speech\nemotion\nrecognition\nbased"
        },
        {
          "9": ""
        },
        {
          "9": "on hierarchical attributes using feature nets,” Int’l Journal of Parallel,"
        },
        {
          "9": "Emergent and Distributed Systems, vol. 35, no. 3, pp. 354–364, 2020."
        },
        {
          "9": "[24]\nS. Park, J. Kim, J. Jeon, H. Park, and A. Oh, “Toward dimensional emo-"
        },
        {
          "9": "tion detection from categorical emotion annotations,” arXiv:1911.02499,"
        },
        {
          "9": "2019."
        },
        {
          "9": "[25]\nS. M. Mohammad,\n“Obtaining\nreliable\nhuman\nratings\nof Valence,"
        },
        {
          "9": "Arousal, and Dominance for 20,000 English words,”\nin Proc. Annual"
        },
        {
          "9": "Conf.\nof\nthe Association\nfor Computational\nLinguistics, Melbourne,"
        },
        {
          "9": "Australia, Jul. 2018."
        },
        {
          "9": "[26] M. M. Bradley\nand P.\nJ. Lang,\n“Affective norms\nfor English words"
        },
        {
          "9": "(ANEW):\nInstruction manual\nand\naffective\nratings,” The Center\nfor"
        },
        {
          "9": "Research in Psychophysiology, University of Florida, Tech. Rep., 1999."
        },
        {
          "9": "[27]\nF. Zhou, S. Kong, C. C. Fowlkes, T. Chen, and B. Lei, “Fine-grained"
        },
        {
          "9": "facial expression analysis using dimensional emotion model,” Neurocom-"
        },
        {
          "9": ""
        },
        {
          "9": "puting, vol. 392, pp. 38–49, 2020."
        },
        {
          "9": "[28] A. B. Warriner, V. Kuperman, and M. Brysbaert, “Norms of valence,"
        },
        {
          "9": ""
        },
        {
          "9": "arousal, and dominance for 13,915 English lemmas,” Behavior Research"
        },
        {
          "9": ""
        },
        {
          "9": "Methods, vol. 45, pp. 1191–1207, 2013."
        },
        {
          "9": ""
        },
        {
          "9": "[29] C. E. Shannon,\n“A mathematical\ntheory of\ncommunication,” The Bell"
        },
        {
          "9": ""
        },
        {
          "9": "System Technical Journal, vol. 27, no. 3, pp. 379–423, 1948."
        },
        {
          "9": ""
        },
        {
          "9": "[30] B. Settles and M. Craven, “An analysis of active learning strategies for"
        },
        {
          "9": ""
        },
        {
          "9": "sequence labeling tasks,” in Proc. Conf. on Empirical Methods in Natural"
        },
        {
          "9": ""
        },
        {
          "9": "Language Processing, Honolulu, HI, Oct. 2008, pp. 1070–1079."
        },
        {
          "9": ""
        },
        {
          "9": "[31] A. Culotta and A. McCallum, “Reducing labeling effort\nfor structured"
        },
        {
          "9": "prediction tasks,” in Proc. AAAI Conf. on Artiﬁcial\nIntelligence, vol. 5,"
        },
        {
          "9": "Pittsburgh, PA, Jul. 2005, pp. 746–751."
        },
        {
          "9": "[32]\nS. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via"
        },
        {
          "9": "transfer component analysis,” IEEE Trans. on Neural Networks, vol. 22,"
        },
        {
          "9": "no. 2, pp. 199–210, 2010."
        },
        {
          "9": "[33]\nJ. Wang, Y. Chen, S. Hao, W. Feng, and Z. Shen, “Balanced distribution"
        },
        {
          "9": "IEEE Int’l Conf.\non Data\nadaptation for\ntransfer\nlearning,”\nin Proc."
        },
        {
          "9": "Mining, New Orleans, LA, November 2017, pp. 1129–1134."
        },
        {
          "9": "[34] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, J. N. Kim,"
        },
        {
          "9": "Samueland Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive"
        },
        {
          "9": "emotional dyadic motion capture database,” Language Resources and"
        },
        {
          "9": "Evaluation, vol. 42, no. 4, pp. 335–359, 2008."
        },
        {
          "9": "[35]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihal-"
        },
        {
          "9": "cea, “MELD: A multimodal multi-party dataset for emotion recognition"
        },
        {
          "9": "the Association for\nin conversations,” in Proc. 57th Annual Meeting of"
        },
        {
          "9": "Computational Linguistics, Florence, Italy, Jul. 2019, pp. 527–536."
        },
        {
          "9": ""
        },
        {
          "9": "[36] M. Grimm, K. Kroschel, and S. Narayanan, “The Vera am Mittag German"
        },
        {
          "9": ""
        },
        {
          "9": "IEEE Int’l Conf. on\naudio-visual emotional speech database,” in Proc."
        },
        {
          "9": ""
        },
        {
          "9": "Multimedia and Expo, Hannover, Germany, Jun. 2008, pp. 865–868."
        },
        {
          "9": ""
        },
        {
          "9": "[37] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0: A"
        },
        {
          "9": ""
        },
        {
          "9": "framework for\nself-supervised learning of\nspeech representations,”\nin"
        },
        {
          "9": ""
        },
        {
          "9": "Proc.\nInt’l Conf. on Neural\nInformation Processing Systems, vol. 33,"
        },
        {
          "9": ""
        },
        {
          "9": "Virtual Event, Dec. 2020, pp. 12 449–12 460."
        },
        {
          "9": "[38] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An"
        },
        {
          "9": "Int’l Conf.\nASR corpus based on public domain audio books,” in Proc."
        },
        {
          "9": "on Acoustics, Speech and Signal Processing, South Brisbane, Australia,"
        },
        {
          "9": "Apr. 2015, pp. 5206–5210."
        },
        {
          "9": "[39] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf,"
        },
        {
          "9": "and A. J. Smola, “Integrating structured biological data by kernel max-"
        },
        {
          "9": "imum mean discrepancy,” Bioinformatics, vol. 22, no. 14, pp. 49–57,"
        },
        {
          "9": "2006."
        },
        {
          "9": "[40]\nS. Holm,\n“A simple\nsequentially\nrejective multiple\ntest\nprocedure,”"
        },
        {
          "9": "Scandinavian Journal of Statistics, vol. 6, no. 2, pp. 65–70, 1979."
        },
        {
          "9": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic analysis of facial expressions: The state of the art",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Real-time inference of complex mental states from facial expressions and head gestures",
      "authors": [
        "R Kaliouby",
        "P Robinson"
      ],
      "year": "2004",
      "venue": "Proc. Int'l Conf. on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Affect estimation in 3D space using multi-task active learning for regression",
      "authors": [
        "D Wu",
        "J Huang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Affective brain-computer interfaces (aBCIs): A tutorial",
      "authors": [
        "D Wu",
        "B.-L Lu",
        "B Hu",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Proc. of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition",
      "authors": [
        "Y Jiang",
        "W Li",
        "M Hossain",
        "M Chen",
        "A Alelaiwi",
        "M Al-Hammadi"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Emotion based music recommendation system using wearable physiological sensors",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamasak"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Consumer Electronics"
    },
    {
      "citation_id": "7",
      "title": "Universals and cultural differences in the judgments of facial expressions of emotion",
      "authors": [
        "P Ekman",
        "W Friesen",
        "M O'sullivan",
        "A Chan",
        "I Diacoyanni-Tarlatzis",
        "K Heider",
        "R Krause",
        "W Lecompte",
        "T Pitcairn",
        "P Ricci-Bitti"
      ],
      "year": "1987",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "8",
      "title": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1980",
      "venue": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "10",
      "title": "Active learning literature survey",
      "authors": [
        "B Settles"
      ],
      "year": "2009",
      "venue": "Active learning literature survey"
    },
    {
      "citation_id": "11",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE Trans. on Knowledge and Data Engineering"
    },
    {
      "citation_id": "12",
      "title": "User emotion recognition from a larger pool of social network data using active learning",
      "authors": [
        "G Muhammad",
        "M Alhamid"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "13",
      "title": "Dynamic active learning based on agreement and applied to emotion recognition in spoken interactions",
      "authors": [
        "Y Zhang",
        "E Coutinho",
        "Z Zhang",
        "C Quan",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "Proc. of the ACM on Int'l Conf. on Multimodal Interaction"
    },
    {
      "citation_id": "14",
      "title": "Active learning for regression using greedy sampling",
      "authors": [
        "D Wu",
        "C.-T Lin",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "15",
      "title": "Active learning for speech emotion recognition using deep neural network",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2019",
      "venue": "Proc. Int'l Conf. on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "16",
      "title": "Transfer learning for EEG-based braincomputer interfaces: A review of progress made since 2016",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "IEEE Trans. on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "17",
      "title": "A survey on negative transfer",
      "authors": [
        "W Zhang",
        "L Deng",
        "L Zhang",
        "D Wu"
      ],
      "year": "2023",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "18",
      "title": "Can emotion be transferred?-A review on transfer learning for EEG-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "19",
      "title": "Transferable positive/negative speech emotion recognition via class-wise adversarial domain adaptation",
      "authors": [
        "H Zhou",
        "K Chen"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Int'l Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "H Kaya",
        "F Gürpınar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "21",
      "title": "Facial expression recognition on static images",
      "authors": [
        "T Ngo",
        "S Yoon"
      ],
      "year": "2019",
      "venue": "Proc. Future Data and Security Engineering, Nha Trang City, Vietnam"
    },
    {
      "citation_id": "22",
      "title": "Cross-domain knowledge transfer for incremental deep learning in facial expression recognition",
      "authors": [
        "N Sugianto",
        "D Tjondronegoro"
      ],
      "year": "2019",
      "venue": "Proc. Int'l Conf. on Robot Intelligence Technology and Applications"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition based on hierarchical attributes using feature nets",
      "authors": [
        "H Zhao",
        "N Ye",
        "R Wang"
      ],
      "year": "2020",
      "venue": "Int'l Journal of Parallel, Emergent and Distributed Systems"
    },
    {
      "citation_id": "24",
      "title": "Toward dimensional emotion detection from categorical emotion annotations",
      "authors": [
        "S Park",
        "J Kim",
        "J Jeon",
        "H Park",
        "A Oh"
      ],
      "year": "2019",
      "venue": "Toward dimensional emotion detection from categorical emotion annotations",
      "arxiv": "arXiv:1911.02499"
    },
    {
      "citation_id": "25",
      "title": "Obtaining reliable human ratings of Valence, Arousal, and Dominance for 20,000 English words",
      "authors": [
        "S Mohammad"
      ],
      "year": "2018",
      "venue": "Proc. Annual Conf. of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Affective norms for English words (ANEW): Instruction manual and affective ratings",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1999",
      "venue": "The Center for Research in Psychophysiology"
    },
    {
      "citation_id": "27",
      "title": "Fine-grained facial expression analysis using dimensional emotion model",
      "authors": [
        "F Zhou",
        "S Kong",
        "C Fowlkes",
        "T Chen",
        "B Lei"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "Norms of valence, arousal, and dominance for 13,915 English lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "29",
      "title": "A mathematical theory of communication",
      "authors": [
        "C Shannon"
      ],
      "year": "1948",
      "venue": "The Bell System Technical Journal"
    },
    {
      "citation_id": "30",
      "title": "An analysis of active learning strategies for sequence labeling tasks",
      "authors": [
        "B Settles",
        "M Craven"
      ],
      "year": "2008",
      "venue": "Proc. Conf. on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Reducing labeling effort for structured prediction tasks",
      "authors": [
        "A Culotta",
        "A Mccallum"
      ],
      "year": "2005",
      "venue": "Proc. AAAI Conf. on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE Trans. on Neural Networks"
    },
    {
      "citation_id": "33",
      "title": "Balanced distribution adaptation for transfer learning",
      "authors": [
        "J Wang",
        "Y Chen",
        "S Hao",
        "W Feng",
        "Z Shen"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Int'l Conf. on Data Mining"
    },
    {
      "citation_id": "34",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "J Kim",
        "Samueland Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "35",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "The Vera am Mittag German audio-visual emotional speech database",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. IEEE Int'l Conf. on Multimedia and Expo"
    },
    {
      "citation_id": "37",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. Int'l Conf. on Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. Int'l Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Integrating structured biological data by kernel maximum mean discrepancy",
      "authors": [
        "K Borgwardt",
        "A Gretton",
        "M Rasch",
        "H.-P Kriegel",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "40",
      "title": "A simple sequentially rejective multiple test procedure",
      "authors": [
        "S Holm"
      ],
      "year": "1979",
      "venue": "Scandinavian Journal of Statistics"
    }
  ]
}