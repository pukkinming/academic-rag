{
  "paper_id": "2107.03748v2",
  "title": "Expressive Voice Conversion: A Joint Framework For Speaker Identity And Emotional Style Transfer",
  "published": "2021-07-08T10:48:04Z",
  "authors": [
    "Zongyang Du",
    "Berrak Sisman",
    "Kun Zhou",
    "Haizhou Li"
  ],
  "keywords": [
    "Voice conversion",
    "emotion style features",
    "StarGAN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditional voice conversion (VC) has been focused on speaker identity conversion for speech with a neutral expression. We note that emotional expression plays an essential role in daily communication, and the emotional style of speech can be speaker-dependent. In this paper, we study a technique to jointly convert the speaker identity and speakerdependent emotional style, that is called expressive voice conversion. We propose a StarGAN-based framework to learn a many-to-many mapping across different speakers, that takes into account speaker-dependent emotional style without the need for parallel data. To this end, we condition the generator on emotional style encoding derived from a pre-trained speech emotion recognition (SER) model. The experiments validate the effectiveness of our proposed framework in both objective and subjective evaluations. To our best knowledge, this is the first study on expressive voice conversion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Traditional voice conversion (VC) aims to modify one's voice to sound like that of another while keeping linguistic content and emotional style unchanged  [1] . VC is an enabling technology for various tasks, such as conversational assistants, cross-lingual speech synthesis  [2] [3] [4] , and speaker verification  [5] . In this paper, we formulate a new research topic denoted as Expressive Voice Conversion, and propose a solution that jointly performs speaker identity and emotional style transfer for emotional speakers.\n\nIt is known that human speech is expressive and emotive in nature, and people usually express themselves with various emotional states such as happy and sad  [6] . Studies also reveal the fact that emotional speech style contains speakerdependent elements which has never been studied in traditional voice conversion  [7] [8] [9] . The study of expressive voice conversion is motivated to fill the gap. In expressive voice conversion, both speaker identity and emotional style need to be carefully dealt with, as illustrated in Fig.  1 . In general, emotional speech style is more complex and difficult to model  [10] [11] [12]  than speaker identity which can be characterized by frame-based spectral features. It is known that emotional speech style can be expressed with universal patterns linked to lexical stress and speech acts (speaker-independent emotional style)  [13, 14] , and with personal and other background characteristics associated with an individual (speaker-dependent emotional style)  [8, 12] . It makes sense to only transfer the speaker-dependent emotional style while preserving the speaker-independent elements in the process, which makes expressive voice conversion a challenging task.\n\nWhile expressive voice conversion shares similarities with emotional voice conversion, it differs from emotional voice conversion in many ways. For example, emotional voice conversion aims to convert the emotional state of an utterance while preserving the speaker identity  [15, 16] . In emotional voice conversion, the emotional state similarity (e.g. happy, sad, angry) between generated and target speech is the major concern  [17] [18] [19] . However, in expressive voice conversion, we do not seek to change the emotional state, instead we jointly transfer speaker-dependent characteristics such as speaker identity and emotional style to the converted speech.\n\nWe note that some previous studies  [20, 21]  refer expressive voice conversion to emotional voice conversion. To avoid any confusion, in this paper we refer expressive voice conver-sion to converting an input speech towards a target speaker both in terms of speaker identity and emotional style. We propose an expressive voice conversion framework that performs the conversion without the need for parallel data.\n\nIn traditional voice conversion, we seek to convert the speaker identity while preserving the linguistic information. As speaker identity is determined by the vocal timbre that is manifested in spectrum  [22] , early voice conversion studies are mainly focused on modeling the spectral mapping between the source and target with statistical parametric methods such as Gaussian mixture model (GMM)  [23] [24] [25] . Recent deep learning methods such as deep neural network (DNN)  [26] , and recurrent neural network (RNN)  [27]  have significantly improved the performance. We note that these methods require parallel data, that limits the scope of real-life applications. To enable non-parallel voice conversion, many frameworks based on generative models such as variational auto-encoders (VAE)  [28] [29] [30] [31]  and generative adversarial network (GAN)  [32] [33] [34] [35] [36] [37] [38]  are proposed in recent years. Among the GAN-based methods, StarGAN-VC  [39, 40]  extends nonparallel VC from two domains to multiple domains. It allows for sharing of knowledge across multiple pairwise mapping, and represents one of the successful attempts in non-parallel VC, which motivates our study. We would like to study the use of a StarGAN architecture to learn a translation model across multiple spectral domains from different speakers with different emotional styles.\n\nOur proposed framework consists of two stages for training: 1) emotional style descriptor training, where we train a SER network to learn emotion-related information such as emotional style features through acoustic features; 2) Star-GAN training, where we condition the generator with both speaker label and emotional style features. We believe that emotional style features serve as an excellent tool to describe the emotional style in a continuous space, thus more suitable for speaker-dependent emotional style transfer.\n\nThe main contributions of this paper include: 1) we formulate a novel research topic, expressive voice conversion, as an extension of voice conversion research; 2) we propose an expressive voice conversion framework based on StarGAN without the need for parallel data, and that is flexible for many-to-many conversion; 3) we conduct experiments on a publicly available multi-speaker database under different emotional states, and show the effectiveness of our proposed expressive voice conversion framework for both emotional style and speaker identity transfer. To our best knowledge, this is the first paper to study expressive voice conversion.\n\nThe rest of this paper is organized as follows: In Section 2, we introduce the related work. In Section 3, we investigate the speaker-dependent emotional style with deep emotional style features, which motivates our study. In Section 4, we introduce our proposed voice conversion framework with StarGAN. In Section 4, we report the experiments. Section 5 concludes the study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Traditional Voice Conversion And Datasets",
      "text": "Traditional voice conversion is studied for speech with a neutral expression, where voice quality has been the main focus. The most widely used speech databases for voice conversion include VCTK database  [41] , CMU-Arctic database  [42] , and Voice Conversion Challenge (VCC) corpus  [43] [44] [45] . Since these speech databases are emotion-free, it is straightforward to pay attention to the vocal timbre when conducting voice conversion research.\n\nWe note that voice conversion with emotional speech data has never been studied before, mostly due to the lack of suitable databases. Recently, ESD database  [16]  is released for emotional voice conversion studies, which includes multispeaker and multi-lingual parallel speech data with different emotions, and makes possible emotion-related studies, such as expressive voice conversion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Stargan For Voice Conversion",
      "text": "StarGAN  [46]  is first proposed to multi-domain image-toimage translation in computer vision, and then adopted to voice conversion  [39] . StarGAN  [39]  and its variants  [33, 37, 40]  have shown promising results in many-to-many voice conversion without the need for parallel data. A StarGAN consists of a generator, a discriminator, and a domain classifier. The generator takes both spectral features and domain information such as one-hot label, and translates the spectral features into the corresponding domain while the discriminator distinguishes whether the input features are real or fake. A domain classifier is used to further verify the label correctness of both real and generated spectral features. StarGAN also has been applied to other tasks such as affect generation  [47]  and emotional voice conversion  [48] . Motivated by the success of StarGAN in many-to-many voice conversion, we extend the idea and propose a many-to-many VC framework that can jointly transfer both speaker identity and emotional style, which will be further introduced in Section 4.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker-Dependent Emotional Style",
      "text": "Emotional feature extraction has been a research hotspot in speech emotion recognition (SER)  [49] . With the advent of deep learning, there has been a shift from traditional humancrafted emotional features such as those extracted by low level descriptors (LLDs)  [50]  or openSMILE  [51] , to the features automatically learned by deep neural networks (DNN)  [52] . Many studies  [52, 53]  have shown that DNNs are capable of extracting hierarchical feature representations from expressive speech, which are more suitable for SER. Meanwhile, recent speech synthesis studies  [54] [55] [56]  also propose to leverage those deep emotional features to characterize different emotional styles over a continuum  [57] . These successful attempts have served as the source of motivation for this paper.   We propose to study a scenario for expressive voice conversion, where source and target speakers are expressing the same emotion in their own styles, that we refer to as speaker-dependent emotional styles. As emotional styles are hierarchical in nature, they are difficult to describe. We believe that deep emotional features bear a huge potential to describe both speaker-dependent and speaker-independent attributes for emotional styles. Therefore, we would like to investigate the use of deep emotional features for expressive voice conversion.\n\nWe first train a SER network and use the emotional style features before the last projection layer as illustrated in Fig.  4(a) . We then use t-SNE algorithm  [58]  to visualize the emotional style features of two female speakers (0016, 0018) and two male speakers (0013, 0020). As shown in Fig.  3 , we observe that the emotional style features form emotional groups for each speaker. These results suggest that these emotional style features characterize well the emotional states.\n\nWe further look into how emotional style features differ between speakers. We measure the similarity of the emotional style features between speaker pairs in terms of euclidean distance and root mean square error (RMSE). From Fig.  2 , we observe the euclidean distance and RMSE within a speaker are lower than those between two speakers, which indicates that emotional style features from SER carry speak-dependent information.\n\nThe above analysis shows that we may use emotional style features to encode both speaker-dependent and speakerindependent emotional styles in expressive voice conversion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Stage I: Emotional Style Descriptor Training",
      "text": "As emotional style presents both speaker-dependent and speaker-independent characteristics at the same time, it is insufficient for us to use a discrete representation to represent different emotional styles, such as one-hot emotion label  [56] . Therefore, we propose to use deep emotional features that learnt from a large emotional speech corpus to describe different emotional styles.\n\nWe propose to train an SER model to learn the emotional style features for different speakers with various emotions. The model architecture is the same as that in  [59] , as illustrated in Fig.  4(a) . The SER network consists of the following layers: 1) a three-dimensional (3-D) CNN layer; 2) a BLSTM layer; 3) an attention layer; and 4) a fully-connected (FC) layer. The Mel-spectrum input is first projected into a fixed size latent representation by the 3-D CNN, which preserves the useful emotional information while reducing the influence of emotional irrelevant elements. The next BLSTM and attention layer then summarizes the temporal input from the prior layer and produces an utterance-level feature E for emotion category prediction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stage Ii: Jes-Stargan Training",
      "text": "StarGAN has been widely used for many-to-many voice conversion and does not need parallel data. We propose a StarGAN-based architecture as shown in Fig.  4 (b), which consists of three modules: a generator G, a discriminator D, and a domain classifier C. During stage II, the proposed framework learns a feature mapping of speaker identity and speaker-dependent emotional style across different speakers.\n\nGiven the source acoustic feature sequence x, target speaker label c y , and target emotional style feature E y , the generator learns to translate the source acoustic feature sequence x to the target domain by conditioning on the emotional style feature E y and target speaker label c y . The converted acoustic feature ŷ can be represented as:\n\nwhere c y is a one-hot vector to represent each speaker. In this way, the generator jointly learns the speaker identity information and speaker-dependent emotional style information from the input features. The discriminator D is designed to judge whether the input is real or not, while the classifier C judges whether the input acoustic features belong to the target speaker. The training process of our proposed method is illustrated in Fig.  4(b) . The training losses for our proposed method are described as follows:\n\n1) Adversarial loss: An adversarial loss is applied to train G and D as follows:\n\nwhere E[•] represents the expectation operation, c x and E x represent the speaker label and the emotional style features of the source speaker respectively. During training, D tries to minimize L D adv and G tries to minimize L G adv . To G, a smaller value of the adversarial loss indicates a higher similarity between the converted speech and the target emotional speech in terms of both speaker similarity and emotional style.\n\n2) Domain classification loss: A domain classification loss is applied to train the C and G, which is defined as:\n\nwhere p C represents the output probability distribution from C. During training, C learns to classify a real acoustic feature sequence x to its corresponding speaker label c x by minimizing Equation  4 . G learns to generate acoustic feature sequence G(x, E y , c y ) with higher classification accuracy for the target domain c y by minimizing L G dom in Equation  5 . The additional input E y in G(x, E y , c y ) encourages G(x, E y , c y ) to carry speaker-dependent emotional style information, so that C can more easily classify G(x, E y , c y ) with high accuracy in target domain.\n\n3) Cycle-consistency loss: A cycle-consistency loss is proposed to guarantee the consistency of the contextual information between input and output while converting the speaker identity and emotional style. It is defined as:\n\nSince the emotional style of source speaker is different from that of target speaker, it is necessary to input E x as an additional condition for the G to make G(G(x, E y , c y ), E x , c x ) closer to the source acoustic features. 4) Identity mapping loss: An identity mapping loss is used to preserve the linguistic information between the same source and target speaker when inputting x, E x and c x for G. It is defined:\n\nBoth speaker identity c and E x are speaker-dependent features for the same speaker. L G id helps G to generate the acoustic feature sequences from the same speaker be consistent by considering these speaker-dependent features.\n\nThe full objective functions of our proposed method are given as follows:\n\nwhere λ dom ,λ cyc and λ id are trade-off factors to control the relevance of the domain classification loss, the cycle consistency loss and the identity mapping loss to the overall adversarial losses.\n\nAt training stage, we condition the generator on emotional style encoding derived from a pre-trained SER model from stage I. JES-StarGAN optimizes the distribution of the generated acoustic features to match that of the target features from emotional data. With the additional input emotional style features, JES-StarGAN learns to project both emotional style and speaker identity into the converted speech.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Stage Iii: Run-Time Conversion",
      "text": "During the run-time conversion, we can convert the acoustic feature sequence x of an input utterance: ŷ = G(x, E y , c y ). Since the real target emotion style features are not available from stage I, we calculate the mean of emotion style features collected from the corresponding reference utterances as E y , which can be defined as E y = mean(E r ye ), where y e represents target speaker with a certain emotion state.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We conduct objective and subjective evaluations to assess the performance of our proposed framework in terms of speaker identity and emotional style. We use a multi-speaker emotional speech dataset, ESD  [16] , to conduct all the experiments. ESD consists of multi-lingual and multi-speaker parallel emotional speech data with five emotions (neutral, happy, sad, angry and surprise), and has been used in emotional voice conversion  [56, 60]  and emotional text-to-speech  [61] .\n\nWe randomly choose three emotions (neutral, happy and sad) and four speakers (two male and two female) from ESD. For each speaker and each emotion, we use 300 utterances for training, 20 utterances for evaluation, and the rest 30 utterances are used as the reference set. As a comparative study, we choose StarGAN-VC  [39]  as the baseline.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "All the speech data is sampled at 16 kHz and encoded with 16 bits. We extract 36-dimensional Mel-cepstral coefficients (MCEPs), fundamental frequency (F0), and aperiodicity (APs) every 5 ms using WORLD vocoder  [62] . 36-dimensional MCEPs are used as spectral features, F0 is converted through the logarithm Gaussian (LG) normalized transformation  [63] , and APs are directly copied from the source without any modifications.\n\nWe first train the SER network with IEMOCAP dataset  [64]  and then fine-tune it with ESD. We follow the model architecture and training configuration in  [59]  for SER training. We obtain the 64-dimensional emotional style features as in Fig.  4 (a). We then merge the emotion style features together with 36-dimensional MCEPs using a full-connected layer, which is later used as the input to the generator.\n\nThe proposed JES-StarGAN has the following architecture: G consist of an encoder and a decoder. The encoder consists of 5 layers of CNN, and each is followed by a batch normalization layer and a gated linear unit. The output channel of the encoder is {64, 128, 256, 128, 10}. The decoder consists of 4 layers of CNN followed by a batch normalization layer and a gated linear unit, and a transposed convolution layer. Its output channel is {64, 128, 64, 32}. D consists of four CNN layers (each layer is followed by a batch normalization layer and a gated linear unit), a CNN layer, a sigmoid layer and product pooling layers. The output channel for D is {32, 32, 32, 32, 1}. C consists of a slice layer, four CNN layers (each layer is followed by a batch normalization layer and a gated linear unit), a CNN layer, a softmax layer and product pooling layers. The output channel of C is {8, 16, 32, 16}.\n\nDuring training, JES-StarGAN is trained using ADAM optimizer with a learning rate of 0.0001. The batch size is 4 and the training process takes 200k iterations. We set λ dom =2, λ cyc =10 and λ id = 5.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Objective Evaluation",
      "text": "We conduct objective evaluation to asses the performance of our proposed framework for neutral, happy and sad utterances. We calculate Mel-cepstral distortion (MCD)  [65]  to measure the spectral distortion between the converted and target speech. A smaller value of MCD indicates a smaller spectral distortion and a better conversion performance. We first report the MCD results for intra-gender combinations in Table  1 : 1) from male to male (denoted as M-M ); 2) from female to female (denoted as F-F ), and then report the MCD for inter-gender in Table  2 : 3) from male to female (denoted as M-F ) and 4) from female to male (denoted as F-M ). From these results, we observe that the proposed method JES-StarGAN consistently outperforms the baseline StarGAN-VC in both inter-gender and intra-gender conversion, which indicates the effectiveness of our proposed framework.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We conduct three listening tests to assess speech quality, speaker similarity and emotional style similarity. 13 subjects participate in all listening tests, in which each listens to 90 converted utterances in total.\n\nWe first report the mean opinion score (MOS) results to evaluate the speech quality. 15 sentences are randomly selected from the evaluation set. A higher MOS score indicates better speech quality. As shown in Table  3 , JES-StarGAN outperforms the StarGAN-VC baseline.\n\nWe further conduct two ABX preference tests to evaluate speaker similarity and emotional style similarity respectively. First, we report the results for ABX tests for speaker similarity, where all the subjects are asked to choose the one which sounds closer to the the reference target speech samples in terms of the speaker similarity. As shown in Fig.  5 , we observe that the proposed JES-StarGAN significantly outperforms the baseline StarGAN-VC in speaker similarity. Benefiting from the joint transfer of speaker identity and emotional style, the proposed JES-StarGAN has a much better performance on the speaker identity conversion than the baseline, which further validates our idea on expressive voice conversion.\n\nWe then report the ABX test results for emotional style similarity, where all the subjects are asked to choose the one which sounds closer to the reference target speech samples in terms of the emotional style. As shown in Fig.  6 , the proposed JES-StarGAN still outperforms the baseline StarGAN-VC in terms of emotional style similarity. It shows the effectiveness  of the deep emotional features on emotional style transfer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "This paper marks as the first study for expressive voice conversion. We formulate the problem of expressive voice conversion and propose a novel solution based on StarGAN to jointly transfer speaker identity and speaker-dependent emotional style without the need for parallel data. We propose to use deep emotional features from SER to characterize different emotional styles in a continuous space. By conditioning the generator with deep emotional features, the framework jointly learn a mapping of speaker-dependent features across different speakers. Experiments show that the proposed JES-StarGAN consistently outperforms the baseline.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Acknowledgment",
      "text": "The research is funded by SUTD Start-up Grant Artificial Intelligence for Human Voice Conversion (SRG ISTD 2020 158) and SUTD AI Grant -Thrust 2 Discovery by AI (SG-PAIRS1821).",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of expressive voice conversion which",
      "page": 1
    },
    {
      "caption": "Figure 2: RMSE and Euclidean distance calculation results between emotional style features for 50 utterances from different",
      "page": 3
    },
    {
      "caption": "Figure 3: T-SNE plot of emotional style features for 20 utter-",
      "page": 3
    },
    {
      "caption": "Figure 4: Schematic diagram of the proposed framework JES-StarGAN. Blue boxes represent the modules involved in the training",
      "page": 4
    },
    {
      "caption": "Figure 5: ABX preference results for speaker similarity with",
      "page": 6
    },
    {
      "caption": "Figure 6: ABX preference results for emotional style similarity",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 3: , JES-StarGAN 6. CONCLUSIONS",
      "data": [
        {
          "StarGAN-VC": "6.187",
          "JES-StarGAN": "5.419"
        },
        {
          "StarGAN-VC": "5.917",
          "JES-StarGAN": "5.928"
        },
        {
          "StarGAN-VC": "6.656",
          "JES-StarGAN": "6.534"
        },
        {
          "StarGAN-VC": "6.190",
          "JES-StarGAN": "6.105"
        },
        {
          "StarGAN-VC": "6.739",
          "JES-StarGAN": "6.333"
        },
        {
          "StarGAN-VC": "7.240",
          "JES-StarGAN": "7.113"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: , JES-StarGAN 6. CONCLUSIONS",
      "data": [
        {
          "StarGAN-VC": "7.646",
          "JES-StarGAN": "7.444"
        },
        {
          "StarGAN-VC": "7.655",
          "JES-StarGAN": "7.023"
        },
        {
          "StarGAN-VC": "7.051",
          "JES-StarGAN": "6.698"
        },
        {
          "StarGAN-VC": "7.479",
          "JES-StarGAN": "7.185"
        },
        {
          "StarGAN-VC": "8.227",
          "JES-StarGAN": "7.860"
        },
        {
          "StarGAN-VC": "8.222",
          "JES-StarGAN": "7.906"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "Berrak Sisman",
        "Junichi Yamagishi",
        "Simon King",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Cross-lingual voice conversion with bilingual phonetic posteriorgram and average modeling",
      "authors": [
        "Yi Zhou",
        "Xiaohai Tian",
        "Haihua Xu",
        "Rohan Kumar Das",
        "Haizhou Li"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "On the study of generative adversarial networks for cross-lingual voice conversion",
      "authors": [
        "Berrak Sisman",
        "Mingyang Zhang",
        "Minghui Dong",
        "Haizhou Li"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "5",
      "title": "Spectrum and prosody conversion for cross-lingual voice conversion with cyclegan",
      "authors": [
        "Zongyang Du",
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "6",
      "title": "Spoofing and countermeasures for speaker verification: A survey",
      "authors": [
        "Zhizheng Wu",
        "Nicholas Evans",
        "Tomi Kinnunen",
        "Junichi Yamagishi",
        "Federico Alegre",
        "Haizhou Li"
      ],
      "year": "2015",
      "venue": "speech communication"
    },
    {
      "citation_id": "7",
      "title": "Vocal cues in emotion encoding and decoding",
      "authors": [
        "Klaus Scherer",
        "Rainer Banse",
        "Harald Wallbott"
      ],
      "year": "1991",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "8",
      "title": "Emotional style: The cultural ordering of emotions",
      "authors": [
        "Dewight R Middleton"
      ],
      "year": "1989",
      "venue": "Ethos"
    },
    {
      "citation_id": "9",
      "title": "Emotion and personality",
      "authors": [
        "Magda B Arnold"
      ],
      "year": "1960",
      "venue": "Emotion and personality"
    },
    {
      "citation_id": "10",
      "title": "Emotional style and susceptibility to the common cold",
      "authors": [
        "Sheldon Cohen",
        "William Doyle",
        "Ronald Turner",
        "Cuneyt Alper",
        "David Skoner"
      ],
      "year": "2003",
      "venue": "Psychosomatic medicine"
    },
    {
      "citation_id": "11",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "12",
      "title": "Pragmatics and intonation",
      "authors": [
        "Julia Hirschberg"
      ],
      "year": "2004",
      "venue": "The handbook of pragmatics"
    },
    {
      "citation_id": "13",
      "title": "Comparing emotions using acoustics and human perceptual dimensions",
      "authors": [
        "Keshi Dai",
        "Harriet Fell",
        "Joel Macauslan"
      ],
      "year": "2009",
      "venue": "CHI'09 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "14",
      "title": "Speaker-independent emotion recognition exploiting a psychologically-inspired binary cascade classification schema",
      "authors": [
        "Margarita Kotti",
        "Fabio Paternò"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "15",
      "title": "Converting anyone's emotion: Towards speaker-independent emotional voice conversion",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Mingyang Zhang",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "16",
      "title": "Sequence-tosequence modelling of f0 for speech emotion conversion",
      "authors": [
        "Carl Robinson",
        "Nicolas Obin",
        "Axel Roebel"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Emotional voice conversion: Theory, databases and esd",
      "arxiv": "arXiv:2105.14762"
    },
    {
      "citation_id": "18",
      "title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2020",
      "venue": "Proc. Odyssey 2020 The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "19",
      "title": "Nonparallel emotional speech conversion",
      "authors": [
        "Jian Gao",
        "Deep Chakraborty",
        "Hamidou Tembine",
        "Olaitan Olaleye"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "21",
      "title": "Durationembedded bi-hmm for expressive voice conversion",
      "authors": [
        "Chi-Chun Hsia",
        "Chung-Hsien Wu",
        "Te-Hsien Liu"
      ],
      "year": "2005",
      "venue": "Durationembedded bi-hmm for expressive voice conversion"
    },
    {
      "citation_id": "22",
      "title": "Conversion function clustering and selection for expressive voice conversion",
      "authors": [
        "Chi-Chun Hsia",
        "Chung-Hsien Wu",
        "Jian-Qi Wu"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07. IEEE"
    },
    {
      "citation_id": "23",
      "title": "Speech Enhancement, Modeling and Recognition-Algorithms and Applications",
      "authors": [
        "Ramakrishnan"
      ],
      "year": "2012",
      "venue": "Speech Enhancement, Modeling and Recognition-Algorithms and Applications"
    },
    {
      "citation_id": "24",
      "title": "Cross-language voice conversion evaluation using bilingual databases",
      "authors": [
        "Mikiko Mashimo",
        "Tomoki Toda",
        "Hiromichi Kawanami",
        "Kiyohiro Shikano",
        "Nick Campbell"
      ],
      "year": "2002",
      "venue": "Cross-language voice conversion evaluation using bilingual databases"
    },
    {
      "citation_id": "25",
      "title": "Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory",
      "authors": [
        "Tomoki Toda",
        "Alan Black",
        "Keiichi Tokuda"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Incorporating global variance in the training phase of gmm-based voice conversion",
      "authors": [
        "Hsin-Te Hwang",
        "Yu Tsao",
        "Hsin-Min Wang",
        "Yih-Ru Wang",
        "Sin-Horng Chen"
      ],
      "year": "2013",
      "venue": "2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "27",
      "title": "A fast learning algorithm for deep belief nets",
      "authors": [
        "Geoffrey Hinton",
        "Simon Osindero",
        "Yee-Whye Teh"
      ],
      "year": "2006",
      "venue": "Neural computation"
    },
    {
      "citation_id": "28",
      "title": "High-order sequence modeling using speaker-dependent recurrent temporal restricted boltzmann machines for voice conversion",
      "authors": [
        "Toru Nakashika",
        "Tetsuya Takiguchi",
        "Yasuo Ariki"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "29",
      "title": "Voice conversion from non-parallel corpora using variational auto-encoder",
      "authors": [
        "Chin-Cheng Hsu",
        "Hsin-Te Hwang",
        "Yi-Chiao Wu",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "30",
      "title": "Vectorquantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge",
      "authors": [
        "Leanne Benjamin Van Niekerk",
        "Herman Nortje",
        "Kamper"
      ],
      "year": "2020",
      "venue": "Vectorquantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge",
      "arxiv": "arXiv:2005.09409"
    },
    {
      "citation_id": "31",
      "title": "F0-consistent many-to-many non-parallel voice conversion via conditional autoencoder",
      "authors": [
        "Zeyu Kaizhi Qian",
        "Mark Jin",
        "Gautham Hasegawa-Johnson",
        "Mysore"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Voice conversion based on cross-domain features using variational auto encoders",
      "authors": [
        "Wen-Chin Huang",
        "Hsin-Te Hwang",
        "Yu-Huai Peng",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "year": "2018",
      "venue": "2018 11th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
      "authors": [
        "Chin-Cheng Hsu",
        "Hsin-Te Hwang",
        "Yi-Chiao Wu",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "year": "2017",
      "venue": "Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks",
      "arxiv": "arXiv:1704.00849"
    },
    {
      "citation_id": "34",
      "title": "Cyclegan voice conversion of spectral envelopes using adversarial weights",
      "authors": [
        "Rafael Ferro",
        "Nicolas Obin",
        "Axel Roebel"
      ],
      "year": "2021",
      "venue": "2020 28th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "35",
      "title": "Adaptive wavenet vocoder for residual compensation in gan-based voice conversion",
      "authors": [
        "Berrak Sisman",
        "Mingyang Zhang",
        "Sakriani Sakti",
        "Haizhou Li",
        "Satoshi Nakamura"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "36",
      "title": "Cyclegan-vc: Non-parallel voice conversion using cycle-consistent adversarial networks",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka"
      ],
      "year": "2018",
      "venue": "2018 26th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "37",
      "title": "Cyclegan-vc2: Improved cyclegan-based non-parallel voice conversion",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Nobukatsu Hojo"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Cyclegan-vc3: Examining and improving cyclegan-vcs for mel-spectrogram conversion",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Nobukatsu Hojo"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "39",
      "title": "Parallel-data-free voice conversion using cycle-consistent adversarial networks",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka"
      ],
      "year": "2017",
      "venue": "ArXiv"
    },
    {
      "citation_id": "40",
      "title": "Stargan-vc: Non-parallel many-to-many voice conversion using star generative adversarial networks",
      "authors": [
        "Hirokazu Kameoka",
        "Takuhiro Kaneko",
        "Kou Tanaka",
        "Nobukatsu Hojo"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "41",
      "title": "Stargan-vc2: Rethinking conditional methods for stargan-based voice conversion",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Nobukatsu Hojo"
      ],
      "year": "2019",
      "venue": "Stargan-vc2: Rethinking conditional methods for stargan-based voice conversion",
      "arxiv": "arXiv:1907.12279"
    },
    {
      "citation_id": "42",
      "title": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92)",
      "authors": [
        "Junichi Yamagishi",
        "Christophe Veaux",
        "Kirsten Macdonald"
      ],
      "year": "2019",
      "venue": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92)"
    },
    {
      "citation_id": "43",
      "title": "The cmu arctic speech databases",
      "authors": [
        "John Kominek",
        "Alan Black"
      ],
      "year": "2004",
      "venue": "Fifth ISCA workshop on speech synthesis"
    },
    {
      "citation_id": "44",
      "title": "The voice conversion challenge 2016",
      "authors": [
        "Tomoki Toda",
        "Ling-Hui Chen",
        "Daisuke Saito",
        "Fernando Villavicencio",
        "Mirjam Wester",
        "Zhizheng Wu",
        "Junichi Yamagishi"
      ],
      "year": "2016",
      "venue": "The voice conversion challenge 2016"
    },
    {
      "citation_id": "45",
      "title": "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods",
      "authors": [
        "Jaime Lorenzo-Trueba",
        "Junichi Yamagishi",
        "Tomoki Toda",
        "Daisuke Saito",
        "Fernando Villavicencio",
        "Tomi Kinnunen",
        "Zhenhua Ling"
      ],
      "year": "2018",
      "venue": "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods",
      "arxiv": "arXiv:1804.04262"
    },
    {
      "citation_id": "46",
      "title": "Voice conversion challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion",
      "authors": [
        "Yi Zhao",
        "Wen-Chin Huang",
        "Xiaohai Tian",
        "Junichi Yamagishi",
        "Rohan Kumar Das",
        "Tomi Kinnunen",
        "Zhenhua Ling",
        "Tomoki Toda"
      ],
      "year": "2020",
      "venue": "Voice conversion challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion",
      "arxiv": "arXiv:2008.12527"
    },
    {
      "citation_id": "47",
      "title": "Stargan: Unified generative adversarial networks for multi-domain image-to-image translation",
      "authors": [
        "Yunjey Choi",
        "Minje Choi",
        "Munyoung Kim",
        "Jung-Woo Ha",
        "Sunghun Kim",
        "Jaegul Choo"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "48",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "49",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of end-to-end emotion recognition",
      "authors": [
        "Georgios Rizos",
        "Alice Baird",
        "Max Elliott",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "51",
      "title": "Low-level fusion of audio and video feature for multimodal emotion recognition",
      "authors": [
        "Matthias Wimmer",
        "Björn Schuller",
        "Dejan Arsic",
        "Bernd Radig",
        "Gerhard Rigoll"
      ],
      "year": "2008",
      "venue": "Proc. 3rd Int. Conf. on Computer Vision Theory and Applications VISAPP"
    },
    {
      "citation_id": "52",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "A review on five recent and near-future developments in computational processing of emotion in the human voice",
      "authors": [
        "M Dagmar",
        "Björn Schuller",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "54",
      "title": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Junaid Qadir",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Deep representation learning in speech processing: Challenges, recent advances, and future trends",
      "arxiv": "arXiv:2001.00378"
    },
    {
      "citation_id": "55",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "Se-Yun Um",
        "Sangshin Oh",
        "Kyungguen Byun",
        "Inseon Jang",
        "Chunghyun Ahn",
        "Hong-Goo Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "Expressive tts training with frame and style reconstruction loss",
      "authors": [
        "Rui Liu",
        "Berrak Sisman",
        "Guang Lai Gao",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "57",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Dnn-based emotion recognition based on bottleneck acoustic features and lexical features",
      "authors": [
        "Eesung Kim",
        "Jong Shin"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "59",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "60",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "61",
      "title": "Limited data emotional voice conversion leveraging text-to-speech: Two-stage sequence-tosequence training",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Limited data emotional voice conversion leveraging text-to-speech: Two-stage sequence-tosequence training"
    },
    {
      "citation_id": "62",
      "title": "Reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability",
      "authors": [
        "Rui Liu",
        "Berrak Sisman",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability",
      "arxiv": "arXiv:2104.01408"
    },
    {
      "citation_id": "63",
      "title": "World: a vocoder-based high-quality speech synthesis system for real-time applications",
      "authors": [
        "Masanori Morise",
        "Fumiya Yokomori",
        "Kenji Ozawa"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "64",
      "title": "High quality voice conversion through phoneme-based linear mapping functions with straight for mandarin",
      "authors": [
        "Kun Liu",
        "Jianping Zhang",
        "Yonghong Yan"
      ],
      "year": "2007",
      "venue": "Fourth International Conference on Fuzzy Systems and Knowledge Discovery"
    },
    {
      "citation_id": "65",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "66",
      "title": "Mel-cepstral distance measure for objective speech quality assessment",
      "authors": [
        "Robert Kubichek"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing"
    }
  ]
}