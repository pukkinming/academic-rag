{
  "paper_id": "2308.12648v1",
  "title": "From Chatter To Matter: Addressing Critical Steps Of Emotion Recognition Learning In Task-Oriented Dialogue",
  "published": "2023-08-24T08:46:30Z",
  "authors": [
    "Shutong Feng",
    "Nurul Lubis",
    "Benjamin Ruppik",
    "Christian Geishauser",
    "Michael Heck",
    "Hsien-chin Lin",
    "Carel van Niekerk",
    "Renato Vukovic",
    "Milica Gašić"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversations (ERC) is a crucial task for building human-like conversational agents. While substantial efforts have been devoted to ERC for chit-chat dialogues, the task-oriented counterpart is largely left unattended. Directly applying chit-chat ERC models to task-oriented dialogues (ToDs) results in suboptimal performance as these models overlook key features such as the correlation between emotions and task completion in ToDs. In this paper, we propose a framework that turns a chit-chat ERC model into a task-oriented one, addressing three critical aspects: data, features and objective. First, we devise two ways of augmenting rare emotions to improve ERC performance. Second, we use dialogue states as auxiliary features to incorporate key information from the goal of the user. Lastly, we leverage a multi-aspect emotion definition in ToDs to devise a multi-task learning objective and a novel emotion-distance weighted loss function. Our framework yields significant improvements for a range of chitchat ERC models on EmoWOZ, a large-scale dataset for user emotion in ToDs. We further investigate the generalisability of the best resulting model to predict user satisfaction in different ToD datasets. A comparison with supervised baselines shows a strong zero-shot capability, highlighting the potential usage of our framework in wider scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversations (ERC) is a crucial task in conversational artificial intelligence research because it lays the foundation for affective abilities in computers such as empathetic response generation  (Picard, 1997) . Over years, it has shown values in downstream applications such as opinion mining  (Colneric and Demšar, 2020)  and humanlike dialogue modelling  (Zhou et al., 2018) .\n\nDialogue systems can be broadly categorised into two categories: (1) chit-chat or open-domain (a) Chit-chat dialogue from  Li et al. (2017)  (b) Task-oriented dialogue from  Budzianowski et al. (2018)  Figure  1 : Comparison of dialogues about holiday in chit-chat dialogues and task-oriented dialogues.\n\nsystems and (2) task-oriented dialogue (ToD) systems. Chit-chat systems are set up to mimic human behaviours in a conversation  (Jurafsky and Martin, 2009) . There are no particular goals associated with the dialogue and the system aims to keep the user engaged with natural and coherent responses. On the other hand, ToD systems are concerned with fulfilling user goals, such as information retrieval for hotel booking  (Young, 2002) .\n\nRecently, the difference between chit-chat and ToD systems have been blurred by the utilisation of pre-trained language models as back-bone to both types of systems. However, emotions in ToDs and chit-chat dialogues play different roles and are therefore expressed differently  (Feng et al., 2022) . This highlights the need for dedicated emotion modelling methods for each system.\n\nAs illustrated in Figure  1 , in chit-chat dialogues, speakers make use of emotions to facilitate communication by, for example, raising empathy as a result of emotion-eliciting situations or topics. On the other hand, emotions in ToDs are centred around the user's goal, and therefore emotion cues lie in both the user's wording and the task performance.\n\nWhile many large-scale corpora for emotions in chit-chat dialogues exist  (Busso et al., 2008; McKeown et al., 2012; Lubis et al., 2015; Li et al., 2017; Zahiri and Choi, 2018) , there are considerably fewer resources for emotions in ToDs. EmoWOZ, which evolved from MultiWOZ, a widely used ToD dataset, is one notable exception  (Feng et al., 2022) . It contains a novel emotion description that is designed for ToDs and inspired by the Ortony-Clore-Collins (OCC) model  (Ortony et al., 1988) . Emotion is described in terms of three aspects: valenced (positive or negative) reactions towards elicitors (operator, user, or event) in a certain conduct (polite or impolite). However, due to the nature of ToDs, the occurrence of some emotions (e.g. users expressing feelings about their situations) are very rare, leading to a class imbalance in the corpus.\n\nSimilarly, advancements on the ERC task are mainly focused on chit-chat dialogues, involving an array of diverse factors from speaker personality  (Majumder et al., 2019)  to commonsense knowledge  (Ghosal et al., 2020) . Nevertheless, since these models are designed for chit-chat dialogues, they overlook how emotions are triggered and expressed with respect to goal completion in task-oriented context. The work of  Devillers et al. (2003)  is among one of the earliest and very few to address emotion detection in ToDs but uses generic unigram models instead of dedicated approaches.\n\nIn this work, we tackle critical steps of ERC in ToDs from three angles: the data, the features, and the learning objective. In particular, Data: we address the poor ERC performance of particularly rare emotions in ToDs via two strategies of data augmentation (DA),\n\nFeatures: we leverage dialogue state information and sentiment-aware textual features,\n\nObjective: we exploit the three aspects of emotions, namely valence, elicitor, and conduct, in two ways: as a multi-task learning (MTL) objective and to define a novel emotion-distanceweighted loss (EmoDistLoss).\n\nTo the best of our knowledge, our work is the first to provide dedicated methods for emotion recognition in ToDs. Our experiments and analyses show that our framework leads to significant improvements for a range of chit-chat ERC models when evaluated on EmoWOZ.\n\nWe further investigate the generalisability of the best resulting model to predict user satisfaction in various ToD datasets under zero-shot transfer. Our model achieves comparable results as supervised baselines, demonstrating strong zero-shot capability and potential to be applied in wider scenarios.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Erc Datasets",
      "text": "Early work on ERC relied on small scale datasets  (Busso et al., 2008; McKeown et al., 2012; Lubis et al., 2015) . More recently, a few large-scale datasets have been made available to the research community. They contain dialogues from emotionrich and spontaneous scenarios such as daily communications  (Li et al., 2017)  and situation comedies  (Zahiri and Choi, 2018) .\n\nFor ToDs, the majority of available datasets address only one particular aspect of emotions such as sentiment polarity  (Saha et al., 2020; Shi and Yu, 2018) , user satisfaction  (Schmitt et al., 2012; Sun et al., 2021) , and politeness  (Hu et al., 2022; Mishra et al., 2023) . For more fine-grained emotions,  Singh et al. (2022)  constructed EmoInHindi for emotion category and intensity recognition in mental health and legal counselling dialogues in Hindi, and  Feng et al. (2022)  released EmoWOZ, which concerns user emotions in human-human and human-machine in information-seeking dialogues. Among these datasets, EmoWOZ has the largest scale, accompanied with a label set tailored to the task-oriented scenario.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation (Da)",
      "text": "DA is an effective approach to improve model performance by improving data diversity without explicitly collecting more data. While textual DA can be performed in the feature space via interpolation and sampling  (Kumar et al., 2019) , it is commonly performed in the data space for controllability. Rule-based methods involve operations such as insertion and substitution  (Wei and Zou, 2019) . While they are easy to implement, the diversity in augmented samples depends on the complexity of the rules. On the contrary, model-based methods are more scalable. These typically include the use of language models  (Jiao et al., 2020) , translation models  (Xie et al., 2020a) , and paraphrasing methods  (Hou et al., 2018) .\n\nAdditional training samples can also be obtained from unlabelled data via weak supervision  (Ratner et al., 2017) . To generate the automatic labels, a single model or an ensemble of models may be used. This method can be interpreted as selfaugmentation  (Xu et al., 2022) , self-training  (Xie et al., 2020b) , or distillation  (Radosavovic et al., 2017) .\n\nDA has also been also deployed in ToD modelling.  Hou et al. (2018)  generated samples by paraphrasing delexicalised utterances.  Gritta et al. (2021)  conceptualised ToDs into transitional graphs and generate new dialogue paths by sampling.  Heck et al. (2022)  proposed a weak supervision framework to address the lack of fine-grained span labels for dialogue state tracking. DA for emotions in ToDs requires careful considerations to avoid emotion mismatch and is not yet explored.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Erc Models And Features",
      "text": "Text-based ERC is in essence a text classification problem with an emphasis on contextual modelling.  Poria et al. (2017)  proposed a recurrent neural network (RNN) for multimodal ERC. The follow-up work of  Majumder et al. (2019)  considered speakerspecific context. ERC performance has been continuously improved by techniques such as incorporating external knowledge  (Ghosal et al., 2020)  and contrastive learning  (Song et al., 2022) .\n\nSentiment-aware Embeddings Word-vector embeddings tailored for a particular natural language processing task can effectively improve the performance for that task  (Naseem et al., 2021) . In a similar vein,  Tang et al. (2014)  incorporated sentiment classification objectives in the training of the word embedding model of  Collobert and Weston (2008)  specifically for sentiment analysis.  Yu et al. (2017)  refined static word embeddings with the aid of a sentiment lexicon. Later, many sentimentaware variants of pre-trained language models were obtained by incorporating sentiment-related objectives in training  (Xu et al., 2019; Yin et al., 2020; Zhou et al., 2020) . They successively achieved state-of-the-art performance in sentiment analysis tasks among language representation models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Learning Objectives For Erc Models",
      "text": "ERC is often considered a single-label sequential classification problem. Using softmax crossentropy loss has been the norm in the training of deep learning ERC models for categorical emotions  (Poria et al., 2017; Zhong et al., 2019; Ghosal et al., 2020; Kim and Vossen, 2021)  or quantised emotion dimensions  (Cerisara et al., 2018; Wang et al., 2020) . However, this simplistic cross-entropy loss ignores the inter-class relations and output probabilities on incorrect classes.  Chen et al. (2019)  proposed to suppress the output probabilities of incorrect classes equally while minimising the standard cross-entropy loss.  Hou et al. (2016)  proposed squared earth mover's distance to penalise the misclassifications according to a ground distance matrix that quantifies the dissimilarities between classes for image age estimation and aesthetics estimation.\n\nAlthough highly suitable for emotions, learning from misclassifications is rarely considered because the distance between emotion classes is hard to quantify. Therefore, we propose to leverage the structured label definition of EmoWOZ to model inter-class similarity.\n\nMulti-task Learning (MTL) is a technique for learning tasks in parallel using a shared representation. It aims to improve generalisation by using the information in training signals of related tasks as an inductive bias  (Caruana, 1997) . In emotion recognition, auxiliary tasks include topic classification  (Wang et al., 2020)  and personality traits  (Li et al., 2021) . When co-labels are not available, it is also possible to leverage aspects of emotion for additional labels such as valence-arousal  (Kim et al., 2017) . In this work, we exploit the valenceelicitor-conduct labels in EmoWOZ for MTL.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "User Emotion Recognition",
      "text": "We formulate the task as recognising one emotion class e t from a set of n discrete emotions E = {e 1 , e 2 , ..., e n } in the user turn u t , given a dialogue history H t = [u t , s t-1 , u t-1 , ..., s 1 , u 1 ], where s denotes system turns and u denotes user turns. Unlike existing chit-chat ERC models, which are often built for static analysis on the dialogue as a whole, real-time ERC in ToDs does not consider future utterances in dialogue.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "User Satisfaction Prediction",
      "text": "User satisfaction prediction aims to predict one satisfaction level c t from a set of m discrete levels C = {c 1 , c 2 , ..., c m } in the user turn u t , given all previous turns P t = [s t-1 , H t-1 ]. This task differs from ERC in that the user turn u t is not available as a part of model input. Since user satisfaction is highly correlated with the valence aspect in user emotion, this task can also be viewed as user emotion prediction. This is an important task in building ToD systems and has been used for user simulation and system evaluation  (Sun et al., 2021) .\n\n4 Emotion Recogniser for Task-oriented Dialogues (ERToD)\n\nIn this section, we propose our ERToD framework that adapt chit-chat ERC models to the taskoriented domain, as illustrated in Figure  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Augmentation",
      "text": "Unlike emotions in chit-chat dialogues, resources for emotions in ToDs are very limited. In addition, the data scarcity not only lies in the lack of linguistic diversity but also in the limited domains and actions in which emotions are expressed.\n\nIn ToDs, user's emotional expressions have different degrees of connection to the dialogue task. For example, a user can express dissatisfaction towards the system by pointing out the system's mistake. In such a case, simply replacing or paraphrasing the user's utterance based on emotion can potentially break the consistency of the task flow in the context. Such emotions are context-dependent.\n\nOn the other hand, context-independent emotions are expressed without any connection to the user goal, such is the case with abusive utterances. Due to the lack of connection, a simple replacement with a different abusive sentence can fit into the context well without impairing the consistency of task flow in the dialogue.\n\nTo obtain augmented samples with meaningful and coherent context, we adopt two different strategies of DA according to the degree of context dependency of emotional expressions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Context-Independent Emotions",
      "text": "To augment samples for a target emotion e, we select a user utterance u ′ with the equivalent label from other dialogue datasets. We then use it to replace the user utterance u t having label e in the training data while keeping the original context",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Context-Dependent Emotions",
      "text": "We first sample a pool of unlabelled candidate dialogues\n\nfrom other ToD datasets. We train a classifier with an uncertainty estimator to identify the emotion label e t of the user utterance u t and its confidence in each candidate:\n\nThe candidate is selected for emotion e t only if conf t (e) is above a confidence threshold θ.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Task Information Encoder",
      "text": "We use a dialogue state tracker (DST) to determine the status of goal completion at each turn. In ToDs, the dialogue state describes the system's understanding of the the user's goal up to that point in the dialogue  (Young et al., 2010) . It encodes dialogue progress in an abstractive manner.\n\nHere as a proof of concept, we use an ontologydependent DST, which means the concepts that the system can talk about are pre-determined. While we can eliminate the ontology dependency by, for example, using an ontology-independent DST and extracting task features from dialogue state description in natural language, this goes beyond the scope of this work. The DST takes the dialogue history to determine SemDS t , the current dialogue state in semantic form. It is stored as a dictionary that records slots and filled values. SemDS t is then converted into a vector of 0/1's, indicating whether a particular slot has been filled.\n\nTo account for the change of dialogue state, which depicts how the system performs locally, we concatenate dialogue states of three consecutive turns to obtain a contextual dialogue state vector.\n\nV t≤0 are zero vectors, representing the state before the dialogue starts. V t is then fed into a trainable fully connected (FC) layer.\n\nFeature Fusion for Emotion Classification For a chit-chat ERC model with an arbitrary utterance encoder, R t = Encoder(H t ), i.e. R t is the encoded representation of the dialogue history H t . The utterance encoder is replaced with a sentiment-aware encoder in our framework (see Figure  2 ).\n\nThe utterance and the task information encodings are fused via concatenation and fed into the emotion classifier. The output probability of all emotion classes in utterance u t is given by:\n\n(5)",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Learning Objectives",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion-Distance Weighted Loss",
      "text": "Emotion classification is a very challenging task due to the subjectivity in the perception of emotion. Since some emotions are more similar to each other than others, it may be advantageous to distinguish marginally wrong recognitions (satisfied vs excited) from extremely wrong ones (satisfied vs dissatisfied). Furthermore, different misclassifications can elicit different user reactions to the dialogue agent. For example, perceiving satisfaction when the user is neutral may or may not annoy the user, but accusing the user of abusive behavior by mistake is a serious offense to the user. Therefore, it is intuitive to penalise misclassifications according to (1) the distance from the label and (2) output probabilities on incorrect labels.\n\nDefining the Emotion Distance Since emotion labels in EmoWOZ are defined in three aspects, we can define the distance between emotion labels in terms of their distance on each aspect. A matrix D is defined where each element D(i, j) is a vector containing the distance between emotion label i and j in each of three aspects (valence, elicitor, and conduct). The matrix D is symmetric with vector-valued entries.\n\nThe final distance is obtained by the sum of the distance in each aspect, followed by an addition of 1 and smoothing with the log operator. The addition of 1 ensures that the log distance is still 0 for identical labels.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Considering Misclassification Probabilities",
      "text": "For each sample including the dialogue history H t , we look at the softmax output from the model.\n\nWe aim to minimise the probability of each misclassification p t (e = e i ) where e i ̸ = label t . This is done by maximising 1 -p t (e = e i ), the probability of the utterance not being wrongly recognised as e i . We then calculate the log of this probability so that in the case of a perfectly correct recognition, the penalty from misclassification will be 0.\n\nObtaining Weights for Misclassifications We obtain the relevant row in matrix D that contains the distance between each emotion and the groundtruth label j of utterance u t , followed by a normalisation to obtain a vector w t,j of normalised emotion-distance weights for all emotions.\n\no t,j = onehot(label t = j) (10)\n\nw t,j = D(:, j)/sum( D(:, j))\n\nEmoDistLoss The final loss, which we name EmoDistLoss, is calculated from the negative weighted sum of log terms from Equation  9 . Since the distance, hence the weight, between identical labels is 0, this calculation does not involve the output probability of the correct label.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Mtl Via Emotional Aspects",
      "text": "In addition to the emotion classification head, we have a classification head for each emotion aspect from the label definition, namely the valence, the elicitor, and the conduct.\n\nThe overall classification loss L is a weighted sum of the loss from softmax outputs of four classification heads L emo , L val , L eli , L con with a hyperparameter α.\n\n5 User Emotion Recognition in ToDs with ERToD Augmenting Abusive Utterances The user sometimes becomes abusive towards the system. While this correlates with failure to satisfy the user goal, exact abusive expressions uttered by the user are usually independent of the context. Therefore, we apply our DA method for context-independent emotions for Abusive. We utilise ConvAbuse, a dataset for nuanced abusive behaviours in chit-chat conversations (Cercas  Curry et al., 2021) , for more diverse abusive expressions. In ConvAbuse, user utterances are labelled with type, target, strength, and directiveness. We filter for abuses on the system's intellectuality (labelled as type=intellectual and target=system) to better suit ToD context. We combine each selected utterance with the context of a random abusive utterance in EmoWOZ, resulting in 273 augmented samples.\n\nAugmenting Fearful, Apologetic, and Excited Utterances Expressions of these emotions usually contain task information. Fearful and Excited usually co-occur with a description of the situation that prompts the user to interact with the system. Apologetic is frequently associated with a correction of search criteria. There is a strong connection between these emotion expressions and the progression of the task in the dialogue history. Therefore, we apply our DA method for these contextdependent emotions. We look for samples with desired emotions from other ToD datasets using automatic labels. We train a ContextBERT on EmoWOZ (see Section 5.1.2) with a 30% dropout on the BERT output. We train the model with 10 different seeds and run inferences on the training set of existing ToD datasets: Schema-Guided Dialogue (SGD,  Rastogi et al. 2019 ), Taskmaster-1 (TM-1), and Taskmaster-2 (TM-2)  (Byrne et al., 2019) . In addition, we filter for common domains of EmoWOZ: Hotels, RideSharing, Travel, Restaurants in SGD, RestaurantTable, PizzaOrdering, CoffeeOrdering, UberLyft in TM-1, and Ho-telSearch, Restaurants, FoodOrdering in TM-2.\n\nThe classification confidence is measured by votes from 10 models. We use a confidence threshold of 0.7 and cap the number of augmented samples at 1000 for each emotion, resulting in 268 fearful, 872 apologetic, and 1000 excited samples.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "We implement ERToD to a range of ERC models that have been used to benchmark EmoWOZ, as listed in Table  2 . ContextBERT  (Feng et al., 2022)  and EmoBERTa  (Kim and Vossen, 2021)  are simple yet robust transformer-based ERC models, and they have similar spirits except that they respectively use BERT  (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019)     Cun et al. 2015) . We further replace the utterance encoders of chit-chat ERC models with SentiX, a sentiment-adapted BERT  (Zhou et al., 2020) .\n\nWe use our proposed EmoDistLoss for the emotion classification head and cross-entropy loss for MTL heads (valence, elicitor, and conduct). Since the elicitor of Neutral emotion is not distinguishable and therefore not explicitly defined in EmoWOZ, we mark the elicitor of Neutral samples as don't care, and their loss in from elicitor classification is ignored. α in Equation 14 is set to 0.4 based on several rounds of hyperparameter tuning.\n\nTo calculate the EmoDistLoss, we use 1 as the unit distance and define the distance for each emotional aspect as illustrated in Appendix C. For valence, it is commonly adopted to consider negative and positive as two polarities and neutral in the middle  (Socher et al., 2013) . Therefore, the distance is 2 between positive and negative, and 1 between non-neutral and neutral. For emotion elicitors, we set the distance between don't care to any specific elicitor as 0.5 to penalise a \"lazy\" classifier that wrongly recognises the emotion as neutral. Doing so also results in a consistent shortest distance of 1 between any pair of specific elicitors.\n\nWe follow the default training set-up of each model except for ContextBERT. We reduce the context size of ContextBERT from 512 to 128, resulting in stronger performance and faster training.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation",
      "text": "We report F1 for each emotion. For overall performance, we report both macro F1 and weighted F1. Macro F1 considers each emotion equally and reflects the model's ability to recognise rare emotions. Weighted F1 is the weighted sum of F1 scores of each label. Weights are determined by the proportion of each emotion in the dataset. We exclude Neutral from calculating the averages as it makes up more than 70% of labels.\n\nIn addition, we also calculate the average emotion distance (AED) between the recognised emotion and the label to quantify how wrong the model is when it misclassifies. The AED of an emotion e is calculated from the average of D(label=e, recognised_emotion) of samples whose label is e (see Equation  7 ). Lower AED means less severe consequences from mistakes, and is therefore more desirable. All experiments are repeated with 10 different seeds.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Erc Results",
      "text": "Table  2  shows the change in the emotion recognition performance of the selected chit-chat ERC models after incorporating our ERToD framework.  We perform an ablation study on the best resulting model, ContextBERT-ERToD (Table  3 ). We add each technique in the order of data-related, feature-related, and loss-related approaches. Averaged scores can be found in Appendix F.\n\nImpact of DA DA helps improve almost all F1 scores even with a relatively small number of additional samples. There is a small and insignificant drop in the F1 of Excited, which is also frequently confused among human annotators. Further work to resolve the ambiguities would be beneficial.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Dialogue State (+Ds)",
      "text": "Adding dialogue state features further improves most other non-neutral emotions. Although it does not bring advantages for the F1 of Fearful, the AED of it continues to improve, showing that the system is making less severe mistakes.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Impact Of Sentix",
      "text": "Initialising BERT with Sen-tiX parameters further improves the recognition of all other non-neutral emotions except for Abusive. This suggests that the sentiment information encoded in SentiX is useful for resolving ambiguity. We suspect that, while SentiX is good at distinguishing the valence of emotion, its effect is limited for user conduct, the hallmark of Abusive.\n\nImpact of MTL MTL improves F1 for all nonneutral emotions except for Satisfied. It also achieves the best AED for Abusive. This suggests that MTL heads, especially the conduct classification head, help identify emotions in the simpler valence-elicitor-conduct space. There is a slight drop in the F1 score of Satisfied, but it is compensated by the improvement in its AED.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Impact Of Emodistloss (+Ertod)",
      "text": "The final version of the model achieves the best F1 score in {Satisfied, Dissatisfied, Excited, Fearful, Abu-sive} and the best AED score in {Satisfied, Excited, Apologetic, Fearful}, leading to best averaged scores (Table  F8 ). This shows penalising misclassifications according to emotion distance, which is only possible thanks to the emotion model, further helps recognise ambiguous emotions.\n\nFor the degradation of both scores in Neutral, we hypothesise that the model recognises non-neutral emotions more boldly than annotators, who are more cautious about subtle emotional cues.\n\n6 Zero-shot User Satisfaction Prediction 6.1 Experimental Set-up",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our model with User Satisfaction Simulation (USS) dataset where user utterances are annotated with 5-level satisfaction ratings  (Sun et al., 2021) . Dialogues in USS come from 5 different ToD datasets:\n\nJing Dong Dialogue Corpus (JDDC, Chen et al., 2020) is a multi-turn Chinese dialogue dataset for E-commerce customer service. USS contains 54.5k user satisfaction annotations for 3300 dialouges sampled from JDDC. Since JDDC is in Chinese, we translated it into English with Google Translate API first.\n\nSchema-guided Dialogues (SGD,  Rastogi et al., 2020)  is a multi-domain, task-oriented conversations between a human and a virtual assistant. These conversations involve interactions with services and APIs spanning 20 domains, such as banks, events, media, calendar, travel, and weather. USS contains 13.8k user satisfaction annotations for 1000 dialogues sampled from SGD. Although we use SGD for DA, our DA samples do not overlap with SGD dialogues in USS.\n\nRecommendation Dialogue (ReDial,  Li et al., 2018)  is an annotated dataset of dialogues, where users recommend movies to each other. USS contains 11.8k user satisfaction annotations for 1000 dialogues sampled from ReDial.\n\nCoached Conversational Preference Elicitation (CCPE,  Radlinski et al., 2019)  is a dialogue dataset where the \" assistant\" is tasked with eliciting the \"user\" preferences about movies collected in the Wizard-of-Oz framework. USS contains 6.8k user satisfaction annotations for 500 dialogues sampled from CCPE.\n\nMultiWOZ  (Budzianowski et al., 2018 ) is a multi-domain task-oriented dialogue dataset collected in the Wizard-of-Oz framework spanning 7 domains such as restaurant, hotel, and attraction. USS contains 12.5k user satisfaction annotations for 1000 dialogues sampled from MultiWOZ. Since we trained our ERC model on EmoWOZ, which was based on MultiWOZ, we excluded it in our evaluation.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Baselines",
      "text": "We compare our zero-shot results with supervised models of  Sun et al. (2021)  and  Kim and Lipani (2022) . HiGRU  (Yang et al., 2016)  and BERT  (Devlin et al., 2019)  were the best two models trained by  Sun et al. (2021)  to benchmark USS dataset when it was first released. SatAct and SatActUtt are T5-based models  (Raffel et al., 2020) . SatAct is trained to predict user satisfaction and user action in a MTL set-up, whereas SatActUtt additionally incorporates user utterance generation. For satisfaction prediction, these models were set up to predict a 5-level rating during training.\n\nThese baseline models were trained on each one of the five ToD subsets in USS with a 10-fold cross-validation. Although non-3 ratings were upsampled by 10 times in their training, the training data size is still smaller than that of ContextBERT-ERToD (68.9k emotion annotations, EmoWOZ and DA samples altogether).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Zero-Shot Inference",
      "text": "We experimented with ContextBERT-ERToD, the best resulting model from ERC training. After training the model for ERC, we fixed its parameters and ran inference with USS dataset for zeroshot user satisfaction prediction. To adapt to user satisfaction prediction set-up, we excluded information about the user turn at t from the model input as well as the dialogue state. Specifically, for utterance encoding, we excluded u t from the dialogue history to have H t = [s t-1 , u t-1 , ..., s 1 , u 1 ]. For task information encoding, we shifted the context window in Equation 3 by one and have V t = V t-1 ⊕ V t-2 ⊕ V t-3 as the new contextual dialogue state vector.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Evaluation",
      "text": "In the works of baseline models, satisfaction ratings {1,2} were considered the negative class and {3,4,5} as the positive. To map the emotion prediction from our ERC model to binary satisfaction ratings, it is intuitive to leverage the valence aspect of emotions. Emotion classes with a negative valence were considered Not Satisfied and those with a positive valence as Satisfied. The emotion Apologetic is an exception among emotions with a negative valence. Since its elicitor is the user him/herself, it should not be considered as a sign of user dissatisfaction. Regarding the emotion class Neutral, we mapped it to Satisfied because the original evaluation set-up of baseline models considered the medium satisfaction rating, 3, as the positive class.\n\nOverall, we considered {Neutral, Apologetic, Excited, Satisfied} as the positive class and {Fearful, Dissatisfied, Abusive} as negative.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Results",
      "text": "JDDC SGD ReDial CCPE HiGRU  (Sun et al., 2021)  17.1 8.6 8.3 27.4 BERT  (Sun et al., 2021)  18.5 4.8 12.5 24.5 SatAct  (Kim and Lipani, 2022  Following existing work, we first report binary F1 for direct comparison. In Table  4 , ContextBERT-ERToD performs comparably with SatActUtt and significantly outperforms other models. This shows that our ERToD framework in combination with the ERC model generalises well to user satisfaction prediction.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose ERToD, a framework to address three critical steps in learning and effectively adapt chit-chat ERC models to recognise emotions in ToDs. We propose two strategies of DA for different emotions to improve ERC performance in ToDs on rare emotions. We further leverage dialogue state and sentiment-aware embeddings for a richer feature representation. In addition, we apply MTL and devise a novel loss function, EmoDistLoss, which take the similarities between emotions into account. Our framework significantly improves existing chit-chat ERC models' performance in recognising user emotions in ToDs. By further applying our best resulting model to perform the task of user satisfaction prediction, we show that our method generalises well on other similar valence-related classification tasks in ToDs.\n\nAs more sophisticated and powerful dialogue systems such as ChatGPT arise, there is an urge to recognise, understand and handle the emotion of the user, especially in the age where online abuse is omnipresent. The long-term aim of this work is to obtain valuable insight for downstream ToD modelling tasks. This allows further investigation of emotion regulation strategies on the system side to improve task performance and user satisfaction, and to prevent undesirable user behaviours.     Please note that although this class is called \"fearful\" for simplicity, user's negative emotion due to any undesirable events that is out of the control of the operator also belongs to this category in EmoWOZ according to Table  A1 . C Emotional Aspect Distance Definition",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "A Emotion Definitions In Emowoz",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "B.2 Augmentation With Existing Dataset And Utterance Replacement",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "D Examples Of Model Recognitions",
      "text": "",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of dialogues about holiday in",
      "page": 1
    },
    {
      "caption": "Figure 1: , in chit-chat dialogues,",
      "page": 1
    },
    {
      "caption": "Figure 2: Figure 2: Our proposed ERToD Framework.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: and Appendix",
      "page": 5
    },
    {
      "caption": "Table 1: EmoWOZ Emotion definition and distribution.",
      "page": 6
    },
    {
      "caption": "Table 2: ContextBERT (Feng et al., 2022)",
      "page": 6
    },
    {
      "caption": "Table 2: shows the change in the emotion recog-",
      "page": 7
    },
    {
      "caption": "Table 2: Macro- and weighted-average F1 (MF1, WF1)",
      "page": 7
    },
    {
      "caption": "Table 3: F1 (↑) and AED (↓) scores of Neutral, Satisfied,",
      "page": 7
    },
    {
      "caption": "Table 4: Binary F1 scores on different USS subsets.",
      "page": 9
    },
    {
      "caption": "Table 4: , ContextBERT-",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1470"
    },
    {
      "citation_id": "2",
      "title": "MultiWOZ -a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
      "authors": [
        "Paweł Budzianowski",
        "Tsung-Hsien Wen",
        "Bo-Hsiang Tseng",
        "Iñigo Casanueva",
        "Stefan Ultes",
        "Milica Osman Ramadan",
        "Gašić"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1547"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Taskmaster-1: Toward a realistic and diverse dialog dataset",
      "authors": [
        "Bill Byrne",
        "Karthik Krishnamoorthi",
        "Chinnadhurai Sankar",
        "Arvind Neelakantan",
        "Ben Goodrich",
        "Daniel Duckworth",
        "Semih Yavuz",
        "Amit Dubey",
        "Kyu-Young Kim",
        "Andy Cedilnik"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1459"
    },
    {
      "citation_id": "5",
      "title": "Multitask learning",
      "authors": [
        "Rich Caruana"
      ],
      "year": "1997",
      "venue": "Machine Learning",
      "doi": "10.1023/A:1007379606734"
    },
    {
      "citation_id": "6",
      "title": "ConvAbuse: Data, analysis, and benchmarks for nuanced abuse detection in conversational AI",
      "authors": [
        "Amanda Cercas Curry",
        "Gavin Abercrombie",
        "Verena Rieser"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.587"
    },
    {
      "citation_id": "7",
      "title": "Multi-task dialog act and sentiment recognition on mastodon",
      "authors": [
        "Christophe Cerisara",
        "Somayeh Jafaritazehjani",
        "Adedayo Oluokun",
        "Hoa Le"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Complement objective training",
      "authors": [
        "Pei-Hsin Hao-Yun Chen",
        "Chun-Hao Wang",
        "Shih-Chieh Liu",
        "Jia-Yu Chang",
        "Yu-Ting Pan",
        "Wei Chen",
        "Da-Cheng Wei",
        "Juan"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "9",
      "title": "The JDDC corpus: A large-scale multi-turn Chinese dialogue dataset for E-commerce customer service",
      "authors": [
        "Meng Chen",
        "Ruixue Liu",
        "Lei Shen",
        "Shaozu Yuan",
        "Jingyan Zhou",
        "Youzheng Wu",
        "Xiaodong He",
        "Bowen Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "10",
      "title": "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "authors": [
        "Ronan Collobert",
        "Jason Weston"
      ],
      "year": "2008",
      "venue": "International Conference on Machine Learning. Niko Colneric and Janez Demšar"
    },
    {
      "citation_id": "11",
      "title": "Emotion detection in task-oriented spoken dialogues",
      "authors": [
        "L Devillers",
        "L Lamel",
        "I Vasilescu"
      ],
      "year": "2003",
      "venue": "2003 International Conference on Multimedia and Expo. ICME '03. Proceedings",
      "doi": "10.1109/ICME.2003.1221370"
    },
    {
      "citation_id": "12",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "13",
      "title": "Mul-tiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines",
      "authors": [
        "Mihail Eric",
        "Rahul Goel",
        "Shachi Paul",
        "Abhishek Sethi",
        "Sanchit Agarwal",
        "Shuyang Gao",
        "Adarsh Kumar",
        "Anuj Goyal",
        "Peter Ku",
        "Dilek Hakkani-Tur"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "14",
      "title": "EmoWOZ: A large-scale corpus and labelling scheme for emotion recognition in task-oriented dialogue systems",
      "authors": [
        "Shutong Feng",
        "Nurul Lubis",
        "Christian Geishauser",
        "Hsien-Chin Lin",
        "Michael Heck",
        "Carel Van Niekerk",
        "Milica Gasic"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "15",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "16",
      "title": "Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management",
      "authors": [
        "Milan Gritta",
        "Gerasimos Lampouras",
        "Ignacio Iacobacci"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00352"
    },
    {
      "citation_id": "17",
      "title": "Robust Dialogue State Tracking with Weak Supervision and Sparse Data",
      "authors": [
        "Michael Heck",
        "Nurul Lubis",
        "Shutong Carel Van Niekerk",
        "Christian Feng",
        "Hsien-Chin Geishauser",
        "Milica Lin",
        "Gašić"
      ],
      "year": "2022",
      "venue": "Robust Dialogue State Tracking with Weak Supervision and Sparse Data",
      "doi": "10.1162/tacl_a_00513"
    },
    {
      "citation_id": "18",
      "title": "Squared earth mover's distance-based loss for training deep neural networks",
      "authors": [
        "Le Hou",
        "Chen-Ping Yu",
        "Dimitris Samaras"
      ],
      "year": "2016",
      "venue": "Squared earth mover's distance-based loss for training deep neural networks"
    },
    {
      "citation_id": "19",
      "title": "Sequence-to-sequence data augmentation for dialogue language understanding",
      "authors": [
        "Yutai Hou",
        "Yijia Liu",
        "Wanxiang Che",
        "Ting Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "Are current task-oriented dialogue systems able to satisfy impolite users?",
      "authors": [
        "Zhiqiang Hu",
        "Roy Kaa-Wei Lee",
        "Nancy Chen"
      ],
      "year": "2022",
      "venue": "Are current task-oriented dialogue systems able to satisfy impolite users?"
    },
    {
      "citation_id": "21",
      "title": "TinyBERT: Distilling BERT for natural language understanding",
      "authors": [
        "Xiaoqi Jiao",
        "Yichun Yin",
        "Lifeng Shang",
        "Xin Jiang",
        "Xiao Chen",
        "Linlin Li",
        "Fang Wang",
        "Qun Liu"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.372"
    },
    {
      "citation_id": "22",
      "title": "Speech and Language Processing",
      "authors": [
        "Daniel Jurafsky",
        "James Martin"
      ],
      "year": "2009",
      "venue": "Speech and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition based on multi-task learning using a convolutional neural network",
      "authors": [
        "Nam Kyun",
        "Jiwon Lee",
        "Kyu Hun",
        "Geon Ha",
        "Jung Lee",
        "Hong Hyuk Lee",
        "Kim Kook"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)",
      "doi": "10.1109/APSIPA.2017.8282123"
    },
    {
      "citation_id": "24",
      "title": "EmoBERTa: Speaker-aware emotion recognition in conversation with RoBERTa",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-aware emotion recognition in conversation with RoBERTa"
    },
    {
      "citation_id": "25",
      "title": "A multi-task based neural model to simulate users in goal oriented dialogue systems",
      "authors": [
        "Eun To",
        "Aldo Kim",
        "Lipani"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22",
      "doi": "10.1145/3477495.3531814"
    },
    {
      "citation_id": "26",
      "title": "A closer look at feature space data augmentation for few-shot intent classification",
      "authors": [
        "Varun Kumar",
        "Hadrien Glaude",
        "Cyprien De Lichy",
        "William Campbell"
      ],
      "year": "2019",
      "venue": "A closer look at feature space data augmentation for few-shot intent classification"
    },
    {
      "citation_id": "27",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "28",
      "title": "Towards deep conversational recommendations",
      "authors": [
        "Raymond Li",
        "Samira Kahou",
        "Hannes Schulz",
        "Vincent Michalski",
        "Laurent Charlin",
        "Chris Pal"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Yash Mehta, and Erik Cambria. 2021. Multitask learning for emotion and personality detection",
      "authors": [
        "Yang Li",
        "Amirmohammad Kazameini"
      ],
      "venue": "Yash Mehta, and Erik Cambria. 2021. Multitask learning for emotion and personality detection"
    },
    {
      "citation_id": "30",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "31",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "32",
      "title": "Construction and analysis of social-affective interaction corpus in english and indonesian",
      "authors": [
        "Nurul Lubis",
        "Sakriani Sakti",
        "Graham Neubig",
        "Tomoki Toda",
        "Satoshi Nakamura"
      ],
      "year": "2015",
      "venue": "2015 International Conference Oriental COCOSDA held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation"
    },
    {
      "citation_id": "33",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "34",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.20"
    },
    {
      "citation_id": "35",
      "title": "Genpads: Reinforcing politeness in an end-toend dialogue system",
      "authors": [
        "Kshitij Mishra",
        "Mauajama Firdaus",
        "Asif Ekbal"
      ],
      "year": "2023",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0278323"
    },
    {
      "citation_id": "36",
      "title": "A comprehensive survey on word representation models: From classical to state-of-the-art word representation language models",
      "authors": [
        "Usman Naseem",
        "Imran Razzak",
        "Mukesh Shah Khalid Khan",
        "Prasad"
      ],
      "year": "2021",
      "venue": "ACM Trans. Asian Low-Resour. Lang. Inf. Process",
      "doi": "10.1145/3434237"
    },
    {
      "citation_id": "37",
      "title": "The Cognitive Structure of Emotions",
      "authors": [
        "Andrew Ortony",
        "Gerald Clore",
        "Allan Collins"
      ],
      "year": "1988",
      "venue": "The Cognitive Structure of Emotions",
      "doi": "10.1017/CBO9780511571299"
    },
    {
      "citation_id": "38",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "citation_id": "39",
      "title": "Affective Computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "41",
      "title": "Coached conversational preference elicitation: A case study in understanding movie preferences",
      "authors": [
        "Filip Radlinski",
        "Krisztian Balog",
        "Bill Byrne",
        "Karthik Krishnamoorthi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue",
      "doi": "10.18653/v1/W19-5941"
    },
    {
      "citation_id": "42",
      "title": "Data distillation: Towards omni-supervised learning",
      "authors": [
        "Ilija Radosavovic",
        "Piotr Dollár",
        "Ross Girshick",
        "Georgia Gkioxari",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
      "authors": [
        "Abhinav Rastogi",
        "Srinivas Zang",
        "Raghav Sunkara",
        "Pranav Gupta",
        "Khaitan"
      ],
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
      "authors": [
        "Abhinav Rastogi",
        "Xiaoxue Zang",
        "Srinivas Sunkara",
        "Raghav Gupta",
        "Pranav Khaitan"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Proceedings of the VLDB Endowment",
      "authors": [
        "Alexander Ratner",
        "Stephen Bach",
        "Henry Ehrenberg",
        "Jason Fries",
        "Sen Wu",
        "Christopher Ré"
      ],
      "year": "2017",
      "venue": "Proceedings of the VLDB Endowment",
      "doi": "10.14778/3157794.3157797"
    },
    {
      "citation_id": "47",
      "title": "Towards sentiment aided dialogue policy learning for multi-intent conversations using hierarchical reinforcement learning",
      "authors": [
        "Tulika Saha",
        "Sriparna Saha",
        "Pushpak Bhattacharyya"
      ],
      "year": "2020",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0235367"
    },
    {
      "citation_id": "48",
      "title": "A parameterized and annotated spoken dialog corpus of the CMU let's go bus information system",
      "authors": [
        "Alexander Schmitt",
        "Stefan Ultes",
        "Wolfgang Minker"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "49",
      "title": "Sentiment adaptive end-to-end dialog systems",
      "authors": [
        "Weiyan Shi",
        "Zhou Yu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1140"
    },
    {
      "citation_id": "50",
      "title": "EmoInHindi: A multi-label emotion and intensity annotated dataset in Hindi for emotion recognition in dialogues",
      "authors": [
        "Gopendra Vikram Singh",
        "Priyanshu Priya",
        "Mauajama Firdaus"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "51",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "Richard Socher",
        "Alex Perelygin",
        "Jean Wu",
        "Jason Chuang",
        "Christopher Manning",
        "Andrew Ng",
        "Christopher Potts"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "53",
      "title": "Simulating user satisfaction for the evaluation of task-oriented dialogue systems",
      "authors": [
        "Weiwei Sun",
        "Shuo Zhang",
        "Krisztian Balog",
        "Zhaochun Ren",
        "Pengjie Ren",
        "Zhumin Chen",
        "Maarten De Rijke"
      ],
      "year": "2021",
      "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21",
      "doi": "10.1145/3404835.3463241"
    },
    {
      "citation_id": "54",
      "title": "Learning sentiment-specific word embedding for Twitter sentiment classification",
      "authors": [
        "Duyu Tang",
        "Furu Wei",
        "Nan Yang",
        "Ming Zhou",
        "Ting Liu",
        "Bing Qin",
        "; Carel Van Niekerk",
        "Andrey Malinin",
        "Christian Geishauser",
        "Michael Heck",
        "Hsien-Chin Lin"
      ],
      "year": "2014",
      "venue": "Nurul Lubis, Shutong Feng, and Milica Gasic. 2021. Uncertainty measures in neural belief tracking and the effects on dialogue policy performance",
      "doi": "10.18653/v1/2021.emnlp-main.623"
    },
    {
      "citation_id": "55",
      "title": "Sentiment classification in customer service dialogue with topic-aware multitask learning",
      "authors": [
        "Jiancheng Wang",
        "Jingjing Wang",
        "Changlong Sun",
        "Shoushan Li",
        "Xiaozhong Liu",
        "Luo Si",
        "Min Zhang",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i05.6454"
    },
    {
      "citation_id": "56",
      "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "authors": [
        "Jason Wei",
        "Kai Zou"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1670"
    },
    {
      "citation_id": "57",
      "title": "Unsupervised data augmentation for consistency training",
      "authors": [
        "Qizhe Xie",
        "Zihang Dai",
        "Eduard Hovy",
        "Minh-Thang Luong",
        "V Quoc",
        "Le"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20"
    },
    {
      "citation_id": "58",
      "title": "Self-training with noisy student improves imagenet classification",
      "authors": [
        "Qizhe Xie",
        "Minh-Thang Luong",
        "Eduard Hovy",
        "V Quoc",
        "Le"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "59",
      "title": "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "authors": [
        "Hu Xu",
        "Bing Liu",
        "Lei Shu",
        "Philip Yu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1242"
    },
    {
      "citation_id": "60",
      "title": "Sas: Self-augmentation strategy for language model pretraining",
      "authors": [
        "Yifei Xu",
        "Jingqiao Zhang",
        "Ru He",
        "Liangzhu Ge",
        "Chao Yang",
        "Cheng Yang",
        "Ying Wu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v36i10.21412"
    },
    {
      "citation_id": "61",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Zichao Yang",
        "Diyi Yang",
        "Chris Dyer",
        "Xiaodong He",
        "Alex Smola",
        "Eduard Hovy"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N16-1174"
    },
    {
      "citation_id": "62",
      "title": "SentiB-ERT: A transferable transformer-based architecture for compositional sentiment semantics",
      "authors": [
        "Tao Da Yin",
        "Kai-Wei Meng",
        "Chang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.341"
    },
    {
      "citation_id": "63",
      "title": "Talking to machines (statistically speaking)",
      "authors": [
        "Steve Young"
      ],
      "year": "2002",
      "venue": "Seventh International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "64",
      "title": "The hidden information state model: A practical framework for POMDP-based spoken dialogue management",
      "authors": [
        "Steve Young",
        "Milica Gašić",
        "Simon Keizer",
        "François Mairesse",
        "Jost Schatzmann",
        "Blaise Thomson",
        "Kai Yu"
      ],
      "year": "2010",
      "venue": "Computer Speech & Language",
      "doi": "10.1016/j.csl.2009.04.001"
    },
    {
      "citation_id": "65",
      "title": "Refining word embeddings for sentiment analysis",
      "authors": [
        "Liang-Chih Yu",
        "Jin Wang",
        "K Lai",
        "Xuejie Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1056"
    },
    {
      "citation_id": "66",
      "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
      "authors": [
        "Sayyed Zahiri",
        "Jinho Choi"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Workshop on Affective Content Analysis, AFFCON'18"
    },
    {
      "citation_id": "67",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    },
    {
      "citation_id": "68",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "Hao Zhou",
        "Minlie Huang",
        "Tianyang Zhang",
        "Xiaoyan Zhu",
        "Bing Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v32i1.11325"
    },
    {
      "citation_id": "69",
      "title": "SentiX: A sentiment-aware pre-trained model for cross-domain sentiment analysis",
      "authors": [
        "Jie Zhou",
        "Junfeng Tian",
        "Rui Wang",
        "Yuanbin Wu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.49"
    },
    {
      "citation_id": "70",
      "title": "Convlab-3: A flexible dialogue system toolkit based on a unified data format",
      "authors": [
        "Qi Zhu",
        "Christian Geishauser",
        "Carel Hsien-Chin Lin",
        "Baolin Van Niekerk",
        "Zheng Peng",
        "Michael Zhang",
        "Nurul Heck",
        "Dazhen Lubis",
        "Xiaochen Wan",
        "Jianfeng Zhu",
        "Milica Gao",
        "Minlie Gašić",
        "Huang"
      ],
      "year": "2022",
      "venue": "Convlab-3: A flexible dialogue system toolkit based on a unified data format",
      "doi": "10.48550/ARXIV.2211.17148"
    }
  ]
}