{
  "paper_id": "2003.01062v2",
  "title": "Proxemo: Gait-Based Emotion Learning And Multi-View Proxemic Fusion For Socially-Aware Robot Navigation",
  "published": "2020-03-02T17:47:49Z",
  "authors": [
    "Venkatraman Narayanan",
    "Bala Murali Manoghar",
    "Vishnu Sashank Dorbala",
    "Dinesh Manocha",
    "Aniket Bera"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Supplemental version including Code, Video, Datasets at https://gamma.umd.edu/proxemo/ Abstract-We present ProxEmo, a novel end-to-end emotion prediction algorithm for socially aware robot navigation among pedestrians. Our approach predicts the perceived emotions of a pedestrian from walking gaits, which is then used for emotionguided navigation taking into account social and proxemic constraints. To classify emotions, we propose a multi-view skeleton graph convolution-based model that works on a commodity camera mounted onto a moving robot. Our emotion recognition is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. It achieves a mean average emotion prediction precision of 82.47% on the Emotion-Gait benchmark dataset. We outperform current state-of-art algorithms for emotion recognition from 3D gaits. We highlight its benefits in terms of navigation in indoor scenes using a Clearpath Jackal robot.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recent advances in AI and robotics technology are gradually enabling humans and robots to coexist and share spaces in different environments. This is especially common in places such as hospitals, airports, and shopping malls. Navigating a robot with collision-free and socially-acceptable paths in such scenarios poses several challenges  [1] . For example, in the case of a crowded shopping mall, the robot needs to be aware of the intentions of an oblivious shopper coming towards it for friendly navigation. Knowing the perceived emotional state of a human in such scenarios allows the robot to make more informed decisions and navigate in a socially-aware manner.\n\nUnderstanding human emotion has been a well-studied subject in several areas of literature, including psychology, human-robot interaction, etc. There have been several works that try to determine the emotion of a person from verbal (speech, text, and tone of voice)  [2] ,  [3]  and non-verbal (facial expressions, walking styles, postures)  [4] ,  [5]  cues. There also exist multi-modal approaches that use a combination of these cues to determine the person's emotion  [6] -  [8] .\n\nIn our work, we focus on emotionally-aware robot navigation in crowded scenarios. Here, verbal cues for emotion classification are not easily attainable. With non-verbal cues, facial expressions that are often occluded from the egocentric view of the robot and might not be fully visible. Besides, emotion analysis from facial features is a topic of debate in several previous works: these features are inherently unreliable caused by vague expressions emerging from a variety of psychological and environmental factors  [9] ,  [10] . As such, in our work, we focus on using \"walking styles\" or \"gaits\" to extract the emotions of people in crowds.\n\nObtaining perceived emotions from gaits is a challenging problem that has been well documented in the past. More recently, various machine learning solutions  [11] ,  [12]  have been proposed to tackle this problem. However, these approaches suffer from the following drawbacks: The green arrow indicates the new path after an angry emotion is detected. Observe the significant shift away from the pedestrian when an angry gait is detected. This form of navigation is especially useful when the robot is expected to navigate safely through crowds without causing discomfort to nearby pedestrians.\n\n• The training datasets used are singular in direction, i.e., there is motion capture only when a person is walking in a straight line towards the camera. This is a significant disadvantage for our task of socially-aware crowd navigation, where the robot often encounters people walking from several directions towards or away from the camera. • Some approaches that are tailored towards using emotion for enhancing the task of robot navigation assume a static overhead camera that captures the trajectories of pedestrians. This is not ideal, as the overhead camera might not always be available in all scenarios. To overcome these challenges, we propose ProxEmo, a novel algorithm for realtime gait-based emotion classification for socially-guided navigation. ProxEmo is tailored towards working with commodity RGB egocentric cameras that can be retrofitted onto moving platforms or robots for navigating among pedestrians. The major contributions of our work can be summarized as follows:\n\n• We introduce a novel approach using group convolutions to classify pedestrian emotions from gaits, which drastically improves accuracy compared to SOTA. • Our method explicitly takes into consideration pedestrian behavior in crowds as we train our model on skeletal data of people approaching the robot from multiple directions, as opposed to approaching from a single view from the front. • We present a new navigation scheme using Proxemic Fusion that accounts for pedestrian emotions. • Finally, we introduce a Variational Comfort Space, which integrates into our navigation scheme, taking into account varying pedestrian orientations. We note that identifying the true nature of a person's emotion via only a visual medium can be difficult. Therefore in this work, we focus only on the perceived emotions from the point of an external observer as opposed to actual internal emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we present a brief overview of socialrobot navigation algorithms. We also review related work on emotion modeling and classification from visual cues.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Social Robotics And Emotionally-Guided Navigation",
      "text": "As robots have become more commonplace, their impact on humans' social lives has emerged as an active area of research. Studies from multiple domains  [13] -  [16]  have tried to quantify this impact in several ways. In  [1] , Kruse et al. present a comprehensive survey on navigation schemes for robots in social scenarios. They describe various social norms (interpersonal distances, human comfort, sociability) that the robot must consider not to cause discomfort to people around it. Michaid et al.  [17]  discuss about how robots can attain artificial emotions for social interactions. Several classical  [18] -  [20]  and deep learning  [21]  approaches tackle the problem of navigation through highly dynamic environments. More recently, reinforcement learning methods  [22] ,  [23]  have been described for collision avoidance in such environments. For pedestrian handling, in particular, Randhavane et al.  [24]  make use of a pedestrian dominance model (PDM) to identify the dominance level of humans and plan a trajectory accordingly. In  [25] , Rios-Martinez et al. present a detailed survey on the proxemics involved with socially aware navigation. In  [26] , Kitazawa et al. discuss ideas such as Information Process Space of a human. In  [27] , Pandey et al. discuss a strategy to plan a socially aware path using milestones.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Emotion Modeling And Classification",
      "text": "There exists a substantial amount of research that focuses on identifying the emotions of humans based on body posture, movement, and other non-verbal cues. Ruiz-Garcia et al.  [28]  and Tarnowski et al.  [29] , use deep learning to classify different categories of emotion from facial expressions. The approach by  [8]  uses multiple modalities such as facial cues, human pose and scene understanding. Randhavane et al.  [30] ,  [31]  classify emotions into four classes based on affective features obtained from 3D skeletal poses extracted from human gait cycles. Their algorithm, however, requires a large number of 3D skeletal key-points to detect emotions and is limited to single individual cases. Bera et al.  [32] ,  [33]  classify emotions based on facial expressions along with a pedestrian trajectory obtained from overhead cameras. Although this technique achieves good accuracy in predicting emotions from trajectories and facial expressions, it explicitly requires overhead cameras in its pipeline.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Action Recognition For Emotions",
      "text": "The task of action recognition involves identifying human actions from sequences of data (usually videos)  [34] . A common task in many of these models is recognizing gaitbased actions such as walking and running. Thus, the task of gait action recognition is closely related to the task of emotion recognition from gaits, as both perform classification on the same input. Bhattacharya et al.  [12] ,  [35]  use graph convolutions for the emotion recognition task, in a method similar to the action recognition model used in Yan et al.  [36] . Ji et al.  [37]  propose a CNN-based method that gives state-of-the-art results on gait based action recognition tasks. Their model is invariant to viewpoint changes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Overview And Methodology",
      "text": "We propose a novel approach, ProxEmo, for classifying emotions from gaits that works with an egocentric camera setup. Our method uses 3D poses of human gaits obtained from an onboard robot camera to classify perceived emotions. These perceived emotions are then used to compute variable proxemic constraints in order to perform socially aware navigation through a pedestrian environment. Figure  2  illustrates how we incorporate ProxEmo into an end-to-end emotionally-guided navigation pipeline.\n\nThe following subsections will describe our approach in detail. We first discuss the dataset and the augmentation details we used for training. Then, we briefly discuss our pose estimation model, followed by a detailed discussion of our emotion classification model, ProxEmo. Finally, we describe how socially-aware navigation can be performed using the obtained emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Notations",
      "text": "In our formulation, we represent the human with 16 joints as shown in figure  4 . Thus, a pose P ∈ R 16×3 of a human is a set of 3D positions of each joint j i , where i ∈ {0, 1, ..., 15}. For any RGB video V , we represent the gait extracted using 3D pose estimation as G. The gait G is a set of 3D poses P 1 , P 2 , ..., P τ where τ is the number of frames in the input video V .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset Preparation",
      "text": "We make use of two labeled datasets by Randhavane et al.  [38]  and Bhattacharya et al.  [12] , containing time-series 3D joints of 342 and 1835 gait cycles each (a total of 2177 gait samples). Each gait cycle has 75 timesteps with 16 joints as shown in Figure  4 . Thus, each sample in this new dataset has a dimension of joints × time× dimensions = 16 * 75 * 3 = 3600. These samples are labeled into 4 emotion classes: angry, sad, happy, and neutral with 10 labelers per video (to capture the perceptual difference between different labelers). In order to train our network for prediction from multiple views, we augment the dataset as follows. First, we consider a camera placed at a particular distance from the human, as shown in Figure  3 . Then for different camera positions oriented towards the human, we perform augmentation by applying transformations given in equation 1.\n\nwhere j aug are the coordinates of the augmented joints, T x , T y , T z are the translation vectors, and θ is the rotation along Y axis. For our experiments, we attain 72 × 4 = 288 augmentations for each sample by considering θ at gradients of 5 • , with 4 translations of [1m-4m] along the Z axis (T z ). Thus, after augmentation, we have a total of 288 × 2177 = 626, 976 gaits in our dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Human-Pose Estimation",
      "text": "A pose estimation strategy for humans walking in a crowded real-world scenario has to be robust to noise coming from human attire or any items they might be carrying. To account for this, we employ a robust approach described in  [39]  for this task. Their paper describes a two-step network trained in a weakly supervised fashion. First, a Structure-Aware PoseNet (SAP-Net) trained on spatial information provides an initial estimate of joint locations of people in video frames. Later, a Temporal PoseNet (TP-Net) trained on time-series information corrects this initial estimate by adjusting illegal joint angles and joint distances. The final output is a sequence of well-aligned 3D skeletal poses P. Figure  4  is a representation of the skeletal output obtained.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Generating Image Embeddings",
      "text": "We observe that 2D convolutions are much faster and efficient as opposed to graph convolutions  [37] . Hence, we embed the spatial-temporal skeletal gait sequence G as an image I, using the equations described in 2.\n\nHere, R, B, and G are image color channels, x, y are the co-ordinates of image, and X (t,j) , Y (t,j) , Z (t,j) are the co-",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Proxemo: Classifying Emotions From Gaits",
      "text": "Figure  5  illustrates the architecture of our model for emotion classification. The image embedding I obtained from the gaits are passed through two stages of group convolutions to obtain an emotion label.\n\n1) Group Convolutions: We take inspiration from  [40]  and make use of group convolutional layers in designing our ProxEmo architecture. Group Convolution Layers (GC), in essence, operate just like 2D convolution layers, except that they fragment the input into n g groups and perform convolution operations individually on them before stacking the outputs together. The advantage of doing this is that the network learns from different parts of the input in isolation. This is especially useful in our case because we have a dataset that varies based on two factors, view-group and emotion labels. The variation in the view-groups is learned by the different convolution groups GC, and the emotions are learned by the convolutions taking place within each group. Group convolutions increase the number of channels in each layer by n g times. The output (h i ) of each group in the convolution layer is\n\nwhere, h out is the output of the group convolution, x i represents the input, and k i represents the kernel for convolution. The output [h 1 |...|h ng ] is a matrix concatenation of all the group outputs along channel axis. In our case, we choose n g as 4 because we have 4 view-groups.\n\n2) ProxEmo Architecture: The network consists of seven convolution layers. The initial layer is a traditional 2D convolution layer, which performs channel up-sampling for the forthcoming group convolution operations. These operations take place in two stages -Stage 1: This consists of two GC layers, each having 128 convolution filters (32 per group × n g ). The output of each both the group convolution stages, h s are given by,\n\nwhere, s represents the two group convolution stages as described before, x s is the input to the group convolution stage 's', k 1 s and k 2 s represent convolution kernels for first and second GC layers within a stage, p * s and h * s are the first and second GC layer outputs determined using equation above.\n\nAfter performing the group convolutions, the output h 2 is passed through two 2D convolution layers. These convolution layers help in gathering the features learned by the GC layers to finally predict both the view-group and emotion of the gait sequences.\n\nRather than using fully-connected layers for predicting the view-group, our method utilizes convolution layers to predict the n k × n g output, where n k is the number of emotions and n g is the number of view-groups. This makes our model considerably lighter (number of model parameters) and faster (run-time performance), compared to other state-of-the-art algorithms.\n\nThe final output of the classifier consists of multi-class sof tmax prediction, E i,j , given by the equation 4. Here e i,j refers to the final hidden layer output of the network, where i = 0, 1, . . . (n k -1) is the emotion class and j = 0, 1, . . . , (n g -1) is the view-group class.\n\nE i,j can be considered as a 4 × 4 matrix containing 16 values corresponding to different view-groups and emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F. Emotion-Guided Navigation Using Proxemic Fusion",
      "text": "We use the emotions E i,j predicted from ProxEmo to compute the comfort space (c) of a pedestrian, which is the socially comfortable distance (in cm) around a person.\n\nWe combine c along with the LIDAR data (L) to perform \"proxemic fusion\" (III-F.3), obtaining a set of points where it is permissible for the robot to navigate in a sociallyacceptable manner. This is illustrated in figure  6 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1) Comfort Space Computation:",
      "text": "In order to model the predicted emotions E i,j from ProxEmo into a comfort space distance c, we use the following equation:\n\nHere, E j represents a column vector of the softmax output, which corresponds to the group outcomes for each individual emotion. c j is a constant derived from psychological experiments described in  [41]  to compute the limits on an individual's comfort spaces and is chosen from a set {90.04, 112.71, 99.75, 92.03} corresponding to the comfort spaces (radius in cm) for {happy, sad, angry, neutral} respectively. We acknowledge that these distances depend on many factors, including cultural differences, environment, or a pedestrian's personality, and restrict our claims to variations in comfort spaces due to the emotional difference. These distances are based on how comfortable pedestrians are while interacting with others. v g is a view-group constant defined in the following subsection. In scenario 1, the pedestrian approaches the robot from the front. Here, as the pedestrian is aware of the robot's presence, it needs to be more respectful of the proxemic comfort space and take action V comf ort represented by the green arrow. In scenario 2, the robot is approaching the person from behind. An unaware pedestrian need not be disturbed by the robot, due to which it can be more liberal with its actions. The violet arrow representing the safe action V saf e coincides with V comf ort in this case.\n\n2) Variational Comfort Space (v g ): We take inspiration from the Information Process Space defined by Kitazawa et al.  [26]  to define our own Variational Comfort Space constant v g . This constant acts as a scaling factor in the comfort space based on the orientation of the pedestrian in the robot's view. This orientation is easily obtainable as ProxEmo also gives us a view-group output along with the emotion. v g is chosen from a set of {1, 0.5, 0, 0.5} based on the view group g predicted. This is chosen based on the fact that people have varying personal space with respect to their walking direction, i.e., a pedestrian will care more about his/her personal space in front as compared to the sides. Also, the pedestrian might not care about personal and comfort space behind them since it does not lie in their field of view  [42] . In figure  6 , we look at two scenarios to illustrate how the robot handles pedestrians considering variational comfort spaces:\n\n• Scenario 1: The robot is positioned in front of the person walking towards it. This is classified as viewgroup 1, having a v g value of 1. As the robot is visible to the person, in this case, it should be more precautious in safely maneuvering around the person. The comfort space around the pedestrian is larger in this case, and the robot takes a more skewed trajectory. • Scenario 2: The robot is approaching the pedestrian from behind. This gait is classified as view-group 3 and has a v g value of 0. As the robot is not in the person's field of vision, in this case, it can afford to safely pass around the fixed space F s of the person. At any time instant, the velocity of the robot will be directed towards the goal, and if there is an obstacle, it will lead to a collision v coll . If an obstacle avoidance algorithm is used, the navigation scheme avoids it with an action v saf e . However, for socially acceptable proximally-aware navigation, this is not sufficient, as this requires the robot to follow certain social norms. In order to adhere to these social norms, we incorporate the emotions predicted by ProxEmo to navigate in a socially acceptable manner represented by V comf ort .\n\n3) Proxemic Fusion: We fuse the LIDAR data to include proxemic constraints by performing a Minkowski sum (M ) of the set of LIDAR points L and a set containing the points in a circle Z defined by a radius r. The Minkowski sum M provides us with a set of all the admissible points where the robot can perform emotionally-guided navigation. This is formulated using the following equations.\n\nHere, a 0 is a reference point on the LIDAR, and d lidar is the distance measurement (in metres). r is the inflation radius and is defined using the comfort space c as:\n\nwhere dh ∈ L is a set of the LIDAR distances only for points where a human was detected. The maximum value of dh corresponds to the farthest distance from the person from their fixed inner space F s , while the minimum value of dh corresponds to the closest distance of the person from this space. F s is represented by the blue circle around the person in the figure  6 . In terms of mathematical morphology, the outcome of proxemic fusion is similar the dilation operation of the human, modelled as a obstacle, with the comfort space.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Evaluation Metrics",
      "text": "We evaluate our model using two metrics:\n\n• Mean Accuracy (%) -\n\nP ri,j * Rci,j P ri,j +Rci,j where, n k (= 4) is the number of emotion classes, n g (= 4) is the number of view-groups, T P i,j is the number of true predictions for i th emotion class and j th view-group, N i,j is the total number of data samples for i th emotion class and j th view-group, P r i,j and Rc i,j is the precision and recall for i th emotion class and j th view-group. All the metrics mentioned are derived from a confusion matrix generated by comparing actual vs predicted emotion and view-group for the data samples.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Implementation Details",
      "text": "For training, our dataset (III-B) has a train-validation split of 90%-10%. We generate a set of angles and translations that are different from the original dataset to formulate the test set.\n\nWe perform training using an ADAM  [43]  optimizer, with decay parameters of (β 1 = 0.9 and β 2 = 0.999). The experiments were run with a learning rate of 0.009 and with 10% decay every 250 epochs. The models were trained with softmax multi-class cross-entropy loss, L, represented in equation 8. The training was done on 2 Nvidia RTX 2080 Ti GPUs having 11GB of GPU memory each and 64 GB of RAM.\n\n-y m,i,j log(E m,i,j )\n\nwhere, y m,i,j is the target one-hot encoded label representing emotion class i{= 0, 1, ..n k } and view-group j{= 0, 1, ...n g } for the data sample m{= 0, 1, .., M }. E m,i,j is the predicted sof tmax output probability for data sample m being emotion class i and view-group class j.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Comparing Proxemo With Other Emotion Classifiers",
      "text": "We evaluate the performance of our ProxEmo network, against two other emotion classification algorithms  [12]    [38] . Since the other emotion classification algorithms don't consider the arbitrary view scenario, we compare our results with just single-view data, i.e., skeletal gaits that are directly approaching the RGB-D camera. Table  I  presents these results. The accuracy metrics reported are generated by modifying the equations in IV-A, for a single view-group (i.e., n g = 1).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "Accuracy (%) Venture et al.  [44]  30.83 Daoudi et al  [45]  42.5 Li et al.  [46]  53.7 Baseline (Vanilla LSTM)  [38]  55.  47 Crenn et al [47]  66.2 STEP  [12]  78.24 ProxEmo (ours) 82.4 We compare the accuracy (%) of our ProxEmo network with existing emotion classification algorithms on single-view (facing the camera) data samples. We observe that our network outperforms the current state-of-theart algorithm by 4%. Furthermore, our network outperforms the state-of-the-art algorithm across each emotion class. The accuracy numbers reported for  [38] ,  [12]  and ProxEmo are evaluated on the same dataset discussed in section III-B. The other methods are evaluated on different datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Comparing Proxemo With Action Recognition Models",
      "text": "As mentioned in section II-C, action recognition models and emotion recognition models that have inputs as gaits are closely related tasks. Thus, we can evaluate ProxEmo on pre-existing action recognition models by fine-tuning them on the emotion recognition task. We compare our model with two existing state-of-the-art action recognition models, (i) Spatial-Temporal Graph convolution networks (ST-GCN)  [36] , and (ii) VS-CNN  [37] . These architectures were trained using the datasets  [12] ,  [38]  (discussed in Section III-B).\n\n1) ST-GCN: The spatial-temporal graph convolution networks  [36]  perform skeletal action recognition using undirected spatial-temporal graphs for hierarchical representation of skeleton gait sequences. In the original implementation, the spatial-temporal graphs are used in a graph convolution network to detect the action performed through the sequence.\n\nWe fine-tune ST-GCN to predict human emotion instead of the actions. The human emotions modeled as a class label for the implementation.  2) VS-CNN: One of the major drawbacks of ST-GCN is that it is not tuned for multi-view/arbitrary-view skeletal gait sequences. View-guided Skeleton CNN (VS-CNN)  [37]  approaches this problem by building a dataset that multiple view-points with respect to the human reference frame. The multiple views are combined into four groups, each consisting of the one-quarter (90 degrees) of the view-points sequences. The action recognition is performed in three stages: (i) a view-group predictor network that predicts the view-group C (of 4 view-groups) of the sequence. (ii) a view-group feature network that consists of four individual networks, based on SK-CNN  [48] , for each view-group, and finally, (iii) a channel classifier network that combines (i) and (ii) to predict the action label for the skeletal gait sequence.\n\nThe VS-CNN also steers away from graph convolutions with an aim to increase the run-time performance of the network. 2D convolutions were observed to be much faster and efficient as opposed to graph convolutions. Hence, the spatial-temporal skeletal gait sequences are transformed into images. In our experiment, we tweak the final output of VS-CNN architecture using equation 4 to predict human emotions as opposed to actions. The network was trained with a sof tmax cross-entropy loss function, represented in equation 8.\n\nThe table I and figure  8  present a comparison of our model against VS-CNN and ST-GCN. We can observe that ProxEmo outperforms the state-of-the-art action recognition algorithms in both single-view and arbitrary-view skeletal gait sequences. Also, observe that in table II, ProxEmo takes up the least number of model parameters. This is because we perform group convolutions and eliminate Fully Connected layers in our network. Figure  9  is a confusion matrix of the predicted vs actual emotion classes of ProxEmo. We can infer from this matrix that our model performs fairly well across all emotion classes with a high accuracy. Since, the evaluation metrics for socially acceptable is not welldefined, we don't report any evaluation on our emotionguided navigation planning.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "View-Groups",
      "text": "Model Parameters ST-GCN  [36]  VS-CNN  [37]      [36]  and VS-CNN  [37] . This is due to the fact that we use Group Convolutions (GC) and eliminate Fully Connected (FC) layers in our network.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion, Limitations And Future Work",
      "text": "We present ProxEmo, a novel group convolution-based deep learning network that takes 3D skeletal gaits of a human and predicts the perceived emotional states {happy, sad, angry, neutral} for emotionally-guided robot navigation. Our model specifically takes into consideration arbitrary orientations of pedestrians and is trained using augmented data comprising of multiple view-groups. We also present a new approach for socially-aware navigation that takes into consideration the predicted emotion and view-group of the pedestrian in the robot's field of view. In doing this, we also define a new metric for computing comfort space, that incorporates constants derived from emotion and view-group predictions. The limitation of our model during inference time is that it is reliant on real-time 3D skeletal tracking.\n\nIn the future, we plan to look at multi-modal cues for emotion recognition. We intend to dynamically compute proxemic constraints using continual feedback in a rewardbased training scheme. We also plan to add higher-level information, with regards to the environmental or cultural context that are known to influence human emotions, which can further improve our classification results.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ProxEmo: We present a gait-based emotion and proxemics",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of our Pipeline: We ﬁrst capture an RGB video from an onboard camera and extract pedestrian poses and track them",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates how we incorporate ProxEmo into an",
      "page": 3
    },
    {
      "caption": "Figure 4: Thus, each sample in this new dataset",
      "page": 3
    },
    {
      "caption": "Figure 3: Then for different camera positions",
      "page": 3
    },
    {
      "caption": "Figure 3: Data Augmentation: By applying speciﬁc translations and",
      "page": 3
    },
    {
      "caption": "Figure 4: is a representation of the skeletal output obtained.",
      "page": 3
    },
    {
      "caption": "Figure 4: Skeleton Representation: We represent a pedestrian by 16",
      "page": 3
    },
    {
      "caption": "Figure 5: ProxEmo Network Architecture: The network is trained on image embeddings of the 5D gait set G, which are scaled up to",
      "page": 4
    },
    {
      "caption": "Figure 5: illustrates the architecture of our model for emo-",
      "page": 4
    },
    {
      "caption": "Figure 6: Variational Comfort Space: We consider a varying comfort",
      "page": 5
    },
    {
      "caption": "Figure 7: Emotionally-Guided Navigation: We use the emotions",
      "page": 6
    },
    {
      "caption": "Figure 8: Comparison of ProxEmo with other arbitrary view algorithms : Here we present the performance metrics (discussed in section",
      "page": 7
    },
    {
      "caption": "Figure 9: Confusion Matrix: We show the percentage of gaits",
      "page": 7
    },
    {
      "caption": "Figure 9: is a confusion matrix of",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Venture et al.\n[44]",
          "Accuracy (%)": "30.83"
        },
        {
          "Method": "Daoudi et al\n[45]",
          "Accuracy (%)": "42.5"
        },
        {
          "Method": "Li et al.\n[46]",
          "Accuracy (%)": "53.7"
        },
        {
          "Method": "Baseline (Vanilla LSTM)\n[38]",
          "Accuracy (%)": "55.47"
        },
        {
          "Method": "Crenn et al\n[47]",
          "Accuracy (%)": "66.2"
        },
        {
          "Method": "STEP [12]",
          "Accuracy (%)": "78.24"
        },
        {
          "Method": "ProxEmo (ours)",
          "Accuracy (%)": "82.4"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "View-Groups": "",
          "Model Parameters": "ProxEmo(ours)\nST-GCN [36]\nVS-CNN [37]"
        },
        {
          "View-Groups": "4",
          "Model Parameters": "0.33M\n1.4M\n63M"
        },
        {
          "View-Groups": "6",
          "Model Parameters": "0.5M\n1.4M\n65M"
        },
        {
          "View-Groups": "8",
          "Model Parameters": "0.69M\n1.4M\n68M"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human-aware robot navigation: A survey",
      "authors": [
        "T Kruse",
        "A Pandey",
        "R Alami",
        "A Kirsch"
      ],
      "year": "2013",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "2",
      "title": "Speech based emotion classification",
      "authors": [
        "Tin Lay Nwe",
        "Say Wei",
        "L Silva"
      ],
      "year": "2001",
      "venue": "Proceedings of TENCON 2001"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion analysis in noisy realworld environment",
      "authors": [
        "A Tawari",
        "M Trivedi"
      ],
      "year": "2010",
      "venue": "2010 20th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martínez"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "5",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "7",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Z Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "Analysis of emotion recognition using facial expressions, speech and multimodal information"
    },
    {
      "citation_id": "8",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "An intelligent and generic approach for detecting human emotions: a case study with facial expressions",
      "authors": [
        "L Mano",
        "V Gonc ¸alves",
        "G Pessin",
        "P Gomes",
        "A De Carvalho",
        "J Ueyama"
      ],
      "year": "2019",
      "venue": "Soft Computing"
    },
    {
      "citation_id": "10",
      "title": "Emotional expressions reconsidered: challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition through gait on mobile devices",
      "authors": [
        "M Chiu",
        "J Shu",
        "P Hui"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "12",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Toward sociable robots",
      "year": "2003",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "14",
      "title": "A survey of socially interactive robots",
      "authors": [
        "T Fong",
        "I Nourbakhsh",
        "K Dautenhahn"
      ],
      "year": "2003",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "15",
      "title": "Glmprealtime pedestrian path prediction using global and local movement patterns",
      "authors": [
        "A Bera",
        "S Kim",
        "T Randhavane",
        "S Pratapa",
        "D Manocha"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "16",
      "title": "Cmetric: A driving behavior measure using centrality functions",
      "authors": [
        "R Chandra",
        "U Bhattacharya",
        "T Mittal",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "17",
      "title": "Artificial Emotion and Social Robotics",
      "authors": [
        "F Michaud",
        "P Pirjanian",
        "J Audet",
        "D Létourneau"
      ],
      "year": "2000",
      "venue": "Artificial Emotion and Social Robotics"
    },
    {
      "citation_id": "18",
      "title": "Generalized velocity obstacles",
      "authors": [
        "D Wilkie",
        "J Van Den",
        "D Berg",
        "Manocha"
      ],
      "year": "2009",
      "venue": "2009 IEEE/RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "19",
      "title": "Optimal reciprocal collision avoidance for multi-agent navigation",
      "authors": [
        "J Van Den",
        "S Berg",
        "M Guy",
        "D Lin",
        "Manocha"
      ],
      "year": "2010",
      "venue": "Proc. of the IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "20",
      "title": "Reciprocal velocity obstacles for real-time multi-agent navigation",
      "authors": [
        "J Van Den Berg",
        "M Lin",
        "D Manocha"
      ],
      "year": "2008",
      "venue": "2008 IEEE International Conference on Robotics and Automation"
    },
    {
      "citation_id": "21",
      "title": "Dronet: Learning to fly by driving",
      "authors": [
        "A Loquercio",
        "A Maqueda",
        "C Del-Blanco",
        "D Scaramuzza"
      ],
      "year": "2018",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "22",
      "title": "Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning",
      "authors": [
        "P Long",
        "T Fanl",
        "X Liao",
        "W Liu",
        "H Zhang",
        "J Pan"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "23",
      "title": "Crowdmove: Autonomous mapless navigation in crowded scenarios",
      "authors": [
        "T Fan",
        "X Cheng",
        "J Pan",
        "D Manocha",
        "R Yang"
      ],
      "year": "2018",
      "venue": "Crowdmove: Autonomous mapless navigation in crowded scenarios",
      "arxiv": "arXiv:1807.07870"
    },
    {
      "citation_id": "24",
      "title": "Pedestrian dominance modeling for socially-aware robot navigation",
      "authors": [
        "T Randhavane",
        "A Bera",
        "E Kubin",
        "A Wang",
        "K Gray",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "25",
      "title": "From proxemics theory to socially-aware navigation: A survey",
      "authors": [
        "J Rios-Martinez",
        "A Spalanzani",
        "C Laugier"
      ],
      "year": "2015",
      "venue": "IJSR"
    },
    {
      "citation_id": "26",
      "title": "Pedestrian vision and collision avoidance behavior: Investigation of the information process space of pedestrians using an eye tracker",
      "authors": [
        "K Kitazawa",
        "T Fujiyama"
      ],
      "year": "2008",
      "venue": "Pedestrian and Evacuation Dynamics"
    },
    {
      "citation_id": "27",
      "title": "A framework towards a socially aware mobile robot motion in human-centered dynamic environment",
      "authors": [
        "A Pandey",
        "R Alami"
      ],
      "year": "2010",
      "venue": "2010 IEEE/RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "28",
      "title": "Deep learning for emotion recognition in faces",
      "authors": [
        "A Ruiz-Garcia",
        "M Elshaw",
        "A Altahhan",
        "V Palade"
      ],
      "year": "2016",
      "venue": "Artificial Neural Networks and Machine Learning"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski",
        "M Koodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2017",
      "venue": "Conference on Computational Science"
    },
    {
      "citation_id": "30",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "A Bera",
        "K Kapsaskis",
        "U Bhattacharya",
        "K Gray",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "31",
      "title": "The liar's walk: Detecting deception with gait and gesture",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "The liar's walk: Detecting deception with gait and gesture",
      "arxiv": "arXiv:1912.06874"
    },
    {
      "citation_id": "32",
      "title": "The emotionally intelligent robot: Improving socially-aware human prediction in crowded environments",
      "authors": [
        "A Bera",
        "T Randhavane",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "33",
      "title": "How are you feeling? multimodal emotion learning for socially-assistive robot navigation",
      "authors": [
        "A Bera",
        "T Randhavane",
        "R Prinja",
        "K Kapsaskis",
        "A Wang",
        "K Gray",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020) (FG)"
    },
    {
      "citation_id": "34",
      "title": "Going deeper into action recognition: A survey",
      "authors": [
        "S Herath",
        "M Harandi",
        "F Porikli"
      ],
      "year": "2017",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "35",
      "title": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping",
      "authors": [
        "U Bhattacharya",
        "C Roncal",
        "T Mittal",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping"
    },
    {
      "citation_id": "36",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Thirty-second AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "37",
      "title": "A large-scale varying-view rgb-d action dataset for arbitrary-view human action recognition",
      "authors": [
        "Y Ji",
        "F Xu",
        "Y Yang",
        "F Shen",
        "H Shen",
        "W.-S Zheng"
      ],
      "year": "2019",
      "venue": "A large-scale varying-view rgb-d action dataset for arbitrary-view human action recognition",
      "arxiv": "arXiv:1904.10681"
    },
    {
      "citation_id": "38",
      "title": "Learning perceived emotion using affective and deep features for mental health applications",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Learning perceived emotion using affective and deep features for mental health applications"
    },
    {
      "citation_id": "39",
      "title": "Learning 3d human pose from structure and motion",
      "authors": [
        "R Dabral",
        "A Mundhada"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "40",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "41",
      "title": "The effect of facial expressions on peripersonal and interpersonal spaces",
      "authors": [
        "G Ruggiero",
        "F Frassinetti",
        "Y Coello",
        "M Rapuano",
        "A Di Cola",
        "T Iachini"
      ],
      "year": "2017",
      "venue": "Psychological research"
    },
    {
      "citation_id": "42",
      "title": "Personal space, evasive movement and pedestrian level of service",
      "authors": [
        "S Kim",
        "J Choi",
        "S Kim",
        "R Tay"
      ],
      "year": "2014",
      "venue": "Journal of advanced transportation"
    },
    {
      "citation_id": "43",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "44",
      "title": "Recognizing emotions conveyed by human gait",
      "authors": [
        "G Venture",
        "H Kadone",
        "T Zhang",
        "J Grèzes",
        "A Berthoz",
        "H Hicheur"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "45",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "M Daoudi",
        "S Berretti",
        "P Pala",
        "Y Delevoye",
        "A Del Bimbo"
      ],
      "year": "2017",
      "venue": "International Conference on Image Analysis and Processing"
    },
    {
      "citation_id": "46",
      "title": "Identifying emotions from noncontact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)"
    },
    {
      "citation_id": "48",
      "title": "Enhanced skeleton visualization for view invariant human action recognition",
      "authors": [
        "M Liu",
        "H Liu",
        "C Chen"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    }
  ]
}