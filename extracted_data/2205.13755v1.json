{
  "paper_id": "2205.13755v1",
  "title": "Acoustic-To-Articulatory Speech Inversion With Multi-Task Learning",
  "published": "2022-05-27T03:52:25Z",
  "authors": [
    "Yashish M. Siriwardena",
    "Ganesh Sivaraman",
    "Carol Espy-Wilson"
  ],
  "keywords": [
    "acoustic-to-articulatory speech inversion",
    "multitask learning",
    "acoustic-to-phoneme mapping",
    "biGRNNs"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-task learning (MTL) frameworks have proven to be effective in diverse speech related tasks like automatic speech recognition (ASR) and speech emotion recognition. This paper proposes a MTL framework to perform acoustic-to-articulatory speech inversion by simultaneously learning an acoustic to phoneme mapping as a shared task. We use the Haskins Production Rate Comparison (HPRC) database which has both the electromagnetic articulography (EMA) data and the corresponding phonetic transcriptions. Performance of the system was measured by computing the correlation between estimated and actual tract variables (TVs) from the acoustic to articulatory speech inversion task. The proposed MTL based Bidirectional Gated Recurrent Neural Network (RNN) model learns to map the input acoustic features to nine TVs while outperforming the baseline model trained to perform only acoustic to articulatory inversion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human speech production is a highly complex task which involves synchronized motor control of speech articulators. The inverse problem of determining the trajectories of the movement of speech articulators from the speech signal is referred to as acoustic-to-articulatory speech inversion  [1, 2] . This mapping from acoustics to articulation is an ill-posed problem which is known to be highly non-linear and non-unique  [3] . However, developing Speech Inversion (SI) systems have gained attention over the recent years mainly due to its potential in a wide range of speech applications like Automatic Speech Recognition (ASR)  [4, 5, 6] , speech synthesis  [7, 8] , speech therapy  [9]  and most recently with detecting mental health disorders like Major Depressive Disorder and Schizophrenia  [10, 11] . Real articulatory data are collected by techniques like X-ray microbeam  [12] , Electromagnetic Articulometry (EMA)  [13]  and real-time Magnetic Resonance Imaging (rt-MRI)  [14] . All these techniques are expensive, time consuming and need specialized equipment for observing articulatory movements directly  [1] . This explains why developing a speaker-independent SI system that can accurately estimate articulatory features for any unseen speaker is of greater need.\n\nOver the past few years, deep neural network (DNN) based models have propelled the development of SI systems to new heights. Bidirectional LSTMs (BiLSTMS)  [15, 16] , CNN-BiLSTMs  [17, 18] , Temporal Convolutional Networks (TCN)  [19]  and transformer models  [20]  have gained state-of-the-art results with multiple articulatory datasets  [21] . To further improve the speech inversion task, people have tried incorporating phonetic transcriptions as an input along with acoustic features  [22, 17] . One of the limitations of these models is that you need phonetic transcriptions of the speech audio file at the time of inference. To address this issue while also leveraging on the additional information that phonetic transcriptions offer, we propose a Bidirectional Gated Recurrent Neural Network (BiGRNN) model, implemented with a multi-task learning framework, to perform acoustic-to-articulatory speech inversion. The MTL based model does not need phonetic transcriptions at the time of inference, but benefits from learning the mapping from acoustics-to-phonetics to improve generalizability of SI systems.\n\nThe key contributions of the paper can be listed as follows :\n\n• We propose a MTL based BiGRNN model to perform acoustic-to articulatory speech inversion by also learning a shared task of acoustic-to-phoneme inversion.\n\n• We compare and contrast two training algorithms to optimize the proposed MTL model\n\n• By conducting an ablation study we assert the importance of doing multi-task learning for speech inversion",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Inversion System",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Task Learning : Related Work",
      "text": "The idea of Multi-task learning (MTL) was formally presented by Caruana et al.  [23]  as an inductive transfer mechanism with the principle goal of improving generalization capability of Machine Learning (ML) models. MTL helps improve generalizability of ML models by leveraging domain-specific information of training data which can be used in related tasks. Effectively, what happens is that the training data for the parallel task serve as an inductive bias  [23] . MTL has also been utilized as a solution for the data sparsity problem where one task has a limited number of labeled data and training individual models for each task is difficult. From this perspective, MTL is a useful tool which can reuse the existing knowledge and reduce the cost of collecting challenging datasets (e.g. articulatory datasets).\n\nThe secret behind the success of MTL lies with the use of more data from different learning tasks compared to learning a single task, hence learning better representations and reducing the risk for overfitting  [24] . MTL has widely been used in computer vision and a recent work  [25]  has implemented a MTL model to work on 12 different datasets while achieving the state-of-the-art with 11 of them. MTL has also been explored in Automatic Speech Recognition (ASR) tasks  [26, 27] , text-to-speech (TTS)  [28]  and in speech emotion recognition (SER)  [29, 30] . Cai et al  [30]  recently presented the state-of-the-art results for the SER task with IEMO-CAP dataset using their model based on a MTL framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Description",
      "text": "We used the Haskins Production Rate Comparison (HPRC) database which contains recordings from 4 female and 4 male    [31]  at normal and fast production rates  [21] . The recordings were done using a 5-D electromagnetic articulometry (EMA) system (WAVE; Northern Digital). First, every sentence was produced at speaker's preferred 'normal' speaking rate and then a 'fast' repetition of the same, without making errors. Sensors were placed on the tongue (tip (TT), body (TB), root (TR)), lips (upper (UL) and lower (LL)) and mandible, together with sensors on the left and right mastoids, and upper and lower incisors (UI, LI). These EMA trajectories were obtained at 100 Hz and then were low-pass filtered at 5 Hz for references and 20 Hz for articulator sensors. Synchronized audio was recorded at 22050 Hz. The following geometric transformations were used to obtain 9 TVs (namely Lip Aperture (LA), Lip Protrusion (LP), Tongue Body Constriction Location (TBCL), Tongue Body Constriction Degree (TBCD), Tongue Tip Constriction Location (TTCL), Tongue Tip Constriction Degree (TTCD), Jaw Angle (JA), Tongue Middle Constriction Location (TMCL) and Tongue Middle Constriction Degree (TMCD)). The equations to compute the geometric transformations are presented in our previous work in  [1, 32] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Features",
      "text": "All the audio files from the HPRC dataset (both 'normal' and 'fast' rate) are first segmented into 2 second long segments and the shorter audios are zero padded at the end. Previous studies with developing SI systems have shown MFCCs to be superior over conventional Melspectrograms and Perceptual Linear Predictions (PLPs) as acoustic features  [1] . Based on that, in this study, we use Mel-Frequency Cepstral Coefficients (MFCCs) as the input audio feature for the proposed SI systems. MFCCs are extracted using a 20ms Hamming analysis window with a 10ms frame shift. 13 cepstral coefficients were extracted for each frame while 40 Mel frequencies were used. Each MFCC was utterance wise normalized (z-normalized) prior to model training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Phoneme Features",
      "text": "The HPRC dataset contains phonetic alignment for the recorded utterances. The phone alignment is extracted using the Penn Phonetics Lab Forced Aligner(P2FA) 1 . We remove the allophonic variations of the monophones and retain only 40 monophone units. Using the forced alignment we created frame wise monophone labels for all of the HPRC dataset. The one-hot encoded frame-wise monophone labels are the phonetic features used in this study. 1 https://github.com/jaekookang/",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "In this paper, we use a novel Bidirectional Gated Recurrent Neural Network (BiGRNN) model to implement both the single-task and multi-task SI systems. Both the single-task and multi-task models have the same backbone which includes 3 bidirectional layers of Gated Recurrent Units (GRUs) followed by a time distributed fully connected layer. Single-task model which predicts TVs has an additional time distributed fully connected layer to predict the TVs (output layer). On the other hand, the multi-task model has two output layers, one a time distributed fully connected layer to predict the TVs and the other a softmax layer to predict phoneme labels. Figure  1  shows the architecture of the single task model on the left and the multi-task model on the right.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Performance Measurements",
      "text": "All the models are evaluated with the Pearson Product Moment Correlation (PPMC) scores computed between the estimated TVs and the corresponding ground-truth TVs. Equation 1 is used to computed the PPMC score, where X represents the estimated TVs, X the mean of the estimated TVs, Y the ground-truth TVs, Y the mean of the ground-truth TVs and N the number of TVs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Training",
      "text": "The HPRC dataset was divided into training, development, and testing sets, so that the training set has utterances from 6 speakers (3 Males, 3 Females) and the development and testing sets have utterances of 2 speakers (1 male,1 female) equally split between them. None of the subjects in training are present in the development and testing sets and hence all the models are trained in a 'speaker-independent' fashion. The split also ensured that around 80% of the total number of utterances were present in the training, and the development and testing sets have a nearly equal number of utterances. This allocation was done in a completely random manner.\n\nAll the models were implemented with Tensorflow-Keras machine learning framework and trained with NVIDIA TITAN X GPUs. For all single-task and multi-task models, ADAM optimizer with a starting learning rate of 1e-3 and an exponential learning rate scheduler was used. The starting learning rate was maintained up to 10 epochs and then decayed exponentially after each subsequent 5 epochs. To choose the best starting 'learning rate' (LR), we did a grid search on [1e-3, 3e-4, 1e-4] and to choose the training batch size we did a similar grid search on  [16, 32, 64, 128] . The best PPMC scores were obtained for 1e-3 as the LR and 128 as the batch size for training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Paradigms For Multi-Task Si Systems",
      "text": "We experimented with two distinct training algorithms to optimize the MTL model. We denote the input MFCC features to the model as xǫR L×d where L (=200) is the number of samples in each utterance and d (=13) is the number of MFCCs. Let f φ be the mapping from MFCCs to TVs from the multi-task model where φ defines the shared model parameters to be learned. Similarly, let g φ be the mapping from MFCCs to phoneme logits. Then the output TV prediction from the TV output layer ŷtvǫR\n\nL×T can be defined from equation 2 and similarly the output logits from the phoneme prediction, ŷph ǫR L×V can be defined from eqaution 3. Here T (=9) is the number of TVs predicted and V (=41) is the number of phonemes in the dictionary + the symbol for zeros (padded for shorter utterances). We used the Mean Absolute Error (MAE) loss between ground truth TVs ytv and predicted TVs ŷtv and cross entropy error loss between ground truth one-hot encoding labels of phonemes y ph and the predicted phonemes ŷph .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Algorithm 1",
      "text": "Here the multi-task model is optimized for each task in an alternating fashion. In each epoch, the model weights φ are first learned from the TV prediction task and the learned weights are then used for computing phoneme labels ŷph . The final model weights φ[i] * are then updated with the phoneme prediction task and the process is repeated for the given number of Epochs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Algorithm 1 Iterative Loss Optimization",
      "text": "Require: : xǫR L×d , y ph , ytv, Epochs(ǫR)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Algorithm 2",
      "text": "In this training algorithm we optimize a joint loss Ljoint, where the phoneme prediction loss L ph is weighted to combine with the TV prediction loss Ltv. The contribution of L ph is controlled by the weight αǫ(0, 1), which is a hyper-parameter to be tuned. Here the model is trained with an early stopping criteria monitoring the validation loss (V alLoss) with a patience p (=10).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Single-Task Vs Multi-Task Learning For Tv Prediction",
      "text": "Table  1  shows the results of the single-task model when compared to the two multi-task models trained with two training algorithms. The reported PPMC scores are from evaluations of the speaker-independent test set. Figure  2  shows ground-truth TVs and predicted TVs, LA, TBCD, TTCD and TMCD for an example utterance estimated by the multi-task and the singletask models. Figure  3  shows ground-truth TVs and predicted Algorithm 2 Joint Loss Optimization Require: : xǫR L×d , V alLoss, pǫR, α(0 < α < 1), y ph , ytv while\n\nTVs, LP, TBCL, TTCL and TMCL for the same utterance estimated by the multi-task and the single-task models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Comparison On Previous Work With Hprc Dataset",
      "text": "Table  2  lists the reported PPMC scores of the work in Shahrebabaki et. al.  [2, 17]  for speaker-independent SI task using the HPRC dataset. Both the studies and our study use same target TVs (derived from same transformations  [1] ) and use both 'normal' and 'fast' rate utterances without any speaker rate matching at evaluations. The comparison is still not a perfect one given that there can be differences in the test splits used for evaluation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "We changed the weight α in the MTL model trained with algorithm 2 to explore how the phoneme learning task would help the desired SI task. Recall that α controls the amount of contribution from the phoneme prediction loss L ph to the joint loss Ljoint. Here setting α = 0 is equivalent to the single-task model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Discussion",
      "text": "The results in Table  1  clearly confirms the impact of multi-task learning for the SI task with a relative improvement of 2.5% over the single-task model. Over the two training algorithms, algorithm 2 has a slight edge in TV prediction. However, when training time for the two algorithms are considered (table  4 ), algorithm 2 has a considerable advantage by only taking nearly quarter of the time of algorithm 1. Hence for the subsequent experiments and comparisons we used the MTL model trained with algorithm 2. It should also be mentioned that in a previous work with developing a multi-corpus SI system  [32] , a similar training procedure to algorithm 1 was used. Figure  2  and figure  3  shows the ground-truth TVs and the predicted TVs from the multi-task and single-task models. The key difference between the two figures is that figure  2  shows the TVs which characterise the constriction degree of articulators, whereas figure  3  shows TVs which characterizes the constriction location. It is usually observed that SI systems tend to do better with constriction degree related TVs compared to ones which capture constriction location mainly due to the fact that the same speech sound can be produced with different vocal tract configurations (speaker-dependent characteristics). The same can be observed with the PPMC scores for each TV in Table  1 . But an interesting observation is that the multi-task models mostly improve in estimating location related TVs with respect to the single-task model suggesting that learning the Table  2  shows that the proposed MTL based SI system achieves the best PPMC scores over the existing SI systems on the HPRC dataset. The results also suggest that the BiGRNN and the CNN-BiLSTM models clearly outperform the conventional feed-forward neural network models in the SI task. Moreover, with the ablation study in section 3.3, we show the importance of multi-task learning (i.e. learning a related, shared task) on improving the SI systems for TV prediction. This also suggests that with the joint loss Ljoint optimization, an additional hyper-parameter α needs to be fine-tuned properly to achieve the best results.\n\nFinally, it should also be highlighted that the proposed MTL based SI system only uses phoneme transcriptions for training. At the time of inference, only the acoustic features are needed and it draws the key difference between the proposed SI system and the SI systems using both phoneme and acoustic features as inputs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Future Work",
      "text": "The lack of larger articulatory datasets for training DNN based models is a key challenge in developing generalizable SI systems. One of the envisions of developing a MTL based SI system lies with the idea of tapping into larger, existing datasets of audio, phonetic transcriptions (e.g. Librispeech  [33] ). The authors wish to work on transfer-learning and model adaptation paradigms to improve the current MTL framework by pretraining the models with existing corpora of audio, phonetic transcriptions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "This work was supported by the National Science Foundation grant #1764010",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Single-task and Multi-task model architectures",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the ar-",
      "page": 2
    },
    {
      "caption": "Figure 2: shows ground-truth",
      "page": 3
    },
    {
      "caption": "Figure 3: shows ground-truth TVs and predicted",
      "page": 3
    },
    {
      "caption": "Figure 2: LA and constriction degree TV plots for the utterance",
      "page": 4
    },
    {
      "caption": "Figure 2: and ﬁgure 3 shows the ground-truth TVs and the",
      "page": 4
    },
    {
      "caption": "Figure 3: LP and constriction location TV plots for the utter-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "Abstract"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "Multi-task learning (MTL)\nframeworks have proven to be ef-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "fective in diverse speech related tasks\nlike automatic speech"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "recognition (ASR) and speech emotion recognition. This paper"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "proposes a MTL framework to perform acoustic-to-articulatory"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "speech\ninversion\nby\nsimultaneously\nlearning\nan\nacoustic\nto"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "phoneme mapping as a shared task. We use the Haskins Pro-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "duction Rate Comparison (HPRC) database which has both"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "the electromagnetic articulography (EMA) data and the corre-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "sponding phonetic transcriptions.\nPerformance of\nthe system"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "was measured by computing the correlation between estimated"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "and actual tract variables (TVs) from the acoustic to articulatory"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "speech inversion task. The proposed MTL based Bidirectional"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "Gated Recurrent Neural Network (RNN) model\nlearns to map"
        },
        {
          "2Pindrop, GA, USA": "the input acoustic features to nine TVs while outperforming the"
        },
        {
          "2Pindrop, GA, USA": "baseline model\ntrained to perform only acoustic to articulatory"
        },
        {
          "2Pindrop, GA, USA": "inversion."
        },
        {
          "2Pindrop, GA, USA": "Index Terms: acoustic-to-articulatory speech inversion, multi-"
        },
        {
          "2Pindrop, GA, USA": "task learning, acoustic-to-phoneme mapping, biGRNNs"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "1.\nIntroduction"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "Human speech production is a highly complex task which in-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "volves synchronized motor control of speech articulators. The"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "inverse problem of determining the trajectories of the movement"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "of speech articulators from the speech signal\nis referred to as"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "acoustic-to-articulatory speech inversion [1, 2]. This mapping"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "from acoustics to articulation is an ill-posed problem which is"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "known to be highly non-linear and non-unique [3]. However,"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "developing Speech Inversion (SI)\nsystems have gained atten-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "tion over the recent years mainly due to its potential\nin a wide"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "range of speech applications like Automatic Speech Recogni-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "tion (ASR)\n[4, 5, 6],\nspeech synthesis\n[7, 8],\nspeech therapy"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "[9] and most\nrecently with detecting mental health disorders"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "like Major Depressive Disorder\nand Schizophrenia\n[10, 11]."
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "Real articulatory data are collected by techniques like X-ray mi-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "crobeam [12], Electromagnetic Articulometry (EMA) [13] and"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "real-time Magnetic Resonance Imaging (rt-MRI) [14]. All these"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "techniques are expensive, time consuming and need specialized"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "equipment\nfor observing articulatory movements directly [1]."
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "This explains why developing a speaker-independent SI system"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "that can accurately estimate articulatory features for any unseen"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "speaker is of greater need."
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "Over the past few years, deep neural network (DNN) based"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "models have propelled the development of SI systems to new"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "heights.\nBidirectional LSTMs\n(BiLSTMS)\n[15, 16], CNN-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "BiLSTMs [17, 18], Temporal Convolutional Networks (TCN)"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "[19] and transformer models [20] have gained state-of-the-art"
        },
        {
          "2Pindrop, GA, USA": "results with multiple articulatory datasets [21]. To further im-"
        },
        {
          "2Pindrop, GA, USA": ""
        },
        {
          "2Pindrop, GA, USA": "prove the speech inversion task, people have tried incorporat-"
        },
        {
          "2Pindrop, GA, USA": "ing phonetic transcriptions as an input along with acoustic fea-"
        },
        {
          "2Pindrop, GA, USA": "tures [22, 17]. One of\nthe limitations of\nthese models is that"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "In\nthis\npaper, we\nuse\na\nnovel Bidirectional Gated Recur-"
        },
        {
          "2.5. Model Architecture": "rent Neural Network (BiGRNN) model\nto implement both the"
        },
        {
          "2.5. Model Architecture": "single-task and multi-task SI systems. Both the single-task and"
        },
        {
          "2.5. Model Architecture": "multi-task models have the same backbone which includes 3"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "bidirectional\nlayers of Gated Recurrent Units (GRUs) followed"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "by a time distributed fully connected layer. Single-task model"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "which predicts TVs has an additional time distributed fully con-"
        },
        {
          "2.5. Model Architecture": "nected layer\nto predict\nthe TVs (output\nlayer).\nOn the other"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "hand, the multi-task model has two output layers, one a time dis-"
        },
        {
          "2.5. Model Architecture": "tributed fully connected layer to predict the TVs and the other a"
        },
        {
          "2.5. Model Architecture": "softmax layer to predict phoneme labels. Figure 1 shows the ar-"
        },
        {
          "2.5. Model Architecture": "chitecture of the single task model on the left and the multi-task"
        },
        {
          "2.5. Model Architecture": "model on the right."
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "2.6. Performance Measurements"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "All\nthe models are evaluated with the Pearson Product Mo-"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "ment Correlation (PPMC) scores computed between the esti-"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "mated TVs and the corresponding ground-truth TVs. Equation"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "1 is used to computed the PPMC score, where X represents"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "the estimated TVs, X the mean of\nthe estimated TVs, Y the"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "ground-truth TVs, Y the mean of the ground-truth TVs and N"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "the number of TVs."
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "N i\n(X[i] − X)(Y [i] − Y )"
        },
        {
          "2.5. Model Architecture": "(1)\nP P M C =\nP"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "N i\n(X[i] − X)2(Y [i] − Y )2"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "qP"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "2.7. Model training"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "The HPRC dataset was divided into training, development, and"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "testing sets, so that the training set has utterances from 6 speak-"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "ers (3 Males, 3 Females) and the development and testing sets"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "have utterances of 2 speakers (1 male,1 female) equally split"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "between them. None of\nthe subjects in training are present\nin"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "the development and testing sets and hence all\nthe models are"
        },
        {
          "2.5. Model Architecture": "trained in a ‘speaker-independent’\nfashion.\nThe split also en-"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "sured that around 80% of\nthe total number of utterances were"
        },
        {
          "2.5. Model Architecture": "present\nin the training,\nand the development and testing sets"
        },
        {
          "2.5. Model Architecture": "have a nearly equal number of utterances. This allocation was"
        },
        {
          "2.5. Model Architecture": "done in a completely random manner."
        },
        {
          "2.5. Model Architecture": "All\nthe models were implemented with Tensorﬂow-Keras"
        },
        {
          "2.5. Model Architecture": "machine learning framework and trained with NVIDIA TITAN"
        },
        {
          "2.5. Model Architecture": "X GPUs. For all single-task and multi-task models, ADAM op-"
        },
        {
          "2.5. Model Architecture": "timizer with a starting learning rate of 1e-3 and an exponential"
        },
        {
          "2.5. Model Architecture": "learning rate scheduler was used. The starting learning rate was"
        },
        {
          "2.5. Model Architecture": "maintained up to 10 epochs and then decayed exponentially af-"
        },
        {
          "2.5. Model Architecture": "ter each subsequent 5 epochs. To choose the best starting ‘learn-"
        },
        {
          "2.5. Model Architecture": "ing rate’ (LR), we did a grid search on [1e-3, 3e-4, 1e-4] and to"
        },
        {
          "2.5. Model Architecture": "choose the training batch size we did a similar grid search on"
        },
        {
          "2.5. Model Architecture": "[16,32,64,128]. The best PPMC scores were obtained for 1e-3"
        },
        {
          "2.5. Model Architecture": "as the LR and 128 as the batch size for training."
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "2.8. Training Paradigms for multi-task SI systems"
        },
        {
          "2.5. Model Architecture": ""
        },
        {
          "2.5. Model Architecture": "We experimented with two distinct\ntraining algorithms to opti-"
        },
        {
          "2.5. Model Architecture": "mize the MTL model. We denote the input MFCC features to"
        },
        {
          "2.5. Model Architecture": "the model as xǫRL×d where L (=200) is the number of samples"
        },
        {
          "2.5. Model Architecture": "in each utterance and d (=13) is the number of MFCCs. Let fφ"
        },
        {
          "2.5. Model Architecture": "be the mapping from MFCCs to TVs from the multi-task model"
        },
        {
          "2.5. Model Architecture": "where φ deﬁnes\nthe shared model parameters\nto be learned."
        },
        {
          "2.5. Model Architecture": "Similarly, let gφ be the mapping from MFCCs to phoneme log-"
        },
        {
          "2.5. Model Architecture": "its.\nThen the output TV prediction from the TV output\nlayer"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "Multi-task (Algo 2)\n0.794\n0.680\n0.806\n0.741",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "0.770\n0.797\n0.775\n0.806\n0.762\n0.766"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "output\ncan be\nlogits from the phoneme prediction, ˆyphǫRL×V",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "Algorithm 2 Joint Loss Optimization"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "deﬁned from eqaution 3. Here T (=9) is the number of TVs pre-",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "Require:\n: xǫRL×d, V alLoss, pǫR, α(0 < α < 1), yph, ytv"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "dicted and V (=41) is the number of phonemes in the dictionary",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "while V alLoss[i] < V alLoss[i − p] do"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "+ the symbol for zeros (padded for shorter utterances). We used",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "yph ← gφ[i−1](x)"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "the Mean Absolute Error (MAE) loss between ground truth TVs",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "ytv ← fφ[i−1](x)"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "ytv and predicted TVs ˆytv and cross entropy error loss between",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "Lph ← CrossEntropy(ˆyph, yph)"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "ground truth one-hot encoding labels of phonemes yph and the",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "Ltv ← M AE(ˆytv, ytv)"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "predicted phonemes ˆyph.",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "Ljoint ← Ltv + αLph"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "φ[i] ← minφ Ljoint"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "(2)\nytv = fφ(x) ; xǫRL×d",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "i ← i + 1"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "end while"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "(3)\nyph = gφ(x) ; xǫRL×d",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "2.8.1.\nTraining Algorithm 1",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "TVs, LP, TBCL, TTCL and TMCL for the same utterance esti-"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "mated by the multi-task and the single-task models."
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "Here the multi-task model\nis optimized for each task in an al-",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "ternating fashion.\nIn each epoch,\nthe model weights φ are ﬁrst",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "3.2. Baseline\ncomparison on previous work with HPRC"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "learned from the TV prediction task and the learned weights are",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "dataset"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "then used for computing phoneme labels ˆyph. The ﬁnal model",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "weights φ[i]∗ are then updated with the phoneme prediction task",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "Table 2 lists the reported PPMC scores of the work in Shahre-"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "and the process is repeated for the given number of Epochs.",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "babaki et. al.\n[2, 17] for speaker-independent SI task using the"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "HPRC dataset. Both the studies and our study use same target"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "TVs (derived from same transformations [1]) and use both ‘nor-"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "Algorithm 1 Iterative Loss Optimization",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": ""
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "mal’ and ‘fast’ rate utterances without any speaker rate match-"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "Require:\n: xǫRL×d, yph, ytv, Epochs(ǫR)",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "ing at evaluations.\nThe comparison is still not a perfect one"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "while i < Epochs do",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "given that\nthere can be differences\nin the test\nsplits used for"
        },
        {
          "Multi-task (Algo 1)\n0.792\n0.681\n0.796\n0.747": "ytv ← fφ[i−1](x)",
          "0.793\n0.775\n0.799\n0.760\n0.764\n0.767": "evaluation."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(=10).": "",
          "model.": ""
        },
        {
          "(=10).": "3. Experiments and Results",
          "model.": ""
        },
        {
          "(=10).": "3.1.\nSingle-task vs Multi-task Learning for TV prediction",
          "model.": ""
        },
        {
          "(=10).": "",
          "model.": ""
        },
        {
          "(=10).": "Table 1 shows the results of the single-task model when com-",
          "model.": ""
        },
        {
          "(=10).": "",
          "model.": "α = 0.0"
        },
        {
          "(=10).": "pared to the two multi-task models\ntrained with two training",
          "model.": ""
        },
        {
          "(=10).": "",
          "model.": "α = 0.1"
        },
        {
          "(=10).": "algorithms. The reported PPMC scores are from evaluations of",
          "model.": ""
        },
        {
          "(=10).": "",
          "model.": "α = 0.3"
        },
        {
          "(=10).": "the speaker-independent\ntest set. Figure 2 shows ground-truth",
          "model.": "α = 0.5"
        },
        {
          "(=10).": "",
          "model.": ""
        },
        {
          "(=10).": "TVs and predicted TVs, LA, TBCD, TTCD and TMCD for an",
          "model.": ""
        },
        {
          "(=10).": "",
          "model.": "α = 0.8"
        },
        {
          "(=10).": "example utterance estimated by the multi-task and the single-",
          "model.": ""
        },
        {
          "(=10).": "",
          "model.": "α = 1.0"
        },
        {
          "(=10).": "task models.\nFigure 3 shows ground-truth TVs and predicted",
          "model.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: But an interesting observation is that the multi-task",
      "data": [
        {
          "01": "-1",
          "0": "-1"
        },
        {
          "01": "0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2",
          "0": "0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2"
        },
        {
          "01": "Figure 2: LA and constriction degree TV plots for the utterance",
          "0": "Figure 3: LP and constriction location TV plots for the utter-"
        },
        {
          "01": "‘Write fast\nif you want\nto ﬁnish early’ estimated using Multi-",
          "0": "ance ‘Write fast\nif you want\nto ﬁnish early’ estimated using"
        },
        {
          "01": "task model and the Single-task model.\nSolid blue Line - ac-",
          "0": "Multi-task model and the Single-task model.\nSolid blue Line"
        },
        {
          "01": "tual TV (from HPRC database), red dotted line - estimated TV",
          "0": "- actual TV (from HPRC database), red dotted line - estimated"
        },
        {
          "01": "from Multi-task model, black dashed Line - estimated TV from",
          "0": "TV from Multi-task model, black dashed Line - estimated TV"
        },
        {
          "01": "Single-task model",
          "0": "from Single-task model"
        },
        {
          "01": "",
          "0": "phoneme mapping is helping the SI task with additional subject-"
        },
        {
          "01": "Table 4: Training Time : Single-task and Multi-task models",
          "0": "dependent\ninformation."
        },
        {
          "01": "",
          "0": "Table 2 shows\nthat\nthe proposed MTL based SI\nsystem"
        },
        {
          "01": "Model Type\nNo. of Trainable Parameters\nTraining Time",
          "0": ""
        },
        {
          "01": "",
          "0": "achieves the best PPMC scores over the existing SI systems on"
        },
        {
          "01": "Single-task\n2.19 M\n10 (±2) min",
          "0": "the HPRC dataset. The results also suggest\nthat\nthe BiGRNN"
        },
        {
          "01": "Multi-task (Algo 1)\n2.20 M\n61 (±5) min",
          "0": "and the CNN-BiLSTM models clearly outperform the conven-"
        },
        {
          "01": "Multi-task (Algo 2)\n2.20 M\n15 (±2) min",
          "0": ""
        },
        {
          "01": "",
          "0": "tional feed-forward neural network models in the SI task. More-"
        },
        {
          "01": "",
          "0": "over, with the ablation study in section 3.3, we show the impor-"
        },
        {
          "01": "",
          "0": "tance of multi-task learning (i.e.\nlearning a related, shared task)"
        },
        {
          "01": "4. Discussion",
          "0": "on improving the SI systems for TV prediction. This also sug-"
        },
        {
          "01": "",
          "0": "gests that with the joint loss Ljoint optimization, an additional"
        },
        {
          "01": "The results in Table 1 clearly conﬁrms the impact of multi-task",
          "0": ""
        },
        {
          "01": "",
          "0": "hyper-parameter α needs to be ﬁne-tuned properly to achieve"
        },
        {
          "01": "learning for\nthe SI\ntask with a relative improvement of 2.5%",
          "0": ""
        },
        {
          "01": "",
          "0": "the best results."
        },
        {
          "01": "over\nthe single-task model. Over\nthe two training algorithms,",
          "0": ""
        },
        {
          "01": "",
          "0": "Finally, it should also be highlighted that the proposed MTL"
        },
        {
          "01": "algorithm 2 has a slight edge in TV prediction. However, when",
          "0": ""
        },
        {
          "01": "",
          "0": "based SI system only uses phoneme transcriptions for training."
        },
        {
          "01": "training time for\nthe two algorithms are considered (table 4),",
          "0": ""
        },
        {
          "01": "",
          "0": "At\nthe time of inference, only the acoustic features are needed"
        },
        {
          "01": "algorithm 2 has a considerable advantage by only taking nearly",
          "0": ""
        },
        {
          "01": "",
          "0": "and it draws the key difference between the proposed SI system"
        },
        {
          "01": "quarter of\nthe time of algorithm 1. Hence for\nthe subsequent",
          "0": ""
        },
        {
          "01": "",
          "0": "and the SI systems using both phoneme and acoustic features as"
        },
        {
          "01": "experiments and comparisons we used the MTL model\ntrained",
          "0": ""
        },
        {
          "01": "",
          "0": "inputs."
        },
        {
          "01": "with algorithm 2. It should also be mentioned that in a previous",
          "0": ""
        },
        {
          "01": "work with developing a multi-corpus SI system [32], a similar",
          "0": ""
        },
        {
          "01": "",
          "0": "5. Future Work"
        },
        {
          "01": "training procedure to algorithm 1 was used.",
          "0": ""
        },
        {
          "01": "Figure 2 and ﬁgure 3 shows the ground-truth TVs and the",
          "0": ""
        },
        {
          "01": "",
          "0": "The lack of larger articulatory datasets for training DNN based"
        },
        {
          "01": "predicted TVs from the multi-task and single-task models. The",
          "0": ""
        },
        {
          "01": "",
          "0": "models is a key challenge in developing generalizable SI sys-"
        },
        {
          "01": "key difference between the two ﬁgures is that ﬁgure 2 shows the",
          "0": ""
        },
        {
          "01": "",
          "0": "tems. One of the envisions of developing a MTL based SI sys-"
        },
        {
          "01": "TVs which characterise the constriction degree of articulators,",
          "0": ""
        },
        {
          "01": "",
          "0": "tem lies with the idea of\ntapping into larger, existing datasets"
        },
        {
          "01": "whereas ﬁgure 3 shows TVs which characterizes the constric-",
          "0": ""
        },
        {
          "01": "",
          "0": "of audio, phonetic transcriptions (e.g. Librispeech [33]). The"
        },
        {
          "01": "tion location.\nIt\nis usually observed that SI systems tend to do",
          "0": ""
        },
        {
          "01": "",
          "0": "authors wish to work on transfer-learning and model adapta-"
        },
        {
          "01": "better with constriction degree related TVs compared to ones",
          "0": ""
        },
        {
          "01": "",
          "0": "tion paradigms to improve the current MTL framework by pre-"
        },
        {
          "01": "which capture constriction location mainly due to the fact\nthat",
          "0": ""
        },
        {
          "01": "",
          "0": "training the models with existing corpora of audio, phonetic"
        },
        {
          "01": "the same speech sound can be produced with different vocal",
          "0": ""
        },
        {
          "01": "",
          "0": "transcriptions."
        },
        {
          "01": "tract conﬁgurations\n(speaker-dependent\ncharacteristics).\nThe",
          "0": ""
        },
        {
          "01": "same can be observed with the PPMC scores for each TV in",
          "0": ""
        },
        {
          "01": "",
          "0": "6. Acknowledgements"
        },
        {
          "01": "Table 1.\nBut an interesting observation is that\nthe multi-task",
          "0": ""
        },
        {
          "01": "models mostly improve in estimating location related TVs with",
          "0": "This work was supported by the National Science Foundation"
        },
        {
          "01": "respect\nto the single-task model\nsuggesting that\nlearning the",
          "0": "grant #1764010"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "sen,\n“Sequence-to-Sequence\nArticulatory\nInversion\nThrough"
        },
        {
          "7. References": "[1] G. Sivaraman, V. Mitra, H. Nam, M. Tiede, and C. Espy-Wilson,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Time Convolution\nof Sub-Band Frequency Signals,”\nin Proc."
        },
        {
          "7. References": "“Unsupervised\nspeaker\nadaptation\nfor\nspeaker\nindependent",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Interspeech\n2020,\n2020,\npp.\n2882–2886.\n[Online]. Available:"
        },
        {
          "7. References": "The\nJournal\nof\nthe\nacoustic\nto articulatory\nspeech\ninversion,”",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "http://dx.doi.org/10.21437/Interspeech.2020-1140"
        },
        {
          "7. References": "Acoustical\nSociety\nof America,\nvol. 146,\nno. 1,\npp. 316–329,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "2019. [Online]. Available: https://doi.org/10.1121/1.5116130",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[18] A.\nIlla and P. K. Ghosh, “Representation learning using convo-"
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "lution neural network for acoustic-to-articulatory inversion,”\nin"
        },
        {
          "7. References": "[2] A. S. Shahrebabaki, G. Salvi, T. Svendsen,\nand S. M. Sinis-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "ICASSP 2019 - 2019 IEEE International Conference on Acous-"
        },
        {
          "7. References": "calchi, “Acoustic-to-articulatory mapping with joint optimization",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "tics, Speech and Signal Processing (ICASSP), 2019, pp. 5931–"
        },
        {
          "7. References": "of deep speech enhancement and articulatory inversion models,”",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "5935."
        },
        {
          "7. References": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[19] A. S. Shahrebabaki, S. M. Siniscalchi,\nand T. Svendsen, “Raw"
        },
        {
          "7. References": "cessing, vol. 30, pp. 135–147, 2022.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Speech-to-Articulatory Inversion by Temporal Filtering and Dec-"
        },
        {
          "7. References": "´\n[3] C. Qin and M.\nA. Carreira-Perpi˜n´an, “An empirical\ninvestigation",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "imation,” in Proc. Interspeech 2021, 2021, pp. 1184–1188."
        },
        {
          "7. References": "of the nonuniqueness in the acoustic-to-articulatory mapping.” In-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[20]\nS. Udupa, A. Roy, A. Singh, A. Illa, and P. K. Ghosh, “Estimating"
        },
        {
          "7. References": "terspeech, pp. 74–77, 2007.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Articulatory Movements in Speech Production with Transformer"
        },
        {
          "7. References": "[4]\nJ. Frankel and S. King, “Asr - articulatory speech recognition,” in",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Networks,” in Proc. Interspeech 2021, 2021, pp. 1154–1158."
        },
        {
          "7. References": "INTERSPEECH, 2001.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[21] M. Tiede, C. Y. Espy-Wilson, D. Goldenberg, V. Mitra, H. Nam,"
        },
        {
          "7. References": "[5] V. Mitra, H. Nam, C. Y. Espy-Wilson, E. Saltzman, and L. Gold-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "and G. Sivaraman, “Quantifying kinematic aspects of reduction in"
        },
        {
          "7. References": "stein, “Retrieving tract variables from acoustics: A comparison of",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "the Acoustical\na contrasting rate production task,” The Journal of"
        },
        {
          "7. References": "different machine learning strategies,” IEEE Journal on Selected",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Society\nof America,\nvol.\n141,\nno.\n5,\npp.\n3580–3580,\n2017."
        },
        {
          "7. References": "Topics\nin Signal Processing, vol. 4, no. 6, pp. 1027–1045,\nsep",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[Online]. Available: https://doi.org/10.1121/1.4987629"
        },
        {
          "7. References": "2010.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[22] A. Singh, A. Illa, and P. K. Ghosh, “A comparative study of es-"
        },
        {
          "7. References": "[6] V. Mitra, C. Y. Espy-Wilson, E. Saltzman,\nand L. Goldstein,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "timating articulatory movements\nfrom phoneme\nsequences\nand"
        },
        {
          "7. References": "“Articulatory Information for Noise Robust Speech Recognition,”",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "acoustic features,” ICASSP 2020 - 2020 IEEE International Con-"
        },
        {
          "7. References": "IEEE Transactions on Audio, Speech, and Language Processing,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "ference on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "7. References": "vol. 19, no. 7, pp. 1913–1924, sep 2011.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "pp. 7334–7338, 2020."
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[23] R. Caruana,\n“Multitask Learning,”\n1997.\n[Online]. Available:"
        },
        {
          "7. References": "[7]\nZ.-H. Ling, K. Richmond, and J. Yamagishi, “Articulatory con-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "https://doi.org/10.1023/A:1007379606734"
        },
        {
          "7. References": "trol of hmm-based parametric\nspeech\nsynthesis using feature-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "space-switched multiple regression,” IEEE Transactions on Au-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[24] Y. Zhang and Q. Yang, “A survey on multi-task learning,” IEEE"
        },
        {
          "7. References": "dio, Speech, and Language Processing, vol. 21, no. 1, pp. 207–",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Transactions on Knowledge and Data Engineering, pp. 1–1, 2021."
        },
        {
          "7. References": "219, 2013.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[25]\nJ. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee, “12-in-1:"
        },
        {
          "7. References": "[8] K. Richmond and S. King, “Smooth talking: Articulatory join",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Multi-task vision and language representation learning,” in 2020"
        },
        {
          "7. References": "costs for unit selection,” in 2016 IEEE International Conference",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "IEEE/CVF Conference on Computer Vision and Pattern Recogni-"
        },
        {
          "7. References": "on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "tion (CVPR).\nLos Alamitos, CA, USA: IEEE Computer Society,"
        },
        {
          "7. References": "5150–5154.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "jun 2020, pp. 10 434–10 443."
        },
        {
          "7. References": "[9]\nS. Fagel and K. Madany, “A 3-d virtual head as a tool for speech",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[26]\nS. Kim, T. Hori, and S. Watanabe, “Joint ctc-attention based end-"
        },
        {
          "7. References": "therapy for children,” in INTERSPEECH, 2008.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "to-end speech recognition using multi-task learning,” 2017 IEEE"
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "7. References": "[10] C. Espy-Wilson, A. C. Lammert, N. Seneviratne,\nand T. F.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "cessing (ICASSP), pp. 4835–4839, 2017."
        },
        {
          "7. References": "Quatieri, “Assessing Neuromotor Coordination in Depression Us-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "ing Inverted Vocal Tract Variables,”\nin Proc.\nInterspeech 2019,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[27]\nT. Hori, S. Watanabe, Y. Zhang, and W. Chan, “Advances in joint"
        },
        {
          "7. References": "2019, pp. 1448–1452.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "ctc-attention based end-to-end speech recognition with a deep cnn"
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "encoder and rnn-lm,” in INTERSPEECH, 2017."
        },
        {
          "7. References": "[11] Y. M. Siriwardena,\nC. Espy-Wilson, C. Kitchen,\nand D. L.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[28] C.-M. Chien,\nJ.-H. Lin, C.-y. Huang, P.-c. Hsu, and H.-y. Lee,"
        },
        {
          "7. References": "Approach\nfor\nAssessing Neuromotor Co-\nKelly, Multimodal",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "“Investigating on incorporating pretrained and learnable speaker"
        },
        {
          "7. References": "ordination\nin\nSchizophrenia\nUsing\nConvolutional\nNeural",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "representations\nfor multi-speaker multi-style\ntext-to-speech,”\nin"
        },
        {
          "7. References": "Networks.\nNew\nYork,\nNY,\nUSA:\nAssociation\nfor\nCom-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "ICASSP 2021 - 2021 IEEE International Conference on Acous-"
        },
        {
          "7. References": "puting Machinery,\n2021,\np.\n768–772.\n[Online].\nAvailable:",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "tics, Speech and Signal Processing (ICASSP), 2021, pp. 8588–"
        },
        {
          "7. References": "https://doi.org/10.1145/3462244.3479967",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "8592."
        },
        {
          "7. References": "[12]\nJ. R. Westbury,\n“Speech Production Database User\n’ S Hand-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[29] Y. Li, T. Zhao, and T. Kawahara, “Improved end-to-end speech"
        },
        {
          "7. References": "-\nbook,” IEEE Personal Communications\nIEEE Pers. Commun.,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "emotion recognition using self attention mechanism and multitask"
        },
        {
          "7. References": "vol. 0, no. June, 1994.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "learning,” in INTERSPEECH, 2019."
        },
        {
          "7. References": "[13]\nP. W. Sch¨onle, K. Gr¨abe, P. Wenig,\nJ. H¨ohne,\nJ. Schrader, and",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[30] X. Cai,\nJ. Yuan, R. Zheng, L. Huang, and K. Church, “Speech"
        },
        {
          "7. References": "B. Conrad, “Electromagnetic articulography: Use of alternating",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Emotion Recognition with Multi-Task Learning,” in Proc. Inter-"
        },
        {
          "7. References": "magnetic ﬁelds for tracking movements of multiple points inside",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "speech 2021, 2021, pp. 4508–4512."
        },
        {
          "7. References": "and outside the vocal\ntract,” Brain and Language, vol. 31, no. 1,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "pp. 26–35, may 1987.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[31]\n“Ieee recommended practice for speech quality measurements,”"
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "IEEE Transactions on Audio and Electroacoustics, vol. 17, no. 3,"
        },
        {
          "7. References": "[14]\nS. Narayanan, K. Nayak, S. Lee, A. Sethy, and D. Byrd,\n“An",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "pp. 225–246, 1969."
        },
        {
          "7. References": "approach\nto real-time magnetic\nresonance\nimaging for\nspeech",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "production,” The Journal of\nthe Acoustical Society of America,",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[32] N. Seneviratne, G. Sivaraman,\nand C. Espy-Wilson,\n“Multi-"
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "In-\nCorpus Acoustic-to-Articulatory Speech Inversion,” in Proc."
        },
        {
          "7. References": "vol. 115, no. 4, pp. 1771–1776, mar 2004.\n[Online]. Available:",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "terspeech 2019, 2019, pp. 859–863."
        },
        {
          "7. References": "http://asa.scitation.org/doi/10.1121/1.1652588",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "[33] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-"
        },
        {
          "7. References": "[15] A. Illa and P. K. Ghosh, “Low Resource Acoustic-to-articulatory",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "rispeech: An asr corpus based on public domain audio books,”"
        },
        {
          "7. References": "Inversion Using Bi-directional Long Short Term Memory,”\nin",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "in 2015 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "7. References": "Proc. Interspeech 2018, 2018, pp. 3122–3126.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": "Signal Processing (ICASSP), 2015, pp. 5206–5210."
        },
        {
          "7. References": "[16] Aravind Illa and Prasanta Kumar Ghosh, “Speaker Conditioned",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "Acoustic-to-Articulatory Inversion Using x-Vectors,” in Proc. In-",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        },
        {
          "7. References": "terspeech 2020, 2020, pp. 1376–1380.",
          "[17] A. S. Shahrebabaki, S. M. Siniscalchi, G. Salvi, and T. Svend-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Unsupervised speaker adaptation for speaker independent acoustic to articulatory speech inversion",
      "authors": [
        "G Sivaraman",
        "V Mitra",
        "H Nam",
        "M Tiede",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "The Journal of the Acoustical Society of America",
      "doi": "10.1121/1.5116130"
    },
    {
      "citation_id": "3",
      "title": "Acoustic-to-articulatory mapping with joint optimization of deep speech enhancement and articulatory inversion models",
      "authors": [
        "A Shahrebabaki",
        "G Salvi",
        "T Svendsen",
        "S Siniscalchi"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "An empirical investigation of the nonuniqueness in the acoustic-to-articulatory mapping",
      "authors": [
        "C Qin",
        "M Carreira-Perpiñán"
      ],
      "year": "2007",
      "venue": "An empirical investigation of the nonuniqueness in the acoustic-to-articulatory mapping"
    },
    {
      "citation_id": "5",
      "title": "Asr -articulatory speech recognition",
      "authors": [
        "J Frankel",
        "S King"
      ],
      "year": "2001",
      "venue": "Asr -articulatory speech recognition"
    },
    {
      "citation_id": "6",
      "title": "Retrieving tract variables from acoustics: A comparison of different machine learning strategies",
      "authors": [
        "V Mitra",
        "H Nam",
        "C Espy-Wilson",
        "E Saltzman",
        "L Goldstein"
      ],
      "year": "2010",
      "venue": "IEEE Journal on Selected Topics in Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Articulatory Information for Noise Robust Speech Recognition",
      "authors": [
        "V Mitra",
        "C Espy-Wilson",
        "E Saltzman",
        "L Goldstein"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Articulatory control of hmm-based parametric speech synthesis using featurespace-switched multiple regression",
      "authors": [
        "Z.-H Ling",
        "K Richmond",
        "J Yamagishi"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Smooth talking: Articulatory join costs for unit selection",
      "authors": [
        "K Richmond",
        "S King"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "A 3-d virtual head as a tool for speech therapy for children",
      "authors": [
        "S Fagel",
        "K Madany"
      ],
      "year": "2008",
      "venue": "A 3-d virtual head as a tool for speech therapy for children"
    },
    {
      "citation_id": "11",
      "title": "Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables",
      "authors": [
        "C Espy-Wilson",
        "A Lammert",
        "N Seneviratne",
        "T Quatieri"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Multimodal Approach for Assessing Neuromotor Coordination in Schizophrenia Using Convolutional Neural Networks",
      "authors": [
        "Y Siriwardena",
        "C Espy-Wilson",
        "C Kitchen",
        "D Kelly"
      ],
      "year": "2021",
      "venue": "Multimodal Approach for Assessing Neuromotor Coordination in Schizophrenia Using Convolutional Neural Networks",
      "doi": "10.1145/3462244.3479967"
    },
    {
      "citation_id": "13",
      "title": "Speech Production Database User ' S Handbook",
      "authors": [
        "J Westbury"
      ],
      "year": "1994",
      "venue": "IEEE Personal Communications -IEEE Pers. Commun"
    },
    {
      "citation_id": "14",
      "title": "Electromagnetic articulography: Use of alternating magnetic fields for tracking movements of multiple points inside and outside the vocal tract",
      "authors": [
        "P Schönle",
        "K Gräbe",
        "P Wenig",
        "J Höhne",
        "J Schrader",
        "B Conrad"
      ],
      "year": "1987",
      "venue": "Brain and Language"
    },
    {
      "citation_id": "15",
      "title": "An approach to real-time magnetic resonance imaging for speech production",
      "authors": [
        "S Narayanan",
        "K Nayak",
        "S Lee",
        "A Sethy",
        "D Byrd"
      ],
      "year": "2004",
      "venue": "The Journal of the Acoustical Society of America",
      "doi": "10.1121/1.1652588"
    },
    {
      "citation_id": "16",
      "title": "Low Resource Acoustic-to-articulatory Inversion Using Bi-directional Long Short Term Memory",
      "authors": [
        "A Illa",
        "P Ghosh"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Speaker Conditioned Acoustic-to-Articulatory Inversion Using x-Vectors",
      "authors": [
        "Aravind Illa",
        "Prasanta Kumar"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "18",
      "title": "Sequence-to-Sequence Articulatory Inversion Through Time Convolution of Sub-Band Frequency Signals",
      "authors": [
        "A Shahrebabaki",
        "S Siniscalchi",
        "G Salvi",
        "T Svendsen"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2020-1140"
    },
    {
      "citation_id": "19",
      "title": "Representation learning using convolution neural network for acoustic-to-articulatory inversion",
      "authors": [
        "A Illa",
        "P Ghosh"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Raw Speech-to-Articulatory Inversion by Temporal Filtering and Decimation",
      "authors": [
        "A Shahrebabaki",
        "S Siniscalchi",
        "T Svendsen"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Estimating Articulatory Movements in Speech Production with Transformer Networks",
      "authors": [
        "S Udupa",
        "A Roy",
        "A Singh",
        "A Illa",
        "P Ghosh"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "22",
      "title": "Quantifying kinematic aspects of reduction in a contrasting rate production task",
      "authors": [
        "M Tiede",
        "C Espy-Wilson",
        "D Goldenberg",
        "V Mitra",
        "H Nam",
        "G Sivaraman"
      ],
      "year": "2017",
      "venue": "The Journal of the Acoustical Society of America",
      "doi": "10.1121/1.4987629"
    },
    {
      "citation_id": "23",
      "title": "A comparative study of estimating articulatory movements from phoneme sequences and acoustic features",
      "authors": [
        "A Singh",
        "A Illa",
        "P Ghosh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Multitask Learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Multitask Learning",
      "doi": "10.1023/A:1007379606734"
    },
    {
      "citation_id": "25",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "26",
      "title": "12-in-1: Multi-task vision and language representation learning",
      "authors": [
        "J Lu",
        "V Goswami",
        "M Rohrbach",
        "D Parikh",
        "S Lee"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "Joint ctc-attention based endto-end speech recognition using multi-task learning",
      "authors": [
        "S Kim",
        "T Hori",
        "S Watanabe"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm",
      "authors": [
        "T Hori",
        "S Watanabe",
        "Y Zhang",
        "W Chan"
      ],
      "year": "2017",
      "venue": "Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm"
    },
    {
      "citation_id": "29",
      "title": "",
      "authors": [
        "C.-M Chien",
        "J.-H Lin"
      ],
      "venue": ""
    },
    {
      "citation_id": "30",
      "title": "Investigating on incorporating pretrained and learnable speaker representations for multi-speaker multi-style text-to-speech",
      "authors": [
        "P.-C Huang",
        "H.-Y Hsu",
        "Lee"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "32",
      "title": "Speech Emotion Recognition with Multi-Task Learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "33",
      "title": "Ieee recommended practice for speech quality measurements",
      "year": "1969",
      "venue": "IEEE Transactions on Audio and Electroacoustics"
    },
    {
      "citation_id": "34",
      "title": "Multi-Corpus Acoustic-to-Articulatory Speech Inversion",
      "authors": [
        "N Seneviratne",
        "G Sivaraman",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "35",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}