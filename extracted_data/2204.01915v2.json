{
  "paper_id": "2204.01915v2",
  "title": "An Exploration Of Active Learning For Affective Digital Phenotyping",
  "published": "2022-04-05T01:01:32Z",
  "authors": [
    "Peter Washington",
    "Cezmi Mutlu",
    "Aaron Kline",
    "Cathy Hou",
    "Kaitlyn Dunlap",
    "Jack Kent",
    "Arman Husic",
    "Nate Stockham",
    "Brianna Chrisman",
    "Kelley Paskov",
    "Jae-Yoon Jung",
    "Dennis P. Wall"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Some of the most severe bottlenecks preventing widespread development of machine learning models for human behavior include a dearth of labeled training data and difficulty of acquiring high quality labels. Active learning is a paradigm for using algorithms to computationally select a useful subset of data points to label using metrics for model uncertainty and data similarity. We explore active learning for naturalistic computer vision emotion data, a particularly heterogeneous and complex data space due to inherently subjective labels. Using frames collected from gameplay acquired from a therapeutic smartphone game for children with autism, we run a simulation of active learning using gameplay prompts as metadata to aid in the active learning process. We find that active learning using information generated during gameplay slightly outperforms random selection of the same number of labeled frames. We next investigate a method to conduct active learning with subjective data, such as in affective computing, and where multiple crowdsourced labels can be acquired for each image. Using the Child Affective Facial Expression (CAFE) dataset, we simulate an active learning process for crowdsourcing many labels and find that prioritizing frames using the entropy of the crowdsourced label distribution results in lower categorical cross-entropy loss compared to random frame selection. Collectively, these results demonstrate pilot evaluations of two novel active learning approaches for subjective affective data collected in noisy settings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Interactive mobile systems often generate large datasets of naturalistic behavior as a byproduct of their use  [20] [21] [22] [23] [24] , and these datasets can be leveraged to train new machine learning classifiers that may be more effective in real-time interactive systems. However, the data must first be labeled to be useful for machine learning. While data annotators could label the entirety of the dataset, this process can be infeasible for large and complex datasets. Neural networks take a long time to train -on the scale of days or weeks on the most powerful graphics processing units (GPUs) with large image datasets. This can result in thousands of dollars spent until training convergence. Active learning is a framework for identifying the most salient data points for humans to label in vast unlabeled datasets. Effective active learning strategies can drastically reduce the inefficiencies associated with labeling an entire dataset. An additional benefit of active learning is the generation of more focused and lightweight datasets with predictive power approaching that of the entire pool of data.\n\nActive learning techniques typically use a machine learning model trained on a smaller labeled dataset to identify unlabeled data points with a low classifier probability. Traditional approaches to active learning include uncertainty-based approaches, where data for which the classifier is probabilistically uncertain of the correct class are prioritized, as well as a representation-based approaches, where similar unlabeled data points (where similarity may be determined by unsupervised approaches) are prioritized  [37] [38] 57] . Early approaches to active learning for affective computing have demonstrated noticeable improvements. Ahmed et al. achieve 88% emotion detection accuracy for 7 emotions (happy, disgusted, sad, angry, surprised, fear, and neutral) when applying active learning compared to less than 73% when selecting frames at random using a custom dataset curated from webcam images  [1] . Their active learning method consists of evaluating mini batches of images with a pretrained model and replacing labels with a confidence below a static threshold. Thiam et al. also follows this paradigm by pretraining a background model on a dataset of emotional events and asking the human annotator to label examples that are not well explained by this background model, finding that 14% of the original dataset identified with active learning contains 75% of the total emotional events  [43] . Senechal et al. apply active learning to the labeling of facial action units in naturalistic videos collected from webcams  [36]  to create a computationally tractable radial basis function support vector machine classifier reaching performance above the previously published baseline on AM-FED, a dataset of naturalistic facial expressions  [31] .\n\nThe largest challenge in active learning research is that there is no one-size-fits-all approach for all datasets  [37] [38] . For example, maximum entropy active learning consistently outperforms random selection of labels on some datasets while performing drastically worse than random on others  [18, 35] . We have found that this issue is magnified when attempting to apply active learning on highly heterogeneous affective computer vision data.\n\nHere, we explore active learning for affective datasets collected from an at-home digital phenotyping application for children with autism. Digital therapeutics for pediatric autism which help children practice emotion evocation  [2, 5-7, 12-14, 26, 45-48, 61-62]  are increasing being developed and studied for at-home therapy for use by families without access to traditional healthcare services. The use of these therapies generates massive datasets which contain emotion evocations. Because the therapies are administered by parents in home settings, the datasets are noisy and heterogeneous.\n\nThe dataset we use here consists of frames extracted from video recorded by a mobile autism therapeutic application consisting of a Charades game played by parents and children called GuessWhat  [19, 20-24, 34, 49] . During game sessions, the parent places the phone on their forehead, facing the child. The child acts out the emotion prompt depicted on the device and the parent guesses the emotion performed by the child. If the parent tilts the phone forward, indicating a correct evocation of the emotion prompt by the child, then the corresponding frames receive the prompt as a metadata label. In the meantime, the front-facing camera of the phone records real time video of the interaction. We consider these \"automatic label\" metadata as potentially useful to the active learning process.\n\nWe run a post-hoc simulation of active learning on the dataset acquired from GuessWhat gameplay by ensuring that all emotions, as represented by the automatic label metadata, are equally represented. We try this approach with and without maximum entropy. Because we found marginal performance gains over random frame selection, we sought to understand why active learning was not more effective. We explore simulations with synthetic low-dimensional datasets to understand which dataset and model configurations fail during active learning procedures using noisy automatic labels. Finally, we recognize that emotion datasets often have subjective labels  [50, 56] , including in the GuessWhat dataset, so we pilot a crowdsourcingbased active learning algorithm to determine how many labels each data point should receive. This work provides preliminary explorations of the challenges and opportunities which arise in active learning with heterogeneous image datasets with noisy labels.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment 1: Active Learning Simulation Using Metadata From A Mobile Autism Therapeutic",
      "text": "We experiment with a dataset generated from GuessWhat, a smartphone app which provides athome therapy to children with autism  [20] [21] [22] [23] [24] 34] . During therapy sessions consisting of gameplay between a parent and child with autism the parent holds the smartphone on their forehead while the phone displays emotional Charades prompts to the child. If the child correctly evokes the emotion, then the parent tilts the phone forward, and this response is recorded using accelerometer sensors. The parent tilts the phone backwards if the child cannot correctly evoke the emotion. Throughout the gameplay session, GuessWhat collects video recordings of the child through the front-facing camera of the smartphone.\n\nThe gameplay sessions generate a video with timestamp labels marking the start and end frames of the prompt (Figure  1 ). Each frame is marked with the corresponding prompt on the GuessWhat session if any is present. These emotive metadata can be used as noisy automatic labels for training an emotion detection classifier. Here, we experiment with the potential of these metadata to aid in the active learning process. After acquiring ground truth labels for all frames as described in Washington et al.  [49] , we ran a post hoc simulation using our final labeled dataset to understand the most effective strategy for ordering the frames to converge test performance with as few manual labels as possible (Figure  2 ). We evaluate the following active learning strategies, where X is the set of unlabeled data frames and Y is the set of possible predictions:\n\n• Baseline: random frames. As a baseline without active learning, we order frames randomly. Sequential frames corresponding to a random emotion prompt from a random video session for a random child are consecutively displayed.\n\n• Automatic labels: cycle through (emotion, child) tuples. There is a drastic class imbalance in the emotions that children evoked during gameplay sessions. To balance the labeled data to the extent possible, we cycle through one random frame for each (automatic emotion label, child) tuple until all tuples have been exhausted. The order of cycling through (automatic emotion label, child) tuples is chosen randomly.\n\n• Confidence-based approach using automatic labels: cycle through (emotion, child) tuples with maximum entropy frames first. Same ordering as above, where (emotion, child) tuples are cycled through randomly, except instead of selecting a random (emotion, tuple) frame, the frame with the maximum entropy is selected. Entropy is defined as follows, where x is the matrix of pixels constituting a frame, y is the predicted class out of N possible emotion classes, and P(y|x) is the classifier's emitted probability of frame x having class y:\n\nFor each of these active learning strategies, we calculate a frame queue using that method and add the top 35 frames (5 per class) from the queue to the training set. We then train a classifier with the training set so far, recalculate the frame queue with the updated classifier, and repeat. We run 10 iterations of each active learning strategy.\n\nWe use a ResNet152V2  [17]  convolutional neural network pretrained on ImageNet weights  [8]  for all active learning conditions and strategies. The network was trained with Adam optimization  [25]  and categorical cross-entropy loss. The following data augmentations were applied at random: rotation range of up to 7 degrees, a brightness range between 70% and 130%, and horizontal flips.\n\nAt each iteration, we evaluate the model on the Child Affective Facial Expression (CAFE) dataset  [30] , a publicly available image dataset of children emoting \"anger\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", and \"surprise\" in controlled settings. CAFE contains 1,192 images of 90 females and 64 males. The racial distribution of subjects is: 27 African American, 16 Asian, 77 Caucasian/European American, 23 Latino, and 11 South Asian. To emphasize the importance of working towards creating machine learning models that minimize bias and work for all individuals, we also evaluate the model against a subset of CAFE balanced by race, gender, and emotion expression. This subset contains N=125 images and has been used in prior literature due to its demographic balance  [32] .\n\nFinally, we estimate how many frames are needed for the classifier to converge by fitting a power law distribution to the plot of classifier accuracy and F1 score vs. number of frames labeled. We use the scipy Python library to estimate parameters a, b, and c of the following equation, where x is the number of frames collected and f(x) is the accuracy or F1 score of the resulting classifier:\n\nExperiment 2: Active learning with soft target labels\n\nAffective computing datasets often contain subjective labels. One case of subjective labels is for compound emotions. For example, a subject may be depicting compound emotions such as \"happily surprised\", \"angrily surprised\", or \"fearfully surprised\"  [10] . In other cases, the label may be ambiguous. For example, some human raters may rate an image as depicting anger while others may rate it as disgusted  [50, 56] . Both answers could be correct, or the true expression might require additional context. To account for this type of subjectivity in emotional labels, training emotion classifiers with soft-target labels rather than one-hot encoded labels has been proposed and validated as a feasible method to create classifiers which output probability distributions which mimic the variation in human interpretation  [50, 56] .\n\nWe investigate active learning with such subjective data where multiple labels are required per image to construct a soft-targe label from the crowd. We use CAFE for an active learning simulation, as CAFE contains 100 independent human annotations per image. We evaluate on 3 separate folds of CAFE to verify robustness of results. Each of the 3 folds consists of two-thirds of CAFE in the training set and the rest in the test set. To avoid overfitting, we ensured that no child subject in the training set ever appeared in the test set for all folds. The final one-hot labels were determined by majority voting, with ties broken randomly.\n\nAt each iteration of the simulation, we randomly sample labels from the distribution of 100 human annotations provided in CAFE for all images in the CAFE dataset. A total of 3N labels are sampled per iteration, where N is the total number of images. In the baseline condition, we sample 3 labels per image. We train a ResNet classifier for 200 epochs with the same configuration as the ResNet in Experiment 1. To generate each number at each iteration, we take the mean and standard deviation of the held-out validation data for the final 50 epochs. Training accuracy converged prior to epoch 150 for all conditions, folds, and simulation iterations.\n\nIn the active learning condition, we sample 1 label for all images. For the remainder of labels, we sort the remaining frames by decreasing entropy of the crowd label distribution. For the frames in the top 10 th percentile of maximum entropy crowd labels, we sample 6 additional times (7 samples total). For the frames in the 10 th through 25 th percentile, we sample 4 additional times (5 samples total). For frames in the 25 th through 65 th percentile, we sample 2 additional times (3 samples total). The remaining frames do not receive additional labels at that iteration (1 sample total). This again results in 3N samples distributed in total in the active learning condition (N + 6*0.1*N + 4*0.15*N + 2*0.4*N = 3N).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Experiment 1: Active learning simulation using data from a mobile autism therapeutic\n\nWe first perform a simulation without including any public datasets, as training time to convergence would drastically increase with the additional number of training frames. We first evaluate against a subset of CAFE balanced by emotion, gender, and race, which we call the \"balanced CAFE subset\". This balanced CAFE subset has been evaluated in prior work.  [32] .\n\nThe network trained on all images achieved an overall accuracy of 84.8% accuracy and an 83.9% F1-score on the balanced CAFE subset. When excluding the Hollywood Squares dataset and only training using the RaFD, CK+, and JAFFE datasets, the resulting model achieves 70.4% accuracy on the balanced CAFE subset, highlighting the added value of the GuessWhat data.\n\nCycling through (user-derived emotion label, child) tuples and selecting the highest entropy frame satisfying that tuple outperforms selection of random frames by a large margin and outperforms cycling through tuples and choosing a random frame in that tuple by a smaller margin (Figure  3 ). Cycling through frame metadata with maximum entropy starts to reach near convergence at 4 iterations (140 frames) while cycling through frame metadata alone does no reach this point until at least iteration 8 (305 frames).  We next evaluate active learning on the entirety of CAFE. Figure  4  shows that cycling through (emotion autolabel, child) tuples and selecting the maximum entropy frame outperforms random baseline when evaluating on the entirety of CAFE, although the margin is not as dramatic as compared to the balanced CAFE subset. Interestingly, cycling through (emotion autolabel, child) tuples without choosing the maximum entropy frame performs worse than the random baseline. This is possibly due to the severe class imbalance of the full CAFE dataset. Figure  5  shows that cycling through (emotion, child) tuples and selecting the maximum entropy frame also outperforms random baseline when starting with a model pretrained on JAFFE and CK+.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiment 2: Active Learning With Soft Targets",
      "text": "We recorded the categorical cross-entropy loss when training and testing with one-hot encoded labels (Table  1 ), training and testing with soft-target labels (Table  S1 ), training with one-hot encoded labels and testing with soft-target labels (Table  S2 ), and training with soft-target labels and testing with one-hot encoded labels (Table  S3 ). We find that active learning with crowd labels consistently outperforms random cycling of frames when training and testing with one-hot encoded labels across all folds: in all 11 iterations for fold 1, in 9 out of 11 iterations in fold 2, and in 8 out of 11 iterations in fold 3.   1 . Categorical cross-entropy loss when training and testing with one-hot encoded labels after various iterations of frame labeling. The number of labeled frames is beyond the first 3 labels acquired per frame. Instances where the categorical cross-entropy loss is higher (by greater than 0.01) for active learning than equally labeling all frames are highlighted in red.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Fold",
      "text": "By contrast, active learning did not demonstrate any benefit over random cycling of frames when using soft-target labels in either the training or testing process. When training with soft-target labels (Tables  S1  and S3 ), the losses between active learning and cycling conditions are nearly identical across all simulation iterations both when one-hot encoded and soft-target labels are used to evaluate the test set. When training with one-hot labels and testing on soft-targets (Table  S2 ), there is no clear trend, with active learning performing better on some iterations and cycling performing better on others.\n\nUsing soft-target labels for emotion classification has resulted in lower loss compared to one-hot encoded labels in prior works  [50, 56] . We replicate these results here: when testing with one-hot encoded labels, training with soft-target labels (Table  S3 ) results in noticeably lower loss than training with one-hot encoded labels (Table  1 ) by a margin larger than the error bars. When testing with soft-target labels, training with soft-target labels (Table  S1 ) results in similarly lower loss compared to training with one-hot encodings (Table  S2 ).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "We explore some of the challenges associated with active learning for heterogeneous, noisy, and subjective datasets such as emotion data. The first approach we take incorporates metadata which are collected simultaneously with the original dataset. These metadata provide a noisy prior estimation of the true label, allowing for ensuring of some degree of class balance (with some degree of error) during the active learning process. We find that this approach outperforms random cycling of frames. We recommend further explorations of the utility of automatically generated metadata in machine learning processes.\n\nWhile we scratch the surface of exploring the incorporation of metadata in the active learning process, there are several possible areas of further study. Modern active learning approaches have been proposed which work well for deep learning with image data. For example, Bayesian convolutional neural networks have been demonstrated to aid active learning for high dimensional image datasets  [11] , as the neural network weights are modeled as distributions and the output of the neural network can therefore be interpreted as a calibrated probability distribution.\n\nThe major limitation of this portion of the study is the exploration of this technique on only a single dataset. Further studies should validate this method on other datasets with different properties, including for classifiers related to other behavioral features of autism  [3-4, 9, 15-16, 27-28, 33, 39, 44, 52] . One possible source of confounding variation in the automatic labels could be different biases towards certain incorrect classes. In methods such as the one presented here where the metadata plays a fundamental role in which data are prioritized, bias mitigation strategies for the metadata are equally as important as strategies for the data itself. Avoiding bias in active learning in general is an under-explored topic.\n\nAnother challenge of active learning with \"difficult data\" containing subjective classes is the incorporation of multiple independent human annotations in the active learning process. In particular, we study active learning in situations where multiple crowdsourced acquisitions are acquired for each video, a common practice in digital behavioral phenotyping  [29, 40-42, 51, 53-60] . We find that prioritizing frames with the maximum entropy according to the distribution of crowdsourced responses results in lower loss than obtaining an equal number of human labels per image. This result does not hold for incorporating soft-target labels in either the training or test sets. These results indicate that crowdsourced active learning is useful as an uncertainty metric when crowdsourcing the labeling of subjective data points and training with standard approaches.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "Active learning is challenging, as one-size-fits-all approaches do not work for all datasets, and the difficulty of creating useful active learning algorithms is exacerbated when the data are heterogeneous, noisy, and subjective. We propose two approaches to active learning with affective computing datasets in two situations: (1) when noisy metadata provide an estimation of the true class to enable balanced prioritization of data points, and (2) when multiple labels are acquired for each image.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Each frame is marked with the corresponding prompt on the GuessWhat",
      "page": 4
    },
    {
      "caption": "Figure 1: Generation of automatically labeled frames from gameplay. All time points during a",
      "page": 4
    },
    {
      "caption": "Figure 2: ). We evaluate the following active learning strategies, where X is the set of unlabeled data frames",
      "page": 4
    },
    {
      "caption": "Figure 2: Maximum entropy-based active learning.",
      "page": 5
    },
    {
      "caption": "Figure 3: ). Cycling through frame metadata with maximum entropy starts to reach near",
      "page": 7
    },
    {
      "caption": "Figure 3: Comparison of active learning techniques to baseline when evaluating against a",
      "page": 8
    },
    {
      "caption": "Figure 4: F1 score per active learning iteration when evaluating against the entirety of CAFE.",
      "page": 8
    },
    {
      "caption": "Figure 4: shows that cycling through",
      "page": 8
    },
    {
      "caption": "Figure 5: shows that",
      "page": 8
    },
    {
      "caption": "Figure 5: F1 score per active learning iteration when evaluating against the entirety of CAFE.",
      "page": 9
    },
    {
      "caption": "Figure 6: shows an active learning simulation when including JAFFE and CK+ prior to the first",
      "page": 9
    },
    {
      "caption": "Figure 6: Fitting a power law distribution to the plot of balanced CAFE subset accuracy vs. the",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: ), training and testing with soft-target labels (Table S1), training with one-hot",
      "data": [
        {
          "Fold 1": "Cycling",
          "Fold 2": "Cycling",
          "Fold 3": "Cycling"
        },
        {
          "Fold 1": "1.56 ± 0.26",
          "Fold 2": "1.84 ± 0.35",
          "Fold 3": "1.69 ± 0.29"
        },
        {
          "Fold 1": "1.49 ± 0.35",
          "Fold 2": "2.05 ± 0.59",
          "Fold 3": "2.02 ± 0.40"
        },
        {
          "Fold 1": "1.75 ± 0.49",
          "Fold 2": "1.99 ± 0.56",
          "Fold 3": "2.03 ± 0.45"
        },
        {
          "Fold 1": "1.60 ± 0.36",
          "Fold 2": "2.09 ± 0.56",
          "Fold 3": "2.04 ± 0.74"
        },
        {
          "Fold 1": "1.60 ± 0.38",
          "Fold 2": "2.08 ± 0.38",
          "Fold 3": "2.24 ± 0.54"
        },
        {
          "Fold 1": "1.74 ± 0.45",
          "Fold 2": "2.05 ± 0.52",
          "Fold 3": "1.69 ± 0.36"
        },
        {
          "Fold 1": "1.70 ± 0.36",
          "Fold 2": "2.08 ± 0.70",
          "Fold 3": "2.26 ± 0.43"
        },
        {
          "Fold 1": "1.62 ± 0.32",
          "Fold 2": "2.50 ± 0.63",
          "Fold 3": "1.93 ± 0.39"
        },
        {
          "Fold 1": "1.90 ± 0.38",
          "Fold 2": "2.11 ± 0.53",
          "Fold 3": "1.94 ± 0.45"
        },
        {
          "Fold 1": "1.56 ± 0.26",
          "Fold 2": "2.34 ± 0.63",
          "Fold 3": "2.03 ± 0.56"
        },
        {
          "Fold 1": "1.76 ± 0.49",
          "Fold 2": "2.60 ± 0.51",
          "Fold 3": "2.13 ± 0.48"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold 1": "Cycling",
          "Fold 2": "Cycling",
          "Fold 3": "Cycling"
        },
        {
          "Fold 1": "0.96 ± 0.01",
          "Fold 2": "1.00 ± 0.01",
          "Fold 3": "1.04 ± 0.02"
        },
        {
          "Fold 1": "0.97 ± 0.01",
          "Fold 2": "1.00 ± 0.01",
          "Fold 3": "1.04 ± 0.01"
        },
        {
          "Fold 1": "0.96 ± 0.01",
          "Fold 2": "1.00 ± 0.01",
          "Fold 3": "1.05 ± 0.01"
        },
        {
          "Fold 1": "0.97 ± 0.01",
          "Fold 2": "0.99 ± 0.01",
          "Fold 3": "1.04 ± 0.01"
        },
        {
          "Fold 1": "0.96 ± 0.01",
          "Fold 2": "1.00 ± 0.01",
          "Fold 3": "1.05 ± 0.01"
        },
        {
          "Fold 1": "0.97 ± 0.01",
          "Fold 2": "0.99 ± 0.01",
          "Fold 3": "1.05 ± 0.01"
        },
        {
          "Fold 1": "0.96 ± 0.01",
          "Fold 2": "0.99 ± 0.01",
          "Fold 3": "1.05 ± 0.01"
        },
        {
          "Fold 1": "0.96 ± 0.01",
          "Fold 2": "1.01 ± 0.01",
          "Fold 3": "1.04 ± 0.01"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold 1": "Cycling",
          "Fold 2": "Cycling",
          "Fold 3": "Cycling"
        },
        {
          "Fold 1": "3.24 ± 0.51",
          "Fold 2": "3.39 ± 0.49",
          "Fold 3": "2.81 ± 0.61"
        },
        {
          "Fold 1": "2.88 ± 0.56",
          "Fold 2": "3.14 ± 0.68",
          "Fold 3": "3.39 ± 0.62"
        },
        {
          "Fold 1": "3.26 ± 0.71",
          "Fold 2": "3.16 ± 0.80",
          "Fold 3": "3.55 ± 0.76"
        },
        {
          "Fold 1": "3.13 ± 0.47",
          "Fold 2": "3.34 ± 0.60",
          "Fold 3": "3.47 ± 0.88"
        },
        {
          "Fold 1": "3.38 ± 0.42",
          "Fold 2": "3.20 ± 0.74",
          "Fold 3": "3.72 ± 0.64"
        },
        {
          "Fold 1": "3.54 ± 0.46",
          "Fold 2": "3.42 ± 0.57",
          "Fold 3": "3.62 ± 0.51"
        },
        {
          "Fold 1": "3.24 ± 0.51",
          "Fold 2": "3.29 ± 0.76",
          "Fold 3": "3.76 ± 0.81"
        },
        {
          "Fold 1": "3.26 ± 0.71",
          "Fold 2": "3.51 ± 0.49",
          "Fold 3": "3.41 ± 0.65"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold 1": "Cycling",
          "Fold 2": "Cycling",
          "Fold 3": "Cycling"
        },
        {
          "Fold 1": "0.60 ± 0.02",
          "Fold 2": "0.69 ± 0.03",
          "Fold 3": "0.71 ± 0.03"
        },
        {
          "Fold 1": "0.62 ± 0.02",
          "Fold 2": "0.70 ± 0.03",
          "Fold 3": "0.70 ± 0.03"
        },
        {
          "Fold 1": "0.61 ± 0.02",
          "Fold 2": "0.70 ± 0.03",
          "Fold 3": "0.71 ± 0.02"
        },
        {
          "Fold 1": "0.62 ± 0.02",
          "Fold 2": "0.70 ± 0.03",
          "Fold 3": "0.70 ± 0.02"
        },
        {
          "Fold 1": "0.61 ± 0.02",
          "Fold 2": "0.68 ± 0.03",
          "Fold 3": "0.73 ± 0.03"
        },
        {
          "Fold 1": "0.62 ± 0.02",
          "Fold 2": "0.69 ± 0.03",
          "Fold 3": "0.69 ±  0.02"
        },
        {
          "Fold 1": "0.60 ± 0.02",
          "Fold 2": "0.72 ± 0.02",
          "Fold 3": "0.70 ± 0.02"
        },
        {
          "Fold 1": "0.61 ± 0.02",
          "Fold 2": "0.71 ± 0.03",
          "Fold 3": "0.70 ± 0.02"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wild facial expression recognition based on incremental active learning",
      "authors": [
        "Minhaz Ahmed",
        "Uddin",
        "Jin Kim",
        "Kim Woo",
        "Md Yeong Hyeon",
        "Phill Rezaul Bashar",
        "Rhee Kyu"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "2",
      "title": "Training and Profiling a Pediatric Emotion Recognition Classifier on Mobile Devices",
      "authors": [
        "Agnik Banerjee",
        "Peter Washington",
        "Cezmi Mutlu",
        "Aaron Kline",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "Training and Profiling a Pediatric Emotion Recognition Classifier on Mobile Devices",
      "arxiv": "arXiv:2108.11754"
    },
    {
      "citation_id": "3",
      "title": "Combining gaze and demographic feature descriptors for autism classification",
      "authors": [
        "Shaun Canavan",
        "Melanie Chen",
        "Song Chen",
        "Robert Valdez",
        "Miles Yaeger",
        "Huiyi Lin",
        "Lijun Yin"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "4",
      "title": "Classifying Autism from Crowdsourced Semi-Structured Speech Recordings: A Machine Learning Approach",
      "authors": [
        "Nathan Chi",
        "Aaron Peter Washington",
        "Arman Kline",
        "Cathy Husic",
        "Chloe Hou",
        "Kaitlyn He",
        "Dennis Dunlap",
        "Wall"
      ],
      "year": "2022",
      "venue": "Classifying Autism from Crowdsourced Semi-Structured Speech Recordings: A Machine Learning Approach",
      "arxiv": "arXiv:2201.00927"
    },
    {
      "citation_id": "5",
      "title": "Feasibility testing of a wearable behavioral aid for social learning in children with autism",
      "authors": [
        "Jena Daniels",
        "Nick Haber",
        "Catalin Voss",
        "Jessey Schwartz",
        "Serena Tamura",
        "Azar Fazel",
        "Aaron Kline"
      ],
      "year": "2018",
      "venue": "Applied clinical informatics"
    },
    {
      "citation_id": "6",
      "title": "5.13 Design and efficacy of a wearable device for social affective learning in children with autism",
      "authors": [
        "Jena Daniels",
        "Jessey Schwartz",
        "Nick Haber",
        "Catalin Voss",
        "Aaron Kline",
        "Azar Fazel",
        "Peter Washington"
      ],
      "year": "2017",
      "venue": "Journal of the American Academy of Child & Adolescent Psychiatry"
    },
    {
      "citation_id": "7",
      "title": "Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism",
      "authors": [
        "Jena Daniels",
        "Jessey Schwartz",
        "Catalin Voss",
        "Nick Haber",
        "Azar Fazel",
        "Aaron Kline",
        "Peter Washington",
        "Carl Feinstein",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "8",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Detecting autism by analyzing a simulated social interaction",
      "authors": [
        "Hanna Drimalla",
        "Niels Landwehr",
        "Irina Baskow",
        "Behnoush Behnia",
        "Stefan Roepke",
        "Isabel Dziobek",
        "Tobias Scheffer"
      ],
      "year": "2018",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "10",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "11",
      "title": "Deep bayesian active learning with image data",
      "authors": [
        "Yarin Gal",
        "Riashat Islam",
        "Zoubin Ghahramani"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "12",
      "title": "Making emotions transparent: Google Glass helps autistic kids understand facial expressions through augmented-reaiity therapy",
      "authors": [
        "Nick Haber",
        "Catalin Voss",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "IEEE Spectrum"
    },
    {
      "citation_id": "13",
      "title": "A wearable social interaction aid for children with autism",
      "authors": [
        "Nick Haber",
        "Catalin Voss",
        "Jena Daniels",
        "Peter Washington",
        "Azar Fazel",
        "Aaron Kline",
        "Titas De",
        "Terry Winograd",
        "Carl Feinstein",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "A wearable social interaction aid for children with autism",
      "arxiv": "arXiv:2004.14281"
    },
    {
      "citation_id": "14",
      "title": "A practical approach to real-time neutral feature subtraction for facial expression recognition",
      "authors": [
        "Nick Haber",
        "Catalin Voss",
        "Azar Fazel",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "15",
      "title": "A computer vision approach for the assessment of autism-related behavioral markers",
      "authors": [
        "Jordan Hashemi",
        "Thiago Vallin Spina",
        "Mariano Tepper",
        "Amy Esler",
        "Vassilios Morellas",
        "Nikolaos Papanikolopoulos",
        "Guillermo Sapiro"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL)"
    },
    {
      "citation_id": "16",
      "title": "Computer vision tools for low-cost and noninvasive measurement of autism-related behaviors in infants",
      "authors": [
        "Jordan Hashemi",
        "Mariano Tepper",
        "Thiago Vallin Spina",
        "Amy Esler",
        "Vassilios Morellas",
        "Nikolaos Papanikolopoulos",
        "Helen Egger",
        "Geraldine Dawson",
        "Guillermo Sapiro"
      ],
      "year": "2014",
      "venue": "Autism research and treatment"
    },
    {
      "citation_id": "17",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Entropy-based active learning for object recognition",
      "authors": [
        "Alex Holub",
        "Pietro Perona",
        "Michael Burl"
      ],
      "year": "2008",
      "venue": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "19",
      "title": "Leveraging video data from a digital smartphone autism therapy to train an emotion detection classifier",
      "authors": [
        "Cathy Hou",
        "Haik Kalantarian",
        "Peter Washington",
        "Kaiti Dunlap",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    },
    {
      "citation_id": "20",
      "title": "The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study",
      "authors": [
        "Haik Kalantarian",
        "Khaled Jedoui",
        "Kaitlyn Dunlap",
        "Jessey Schwartz",
        "Peter Washington",
        "Arman Husic",
        "Qandeel Tariq",
        "Michael Ning",
        "Aaron Kline",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "21",
      "title": "A mobile game for automatic emotion-labeling of images",
      "authors": [
        "Haik Kalantarian",
        "Khaled Jedoui",
        "Peter Washington",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "IEEE transactions on games"
    },
    {
      "citation_id": "22",
      "title": "Labeling images with facial emotion and the potential for pediatric healthcare",
      "authors": [
        "Haik Kalantarian",
        "Khaled Jedoui",
        "Peter Washington",
        "Qandeel Tariq",
        "Kaiti Dunlap",
        "Jessey Schwartz",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "Artificial intelligence in medicine"
    },
    {
      "citation_id": "23",
      "title": "A gamified mobile system for crowdsourcing video for autism research",
      "authors": [
        "Haik Kalantarian",
        "Peter Washington",
        "Jessey Schwartz",
        "Jena Daniels",
        "Nick Haber",
        "Dennis Wall"
      ],
      "venue": "2018 IEEE international conference on healthcare informatics (ICHI)"
    },
    {
      "citation_id": "24",
      "title": "Guess what?",
      "authors": [
        "Haik Kalantarian",
        "Peter Washington",
        "Jessey Schwartz",
        "Jena Daniels",
        "Nick Haber",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "Journal of healthcare informatics research"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "Diederik Kingma",
        "Jimmy Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "26",
      "title": "Superpower glass",
      "authors": [
        "Aaron Kline",
        "Catalin Voss",
        "Peter Washington",
        "Nick Haber",
        "Hessey Schwartz",
        "Qandeel Tariq",
        "Terry Winograd",
        "Carl Feinstein",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "GetMobile: Mobile Computing and Communications"
    },
    {
      "citation_id": "27",
      "title": "Classification of Hand Movement for Aiding in Autism Detection: Machine Learning Study",
      "authors": [
        "Anish Lakkapragada",
        "Aaron Kline",
        "Onur Cezmi Mutlu",
        "Kelley Paskov",
        "Brianna Chrisman",
        "Nate Stockham",
        "Peter Washington",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "Classification of Hand Movement for Aiding in Autism Detection: Machine Learning Study",
      "arxiv": "arXiv:2108.07917"
    },
    {
      "citation_id": "28",
      "title": "Activity recognition for autism diagnosis",
      "authors": [
        "Anish Lakkapragada",
        "Peter Washington",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "Activity recognition for autism diagnosis"
    },
    {
      "citation_id": "29",
      "title": "Feature replacement methods enable reliable home video analysis for machine learning detection of autism",
      "authors": [
        "Emilie Leblanc",
        "Peter Washington",
        "Maya Varma",
        "Kaitlyn Dunlap",
        "Yordan Penev",
        "Aaron Kline",
        "Dennis Wall"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "30",
      "title": "The Child Affective Facial Expression (CAFE) set: Validity and reliability from untrained adults",
      "authors": [
        "Vanessa Lobue",
        "Cat Thrasher"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "31",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Thibaud Senechal",
        "May Amr",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "32",
      "title": "Toward continuous social phenotyping: analyzing gaze patterns in an emotion recognition task for children with autism through wearable smart glasses",
      "authors": [
        "Anish Nag",
        "Nick Haber",
        "Catalin Voss",
        "Serena Tamura",
        "Jena Daniels",
        "Jeffrey Ma",
        "Bryan Chiang"
      ],
      "year": "2020",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "33",
      "title": "Vision-assisted recognition of stereotype behaviors for early diagnosis of Autism Spectrum Disorders",
      "authors": [
        "Farhood Negin",
        "Baris Ozyer",
        "Saeid Agahian",
        "Sibel Kacdioglu",
        "Gulsah Tumuklu Ozyer"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "34",
      "title": "A Mobile Game Platform for Improving Social Communication in Children with Autism: A Feasibility Study",
      "authors": [
        "Yordan Penev",
        "Kaitlyn Dunlap",
        "Arman Husic",
        "Cathy Hou",
        "Peter Washington",
        "Emilie Leblanc",
        "Aaron Kline"
      ],
      "year": "2021",
      "venue": "Applied clinical informatics"
    },
    {
      "citation_id": "35",
      "title": "Active learning for logistic regression: an evaluation",
      "authors": [
        "Andrew Schein",
        "Lyle Ungar"
      ],
      "year": "2007",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "Facial action unit detection using active learning and an efficient non-linear kernel approximation",
      "authors": [
        "Thibaud Senechal",
        "Daniel Mcduff",
        "Rana Kaliouby"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "37",
      "title": "Active learning literature survey",
      "authors": [
        "Burr Settles"
      ],
      "year": "2009",
      "venue": "Active learning literature survey"
    },
    {
      "citation_id": "38",
      "title": "From theories to queries: Active learning in practice",
      "authors": [
        "Burr Settles"
      ],
      "year": "2011",
      "venue": "Active Learning and Experimental Design workshop In conjunction with AISTATS 2010"
    },
    {
      "citation_id": "39",
      "title": "Cortical surface thickness as a classifier: boosting for autism classification",
      "authors": [
        "Vikas Singh",
        "Lopamudra Mukherjee",
        "Moo Chung"
      ],
      "year": "2008",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention"
    },
    {
      "citation_id": "40",
      "title": "Mobile detection of autism through machine learning on home video: A development and prospective validation study",
      "authors": [
        "Qandeel Tariq",
        "Jena Daniels",
        "Jessey Schwartz",
        "Peter Washington",
        "Haik Kalantarian",
        "Dennis Wall"
      ],
      "year": "2018",
      "venue": "PLoS medicine"
    },
    {
      "citation_id": "41",
      "title": "Detecting developmental delay and autism through machine learning models using home videos of Bangladeshi children: Development and validation study",
      "authors": [
        "Qandeel Tariq",
        "Scott Fleming",
        "Jessey Schwartz",
        "Kaitlyn Dunlap",
        "Conor Corbin",
        "Peter Washington",
        "Haik Kalantarian",
        "Z Naila",
        "Gary Khan",
        "Dennis Darmstadt",
        "Wall"
      ],
      "year": "2019",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "42",
      "title": "Detecting developmental delay and autism through machine learning models on home videos of Bangladeshi children",
      "authors": [
        "Qandeel Tariq",
        "Scott Fleming",
        "Jessey Schwartz",
        "Kaitlyn Dunlap",
        "Conor Corbin",
        "Peter Washington",
        "Haik Kalantarian",
        "Z Naila",
        "Gary Khan",
        "Dennis Darmstadt",
        "Wall"
      ],
      "venue": "Detecting developmental delay and autism through machine learning models on home videos of Bangladeshi children"
    },
    {
      "citation_id": "43",
      "title": "Detection of emotional events utilizing support vector methods in an active learning HCI scenario",
      "authors": [
        "Patrick Thiam",
        "Sascha Meudt",
        "Markus Kächele",
        "Günther Palm",
        "Friedhelm Schwenker"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 workshop on emotion representation and modelling in human-computer-interaction-systems"
    },
    {
      "citation_id": "44",
      "title": "Identification of social engagement indicators associated with autism spectrum disorder using a game-based mobile application",
      "authors": [
        "Maya Varma",
        "Peter Washington",
        "Brianna Chrisman",
        "Aaron Kline",
        "Emilie Leblanc",
        "Kelley Paskov",
        "Nate Stockham",
        "Jae-Yoon Jung",
        "Min Sun",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    },
    {
      "citation_id": "45",
      "title": "The Potential for Machine Learning-Based Wearables to Improve Socialization in Teenagers and Adults With Autism Spectrum Disorder-Reply",
      "authors": [
        "Catalin Voss",
        "Nick Haber",
        "Dennis Wall"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "46",
      "title": "Designing a holistic athome learning aid for autism",
      "authors": [
        "Catalin Voss",
        "Nick Haber",
        "Peter Washington",
        "Aaron Kline",
        "Beth Mccarthy",
        "Jena Daniels",
        "Azar Fazel"
      ],
      "year": "2020",
      "venue": "Designing a holistic athome learning aid for autism",
      "arxiv": "arXiv:2002.04263"
    },
    {
      "citation_id": "47",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial",
      "authors": [
        "Catalin Voss",
        "Jessey Schwartz",
        "Jena Daniels",
        "Aaron Kline",
        "Nick Haber",
        "Peter Washington",
        "Qandeel Tariq"
      ],
      "year": "2019",
      "venue": "JAMA pediatrics"
    },
    {
      "citation_id": "48",
      "title": "Superpower glass: delivering unobtrusive real-time social cues in wearable systems",
      "authors": [
        "Catalin Voss",
        "Peter Washington",
        "Nick Haber",
        "Aaron Kline",
        "Jena Daniels",
        "Azar Fazel",
        "Titas De"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct"
    },
    {
      "citation_id": "49",
      "title": "Training an Emotion Detection Classifier using Frames from a Mobile Therapeutic Game for Children with Developmental Disorders",
      "authors": [
        "Peter Washington",
        "Haik Kalantarian",
        "Jack Kent",
        "Arman Husic",
        "Aaron Kline",
        "Emilie Leblanc",
        "Cathy Hou"
      ],
      "year": "2020",
      "venue": "Training an Emotion Detection Classifier using Frames from a Mobile Therapeutic Game for Children with Developmental Disorders",
      "arxiv": "arXiv:2012.08678"
    },
    {
      "citation_id": "50",
      "title": "Training Affective Computer Vision Models by Crowdsourcing Soft-Target Labels",
      "authors": [
        "Peter Washington",
        "Haik Kalantarian",
        "Jack Kent",
        "Arman Husic",
        "Aaron Kline",
        "Emilie Leblanc",
        "Cathy Hou"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "51",
      "title": "Validity of online screening for autism: crowdsourcing study comparing paid and unpaid diagnostic tasks",
      "authors": [
        "Peter Washington",
        "Haik Kalantarian",
        "Qandeel Tariq",
        "Jessey Schwartz",
        "Kaitlyn Dunlap",
        "Brianna Chrisman",
        "Maya Varma"
      ],
      "year": "2019",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "52",
      "title": "Activity Recognition with Moving Cameras and Few Training Examples: Applications for Detection of Autism-Related Headbanging",
      "authors": [
        "Peter Washington",
        "Aaron Kline",
        "Onur Cezmi Mutlu",
        "Emilie Leblanc",
        "Cathy Hou",
        "Nate Stockham",
        "Kelley Paskov",
        "Brianna Chrisman",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "53",
      "title": "Precision telemedicine through crowdsourced machine learning: testing variability of crowd workers for video-based autism feature recognition",
      "authors": [
        "Peter Washington",
        "Emilie Leblanc",
        "Kaitlyn Dunlap",
        "Yordan Penev",
        "Aaron Kline",
        "Kelley Paskov",
        "Min Sun"
      ],
      "year": "2020",
      "venue": "Journal of personalized medicine"
    },
    {
      "citation_id": "54",
      "title": "Selection of trustworthy crowd workers for telemedical diagnosis of pediatric autism spectrum disorder",
      "authors": [
        "Peter Washington",
        "Emilie Leblanc",
        "Kaitlyn Dunlap",
        "Yordan Penev",
        "Maya Varma",
        "Jae-Yoon Jung",
        "Brianna Chrisman"
      ],
      "year": "2020",
      "venue": "BIOCOMPUTING 2021: Proceedings of the Pacific Symposium"
    },
    {
      "citation_id": "55",
      "title": "Crowd Annotations Can Approximate Clinical Autism Impressions from Short Home Videos with Privacy Protections",
      "authors": [
        "Peter Washington",
        "Emilie Leblanc",
        "Kaitlyn Dunlap",
        "Aaron Kline",
        "Cezmi Mutlu",
        "Brianna Chrisman",
        "Nate Stockham",
        "Kelley Paskov",
        "Dennis Wall"
      ],
      "year": "2021",
      "venue": "medRxiv"
    },
    {
      "citation_id": "56",
      "title": "Using Crowdsourcing to Train Facial Emotion Machine Learning Models with Ambiguous Labels",
      "authors": [
        "Peter Washington",
        "Onur Cezmi Mutlu",
        "Emilie Leblanc",
        "Aaron Kline",
        "Cathy Hou",
        "Brianna Chrisman",
        "Nate Stockham"
      ],
      "year": "2021",
      "venue": "Using Crowdsourcing to Train Facial Emotion Machine Learning Models with Ambiguous Labels",
      "arxiv": "arXiv:2101.03477"
    },
    {
      "citation_id": "57",
      "title": "Challenges and Opportunities for Machine Learning Classification of Behavior and Mental State from Images",
      "authors": [
        "Peter Washington",
        "Cezmi Onur Mutlu",
        "Aaron Kline",
        "Kelley Paskov",
        "Nate Tyler Stockham",
        "Brianna Chrisman",
        "Nick Deveau",
        "Mourya Surhabi",
        "Nick Haber",
        "Dennis Wall"
      ],
      "year": "2022",
      "venue": "Challenges and Opportunities for Machine Learning Classification of Behavior and Mental State from Images",
      "arxiv": "arXiv:2201.11197"
    },
    {
      "citation_id": "58",
      "title": "Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry",
      "authors": [
        "Peter Washington",
        "Natalie Park",
        "Parishkrita Srivastava",
        "Catalin Voss",
        "Aaron Kline",
        "Maya Varma",
        "Qandeel Tariq"
      ],
      "year": "2020",
      "venue": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
    },
    {
      "citation_id": "59",
      "title": "Crowdsourced privacy-preserved feature tagging of short home videos for machine learning ASD detection",
      "authors": [
        "Peter Washington",
        "Qandeel Tariq",
        "Emilie Leblanc",
        "Brianna Chrisman",
        "Kaitlyn Dunlap",
        "Aaron Kline",
        "Haik Kalantarian"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "60",
      "title": "Crowdsourced feature tagging for scalable and privacy-preserved autism diagnosis",
      "authors": [
        "Peter Washington",
        "Qandeel Tariq",
        "Emilie Leblanc",
        "Brianna Chrisman",
        "Kaitlyn Dunlap",
        "Aaron Kline",
        "Haik Kalantarian"
      ],
      "year": "2020",
      "venue": "medRxiv"
    },
    {
      "citation_id": "61",
      "title": "A wearable social interaction aid for children with autism",
      "authors": [
        "Peter Washington",
        "Catalin Voss",
        "Nick Haber",
        "Serena Tanaka",
        "Jena Daniels",
        "Carl Feinstein",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "62",
      "title": "Superpowerglass: A wearable aid for the at-home therapy of children with autism",
      "authors": [
        "Peter Washington",
        "Catalin Voss",
        "Aaron Kline",
        "Nick Haber",
        "Jena Daniels",
        "Azar Fazel",
        "Titas De",
        "Carl Feinstein",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies"
    }
  ]
}