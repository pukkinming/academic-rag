{
  "paper_id": "2302.09856v3",
  "title": "Knowledge-Aware Bayesian Co-Attention For Multimodal Emotion Recognition",
  "published": "2023-02-20T09:38:11Z",
  "authors": [
    "Zihan Zhao",
    "Yu Wang",
    "Yanfeng Wang"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "transfer learning",
    "Bayesian attention",
    "knowledge injection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition is a challenging research area that aims to fuse different modalities to predict human emotion. However, most existing models that are based on attention mechanisms have difficulty in learning emotionally relevant parts on their own. To solve this problem, we propose to incorporate external emotion-related knowledge in the co-attention based fusion of pre-trained models. To effectively incorporate this knowledge, we enhance the coattention model with a Bayesian attention module (BAM) where a prior distribution is estimated using the emotionrelated knowledge. Experimental results on the IEMOCAP dataset show that the proposed approach can outperform several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is the process of identifying human emotion. Multi-modality for emotion recognition has recently received great interest because different modalities can provide complementary clues and the fusion of them can effectively improve the performance  [1, 2, 3] . For example, the meanings of words and their relation express one's emotion in text modality, while intonation and pitch can also be useful for recognizing emotions in speech modality. In this paper, we focus our research on text and speech modalities.\n\nEmotion datasets usually face the problem of insufficient data  [4] , mainly due to the difficulties caused by the subjective nature of labeling emotions. The common solution to solve this problem is to leverage pre-trained models. For the text modality, BERT  [5]  is the most commonly used model and is well suited for feature extraction. Recently, speech-based pre-trained models have also emerged, and the most stateof-the-art of them is the wav2vec 2.0  [6] , which has shown promising performance in ASR  [7] , speaker verification  [8]  and emotion recognition  [1] .\n\nAttention mechanisms enable models to learn which parts are important and which parts are irrelevant  [9] . So it is well suited for handling sequential inputs, such as text and speech. Self-attention is an attention mechanism relating different positions of a single sequence to compute a representation of the sequence  [9] . Based on attention mechanisms, co-attention is a fusion method in which the representation of the sequence of a modality is computed by relating the sequence of the other modality  [10] . Several works have proposed to leverage pre-trained models of both speech and text modalities and attention mechanisms for multimodal emotion recognition and achieved promising results  [1, 2, 11, 12] .\n\nHowever, not all parts in the data sequences are related to emotion, and without the guidance of external knowledge, it is challenging for the attention to learn emotionally relevant parts accurately on its own  [13] . To solve this problem, we propose to incorporate external emotion-related knowledge. More specifically, an emotion lexicon is leveraged with the attention mechanisms to improve its interpretability and robustness for the emotion recognition task. The emotion lexicon has shown its effectiveness in the previous methods. The work in  [14]  used commonsense knowledge to build a graph and retrieved emotion-related information for the nodes in it using an emotion lexicon. Some other works proposed to integrate an emotion lexicon with contextual information for emotion recognition in conversations  [15] . However, there is no current work that combines the emotion lexicon with attention mechanisms. In this paper, we propose a Bayesian framework  [16]  with an emotion knowledge-related attention map as the prior distribution. The posterior distribution of the attention weights can then be learned. The reason that a Bayesian attention module (BAM) is introduced instead of simply adding the knowledge-related attention map on the original one is that the knowledge we introduce is not collected on the dataset we use, which may introduce bias. If a rather \"hard\" way is used for incorporating the knowledge, it may affect the learning of the attention model. The approach we propose allows the introduction of prior knowledge while achieving an integrated training of the attention weights, thus preventing this problem. We also show that the combination with the late fusion model further improves the performance.\n\nWe summarize our major contributions as follows:\n\n• We introduce emotion-related knowledge in the attention model to help the model focus more on relevant information for emotion recognition. • To effectively incorporate knowledge, we further propose to use BAM, which not only brings randomness in the model and thus can help in modeling complex dependencies but also incorporates knowledge as prior in a \"soft\" way. We also show that it's complementary to the late fusion model. • We evaluate the proposed models on the popular IEMOCAP dataset  [17] . Experimental results show that they can outperform existing multimodal approaches using both speech and text modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methods",
      "text": "The proposed model is shown in Figure  1 . First, we use BERT to extract text embeddings and wav2vec 2.0 to extract frame-level speech embeddings. Then we use word-level force alignment to get word-level speech embeddings in Section 2.1. Next, we apply self-attention to both embeddings to process input sequences and soften knowledge in Section 2.2.\n\nWe introduce the general co-attention module in Section 2.3 and the proposed knowledge-aware co-attention module to fuse two modalities in Section 2.4. Average pooling layers and linear layers are then used to output the probability distribution for emotion classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Word-Level Force Alignment",
      "text": "The knowledge we adopt is of word-level, and we want to introduce knowledge for both text and speech modalities, so we use the word-level force alignment to cope with framelevel speech embeddings:\n\nwhere u W k is the k th segmentation of the word-level speech embedding U W , u f is the f th frame of the frame-level speech embedding, s is the starting frame, e is the end frame. Segment-level speech embeddings like word-level speech embeddings are closely related to the prosody, and prosody can convey characteristics of the utterance like emotional state because it contains the information of the cadence of speech signals  [1] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Text",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Add & Layernorm",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feedforward",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Add & Layernorm",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Add & Layernorm",
      "text": "Feedforward Fig.  2 . Co-attention module. Multi-head attention is used to fuse two modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Attention Module",
      "text": "Self-attention is an attention mechanism relating different positions of a single sequence to compute a representation of the sequence  [9] . Without loss of generality, the computation of one-head attention is given here:\n\nwhere U i is the embedding for modality i ∈ {W, T}, W is for word-level speech embeddings and T is for text embeddings. W is the weight and b is the bias. In the self-attention module, i in Equation 2 are the same for Q s , K s and V s . Thus, we can obtain the processed embeddings as:\n\nwhere d represents the dimension of the embeddings. Multi-head attention performs this process multiple times to learn information from various representation subspaces. We also use the self-attention module to soften the knowledge from the NRC VAD lexicon  [18] , which is the emotion lexicon we use. Because the knowledge is relatively sparse, without this step, the model will have difficulty learning useful information from the knowledge. The process of softening the knowledge is as follows: Given a sequence, we first look up the lexicon to find words that appeared in the lexicon. The VAD values we use range from -1 to 1. Then we compute the L 2 -norm of each word to get its intensity:\n\n, where i k is the k th of vector i which is the intensity for the sequence and v, a, d are for valence, arousal, dominance respectively. Intensity is assigned to 0 for words that do not appear in the lexicon. Now we get the knowledge i for the sequence, but it faces the problem of sparsity. So we dot product the k th row of the attention map in Equation 3 with the i to get the softening intensity i sof t k which is the k th of vector i sof t :\n\n(5)",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Co-Attention Module",
      "text": "The co-attention module  [10]  is depicted in Figure  2 . There are two branches in the co-attention module, each of which has the same structure as the Transformer encoder  [9] . First embeddings from two modalities are sent to multi-head attention respectively. The computation of Q c , K c and V c is the same as Equation  2 , but here each branch has different modalities as Q c or K c ,V c . Then attention map is computed using Equation  3 . The output of multi-head attention is computed using Equation  4 and then added with its input which is used to compute its Q c . Layernorm layers are added to help the model converge  [19] . The feed-forward layer is then applied, and it consists of two linear layers with a nonlinear function. The output of the feed-forward layer is added with its input and then Layernorm is applied. Co-attention module can be applied several times. The output is the mean of the two branches of the final co-attention module.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Knowledge-Aware Bayesian Co-Attention Module",
      "text": "The proposed Knowledge-aware Bayesian Co-attention Module has the same structure as the co-attention module in Section 2.3 except the computation of the attention map in Equation 3 which is shown in Figure  3 . In the BAM  [16] , attention weights W are not deterministic but data-dependent local random variables from a variational distribution. It was shown that this variational distribution can approximate the posterior of attention weights W using reparameterizable attention distributions and also a prior as regularization. Following  [16] , we use a Weibull distribution as the variational distribution and Gamma distribution as the prior distribution, then the regularization KL divergence can be computed as:\n\nwhere k, λ are the parameters of Weibull distribution, α, β are the parameters of Gamma distribution, γ is the Euler's constant, and Γ is the gamma function. The original attention map of the co-attention module computed using Equation 3 is used to compute λ of Weibull distribution. The sample of the Weibull distribution after normalization is the attention map of the BAM during training. During inference, the posterior expectations are used to obtain the point esti-mates  [20] . However, sampling directly from the distribution will fail backpropagation of gradients. Following  [16] , the Weibull distribution is a reparameterizable distribution, so this problem can be solved by sampling from the standard uniform distribution ε ∼ Uniform (0, 1). Then the sample from Weibull distribution is equivalent to drawing: S := λ(-log (1 -ε))\n\n1/k , where λ, k are the parameter of Weibull distribution. The sample S is multiplied with V c to get the output of the knowledge-aware Bayesian attention, similar to Equation  4 .\n\nUnlike  [16] , which uses K c to compute parameters of the prior distribution in Equation  6 , we use the knowledge vector i sof t Qc , i sof t Kc in Equation  5 to compute them where i sof t Qc/Kc is the intensity for the sequence which computes Q c /K c :",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Kc",
      "text": ". We use P to compute α of the Gamma distribution in Equation  6 . In this way, prior knowledge can be introduced while still leaving the attention weights to be learned by the model itself, which alleviates the problem of bias in the emotion lexicon. In this way the k th of the intensity i sof t Qc,k works like temperature for i sof t Kc in every row. If i sof t Qc,k is high, then this word is highly related to emotion, and the temperature will strengthen its connection with other emotional words. If i sof t Qc,k is low, a preposition for example, then this word may not be related to emotion, and the temperature will weaken its connection with other emotional words. The β of the Gamma distribution and the k of the Weibull distribution remain as hyperparameters.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "The IEMOCAP dataset is an acted, multimodal and multispeaker dataset with five sessions  [17] . We use 4 emotional classes: anger, happiness, sadness, and neutral. The number of utterances representing them was 1103, 1636, 1084, and 1708 respectively. Alignment information of IEMOCAP is provided in their datasets. Other datasets can also get alignment information using force alignment tools such as Prosody Aligner  [21]  and Montreal Forced Aligner  [22] . We use the NRC VAD lexicon  [18]  as the emotion lexicon. It includes a list of more than 20,000 English words and their valence (positiveness-negativeness), arousal (active-passive), and dominance (dominant-submissive) scores.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Settings And Metrics",
      "text": "Cross-entropy loss plus KL loss in Equation 6 was used as the loss function, and an Adam optimizer  [23]  was applied using a learning rate of 1e-4. The models were trained using a batch size of 32 and early stopping was applied with a patience of 6. Dropout  [20]  was applied with a probability of 0.1 after every feed-forward layer except the output layer to prevent overfitting. We use wav2vec 2.0-base and BERTbase un-cased models, both of which have 768-dimensional embeddings. Following  [6] , we extract embeddings from all 12 transformer encoder layers in wav2vec 2.0, and we also apply the weighted average for different layers to get the final word-level speech embeddings. Similarly, we also apply the weighted average for the text modality. All speech samples are normalized by global normalization which is a frequently used setting for this dataset. k of Weibull distribution was set to 1 and β of Gamma distribution was set to 10. Five-fold cross-validation was repeated 5 times and the results were the average of them. A frame length of 400 is used. We observed that more layers of the self-attention module and co-attention module didn't bring improvement in all models, so they were set to 1. Unweighted accuracy (UA) and weighted accuracy (WA) were used as our evaluation metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results Of The Single Modality",
      "text": "The first part of Table  1  shows the results of the single modality. They have the same structure as one branch of Figure  1  except that there is no co-attention module. Word-level speech embeddings are computed from frame-level speech embeddings using Equation  1 . Text modality has the best performance. This is because in IEMOCAP there are not many complex situations where speech is more effective, such as sarcasm. We can also observe that using the word-level alignment information still has similar results compared to the complete frame-level speech embeddings, but with a much smaller size.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results Of The Multimodality",
      "text": "The second part of Table  1  shows the results of multimodality. Compared to the results of the single modality, we can see that the multimodality can yield good performance improvements which are consistent with that observed in  [1, 2] . It can also be seen that the word-level speech embeddingbased co-attention fusion gives a similar performance as the frame-level one. Next, the results of word-level co-attention with a knowledge map directly added to the original attention map (Co-att+knowledge) are shown. It's a rather \"hard\" way which is shown to degrade the performance slightly. We show the results of word-level co-attention with BAM (Co-att+BAM) next, in which we use the prior [16] computed using K s in Equation  2 . We can observe that incorporating the BAM alone without knowledge cannot improve the  model performance. This may be because it's challenging to train BAM to fuse two modalities for multimodal emotion recognition without external guidance. Finally, with the BAM and prior estimated using the emotion-related knowledge (Co-att+BAM+knowledge), it yields the best results among all the co-attention-based models, which validates the effectiveness of the proposed knowledge-enhance BAM model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results Of The Combination With Late Fusion",
      "text": "In  [1] , they show that the co-attention model can combine with the late fusion model simply with score fusion to boost the performance. Table  2  shows that this combination can also give performance gains for our knowledge-aware coattention. The late fusion model that fuses text and wordlevel speech embeddings has the same structure as that in  [1] . The late fusion model that fuses text and frame-level speech embeddings has a similar performance and is omitted here. We can see that late fusion with the knowledge-aware Bayesian co-attention model has the best performance, yielding 77% UA and 75.5% WA, which demonstrates the complementarity of the late fusion and knowledge-aware Bayesian co-attention.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Existing Methods",
      "text": "We compare the proposed method with state-of-the-art multimodal emotion recognition methods in Table  3 . For a fair comparison, all the experiments use the text and speech data from IEMOCAP, 5-fold cross-validation, and four classes. It shows that our proposed model achieves the best performance and surpasses other methods by at least 0.7% UA.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have proposed to incorporate emotionrelated knowledge in Bayesian co-attention modules for multimodal emotion recognition. Experimental results have shown that the proposed Bayesian co-attention model can outperform the baseline multimodal emotion recognition methods and achieves an accuracy of 77.0% UA and 75.5% WA on the 5-fold CV on IEMOCAP.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed model. Word-level force alignment is",
      "page": 2
    },
    {
      "caption": "Figure 1: First, we use",
      "page": 2
    },
    {
      "caption": "Figure 2: Co-attention module. Multi-head attention is used to",
      "page": 2
    },
    {
      "caption": "Figure 3: Knowledge aware Bayesian attention during training.",
      "page": 3
    },
    {
      "caption": "Figure 3: In the BAM [16], atten-",
      "page": 3
    },
    {
      "caption": "Figure 1: except that there is no co-attention module. Word-level",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "ABSTRACT"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "Multimodal emotion recognition is\na\nchallenging research"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "area that aims to fuse different modalities to predict human"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "emotion. However, most existing models that are based on"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "attention mechanisms have difﬁculty in learning emotion-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "ally relevant parts on their own. To solve this problem, we"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "propose to incorporate external emotion-related knowledge"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "in the co-attention based fusion of pre-trained models.\nTo"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "effectively incorporate this knowledge, we enhance the co-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "attention model with a Bayesian attention module (BAM)"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "where a prior distribution is estimated using the emotion-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "related knowledge. Experimental results on the IEMOCAP"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "dataset show that the proposed approach can outperform sev-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "eral state-of-the-art approaches by at\nleast 0.7% unweighted"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "accuracy (UA)."
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "Index Terms— multimodal emotion recognition, transfer"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "learning, Bayesian attention, knowledge injection"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "1.\nINTRODUCTION"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "Emotion recognition is the process of identifying human emo-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "tion. Multi-modality for emotion recognition has recently re-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "ceived great interest because different modalities can provide"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "complementary clues and the fusion of them can effectively"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "improve the performance [1, 2, 3].\nFor example,\nthe mean-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "ings of words and their relation express one’s emotion in text"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "modality, while intonation and pitch can also be useful\nfor"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "recognizing emotions in speech modality.\nIn this paper, we"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "focus our research on text and speech modalities."
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "Emotion datasets usually face the problem of insufﬁcient"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "data [4], mainly due to the difﬁculties caused by the subjective"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "nature of labeling emotions. The common solution to solve"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "this problem is to leverage pre-trained models.\nFor the text"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "modality, BERT [5]\nis the most commonly used model and"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "is well suited for feature extraction. Recently, speech-based"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "pre-trained models have also emerged, and the most\nstate-"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "of-the-art of\nthem is the wav2vec 2.0 [6], which has shown"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "* Corresponding authors"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "This\nwork\nwas\nsupported\nby\nNational\nKey\nR&D\nProgram\nof"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "China\n(No.2022ZD0162100),\nNational Natural\nScience\nFoundation\nof"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "China\n(No.62106140),\nShanghai\nScience\nand\nTechnology\nCommittee"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": ""
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "(No.21511101100) and Shanghai Key Lab of Digital Media Processing and"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "Transmission (No.18DZ2270700)"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or"
        },
        {
          "1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory": "redistribution to servers or lists, or reuse of any copyrighted component of this work in other works."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier\nBayesian": "Force \nSelf-\nAverage"
        },
        {
          "Classifier\nBayesian": "BERT\nCoattention"
        },
        {
          "Classifier\nBayesian": "alignment\nattention\nPooling"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "Fig. 1. The proposed model. Word-level force alignment\nis"
        },
        {
          "Classifier\nBayesian": "applied for using a word-level emotion lexicon. Knowledge-"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "aware Bayesian co-attention is used to fuse two modalities"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "and also inject knowledge."
        },
        {
          "Classifier\nBayesian": "with the late fusion model further improves the performance."
        },
        {
          "Classifier\nBayesian": "We summarize our major contributions as follows:"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "• We introduce emotion-related knowledge in the atten-"
        },
        {
          "Classifier\nBayesian": "tion model\nto help the model\nfocus more on relevant"
        },
        {
          "Classifier\nBayesian": "information for emotion recognition."
        },
        {
          "Classifier\nBayesian": "• To effectively incorporate knowledge, we further pro-"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "pose to use BAM, which not only brings randomness"
        },
        {
          "Classifier\nBayesian": "in the model and thus can help in modeling complex"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "dependencies but also incorporates knowledge as prior"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "in a \"soft\" way. We also show that it’s complementary"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "to the late fusion model."
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "• We\nevaluate\nthe\nproposed models\non\nthe\npopular"
        },
        {
          "Classifier\nBayesian": "IEMOCAP dataset\n[17].\nExperimental\nresults\nshow"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "that\nthey\ncan\noutperform existing multimodal\nap-"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "proaches using both speech and text modalities."
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "2. PROPOSED METHODS"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "The proposed model\nis\nshown in Figure 1.\nFirst, we use"
        },
        {
          "Classifier\nBayesian": "BERT to extract\ntext embeddings and wav2vec 2.0 to ex-"
        },
        {
          "Classifier\nBayesian": "tract frame-level speech embeddings. Then we use word-level"
        },
        {
          "Classifier\nBayesian": "force alignment to get word-level speech embeddings in Sec-"
        },
        {
          "Classifier\nBayesian": "tion 2.1. Next, we apply self-attention to both embeddings to"
        },
        {
          "Classifier\nBayesian": "process input sequences and soften knowledge in Section 2.2."
        },
        {
          "Classifier\nBayesian": "We introduce the general co-attention module in Section 2.3"
        },
        {
          "Classifier\nBayesian": "and the proposed knowledge-aware co-attention module to"
        },
        {
          "Classifier\nBayesian": "fuse two modalities in Section 2.4. Average pooling layers"
        },
        {
          "Classifier\nBayesian": "and linear layers are then used to output the probability distri-"
        },
        {
          "Classifier\nBayesian": "bution for emotion classiﬁcation."
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "2.1. Word-level Force Alignment"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "The knowledge we adopt\nis of word-level, and we want\nto"
        },
        {
          "Classifier\nBayesian": "introduce knowledge for both text and speech modalities, so"
        },
        {
          "Classifier\nBayesian": "we use the word-level\nforce alignment\nto cope with frame-"
        },
        {
          "Classifier\nBayesian": "level speech embeddings:"
        },
        {
          "Classifier\nBayesian": "ek"
        },
        {
          "Classifier\nBayesian": "f =sk uf"
        },
        {
          "Classifier\nBayesian": "uW\n(1)\n,"
        },
        {
          "Classifier\nBayesian": "k ="
        },
        {
          "Classifier\nBayesian": "sk"
        },
        {
          "Classifier\nBayesian": "P\nek −"
        },
        {
          "Classifier\nBayesian": "where uW\nis the kth segmentation of the word-level speech"
        },
        {
          "Classifier\nBayesian": "k"
        },
        {
          "Classifier\nBayesian": "is\nthe\nframe of\nthe\nframe-level\nf th\nembedding UW, uf"
        },
        {
          "Classifier\nBayesian": "speech embedding, s is the starting frame, e is the end frame."
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "Segment-level\nspeech\nembeddings\nlike word-level\nspeech"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "embeddings are closely related to the prosody, and prosody"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "can convey characteristics of\nthe utterance\nlike\nemotional"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "state because it contains the information of\nthe cadence of"
        },
        {
          "Classifier\nBayesian": ""
        },
        {
          "Classifier\nBayesian": "speech signals [1]."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "tion will\nfail backpropagation of gradients.\nFollowing [16],"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "the Weibull distribution is a reparameterizable distribution,"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "so this problem can be solved by sampling from the stan-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "dard uniform distribution ε\nUniform (0, 1). Then the sam-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "∼"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "ple from Weibull distribution is equivalent to drawing: S :="
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "λ(\nlog (1\nε))\n1/k, where λ, k are the parameter of Weibull"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "−\n−"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "the\ndistribution. The sample S is multiplied with Vc to get"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "output of the knowledge-aware Bayesian attention, similar to"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Equation 4."
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Unlike [16], which uses Kc to compute parameters of the"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "prior distribution in Equation 6, we use the knowledge vec-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "in Equation 5 to compute them where isof t\n, isof t\ntor isof t"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Kc\nQc\nQc/Kc"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "is\nthe intensity for\nthe sequence which computes Qc/Kc:"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "⊤"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "isof t\nisof t\n. We use P to compute α of\nP = softmax"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Qc\nKc"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "the Gamma distribution in Equation 6.\nIn this way, prior"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "(cid:17)\n(cid:16)"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "knowledge can be introduced while still leaving the attention"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "weights to be learned by the model itself, which alleviates the"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "problem of bias in the emotion lexicon.\nIn this way the kth"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "of the intensity isof t"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "in ev-\nQc,k works like temperature for isof t"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "ery row.\nthen this word is highly related to\nIf isof t"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Qc,k is high,"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "emotion, and the temperature will strengthen its connection"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "with other emotional words.\nIf isof t"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Qc,k is low, a preposition for"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "example,\nthen this word may not be related to emotion, and"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "the temperature will weaken its connection with other emo-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "tional words. The β of the Gamma distribution and the k of"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "the Weibull distribution remain as hyperparameters."
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "3. EXPERIMENTS AND DISCUSSION"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "3.1. Datasets"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "The IEMOCAP dataset\nis an acted, multimodal and multi-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "speaker dataset with ﬁve sessions [17]. We use 4 emotional"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "classes:\nanger, happiness,\nsadness, and neutral.\nThe num-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "ber of utterances representing them was 1103, 1636, 1084,"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "and 1708 respectively. Alignment information of IEMOCAP"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "is provided in their datasets.\nOther datasets can also get"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "alignment\ninformation using force alignment\ntools\nsuch as"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Prosody Aligner [21] and Montreal Forced Aligner [22]. We"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "use the NRC VAD lexicon [18] as the emotion lexicon.\nIt"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "includes a list of more than 20,000 English words and their"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "valence (positiveness-negativeness), arousal (active-passive),"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "and dominance (dominant-submissive) scores."
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "3.2.\nSettings and Metrics"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "Cross-entropy loss plus KL loss in Equation 6 was used as"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": ""
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "the loss function, and an Adam optimizer\n[23] was applied"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "using a learning rate of 1e-4.\nThe models were trained us-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "ing a batch size of 32 and early stopping was applied with a"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "patience of 6. Dropout [20] was applied with a probability"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "of 0.1 after every feed-forward layer except\nthe output\nlayer"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "to prevent overﬁtting. We use wav2vec 2.0-base and BERT-"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "base un-cased models, both of which have 768-dimensional"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "embeddings. Following [6], we extract embeddings from all"
        },
        {
          "mates [20].\nHowever,\nsampling directly from the distribu-": "12 transformer encoder\nlayers in wav2vec 2.0, and we also"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: shows that this combination can",
      "data": [
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "tion module"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "Modalities"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "T"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "F"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "W"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "T+F"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "T+W"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "T+W"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "T+W"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": "T+W"
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        },
        {
          "- text, F - frame-level speech, W - word-level speech, Self-att": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows that this combination can",
      "data": [
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "Zhao et al. [1], 2022\n76.3%\n-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "apply the weighted average for different layers to get the ﬁnal",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "Ours\n75.5%\n77.0%"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "word-level speech embeddings. Similarly, we also apply the",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "weighted average for the text modality. All speech samples",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "model performance.\nThis may be because it’s challenging"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "are normalized by global normalization which is a frequently",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "to train BAM to fuse two modalities for multimodal emotion"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "used setting for this dataset. k of Weibull distribution was set",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "recognition without external guidance. Finally, with the BAM"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "to 10.\nFive-fold\nto 1 and β of Gamma distribution was set",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "and prior estimated using the emotion-related knowledge (Co-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "cross-validation was repeated 5 times and the results were the",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "att+BAM+knowledge), it yields the best results among all the"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "average of them. A frame length of 400 is used. We observed",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "co-attention-based models, which validates the effectiveness"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "that more layers of the self-attention module and co-attention",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "of the proposed knowledge-enhance BAM model."
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "module didn’t bring improvement in all models, so they were",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "3.5. Results of the Combination with Late Fusion"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "set\nto 1. Unweighted accuracy (UA) and weighted accuracy",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "In [1],\nthey show that\nthe co-attention model can combine"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "(WA) were used as our evaluation metrics.",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "with the late fusion model simply with score fusion to boost"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "3.3. Results of the Single Modality",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "the performance.\nTable 2 shows that\nthis combination can"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "The ﬁrst part of Table 1 shows the results of the single modal-",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "also give performance gains\nfor our knowledge-aware co-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "ity.\nThey have the same structure as one branch of Figure",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "attention.\nThe late fusion model\nthat\nfuses text and word-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "1 except\nthat\nthere is no co-attention module. Word-level",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "level\nspeech embeddings has\nthe same structure as\nthat\nin"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "speech embeddings are computed from frame-level speech",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "[1].\nThe late fusion model\nthat\nfuses\ntext and frame-level"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "embeddings using Equation 1.\nText modality has\nthe best",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "speech embeddings has a similar performance and is omitted"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "performance.\nThis\nis because in IEMOCAP there are not",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "here. We can see that\nlate fusion with the knowledge-aware"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "many complex situations where speech is more effective, such",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "Bayesian co-attention model has the best performance, yield-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "as sarcasm. We can also observe that using the word-level",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "ing 77% UA and 75.5% WA, which demonstrates the comple-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "alignment information still has similar results compared to the",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "mentarity of\nthe late fusion and knowledge-aware Bayesian"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "complete frame-level speech embeddings, but with a much",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "co-attention."
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "smaller size.",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "3.6. Comparison with Existing Methods"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "3.4. Results of the Multimodality",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "We compare the proposed method with state-of-the-art mul-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "The second part of Table 1 shows the results of multimodal-",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "timodal emotion recognition methods in Table 3.\nFor a fair"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "ity. Compared to the results of the single modality, we can",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "comparison, all\nthe experiments use the text and speech data"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "see that\nthe multimodality can yield good performance im-",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "from IEMOCAP, 5-fold cross-validation, and four classes.\nIt"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "provements which are consistent with that observed in [1, 2].",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "shows that our proposed model achieves the best performance"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "It can also be seen that\nthe word-level speech embedding-",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "and surpasses other methods by at least 0.7% UA."
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "based co-attention fusion gives a similar performance as the",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "4. CONCLUSION"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "frame-level one. Next,\nthe results of word-level co-attention",
          "Chen et al. [2], 2022\n75.3%\n74.3%": ""
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "with a knowledge map directly added to the original atten-",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "In this\npaper, we\nhave proposed to\nincorporate\nemotion-"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "tion map (Co-att+knowledge) are shown.\nIt’s a rather \"hard\"",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "related\nknowledge\nin Bayesian\nco-attention modules\nfor"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "way which is shown to degrade the performance slightly. We",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "multimodal emotion recognition. Experimental results have"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "show the results of word-level co-attention with BAM (Co-",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "shown that\nthe proposed Bayesian co-attention model can"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "att+BAM) next,\nin which we use the prior\n[16] computed",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "outperform the\nbaseline multimodal\nemotion\nrecognition"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "in Equation 2. We can observe that\nincorporat-\nusing Ks",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "methods and achieves an accuracy of 77.0% UA and 75.5%"
        },
        {
          "T+W\nCo-att+BAM+knowledge\n75.6%\n74.0%": "ing the BAM alone without knowledge cannot\nimprove the",
          "Chen et al. [2], 2022\n75.3%\n74.3%": "WA on the 5-fold CV on IEMOCAP."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "tics: Human Language Technologies, 2022, pp. 107–"
        },
        {
          "5. REFERENCES": "[1] Z. Zhao, Y. Wang, and Y. Wang,\n“Multi-level fusion of",
          "Chapter of\nthe Association for Computational Linguis-": "115."
        },
        {
          "5. REFERENCES": "wav2vec 2.0 and BERT for multimodal emotion recog-",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[14] Y. Fu, S. Okada, L. Wang, and et al.,\n“CONSK-GCN:"
        },
        {
          "5. REFERENCES": "nition,” in Proc. Interspeech, 2022, pp. 4725–4729.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "conversational semantic-and knowledge-oriented graph"
        },
        {
          "5. REFERENCES": "[2] W. Chen, X. Xing, X. Xu, and et al., “Key-sparse trans-",
          "Chapter of\nthe Association for Computational Linguis-": "convolutional network for multimodal emotion recogni-"
        },
        {
          "5. REFERENCES": "former for multimodal speech emotion recognition,” in",
          "Chapter of\nthe Association for Computational Linguis-": "tion,” in IEEE International Conference on Multimedia"
        },
        {
          "5. REFERENCES": "IEEE International Conference on Acoustics,\nSpeech",
          "Chapter of\nthe Association for Computational Linguis-": "and Expo (ICME). IEEE, 2021, pp. 1–6."
        },
        {
          "5. REFERENCES": "and Signal Processing (ICASSP), 2022, pp. 6897–6901.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[15] G. Tu,\nJ. Wen, C. Liu,\nand et\nal.,\n“Context-\nand"
        },
        {
          "5. REFERENCES": "[3] M. Chen and X. Zhao, “A multi-scale fusion framework",
          "Chapter of\nthe Association for Computational Linguis-": "sentiment-aware networks\nfor emotion recognition in"
        },
        {
          "5. REFERENCES": "for bimodal speech emotion recognition,”\nin Proc. In-",
          "Chapter of\nthe Association for Computational Linguis-": "IEEE Transactions on Artiﬁcial Intelli-\nconversation,”"
        },
        {
          "5. REFERENCES": "terspeech, 2020, pp. 374–378.",
          "Chapter of\nthe Association for Computational Linguis-": "gence, vol. 3, no. 5, pp. 699–708, 2022."
        },
        {
          "5. REFERENCES": "[4] L. Pepino, P. Riera, and L. Ferrer,\n“Emotion Recogni-",
          "Chapter of\nthe Association for Computational Linguis-": "[16] X. Fan, S. Zhang, B. Chen, and et al.,\n“Bayesian at-"
        },
        {
          "5. REFERENCES": "tion from Speech Using Wav2vec 2.0 Embeddings,”\nin",
          "Chapter of\nthe Association for Computational Linguis-": "tention modules,” Advances in Neural Information Pro-"
        },
        {
          "5. REFERENCES": "Proc. Interspeech, 2021, pp. 3400–3404.",
          "Chapter of\nthe Association for Computational Linguis-": "cessing Systems, vol. 33, pp. 16362–16376, 2020."
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[17] C. Busso, M. Bulut, C. C. Lee, and et al., “IEMOCAP:"
        },
        {
          "5. REFERENCES": "[5]\nJ. Devlin, M. W. Chang, K. Lee, and et al., “BERT: Pre-",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "Interactive emotional dyadic motion capture database,”"
        },
        {
          "5. REFERENCES": "training of deep bidirectional transformers for language",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "Language resources and evaluation, vol. 42, no. 4, pp."
        },
        {
          "5. REFERENCES": "arXiv\npreprint\nunderstanding,”\narXiv:1810.04805,",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "335–359, 2008."
        },
        {
          "5. REFERENCES": "2018.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[18] S. Mohammad,\n“Obtaining reliable human ratings of"
        },
        {
          "5. REFERENCES": "[6] A. Baevski, Y. Zhou, A. Mohamed, and et al., “wav2vec",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "valence,\narousal,\nand\ndominance\nfor\n20,000 english"
        },
        {
          "5. REFERENCES": "2.0: A framework for self-supervised learning of speech",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "the 56th annual meeting of\nwords,”\nin Proceedings of"
        },
        {
          "5. REFERENCES": "representations,” Advances in Neural Information Pro-",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "the association for computational linguistics (volume 1:"
        },
        {
          "5. REFERENCES": "cessing Systems, vol. 33, pp. 12449–12460, 2020.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "Long papers), 2018, pp. 174–184."
        },
        {
          "5. REFERENCES": "[7] Y. Zhang, J. Qin, D. S. Park, and et al.,\n“Pushing the",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[19]\nJ. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normal-"
        },
        {
          "5. REFERENCES": "limits of semi-supervised learning for automatic speech",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "ization,” arXiv preprint arXiv:1607.06450, 2016."
        },
        {
          "5. REFERENCES": "recognition,” arXiv preprint arXiv:2010.10504, 2020.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[20] N. Srivastava, G. Hinton, A. Krizhevsky,\nand et\nal.,"
        },
        {
          "5. REFERENCES": "[8] Z. Fan, M. Li, S. Zhou, and et al., “Exploring wav2vec",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "“Dropout: a simple way to prevent neural networks from"
        },
        {
          "5. REFERENCES": "2.0 on speaker veriﬁcation and language identiﬁcation,”",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "overﬁtting,” The journal of machine learning research,"
        },
        {
          "5. REFERENCES": "arXiv preprint arXiv:2012.06185, 2020.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "vol. 15, no. 1, pp. 1929–1958, 2014."
        },
        {
          "5. REFERENCES": "[9] A. Vaswani, N. Shazeer, N. Parmar, and et al.,\n“Atten-",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[21] K. Gorman, J. Howell, and M. Wagner,\n“Prosodylab-"
        },
        {
          "5. REFERENCES": "Advances in neural\ninformation\ntion is all you need,”",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "aligner:\nA tool\nfor\nforced\nalignment\nof\nlaboratory"
        },
        {
          "5. REFERENCES": "processing systems, vol. 30, 2017.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "speech,” Canadian Acoustics, vol. 39, pp. 192, 2011."
        },
        {
          "5. REFERENCES": "[10]\nJ. Lu, D. Batra, D. Parikh, and et al.,\n“ViLBERT: Pre-",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[22] M. McAuliffe, M. Socolof, S. Mihuc, and et al., “Mon-"
        },
        {
          "5. REFERENCES": "training task-agnostic visiolinguistic representations for",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "treal forced aligner: Trainable text-speech alignment us-"
        },
        {
          "5. REFERENCES": "Advances in neural\ninfor-\nvision-and-language tasks,”",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "ing kaldi,” in Proc. Interspeech, 2017, pp. 498–502."
        },
        {
          "5. REFERENCES": "mation processing systems, vol. 32, 2019.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[23] D. P. Kingma and J. Ba, “Adam: A method for stochastic"
        },
        {
          "5. REFERENCES": "[11] S. Schneider, A. Baevski, R. Collobert,\nand\net\nal.,",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "optimization,” arXiv preprint arXiv:1412.6980, 2014."
        },
        {
          "5. REFERENCES": "“wav2vec:\nUnsupervised\nPre-Training\nfor\nSpeech",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "Recognition,”\nin Proc.\nInterspeech, 2019, pp. 3465–",
          "Chapter of\nthe Association for Computational Linguis-": "[24] M. R. Makiuchi, K. Uto, and K. Shinoda, “Multimodal"
        },
        {
          "5. REFERENCES": "3469.",
          "Chapter of\nthe Association for Computational Linguis-": "emotion recognition with high-level speech and text fea-"
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "tures,”\nin 2021 IEEE Automatic Speech Recognition"
        },
        {
          "5. REFERENCES": "[12] Y. Liu, M. Ott, N. Goyal, and et al.,\n“RoBERTa: A",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "and Understanding Workshop (ASRU). IEEE, 2021, pp."
        },
        {
          "5. REFERENCES": "robustly optimized BERT pretraining approach,” arXiv",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "350–357."
        },
        {
          "5. REFERENCES": "preprint arXiv:1907.11692, 2019.",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "[25] P. Kumar, V. Kaushik, and B. Raman, “Towards the ex-"
        },
        {
          "5. REFERENCES": "[13]\nJ. Bai, Y. Wang, H. Sun, and et al.,\n“Enhancing self-",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "plainability of multimodal speech emotion recognition,”"
        },
        {
          "5. REFERENCES": "attention with knowledge-assisted attention maps,”\nin",
          "Chapter of\nthe Association for Computational Linguis-": ""
        },
        {
          "5. REFERENCES": "",
          "Chapter of\nthe Association for Computational Linguis-": "in Proc. Interspeech, 2021, pp. 1748–1752."
        },
        {
          "5. REFERENCES": "Proceedings of\nthe Conference of\nthe North American",
          "Chapter of\nthe Association for Computational Linguis-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multi-level fusion of wav2vec 2.0 and BERT for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "A multi-scale fusion framework for bimodal speech emotion recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "6",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee"
      ],
      "year": "2018",
      "venue": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Pushing the limits of semi-supervised learning for automatic speech recognition",
      "authors": [
        "Y Zhang",
        "J Qin",
        "D Park"
      ],
      "year": "2020",
      "venue": "Pushing the limits of semi-supervised learning for automatic speech recognition",
      "arxiv": "arXiv:2010.10504"
    },
    {
      "citation_id": "9",
      "title": "Exploring wav2vec 2.0 on speaker verification and language identification",
      "authors": [
        "Z Fan",
        "M Li",
        "S Zhou"
      ],
      "year": "2020",
      "venue": "Exploring wav2vec 2.0 on speaker verification and language identification",
      "arxiv": "arXiv:2012.06185"
    },
    {
      "citation_id": "10",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "14",
      "title": "Enhancing selfattention with knowledge-assisted attention maps",
      "authors": [
        "J Bai",
        "Y Wang",
        "H Sun"
      ],
      "year": "2022",
      "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "15",
      "title": "CONSK-GCN: conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Y Fu",
        "S Okada",
        "L Wang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "16",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Bayesian attention modules",
      "authors": [
        "X Fan",
        "S Zhang",
        "B Chen"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "19",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
      "authors": [
        "S Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "20",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "21",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "22",
      "title": "Prosodylabaligner: A tool for forced alignment of laboratory speech",
      "authors": [
        "K Gorman",
        "J Howell",
        "M Wagner"
      ],
      "year": "2011",
      "venue": "Canadian Acoustics"
    },
    {
      "citation_id": "23",
      "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
      "authors": [
        "M Mcauliffe",
        "M Socolof",
        "S Mihuc"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "25",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "26",
      "title": "Towards the explainability of multimodal speech emotion recognition",
      "authors": [
        "P Kumar",
        "V Kaushik",
        "B Raman"
      ],
      "venue": "Proc. Interspeech, 2021"
    }
  ]
}