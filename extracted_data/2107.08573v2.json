{
  "paper_id": "2107.08573v2",
  "title": "Affectivetda: Using Topological Data Analysis To Improve Analysis And Explainability In Affective Computing",
  "published": "2021-07-19T01:28:50Z",
  "authors": [
    "Hamza Elhamdadi",
    "Shaun Canavan",
    "Paul Rosen"
  ],
  "keywords": [
    "Affective computing",
    "topological data analysis",
    "explainability",
    "visualization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Fig. 1. Our affective computing visualization provides numerous options for comparing and contrasting the data. For example, the small multiples view (left) is comparing a male (left) subject to a female (right) subject considering different subsets of facial landmarks (columns), across emotions (rows) of anger , disgust, fear , happiness, sadness, and surprise. Each point represents one facial pose. The embedding graph (top right) compares all facial poses across all emotions, in this case showing the female full face using MDS on non-metric topology. The 3D landmarks (lower right) show a single facial pose per emotion. Settings are selected on the bottom.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing, computer-based detection of human affects, has applications that span education (e.g., judging learners' confidence), healthcare (e.g., judging pain), and product marketing (e.g., measuring consumers' response to products). Early work in measuring affect began in the late 1960's spearheaded by Ekman and Friesen  [23] . Their work culminated in a classification of six basic emotions: anger, disgust, fear, happiness, sadness, and surprise  [24] , which were later expanded  [22] . The field of affective computing has seen significant growth since the seminal work from Rosalind Picard  [65] . The vast majority of research in affective computing has been focused on machine learning algorithms trained on emotion data to classify affect. Like many machine learning solutions, these neural networks focused on classifying the input emotion and ignored data inspection and decisionmaking explainability.\n\n• Hamza Elhamdadi is with the University of Massachusetts Amherst. E-mail: helhamdadi@umass.edu. • Shaun Canavan is with the University of South Florida. E-mail: scanavan@usf.edu. • Paul Rosen is with the University of South Florida. E-mail: prosen@usf.edu. To inspect the data, an effective visual representation of emotion data must address numerous challenges. First, the affect data are quite large, captured by multiple high-speed video cameras. Fortunately, previous affective computing research already partially addressed this issue by reducing the data to 83 landmark points tracked temporally. Nevertheless, the problem remains challenging because patterns in emotion occur over extended time periods, represented by a series of 83-landmark poses. Furthermore, patterns of interest may occur in different time sequences lasting for different lengths of time, making alignment and comparison non-trivial. Finally, changes in landmarks are simultaneously subtle and subject to noise from the extraction process, making them difficult to observe.\n\nThis paper presents a visual analytics approach utilizing Topological Data Analysis (TDA) to examine emotion data respective and irrespective of time. By using TDA to address this problem, our approach can capture and track the topological \"shape\" of facial landmarks over time in a manner robust to noise  [20] . After analysis, the data are presented for investigation using familiar visualizations, e.g., timelines (see Fig.  5  top) and scatterplots (see Fig.  1  top right), and through landmark-based representations (see Fig.  1  bottom right or Fig.  10 ). These interfaces enable tracking facial movement, comparing emotions, and comparing individuals, while also providing the ability to derive precise explanations for features identified in the data.\n\nA natural question at this point would be, why is TDA well-suited to this problem? Our approach utilizes one of the foundational tools of TDA, namely persistent homology. There are four main advantages 1. a mapping of affective computing data to TDA (see Sect. 4), including a novel non-metric formulation of geometry for faster and more accurate topology extraction (see Sect. 4.3); 2. a visual analytics interface that enables analyzing, comparing, and contrasting multiple data configurations (see Sect. 6); 3. an evaluation that uses our methodology to explain features in data that were extracted by state-of-the-art emotion detection machine learning algorithms (see Sect. 7.2); and 4. an evaluation of the ability of TDA to differentiate emotions within the same individual (see Sect.  7 .3) and differentiate multiple individuals showing the same emotion (see Sect. 7.4).\n\nPerhaps most importantly, our approach opens the door to explainability in a way that may help to unlock open questions in the affective computing community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background In Affective Computing",
      "text": "Affective computing has applications in fields as varied as medicine  [85] , entertainment  [31] , and security  [45] . Most notably, the expression recognition sub-field focuses on detecting subjects' affective states automatically.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Expression Recognition",
      "text": "While successful 2D facial-expression image recognition exists  [29, 43, 49] , the approaches suffer from weaknesses, such as occlusion from, e.g., a rotating head. We focus our discussion instead on a few representative 3D facial recognition approaches. Zhen et al.  [91]  developed a model that localized points within each muscular region of the face and extracted features that include coordinate, normal, and shape index  [46] . The features were then used to train a Support Vector Machine (SVM)  [77]  to recognize expressions. Xue et al.  [82]  proposed a method for 4D (3D + time) expression recognition, which showed promise differentiating difficult emotions, such as anger and sadness. The method extracted local patch sequences from consecutive 3D video frames and represented them with a 3D discrete cosine transform. Then, a nearest-neighbor classifier was used to recognize the expressions. Hariri et al.  [36]  proposed an approach to expression recognition using manifold-based classification. The approach sampled the face by extracting local geometry as covariance regions, which were used with an SVM to recognize expressions. Some recent techniques showed that not all regions of the face carry the same importance in emotion recognition. Hernandez-Matamoros et al.  [37]  found that segmenting the face based on the eyes and mouth resulted in improved expression recognition. Fabiano et al.  [28]  further illustrated that different areas of the face carry different levels of importance for emotions, e.g., one subject happiness had more important features on the right eye and eyebrow, while embarrassment had more on the left eye and eyebrow. We utilize this information in our visualization design by targeting specific subsets of facial features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Affective Computing In Visualization",
      "text": "There has been limited work in the visualization community on affective computing; what exists has been primarily focused on visualizing affective states, i.e., considering valence and arousal, not inspecting the landmarks used as input to affective computing algorithms.\n\nEarly work on visualizing affective states concerns the glyph-based Self-Assessment Manikin (SAM), which measures pleasure, arousal, and dominance of a person's affective state  [7] . Cernea et al.  [9]  later described guidelines for conveying the user emotion through the use of widgets that depict the affective states of valence and arousal. The widgets employed emotion scents, hue-varied colormaps representing either valance or arousal, e.g., red and green represent negative and positive valance, respectively. Emotion-prints was an early system to provided real-time feedback of valance and arousal to users using touch-displays  [10] . More recently, Kovacevik et al.  [47]  employed ideas from SAM and emotion scents to create a glyph for simultaneous representation of valence and arousal. Their research focused on video game players' and developers' awareness of emotions elicited from a particular gaming experience. For visualizing affect over extended periods, AffectAura provided an interface that enabled users to visualize emotional states over time for the purpose of reflection  [56] .\n\nThere has also been some work visualizing the affective state of multiple individuals using, e.g., virtual agents in collaborative work  [11]  or using a visual analytics interface to access the emotional state of students in a classroom  [86] . Qin et al.  [66]  created HeartBees, which was an interface to demonstrate the affect of a crowd using physiological data. The interface used an abstract flocking behavior to demonstrate the collective emotional state.\n\nIn contrast to all of these prior approaches, our work focuses on using TDA and visualization to investigate the data used in classifying expression, i.e., the input data, not the emotional state itself. There has been some recent work that looked at the explainability of deep networks in expression recognition, e.g.,  [62] . These approaches focus on visualizing heatmaps that highlight what parts of the image most influenced decision making, not necessarily why.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "To evaluate our approach, we use the BU4DFE 3D facial expression dataset  [84] , which has been extensively used for expression recognition  [12, 27, 63, 74] , 3D shape reconstruction  [32, 33, 53] , face tracking  [8, 64] , and face recognition  [3, 30, 42, 72] . The dataset contains 101 subjects (58 female and 43 male) from multiple ethnicities, including Caucasian, African American, Asian, and Hispanic, with an age range of 18-45 years old. Each modality has the six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. For each sequence, the expression is the result of gradually building from neutral to low then high intensity and back again. Each of the video sequences is 3-4 seconds in length.\n\nThe data are captured using the Di3D dynamic face capturing system  [17] , which consists of three cameras, two to capture stereo and one to capture texture. Passive stereophotogrammetry is used on each pair of stereo images to create the 3D facial pose models with an RMS accuracy of 0.2 mm. Each 3D model contains 83 facial landmarks (see Fig.  2 ), which correspond to the key areas of the face that include the mouth, eyes, eyebrows, nose, and jawline. The landmarks are the result of using an active appearance model  [14]  that detects the landmarks on the 2D texture images, which are aligned and projected into the corresponding 3D models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview Of The Pipeline",
      "text": "TDA has received significant attention in the visualization community, e.g.,  [73] . We utilize a foundational tool of TDA, persistent homology, which has been studied in graph analysis  [34, 35, 67, 71] , high-dimensional data analysis  [78] , and multivariate analysis  [68] . We utilize persistent homology to capture the topology of the landmarks of each facial pose into a structure known as a persistence diagram. We then compare the topology of different subsets of facial poses to reveal their relationships. Our processing pipeline contains three main stages, which are fed into the visualization (see Sect. 6).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stage 1:",
      "text": "The first stage is extracting the topology of a single facial pose. We offer two variations, a Euclidean metric-based approach (see Sect. 4.1) and a novel non-metric-based approach (see Sect.  Stage 3: Finally, using the topological dissimilarity, we utilize a variety of dimension reduction techniques to highlight different aspects of the dissimilarity between groups of facial poses (see Sect. 5.2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Topological Data Analysis Of Facial Landmarks",
      "text": "We consider two variations for extracting the topology of facial poses, a Euclidean metric approach, followed by a novel non-metric variant.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Euclidean Metric Persistent Homology On Landmarks",
      "text": "Homology deals with the topological features of a space. Given a topological space X, we are interested in extracting the H 0 (X) and H 1 (X) homology groups, which correspond to (connected) components and tunnels/cycles of X, respectively  2  . In practice, there may not exist a single scale that captures the topological structures of the data. Instead, we use a multi-scale notion of homology, called persistent homology, to describe the topological features of a space at different spatial resolutions. We briefly describe persistent homology in our limited context. Nevertheless, understanding persistent homology can be daunting for those who are unfamiliar with it. For a high-level overview, see  [79] , or for detailed background, see  [19] .\n\nTo calculate the persistent homology of a single facial pose, we first calculate the Euclidean distance between all 83 landmarks. We then apply a geometric construction, the Rips complex, R(r), on the point set. In brief, for a given distance, r, the Rips complex has all 0-simplicies, i.e., points, for all values of r. A 1-simplex, i.e., an edge, between two points is formed iff r is greater than or equal to their distance. A 2-simplex, i.e., a triangle, is formed among three points iff r is greater than or equal to every pairwise distance between the points.\n\nTo extract the persistent homology (see Fig.  2 (a)), we consider a finite sequence of increasing distances, 0 = r 0 ≤ r 1 ≤ • • • ≤ r m = ∞. A sequence of Rips complexes, known as a Rips filtration, is connected by inclusions, R(r 0 ) → R(r 1 ) → • • • → R(r m ), and the homology of each is calculated, tracking the homomorphisms induced by the inclusions,\n\nAs the distance increases, topological features, i.e., components and tunnels, appear and disappear. The appearance is known as a birth event, r b i , and the disappearance is known as a death event, r d i . The birth and death of all features are stored as a multi-set of points in the plane, (r b i , r d i ), known as the persistence diagram, which is often visualized in the scatterplot display (see Fig.  2(b) ). From the points, we devise an importance measure, called persistence, which helps to differentiate signal from noise. The persistence is simply the difference between the birth and death of a feature, i.e., r d ir b i . Furthermore, in visualizations of the persistence diagram, such as Fig.  2 (b), distance from the diagonal dotted line represents the persistence of a feature.\n\nIn addition to considering all the topology of all landmarks, we provide the user the functionality to consider only related subsets of features. In particular, they have the option of including/excluding jawline, mouth, nose, left/right eyes, and left/right eyebrows in the calculation of the topology.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Interpolating Known Geometry",
      "text": "Our computation using facial landmarks ignores an important aspect of the data, namely the known connectivity between landmarks. In other words, landmarks of, e.g., the mouth, have known connectivity to their neighboring landmarks. Fig.  3 (f) shows this connectivity. This raises two questions. First, does our failure to consider this connectivity impact the features we extract, and second, how do we efficiently consider the connectivity?\n\nWe first consider using interpolation of the connectivity to supersample additional landmarks. For our experiment, we take the known connectivity and interpolate across each edge, such that points are no further than a user-defined ε apart. Fig.  3 (b) through Fig.  3(e)  show four examples with ever-smaller ε values. As expected, as ε gets smaller, the data looks increasingly similar to the known connectivity in Fig.  3(f) .\n\nWe now consider the impact of the connectivity by comparing the persistence diagrams of H 1 features in the original data in Fig.  3 (a) to the lowest ε data in Fig.  3 (e). The persistence diagrams are clearly different (the H 0 features are also different but more difficult to observe pictorially). The difference is exceedingly important because it means using the 83 landmark points alone is insufficient to capture the topological structure of the data.\n\nTo overcome this limitation, we considered using the supersampled landmarks for calculations. However, there are three interrelated problems to this approach. (1) The first is the challenge of selecting an appropriate ε value. The smaller the value, the closer the representation is to the geometric structure. For example, Fig.  3 (c) appears sufficient for this example, but it is unclear if this is sufficient for all of the data, leading one to perhaps select an even smaller ε. (2) However, the second challenge is that the smaller the ε, the longer the computation time for detecting the topological features. Fig.  4 (a) shows that as ε is divided in half, the compute time grows exponentially. (3) The third related challenge is that the smaller ε, the greater the number of topological features generated. Fig.  4(b)  shows this extreme growth. To make matters worse, the vast majority of these features are topological noise with very low persistence. In other words, they do not contribute to our understanding of the shape of the face.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A Non-Metric Variant Of Persistent Homology",
      "text": "We instead use a novel modification to the persistent homology calculation to utilize this connectivity as follows. Instead of considering 83 landmark points, we consider the relationship between 81 landmark edges formed by the known connectivity of the landmark points (see Fig.  3(f) ). We calculate a distance matrix representation of the landmark edges, where the distance is the shortest Euclidean distance between line segments. Finally, we run persistent homology calculations on this distance matrix.\n\nOne immediate question should be the appropriateness of this configuration for persistent homology calculations, particularly considering that this representation breaks two important axioms of a metric space, namely the identity of indiscernibles and the triangle inequality. Fortunately, persistent homology calculations themselves do not explicitly require a metric space-they have a weaker requirement of inclusion  [19] . In other words, as long as in the filtration R(r i ) ⊂ R(r i+1 ), the calculation can proceed. The challenge is that the Rips complex does require that the underlying space is metric. We define a new non-metric Rips complex that satisfies the inclusion property, where: 0-simplicies, representing landmark edges, are present for all values of r; 1-simplicies appear when r is strictly greater than the non-metric distance between a pair of 0-simplicies; and 2-simplices appear when r is strictly greater than all of the non-metric distances of the three related 1-simplicies. Fortunately, this definition is similar enough to the standard Rips complex that careful ordering of inclusions (i.e., observing the strictly greater than cases) in the filtration allows us to utilize conventional persistent homology tools on our non-metric distances.\n\nFig.  3 (f) shows the landmark edges and the persistence diagram of the associated H 1 features. Our non-metric approach overcomes all three limitations of supersampling. (1) The result is very similar to the output of the supersampling in Fig.  3 (e) without the need for specifying any ε parameter. (2) Furthermore, Fig.  4 (a) shows that the compute time for our non-metric approach is approximately the same as that of the original 83 landmark points. (3) Finally, Fig.  4(b)  shows the number of topological features output is small (i.e., we avoid outputting extraneous topological noise).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparing Facial Pose Topology",
      "text": "Thus far, we have introduced a method for extracting the topological features from a single facial pose. We now describe how we compare the topology of multiple facial poses. We start by describing the notion of topological distance between persistence diagrams, which serves as a pairwise dissimilarity between them (see Sect. 5.1). Next, we discuss how dimension reduction is used on all pairwise dissimilarities to cluster, compare, and summarize changes in topology (see Sect. 5.2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dissimilarity Between Poses",
      "text": "Once persistence diagrams are calculated, we wish to explore the relationship between them by performing pairwise comparisons of features of the persistence diagrams. This type of pairwise comparison is commonly performed using bottleneck or Wasserstein distance  [20] .\n\nIntuitively speaking, these measures find the best match between the features of two persistence diagrams and report the topological feature of the largest distortion, in the case of bottleneck distance, or the average topological distortion, in the case of Wasserstein distance.\n\nTechnically speaking, consider two persistence diagrams, X and Y , let η be a bijection, with all diagonal points, (x, x), added for infinite cardinality  [44] . The bottleneck distance is W ∞ (X,Y ) = inf η:X→Y sup x∈X xη(x) ∞ , and the 1-Wasserstein distance, which we use, is W 1 (X,Y ) = inf η:X→Y Σ x∈X xη(x) ∞ . Our implementation computes the bottleneck and 1-Wasserstein distance for H 0 and H 1 features separately, and combines the results. In other words, for bottleneck,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Summarizing Topological Dissimilarity",
      "text": "Once the set of all persistence diagrams is calculated, we explore the relationship between them by calculating all pairwise dissimilarities between poses, forming a dissimilarity matrix representing all of the topological variations between facial poses. However, a dissimilarity matrix, such as this, is difficult to explore directly. We investigated several options to represent and evaluate the relationship between different facial poses and emotions. Importantly, each technique preserves a different aspect of the dissimilarity matrix, providing different perspectives on the data.\n\nThe first approach we used is 1D relative distance. In this approach, a keyframe or focal pose is selected by the user. All other facial poses are positioned by their relative distance (i.e., pairwise distance) to that keyframe. Relative distance perfectly preserves the relationship between the keyframe and all other frames. It does not, however, provide information about the relationship between other pairs of frames.\n\nNext, we consider two dimension reduction techniques, with each using the pairwise dissimilarity matrix directly. We first consider Multidimensional Scaling (MDS)  [48] , which tries to preserve pairwise distances between the topology of poses. Second, we use t-SNE  [76]  and UMAP  [57] , which attempt to preserve the clustering structure by considering a local neighbor. Both t-SNE and UMAP contain hyperparameters that can impact the structures visible to the user. We have performed a structured evaluation of various hyperparameters and found that the structures visible in our results are, by-and-large, stable across a wide variety of parameter values (see our supplement for an example). Therefore, we use the default parameters in our evaluation. To measure the dimension reduction quality for all methods, we cal-culate the goodness-of-fit using the Spearman rank correlation of the Shepard diagram (denoted in the lower right of images as Rank).\n\nNote that none of these approaches directly consider time. Nevertheless, the temporal components of the data are presented in the visualization when relevant.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Visualization",
      "text": "To examine the topological structure of facial landmark data, we built a visualization (see Fig.  1  and Fig.  10 ) with the following design criteria:\n\n[D1] provide multiple ways to evaluate temporal and non-temporal aspects of the data (e.g., animated, static, and non-temporal visualizations); [D2] provide multiple conditional perspectives (e.g., bottleneck vs.\n\nWasserstein, MDS vs. t-SNE vs. UMAP, etc.) on the topology; [D3] allow comparison of data between two or more emotions;\n\n[D4] allow for investigating subsets of landmarks; and\n\n[D5] provide direct explanations for the topological differences between facial poses.\n\nSmall Multiples (Fig.  1   . Each small multiple is shown using the visualization modality chosen in the settings at the bottom.\n\nEmbedding Graph (Fig.  1   Persistence Diagrams (Fig.  10 ) When additional details about a given facial pose are desired, the persistence diagram captured by persistent homology is represented by a scatterplot [D5]. The persistence diagram plots feature birth horizontally and death vertically. In this context, H 0 features are represented as solid squares, and H 1 features are represented as rings. The size of each element is proportional to its persistence (i.e., importance). Furthermore, the distance from the dashed diagonal to a feature is also a measure of persistence.\n\nRepresentative Components and Cycles (Fig.  10 ) A byproduct of the calculation of persistent homology is a structure known as generators, which are the landmark elements that generated a particular topological feature. For H 0 , the generators are the 0-simplices representing the joining of two components. For H 1 features, the data are output in the form of a representative cycle 3 . Each topological feature 3 For reasons outside the scope of this paper, there is no single generator for a cycle but instead a class of generators. A representative cycle, which is a is associated with a generator that we use to identify what input data generated that topological feature, with a general focus on the high persistence features in the data [D5].",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "We evaluate our approach by first performing a detailed evaluation of two individuals-one female ('F001') and one male ('M001') from the BU4DFE expression dataset  [84] . We then evaluate the ability of our approach to differentiate individuals using the entire dataset of 101 subjects. Each of these individuals has approximately 600 facial poses (6 emotions × ∼ 100 frames per emotion). Since the data provided are large and time-varying, our approach allows conditional observation of the topology of emotions based upon individuals, emotions, selected subset of facial features (full face, eyes+nose, mouth+nose, eyebrows+nose), topological dissimilarity, and dimension reduction technique. Our evaluation looks at how these conditional comparisons can be matched to known phenomena in affective computing. We note that one of the coauthors of this paper, Shaun Canavan, is a researcher in affective computing and provided detailed feedback at every stage of the design.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation And Performance",
      "text": "We implemented our approach using Python for data management, non-metric distance, and dimension reduction calculations, ripser  [6]  for persistent homology calculations, Hera  [44]  for topological distance, and D3.js for the user interface. Persistent homology and topological dissimilarity are pre-calculated for all combinations of landmark subsets. Dimension reduction is performed at run-time, taking at most a few seconds; as this data is calculated, it is also stored in a short-term cache to improve performance. The user interface is interactive. Our source code is available at https://github.com/ USFDataVisualization/AffectiveTDA.\n\nWe evaluated the computational performance of the persistent homology and bottleneck and Wasserstein dissimilarity matrix calculations for F001 and M001 in Table  1 . The calculations were performed on a Linux workstation with a 3.40GHz Intel i7-6700 CPU and 48 GB of RAM. In this table, we compare the metric landmark point-based approach and our novel non-metric landmark edge-based approach. Comparing persistent homology calculations, our non-metric approach took approximately twice as long as the metric approach. This is entirely attributable to the extra cost of calculating segment-segment distance (instead of point-point distance). The performance benefit of the non-metric approach comes with the calculation of the dissimilarity matrices, which saw a 10x -15x speedup over the metric approach. This is attributable to the reduced number of noise features created by the non-metric approach, as described in Sect. 4.3. Overall, our approach saw a speedup of ∼ 7.5x. Table  1 . Computation time for metric (M) and non-metric (NM) approaches to extract persistent homology features and calculate the dissimilarity matrix for all frames from each subject, including subsets of facial landmarks (full face, eyes+nose, mouth+nose, eyebrows+nose). The number of landmarks input to each method is similar, 83 for full face metric and 81 for non-metric. In addition, we show the average number of H 0 and H 1 features generated per frame.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Relative Distance Topology And Action Units (Aus)",
      "text": "In affective computing, there are various approaches for recognizing expressions, as detailed in Sect. 2.1. One promising approach is the use of action units (AUs)  [25] , which are facial muscle movements linked to expression. AUs are represented as an intensity from [0, 5], where 0 is byproduct of a process called boundary matrix reduction, is output instead  [21] .\n\ninactive, and > 0 is an active AU, with higher values representing more intense movement. Specific configurations of active AUs have been shown to be useful for recognizing facial expressions  [51, 52, 54, 69, 81] . AUs are generally created in one of two ways. Either an expert manually annotates video frames, or a machine learning algorithm extracts them from the data. While the former is a slow and tedious process, the latter is fast but lacks any explainability in measuring the activity of AUs. We automatically detect 17 AUs (see Table  2 ) using the publicly available OpenFace toolkit  [4] , which is commonly used in affective computing literature  [75] . However, coming from a machine learning model, the extracted AUs lack specific explainability.\n\nWe now demonstrate how our topology-based system can explain certain AU features detected by OpenFace by comparing the output of each. Our approach is as follows, since all sequences begin with a neutral pose, we consider relative distance with respect to the first frame of the sequence. We hypothesized that we would observe similar signals in the AUs and their associated facial features using our topology-based approach. Fig.  5  shows two examples of this relationship. In Fig.  5 (a), we compare the activity of the nose and eyes to AU45 (blink) for the F001 disgust emotion. The relative distance shows three clear spikes at the same frames as AU45 (approximately frames 28, 52, and 75). However, AU45 does not tell the entire story of the activity that the topology is capturing.\n\nInstead, we hypothesize that the topology is a combination of multiple AUs. Fig.  5(b)  shows an example comparing the mouth+nose to AU14 (dimple) and AU25 (lips part) for the F001 fear emotion. In this case, a linear combination of both AUs seems to capture a more complete picture of the activity represented in the topology. A broader analysis of both subjects, multiple emotions, and multiple facial features, as seen in Fig.  7  and Fig.  8 , revealed these relationships are widely observable. This confirms our hypothesis of a strong similarity between topology features and AUs.\n\nNevertheless, there is still not a perfect one-to-one relationship between the topology and AUs. The AUs go through further contextual processing than the topology does, e.g., to separate the activity of AU7 (eyelid tightening) from AU45 (blinking) and other related movements. One challenge with the contextual processing in the state-of-the-art in affective computing is the lack of explainability, which our topologybased approach provides (see Sect. 8).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Comparing And Differentiating Expressions",
      "text": "Next, we consider whether the topological features are sufficient for differentiating the six different emotions present in the data. To perform this evaluation, we look at the full face topology for the female subject using t-SNE (Fig.  6 (a), Shepard fitness: 0.79) and MDS (Fig.  1 , Shepard fitness: 0.88), and male subject using t-SNE (Fig.  6 (b), Shepard fitness: 0.78) and MDS (Fig.  6 (c), Shepard fitness: 0.88). We begin by examining the female and male subjects using t-SNE, as seen in Fig.  6 (a) and Fig.  6 (b), respectively. We make three important observations about the resulting images. (1) First, for both subjects, the emotional states tend to form separate clusters, indicating that they are indeed differentiable. This is particularly important if the topology were to be used for predicting unknown emotional states. (  2 ) Second, most of the emotions begin and end towards the centers of the plots. This colocation is caused by the neutral facial pose that subjects were asked to begin and end with each sequence. (3) The final observation is that the facial poses form temporally coherent 'strings.' This observation is particularly poignant, considering that nowhere in calculating the topological dissimilarity does it utilize temporal information.\n\nWe next consider the MDS projections for the female subject and male subject in Fig.  1  and Fig.  6(c) , respectively. With the female  subject, we observe that happiness, surprise, fear, and sadness largely cluster into separate regions of the plot with limited overlap or mixing. The anger and disgust emotions, on the other hand, overlap significantly, which corresponds with the recent literature in the affective computing community that considers them to be similar expressions  [59] . Another interesting finding is that the emotional states of the male are less differentiated than those of the female. Interestingly, it is commonly accepted in affective computing that, in general, the expressiveness of females is more differentiable than that of males  [15] . Given that we are only observing two subjects, no broad gender-based conclusions can be made in our case. Nevertheless, this female's expressions are more differentiated than this male's expressions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Comparing And Differentiating Individuals",
      "text": "Finally, we consider how topological features allow the differentiation of each of the 101 subjects in the BU4DFE dataset.\n\nTo perform this evaluation, we look at the full face topology of a subset of 10 subjects (F001-F010) using t-SNE (Fig.  9 (a)-9(f)). We notice that, for all six emotions, all ten subjects form relatively independent clusters; this is particularly true of the anger (Fig.  9(a) ) and sadness (Fig.  9 (e)) emotions, while some minor overlap occurs for a few of the individuals in the other emotions.\n\nWe performed a similar evaluation for all 101 subjects (58 female and 43 male) (Fig.  9 (g)-9(l)). We can see that the clustering behavior seen with only 10 subjects scales to all the subjects of the dataset. To test the robustness of this t-SNE result to variations in hyperparameters, we ran the tests with four different perplexities (30, 40, 50, and 100) and found that the clusters remained roughly constant throughout (see supplemental material). The clustering phenomenon present in the t-SNE dimension reduction images was also present when we used UMAP (see supplemental materials).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Contribution To Affective Computing",
      "text": "Due to the challenging nature of detecting AUs and determining emotion from expression, we hypothesize that our TDA-based approach can be used to provide new insights into these challenges. As it has been shown that temporal AU information can make recognizing emotions easier, our approach evaluates temporal facial expressions (i.e., AUs), which allows us to visualize a new representation of this data. As shown in Sect. 7, this representation shows the similarity between the topological signals and the AU signals over time, which provides the following insight, as validated by the coauthor on this paper, who is a researcher in affective computing.\n\nValidation That AUs Are Correctly Detected A limitation of current machine learning AU detection models is their accuracy, as little improvement has been made compared to previous models  [39] . This is mainly due to the models detecting AUs that are not active, as well as not detecting AUs that are active. Our TDA-based approach can validate that the detected AUs are correctly capturing the muscle movement of the face. More specifically, the proposed approach will ensure that the AUs that have been detected are correct. As shown in Fig.  5 (a), AU45 has high-intensity values three times during the sequence. This correctly corresponds to the three blinks that occur in the data, which are also captured in the topological signal. If the blinks were not captured in the topological signal, then the spikes in the AU intensity signal could be attributed to mislabelling or noise. This could facilitate more intelligent active learning  [1]  that would improve the machine learning detection models.\n\nRelationship Between Which AUs Occur Together AU cooccurrences  [70]  and patterns  [39]  can have a significant impact on the accuracy of machine learning models. Our approach can also give insight into multiple AUs that occur together, including specific AUs that occur when an emotion is expressed. This is detailed in Fig.  5(b) , where AU14 and AU25 are active at different intensities at different times during the sequence. The topological signal shows a combination of the two AU signals corresponding to the most intense segments of the active AUs. AU14 is a dimple, and AU25 is active when the lips part, which are common muscle movements that could occur during a wide smile. When a smile occurs, AU6 and AU12 are commonly found together, according to the Facial Action Coding System (FACS)  [25] . However, according to Barrett et al.  [5] , expressions vary across cultures and situations meaning, AU6 and AU12 may not be active in all smiles. The proposed approach can provide insight into this phenomenon, allowing investigations of the relationships of new AUs, over time, for different expressions.\n\nDetecting Facial Expressions There are many successful approaches to detecting facial expressions in affective computing  [49, 50, 58, 83, 87] . Considering this, the purpose of the proposed approach is not to detect facial expressions but to provide greater analysis and explainability of the data. The insight provided by our visualization will allow new insight not previously seen in affective computing, as we can analyze the movement of the face using the proposed TDA-based approach, which directly corresponds to AU movements. This will result in new, more accurate ways of building facial expression detection models.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Explainability Of Machine Learning Models Machine Learning Has",
      "text": "given us many advancements in fields as diverse as medicine  [80] , security  [2] , and education  [16] . However, one of the main limitations is the lack of explainability  [18] . Considering this, one of the key advantages of TDA over machine learning is the explainability of the features identified in the process. We demonstrate this using an example of four facial poses from the female surprise data, as shown in Fig.  10 . These examples focus on the opening and closing of the eyes and mouth, which is commonly associated with a surprised expression. Given a machine learning model that successfully detected the AUs associated with this expression, with the long list of possible muscle movements (e.g., AU1, AU2, AU5, AU25, and AU26), it is difficult to understand why such a model detected them. This is especially true given the black-box nature of neural networks  [61] . In Fig.  10 , we can directly see the features that change the data. In the persistence diagrams, the number and persistence of the most important features are clearly different. Furthermore, when evaluating the representative cycles, we can further associate the landmark geometry of each high persistence feature in the data. This explainability will facilitate more accurate emotion recognition systems. This is due to the insight that the explainability will give affective computing researchers to better understand and tune their models, resulting in more accurate models.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Topology Doesn'T Capture Everything",
      "text": "Limits of Topology Some of the advantages of topology over geometry are also its biggest weaknesses. There are certain shapes of the data that may not be captured by topology alone. For example, smiles and frowns may have the same topological shape. That said, the relationship between the smile, nose, and jawline may be sufficient enough to disambiguate between smiles and frowns. Furthermore, smiles and frowns will also be associated with other changes in facial features, e.g., changes in eye or eyebrow shape. At the very least, our evaluation showed that smile emotions, e.g., happiness, and frown emotions, e.g., sadness, were differentiable in Sect. 7.3.\n\nDifferences Between TDA and Machine Learning Topology does not capture all features of AUs. The AU extraction may utilize other data, nonlinearities, knowledge of physiological relationships of AUs, etc., to determine the extent of the activation of the AUs. That said, it is also important to understand that the AU information is not ground truth. It is the output of a machine learning technique, and it may, in fact, not be showing genuine AU activation. On the other hand, all of the topological features we observed are in the data, and for any matching feature, TDA provides evidence as to why the machine learning algorithm classified AU activation as it did.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Challenges And Limitations Of Automatic Au Detection",
      "text": "The use of AUs for expression recognition is a promising approach. However, there are significant challenges in the detection of them  [26] . Many works that have developed approaches for automatically detecting AUs using machine learning have focused on learning single AUs. However, it has been shown that patterns of AUs can have a significant impact on detection  [39] . This leads to a bigger limitation of current machine learning approaches to AU detection, namely the data. Stateof-the-art machine learning models require a large amount of good data to be accurate. Current models are trained on data that has biases in ethnicity  [55] , as well as significant imbalances in the distribution of AUs  [88, 89] . Along with these data biases and imbalances, the ground truth data is often manually annotated, which is subjective and can lead to errors  [60] . This results in machine learning models that are not learning how to represent an AU but learning the distribution of data  [39] . In machine learning, many times, the solution to the problem is to collect more data. In the case of AUs, it has the additional challenge that multiple AUs occur simultaneously  [70] , resulting in an unresolvable imbalance of data for the AUs that occur more often (e.g., AU6 or AU12)  [39] . These challenges lead to machine learning models that often fail to recognize AUs that are active, as well as recognize AUs that are not active  [90] .\n\nAlong with challenges in detecting AUs, there is a larger discussion of how humans learn and express emotions  [41] . Broadly, this discussion can be categorized into two hypotheses. The first hypothesis states that emotions can be recognized from facial expressions (AUs)  [24] . This hypothesis is the basis for the Facial Action Coding System (FACS) AUs  [25]  and is a significant motivation for the field of affective computing. The second hypothesis contradicts that and states that expressions vary across cultures, situations, and people in the same situation. Because of this, emotions cannot be recognized from facial expressions alone  [5] . Instead, other factors such as context, physiology, age, and gender should be considered. While these two hypotheses contradict one another, recent work has shown validity in both hypotheses. More specifically, while it is difficult to determine emotion from AUs given a single facial image when temporal AU information and context are considered, the task becomes easier  [38] .\n\nAlong with this, it has also been shown that the fusion of physiological signals, such as heart rate and respiration, along with AUs can be used to accurately recognize pain in subjects  [40] . Although it is a challenging problem to automatically detect AUs using machine learning, these works show that AUs are still a promising approach to solve the challenging problem of emotion from expression.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we have demonstrated that TDA can be used to discern and better understand patterns that exist between emotions. Paired with machine learning approaches to affective computing, TDA provides a means to evaluate particular aspects of the data to discern what parts of the face may be causing the machine learning algorithm to recognize the data as a particular emotion or explain the shortcomings or misdiagnoses that the machine learning algorithm provides, e.g., if the algorithm recognizes a happiness emotion as anger, TDA may help to discern what led to this misdiagnosis. The next phase of this work is to begin evaluating these affective computing hypotheses, particularly those discussed in Sect. 8, using our TDA-based analysis and interface.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Additional Examples Of Relative Distance Topology",
      "text": "",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our affective computing visualization provides numerous options for comparing and contrasting the data. For example, the",
      "page": 1
    },
    {
      "caption": "Figure 5: top) and scatterplots (see Fig. 1 top right), and through",
      "page": 1
    },
    {
      "caption": "Figure 1: bottom right or Fig. 10).",
      "page": 1
    },
    {
      "caption": "Figure 2: ), which correspond to the key areas of the face that include the",
      "page": 2
    },
    {
      "caption": "Figure 2: An illustration of persistent homology on the 83 facial landmarks on the female subject F001. (a) The persistent homology is calculated",
      "page": 3
    },
    {
      "caption": "Figure 2: (a)), we consider a",
      "page": 3
    },
    {
      "caption": "Figure 2: (b)). From the points, we devise an importance measure,",
      "page": 3
    },
    {
      "caption": "Figure 2: (b), distance from the diagonal dotted line",
      "page": 3
    },
    {
      "caption": "Figure 3: (f) shows this connectivity. This",
      "page": 3
    },
    {
      "caption": "Figure 3: (b) through Fig. 3(e) show four",
      "page": 3
    },
    {
      "caption": "Figure 3: (e). The persistence diagrams are clearly",
      "page": 3
    },
    {
      "caption": "Figure 3: (c) appears sufﬁcient",
      "page": 3
    },
    {
      "caption": "Figure 4: (a) shows that as",
      "page": 3
    },
    {
      "caption": "Figure 4: (b) shows this extreme growth. To",
      "page": 3
    },
    {
      "caption": "Figure 3: (f)). We calculate a distance matrix representation of the landmark",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustration of supersampling and non-metric persistent homology shows the data and the persistence diagram of H1 features. (a) The",
      "page": 4
    },
    {
      "caption": "Figure 3: (f) shows the landmark edges and the persistence diagram of",
      "page": 4
    },
    {
      "caption": "Figure 3: (e) without the need for specifying",
      "page": 4
    },
    {
      "caption": "Figure 4: (a) shows that the compute",
      "page": 4
    },
    {
      "caption": "Figure 4: (b) shows the",
      "page": 4
    },
    {
      "caption": "Figure 4: Plots of (a) the compute time and (b) the number of topological",
      "page": 4
    },
    {
      "caption": "Figure 1: and Fig. 10) with the following design criteria:",
      "page": 5
    },
    {
      "caption": "Figure 1: top right)",
      "page": 5
    },
    {
      "caption": "Figure 5: top) [D2].",
      "page": 5
    },
    {
      "caption": "Figure 6: (a)) if the user wants a",
      "page": 5
    },
    {
      "caption": "Figure 1: lower right)",
      "page": 5
    },
    {
      "caption": "Figure 5: shows two examples of this relationship. In Fig. 5(a),",
      "page": 6
    },
    {
      "caption": "Figure 5: (b) shows an example comparing the mouth+nose to",
      "page": 6
    },
    {
      "caption": "Figure 7: and Fig. 8, revealed these relationships are",
      "page": 6
    },
    {
      "caption": "Figure 5: A comparison of relative distance on non-metric topology (top)",
      "page": 6
    },
    {
      "caption": "Figure 6: (a), Shepard ﬁtness: 0.79) and MDS (Fig. 1, Shepard",
      "page": 6
    },
    {
      "caption": "Figure 6: (b), Shepard ﬁtness:",
      "page": 6
    },
    {
      "caption": "Figure 6: (c), Shepard ﬁtness: 0.88).",
      "page": 6
    },
    {
      "caption": "Figure 6: (a) and Fig. 6(b), respectively. We make three important",
      "page": 6
    },
    {
      "caption": "Figure 1: and Fig. 6(c), respectively. With the female",
      "page": 6
    },
    {
      "caption": "Figure 6: Evaluation using (a-b) t-SNE and (c) MDS to determine how",
      "page": 6
    },
    {
      "caption": "Figure 7: Comparison of F001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2),",
      "page": 7
    },
    {
      "caption": "Figure 9: (a)-9(f)). We",
      "page": 7
    },
    {
      "caption": "Figure 9: (e)) emotions, while some minor overlap occurs for a",
      "page": 7
    },
    {
      "caption": "Figure 9: (g)-9(l)). We can see that the clustering behavior",
      "page": 7
    },
    {
      "caption": "Figure 8: Comparison of M001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2),",
      "page": 8
    },
    {
      "caption": "Figure 10: These examples focus on the opening and closing of the eyes and",
      "page": 8
    },
    {
      "caption": "Figure 9: Clustering of 10 subjects (F001-F010) on the top and all 101 subjects on the bottom. For 10 subjects, each subject is colored differently. For",
      "page": 9
    },
    {
      "caption": "Figure 10: Illustration of the explainability of our TDA-based approach using",
      "page": 9
    },
    {
      "caption": "Figure 11: F002 Anger using eyes, eyebrows, nose, and mouth",
      "page": 12
    },
    {
      "caption": "Figure 12: F002 Fear using eyes, eyebrows, nose, and mouth",
      "page": 12
    },
    {
      "caption": "Figure 13: F002 Disgust using eyes, eyebrows, nose, and mouth",
      "page": 12
    },
    {
      "caption": "Figure 14: F002 Happiness using eyes, eyebrows, nose, and mouth",
      "page": 12
    },
    {
      "caption": "Figure 15: F002 Sadness using eyes, eyebrows, nose, and mouth",
      "page": 13
    },
    {
      "caption": "Figure 16: M002 Anger using eyes, eyebrows, nose, and mouth",
      "page": 13
    },
    {
      "caption": "Figure 17: F002 Surprise using eyes, eyebrows, nose, and mouth",
      "page": 13
    },
    {
      "caption": "Figure 18: M002 Disgust using eyes, eyebrows, nose, and mouth",
      "page": 13
    },
    {
      "caption": "Figure 19: M002 Fear using eyes, eyebrows, nose, and mouth",
      "page": 14
    },
    {
      "caption": "Figure 20: M002 Sadness using eyes, eyebrows, nose, and mouth",
      "page": 14
    },
    {
      "caption": "Figure 21: M002 Happiness using eyes, eyebrows, nose, and mouth",
      "page": 14
    },
    {
      "caption": "Figure 22: M002 Surprise using eyes, eyebrows, nose, and mouth",
      "page": 14
    },
    {
      "caption": "Figure 23: t-SNE clustering of individual topological data for Anger emotion",
      "page": 15
    },
    {
      "caption": "Figure 24: t-SNE clustering of individual topological data for Disgust emo-",
      "page": 15
    },
    {
      "caption": "Figure 25: t-SNE clustering of individual topological data for Fear emotion",
      "page": 16
    },
    {
      "caption": "Figure 26: t-SNE clustering of individual topological data for Happiness",
      "page": 16
    },
    {
      "caption": "Figure 27: t-SNE clustering of individual topological data for Sadness",
      "page": 17
    },
    {
      "caption": "Figure 28: t-SNE clustering of individual topological data for Surprise",
      "page": 17
    },
    {
      "caption": "Figure 29: UMAP clustering of individual topological data for Anger emotion",
      "page": 18
    },
    {
      "caption": "Figure 30: UMAP clustering of individual topological data for Disgust",
      "page": 18
    },
    {
      "caption": "Figure 31: UMAP clustering of individual topological data for Fear emotion",
      "page": 19
    },
    {
      "caption": "Figure 32: UMAP clustering of individual topological data for Happiness",
      "page": 19
    },
    {
      "caption": "Figure 33: UMAP clustering of individual topological data for Sadness",
      "page": 20
    },
    {
      "caption": "Figure 34: UMAP clustering of individual topological data for Surprise",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Computation time for metric (M) and non-metric (NM) ap-",
      "data": [
        {
          "Female M\n(F001) NM": "",
          "|H0|\n(avg)": "43.7\n4.3",
          "|H1|\n(avg)": "12.8\n6.3",
          "Persistent\nHomology": "84.3 s\n188.5 s",
          "Topological Distance\nBottle.\nWasser.": "1833.8 s\n1851.7 s\n189.8 s\n133.7 s",
          "Total": "3769.8 s\n512.1 s"
        },
        {
          "Female M\n(F001) NM": "Male\nM\n(M001) NM",
          "|H0|\n(avg)": "43.7\n4.3",
          "|H1|\n(avg)": "13.4\n6.5",
          "Persistent\nHomology": "91.6 s\n188.1 s",
          "Topological Distance\nBottle.\nWasser.": "1877.6 s\n2039.2 s\n207.5 s\n136.4 s",
          "Total": "4008.4 s\n532.0 s"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Action Unit": "AU1",
          "Facial Muscles": "Frontalis, pars medialis",
          "Description": "Inner eyebrow raise"
        },
        {
          "Action Unit": "AU2",
          "Facial Muscles": "Frontalis, pars lateralis",
          "Description": "Outer eyebrow raise"
        },
        {
          "Action Unit": "AU4",
          "Facial Muscles": "Depressor Glabellae,\nDepressor Supercilli,\nCurrugator",
          "Description": "Eyebrow lower"
        },
        {
          "Action Unit": "AU5",
          "Facial Muscles": "Levator palpebrae\nsuperioris",
          "Description": "Upper eyelid raise"
        },
        {
          "Action Unit": "AU6",
          "Facial Muscles": "Orbicularis oculi,\npars orbitalis",
          "Description": "Cheek raise"
        },
        {
          "Action Unit": "AU7",
          "Facial Muscles": "Orbicularis oculi,\npars palpebralis",
          "Description": "Eyelid tighten"
        },
        {
          "Action Unit": "AU9",
          "Facial Muscles": "Levator labii superioris\nalaquae nasi",
          "Description": "Nose wrinkle"
        },
        {
          "Action Unit": "AU10",
          "Facial Muscles": "Levator Labii Superioris,\nCaput infraorbitalis",
          "Description": "Upper lip raise"
        },
        {
          "Action Unit": "AU12",
          "Facial Muscles": "Zygomatic Major",
          "Description": "Lip corner pull"
        },
        {
          "Action Unit": "AU14",
          "Facial Muscles": "Buccinator",
          "Description": "Dimple"
        },
        {
          "Action Unit": "AU15",
          "Facial Muscles": "Depressor anguli oris",
          "Description": "Lip corner depress"
        },
        {
          "Action Unit": "AU17",
          "Facial Muscles": "Mentalis",
          "Description": "Chin raise"
        },
        {
          "Action Unit": "AU20",
          "Facial Muscles": "Risorius",
          "Description": "Lip stretch"
        },
        {
          "Action Unit": "AU23",
          "Facial Muscles": "Orbicularis oris",
          "Description": "Lip tighten"
        },
        {
          "Action Unit": "AU25",
          "Facial Muscles": "Depressor Labii,\nRelaxation of Mentalis,\nOrbicularis Oris",
          "Description": "Lips part"
        },
        {
          "Action Unit": "AU26",
          "Facial Muscles": "Masetter,\nTemporal/Internal Pterygoid",
          "Description": "Jaw drop"
        },
        {
          "Action Unit": "AU45",
          "Facial Muscles": "Levator Palpebrae,\nOrbicularis Oculi,\nPars Palpebralis",
          "Description": "Blink"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wild facial expression recognition based on incremental active learning",
      "authors": [
        "M Ahmed",
        "K Woo",
        "K Hyeon",
        "M Bashar",
        "P Rhee"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "2",
      "title": "Analysis of gender inequality in face recognition accuracy",
      "authors": [
        "V Albiero",
        "K Ks",
        "K Vangara",
        "K Zhang",
        "M King",
        "K Bowyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision Workshops"
    },
    {
      "citation_id": "3",
      "title": "3D Deformation Signature for Dynamic Face Recognition",
      "authors": [
        "D Aouada",
        "K Cherenkova",
        "G Gusev",
        "B Ottersten"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "5",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "6",
      "title": "Ripser: Efficient Computation of Vietoris-Rips Persistence Barcodes",
      "authors": [
        "U Bauer"
      ],
      "year": "2019",
      "venue": "Ripser: Efficient Computation of Vietoris-Rips Persistence Barcodes",
      "arxiv": "arXiv:1908.02518"
    },
    {
      "citation_id": "7",
      "title": "Measuring Emotion: The Self-Assessment Manikin and the Semantic Differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "8",
      "title": "Landmark Localization on 3D/4D Range Data Using a Shape Index-Based Statistical Shape Model with Global and Local Constraints. Computer Vision and Image Understanding",
      "authors": [
        "S Canavan",
        "P Liu",
        "X Zhang",
        "L Yin"
      ],
      "year": "2015",
      "venue": "Landmark Localization on 3D/4D Range Data Using a Shape Index-Based Statistical Shape Model with Global and Local Constraints. Computer Vision and Image Understanding"
    },
    {
      "citation_id": "9",
      "title": "Emotion Scents -A Method of Representing User Emotions on GUI Widgets",
      "authors": [
        "D Cernea",
        "C Weber",
        "A Ebert",
        "A Kerren"
      ],
      "year": "2013",
      "venue": "Emotion Scents -A Method of Representing User Emotions on GUI Widgets"
    },
    {
      "citation_id": "10",
      "title": "Emotion-prints: Interactiondriven emotion visualization on multi-touch interfaces",
      "authors": [
        "D Cernea",
        "C Weber",
        "A Ebert",
        "A Kerren"
      ],
      "year": "2015",
      "venue": "Visualization and Data Analysis"
    },
    {
      "citation_id": "11",
      "title": "Group affective tone awareness and regulation through virtual agents",
      "authors": [
        "D Cernea",
        "C Weber",
        "A Kerren",
        "A Ebert"
      ],
      "year": "2014",
      "venue": "Proceeding of IVA 2014 Workshop on Affective Agents"
    },
    {
      "citation_id": "12",
      "title": "Mobileface: 3D Face Reconstruction with Efficient CNN Regression",
      "authors": [
        "N Chinaev",
        "A Chigorin",
        "I Laptev"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "13",
      "title": "Stability of persistence diagrams",
      "authors": [
        "D Cohen-Steiner",
        "H Edelsbrunner",
        "J Harer"
      ],
      "year": "2007",
      "venue": "Discrete & computational geometry"
    },
    {
      "citation_id": "14",
      "title": "Active Appearance Models",
      "authors": [
        "T Cootes",
        "G Edwards",
        "C Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Gender differences in emotional response: Inconsistency between experience and expressivity",
      "authors": [
        "Y Deng",
        "L Chang",
        "M Yang",
        "M Huo",
        "R Zhou"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "16",
      "title": "Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges",
      "authors": [
        "A Dhall",
        "G Sharma",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2020",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "17",
      "title": "Explainable artificial intelligence: A survey",
      "authors": [
        "F Došilović",
        "M Brčić",
        "N Hlupić"
      ],
      "year": "2018",
      "venue": "International Convention on Information and Communication Technology, Electronics and Microelectronics"
    },
    {
      "citation_id": "18",
      "title": "Persistent Homology -A Survey",
      "authors": [
        "H Edelsbrunner",
        "J Harer"
      ],
      "year": "2008",
      "venue": "Contemporary Mathematics"
    },
    {
      "citation_id": "19",
      "title": "Computational Topology: An Introduction",
      "authors": [
        "H Edelsbrunner",
        "J Harer"
      ],
      "year": "2010",
      "venue": "Computational Topology: An Introduction"
    },
    {
      "citation_id": "20",
      "title": "Topological persistence and simplification",
      "authors": [
        "H Edelsbrunner",
        "D Letscher",
        "A Zomorodian"
      ],
      "year": "2000",
      "venue": "Proceedings 41st Annual Symposium on Foundations of Computer Science"
    },
    {
      "citation_id": "21",
      "title": "Basic Emotions. Handbook of Cognition and Emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Basic Emotions. Handbook of Cognition and Emotion"
    },
    {
      "citation_id": "22",
      "title": "The Repertoire of Nonverbal Behavior: Categories",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Origins, Usage, and Coding. Semiotica"
    },
    {
      "citation_id": "23",
      "title": "Universals and Cultural Differences in the Judgments of Facial Expressions of Emotion",
      "authors": [
        "P Ekman",
        "W Friesen",
        "M O'sullivan",
        "A Chan",
        "I Diacoyanni-Tarlatzis",
        "K Heider",
        "R Krause",
        "W Lecompte",
        "T Pitcairn",
        "P Ricci-Bitti"
      ],
      "year": "1987",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "24",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "25",
      "title": "Crossing domains for au coding: Perspectives, approaches, and measures",
      "authors": [
        "I Ertugrul",
        "J Cohn",
        "L Jeni",
        "Z Zhang",
        "L Yin",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "26",
      "title": "Deformable Synthesis Model for Emotion Recognition",
      "authors": [
        "D Fabiano",
        "S Canavan"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "27",
      "title": "Impact of Multiple Modalities on Emotion Recognition: Investigation into 3d Facial Landmarks, Action Units, and Physiological Data",
      "authors": [
        "D Fabiano",
        "M Jaishanker",
        "S Canavan"
      ],
      "year": "2020",
      "venue": "Impact of Multiple Modalities on Emotion Recognition: Investigation into 3d Facial Landmarks, Action Units, and Physiological Data",
      "arxiv": "arXiv:2005.08341"
    },
    {
      "citation_id": "28",
      "title": "Facial Expression Recognition with Deeply-Supervised Attention Network",
      "authors": [
        "Y Fan",
        "V Li",
        "J Lam"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "3D and 4D Face Recognition: A Comprehensive Review",
      "authors": [
        "S Fernandes",
        "G Bala"
      ],
      "year": "2014",
      "venue": "Recent Patents on Engineering"
    },
    {
      "citation_id": "30",
      "title": "Physiological-Based Affect Event Detector for Entertainment Video Applications",
      "authors": [
        "J Fleureau",
        "P Guillotel",
        "Q Huynh-Thu"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "3D Face Reconstruction From Volumes of Videos Using a MapReduce Framework",
      "authors": [
        "W Gao",
        "X Zhao",
        "Z Gao",
        "J Zou",
        "P Dou",
        "I Kakadiaris"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "32",
      "title": "Multilinear Modelling of Faces and Expressions",
      "authors": [
        "S Grasshof",
        "H Ackermann",
        "S Brandt",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Fast and scalable complex network descriptor using pagerank and persistent homology",
      "authors": [
        "M Hajij",
        "E Munch",
        "P Rosen"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Intelligent Data Science Technologies and Applications (IDSTA)"
    },
    {
      "citation_id": "34",
      "title": "Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology",
      "authors": [
        "M Hajij",
        "B Wang",
        "C Scheidegger",
        "P Rosen"
      ],
      "year": "2018",
      "venue": "IEEE Pacific Visualization"
    },
    {
      "citation_id": "35",
      "title": "3D Facial Expression Recognition Using Kernel Methods on Riemannian Manifold",
      "authors": [
        "W Hariri",
        "H Tabia",
        "N Farah",
        "A Benouareth",
        "D Declercq"
      ],
      "year": "2017",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Facial Expression Recognition with Automatic Segmentation of Face Regions Using a Fuzzy-Based Classification Approach",
      "authors": [
        "A Hernandez-Matamoros",
        "A Bonarini",
        "E Escamilla-Hernandez",
        "M Nakano-Miyatake",
        "H Perez-Meana"
      ],
      "year": "2016",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "37",
      "title": "Recognizing context using facial expression dynamics from action unit patterns",
      "authors": [
        "S Hinduja",
        "S Aathreya",
        "S Canavan",
        "J Cohn",
        "L Yin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing (Under Review)"
    },
    {
      "citation_id": "38",
      "title": "Impact of action unit occurrence patterns on detection",
      "authors": [
        "S Hinduja",
        "S Canavan",
        "S Aathreya"
      ],
      "year": "2020",
      "venue": "Impact of action unit occurrence patterns on detection",
      "arxiv": "arXiv:2010.07982"
    },
    {
      "citation_id": "39",
      "title": "Multimodal fusion of physiological signals and facial action units for pain recognition",
      "authors": [
        "S Hinduja",
        "S Canavan",
        "G Kaur"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "40",
      "title": "Developing an understanding of emotion categories: Lessons from objects",
      "authors": [
        "K Hoemann",
        "R Wu",
        "V Lobue",
        "L Oakes",
        "F Xu",
        "L Barrett"
      ],
      "year": "2020",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "41",
      "title": "Subject Identification across Large Expression Variations Using 3D Facial Landmarks",
      "authors": [
        "S Jannat",
        "D Fabiano",
        "S Canavan",
        "T Neal"
      ],
      "year": "2020",
      "venue": "Subject Identification across Large Expression Variations Using 3D Facial Landmarks",
      "arxiv": "arXiv:2005.08339"
    },
    {
      "citation_id": "42",
      "title": "Facial Expression Recognition Using Local Composition Pattern",
      "authors": [
        "A Kalam",
        "M Haque",
        "M Jashem",
        "M Hasan",
        "M Ibrahim",
        "T Jabid"
      ],
      "year": "2019",
      "venue": "International Conference on Computer and Communications Management"
    },
    {
      "citation_id": "43",
      "title": "Geometry Helps to Compare Persistence Diagrams",
      "authors": [
        "M Kerber",
        "D Morozov",
        "A Nigmetov"
      ],
      "year": "2017",
      "venue": "Journal of Experimental Algorithmics"
    },
    {
      "citation_id": "44",
      "title": "CONTVERB: Continuous Virtual Emotion Recognition Using Replaceable Barriers for Intelligent Emotion-based IoT Services and Applications",
      "authors": [
        "H Kim",
        "J Ben-Othman",
        "L Mokdad",
        "K Lim"
      ],
      "year": "2020",
      "venue": "IEEE Network"
    },
    {
      "citation_id": "45",
      "title": "Surface Shape and Curvature Scales",
      "authors": [
        "J Koenderink",
        "A Doorn"
      ],
      "year": "1992",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "46",
      "title": "Glyph-Based Visualization of Affective States",
      "authors": [
        "N Kovacevic",
        "R Wampfler",
        "B Solenthaler",
        "M Gross",
        "T Günther"
      ],
      "year": "2020",
      "venue": "Glyph-Based Visualization of Affective States"
    },
    {
      "citation_id": "47",
      "title": "Multidimensional Scaling by Optimizing Goodness of Fit to a Non-Metric Hypothesis",
      "year": "1964",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "48",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "50",
      "title": "Automated facial expression recognition based on facs action units",
      "authors": [
        "J Lien",
        "T Kanade",
        "J Cohn",
        "C.-C Li"
      ],
      "year": "1998",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "51",
      "title": "Saanet: Siamese action-units attention network for improving dynamic facial expression recognition",
      "authors": [
        "D Liu",
        "X Ouyang",
        "S Xu",
        "P Zhou",
        "K He",
        "S Wen"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "52",
      "title": "3D Face Modeling from Diverse Raw Scan Data",
      "authors": [
        "F Liu",
        "L Tran",
        "X Liu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "53",
      "title": "Automatically detecting pain in video through facial action units",
      "authors": [
        "P Lucey",
        "J Cohn",
        "I Matthews",
        "S Lucey",
        "S Sridharan",
        "J Howlett",
        "K Prkachin"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "54",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Affectaura: an intelligent system for emotional memory",
      "authors": [
        "D Mcduff",
        "A Karlson",
        "A Kapoor",
        "A Roseway",
        "M Czerwinski"
      ],
      "year": "2012",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "56",
      "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "J Melville"
      ],
      "year": "2018",
      "venue": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "57",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "M Minaei",
        "A Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "58",
      "title": "Disgust and anger relate to different aggressive responses to moral violations",
      "authors": [
        "C Molho",
        "J Tybur",
        "E Güler",
        "D Balliet",
        "W Hofmann"
      ],
      "year": "2017",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "59",
      "title": "Promoting reproducible research for characterizing nonmedical use of medications through data annotation: Description of a twitter corpus and guidelines",
      "authors": [
        "K O'connor",
        "A Sarker",
        "J Perrone",
        "G Hernandez"
      ],
      "year": "2020",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "60",
      "title": "Towards reverse-engineering blackbox neural networks",
      "authors": [
        "S Oh",
        "B Schiele",
        "M Fritz"
      ],
      "year": "2019",
      "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning"
    },
    {
      "citation_id": "61",
      "title": "Automatic analysis of facial expressions based on deep covariance trajectories",
      "authors": [
        "N Otberdout",
        "A Kacem",
        "M Daoudi",
        "L Ballihi",
        "S Berretti"
      ],
      "year": "2019",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "62",
      "title": "Emotion Recognition from 3D Videos Using Optical Flow Method",
      "authors": [
        "G Patil",
        "P Suja"
      ],
      "year": "2017",
      "venue": "IEEE International Conference On Smart Technologies For Smart Nation (SmartTechCon)"
    },
    {
      "citation_id": "63",
      "title": "Robust Real-Time Performance-Driven 3D Face Tracking",
      "authors": [
        "H Pham",
        "V Pavlovic",
        "J Cai",
        "T Cham"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "64",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Heartbees: Visualizing crowd affects",
      "authors": [
        "C Qin",
        "M Constantinides",
        "L Aiello",
        "D Quercia"
      ],
      "year": "2020",
      "venue": "Heartbees: Visualizing crowd affects",
      "arxiv": "arXiv:2010.07209"
    },
    {
      "citation_id": "66",
      "title": "Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks",
      "authors": [
        "B Rieck",
        "U Fugacci",
        "J Lukasczyk",
        "H Leitte"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "67",
      "title": "Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures",
      "authors": [
        "B Rieck",
        "H Mara",
        "H Leitte"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "68",
      "title": "A novel machine vision-based 3d facial action unit identification for fatigue detection",
      "authors": [
        "G Sikander",
        "S Anwar"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "69",
      "title": "Exploiting sparsity and co-occurrence structure for action unit recognition",
      "authors": [
        "Y Song",
        "D Mcduff",
        "D Vasisht",
        "A Kapoor"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "70",
      "title": "Persistent Homology Guided Force-Directed Graph Layouts",
      "authors": [
        "A Suh",
        "M Hajij",
        "B Wang",
        "C Scheidegger",
        "P Rosen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "71",
      "title": "3D Spatio-Temporal Face Recognition Using Dynamic Range Model Sequences",
      "authors": [
        "Y Sun",
        "L Yin"
      ],
      "year": "2008",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "72",
      "title": "The Topology Toolkit",
      "authors": [
        "J Tierny",
        "G Favelier",
        "J Levine",
        "C Gueunet",
        "M Michaux"
      ],
      "year": "2017",
      "venue": "The Topology Toolkit"
    },
    {
      "citation_id": "73",
      "title": "3D Facial Action Units and Expression Recognition Using a Crisp Logic",
      "authors": [
        "S Tornincasa",
        "E Vezzetti",
        "S Moos",
        "M Violante",
        "F Marcolin",
        "N Dagnes",
        "L Ulrich",
        "G Tregnaghi"
      ],
      "year": "2019",
      "venue": "Computer Aided Design and Applications"
    },
    {
      "citation_id": "74",
      "title": "Quantified facial temporal-expressiveness dynamics for affect analysis",
      "authors": [
        "M Uddin",
        "S Canavan"
      ],
      "year": "2020",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "75",
      "title": "Visualizing Data Using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "76",
      "title": "The Nature of Statistical Learning Theory",
      "authors": [
        "V Vapnik"
      ],
      "year": "2013",
      "venue": "Springer science & business media"
    },
    {
      "citation_id": "77",
      "title": "Branching and Circular Features in High Dimensional Data",
      "authors": [
        "B Wang",
        "B Summa",
        "V Pascucci",
        "M Vejdemo-Johansson"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "78",
      "title": "What is... persistent homology? Notices of the AMS",
      "authors": [
        "S Weinberger"
      ],
      "year": "2011",
      "venue": "What is... persistent homology? Notices of the AMS"
    },
    {
      "citation_id": "79",
      "title": "Preparing medical imaging data for machine learning",
      "authors": [
        "M Willemink",
        "W Koszek",
        "C Hardell",
        "J Wu",
        "D Fleischmann",
        "H Harvey",
        "L Folio",
        "R Summers",
        "D Rubin",
        "M Lungren"
      ],
      "year": "2020",
      "venue": "Radiology"
    },
    {
      "citation_id": "80",
      "title": "Exploring multidimensional measurements for pain evaluation using facial action units",
      "authors": [
        "X Xu",
        "V De Sa"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "81",
      "title": "Automatic 4D Facial Expression Recognition Using DCT Features",
      "authors": [
        "M Xue",
        "A Mian",
        "W Liu",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "82",
      "title": "Facial expression recognition by deexpression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "83",
      "title": "A High-Resolution 3D Dynamic Facial Expression Database",
      "authors": [
        "L Yin",
        "X Chen",
        "Y Sun",
        "T Worm",
        "M Reale"
      ],
      "year": "2008",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "84",
      "title": "An Approach for Automated Multimodal Analysis of Infants' Pain",
      "authors": [
        "G Zamzmi",
        "C.-Y Pai",
        "D Goldgof",
        "R Kasturi",
        "T Ashmeade",
        "Y Sun"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "85",
      "title": "Emotioncues: Emotion-oriented visual summarization of classroom videos",
      "authors": [
        "H Zeng",
        "X Shu",
        "Y Wang",
        "Y Wang",
        "L Zhang",
        "T.-C Pong",
        "H Qu"
      ],
      "year": "2020",
      "venue": "Emotioncues: Emotion-oriented visual summarization of classroom videos"
    },
    {
      "citation_id": "86",
      "title": "Facial expression recognition via learning deep sparse autoencoders",
      "authors": [
        "N Zeng",
        "H Zhang",
        "B Song",
        "W Liu",
        "Y Li",
        "A Dobaie"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "87",
      "title": "spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "88",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "89",
      "title": "Region of interest based graph convolution: A heatmap regression approach for action unit detection",
      "authors": [
        "Z Zhang",
        "T Wang",
        "L Yin"
      ],
      "year": "2020",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "90",
      "title": "Muscular Movement Model-Based Automatic 3D/4D Facial Expression Recognition",
      "authors": [
        "Q Zhen",
        "D Huang",
        "Y Wang",
        "L Chen"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    }
  ]
}