{
  "paper_id": "2502.04976v1",
  "title": "Towards Multimodal Empathetic Response Generation: A Rich Text-Speech-Vision Avatar-Based Benchmark",
  "published": "2025-02-07T14:50:10Z",
  "authors": [
    "Han Zhang",
    "Zixiang Meng",
    "Meng Luo",
    "Hong Han",
    "Lizi Liao",
    "Erik Cambria",
    "Hao Fei"
  ],
  "keywords": [
    "Empathetic Response Generation",
    "Multimodal Large Language Model",
    "Avatar Generation",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Empathetic Response Generation (ERG) is one of the key tasks of the affective computing area, which aims to produce emotionally nuanced and compassionate responses to user's queries. However, existing ERG research is predominantly confined to the singleton text modality, limiting its effectiveness since human emotions are inherently conveyed through multiple modalities. To combat this, we introduce an avatar-based Multimodal ERG (MERG) task, entailing rich text, speech, and facial vision information. We first present a large-scale high-quality benchmark dataset, AvaMERG, which extends traditional text ERG by incorporating authentic human speech audio and dynamic talking-face avatar videos, encompassing a diverse range of avatar profiles and broadly covering various topics of real-world scenarios. Further, we deliberately tailor a system, named Empatheia, for MERG. Built upon a Multimodal Large Language Model (MLLM) with multimodal encoder, speech and avatar generators, Empatheia performs end-to-end MERG, with Chainof-Empathetic reasoning mechanism integrated for enhanced empathy understanding and reasoning. Finally, we devise a list of empathetic-enhanced tuning strategies, strengthening the capabilities of emotional accuracy and content, avatar-profile consistency across modalities. Experimental results on AvaMERG data demonstrate that Empatheia consistently shows superior performance",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, the advent of Large Language Models (LLMs)  [8-10, 20, 45, 52]  has endowed machines with unprecedented levels of intelligence, bringing us closer to the realization of Artificial General Intelligence (AGI). However, the true essence of AGI extends beyond merely achieving human-level intelligent abilities; it must also encompass emotional understanding and empathetic capabilities comparable to those of humans. For instance, during human-machine interactions, it is crucial for machines to comprehend human emotions and intentions  [12, 17, 28, 29, 53] . This necessity has driven the development of Empathetic Response Generation (ERG)  [36] , a task aimed at enabling machines to produce emotionally nuanced and compassionate responses to user queries, thereby facilitating emotion-aware conversations. Over the past decade, ERG has garnered significant research attention  [31, 38, 49] . Due to its ability to support emotional interactions with humans, ERG has been applied in various practical scenarios, such as psychological therapy and elderly companionship dialogue systems.\n\nHowever, current ERG research might encounter significant challenges due to its confinement to a singleton textual modality as task definition. It is worthwhile to reflect on how humans naturally express emotions; in many cases, the subtleties of emotions are more effectively and comprehensively conveyed through nontextual modalities. Specifically, in dynamic visual contexts, subtle facial expressions and body movements can communicate richer emotions and intentions. Simultaneously, in the auditory domain, variations in speech intonation and pitch can also convey emotional states that text alone cannot express. Figure  1  demonstrates a multimodal empathetic dialogue process. Existing text-based ERG tasks are restricted to providing users with mere textual responses, which lack enough warmth and emotional resonance inherent in human interactions, thereby falling short of achieving adequate empathetic effects. Furthermore, from the user's perspective, there is a desire to express emotions directly through speech or talking-facial video rather than being confined to text-based queries. In practical applications, numerous ERG scenarios require the ability to accept multimodal signal inputs and generate empathetic responses in multimodalities, such as in psychological therapy, companion robots, and electronic personal assistants. Unfortunately, there has yet to be any research on avatar-based Multimodal Empathetic Response Generation (MERG) within the community.\n\nTo bridge this gap, in this paper we present an Avatar-based Multimodal Empathetic Response Generation benchmark dataset (namely, AvaMERG). Building upon existing text-based ERG benchmark  [36] , we further augment the dataset to include multimodal signals and annotations. Specifically, for each utterance in the dialogue, we provide 1) authentic human-reading speech and 2) dynamic talking-face avatar videos (2D facial modeling) that both correspond to the intended emotion. AvaMERG features a wide variety of avatar profiles and covers broad common topics of real-world scenarios, including multiple age groups, genders, vocal tones, intonations, and appearances, thereby effectively simulating a diverse range of multimodal empathetic dialogue scenarios in realistic environments. We maintain the high quality of annotations through meticulous manual verification, guaranteeing the emotional accuracy and consistency of both the avatars' speech and video. Finally, we compile 33,048 annotated dialogues with 152,021 multimodal utterances, establishing a foundation for MERG research.\n\nA direct approach to generating multimodal empathetic responses can be first producing the textual part of the response using existing text-based ERG models (e.g., high-performing LLMs), and then through a pipeline paradigm to invoke external well-trained speech generator and talking-head generator (e.g., diffusion-based models) to generate the corresponding multimodal content. However, there can be several non-trivial issues and inherent challenges. First, ensuring the emotional accuracy across the text, audio, and video is the most fundamental capability. Second, it is essential to maintain synchronization and consistency among the three modalities in terms of content, emotion, and style. Pipeline models often suffer from inadequate interaction between different modules, making it difficult to guarantee consistency. For example, the generated speech may convey the emotion of a happy girl, while the corresponding avatar depicts a crying boy. Third, the discrete approach (where LLMs invoke external audio and video generators) can largely lead to the quality decrease of the generated content due to error propagation.\n\nTo achieve high-quality MERG, we thus propose a novel Multimodal LLM, termed Empatheia. Architecturally, we employ a multimodal encoder to feed all input signals into the central LLM for comprehension and reasoning. We then utilize StyleTTS2  [22]  as the speech generation module and DreamTalk  [30]  as the Talking Face Generation module. By using continuous embeddings as the medium for message passing, we connect the LLM to the frontend encoders and backend cross-modal generation modules, resulting in a full end-to-end system. Next, we optimize Empatheia by implementing a series of tuning strategies. We first devise a Chain-of-Empathetic Inference to assist the LLM to reason step-bystep, from understanding the emotion to identifying the underlying rationale and intent, and ultimately determining how to respond to the user's input. Then, we introduce Content Consistency Learning, which encourages the LLM to guide the two backend modules to produce speech and talking-face avatar videos that align with the empathetic textual content. Further, we propose a Style-aware Alignment and Consistency Learning mechanism to accurately identify the style signals transmitted by the central LLM, and ensure consistency in the style of both speech and video avatars, including emotion and profile. Finally, we perform overall MERG tuning to achieve overall high-quality multimodal empathetic responses.\n\nWe conduct experiments on the AvaMERG dataset, where the results demonstrate that our Empatheia system generates both textual and multimodal empathetic responses of higher quality compared to baseline models. In-depth analyses further reveal the underlying rationales for our model's advancements. Overall, this work pioneers the research of MERG, contributing a benchmark dataset and a strong-performing end-to-end MERG model, laying a solid foundation for future exploration in multimodal empathetic response generation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "ERG  [33, 34]  is one of the crucial tasks within the field of affective computing, which aims at enabling dialogue models to produce responses imbued with empathy during human-machine conversations. Due to its significant practical applications, ERG has attracted substantial and sustained prior research attention  [11, 24, 54] . Existing studies have developed various methods to enhance the performance of ERG systems  [2, 13, 38, 50] .\n\nYet current ERG approaches can be limited to a single text modality, which significantly restricts their effectiveness. In real-world dialogue scenarios, multiple modalities are often involved. As previously emphasized, multimodal information is crucial for generating more empathetic responses. Therefore, this paper tries to pioneer the research of Multimodal Empathetic Response Generation (MERG) by presenting a novel benchmark. It is also noteworthy that several recent related works have also touched upon multimodal ERG  [48, 51] .\n\nHowever, we emphasize that these studies do not fully address or cover all the modalities most relevant to empathy. Intuitively, both audio (capturing variations in a person's tone) and visual (capturing facial expressions) modalities can be important, and need to be simultaneously addressed. Moreover, it is insufficient to rely solely on emoticon-type visual features. Effective ERG that closely aligns with real-world application scenarios should present authentic facial visual signals.\n\nUnlike existing text-based ERG models and methods, achieving multimodal emotional understanding and generating multimodal signals requires the utilization of multimodal-related technologies. First, our approach is related to research on Multimodal Large Language Models (MLLMs), with our system being based on a backbone MLLM. Various MLLMs, such as LLaVA  [25] , MiniGPT-4  [55] , have been investigated and widely validated for their strong semantic understanding capabilities. However, most MLLMs are limited to multimodal information comprehension yet do not support the flexible generation of diverse modal content beyond text  [1, 19, 39] , such as audio and visual outputs. Although there are a few MLLMs that support the generation of various modal signals, such as NExT-GPT  [46]  and Unified-IO 2  [27] , these models, unfortunately, are only capable of understanding and generating signals in general scenarios. They lack sufficient capabilities in emotion detection and emotional content generation. In other words, these MLLMs are unable to generate emotionally expressive speech or talkingface avatars. Therefore, we consider developing a novel MLLM for MERG, which is able to accurately generate emotionally charged speech and talking-face avatar videos. Additionally, we design a series of emotion-enhancement training strategies to ensure that our MLLM possesses highly-performing MERG capabilities.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Avamerg Benchmark 3.1 Task Definition Of Merg",
      "text": "Given a multimodal dialogue D=(ùëÑ ùëñ |ùê∑ <ùëñ ), where ùëÑ ùëñ denotes the current ùëñ-th round multimodal user query input, and ùê∑ <ùëñ represents the dialogue history, MERG task is to produce a contextually appropriate and empathetic multimodal response ùëÖ ùëñ for ùëÑ ùëñ , with each utterance (i.e., ùëÑ ùëñ and ùëÖ ùëñ ) consisting of three content-synchronized modalities: text ùë° ùëñ , speech audio ùë† ùëñ , and talking-face video ùë£ ùëñ , i.e.,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Child",
      "text": "Young Middle-aged Elderly 0 10,000   ). This results in ùê∑ ùëñ ={(ùëÑ 1 , ùëÖ 1 ), . . . , (ùëÑ ùëñ , ùëÖ ùëñ )}, a total of ùëñ round of a multimodal dialogue, includes the user query ùëÑ ùëñ and model response ùëÖ ùëñ . The task requires maintaining coherence and emotional congruence across these modalities to ensure that the generated response ùëÖ ùëñ well aligns with the emotional cues in user input and also context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset Construction",
      "text": "We construct our Ava-MERG dataset by augmenting the existing pure-text ERG dataset, Empathetic Dialogue (ED)  [36] , where the textual empathetic response ùë° ùëñ ‚àà ùëÖ ùëñ with the query's corresponding emotion categories. First, we consider enriching the data with the identity information for both participants in the dialogue, including ages, genders, and also tone, such that MERG models can learn the correct avatar profile for both audio and video.\n\nAs the OpenAI GPT-4 1  has been validated for its remarkable performance in context understanding and thus extensively employed for data generation  [28, 46] , here we also adopt GPT-4 for our annotation. We define four age periods (child, young, middle-aged, elderly), binary genders (male, female), and three vocal tones (emphatic, mild, gentle). We ask GPT-4 to determine the above labels for each utterance in ED. Since the data in the raw ED is ill-balanced, e.g., most of the dialogues occurred between young or middle-aged participants, we further employ GPT-4 to produce more dialogue of ERG with above meta-information. Also, GPT-4 will detect the dialogue topics. Human annotators with 3-person cross-checking are recruited here to carefully check if the dialogue content, the meta-profile, and the topics are correct and of high quality. This led to the textual part of our AvaMERG data.\n\nNext, we create the multimodal part of the information. First, we recruit a big number of English-speaking volunteers of the above different ages, genders, and vocal characteristics, and also different races (i.e., Asian, Caucasian, African, Latino, Indian). Then, we assign and group different pairs of two participants according to the profile determined in the AvaMERG dialogue. Next, we let these annotators carefully read the utterance text, with the correct emotional performance, including the tone, pitch, timbre and microfacial expressions, where we then record their vocal speeches and talking-head videos. After the recordings, we recruit another group of well-trained annotators to evaluate each dialogue for content accuracy and emotional accuracy with same 3-person cross-checking. We ask each annotator to check: 1) whether the speech and video content match the content in textual utterance; 2) whether the speech and video style (including age, gender, tone, emotion) are consistent. Only the instance will be accepted where all three annotators vote for approval. This results in the final AvaMERG dataset.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Highlight",
      "text": "The data statistics are detailed in Table  1  and Figure  2 . Here we summarize the data characteristics that are key to MERG. Due to the space limitation, we show the complete data description and statistics in Appendix ¬ßC.\n\nLarge Scale and High Quality. AvaMERG comprises a total of 33,048 dialogues with 152,021 utterances, which is large-scale enough to uncover the immense potential of the task. Also the construction undergoes a rigorous manual checking involving both textual and multimodal content verification, ensuring its high quality.\n\nMultimodal Dialogue. Dialogues in AvaMERG cover three modalities: text, speech, and avatar video, which overcome the limitation of single-modality in existing textual ERG benchmarks.\n\nAvatar Profile Diversity. The avatars encompass 4 distinct age groups, with each represented by male and female in 3 different vocal tones. Also avatars come from different races. This rich diversity of avatar profiles ensures the robustness of the MERG.\n\nEmotion Diversity. AvaMERG includes 7 commonly occurred emotions: sad, disgusted, surprised, contempt, happy, fear, and angry.\n\nBroad Topic Coverage. AvaMERG covers 10 primary common topics of real empathetic dialogue, along with hundreds of specific subtopics, fully covering the wide range of potential real-world applications for ERG.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Empatheia: Merg System",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Encoder",
      "text": "To perceive the multimodal dialogue inputs, we employ the Hu-BERT  [14]  and CLIP ViT-L/14@336px  [35]  as the speech encoder and avatar video encoder. Essentially, the latent representations  of synchronous text, speech, and talking face video should convey consistent semantics, meaning that ideally, their embeddings are aligned. We thus align the speech and avatar encoders' representation into the LLM's language semantic space via projections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Llm-Based Core Reasoner",
      "text": "LLM Backbone. The LLM serves as the \"brain\" of our system, responsible for understanding multimodal signals, reasoning about appropriate empathetic responses, and sending signals for multimodal generation. Given that Vicuna  [3]  is widely adopted as a baseline for MLLMs  [6, 23]  and demonstrates superior performance, we select it as our backbone LLM. After encoding the input multimodal dialogue D, LLM is expected to output the representations of 1) text tokens ùëü ùë° ùëñ , 2) speech signal tokens ùëü ùë† ùëñ , and 3) video signal tokens ùëü ùë£ ùëñ . Here ùëü ùë† ùëñ and ùëü ùë£ ùëñ entail rich emotion and style features, which all will be used for controlling the follow-up modules.\n\nChain-of-Empathy Reasoning. Empathy is an advanced human capability that is challenging to interpret, and individuals often engage in several steps of contemplation before responding as listeners. Inspired by Chain-of-Thought  [7, 47] , we design a Chainof-Empathy (CoE) reasoning mechanism. Specifically, we guide the LLM to think through the following progressive steps to gradually derive the final empathetic responses more accurately and more interpretably.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "‚Ä¢ Coe Instruction:",
      "text": "You are an empathetic conversational agent. Your goal is to understand the user's emotions and intentions, and respond or comfort them with appropriate language that helps them feel understood and cared for. Avoid rushing into your response; instead, carefully consider each step before replying by following these steps, one by one: ‚ñ∂ Step-1. Event scenario. Reflect on the event scenarios that arise from the ongoing dialogue. These steps simulate the thought process that humans typically engage in. In the following ¬ß5.1 we expand the training of the CoE reasoning on our system.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Generation",
      "text": "Multimodal Generator Backbones. Following the signal features (ùëü ùë° ùëñ , ùëü ùë† ùëñ , ùëü ùë£ ùëñ ) from LLM, the backbone speech generator and talkinghead generator will produce the non-textual contents, respectively. To ensure high-quality multimodal generation, we employ the current state-of-the-art StyleTTS2  [22]  and DreamTalk  [30] , respectively. Note that these generators are well-trained before integrating into our system. However, directly generating speeches and dynamic avatars would largely lead to the issues of inconsistency of both content and style. That is, two aspects of consistency are required: 1) Consistency of content, both the speech should be synchronized with the talking-head video, both of which should be further aligned with the textual response; 2) Stylistic Coherence, the style within text/speech/vision, including both the emotion and profile (age, gender, tone, appearance), should be kept consistent. For natural and accurate MERG, maintaining synchronized content and style across modalities is crucial.\n\nFor these purposes, we further design two modules before the two generators: content synchronizer and style disentangler.\n\nContent Synchronizer. The content synchronizer (CS) aims to ensure that the speech and vision generators receive the correct response content information. As shown in Figure  4 (a), the module is essentially a Transformer-based  [42]  variational auto-encoder (VAE)  [16] . mainly consists of two transformer blocks, which CS encodes the ùëü ùë° into latent representation ùëß ùëê , from which the decoder reconstructs the content of speech ùê∂ ùë† and vision ùê∂ ùë£ .\n\nwhere ùëû ùë† ùëê and ùëû ùë£ ùëê represent learnable content query features for two modalities, which are fed into the decoder along with the output from the encoder. ùê∂ ùë† guides the speech generator to produce speech that correctly delivers the response text, while ùê∂ ùë£ guides the talkinghead generator to generate accurate mouth movements reflecting the response text.\n\nStyle Disentangler. Style features (including emotions and profiles) can be subtly different in speech module and vision module. The style disentangler (SD) module thus aims to disentangle the style features from the LLM-output ùëü ùë† ùëñ and ùëü ùë£ ùëñ , for two modules, respectively. As shown in Figure  4 (b), similar to CS module, SD also uses VAE blocks to disentangle the emotion and profile representations for speech and video:\n\nùëÉ ùë†/ùë£ = Dec SD (FFN(ùëß\n\nwhere ùê∏ ùë†/ùë£ are the disentangled emotion features. ùëÉ ùë†/ùë£ are the corresponding profile features. ùëû ùë†/ùë£ ùëí and ùëû ùë†/ùë£ ùëù denote the learnable query features. Then, we fuse the ùê∏ ùë†/ùë£ and ùëÉ ùë†/ùë£ by a speech/video style layer, and obtain the final speech/video style feature:\n\nwhich will be passed to two generators separately. To further regulate the successful extraction of emotional and profile-aware features, we also fuse the emotion feature ùê∏ ùë† and ùê∏ ùë£ into ùê∏, and the profile feature ùëÉ ùë† and ùëÉ ùë£ into ùëÉ. Then we use an emotion classifier and a set of profile classifiers to predict the labels of emotion, avatar's age, gender, and tone.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Empathetic-Enhanced Training Strategy",
      "text": "With the above Empatheia model architecture, we now empower it with effective MERG capability via a series of training strategies.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Chain-Of-Empathy Training",
      "text": "For the first stage, to teach Empatheia to learn how to perform CoE, we perform supervised fine-tuning. For this training, we annotate a set of CoE labels based on a subset of the Ava-MERG training data. Then, as shown in Figure  5 (a), this training only updates the core LLM part for text generation, with Lora  [15]  technique.\n\nwhere ùë• ùëñ denotes the output token of the LLM at ùëñ-th time step. Upon completion of training, the LLM is capable of not only generating empathetic responses but also providing a comprehensive CoE reasoning process.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Content Consistency Learning",
      "text": "The aim of the second training stage is to encourage the content signals output by CS module to guide the multimodal generator in producing content-consistent speech and video. This requires aligning the content representations of both sides. Therefore, as\n\nGold Emotion Gold Profile\n\nSince the input text for the speech generator and the input audio for the video generator are well paired, the CS module naturally produces consistent multimodal content signal features after training. In this stage, we keep the LLM frozen to prevent it from forgetting the empathetic response capability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Style Alignment And Consistency Learning",
      "text": "Style Alignment Learning. For the third stage, on the one hand, we aim to align the style features, ensuring that the multimodal generators accurately interpret the style signals provided by the SD module. As illustrated in Figure  5 (c), we minimize the Euclidean distance between ùëÜ ùë† (Equation  7 ) and the audio style features ≈úùë† encoded by the style encoder in the speech generator, as well as between ùëÜ ùë£ and the video style features ≈úùë£ :\n\nStyle Consistency Learning. On the other hand, the target style features are not only exclusively composed of the predefined emotion and profile features, but also include additional modalityspecific representations. For example, video style features may depict facial variations under specific emotional states. To further ensure style consistency across modalities, we constrain the SD to disentangle pure emotion and profile representations. We here introduce two classification losses for emotion and profile prediction:\n\nwhere ùëÄ ùëí represents the number of emotion categories, and ùëÄ ùëù is the set of categories for gender, age, and tone. In this stage, we also fix the LLM to prevent loss of previously acquired capabilities. In summary, the total loss for the third stage is:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overall Merg Tuning",
      "text": "The previous training steps effectively decompose the MERG task into sub-processes of separate capabilities. To enhance the overall performance of MERG, comprehensive end-to-end fine-tuning is necessary. In this stage, we integrate all previous training processes, and jointly fine-tune the LLM, CS, and SD modules. The overall loss can be denoted as:\n\nBy jointly optimizing the components, we aim to improve the consistency and accuracy of the generated speech and video outputs, while maintaining the empathetic dialogue capabilities learned in earlier stages. Furthermore, this unified fine-tuning stage allows the model to leverage cross-modal interactions more effectively, resulting in a more robust and coherent multimodal generation system tailored to the MERG task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment 6.1 Settings",
      "text": "Baseline. In our preliminary experiment, to identify the most suitable backbone LLM, we compare Flan-T5 XXL  [4] , ChatGLM3-6B  [40] , and Vicuna-7B  [3] . Besides MERG, we also compare the text ERG performance with existing models, including KEMP  [21] , CEM  [38]  and CASE  [54] , where we evaluate our Empatheia using only text queries for generating textual responses only. Since no prior work addresses the MERG task, for the speech and video generation, we develop a pipeline-based baseline, where the LLM only outputs the invocation commands for the two backend multimodal generators, without feature embedding passing and end-to-end joint training. It first generates response text from the LLM, then passes the text into StyleTTS2  [22]  to synthesize speech, and then processes the speech using DreamTalk  [30]  to generate the corresponding talking-head video. Evaluation Metrics. For the text ERG task, we employ three evaluation metrics: Emotion Accuracy (Acc), and Distinct metrics (Dist-1 and Dist-2)  [18] . For speech generation, we use the 5-scale Mean Opinion Score (MOS)  [43]  and Similarity MOS (SMOS)  [26] . For talking head generation, we adopt the Cumulative Probability of Blur Detection (CPBD)  [32] , Structural Similarity Index Measure (SSIM)  [44]  and SyncNet confidence score (Sync ùëê ùëì )  [5] . We also consider human evaluations. For textual ERG, we employ 4 human evaluation metrics: Empathy (Emp.), Coherence (Coh.), Informativity (Inf.), and Fluency (Flu.). For MERG, we newly define 6 metrics: Speech Content Accuracy (SCA), Video Content Accuracy (VCA), Speech Style Accuracy (SSA), Video Style Accuracy (VSA), Multimodal Content Consistency (MCC), and Multimodal Style Consistency (MSC). Implementation Details. We fine-tune our model using LoRA  [15]  and DeepSpeed  [37]  techniques on a single 80GB A100 GPU. Each Transformer block comprises four encoder-decoder modules in CS and SD modules. To minimize training time and costs, we utilize BF16 precision and gradient accumulation. Also, we pre-extract content and style features for each speech and audio sample in the training set. Due to the space limitation, we leave more experimental settings in Appendix ¬ßE.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Automatic Evaluation Results",
      "text": "First, we compare the performance of different methods on textual ERG in Table  2 , where we find that the Empatheia model performs the best. When we remove the speech and talking-face video information, a decline in performance is observed (though it still outperforms the baseline), indicating that multimodal information aids in better empathetic understanding. Also, removing the CoE strategy has the greatest impact on the response text, reflecting the importance of CoE. Next, we examine the performance of MERG in multimodal content generation, where we present the results of speech generation and avatar generation in Table  3  and Table ??, respectively. It is evident that our Empatheia model consistently outperforms the pipeline system across all metrics for both speech and avatar video generation. We also analyze the model's ablation results. Firstly, when using different LLMs as backbones, we observe that Vicuna achieves better performance compared to ChatGLM3 and Flan-T5, so our subsequent evaluations are based on Vicuna. Then, when we remove the CS and SD modules individually, we observe a degradation in results, demonstrating the importance of both modules. Finally, we evaluate the impact of different learning strategies, where each causes varying degrees of performance decline, thus validating their effectiveness.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Human Evaluation Results",
      "text": "Since emotions represent a form of high-level human information, the above automatic evaluation metrics might be insufficient for assessing empathy-related capacities. Thus, we further present the results of human evaluations on textual ERG and MERG in Table  4  and Table 5 . It is evident that Empatheia system significantly outperforms the baselines. Also, the model ablation results exhibit trends similar to those observed in the automatic evaluations. As seen, multimodal information contributes to enhanced empathetic    understanding and generation. The effectiveness of the CoE mechanism is further confirmed. Moreover, the proposed CS and SD modules, along with various sophisticated training strategies, influence the overall system performance consistently, again revealing their efficacy and importance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analyses And Discussions",
      "text": "We now conduct more in-depth analyses of several key aspects of Empatheia, offering further insights for better understanding.  varying emotions, genders, and age groups. As shown in Figure  6 , Empatheia is most sensitive to sad emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Q1. How Does",
      "text": "In terms of gender, we observe that the model performs slightly better for males compared to females, which might be attributed to the higher number of male avatars compared to female avatars in the training set. Regarding age groups, Empatheia's accuracy in recognizing children's emotions is relatively low, potentially because children's facial expressions are more dynamic, or their emotional expression patterns differ significantly from adults.\n\nQ2. Has SD module successfully disentangled emotion and profile features? While previous ablation experiments have validated the efficacy of the SD module, it remains uncertain whether it has fully achieved the intended goal of separating emotion and profile features. To explore this, we present the t-SNE  [41]  visualization on the fused multimodal emotion representations in Figure  7 , where we select 500 samples with varying emotions from AvaMERG.\n\nAs shown, the results indicate that SD module significantly increases the separation between different emotion categories while clustering the representations of the same emotion. Similarly, the patterns on profile features confirm that SD has successfully disentangled the non-emotion avatar features.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Qualitative Case Study",
      "text": "Finally, we present two case studies to further demonstrate the specific multimodal empathetic generation capabilities of Empatheia, as illustrated in Figure  8 , where we compare the outputs of the Pipeline baseline (without CoE). In the first instance, the user's text does not exhibit an explicit emotional inclination. However, the accompanying sad speech and facial expressions suggest that the user may feel sentimental about \"meeting a friend from middle school\". The Pipeline model, lacking the integration of the CoE strategy, generates an unempathetic response. Also, due to the absence of a style synchronization mechanism, there are inconsistencies in the emotions conveyed between the video and audio components. In contrast, our Empatheia system not only produces high-quality empathetic response content but also ensures that the speech and talking avatar exhibit correct and consistent emotional expressions. Similarly, in the second example, the Pipeline system erroneously interprets the user's emotion, mistakenly assuming that the user is happy about securing second place, whereas Empatheia accurately identifies the user's true emotional state through comprehensive multimodal understanding. Furthermore, the Pipeline incorrectly assigns the avatar's identity, presenting a male voice paired with a female avatar. On the contrary, our Empatheia shows outstanding capability in correctly handling the avatar profile consistency challenge. In Appendix ¬ßF.4 we showcase more instances for more sufficient case studies.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we pioneer a novel task of avatar-based MERG. We first introduce AvaMERG, a large-scale high-quality benchmark dataset for MERG, which extends traditional text-based ERG by integrating authentic human speech audio and dynamic talking-face avatar videos. AvaMERG encompasses a diverse range of avatar profiles and covers various real-world scenarios, providing a robust foundation for multimodal empathetic dialogue research. Further, we present Empatheia, a benchmark system tailored for MERG.\n\nBased on a backbone LLM as the core reasoner, Empatheia leverages a multimodal encoder, speech generator, and talking-face avatar generator, forming an end-to-end system. We further enhance Empatheia with a Chain-of-Empathetic reasoning mechanism, and implement a series of empathetic-enhanced tuning strategies, including content consistency learning and style-aware alignment and consistency learning, to ensure emotional accuracy and content/profile consistency across modalities. Experimental results demonstrate that Empatheia consistently outperforms baseline methods in both textual ERG and MERG tasks, highlighting the efficacy of our approach.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A Ethic Considerations",
      "text": "We can identify the following potential ethical considerations for our work:\n\nPrivacy and Data Protection. Empatheia relies on multimodal inputs, including text, voice, and video, which contain highly sensitive personal data. It is essential to ensure that all data collected and processed by the system adheres to strict privacy and data protection regulations, such as the GDPR or CCPA. The system must implement strong encryption techniques for storage and transmission, while also ensuring user data is anonymized where possible. Users should have full control over their data, including the ability to delete their inputs and outputs from the system. Regular audits of data handling and retention practices should be conducted to maintain compliance with privacy standards. Emotional Manipulation and User Vulnerability. Since Empatheia is designed to interact empathetically with users, it may encounter individuals in emotionally vulnerable states. The system must avoid exploiting this vulnerability or manipulating emotions in harmful ways. Safeguards should be in place to ensure that the chatbot's responses are supportive but do not give inappropriate advice or encourage dependency. Ethical guidelines should be established to prevent the misuse of the chatbot, and users should be made aware that it is a machine-generated response system and not a substitute for professional psychological help. Where appropriate, the system could be designed to refer users to human professionals in cases of serious emotional distress.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Bias And",
      "text": "Autonomy and Transparency. The nature of Empatheia's multimodal empathetic responses might blur the lines between human and machine interaction. It is essential to maintain transparency about the system's limitations and make users fully aware that they are interacting with an AI. Users should also have the autonomy to make informed decisions about using the system and be provided with clear options to opt-out or disengage at any time. Regular disclosures about the system's AI-driven nature, its data collection practices, and its purpose should be communicated transparently.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Potential For Misuse.",
      "text": "As with any open-source system, there is a risk of Empatheia being misused in ways that could harm individuals or communities. Bad actors might leverage the system's empathetic capabilities for malicious purposes, such as manipulating others through emotion-driven content or creating deepfakes for deceptive purposes. To mitigate this, the development of Empatheia should include security measures to prevent exploitation, such as limiting the use of avatars and ensuring that any generated content is watermarked or traceable. The open-source release should come with strict usage guidelines and community oversight to ensure responsible use of the system.\n\nLong-Term Psychological Effects. The long-term effects of interacting with an empathetic AI system like Empatheia on human users should be carefully considered. While the system aims to foster deeper emotional connections, there is a risk that users may become overly reliant on AI for emotional support, potentially leading to social isolation or reduced human empathy. Further research should be conducted to assess the psychological impact of prolonged use of such systems, and regular evaluations should be made to ensure that the system enhances human emotional well-being rather than detracting from it.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B Future Work With Avamerg",
      "text": "In this paper, we present a comprehensive exploration of multimodal empathetic response generation.\n\nWe believe this work lays the foundation for future advancements in the field of multimodal sentiment analysis and empathetic interaction. From our practice, several promising directions for future research can be identified.\n\nExploring Higher Performance of MLLMs and Efficient Training Methods. Future work can investigate the performance of various MLLMs in the generation of empathetic responses, particularly their advantages and limitations when processing multimodal inputs. Currently, we utilize state-of-the-art speech and avatar generators; however, their performance remains limited. Therefore, it is essential to enhance the quality of multimodal generation. Also, more efficient training methods, such as transfer learning, few-shot learning, or self-supervised learning, can be explored to improve the training efficiency and performance of these models. Through systematic experimental comparisons, the aim is to identify best practices that enhance the quality and responsiveness of empathetic response generation.\n\nDeveloping Multidimensional Evaluation Methods. Currently, the evaluation of the multimodal generation component of MERG relies solely on human evaluations, which introduces significant uncertainty. Future research should aim to establish multidimensional evaluation methods to comprehensively and automatically assess the effectiveness and quality of multimodal empathetic responses. This can be achieved by combining automated evaluations with human assessments. Specifically, deep learning-based evaluation models can be developed to automatically analyze the semantic consistency of generated responses, the accuracy of emotional conveyance, and the synergistic effects of multimodal inputs. Additionally, emotional analysis tools and semantic understanding techniques should be utilized to conduct detailed emotional depth analyses of the generated responses. Furthermore, the research should explore how to assess cross-modal correlations, such as evaluating the consistency between text, audio, and video, to further enhance the comprehensiveness and accuracy of the evaluation.\n\nEnhancing the Model's Contextual Understanding. Future research can focus on improving the model's understanding of conversational context, particularly in retaining and utilizing historical information during long dialogues. Consideration could be given to incorporating more complex memory mechanisms or contextual attention mechanisms to enhance the model's contextual awareness.\n\nExploring Cross-cultural Expressions of Empathy. Future work can investigate how to effectively generate empathetic responses across different cultural contexts. The research could focus on analyzing the impact of cultural differences on emotional expression and communication styles, adjusting the model based on these findings to better accommodate users from diverse cultural backgrounds.\n\nImproving Dataset Diversity and Quality. Future work can focus on collecting and constructing larger-scale, more diverse multimodal datasets to encompass a wider range of emotional expressions and conversational scenarios. By enhancing the representativeness of the dataset, the model's generalization ability and robustness in diverse emotional interaction contexts can be further improved.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C More Details Of Datasets C.1 Extended Details Of Data Constructions",
      "text": "C.1.1 Dialogue Enriching. Augmenting the Empathetic Dialogue (ED) Dataset. We begin by augmenting the existing pure-text Empathetic Dialogue (ED) dataset to construct our AvaMERG dataset. The ED dataset consists of dialogues aimed at empathetic response generation (ERG) but lacks multimodal and identity-specific information essential for Multimodal Empathetic Response Generation (MERG). To address this, we first enrich each textual empathetic response ùë° ùëñ ‚àà ùëÖ ùëñ with corresponding emotion chain, thereby constructing an emotional chain of thought (CoT).\n\nLeveraging the advanced contextual understanding capabilities of OpenAI's GPT-4, we annotate each utterance in the ED dataset with emotion chain. We define an emotion CoT: emotion ‚Üí emo-tion_cause ‚Üí goal_to_response. GPT-4 assigns an appropriate emotion chain to each utterance based on the dialogue context.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "An Example Of Our Prompt Template:",
      "text": "As an expert in empathetic dialogue analysis, your task is to analyze the emotional dynamics and intentions behind a conversation between two participants: a 'speaker' and a 'listener'. The goal is to first consider the event scenario in which the conversation takes place, and then identify the 'Emotion Cause' for the speaker and the 'Goal to Response' for the listener's final reply. Task Overview: 1. Emotion Scenario: Identify the specific event that occurs within the context of the conversation, which serves as the backdrop for the emotional dynamics and interactions between the speaker and listener. 2. Emotion Cause: Based on the conversation context, sentiment, and dialogue history, analyze and identify the underlying emotional cause or trigger for the speaker. 3. Goal to Response: Analyze the last response from the listener and identify the intended goal behind that response.\n\nThe goal should relate to how the listener is attempting to address the speaker's emotional state. Input JSON Field Descriptions: -dia_id: A unique identifier for the dialogue.\n\nsentiment: The emotional tone of the conversation.\n\ncontext: The background information of the conversation, describing the environment, setting, or situation in which the dialogue occurs.\n\ndialogue: The actual conversation or exchange of dialogue between speaker and listener. Expected Output: Please provide the following in **concise** JSON format: -**Event Scenario**: A **short description** summarizing the main context or situation of the dialogue (e.g.,  ' The speaker is expressing fear after experiencing something unsettling').\n\n-**Emotion Cause**: A **brief** explanation of the **specific event** or experience that triggers the speaker's emotion (e.g., 'Elevator game brings horror experience').\n\n-**Goal to Response**: A **concise** goal that describes the **specific emotional state** the listener is attempting to address (e.g., 'Alleviating fear'). Example Output: { \"dia_id\": \"<dia_id>\", \"event_scenario\": \"The speaker is expressing fear after experiencing something unsettling\", \"emotion_cause\": \"Elevator game brings horror experience\", \"goal_to_response\": \"Alleviating fear\" } To ensure the accuracy of the emotion annotations, we implement a validation step where human annotators review the GPT-4 assigned emotions. This process involves cross-referencing the emotion labels with the dialogue content to verify consistency and appropriateness. Any discrepancies are resolved through discussion among annotators, ensuring high-quality emotion annotations.\n\nEnriching Identity Information. To enable MERG models to generate appropriate avatar profiles for both audio and video modalities, we further annotate each utterance with identity information for both participants in the dialogue. This includes:\n\n‚Ä¢ Age: We define four age periods-child (0-15 years), young (16-34 years), middle-aged (35-59 years), and elderly (60+ years). ‚Ä¢ Gender: Binary genders-male and female.\n\n‚Ä¢ Timbre: Three vocal timbres-low, mid, and high.\n\nGPT-4 is utilized to determine the above labels for each utterance, ensuring that the dialogue reflects realistic interactions between participants with diverse profiles. The identity annotations are critical for training models to generate contextually appropriate and personalized empathetic responses in multiple modalities.\n\nData Balancing and Expansion. Observing that the raw ED dataset is imbalanced (e.g., most dialogues involve young or middleaged participants), we employ GPT-4 to generate additional dialogues that include underrepresented age groups and genders, as well as a balanced distribution of timbres. GPT-4 also detects and labels the dialogue topics, covering 10 primary common topics: [Social Issues and Moral Dilemmas, Achievements and Self-Realization, Support and Comfort, Emotions and Feelings, Disappointments and Expectations, Life Events, Interpersonal Relationships, Health and Well-being, Uncertainty About the Future, Personal Struggles and Challenges].\n\nTo enhance diversity, GPT-4 is instructed to generate dialogues that based on various races, cultural backgrounds, and socio-economic statuses, reflecting a realistic and inclusive range of human experiences. This process results in a more balanced and representative dataset.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Enriching Identity For Dialogue Generation",
      "text": "You are an AI language model tasked with generating a dialogue between two participants, incorporating detailed identity information and topic annotations.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Requirements:",
      "text": "‚Ä¢ Dialogue Structure: The dialogue contains 3 turns, with alternating participants (max 6 utterances). Each turn includes the full dialogue history and the listener's empathetic response. {\"conversation_id\": \"string\", \"speaker_profile\": { \"age\": \"string\", \"gender\": \"string\", \"timbre\": \"string\" }, \"listener_profile\": { \"age\": \"string\", \"gender\" : \"string\", \"timbre\": \"string\" }, \"topic\": \"string\", \"turns\": [{\"turn_id\": \"string\", \"context\": \"string\", \"dialogue_history\": [ { \"index\": int, \"role\": \"string\", \"utterance\" : \"string\" }],\n\n\"response\": \"string\", \"chain_of_empathy\": { \"speaker_emotion\": \"string\", \"event_scenario\": \"string\", \"emotion_cause\": \"string\", \"goal_to_response\": \"string\"} ] } Human Annotation and Cross-Checking. To ensure the quality and accuracy of the augmented dataset, we recruit human annotators for rigorous manual checking. Each dialogue undergoes a 3-person cross-checking process where annotators verify:\n\n‚Ä¢ Content Accuracy: Whether the dialogue content is coherent, contextually appropriate, and free from biases or offensive language. ‚Ä¢ Meta-Profile Consistency: Whether the assigned identity information and emotion labels are accurate and consistent with the dialogue content. ‚Ä¢ Topic Relevance: Whether the dialogue topics are correctly identified and relevant.\n\nAnnotators receive detailed guidelines and training to ensure consistency in their evaluations. Discrepancies among annotators are discussed and resolved collectively. Only dialogues that receive unanimous approval from all three annotators are included in the AvaMERG dataset.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "C.1.2 Audio & Video Recording.",
      "text": "Volunteer Recruitment. We recruit a large and diverse group of English-speaking volunteers representing different ages, genders, vocal characteristics, and races (Asian, Caucasian, African, Latino, Indian). Recruitment is conducted through community outreach, social media, and collaboration with institutions to ensure diversity. All volunteers provide informed consent and are compensated for their participation. We standardized the filming environment requirements. Due to the need for diversity, 30% of volunteers with certain characteristics could not be found offline, so they communicated with us online and submitted their recorded results. Similarly, they adhered to consistent environment requirements during their recording.\n\nParticipant Pairing and Assignment. Volunteers are paired and grouped according to the profiles determined in the annotated AvaMERG dialogues. Each pair corresponds to the identity profiles of the dialogue participants, ensuring that the multimodal data accurately reflects the textual annotations. Care is taken to match volunteers to profiles that they can authentically portray, enhancing the realism of the dataset.\n\nRecording Sessions. During the recording sessions, volunteers perform the dialogues by carefully reading the utterance text. They are provided with context about the dialogue, including the emotional state and background of the characters. Instructions are given to exhibit the correct emotional performance, paying close attention to:\n\n‚Ä¢ Vocal Attributes: Tone, pitch, and timbre corresponding to the annotated vocal timbre and emotion. ‚Ä¢ Facial Expressions: Micro-facial expressions that align with the emotional content, captured using high-resolution cameras.\n\nProfessional recording equipment is used to ensure high-quality audio and video data. Sessions are supervised by directors who provide guidance to volunteers to achieve the desired performances.\n\nPost-processing. Recorded data undergoes post-processing to enhance quality. This includes noise reduction in audio files, color correction in videos, and synchronization of audio and video streams. Metadata is added to files to link them with the corresponding textual annotations and identity profiles. After processing, considering storage space and the input requirements of our model, we standardized the format to meet both our storage capacity and the model's needs, while ensuring the audio and audiovisual content remained unchanged.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "C.1.3 Manual Annotation Verification.",
      "text": "Verification Process. To ensure the highest quality of our dataset, we implement a thorough manual annotation verification process following the post-processing stage.\n\nEach recorded dialogue, along with its corresponding annotations and multimodal content, undergoes a comprehensive review by at least two independent annotators. The verification process focuses on:\n\n‚Ä¢ Content Alignment: Checking that the spoken words and visual expressions in the recordings accurately match the textual utterances and annotated emotion chains. ‚Ä¢ Profile Consistency: Ensuring that the age, gender, timbre, and emotional expressions portrayed by the volunteers align with the assigned profiles. ‚Ä¢ Technical Quality: Verifying that the audio and video recordings meet the required technical standards, including clarity, resolution, and synchronization.\n\nQuality Metrics Calculation. We calculate the Cohen's Kappa Score to measure the agreement between annotators. Achieving a score of 0.78 indicates a high level of consistency and reliability in the annotation process. Any instances where annotators disagree or identify potential issues are reviewed collectively, and problematic data is either corrected or discarded.\n\nFinalization. Only dialogues that pass the manual verification process with unanimous approval are included in the final AvaMERG dataset. This rigorous quality control ensures that the dataset is both reliable and suitable for training and evaluating MERG models.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C.2 Detailed Data Highlights",
      "text": "Here, we extend the content of Dataset Construction from the main article to provide a more comprehensive introduction to all the highlights of our AvaMERG dataset. AvaMERG boasts a diverse array of avatar profiles. This diversity ensures that models trained on AvaMERG can generalize across various demographic profiles, promoting inclusivity and reducing biases. AvaMERG contains a total of 33,048 dialogue samples and 152,021 dialogue utterances, distribution across both age groups and gender, as illustrated in the figure  2 . The dataset includes avatars representing different life stages: Child, Young, Middle-aged, and Elderly, with a nearly equal gender representation within each category. This ensures that the models trained on AvaMERG can generalize effectively to a wide variety of demographic profiles, promoting inclusivity and reducing bias in real-world dialogue systems. For children, there is an almost equal split between male (12,563) and female  (12, 577)  utterances. Among the young adult group, the dataset includes 37,010 male utterances and 37,488 female utterances. In the middleaged category, there are 15,208 male utterances and 13,818 female utterances. Lastly, the elderly group shows an equal number of utterances for both genders, with 11,763 male and 11,763 female utterances. This diverse age and gender representation ensures that dialogue systems trained on AvaMERG can perform robustly across various demographic profiles, enhancing fairness and inclusivity in real-world applications.\n\nDetailed and rich emotional design for real-world applicability. We adopt 32 fine-grained textual emotions and 7 coarsegrained multimodal emotions, and map them appropriately, as shown in Figure  10 . As shown in the emotional distribution in Figure  2 , AvaMERG covers a wide range of emotional expressions. Among the 33,048 dialogues, the most prevalent emotion is sadness, accounting for 56.7% of the samples, which reflects the empathetic nature of many conversations, especially in scenarios requiring emotional support or assistance. This is followed by happiness (20.3%), which captures the dialogues involving positive reinforcement or joyful interactions. Other emotions such as anger (7.9%), contempt (7.9%), and surprise (6.3%) are also represented in reasonable proportions, ensuring that the dataset includes not only empathetic responses but also situations where the user expresses negative or unexpected emotions. Fear and disgust appear less frequently, with 5.1% and 1.8% respectively, but still provide valuable instances for training models that can handle a full spectrum of emotional states. This rich emotional diversity ensures that models trained on AvaMERG are capable of understanding and generating appropriate responses to a broad range of emotional expressions, improving the model's ability to handle real-world interactions where emotions play a critical role.\n\nA comprehensive topic design. The AvaMERG dataset showcases a meticulously designed mapping between various real-world topics and the emotional responses they elicit, providing an invaluable resource for training empathetic response models. As shown in the heatmap Figure  11 , topics such as \"Achievements and Self-Realization\", \"Disappointments and Expectations\", \"Health and Well-being\", and \"Personal Struggles and Challenges\" elicit a wide range of emotional responses, including anger, happiness, and sadness. For instance, \"Health and Well-being\" is associated with a significant number of utterances reflecting fear (1,158), sadness (2,214), and disgust (810), which mirrors real-world conversations where health concerns often evoke complex emotions. Similarly, \"Achievements and Self-Realization\" is more frequently linked to positive emotions, with 3,214 utterances expressing happiness, demonstrating the natural alignment between positive life events an gry  and joyful emotions. In contrast, \"Social Issues and Moral Dilemmas\" and \"Uncertainty About the Future\" are characterized by more negative emotions, such as fear and contempt, reflecting the inherent challenges and anxieties that arise when discussing social complexities or future uncertainties. By covering a wide spectrum of topics and emotions, the dataset enables the development of empathetic models capable of understanding and responding to the nuanced emotional underpinnings of different conversations.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "D More Details Of Methods",
      "text": "In this part, we provide an extension to our Empatheia system, including the input construction format, the specifics of the generator, and the training details.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "D.1 Empatheia Text Input",
      "text": "Empatheia is designed with four complementary training stages, each of which employs a distinct input structure. For the CoE learning and Content Consistency Learning (CCL) stages, the input format is as follows:\n\nAn Input Example for the CoE and CCL",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Input:",
      "text": "Provide an empathetic response based on the given dialogue context below. Don't rush to give the response, thinking step by step. Dialogue Context: { Speaker:I paid all my bills today, I feel great! Assistant:Every little accomplishment counts! What bills did you have to pay? Listener:Rent and electricity. We've been struggling financially so it's such a relief to pay bills. } Target: Firstly, the event scenario of this conversation is: Paying off overdue rent and electricity bills amid financial struggles.\n\nSecondly, the emotion of the speaker is: content Thirdly, the emotion cause is: Relief from the burden of financial stress after successfully paying bills. Fourthly, the goal to response is:Providing support and validation for the speaker's sense of accomplishment. Finally, the response is: Sorry to hear that. Well, at least this will be a weight off your shoulders.\n\nFor different modalities of dialogue input, we standardize them into feature vectors, which can then be fed into the LLM for multimodal information comprehension. For the stages of style consistency learning, our input consists solely of multimodal speech and video. This setup encourages the model to learn how to generate multimodal responses with consistent emotion and style. Specifically, the input format to the LLM is structured as follows:",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "An Input Example For Style Learning",
      "text": "Input: Provide an multimodal empathetic response based on the given dialogue context below. Dialogue Context: { Speaker:<Aud> <Vid>. Listener:<Aud> <Vid>. Speaker:<Aud> <Vid>. } Target:\n\nHere, <Aud> and <Vid> are special placeholders, and before being input into the LLM, the token embeddings at these positions will be replaced with the corresponding audio and video features. <AUDi> and <VIDi> are used as multimodal generation signals.\n\nCombining the first two phases, the final overall training input consists of three modalities. We define the input format for the LLM in this phase as follows:",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "An Input Example For Overall Training",
      "text": "Input: Provide a multimodal empathetic response based on the given dialogue context below. Don't rush to give the response, thinking step by step. Dialogue Context: { Speaker:I paid all my bills today, I feel great!<Aud> <Vid>. Listener:Every little accomplishment counts! What bills did you have to pay?<Aud><Vid>. Speaker:Rent and electricity. We've been struggling financially so it's such a relief to pay bills.<Aud> <Vid>. } Target: Firstly, the event scenario of this conversation is: Paying off overdue rent and electricity bills amid financial struggles. Secondly, the emotion of the speaker is: content Thirdly, the emotion cause is: Relief from the burden of financial stress after successfully paying bills. Fourthly, the goal to response is:Providing support and validation for the speaker's sense of accomplishment. Finally, the response is: Sorry to hear that. Well, at least this will be a weight off your shoulders. <AUD1>...AUD16> <VID1>...<VID16>",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "D.2 Technical Details Of Two Generators",
      "text": "Next, we will introduce the details of the speech generator and video generator we used, as well as how they receive the features passed from the CS and SD modules.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "D.2.1 Speech Generator.",
      "text": "For our system's speech generation, we employ the state-of-the-art TTS model, StyleTTS2. Leveraging advanced diffusion models and adversarial training, StyleTTS2 is able to produce speech with emotions that are more authentic and natural than earlier TTS models, which is critical for our application. The input to StyleTTS2 includes the target empathic response text, denoted as ùë°, and an optional reference mel-spectrogram, denoted as ùë•. StyleTTS requires the input of the text script to be converted into speech, along with a reference audio containing emotional cues to serve as the style template. The modules responsible for reconstructing ùë• in StyleTTS2 are listed as follows.\n\nText Encoder. The acoustic text encoder ùê∏ ùëéùëêùëú encodes the input phonemes into hidden representations:\n\nStyle Encoder. The style encoder ùê∏ ùëüùëí ùëì encodes the input reference mel-spectrogram ùë• into a styled vector:\n\nwhere ùëüùëí ùëì ùë† encapsulates style information such as timbre, emotion, and other stylistic characteristics present in the reference audio.\n\nText Aligner. The text aligner ùê¥ utilizes a dot product to extract aligned phoneme representations:\n\nfrom the input speech ùë• and phonemes ùë°. Here, ùëé ùëù ùëüùëíùëë denotes the duration prediction, which is computed by\n\nwhere BERT is pre-trained on extensive corpora of Wikipedia articles as a prosodic text encoder. ùê∑ ùëëùë¢ùëü is the duration predictor.\n\nDuration Predictor. The duration predictor ùëÜ predicts the duration of the reconstructed phonemes by:\n\nProsody Predictor. The prosody predictor ùëÉ predicts the pitch and energy of the reconstructed phonemes by: pùíô , nùíô = ùëÉ (ùíâ ùë°ùëíùë•ùë° , ùíî)\n\nFinally, the reconstructed speech is obtained by a speech decoder ùê∫:",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "D.2.2 Talking Head Generator.",
      "text": "DreamTalk is a sophisticated framework for generating expressive talking heads, utilizing diffusion models to deliver high-quality performance while minimizing reliance on expensive style references. The framework is comprised of a denoising network, a stylesensitive lip expert, and a style predictor. The denoising network employs diffusion models to create audio-driven facial movements that reflect the speaking style indicated by a reference video. The style-sensitive lip expert guarantees accurate lip synchronization and dynamic facial expressions, while the style predictor derives personalized speaking styles directly from the audio input.\n\nHere, we will focus on introducing the denoising network ùê∏ ùúÉ , which learns to denoise the noisy motion ùëö to obtain the predicted motion ùëö * (0) under the conditions of audio window ùê¥ ùë§ and reference video ùëÖ:\n\nùëö * (0) = ùê∏ ùúÉ (ùê¥ ùë§ , ùëÖ, ùëö, ùë°)\n\nwhere ùë° represents the time step, and ùê∏ ùúÉ consists of two encoders: the audio encoder ùê∏ ùëéùë¢ùëë and the style encoder ùê∏ ùë†ùë° ùë¶ . We denote the style code obtained from the style encoder as ùë†. During inference, based on the style code ùë†, DreamTalk employs the DDPM sampling algorithm to generate predicted facial motions.The generated facial motions are subsequently rendered into videos by the PIRenderer.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "D.3 Training Details",
      "text": ". Empatheia comprises four complementary training stages, each playing a crucial role in the overall process. In the following sections, we will provide a more detailed overview of the specific training details for each stage. ‚ñΩTraining Objective: In this phase, we aim for the LLM to learn how to engage in step-by-step reasoning based on CoE to generate high-quality empathetic responses, effectively completing the ERG task. This stage lays the groundwork for subsequent content consistency learning.\n\n‚ñΩTraining Method: During this phase, we employ LoRA and Negative Log-Likelihood (NLL) loss to fine-tune Vicuna, training it to engage in empathetic reasoning by calculating the loss associated with the target component. The training data for this phase not only includes the dialogue context from the first phase but also incorporates the pre-extracted audio and video content representations corresponding to each response text. Specifically, for speech, we input the response text into the two text encoders, ùê∏ ùëéùëêùëú and BERT, in StyleTTS2. The resulting embeddings, ‚Ñé ùë°ùëíùë•ùë° and ‚Ñé ùëèùëíùëüùë° , are concatenated to produce the gold content representation for speech. For video, we utilize the audio encoder ùê∏ ùëéùë¢ùëë in DreamTalk to obtain the gold content representation for video.\n\n‚ñΩTraining Objective: The training objective for this phase is to align the speech and video representations output by the content synthesizer with the gold content representation. This alignment enables the CS to learn to produce outputs that are both accurate and exhibit consistent content signal features.\n\n‚ñΩTraining Method: In this phase, we freeze the parameters of the LLM and only fine-tune the parameters of the CS module. We calculate the L2 loss between the predicted content representation and the ground truth content representation. ‚ñ∂Training step3: Style Aligning and Consistency Learning ‚ñΩ Training Data: The input data format for this phase is illustrated in D.1, which uses multimodal special token placeholders in a dialogue format. The target training data consists of the audio and video style representations. The gold audio style representation is pre-extracted through the ùê∏ ùëüùëí ùëì encoder of StyleTTS2, which processes the gold speech, while the gold video style representation is pre-extracted using DreamTalk's ùê∏ ùë†ùë° ùë¶ encoder, which processes the gold video. To ensure style consistency, in this stage, we also apply supervised constraints to the SD module using emotion and profile labels.\n\n‚ñΩTraining Objective:The goal of this phase is to align the speech and video style representations output by the SD module with the gold style representations, allowing the SD module to learn to produce accurate and consistent style signals.\n\n‚ñΩTraining Method: During training, we use ImageBind and a mapping layer to extract audio and video features from the multimodal dialogue, which are then used to replace the special token placeholders. As in the previous phase, we freeze the parameters of the LLM and train the SD module using L2 loss and classification loss. ‚ñΩTraining Objective: This stage constitutes a comprehensive finetuning process aimed at equipping the model with the full capability of multimodal empathetic responses. It aims to provide not only sufficiently empathetic textual responses but also synchronized, content-and style-accurate multimodal responses.\n\n‚ñΩTraining Method: In this stage, we employ LoRA for fine-tuning the LLM while simultaneously updating the CS and SD modules. The overall loss is calculated as the weighted sum of the losses from the previous three stages.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "E Extended Experiment Settings",
      "text": "In this section, we provide more detailed experimental settings, encompassing hyperparameters, the construction of the pipeline, and the specifics of the evaluation.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.1 Hyper-Parameter Settings",
      "text": "The basic hyperparameter settings for our training process are shown in Table  8 . In order to reduce the usage of GPU memory and accelerate the training speed, we utilized Deepspeed to train our model, with some of the parameter settings presented in the Table  9 .",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.2 Pipeline Baseline Implementation Details",
      "text": "We utilize the same LLM backbone as Empatheia to construct our pipeline baseline, also adopting StyleTTS2 and DreamTalk as multimodal generators. Unlike Empatheia which employs implicit instructional features embedding for end-to-end signal passing, the pipeline solely relies on explicit meta-response texts to propagate generated instructional signals. Also, the pipeline system will not be equipped with the CoE strategy. For the training of our pipeline, we exclusively utilize text data and additionally introduced profile attributes. Consequently, after training, we can guide the subsequent generation of speech and talking face videos based on the response text, emotion, and profile information outputted by the model.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.3 Evaluation Details",
      "text": "Here, we detail how we conduct the evaluation for the various tasks, including the textual ERG task, speech generation task, and talking head generation task on the AvaMERG dataset.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.3.1 Text Erg Evaluation Metrics.",
      "text": "For the text ERG task, we employ three evaluation metrics: Emotion Accuracy (Acc), and Distinct metrics (Dist-1 and Dist-2). These metrics are designed to evaluate both the emotional correctness of the generated responses and their lexical diversity.\n\nEmotion Accuracy (Acc). Acc measures the percentage of correctly predicted emotions in the generated responses. The correct emotion is defined as the exact match with the ground truth emotion label. Acc is computed as follows:\n\nwhere 'correct emotions' represents the total number of responses where the predicted emotion matches the gold label.\n\nDistinct-1 (Dis-1). This evaluates the diversity of unigrams (single words) in the generated responses. It is defined as the ratio of unique unigrams to the total number of unigrams in the generated text:\n\nA higher Distinct-1 score indicates more lexical diversity and reduces the likelihood of repetitive responses.\n\nDistinct-2 (Dis-2). This is analogous to Distinct-1, but it measures the diversity of bigrams (two consecutive words). It is computed as follows:\n\nA higher Distinct-2 score indicates that the model is generating more contextually diverse phrases.\n\nHuman Evaluation of Textual ERG. For human evaluation, we randomly selected 200 dialogues from the test dataset. Taking into account both the cost of human labor and the reliability of the results, we chose competitive models from the last year as representative baselines. Given the dialogue context and the responses generated by these models, we engaged three annotators to score the responses using a majority voting system. They rated each response on a scale from 1 to 5 (1: not at all, 3: adequate, 5: excellent) based on four key criteria: Empathy, Coherence, Informativity, and Fluency. Specifically, these criteria are: 1) Empathy (Emp): whether the response demonstrates an understanding of the user's emotions and experiences, and responds appropriately. 2) Coherence (Coh): whether the response is logically consistent and contextually relevant.\n\n3) Informativity (Inf): whether the response provides useful and meaningful information. 4) Fluency (Flu): whether the response is grammatically wellformed and easy to read.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "E.3.2 Speech Generation Evaluation Metrics.",
      "text": "For the speech generation component of MERG, we use subjective and objective metrics to assess the quality and emotional expressiveness of the generated speech.\n\nMean Opinion Score (MOS). MOS is a subjective evaluation metric where human evaluators rate the naturalness of the generated speech on a scale of 1 to 5, with 5 indicating highly natural speech. MOS is computed as:\n\nSimilarity MOS (SMOS). SMOS measures how similar the generated speech is to a reference speech sample in terms of emotional tone and expressiveness. Like MOS, it is rated on a 5-point scale by human evaluators.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "ùëÜùëÄùëÇùëÜ =",
      "text": "similarity ratings #evaluators\n\nE.3.3 Talking Head Avatar Generation Evaluation Metrics.\n\nFor evaluating the quality of the generated talking head avatars, we employ a combination of perceptual and geometric metrics to measure the visual fidelity and synchronization of the avatar's lip movements with the speech.\n\nCumulative Probability of Blur Detection (CPBD). CPBD quantifies the perceptual sharpness of the generated video frames by estimating the probability that an observer would detect blurring based on edge width analysis. Higher CPBD values indicate sharper images and fewer perceived blurs. The CPBD score is calculated by first analyzing the cumulative distribution of edge widths in an image. For each edge in the image, the probability that the edge is perceived as blurred is computed. The overall CPBD score is the average of these probabilities over all edges. The formula can be expressed as:\n\nwhere ùëÉ (ùëí ùëñ ) is the probability of blur detection for edge ùëí ùëñ , and ùëÅ is the total number of detected edges in the image. This metric gives a perceptual estimate of how likely it is for human viewers to notice blur across the video frames.\n\nStructural Similarity Index Measure (SSIM). SSIM assesses the visual similarity between the generated avatar video frames and the ground truth video frames. It measures the perceived quality in terms of luminance, contrast, and structure:\n\nwhere ùúá ùë• and ùúá ùë¶ are the means of the generated and reference images, ùúé 2 ùë• and ùúé 2 ùë¶ are the variances, and ùúé ùë• ùë¶ is the covariance of the two images. ùê∂ 1 and ùê∂ 2 are constants to stabilize the division.\n\nSyncNet Confidence Score (Sync ùëê ùëì ). Sync ùëê ùëì is used to evaluate the synchronization between the generated speech and the avatar's lip movements. It measures the alignment between the visual lip movements and the audio, with higher scores indicating better synchronization:\n\nùëÜùë¶ùëõùëê ùëê ùëì = aligned speech and lip movements total frames (30)",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "E.3.4 Human Evaluation Of Merg.",
      "text": "To better evaluate the performance of models on the MERG task, we have newly defined six human evaluation metrics, specifically:\n\n‚Ä¢ Speech Content Accuracy (SCA): Assesses whether the content in the generated audio is complete and consistent with the response text.\n\nwhere ùê∂ ùë† ùëîùëíùëõùëí represent the generation speech's content, and the ùê∂ ùë† ùëîùëúùëôùëë represent the gold speech's content.\n\n‚Ä¢ Video Content Accuracy (VCA): Evaluates whether the face in the generated video accurately and fluently reads out the response text.\n\nwhere ùê∂ ùë† ùëîùëíùëõùëí represent the generation video's content, and the ùê∂ ùë† ùëîùëúùëôùëë represent the gold video's content.\n\n‚Ä¢ Speech Style Accuracy (SSA): Determines the accuracy of the emotion conveyed in the generated speech and whether the voice matches the intended character profile.\n\nwhere ùëÜ ùë† ùëîùëíùëõùëí represent the generation speech's style, and the ùëÜ ùë† ùëîùëúùëôùëë represent the gold speech's style.\n\n‚Ä¢ Video Style Accuracy (VSA): Assesses the accuracy of the emotion expressed by the generated avatar and whether the avatar's appearance and behavior match its intended profile.\n\nwhere ùëÜ ùë£ ùëîùëíùëõùëí represent the generation video's style, and the ùëÜ ùë£ ùëîùëúùëôùëë represent the gold video's style.\n\n‚Ä¢ Multimodal Content Consistency (MCC): This is a comprehensive comparison that evaluates the consistency of content across the three modalities (speech, video, and text).\n\n‚Ä¢ Multimodal Style Consistency (MSC): Provides an overall evaluation of the consistency of style across the three modalities.\n\nDuring the testing process, we deliberately engaged three experienced evaluators to ensure the comprehensiveness and accuracy of the evaluation. The task of these three evaluators was to conduct",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "F More Experiments And Analysis",
      "text": "In this part, we present additional experimental results and analyses to further demonstrate the performance of Empatheia.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "F.1 Impact Of Hyperparameter Settings",
      "text": "Here we study the impact of various hyperparameters on model performance, including the number of transformer blocks, the number of special signal tokens used for multimodal generation, and the loss weights alpha and beta.\n\nFigure  12  illustrates the model's performance when varying the number of transformer blocks in CS and SD modules, specifically evaluating the model's output on multiple metrics. The \"Avg. Score\" reflects the average performance across six carefully crafted manual evaluation metrics, which together capture various aspects of the model's ability to generate multimodal empathetic responses. The results show that the transformer block reaches its peak performance at the 4 layers. Deeper layers can, to some extent, enhance the model's learning ability by capturing more complex interactions between patterns. However, further increasing the number of transformer layers did not lead to significant improvement. Therefore, Empatheia sets the number layers of transformer blocks to 4. Figure  13  illustrates the impact of different numbers of audiovisual special tokens on model performance. As shown, the model's performance peaks when the number of tokens reaches 16.\n\nWe scale the content and style learning losses using ùõº and ùõΩ to bring them closer to the magnitude of the Vicuna's inherent loss, aiming to achieve balanced training. The experiments on loss hyperparameters are visualized in a heatmap in Figure  14 , the model performs better when ùõº and ùõΩ are relatively balanced, indicating that a more balanced loss contributes to the model's convergence to an optimal solution.   module for multimodal generation, the average score growth rate of the Pipeline in terms of accuracy and consistency in multimodal generation lags significantly behind Empatheia. This once again demonstrates the robust potential of Empatheia.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "F.2 Impact Of Training Data Amounts",
      "text": "",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "F.3 Impact Of Coe Steps",
      "text": "CoE works by breaking down the overall hard problem into semantically coherent and easier sub-problems. To gain a deeper understanding of how each step of the CoE reasoning process contributes to the ERG, we conduct ablation experiments on CoE, by progressively using more leveled steps of CoE prompts. The results, as illustrated in Figure  16 , show that as Empatheia advances through the steps of CoE, the model's empathetic performance improves significantly, where the capacity becomes stronger with each level of complexity, leading to more accurate and contextually appropriate responses. This indicates that each individual step in CoE plays a vital role, enhancing the model's ability to comprehensively analyze and understand user's emotion and intention. By systematically breaking down the CoE into distinct phases, we highlight that the model's empathetic performance has been progressively enhanced.\n\nStep 1\n\nStep 2\n\nStep 3\n\nStep",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "F.4 More Case Study",
      "text": "We further provide several examples to demonstrate the performance differences between our model and pipeline.\n\nMERG Performance Comparisons. As illustrated in Figure  17 , Figure  18 , Figure  19  and Figure  20 , our model exhibits more consistent and empathetic multimodal responses, indicating Empatheia's deeper understanding of multimodal contexts. This is attributed to our multi-stage training approach, as well as the advanced content synchronizer and style deconstructor employed.\n\nCoE Qualitative Results. We also present several examples to visualize the reasoning process of CoE. As shown in Figure  21 , Figure  22 , Figure  23  and Figure  24 , CoE can significantly enhance the model's empathy capabilities and the accuracy of perceiving users' emotions, the underlying intention, and finally the correct response.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Current User Query",
      "text": "Yes, true, I know they tend to let bills go for things like that, which I don't do.\n\nResponse by Pipeline model √ó Well, vacations aren't everything. There are plenty of other things to focus on instead. Everyone has their own priorities.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Response By Empatheia",
      "text": "It's understandable that you feel that way. It can be tough when you see others enjoying things that are out of reach for you right now.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "‚àö Dialogue History",
      "text": "Being jealous of another person is a perfectly fine response. Despite this, you never know what is going on behind the scenes, she may have taken out huge loans just to fund her trips.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Sad Face, Sad Voice",
      "text": "",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Gold Response",
      "text": "I bet you would go on many vacations if you did the same as her. Instead, you are being a responsible person and doing what is best for your future.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Sad Face, Sad Voice Sad Face, Angry Voice Sad Face, Sad Voice",
      "text": "A little while ago, my friend went on another beach vacation.\n\nIt was her 3rd one for the year. My family and I hadn't been on vacation for several years, due to issues beyond our control, so I begrudged her trip a bit.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Dialogue History",
      "text": "Oh, I don't know why concerts make me feel a bit sentimental.\n\nI went to a heavy metal music festival in Las Vegas a few years ago! I had fun, but to be honest it was a bit of an emotional roller coaster.\n\n1. Event Scenario: A breakup occurred during a heavy metal festival in Las Vegas while sharing a room with the ex-girlfriend.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Coe Reasoning In Empatheia",
      "text": "I think it's because everybody can relate when it comes to live music. My girlfriend broke up with me the second day, but we had to stay in the same room.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Response By Empatheia",
      "text": "Wow, it's a good thing you were so prepared! That must have been such a stressful situation, but having the kit with you really saved the day. It's amazing how a little foresight can make such a big difference.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Dialogue History",
      "text": "I bet you are, that's some pretty good foresight right there! Did you have to use it this past winter?\n\nI am so glad that I decided to pack a kit for my car in case of bad winter weather.\n\n1. Event Scenario: An emergency kit was packed for winter weather, which proved essential during a blizzard.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Dialogue History",
      "text": "It's the worst waiting for vacations! You want the time before to go fast but then it to go slow during. Never works out that way, unfortunately.\n\n1. Event Scenario: Eagerly counting down the days, anxiety built for the upcoming summer cruise.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Coe Reasoning In Empatheia",
      "text": "A week before my cruise this summer, I was counting down the days with bated breath. I thought I would actually DIE before I got to go on it.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Response By Empatheia",
      "text": "Wow, it's a good thing you were so prepared! That must have been such a stressful situation, but having the kit with you really saved the day. It's amazing how a little foresight can make such a big difference.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Dialogue History",
      "text": "We get carried away with our emotions sometimes. Did you explain it to them? I feel so bad when I think back to times that I got angry and yelled at my kids for no real good reason --just a combination of the day's events and feeling irritated, really.\n\n1. Event Scenario: Reflecting on moments of losing temper with children without justification.",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A snippet of avatar-based Multimodal Empathetic",
      "page": 2
    },
    {
      "caption": "Figure 1: demonstrates a",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualized statistics of AvaMERG dataset.",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the overall architecture of our Empatheia sys-",
      "page": 4
    },
    {
      "caption": "Figure 3: Architecture of our Empatheia MLLM for MERG.",
      "page": 4
    },
    {
      "caption": "Figure 4: Illustration of the Content Synchronizer and Style",
      "page": 5
    },
    {
      "caption": "Figure 4: (a), the module",
      "page": 5
    },
    {
      "caption": "Figure 4: (b), similar to CS module, SD also",
      "page": 5
    },
    {
      "caption": "Figure 5: (a), this training only updates the core",
      "page": 5
    },
    {
      "caption": "Figure 5: Illustrations of the proposed training strategies.",
      "page": 6
    },
    {
      "caption": "Figure 5: (b), we minimize the Euclidean distance between",
      "page": 6
    },
    {
      "caption": "Figure 5: (c), we minimize the Euclidean",
      "page": 6
    },
    {
      "caption": "Figure 6: Results on various emotions, ages, and genders.",
      "page": 7
    },
    {
      "caption": "Figure 7: T-SNE visualization of emotion and profile features.",
      "page": 8
    },
    {
      "caption": "Figure 8: , where we compare the outputs of the",
      "page": 8
    },
    {
      "caption": "Figure 8: Qualitative results of two testing instances.",
      "page": 8
    },
    {
      "caption": "Figure 9: , Gentle tones were the most common",
      "page": 14
    },
    {
      "caption": "Figure 9: The distribution of tone.",
      "page": 14
    },
    {
      "caption": "Figure 2: The dataset includes avatars representing different",
      "page": 15
    },
    {
      "caption": "Figure 10: As shown in the emotional distribution in",
      "page": 15
    },
    {
      "caption": "Figure 2: , AvaMERG covers a wide range of emotional expressions.",
      "page": 15
    },
    {
      "caption": "Figure 11: , topics such as \"Achievements",
      "page": 15
    },
    {
      "caption": "Figure 10: The mapping of fine-grained textual emotions to",
      "page": 15
    },
    {
      "caption": "Figure 11: Emotion-topic heatmap for dialogue utterances.",
      "page": 15
    },
    {
      "caption": "Figure 12: Impact of the transformer layer number.",
      "page": 20
    },
    {
      "caption": "Figure 12: illustrates the model‚Äôs performance when varying the",
      "page": 20
    },
    {
      "caption": "Figure 13: illustrates the impact of different numbers of audio-",
      "page": 20
    },
    {
      "caption": "Figure 14: , the model",
      "page": 20
    },
    {
      "caption": "Figure 15: presents the performance of Empatheia and the pipeline",
      "page": 20
    },
    {
      "caption": "Figure 13: Impact of special token numbers.",
      "page": 20
    },
    {
      "caption": "Figure 14: Impact of loss weights.",
      "page": 20
    },
    {
      "caption": "Figure 15: Performance comparison under different quanti-",
      "page": 20
    },
    {
      "caption": "Figure 16: , show that as Empatheia advances through",
      "page": 20
    },
    {
      "caption": "Figure 16: In-Depth Analysis of the Effectiveness of CoE.",
      "page": 21
    },
    {
      "caption": "Figure 18: , Figure 19 and Figure 20, our model exhibits more consis-",
      "page": 21
    },
    {
      "caption": "Figure 22: , Figure 23 and Figure 24, CoE can significantly enhance",
      "page": 21
    },
    {
      "caption": "Figure 17: Qualitative results A of MERG.",
      "page": 21
    },
    {
      "caption": "Figure 18: Qualitative results B of MERG.",
      "page": 22
    },
    {
      "caption": "Figure 19: Qualitative results C of MERG.",
      "page": 22
    },
    {
      "caption": "Figure 20: Qualitative results D of MERG.",
      "page": 23
    },
    {
      "caption": "Figure 21: Qualitative results A of CoE mechanism.",
      "page": 23
    },
    {
      "caption": "Figure 22: Qualitative results B of CoE mechanism.",
      "page": 24
    },
    {
      "caption": "Figure 23: Qualitative results C of CoE mechanism.",
      "page": 24
    },
    {
      "caption": "Figure 24: Qualitative results D of CoE mechanism.",
      "page": 25
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "An Input Example for Overall Training": "{ S\n} T\nInput:\nProvide a multimodal empathetic response based on the\ngiven dialogue context below. Don‚Äôt rush to give the re-\nsponse, thinking step by step.\nDialogue Context:\npeaker:I paid all my bills today, I feel great!<Aud> <Vid>.\nListener:Every little accomplishment counts! What bills\ndid you have to pay?<Aud><Vid>.\nSpeaker:Rent and electricity. We‚Äôve been struggling finan-\ncially so it‚Äôs such a relief to pay bills.<Aud> <Vid>.\narget:\nFirstly, the event scenario of this conversation is: Paying\noff overdue rent and electricity bills amid financial strug-\ngles.\nSecondly, the emotion of the speaker is: content\nThirdly, the emotion cause is: Relief from the burden of\nfinancial stress after successfully paying bills.\nFourthly, the goal to response is:Providing support and\nvalidation for the speaker‚Äôs sense of accomplishment.\nFinally, the response is: Sorry to hear that. Well, at least\nthis will be a weight off your shoulders.\n<AUD1>...AUD16> <VID1>...<VID16>"
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "2",
      "title": "Empathetic Response Generation with Relation-aware Commonsense Knowledge",
      "authors": [
        "Changyu Chen",
        "Yanran Li",
        "Chen Wei",
        "Jianwei Cui",
        "Bin Wang",
        "Rui Yan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 17th ACM International Conference on Web Search and Data Mining"
    },
    {
      "citation_id": "3",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez",
        "Ion Stoica",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
    },
    {
      "citation_id": "4",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "5",
      "title": "Out of time: automated lip sync in the wild",
      "authors": [
        "Joon Son",
        "Andrew Zisserman"
      ],
      "year": "2016",
      "venue": "Computer Vision-ACCV 2016 Workshops: ACCV 2016 International Workshops"
    },
    {
      "citation_id": "6",
      "title": "Dreamllm: Synergistic multimodal comprehension and creation",
      "authors": [
        "Runpei Dong",
        "Chunrui Han",
        "Yuang Peng",
        "Zekun Qi",
        "Zheng Ge",
        "Jinrong Yang",
        "Liang Zhao",
        "Jianjian Sun",
        "Hongyu Zhou",
        "Haoran Wei"
      ],
      "year": "2023",
      "venue": "Dreamllm: Synergistic multimodal comprehension and creation",
      "arxiv": "arXiv:2309.11499"
    },
    {
      "citation_id": "7",
      "title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting",
      "authors": [
        "Bobo Hao Fei",
        "Qian Li",
        "Lidong Liu",
        "Fei Bing",
        "Tat-Seng Li",
        "Chua"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing. Proceedings of the Advances in neural information processing systems",
      "authors": [
        "Shengqiong Hao Fei",
        "Hanwang Wu",
        "Tat-Seng Zhang",
        "Shuicheng Chua",
        "Yan"
      ],
      "year": "2024",
      "venue": "VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing. Proceedings of the Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Enhancing video-language representations with structural spatio-temporal alignment",
      "authors": [
        "Shengqiong Hao Fei",
        "Meishan Wu",
        "Min Zhang",
        "Tat-Seng Zhang",
        "Shuicheng Chua",
        "Yan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning, Efficiency and Beyond",
      "authors": [
        "Yuan Hao Fei",
        "Zhuosheng Yao",
        "Fuxiao Zhang",
        "Ao Liu",
        "Tat-Seng Zhang",
        "Chua"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries"
    },
    {
      "citation_id": "11",
      "title": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot",
      "authors": [
        "Han Hao Fei",
        "Bin Zhang",
        "Lizi Wang",
        "Qian Liao",
        "Erik Liu",
        "Cambria"
      ],
      "year": "2024",
      "venue": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot",
      "arxiv": "arXiv:2406.15177"
    },
    {
      "citation_id": "12",
      "title": "Latent emotion memory for multi-label emotion classification",
      "authors": [
        "Yue Hao Fei",
        "Yafeng Zhang",
        "Donghong Ren",
        "Ji"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "13",
      "title": "Improving empathetic response generation by recognizing emotion cause in conversations",
      "authors": [
        "Jun Gao",
        "Yuhan Liu",
        "Haolin Deng",
        "Wei Wang",
        "Yu Cao",
        "Jiachen Du",
        "Ruifeng Xu"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "14",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "15",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "16",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Kingma"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "17",
      "title": "Diaasq: A benchmark of conversational aspect-based sentiment quadruple analysis",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Fei Li",
        "Yuhan Wu",
        "Jinsong Zhang",
        "Shengqiong Wu",
        "Jingye Li",
        "Yijiang Liu",
        "Lizi Liao",
        "Tat-Seng Chua"
      ],
      "year": "2022",
      "venue": "Diaasq: A benchmark of conversational aspect-based sentiment quadruple analysis",
      "arxiv": "arXiv:2211.05705"
    },
    {
      "citation_id": "18",
      "title": "A diversity-promoting objective function for neural conversation models",
      "authors": [
        "Jiwei Li",
        "Michel Galley",
        "Chris Brockett",
        "Jianfeng Gao",
        "Bill Dolan"
      ],
      "year": "2015",
      "venue": "A diversity-promoting objective function for neural conversation models",
      "arxiv": "arXiv:1510.03055"
    },
    {
      "citation_id": "19",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "A Survey on Benchmarks of Multimodal Large Language Models",
      "authors": [
        "Jian Li",
        "Weiheng Lu"
      ],
      "year": "2024",
      "venue": "A Survey on Benchmarks of Multimodal Large Language Models",
      "arxiv": "arXiv:2408.08632"
    },
    {
      "citation_id": "21",
      "title": "Knowledge bridging for empathetic dialogue generation",
      "authors": [
        "Qintong Li",
        "Piji Li",
        "Zhaochun Ren",
        "Pengjie Ren",
        "Zhumin Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models",
      "authors": [
        "Aaron Yinghao",
        "Cong Li",
        "Vinay Han",
        "Gavin Raghavan",
        "Nima Mischler",
        "Mesgarani"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "24",
      "title": "Moel: Mixture of empathetic listeners",
      "authors": [
        "Zhaojiang Lin",
        "Andrea Madotto",
        "Jamin Shin",
        "Peng Xu",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Moel: Mixture of empathetic listeners",
      "arxiv": "arXiv:1908.07687"
    },
    {
      "citation_id": "25",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods",
      "authors": [
        "Jaime Lorenzo-Trueba",
        "Junichi Yamagishi",
        "Tomoki Toda",
        "Daisuke Saito",
        "Fernando Villavicencio",
        "Tomi Kinnunen",
        "Zhenhua Ling"
      ],
      "year": "2018",
      "venue": "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods",
      "arxiv": "arXiv:1804.04262"
    },
    {
      "citation_id": "27",
      "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action",
      "authors": [
        "Jiasen Lu",
        "Christopher Clark",
        "Sangho Lee",
        "Zichen Zhang",
        "Savya Khosla",
        "Ryan Marten",
        "Derek Hoiem",
        "Aniruddha Kembhavi"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal Conversational Aspect-based Sentiment Analysis",
      "authors": [
        "Meng Luo",
        "Hao Fei",
        "Bobo Li",
        "Shengqiong Wu",
        "Qian Liu",
        "Soujanya Poria",
        "Erik Cambria",
        "Mong-Li Lee",
        "Wynne Hsu"
      ],
      "year": "2024",
      "venue": "PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal Conversational Aspect-based Sentiment Analysis",
      "arxiv": "arXiv:2408.09481"
    },
    {
      "citation_id": "29",
      "title": "NUS-Emo at SemEval-2024 Task 3: Instruction-Tuning LLM for Multimodal Emotion-Cause Analysis in Conversations",
      "authors": [
        "Meng Luo",
        "Han Zhang",
        "Shengqiong Wu",
        "Bobo Li",
        "Hong Han",
        "Hao Fei"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "30",
      "title": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models",
      "authors": [
        "Yifeng Ma",
        "Shiwei Zhang",
        "Jiayu Wang",
        "Xiang Wang",
        "Yingya Zhang",
        "Zhidong Deng"
      ],
      "year": "2023",
      "venue": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models",
      "arxiv": "arXiv:2312.09767"
    },
    {
      "citation_id": "31",
      "title": "MIME: MIMicking emotions for empathetic response generation",
      "authors": [
        "Navonil Majumder",
        "Pengfei Hong",
        "Shanshan Peng",
        "Jiankun Lu",
        "Deepanway Ghosal",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "MIME: MIMicking emotions for empathetic response generation",
      "arxiv": "arXiv:2010.01454"
    },
    {
      "citation_id": "32",
      "title": "A no-reference image blur metric based on the cumulative probability of blur detection (CPBD)",
      "authors": [
        "D Niranjan",
        "Lina Narvekar",
        "Karam"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "33",
      "title": "Harnessing the power of large language models for empathetic response generation",
      "authors": [
        "Yushan Qian",
        "Wei-Nan Zhang",
        "Ting Liu"
      ],
      "year": "2023",
      "venue": "Empirical investigations and improvements",
      "arxiv": "arXiv:2310.05140"
    },
    {
      "citation_id": "34",
      "title": "Empathetic conversational systems: A review of current advances, gaps, and opportunities",
      "authors": [
        "Aravind Sesagiri",
        "Yinping Yang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "36",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "Hannah Rashkin"
      ],
      "year": "2018",
      "venue": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "arxiv": "arXiv:1811.00207"
    },
    {
      "citation_id": "37",
      "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
      "authors": [
        "Jeff Rasley",
        "Samyam Rajbhandari",
        "Olatunji Ruwase",
        "Yuxiong He"
      ],
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "38",
      "title": "Cem: Commonsenseaware empathetic response generation",
      "authors": [
        "Sahand Sabour",
        "Chujie Zheng",
        "Minlie Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Pandagpt: One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "40",
      "title": "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
      "authors": [
        "Aohan Glm Team",
        "Bin Zeng",
        "Bowen Xu",
        "Chenhui Wang",
        "Da Zhang",
        "Diego Yin",
        "Guanyu Rojas",
        "Hanlin Feng",
        "Hanyu Zhao",
        "Lai"
      ],
      "year": "2024",
      "venue": "Chatglm: A family of large language models from glm-130b to glm-4 all tools"
    },
    {
      "citation_id": "41",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "42",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Measuring speech quality for text-to-speech systems: development and assessment of a modified mean opinion score (MOS) scale",
      "authors": [
        "Mahesh Viswanathan",
        "Madhubalan Viswanathan"
      ],
      "year": "2005",
      "venue": "Computer speech & language"
    },
    {
      "citation_id": "44",
      "title": "A universal image quality index",
      "authors": [
        "Zhou Wang",
        "Alan Bovik"
      ],
      "year": "2002",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "45",
      "title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Xiangtai Li",
        "Jiayi Ji",
        "Hanwang Zhang",
        "Tat-Seng Chua",
        "Shuicheng Yan"
      ],
      "year": "2024",
      "venue": "Towards Semantic Equivalence of Tokenization in Multimodal LLM",
      "arxiv": "arXiv:2406.05127"
    },
    {
      "citation_id": "46",
      "title": "NExT-GPT: Any-to-Any Multimodal LLM",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Leigang Qu",
        "Wei Ji",
        "Tat-Seng Chua"
      ],
      "year": "2024",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "47",
      "title": "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
      "authors": [
        "Jundong Xu",
        "Hao Fei",
        "Liangming Pan",
        "Qian Liu",
        "Mong-Li Lee",
        "Wynne Hsu"
      ],
      "year": "2024",
      "venue": "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
      "arxiv": "arXiv:2405.18357"
    },
    {
      "citation_id": "48",
      "title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction",
      "authors": [
        "Haoqiu Yan",
        "Yongxin Zhu",
        "Kai Zheng",
        "Bing Liu",
        "Haoyu Cao",
        "Deqiang Jiang",
        "Linli Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "49",
      "title": "Exploiting emotion-semantic correlations for empathetic response generation",
      "authors": [
        "Zhou Yang",
        "Zhaochun Ren",
        "Yufeng Wang",
        "Xiaofei Zhu",
        "Zhihao Chen",
        "Tiecheng Cai",
        "Yunbing Wu",
        "Yisong Su",
        "Sibo Ju",
        "Xiangwen Liao"
      ],
      "year": "2024",
      "venue": "Exploiting emotion-semantic correlations for empathetic response generation",
      "arxiv": "arXiv:2402.17437"
    },
    {
      "citation_id": "50",
      "title": "Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models",
      "authors": [
        "Zhou Yang",
        "Zhaochun Ren",
        "Wang Yufeng",
        "Shizhong Peng",
        "Haizhou Sun",
        "Xiaofei Zhu",
        "Xiangwen Liao"
      ],
      "year": "2024",
      "venue": "Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models",
      "arxiv": "arXiv:2402.11801"
    },
    {
      "citation_id": "51",
      "title": "STICKER-CONV: Generating Multimodal Empathetic Responses from Scratch",
      "authors": [
        "Yiqun Zhang",
        "Fanheng Kong",
        "Peidong Wang",
        "Shuang Sun",
        "Swangling Swan-Gling",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Kaisong Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "53",
      "title": "ECQED: emotion-cause quadruple extraction in dialogs",
      "authors": [
        "Li Zheng",
        "Donghong Ji",
        "Fei Li",
        "Hao Fei",
        "Shengqiong Wu",
        "Jingye Li",
        "Bobo Li",
        "Chong Teng"
      ],
      "year": "2023",
      "venue": "ECQED: emotion-cause quadruple extraction in dialogs",
      "arxiv": "arXiv:2306.03969"
    },
    {
      "citation_id": "54",
      "title": "Case: Aligning coarse-to-fine cognition and affection for empathetic response generation",
      "authors": [
        "Jinfeng Zhou",
        "Chujie Zheng",
        "Bo Wang",
        "Zheng Zhang",
        "Minlie Huang"
      ],
      "year": "2022",
      "venue": "Case: Aligning coarse-to-fine cognition and affection for empathetic response generation",
      "arxiv": "arXiv:2208.08845"
    },
    {
      "citation_id": "55",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}