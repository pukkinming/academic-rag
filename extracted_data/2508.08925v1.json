{
  "paper_id": "2508.08925v1",
  "title": "Lpgnet: A Lightweight Network With Parallel Attention And Gated Fusion For Multimodal Emotion Recognition",
  "published": "2025-08-12T13:22:16Z",
  "authors": [
    "Zhining He",
    "Yang Xiao"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "Parallel attention",
    "Dual-Gated fusion",
    "Speaker-independent",
    "Lightweight network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversations (ERC) aims to predict the emotional state of each utterance by using multiple input types, such as text and audio. While Transformer-based models have shown strong performance in this task, they often face two major issues: high computational cost and heavy dependence on speaker information. These problems reduce their ability to generalize in real-world conversations. To solve these challenges, we propose LPGNet, a Lightweight network with Parallel attention and Gated fusion for multimodal ERC. The main part of LPGNet is the Lightweight Parallel Interaction Attention (LPIA) module. This module replaces traditional stacked Transformer layers with parallel dot-product attention, which can model both within-modality and between-modality relationships more efficiently. To improve emotional feature learning, LPGNet also uses a dual-gated fusion method. This method filters and combines features from different input types in a flexible and dynamic way. In addition, LPGNet removes speaker embeddings completely, which allows the model to work independently of speaker identity. Experiments on the IEMOCAP dataset show that LPGNet reaches over 87% accuracy and F1-score in 4-class emotion classification. It outperforms strong baseline models while using fewer parameters and showing better generalization across speakers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversations (ERC)  [1, 2]  focuses on identifying the emotional state of each utterance by using multiple types of input, such as text, audio, and visual signals. It plays an important role in human-computer interaction (HCI)  [3, 4] , especially in building systems that can understand and respond to users' emotions. ERC is a key part of many real-world applications, including empathetic dialogue systems, emotion-aware virtual assistants, and mental health support tools. These systems are widely used in areas like intelligent voice assistants, customer service robots, and telemedicine. Compared to traditional emotion recognition based on a single input type, ERC brings more challenges. This is because it requires understanding context over time, handling frequent speaker changes, and processing emotional signals across different modalities. These challenges make ERC a complex but important problem in affective computing.\n\nRecent ERC models based on Transformer  [5]  and graph neural network (GNN)  [6] [7] [8]  architectures have shown strong performance. However, they often face two key challenges. First, deep sequential layers lead to high computational cost, which limits deployment in real-time systems. Second, many models depend heavily on speaker identity features, making them less effective in open-domain or anonymous settings, where speaker information is missing or unavailable. These limitations highlight the need for efficient and speaker-independent ERC models.\n\nTo improve multimodal fusion in ERC, several attention-based approaches have been developed. These methods aim to better integrate features from different input types, such as text and audio. For example, Tang et al.  [9]  introduced an audio-text interactive attention mechanism that strengthens cross-modal understanding. Zhao et al.  [10]  proposed collaborative attention to align emotional information across pre-trained features. Maji et al.  [11]  designed a cross-modal Transformer block to capture both semantic and temporal dependencies across modalities. Although these techniques have demonstrated good performance, they often come with a high cost. Their architectures are usually deep and complex, which increases both training and inference time. As a result, these models are not well-suited for real-time or resource-constrained environments, such as mobile devices.\n\nIn addition to attention-based models, graph neural networks (GNNs) have also been applied to ERC. Methods such as DialogueGCN  [6]  and MMGCN  [12]  aim to capture speaker interactions by constructing dialogue graphs. These models can represent complex conversational structures but rely heavily on speaker identity labels and manually crafted graphs. This dependency reduces their flexibility and limits their usefulness in open-domain applications, where such information may be unavailable or inconsistent.\n\nThese limitations become especially important when considering real-world ERC applications. In practical settings, systems must often deal with unknown, anonymous, or changing speakers  [13] [14] [15] . They also need to perform emotion recognition in a zero-shot manner, without prior training on speaker-specific data. For instance, voice assistants must understand a user's emotions during their first interaction, without relying on stored speaker profiles  [16] . Similarly, customer service bots interact with a wide range of users, many of whom are anonymous or change frequently. In such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart home terminals, require models to handle frequent speaker switching without identity tracking.\n\nRecent ERC methods based on large language models (LLMs)  [17] [18] [19]  also reveal inconsistencies in how speaker information is handled. InstructERC  [20]  treats ERC as a generation task using emotion templates, but it does not explicitly model speaker roles. DialogueLLM  [21]  uses dialogue context and visual cues but omits speaker identity. BiosERC  [22]  goes further by showing that privacy constraints often prevent the use of identity labels, meaning that models must rely only on the dialogue context. These findings underline the importance of speaker-independent emotion modeling for real-world applications.\n\nAlongside attention and graph-based techniques, self-distillation  [23] [24] [25]  has emerged as a promising solution for improving model performance without increasing complexity. Unlike traditional distillation methods that rely on an external teacher model, self-distillation enables a network to learn from its own intermediate features. For example, Li et al.  [26]  introduced a teacher-free feature distillation method in vision tasks, where a model reuses useful internal representations to boost its own learning. This strategy reduces both memory and computation costs, making it well-suited for lightweight deployment.\n\nBased on these insights, we propose LPGNet, a lightweight and speakerindependent framework for multimodal emotion recognition in conversations. LPGNet is designed to address three major issues: high inference cost, reliance on speaker information, and the need for efficient deployment in practical scenarios.\n\nOur key contributions are as follows:\n\n• We design a Lightweight Parallel Interaction Attention (LPIA) module to replace stacked Transformer layers, allowing efficient modeling of intra-and inter-modal relationships.\n\n• We introduce a dual-gated fusion strategy to refine and dynamically combine multimodal features. • We remove speaker embeddings entirely, enabling better generalization to unseen speakers and anonymous environments. • We apply a self-distillation mechanism to internally extract and reuse useful emotional knowledge without requiring external teacher models.\n\nExtensive experiments on the IEMOCAP benchmark show that LPGNet achieves over 87% accuracy and F1-score for 4-class emotion classification. It not only outperforms strong baseline models but also shows better generalization to unknown speakers, all while using fewer parameters.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Fusion Strategies In Emotion Recognition",
      "text": "Multimodal emotion recognition (MER)  [27]  aims to integrate signals from different modalities such as text, speech, and vision. Early models relied on recurrent neural networks and memory-based fusion techniques. For example, CMN  [28]  used GRUs to model speaker history and applied attention to extract relevant emotion patterns from past utterances. ICON  [29]  extended this by introducing interactive memory modules to model affective influence between speakers. These methods successfully leveraged dialogue context but involved complex architectures, making them difficult to scale for large or real-time applications.\n\nMore recently, Transformer-based methods have become dominant. MER-HAN [30] uses a three-stage attention framework to model intra-modal, cross-modal, and global features. While this improves interpretability, it increases model complexity and may suffer from misalignment across modalities. To improve efficiency, models like SDT  [5]  apply parallel intra-and cross-modal attention combined with gated fusion and self-distillation. XMBT  [31]  introduces shared bottleneck tokens for cross-modal interaction, which helps reduce inference cost and allows flexible modality integration.\n\nDespite performance gains, these methods still face key limitations. Many use deep sequential Transformers that are computationally expensive. Others lack dynamic weighting, treating all modalities equally-even when one modality carries more emotional information than others. To address this, LPGNet introduces a Lightweight Parallel Interaction Attention (LPIA) module to replace stacked layers and reduce computation. It also includes a dual-gated fusion mechanism to dynamically filter and combine multimodal features based on signal strength and context.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Speaker Dependency And Real-World Limitations",
      "text": "Several advanced MER models, including COSMIC  [32]  and DialogXL  [33] , incorporate speaker identity information or model speaker-role interactions through GNNs. While this improves performance in speaker-labeled datasets, it reduces flexibility in real-world conditions where speaker labels are unavailable or privacy constraints exist. For instance, voice assistants and multi-user devices often encounter new or anonymous speakers. In such cases, models depending on speaker embeddings may fail to generalize. Recent research has explored speaker-independent modeling. SIMR  [34]  attempts to remove identity bias by disentangling style and semantic content in nonverbal modalities. However, most systems still include speaker-level features or require predefined speaker roles, limiting their adaptability in open-domain dialogue.\n\nExisting approaches do not fully eliminate speaker dependence. In contrast, LPGNet is designed to be speaker-independent by default. It removes all speakerrelated inputs and structures, allowing the model to generalize across unknown speakers and dynamically changing roles-an essential requirement for scalable and privacy-aware systems.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lightweight Learning With Self-Distillation",
      "text": "As emotion recognition moves toward real-time and mobile applications, efficiency has become a core concern. Traditional models like MER-HAN and SDT require deep Transformer stacks, which increase latency and memory usage. Knowledge distillation (KD) has been proposed to compress large models while maintaining accuracy. For example, DMD  [35]  uses cross-modal distillation to decompose and transfer shared and unique features. MT-PKDOT  [36]  applies multi-teacher distillation to guide student models using diverse sources. While effective, these strategies often require additional teacher models and increase training complexity. Some models, like SDT, use selfdistillation to reuse internal knowledge, but they still rely on heavy architectures.\n\nFew models combine self-distillation with lightweight structures. LPGNet adopts a teacher-free self-distillation approach within a compact design. It transfers knowledge across internal layers without needing extra networks, improving performance and efficiency. This allows LPGNet to remain accurate while significantly lowering inference cost and parameter size, making it suitable for real-world, resource-constrained deployment.\n\n3 Proposed Method",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task Definition",
      "text": "The input to the ERC task is a conversation composed of N utterances {u 1 , u 2 , . . . , u N }, where each utterance u i contains both textual and acoustic modalities. After feature extraction, the text and audio features are projected to a common latent space:\n\nHere, B is the batch size, U i denotes the number of utterances, and F is the feature dimension. The goal of ERC is to predict an emotion label for each utterance u i from a set of predefined emotion classes. • Lightweight Parallel Interaction Attention (LPIA) module for capturing both intra-and inter-modal interactions simultaneously; • Dual-Gated Fusion module for refining unimodal representations and adaptively fusing multimodal information; • Emotion Classifier that predicts emotion labels from the final fused representation; • Self-Distillation mechanism that aligns unimodal branches with the fused multimodal output using both hard and soft label supervision.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overview",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lightweight Parallel Interaction Attention (Lpia)",
      "text": "To effectively model both intra-and inter-modal dependencies in a lightweight manner, we propose a parallel attention structure named Lightweight Parallel Interaction Attention (LPIA). This module aims to fully leverage the complementary characteristics of acoustic and textual modalities through parallel attention mechanisms. We first apply a 1D convolution with kernel size 1 to both textual and acoustic input sequences. This operation serves two purposes: (1) to project modality-specific features into a unified latent space of dimension d; and (2) to enable localized context modeling across adjacent utterances within each modality:\n\nwhere X s ∈ R B×U ×Fs denotes the original feature sequence for modality s (text or audio), B is the batch size, U is the number of utterances, and F s is the input feature dimension.\n\n(2) Four-Way Parallel Attention Modeling.\n\nTo simultaneously capture intra-and inter-modal relationships, we define four parallel attention blocks based on scaled dot-product attention:\n\nThe four interaction paths are:\n\n• X ta = Attention(X ′ t , X ′ a , X ′ a ): cross-modal text-to-audio interaction. This fully parallel structure avoids sequential stacking, reduces inference latency, and allows efficient modeling of bidirectional modality interactions.\n\n(3) Mask-Aware Attention Computation.\n\nTo ensure robustness in variable-length conversations, we adopt a mask-aware attention mechanism. Let M ∈ {0, 1} B×U denote a binary mask indicating valid utterances. It is expanded to M ′ ∈ {0, 1} B×U ×U for masking attention scores:\n\nThe masked scores are normalized by softmax:\n\nThis formulation ensures that attention is only paid to valid utterances, suppressing the influence of padding noise.\n\n(4) Two-Stage Residual Enhancement.\n\nAfter computing the attention output Â ∈ R B×U ×d , we apply two refinement stages:\n\n• 1. Global Compression: We first compress the global context using a lightweight convolutional block composed of 1×1 convolution, batch normalization, and LeakyReLU activation:\n\n• 2. Position-wise FFN with Residual Fusion: The compressed feature is broadcast back to the sequence dimension and fused with the original query and a position-wise feedforward network:\n\nHere, W 1 ∈ R d×d f f and W 2 ∈ R d f f ×d are learnable weight matrices, where d is the hidden dimension of the model and d f f is the intermediate feedforward dimension (typically 2d or 4d). The GELU function introduces smooth non-linearity, and Dropout is used for regularization.\n\nThis two-stage refinement integrates both global context and local non-linearity, enhancing feature expressiveness and gradient stability.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dual-Gated Fusion Strategy",
      "text": "To enable both fine-grained intra-modal enhancement and flexible cross-modal integration, we introduce a hierarchical Dual-Gated Fusion mechanism, composed of two sequential stages: Unimodal Gated Fusion and Multimodal Gated Fusion.\n\n(1)Unimodal Gated Fusion Given the outputs from the intra-modal and inter-modal attention modules, we apply a learnable gating operation to each stream independently. Let H ∈ R B×U ×d be the hidden representation of an attention output. The refined representation is computed as:\n\nwhere W g ∈ R d×d is a learnable weight matrix and σ(•) denotes the sigmoid activation function. This mechanism allows the model to emphasize informative features while suppressing irrelevant components.\n\nFor each modality, we gate the intra-modal and cross-modal branches separately:\n\nH a = GatedFusion(X output aa ), H ta = GatedFusion(X output ta )\n\nThen, we concatenate and linearly project each modality-specific pair:\n\n(2)Multimodal Gated Fusion\n\nThe refined textual and acoustic features are then passed into a multimodal gating mechanism that adaptively assigns weights to each modality. Given the fused features T fused , A fused ∈ R B×U ×d , the attention weights are computed as:\n\nLet F = [T fused , A fused ] ∈ R B×U ×2×d , the final multimodal representation is computed as:\n\nThis two-stage gating architecture enables the model to dynamically refine unimodal streams and adaptively integrate multimodal context based on conversational cues.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Classifier",
      "text": "To obtain the emotion distribution over C categories, the final multimodal representation F final ∈ R N ×d is passed through a fully connected layer followed by a softmax activation:\n\nHere, W c ∈ R d×C and b c ∈ R C are trainable parameters. Let Ŷ = [ŷ 1 ; ŷ2 ; . . . ; ŷN ], where ŷi denotes the predicted emotion probability vector for utterance u i . The predicted label is determined by arg max(ŷ i ).\n\nThe task loss is defined using the standard cross-entropy objective:",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Self-Distillation",
      "text": "To further improve modal expressiveness, we adopt a self-distillation strategy that transfers knowledge from the full model (teacher) to each unimodal branch (student). Specifically, we treat the complete model as the teacher and introduce two student branches-textual (t) and acoustic (a)-trained only during training.\n\nFor each modality m ∈ {t, a}, the student predicts the emotion distribution via:\n\nwhere τ is the temperature for softening the output distribution. Higher values of τ yield softer class probabilities. W ′ m ∈ R d×C and b ′ m ∈ R C are learnable parameters. Each student is optimized with two losses: The first one is the Cross-Entropy Loss (hard labels) as:\n\nAnd the soft label loss is KL Divergence Loss as:\n\nFinally, the Total Loss could be formulated as: The overall training loss combines task supervision and both hard/soft label distillation:\n\n4 Experiments and Results",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset And Evaluation",
      "text": "We evaluate our model using the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [37]  dataset, a widely adopted benchmark in multimodal emotion recognition. This dataset consists of approximately 12 hours of audio-visual recordings collected from ten professional actors (five male and five female) performing both scripted and improvised dialogues. Each utterance in the dataset is manually annotated with emotional labels across multiple modalities, including text, audio, video, and motion capture data.\n\nIn our study, we focus only on the transcribed text and speech signals, which are the most accessible and commonly used modalities in practical applications. Following standard practice in previous works, we define the emotion recognition in conversation (ERC) task as a four-class classification problem. The target emotion categories include angry, happy, sad, and neutral. The excited class, which shares similar characteristics with happy, is merged into it to simplify the classification task. After preprocessing and filtering, we obtain a total of 5,531 annotated utterances. The detailed emotion distribution across the dataset is presented in Table  1 , providing insight into the class balance and evaluation.\n\nFor evaluation, we adopt average binary accuracy and F1-score as our main performance metrics, following the standard protocol used in prior work. Specifically, we compute results using a one-vs-all classification strategy for each emotion class. The final scores are then averaged across all classes. This approach ensures that performance is measured fairly, especially in the presence of class imbalance, by giving equal weight to each class regardless of its frequency in the dataset.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Feature Extraction",
      "text": "Text Modality: To represent the text modality, we use the BERT-base-uncased model as our text encoder. This model consists of 12 Transformer layers, each with 768-dimensional hidden states. We fine-tune BERT on the emotion recognition in conversation task using the dialogue data. For each utterance, we extract the finallayer embedding of the [CLS] token, which serves as a summary representation of the entire sentence.\n\nAcoustic Modality: For the audio modality, we adopt Emotion2vec, a selfsupervised acoustic representation model trained on 262 hours of emotional speech data. Emotion2vec is designed to learn both utterance-level and frame-level emotional features through a joint optimization process based on knowledge distillation. From this model, we extract 768-dimensional utterance-level embeddings, which provide rich and generalizable representations of the speech signal.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Implementation Details",
      "text": "Our model is implemented in PyTorch 1.8.1 and RTX 3090. We use the Adam optimizer with a learning rate of 3e-4, a batch size of 32, and a hidden dimension of 768. The temperature parameter τ for self-distillation is set to 1. We apply L2 weight decay of 1 × 10 -5 and dropout with a rate of 0.1. Each model is trained for 150 epochs, and we report average results over 10 independent runs for robustness.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "Table  2  presents a detailed comparison between our proposed LPGNet and several state-of-the-art baselines on the IEMOCAP dataset under the 4-class emotion classification setting. Among existing methods, CFIA and MemoCMT show strong performance, with CFIA achieving 83.37% accuracy using the same BERT and Emo-tion2vec features as our model. However, our full model, LPGNet (Utterance-level), achieves the highest performance with 87.99% accuracy and 87.96% F1-score, representing a relative improvement of over 4% compared to CFIA. These results highlight the effectiveness of our lightweight and speaker-independent design. To ensure that this improvement is due to architectural innovation rather than feature quality, we build a minimal baseline using a simple linear classifier applied to the same BERT and Emotion2vec embeddings. This baseline reaches only 81.68% accuracy, which is 6.31% lower than our full model. This gap confirms that LPGNet's performance gains are not solely due to better features, but come from its efficient multimodal fusion and context modeling capabilities.\n\nWe further evaluate a frame-level version of our model, called LPGNet (Frame), where both text tokens and acoustic frames are padded or truncated to a fixed length of 512. Despite using finer temporal resolution, LPGNet (Frame) still achieves strong results with 83.87% accuracy and 83.68% F1-score, outperforming most baselines. This demonstrates that our model can generalize across different input granularities without losing effectiveness. In summary, LPGNet consistently outperforms existing methods across both utterance-and frame-level settings. These results confirm its robustness, efficiency, and practical potential for real-world multimodal emotion recognition tasks.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study",
      "text": "Effectiveness of LPIA Components and Modal Interactions. To evaluate the role of each component within the LPIA module, we perform a series of ablation studies, as summarized in Table  3 .\n\nWe first examine the impact of removing the intra-modal and inter-modal attention blocks. The results show that eliminating either block leads to a clear performance drop. Specifically, removing inter-modal attention causes a 1.77% decrease in accuracy, indicating that modeling cross-modal emotional dependencies is crucial for effective fusion. On the other hand, removing intra-modal attention results in a 0.96% drop, showing that capturing temporal and contextual patterns within each modality also contributes meaningfully to performance. Together, these findings suggest that the two attention types play complementary roles-with intra-modal attention focusing on modality-specific context, and inter-modal attention capturing relationships across modalities.\n\nIn addition, we analyze the effect of removing the position-wise feedforward network (FFN), which follows the attention blocks. Although FFN does not directly model alignment, it applies non-linear transformations that refine and enrich the learned representations. Removing this component leads to a 1.12% drop in F1-score, confirming that the FFN is essential for final feature integration and improved discriminative power.\n\nThese results collectively validate the design of the LPIA module, showing that each sub-component contributes to the model's ability to understand and fuse multimodal emotional cues effectively.\n\nEffect of Dual-Gated Fusion. To further evaluate the effectiveness of our proposed fusion strategy, we conduct an ablation study by removing the Dual-Gated Fusion module from the model. As reported in Table  3 , this modification leads to a performance drop in F1-score from 87.96% to 86.59%. This confirms the positive impact of our gating mechanism on multimodal fusion.\n\nThe Dual-Gated Fusion module contains two key components: a unimodal-level gate and a multimodal-level gate. The unimodal gate filters out irrelevant or noisy features within each modality before fusion, helping the model focus on emotionally relevant content. The multimodal gate then adaptively assigns importance to each modality during the fusion process, allowing the model to balance their contributions based on context.\n\nWhen this module is removed, the model loses its ability to regulate feature contributions both before and after fusion. This leads to less informative and more noisy multimodal representations. In contrast, the full LPGNet design benefits from dynamic feature calibration at both levels, which is especially important when modalities vary in quality or signal strength. These results demonstrate that combining attention-based interaction modeling (through LPIA) with gated control mechanisms leads to stronger and more robust emotional understanding. The two-stage gating design plays a crucial role in enhancing representation quality, particularly under real-world conditions involving modality imbalance or noise.\n\nEffectiveness of Feature Representation and Multimodal Fusion. To assess the quality of unimodal feature representations in LPGNet, we compare its performance with existing unimodal baselines. As shown in Table  4  and Table  5 , our model outperforms other text-only and audio-only approaches when using the same feature extractors-Emotion2vec for audio and BERT for text. These results confirm that our chosen encoders are effective, and that LPGNet is able to fully leverage them through its attention-based and fusion-oriented architecture. Importantly, LPGNet maintains strong performance even in unimodal settings. This demonstrates that its architecture can enhance emotion-relevant signals and suppress irrelevant noise, even when limited to a single modality. Compared with simpler baselines such as linear classifiers or prior multimodal models adapted for unimodal input, LPGNet achieves noticeable accuracy gains. This suggests that the model's structural advantages go beyond feature quality and contribute directly to its discriminative power.\n\nFurthermore, Table  3  illustrates the added value of integrating multiple modalities. Multimodal fusion significantly improves performance over each unimodal variant, indicating strong cross-modal complementarity. Acoustic and textual modalities contribute different but synergistic cues-such as prosody and semantics-which, when fused, offer a more holistic emotional representation. These findings highlight the necessity of both effective unimodal encoders and carefully designed multimodal integration mechanisms for robust emotion recognition in conversation.\n\nEffect of Self-Distillation Coefficients. To assess the quality of unimodal feature representations in LPGNet, we compare its performance with existing unimodal baselines. As shown in Table  4  and Table  5 , our model consistently outperforms other text-only and audio-only approaches when using the same feature extractor, Emo-tion2vec for audio and BERT for text. These results confirm that our chosen encoders are effective and that LPGNet can leverage them through its attention-based and fusion-oriented architecture. Meanwhile, we observe that a lower KL-divergence weight λ KL = 0.3 yields better generalization. Since LPGNet removes speaker embeddings and focuses on speakerindependent modeling, the soft label distributions may exhibit greater inter-sample variability. Down-weighting KL divergence mitigates the risk of overfitting to potentially noisy or over-smoothed soft labels. On the other hand, keeping λ CE = 1.0 ensures that unimodal branches receive sufficient intermediate supervision during distillation.",
      "page_start": 12,
      "page_end": 14
    },
    {
      "section_name": "Declarations",
      "text": "• Funding: This research received no external funding.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides an overview of the proposed LPGNet model. After extracting",
      "page": 5
    },
    {
      "caption": "Figure 1: Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LPGNet: A Lightweight Network with Parallel": "Attention and Gated Fusion for Multimodal"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "Emotion Recognition"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "Zhining He1 and Yang Xiao2*"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "1Guangzhou University, Guangzhou, China."
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "2*The Unviersity of Melbourne, Melbourne, Australia."
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "*Corresponding author(s). E-mail(s):"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "yxiao9550@student.unimelb.edu.au;"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "Contributing authors: hezhining0624@163.com;"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "Abstract"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "Emotion recognition in conversations (ERC) aims to predict the emotional state"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "of each utterance by using multiple input types, such as text and audio. While"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "Transformer-based models have shown strong performance in this task, they often"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "face two major issues: high computational cost and heavy dependence on speaker"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "information. These problems reduce their ability to generalize in real-world con-"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "versations. To solve these challenges, we propose LPGNet, a Lightweight network"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "with Parallel attention and Gated fusion for multimodal ERC. The main part of"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "LPGNet is the Lightweight Parallel Interaction Attention (LPIA) module. This"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "module replaces traditional stacked Transformer layers with parallel dot-product"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "attention, which can model both within-modality and between-modality relation-"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "ships more efficiently. To improve emotional"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "a dual-gated fusion method. This method filters and combines features from dif-"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "ferent input types in a flexible and dynamic way. In addition, LPGNet removes"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "speaker embeddings completely, which allows the model to work independently"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "of speaker identity. Experiments on the IEMOCAP dataset show that LPGNet"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "reaches over 87% accuracy and F1-score in 4-class emotion classification. It out-"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "performs strong baseline models while using fewer parameters and showing better"
        },
        {
          "LPGNet: A Lightweight Network with Parallel": "generalization across speakers."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 Introduction": "Emotion recognition in conversations (ERC) [1, 2] focuses on identifying the emotional"
        },
        {
          "1 Introduction": "state of each utterance by using multiple types of input, such as text, audio, and visual"
        },
        {
          "1 Introduction": "signals. It plays an important role in human-computer interaction (HCI) [3, 4], espe-"
        },
        {
          "1 Introduction": "cially in building systems that can understand and respond to users’ emotions. ERC"
        },
        {
          "1 Introduction": "is a key part of many real-world applications,\nincluding empathetic dialogue systems,"
        },
        {
          "1 Introduction": "emotion-aware virtual assistants, and mental health support tools. These systems are"
        },
        {
          "1 Introduction": "widely used in areas\nlike\nintelligent voice assistants,\ncustomer\nservice\nrobots, and"
        },
        {
          "1 Introduction": "telemedicine. Compared to traditional emotion recognition based on a single input"
        },
        {
          "1 Introduction": "type, ERC brings more challenges. This is because it requires understanding context"
        },
        {
          "1 Introduction": "over time, handling frequent speaker changes, and processing emotional signals across"
        },
        {
          "1 Introduction": "different modalities. These challenges make ERC a complex but\nimportant problem"
        },
        {
          "1 Introduction": "in affective computing."
        },
        {
          "1 Introduction": "Recent ERC models based on Transformer [5] and graph neural network (GNN) [6–"
        },
        {
          "1 Introduction": "8] architectures have\nshown strong performance. However,\nthey often face\ntwo key"
        },
        {
          "1 Introduction": "challenges. First, deep sequential\nlayers lead to high computational cost, which limits"
        },
        {
          "1 Introduction": "deployment\nin real-time\nsystems. Second, many models depend heavily on speaker"
        },
        {
          "1 Introduction": "identity features, making them less effective in open-domain or anonymous settings,"
        },
        {
          "1 Introduction": "where speaker information is missing or unavailable. These limitations highlight the"
        },
        {
          "1 Introduction": "need for efficient and speaker-independent ERC models."
        },
        {
          "1 Introduction": "To improve multimodal\nfusion in ERC,\nseveral attention-based approaches have"
        },
        {
          "1 Introduction": "been developed. These methods aim to better integrate features from different input"
        },
        {
          "1 Introduction": "types, such as text and audio. For example, Tang et al.\n[9]\nintroduced an audio-text"
        },
        {
          "1 Introduction": "interactive attention mechanism that\nstrengthens\ncross-modal understanding. Zhao"
        },
        {
          "1 Introduction": "et\nal.\n[10] proposed collaborative\nattention to\nalign emotional\ninformation across"
        },
        {
          "1 Introduction": "pre-trained features. Maji\net al.\n[11] designed a cross-modal Transformer block to"
        },
        {
          "1 Introduction": "capture both semantic and temporal dependencies across modalities. Although these"
        },
        {
          "1 Introduction": "techniques have demonstrated good performance, they often come with a high cost."
        },
        {
          "1 Introduction": "Their\narchitectures\nare usually deep and complex, which increases both training"
        },
        {
          "1 Introduction": "and inference\ntime. As a result,\nthese models are not well-suited for\nreal-time or"
        },
        {
          "1 Introduction": "resource-constrained environments, such as mobile devices."
        },
        {
          "1 Introduction": "In addition to attention-based models, graph neural networks\n(GNNs) have also"
        },
        {
          "1 Introduction": "been applied to ERC. Methods\nsuch as DialogueGCN [6] and MMGCN [12] aim to"
        },
        {
          "1 Introduction": "capture speaker interactions by constructing dialogue graphs. These models can rep-"
        },
        {
          "1 Introduction": "resent complex conversational\nstructures but\nrely heavily on speaker\nidentity labels"
        },
        {
          "1 Introduction": "and manually crafted graphs. This dependency reduces their flexibility and limits their"
        },
        {
          "1 Introduction": "usefulness\nin open-domain applications, where such information may be unavailable"
        },
        {
          "1 Introduction": "or inconsistent."
        },
        {
          "1 Introduction": "These limitations become especially important when considering real-world ERC"
        },
        {
          "1 Introduction": "applications.\nIn practical\nsettings,\nsystems must often deal with unknown, anony-"
        },
        {
          "1 Introduction": "mous, or changing speakers [13–15]. They also need to perform emotion recognition"
        },
        {
          "1 Introduction": "in a zero-shot manner, without prior training on speaker-specific data. For instance,"
        },
        {
          "1 Introduction": "voice assistants must understand a user’s emotions during their first interaction, with-"
        },
        {
          "1 Introduction": "out\nrelying on stored speaker profiles\n[16]. Similarly, customer\nservice bots\ninteract"
        },
        {
          "1 Introduction": "with a wide range of users, many of whom are anonymous or change frequently.\nIn"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "home terminals, require models to handle frequent speaker switching without identity"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "tracking."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "Recent ERC methods based on large language models (LLMs) [17–19] also reveal"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "inconsistencies in how speaker information is handled.\nInstructERC [20] treats ERC"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "as a generation task using emotion templates, but it does not explicitly model speaker"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "roles. DialogueLLM [21] uses dialogue context and visual cues but omits speaker iden-"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "tity. BiosERC [22] goes further by showing that privacy constraints often prevent the"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "use of\nidentity labels, meaning that models must\nrely only on the dialogue context."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "These findings underline the importance of speaker-independent emotion modeling for"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "real-world applications."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "Alongside\nattention\nand\ngraph-based\ntechniques,\nself-distillation\n[23–25]\nhas"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "emerged as a promising solution for improving model performance without increasing"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "complexity. Unlike traditional distillation methods\nthat\nrely on an external\nteacher"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "model, self-distillation enables a network to learn from its own intermediate features."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "For example, Li et al. [26] introduced a teacher-free feature distillation method in vision"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "tasks, where a model reuses useful\ninternal representations to boost its own learning."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "This strategy reduces both memory and computation costs, making it well-suited for"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "lightweight deployment."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "Based\non\nthese\ninsights,\nwe\npropose\nLPGNet,\na\nlightweight\nand\nspeaker-"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "independent framework for multimodal emotion recognition in conversations. LPGNet"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "is designed to address\nthree major\nissues: high inference\ncost,\nreliance on speaker"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "information, and the need for efficient deployment in practical scenarios."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "Our key contributions are as follows:"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "• We design a Lightweight Parallel\nInteraction Attention (LPIA) module to replace"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "stacked Transformer\nlayers, allowing efficient modeling of\nintra- and inter-modal"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "relationships."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "• We\nintroduce\na\ndual-gated\nfusion\nstrategy\nto\nrefine\nand\ndynamically\ncombine"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "multimodal\nfeatures."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "• We remove speaker embeddings entirely, enabling better generalization to unseen"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "speakers and anonymous environments."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "• We\napply\na\nself-distillation mechanism to\ninternally\nextract\nand\nreuse\nuseful"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "emotional knowledge without requiring external teacher models."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "Extensive experiments on the IEMOCAP benchmark show that LPGNet achieves"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "over 87% accuracy and F1-score for 4-class emotion classification.\nIt not only out-"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "performs\nstrong baseline models but also shows better generalization to unknown"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "speakers, all while using fewer parameters."
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "2 Related Work"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "2.1 Multimodal Fusion Strategies in Emotion Recognition"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "Multimodal emotion recognition (MER) [27] aims to integrate signals from different"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "modalities\nsuch as\ntext,\nspeech, and vision. Early models\nrelied on recurrent neural"
        },
        {
          "such cases, speaker embeddings offer little benefit. Multi-user devices, such as smart": "networks and memory-based fusion techniques. For example, CMN [28] used GRUs to"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "past utterances. ICON [29] extended this by introducing interactive memory modules"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "to model affective influence between speakers. These methods successfully leveraged"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "dialogue context but\ninvolved complex architectures, making them difficult\nto scale"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "for large or real-time applications."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "More recently, Transformer-based methods have become dominant. MER-HAN [30]"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "uses a three-stage attention framework to model\nintra-modal, cross-modal, and global"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "features. While\nthis\nimproves\ninterpretability,\nit\nincreases model\ncomplexity\nand"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "may suffer\nfrom misalignment across modalities. To improve\nefficiency, models\nlike"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "SDT [5] apply parallel\nintra- and cross-modal attention combined with gated fusion"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "and self-distillation. XMBT [31]\nintroduces shared bottleneck tokens for cross-modal"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "interaction, which helps reduce inference cost and allows flexible modality integration."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "Despite performance gains, these methods still face key limitations. Many use deep"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "sequential Transformers\nthat\nare\ncomputationally expensive. Others\nlack dynamic"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "weighting, treating all modalities equally—even when one modality carries more emo-"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "tional\ninformation than others. To address\nthis, LPGNet\nintroduces a Lightweight"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "Parallel\nInteraction Attention (LPIA) module to replace stacked layers and reduce"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "computation. It also includes a dual-gated fusion mechanism to dynamically filter and"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "combine multimodal\nfeatures based on signal strength and context."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "2.2 Speaker Dependency and Real-World Limitations"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "Several advanced MER models,\nincluding COSMIC [32] and DialogXL [33],\nincorpo-"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "rate speaker\nidentity information or model speaker-role interactions through GNNs."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "While this improves performance in speaker-labeled datasets,\nit reduces flexibility in"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "real-world conditions where speaker labels are unavailable or privacy constraints exist."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "For\ninstance, voice assistants and multi-user devices often encounter new or anony-"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "mous speakers.\nIn such cases, models depending on speaker embeddings may fail\nto"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "generalize. Recent\nresearch has\nexplored speaker-independent modeling. SIMR [34]"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "attempts to remove identity bias by disentangling style and semantic content in non-"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "verbal modalities. However, most systems still include speaker-level features or require"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "predefined speaker roles,\nlimiting their adaptability in open-domain dialogue."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "Existing\napproaches\ndo\nnot\nfully\neliminate\nspeaker\ndependence.\nIn\ncontrast,"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "LPGNet\nis designed to be\nspeaker-independent by default.\nIt\nremoves all\nspeaker-"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "related\ninputs\nand\nstructures,\nallowing\nthe model\nto\ngeneralize\nacross\nunknown"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "speakers and dynamically changing roles—an essential\nrequirement\nfor\nscalable and"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "privacy-aware systems."
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "2.3 Lightweight Learning with Self-Distillation"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "As\nemotion recognition moves\ntoward real-time and mobile applications,\nefficiency"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "has become a core concern. Traditional models like MER-HAN and SDT require deep"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "Transformer stacks, which increase latency and memory usage. Knowledge distillation"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "(KD) has been proposed to compress\nlarge models while maintaining accuracy. For"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "example, DMD [35] uses cross-modal distillation to decompose and transfer shared and"
        },
        {
          "model speaker history and applied attention to extract relevant emotion patterns from": "unique features. MT-PKDOT [36] applies multi-teacher distillation to guide student"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "models using diverse sources. While effective, these strategies often require additional": "teacher models and increase\ntraining complexity. Some models,\nlike SDT, use\nself-"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "distillation to reuse internal knowledge, but they still rely on heavy architectures."
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "Few models combine self-distillation with lightweight structures. LPGNet adopts a"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "teacher-free self-distillation approach within a compact design. It transfers knowledge"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "across\ninternal\nlayers without needing extra networks,\nimproving performance and"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "efficiency. This allows LPGNet to remain accurate while significantly lowering infer-"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "ence cost and parameter size, making it suitable for real-world, resource-constrained"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "deployment."
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "3 Proposed Method"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "3.1 Task Definition"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "The\ninput\nto\nthe\nERC\ntask\nis\na\nconversation\ncomposed\nof\nutterances\nN"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "{u1, u2, . . . , uN }, where each utterance ui contains both textual and acoustic modali-"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "ties. After feature extraction, the text and audio features are projected to a common"
        },
        {
          "models using diverse sources. While effective, these strategies often require additional": "latent space:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "four key components: LPIA Block, Dual Gated Fusion, Classifier, and Self-distillation"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": ""
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "We first apply a 1D convolution with kernel size 1 to both textual and acoustic input"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "sequences. This operation serves two purposes: (1) to project modality-specific features"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "into a unified latent space of dimension d; and (2) to enable localized context modeling"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": ""
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "X′\ns ∈ {t, a}\ns = Conv1D(Xs),"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "the original"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": "audio), B is the batch size, U is the number of utterances, and Fs is the input feature"
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": ""
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": ""
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": ""
        },
        {
          "Fig. 1 Overall architecture of LPGNet. After extracting utterance-level unimodal features, it consists": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Xta = Attention(X′\nt, X′\na, X′\na): cross-modal text-to-audio interaction.": "This fully parallel structure avoids sequential stacking, reduces inference latency, and"
        },
        {
          "• Xta = Attention(X′\nt, X′\na, X′\na): cross-modal text-to-audio interaction.": "allows efficient modeling of bidirectional modality interactions."
        },
        {
          "• Xta = Attention(X′\nt, X′\na, X′\na): cross-modal text-to-audio interaction.": "(3) Mask-Aware Attention Computation."
        },
        {
          "• Xta = Attention(X′\nt, X′\na, X′\na): cross-modal text-to-audio interaction.": "To ensure robustness in variable-length conversations, we adopt a mask-aware atten-"
        },
        {
          "• Xta = Attention(X′\nt, X′\na, X′\na): cross-modal text-to-audio interaction.": "tion mechanism. Let M ∈ {0, 1}B×U denote a binary mask indicating valid utterances."
        },
        {
          "• Xta = Attention(X′\nt, X′\na, X′\na): cross-modal text-to-audio interaction.": "It is expanded to M′ ∈ {0, 1}B×U ×U for masking attention scores:"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.4 Dual-Gated Fusion Strategy": "To enable both fine-grained intra-modal enhancement and flexible cross-modal\ninte-"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "gration, we introduce a hierarchical Dual-Gated Fusion mechanism, composed of"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "two sequential stages: Unimodal Gated Fusion and Multimodal Gated Fusion."
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "(1)Unimodal Gated Fusion"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "Given the outputs from the intra-modal and inter-modal attention modules, we apply"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "a learnable gating operation to each stream independently. Let H ∈ RB×U ×d be the"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "hidden representation of an attention output. The refined representation is computed"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "as:"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "(9)\nZ = σ(WgH),\nHgated = Z ⊙ H"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "where Wg ∈ Rd×d is a learnable weight matrix and σ(·) denotes the sigmoid activation"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "function. This mechanism allows the model\nto emphasize informative features while"
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "suppressing irrelevant components."
        },
        {
          "3.4 Dual-Gated Fusion Strategy": "For each modality, we gate the intra-modal and cross-modal branches separately:"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ˆ": "(16)\nY = softmax(E)"
        },
        {
          "ˆ": "Here, Wc ∈ Rd×C and bc ∈ RC are trainable parameters. Let ˆY = [ˆy1; ˆy2; . . . ; ˆyN ],"
        },
        {
          "ˆ": "where\ndenotes\nthe predicted emotion probability\nvector\nyi\nfor utterance ui. The"
        },
        {
          "ˆ": "predicted label\nis determined by arg max(ˆyi)."
        },
        {
          "ˆ": "The task loss is defined using the standard cross-entropy objective:"
        },
        {
          "ˆ": "1 N\nN(cid:88) i\nC(cid:88) j\n(17)\nLtask = −\nyi,j log ˆyi,j"
        },
        {
          "ˆ": "=1\n=1"
        },
        {
          "ˆ": "3.6 Self-Distillation"
        },
        {
          "ˆ": "To further\nimprove modal\nexpressiveness, we adopt a self-distillation strategy that"
        },
        {
          "ˆ": "transfers knowledge from the full model (teacher) to each unimodal branch (student)."
        },
        {
          "ˆ": "Specifically, we treat\nthe complete model as\nthe teacher and introduce two student"
        },
        {
          "ˆ": "branches—textual (t) and acoustic (a)—trained only during training."
        },
        {
          "ˆ": "For each modality m ∈ {t, a}, the student predicts the emotion distribution via:"
        },
        {
          "ˆ": "(18)\nEm = W′\nm · ReLU(H′\nm) + b′\nm ∈ RN ×C"
        },
        {
          "ˆ": "(19)\nYm = softmax(Em),\nYm\nτ = softmax(Em/τ )"
        },
        {
          "ˆ": "is the temperature for softening the output distribution. Higher values of\nwhere τ"
        },
        {
          "ˆ": "τ yield softer class probabilities. W′\nm ∈ Rd×C and b′\nm ∈ RC are learnable parameters."
        },
        {
          "ˆ": "Each student is optimized with two losses: The first one is the Cross-Entropy Loss"
        },
        {
          "ˆ": "(hard labels) as:"
        },
        {
          "ˆ": "1 N\nN(cid:88) i\nC(cid:88) j\n(20)\nLm\nyi,j log ˆym\nCE = −\ni,j"
        },
        {
          "ˆ": "=1\n=1"
        },
        {
          "ˆ": "And the soft label\nloss is KL Divergence Loss as:"
        },
        {
          "ˆ": "ym"
        },
        {
          "ˆ": "τ,i,j"
        },
        {
          "ˆ": "1 N\nN(cid:88) i\nC(cid:88) j\n(21)\nLm\nym\nKL =\nτ,i,j log"
        },
        {
          "ˆ": "yteacher"
        },
        {
          "ˆ": "τ,i,j\n=1\n=1"
        },
        {
          "ˆ": "Finally, the Total Loss could be formulated as: The overall training loss combines"
        },
        {
          "ˆ": "task supervision and both hard/soft label distillation:"
        },
        {
          "ˆ": "(cid:88) m\n(cid:88) m\n(22)\nLm\nLm\nL = λtask · Ltask + λCE ·\nCE + λKL ·\nKL"
        },
        {
          "ˆ": "4 Experiments and Results"
        },
        {
          "ˆ": "4.1 Dataset and Evaluation"
        },
        {
          "ˆ": "We\nevaluate\nour model using\nthe\nInteractive Emotional Dyadic Motion Capture"
        },
        {
          "ˆ": "(IEMOCAP) [37] dataset, a widely adopted benchmark in multimodal emotion recog-"
        },
        {
          "ˆ": "nition. This dataset\nconsists of approximately 12 hours of audio-visual\nrecordings"
        },
        {
          "ˆ": "collected from ten professional actors\n(five male and five\nfemale) performing both"
        },
        {
          "ˆ": "9"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: EmotiondistributionintheIEMOCAPdataset.",
      "data": [
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "scripted and improvised dialogues. Each utterance in the dataset\nis manually anno-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "tated with emotional\nlabels across multiple modalities,\nincluding text, audio, video,"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "and motion capture data."
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "In our\nstudy, we\nfocus only on the\ntranscribed text and speech signals, which"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "are the most accessible and commonly used modalities in practical applications. Fol-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "lowing\nstandard practice\nin previous works, we define\nthe\nemotion recognition in"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "conversation (ERC)\ntask as a four-class classification problem. The target emotion"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "categories include angry, happy, sad, and neutral. The excited class, which shares sim-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "ilar characteristics with happy,\nis merged into it\nto simplify the classification task."
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "After preprocessing and filtering, we obtain a total of 5,531 annotated utterances. The"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "detailed emotion distribution across\nthe dataset\nis presented in Table 1, providing"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "insight into the class balance and evaluation."
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "For evaluation, we adopt average binary accuracy and F1-score as our main per-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "formance metrics,\nfollowing the standard protocol used in prior work. Specifically, we"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "compute results using a one-vs-all classification strategy for each emotion class. The"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "final\nscores are then averaged across all classes. This approach ensures\nthat perfor-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "mance is measured fairly, especially in the presence of class imbalance, by giving equal"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "weight to each class regardless of\nits frequency in the dataset."
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "4.2 Feature Extraction"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "the\ntext modality, we use\nthe BERT-base-uncased\nText Modality: To represent"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "model as our\ntext encoder. This model consists of 12 Transformer\nlayers, each with"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "768-dimensional hidden states. We fine-tune BERT on the\nemotion recognition in"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "conversation task using the dialogue data. For each utterance, we extract\nthe final-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "layer embedding of the [CLS] token, which serves as a summary representation of the"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "entire sentence."
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "the\naudio modality, we\nadopt Emotion2vec,\na\nself-\nAcoustic Modality: For"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "supervised acoustic representation model\ntrained on 262 hours of emotional\nspeech"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "data. Emotion2vec is designed to learn both utterance-level and frame-level emotional"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "features through a joint optimization process based on knowledge distillation. From"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "this model, we extract 768-dimensional utterance-level embeddings, which provide rich"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "and generalizable representations of the speech signal."
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "4.3 Implementation Details"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "Our model\nis implemented in PyTorch 1.8.1 and RTX 3090. We use the Adam opti-"
        },
        {
          "Total\n1704\n1073\n1703\n1042\n5531": "mizer with a learning rate of 3e-4, a batch size of 32, and a hidden dimension of 768."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: OverallperformancecomparisononIEMOCAP(4-class).",
      "data": [
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "Model"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "GatedxLSTM [38]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "MER-HAN [30]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "MMI-MMER [39]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "MGCMA [40]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "MEP [41]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "Bi-GRU [42]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "MemoCMT[43]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "Linear[44]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "CFIA[45]"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "LPGNet(Frame)"
        },
        {
          "Table 2 Overall performance comparison on IEMOCAP (4-class).": "LPGNet(Utterance)"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: OverallperformancecomparisononIEMOCAP(4-class).",
      "data": [
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "LPGNet(Utterance)\nemotion2vec\nBERT",
          "83.87\n83.68": "87.99\n87.96"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "The temperature parameter τ for self-distillation is set to 1. We apply L2 weight decay",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "of 1 × 10−5 and dropout with a rate of 0.1. Each model",
          "83.87\n83.68": "is trained for 150 epochs, and"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "we report average results over 10 independent runs for robustness.",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "4.4 Results",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "Table\n2 presents\na detailed comparison between our proposed LPGNet",
          "83.87\n83.68": "and sev-"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "eral\nstate-of-the-art baselines on the IEMOCAP dataset under",
          "83.87\n83.68": "the 4-class emotion"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "classification setting. Among existing methods, CFIA and MemoCMT show strong",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "performance, with CFIA achieving 83.37% accuracy using the same BERT and Emo-",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "tion2vec features as our model. However, our full model, LPGNet (Utterance-level),",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "achieves the highest performance with 87.99% accuracy and 87.96% F1-score, repre-",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "senting a relative improvement of over 4% compared to CFIA. These results highlight",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "the effectiveness of our lightweight and speaker-independent design.",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "To ensure that\nthis\nimprovement\nis due to architectural",
          "83.87\n83.68": "innovation rather\nthan"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "feature quality, we build a minimal baseline using a simple linear classifier applied",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "to the same BERT and Emotion2vec embeddings. This baseline reaches only 81.68%",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "accuracy, which is 6.31% lower than our full model. This gap confirms that LPGNet’s",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "performance gains are not\nsolely due to better",
          "83.87\n83.68": "features, but come from its efficient"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "multimodal\nfusion and context modeling capabilities.",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "",
          "83.87\n83.68": "We further evaluate a frame-level version of our model, called LPGNet (Frame),"
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "where both text tokens and acoustic frames are padded or truncated to a fixed length",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "of 512. Despite using finer temporal resolution, LPGNet (Frame) still achieves strong",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "results with 83.87% accuracy and 83.68% F1-score, outperforming most baselines. This",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "demonstrates that our model can generalize across different input granularities without",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "losing effectiveness. In summary, LPGNet consistently outperforms existing methods",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "across both utterance- and frame-level settings. These results confirm its robustness,",
          "83.87\n83.68": ""
        },
        {
          "LPGNet(Frame)\nemotion2vec\nBERT": "efficiency, and practical potential for real-world multimodal emotion recognition tasks.",
          "83.87\n83.68": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Wefirstexaminetheimpactofremovingtheintra-modalandinter-modalattention",
      "data": [
        {
          "4.5 Ablation Study": "Effectiveness of LPIA Components and Modal Interactions. To evaluate the"
        },
        {
          "4.5 Ablation Study": "role of\neach component within the LPIA module, we perform a series of ablation"
        },
        {
          "4.5 Ablation Study": "studies, as summarized in Table 3."
        },
        {
          "4.5 Ablation Study": "We first examine the impact of removing the intra-modal and inter-modal attention"
        },
        {
          "4.5 Ablation Study": "blocks. The results\nshow that eliminating either block leads\nto a clear performance"
        },
        {
          "4.5 Ablation Study": "drop. Specifically, removing inter-modal attention causes a 1.77% decrease in accuracy,"
        },
        {
          "4.5 Ablation Study": "indicating that modeling cross-modal emotional dependencies\nis crucial\nfor effective"
        },
        {
          "4.5 Ablation Study": "fusion. On the other hand,\nremoving intra-modal attention results in a 0.96% drop,"
        },
        {
          "4.5 Ablation Study": "showing that capturing temporal and contextual patterns within each modality also"
        },
        {
          "4.5 Ablation Study": "contributes meaningfully to performance. Together,\nthese findings\nsuggest\nthat\nthe"
        },
        {
          "4.5 Ablation Study": "two attention types play complementary roles—with intra-modal attention focusing"
        },
        {
          "4.5 Ablation Study": "on modality-specific context, and inter-modal attention capturing relationships across"
        },
        {
          "4.5 Ablation Study": "modalities."
        },
        {
          "4.5 Ablation Study": "In addition, we analyze the effect of removing the position-wise feedforward net-"
        },
        {
          "4.5 Ablation Study": "work (FFN), which follows the attention blocks. Although FFN does not directly model"
        },
        {
          "4.5 Ablation Study": "alignment, it applies non-linear transformations that refine and enrich the learned rep-"
        },
        {
          "4.5 Ablation Study": "resentations. Removing this component leads to a 1.12% drop in F1-score, confirming"
        },
        {
          "4.5 Ablation Study": "that\nthe FFN is\nessential\nfor final\nfeature\nintegration and improved discriminative"
        },
        {
          "4.5 Ablation Study": "power."
        },
        {
          "4.5 Ablation Study": "These results collectively validate the design of\nthe LPIA module,\nshowing that"
        },
        {
          "4.5 Ablation Study": "each\nsub-component\ncontributes\nto\nthe model’s\nability\nto\nunderstand\nand\nfuse"
        },
        {
          "4.5 Ablation Study": "multimodal emotional cues effectively."
        },
        {
          "4.5 Ablation Study": "Effect of Dual-Gated Fusion. To further evaluate the effectiveness of our pro-"
        },
        {
          "4.5 Ablation Study": "posed fusion strategy, we\nconduct an ablation study by removing the Dual-Gated"
        },
        {
          "4.5 Ablation Study": "Fusion module\nfrom the model. As\nreported in Table 3,\nthis modification leads\nto"
        },
        {
          "4.5 Ablation Study": "a performance drop in F1-score from 87.96% to 86.59%. This confirms\nthe positive"
        },
        {
          "4.5 Ablation Study": "impact of our gating mechanism on multimodal\nfusion."
        },
        {
          "4.5 Ablation Study": "The Dual-Gated Fusion module contains\ntwo key components: a unimodal-level"
        },
        {
          "4.5 Ablation Study": "gate and a multimodal-level gate. The unimodal gate filters out\nirrelevant or noisy"
        },
        {
          "4.5 Ablation Study": "features within each modality before fusion, helping the model\nfocus on emotionally"
        },
        {
          "4.5 Ablation Study": "relevant\ncontent. The multimodal gate\nthen adaptively assigns\nimportance\nto each"
        },
        {
          "4.5 Ablation Study": "modality during the fusion process, allowing the model to balance their contributions"
        },
        {
          "4.5 Ablation Study": "based on context."
        },
        {
          "4.5 Ablation Study": "When this module is removed, the model\nloses its ability to regulate feature con-"
        },
        {
          "4.5 Ablation Study": "tributions both before and after fusion. This leads to less informative and more noisy"
        },
        {
          "4.5 Ablation Study": "multimodal representations. In contrast, the full LPGNet design benefits from dynamic"
        },
        {
          "4.5 Ablation Study": "feature calibration at both levels, which is especially important when modalities vary in"
        },
        {
          "4.5 Ablation Study": "quality or signal strength. These results demonstrate that combining attention-based"
        },
        {
          "4.5 Ablation Study": "interaction modeling (through LPIA) with gated control mechanisms leads to stronger"
        },
        {
          "4.5 Ablation Study": "and more robust emotional understanding. The two-stage gating design plays a cru-"
        },
        {
          "4.5 Ablation Study": "cial role in enhancing representation quality, particularly under real-world conditions"
        },
        {
          "4.5 Ablation Study": "involving modality imbalance or noise."
        },
        {
          "4.5 Ablation Study": "Effectiveness\nof Feature Representation and Multimodal Fusion. To"
        },
        {
          "4.5 Ablation Study": "assess\nthe quality of unimodal\nfeature\nrepresentations\nin LPGNet, we\ncompare\nits"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 3: AblationresultsofLPGNet.",
      "data": [
        {
          "Table 3 Ablation results of LPGNet.": ""
        },
        {
          "Table 3 Ablation results of LPGNet.": "LPGNet"
        },
        {
          "Table 3 Ablation results of LPGNet.": "Modality"
        },
        {
          "Table 3 Ablation results of LPGNet.": "Text"
        },
        {
          "Table 3 Ablation results of LPGNet.": "Audio"
        },
        {
          "Table 3 Ablation results of LPGNet.": "LPIA Block"
        },
        {
          "Table 3 Ablation results of LPGNet.": "w/o inter attention"
        },
        {
          "Table 3 Ablation results of LPGNet.": "w/o intra attention"
        },
        {
          "Table 3 Ablation results of LPGNet.": "w/o position-wise FFN"
        },
        {
          "Table 3 Ablation results of LPGNet.": "w/o dual-gated fushion"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 3: AblationresultsofLPGNet.",
      "data": [
        {
          "Table 4 Comparison with prior unimodal": "(audio-only) methods."
        },
        {
          "Table 4 Comparison with prior unimodal": "Audio Model"
        },
        {
          "Table 4 Comparison with prior unimodal": "Wav2Vec"
        },
        {
          "Table 4 Comparison with prior unimodal": "Hubert"
        },
        {
          "Table 4 Comparison with prior unimodal": "emotion2vec"
        },
        {
          "Table 4 Comparison with prior unimodal": "emotion2vec"
        },
        {
          "Table 4 Comparison with prior unimodal": "emotion2vec"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 3: AblationresultsofLPGNet.",
      "data": [
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "emotion2vec\nLPGNet\n84.53"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "performance with existing unimodal baselines. As\nshown in Table 4 and Table 5,"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "our model consistently outperforms other text-only and audio-only approaches when"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "using the same feature extractors—Emotion2vec for audio and BERT for text. These"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "results confirm that our chosen encoders are effective, and that LPGNet\nis able to"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "fully leverage them through its attention-based and fusion-oriented architecture."
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "Importantly, LPGNet maintains\nstrong performance\neven in unimodal\nsettings."
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "This demonstrates\nthat\nits\narchitecture\ncan enhance\nemotion-relevant\nsignals\nand"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "suppress\nirrelevant noise,\neven when limited to a single modality. Compared with"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "simpler baselines\nsuch as\nlinear\nclassifiers or prior multimodal models adapted for"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "unimodal\ninput, LPGNet achieves noticeable accuracy gains. This suggests that the"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "model’s structural advantages go beyond feature quality and contribute directly to its"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "discriminative power."
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "Furthermore, Table 3 illustrates\nthe added value of\nintegrating multiple modali-"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "ties. Multimodal fusion significantly improves performance over each unimodal variant,"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "indicating strong cross-modal complementarity. Acoustic and textual modalities con-"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "tribute different but synergistic cues—such as prosody and semantics—which, when"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "fused, offer a more holistic\nemotional\nrepresentation. These findings highlight\nthe"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "necessity\nof\nboth\neffective\nunimodal\nencoders\nand\ncarefully\ndesigned multimodal"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "integration mechanisms for robust emotion recognition in conversation."
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "fea-\nEffect of Self-Distillation Coefficients. To assess the quality of unimodal"
        },
        {
          "emotion2vec\nCFIA-Net[45]\n76.42": "ture representations in LPGNet, we compare its performance with existing unimodal"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 5: Comparisonwithpriorunimodal",
      "data": [
        {
          "Table 5 Comparison with prior unimodal": "(text-only) methods."
        },
        {
          "Table 5 Comparison with prior unimodal": "Text Model"
        },
        {
          "Table 5 Comparison with prior unimodal": "bert"
        },
        {
          "Table 5 Comparison with prior unimodal": "bert"
        },
        {
          "Table 5 Comparison with prior unimodal": "bert"
        },
        {
          "Table 5 Comparison with prior unimodal": "bert (Ours)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 5: Comparisonwithpriorunimodal",
      "data": [
        {
          "Table 6 Effect of distillation loss weights on": "performance."
        },
        {
          "Table 6 Effect of distillation loss weights on": "λtask"
        },
        {
          "Table 6 Effect of distillation loss weights on": "1.0"
        },
        {
          "Table 6 Effect of distillation loss weights on": "1.0"
        },
        {
          "Table 6 Effect of distillation loss weights on": "1.0"
        },
        {
          "Table 6 Effect of distillation loss weights on": "1.5"
        },
        {
          "Table 6 Effect of distillation loss weights on": "1.5"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 5: Comparisonwithpriorunimodal",
      "data": [
        {
          "Table 7 Performance comparison with SDT (4-class and 6-class). SDT* uses our feature": "extractors."
        },
        {
          "Table 7 Performance comparison with SDT (4-class and 6-class). SDT* uses our feature": ""
        },
        {
          "Table 7 Performance comparison with SDT (4-class and 6-class). SDT* uses our feature": "LPGNet (4-class)"
        },
        {
          "Table 7 Performance comparison with SDT (4-class and 6-class). SDT* uses our feature": "SDT* (4-class)"
        },
        {
          "Table 7 Performance comparison with SDT (4-class and 6-class). SDT* uses our feature": "LPGNet (6-class)"
        },
        {
          "Table 7 Performance comparison with SDT (4-class and 6-class). SDT* uses our feature": "SDT* (6-class)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "or non-financial\ninterests\nto disclose. The authors have no competing interests\nto"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "declare\nthat are\nrelevant\nto the\ncontent of\nthis article. All authors\ncertify that"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "they have no affiliations with or\ninvolvement\nin any organization or\nentity with"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "any financial\ninterest or non-financial\ninterest\nin the subject matter or materials"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "discussed in this manuscript. The authors have no financial or proprietary interests"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "in any material discussed in this article."
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "• Ethics approval and consent to participate: Not applicable."
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "• Consent for publication: Not applicable."
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "• Data\navailability: The\ndata\nsupporting\nthe\nfindings\nof\nthis\nstudy\nare\npublicly"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "available."
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "• Materials availability: Not applicable."
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "• Code availability: The implementation code for LPGNet will be made available at"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "GitHub publication."
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "• Author contribution: Conceptualization, methodology,\nsoftware and optimization,"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "validation, formal analysis,\ninvestigation, writing—original draft preparation, Zhin-"
        },
        {
          "• Conflict of\ninterest/Competing interests: The authors have no relevant financial": "ing He; supervision, writing—review and editing, administration, Yang Xiao."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": ""
        },
        {
          "References": "versation: Research challenges, datasets, and recent advances."
        },
        {
          "References": "100943–100953 (2019)"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "(2023)"
        },
        {
          "References": "[3] Ramakrishnan, S., El Emary,"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "W., Taylor,\nJ.G.: Emotion recognition in human-computer"
        },
        {
          "References": "Signal processing magazine 18(1), 32–80 (2001)"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "IEEE Transactions on Multimedia 26, 776–788 (2023)"
        },
        {
          "References": ""
        },
        {
          "References": ""
        },
        {
          "References": "preprint arXiv:1908.11540 (2019)"
        },
        {
          "References": ""
        },
        {
          "References": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Recognition, pp. 10761–10770 (2023)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[8] Song, R., Giunchiglia, F., Shi, L., Shen, Q., Xu, H.: Sunet: Speaker-utterance"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "interaction\ngraph\nneural\nnetwork\nfor\nemotion\nrecognition\nin\nconversations."
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Engineering Applications of Artificial Intelligence 123, 106315 (2023)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[9] Tang, Y., Hu, Y., He, L., Huang, H.: A bimodal network based on audio–text-"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "interactional-attention with arcface loss\nfor\nspeech emotion recognition. Speech"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Communication 143, 21–32 (2022)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[10] Zhao, X., Curtis, A.: Bayesian inversion, uncertainty analysis and interrogation"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "using boosting variational\ninference. arXiv preprint arXiv:2312.17646 (2023)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[11] Maji, B., Swain, M., Guha, R., Routray, A.: Multimodal\nemotion recognition"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "based on deep temporal features using cross-modal transformer and self-attention."
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Signal Processing (ICASSP), pp. 1–5 (2023). IEEE"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[12] Hu, J., Liu, Y., Zhao, J., Jin, Q.: Mmgcn: Multimodal\nfusion via deep graph"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "convolution\nnetwork\nfor\nemotion\nrecognition\nin\nconversation.\narXiv\npreprint"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "arXiv:2107.06779 (2021)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[13] Peng, T., Xiao, Y.: Dark\nexperience\nfor\nincremental\nkeyword\nspotting.\nIn:"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "ICASSP 2025-2025\nIEEE International Conference\non Acoustics, Speech and"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Signal Processing (ICASSP), pp. 1–5 (2025). IEEE"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[14] Xiao, Y., Peng, T., Das, R.K., Hu, Y.,\nZhuang, H.: Analytickws:\ntowards"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "exemplar-free\nanalytic\nclass\nincremental\nlearning\nfor\nsmall-footprint\nkeyword"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "spotting. arXiv preprint:2505.11817 (2025)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[15] Xiao,\nY.,\nYin,\nH.,\nBai,\nJ.,\nDas,\nR.K.: Mixstyle\nbased\ndomain\ngeneral-"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "ization\nfor\nsound\nevent\ndetection with\nheterogeneous\ntraining\ndata.\narXiv"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "preprint:2407.03654 (2024)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[16] Xiao, Y., Das, R.K.: Where’s\nthat voice coming? continual\nlearning for\nsound"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "source localization. arXiv preprint:2407.03661 (2024)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[17] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X.,"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "large language models.\nWang, C., Wang, Y., et al.: A survey on evaluation of"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "ACM transactions on intelligent systems and technology 15, 1–45 (2024)"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "[18] Naveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N.,"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Barnes, N., Mian, A.: A comprehensive overview of\nlarge language models. ACM"
        },
        {
          "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern": "Transactions on Intelligent Systems and Technology (2023)"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[20] Wu, Z., Gong, Z., Ai, L., Shi, P., Donbekci, K., Hirschberg, J.: Beyond silent"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "letters: Amplifying llms in emotion recognition with vocal nuances. arXiv preprint"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "arXiv:2407.21315 (2024)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[21] Zhang, Y., Wang, M., Wu, Y., Tiwari, P., Li, Q., Wang, B., Qin,\nJ.: Dia-"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "loguellm: Context\nand\nemotion\nknowledge-tuned\nlarge\nlanguage models\nfor"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "emotion recognition in conversations. Neural Networks, 107901 (2025)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[22] Xue, J., Nguyen, M.-P., Matheny, B., Nguyen, L.-M.: Bioserc:\nIntegrating biog-"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "raphy speakers supported by llms for erc tasks.\nIn:\nInternational Conference on"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "Artificial Neural Networks, pp. 277–292 (2024). Springer"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[23] Zhang, L., Song, J., Gao, A., Chen, J., Bao, C., Ma, K.: Be your own teacher:"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "Improve the performance of convolutional neural networks via self distillation."
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "In: Proceedings of the IEEE/CVF International Conference on Computer Vision,"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "pp. 3713–3722 (2019)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[24] Zhang, L., Bao, C., Ma, K.: Self-distillation: Towards efficient and compact neural"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(8),"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "4388–4403 (2021)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[25] Xiao, Y., Das, R.K.: Ucil: An unsupervised class incremental\nlearning approach"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "for sound event detection. arXiv preprint:2407.03657 (2024)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[26] Li, L., Liang, S.-N., Yang, Y., Jin, Z.: Teacher-free distillation via regularizing"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "intermediate representation.\nIn: 2022 International Joint Conference on Neural"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "Networks (IJCNN), pp. 01–06 (2022). IEEE"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[27] Zhang, S., Yang, Y., Chen, C., Zhang, X., Leng, Q., Zhao, X.: Deep learning-"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "based multimodal emotion recognition from audio, visual, and text modalities: A"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "systematic review of recent advancements and future prospects. Expert Systems"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "with Applications 237, 121692 (2024)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[28] Hazarika, D., Poria, S., Zadeh, A., Cambria, E., Morency, L.-P., Zimmermann,"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "R.: Conversational memory\nnetwork\nfor\nemotion\nrecognition\nin\ndyadic\ndia-"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "logue videos.\nIn: Proceedings of the Conference. Association for Computational"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "Linguistics. North American Chapter. Meeting, vol. 2018, p. 2122 (2018)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[29] Hazarika, D., Poria, S., Mihalcea, R., Cambria, E., Zimmermann, R.: Icon: Inter-"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "active\nconversational memory network for multimodal\nemotion detection.\nIn:"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "Processing, pp. 2594–2604 (2018)"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "[30] Zhang, S., Yang, Y., Chen, C., Liu, R., Tao, X., Guo, W., Xu, Y., Zhao, X.: Mul-"
        },
        {
          "environment sound event detection system. arXiv preprint:2407.03656 (2024)": "timodal emotion recognition based on audio and text by using hybrid attention"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[31] Nguyen, D.-K., Lim, E., Kim, S.-H., Yang, H.-J., Kim, S.: Enhanced emotion"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "recognition through dynamic restrained adaptive loss and extended multimodal"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "bottleneck transformer. Applied Sciences 15(5), 2862 (2025)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[32] Ghosal, D., Majumder, N., Gelbukh, A., Mihalcea, R., Poria, S.: Cosmic: Com-"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "monsense knowledge for emotion identification in conversations. arXiv preprint"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "arXiv:2010.02795 (2020)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[33] Shen, W., Chen, J., Quan, X., Xie, Z.: Dialogxl: All-in-one xlnet for multi-party"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "conversation emotion recognition.\nIn: Proceedings of\nthe AAAI Conference on"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "Artificial Intelligence, vol. 35, pp. 13789–13797 (2021)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[34] Kuhlen, A.K., Rahman, R.A.: Mental\nchronometry\nof\nspeaking\nin\ndialogue:"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "Semantic interference turns into facilitation. Cognition 219, 104962 (2022)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[35] Yin, Y., Zhu, B., Chen, J., Cheng, L., Jiang, Y.-G.: Mix-dann and dynamic-"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "modal-distillation for video domain adaptation. In: Proceedings of the 30th ACM"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "International Conference on Multimedia, pp. 3224–3233 (2022)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[36] Aslam, M.H., Pedersoli, M., Koerich, A.L., Granger, E.: Multi\nteacher privi-"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "leged knowledge distillation for multimodal expression recognition. arXiv preprint"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "arXiv:2408.09035 (2024)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[37] Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang,"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "J.N., Lee, S., Narayanan, S.S.:\nIemocap:\nInteractive\nemotional dyadic motion"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "capture database. Language resources and evaluation 42, 335–359 (2008)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[38] Li, Y., Sun, Q., Murthy, S.M.K., Alturki, E., Schuller, B.W.: Gatedxlstm: A mul-"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "timodal affective computing approach for emotion recognition in conversations."
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "arXiv preprint arXiv:2503.20919 (2025)"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[39] Wang, J., Li, N., Zhang, L., Shan, L.: Emotion recognition for multimodal\ninfor-"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "mation interaction. In: 2025 International Conference on Intelligent Systems and"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "Computational Networks (ICISCN), pp. 1–7 (2025). IEEE"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[40] Wang, X., Zhao, S., Sun, H., Wang, H., Zhou,\nJ., Qin, Y.: Enhancing mul-"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "timodal\nemotion recognition through multi-granularity cross-modal alignment."
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "In: ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "Signal Processing (ICASSP), pp. 1–5 (2025). IEEE"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "[41] Shi, X., Li, X., Toda, T.: Emotion awareness in multi-utterance turn for improving"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "emotion prediction in multi-speaker conversation. In: Proc. Interspeech, vol. 2023,"
        },
        {
          "networks. Biomedical Signal Processing and Control 85, 105052 (2023)": "pp. 765–769 (2023)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "In: 2024 IEEE International Conference on Industrial Technology (ICIT), pp. 1–6"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "(2024). IEEE"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[43] Khan, M., Tran, P.-N., Pham, N.T., El Saddik, A., Othmani, A.: Memocmt: mul-"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "timodal emotion recognition using cross-modal transformer-based feature fusion."
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "Scientific reports 15(1), 5473 (2025)"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[44] Ma, Z., Zheng, Z., Ye, J., Li, J., Gao, Z., Zhang, S., Chen, X.:\nemotion2vec:"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "Self-supervised pre-training for\nspeech emotion representation. arXiv preprint"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "arXiv:2312.15185 (2023)"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[45] Hu, Y., Yang, H., Huang, H., He, L.: Cross-modal\nfeatures\ninteraction-and-"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "aggregation network with self-consistency training for speech emotion recognition"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[c]. In: Proc. Interspeech 2024, pp. 2335–2339 (2024)"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[46] Sun, H., Lian, Z., Liu, B., Li, Y., Sun, L., Cai, C., Tao, J., Wang, M., Cheng,"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "Y.: Emotionnas: Two-stream neural\narchitecture\nsearch\nfor\nspeech\nemotion"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "recognition. arXiv preprint arXiv:2203.13617 (2022)"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[47] Li, Z., Xing, X., Fang, Y., Zhang, W., Fan, H., Xu, X.: Multi-scale\ntempo-"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "ral transformer for speech emotion recognition. arXiv preprint arXiv:2410.00390"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "(2024)"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "[48] Chauhan, K., Sharma, K.K., Varma, T.: Multimodal emotion recognition using"
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "contextualized audio information and ground transcripts on multiple datasets."
        },
        {
          "networks: An in-depth analysis of acoustic features and model\ninterpretability.": "Arabian Journal\nfor Science and Engineering 49(9), 11871–11881 (2024)"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "2",
      "title": "Bert-erc: Fine-tuning bert is enough for emotion recognition in conversation",
      "authors": [
        "X Qin",
        "Z Wu",
        "T Zhang",
        "Y Li",
        "J Luan",
        "B Wang",
        "L Wang",
        "J Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I El Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "5",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "7",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "F Chen",
        "J Shao",
        "S Zhu",
        "H Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Sunet: Speaker-utterance interaction graph neural network for emotion recognition in conversations",
      "authors": [
        "R Song",
        "F Giunchiglia",
        "L Shi",
        "Q Shen",
        "H Xu"
      ],
      "year": "2023",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "A bimodal network based on audio-textinteractional-attention with arcface loss for speech emotion recognition",
      "authors": [
        "Y Tang",
        "Y Hu",
        "L He",
        "H Huang"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "10",
      "title": "Bayesian inversion, uncertainty analysis and interrogation using boosting variational inference",
      "authors": [
        "X Zhao",
        "A Curtis"
      ],
      "year": "2023",
      "venue": "Bayesian inversion, uncertainty analysis and interrogation using boosting variational inference",
      "arxiv": "arXiv:2312.17646"
    },
    {
      "citation_id": "11",
      "title": "Multimodal emotion recognition based on deep temporal features using cross-modal transformer and self-attention",
      "authors": [
        "B Maji",
        "M Swain",
        "R Guha",
        "A Routray"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "13",
      "title": "Dark experience for incremental keyword spotting",
      "authors": [
        "T Peng",
        "Y Xiao"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Analytickws: towards exemplar-free analytic class incremental learning for small-footprint keyword spotting",
      "authors": [
        "Y Xiao",
        "T Peng",
        "R Das",
        "Y Hu",
        "H Zhuang"
      ],
      "year": "2025",
      "venue": "Analytickws: towards exemplar-free analytic class incremental learning for small-footprint keyword spotting"
    },
    {
      "citation_id": "15",
      "title": "Mixstyle based domain generalization for sound event detection with heterogeneous training data",
      "authors": [
        "Y Xiao",
        "H Yin",
        "J Bai",
        "R Das"
      ],
      "year": "2024",
      "venue": "Mixstyle based domain generalization for sound event detection with heterogeneous training data"
    },
    {
      "citation_id": "16",
      "title": "Where's that voice coming? continual learning for sound source localization",
      "authors": [
        "Y Xiao",
        "R Das"
      ],
      "year": "2024",
      "venue": "Where's that voice coming? continual learning for sound source localization"
    },
    {
      "citation_id": "17",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Y Chang",
        "X Wang",
        "J Wang",
        "Y Wu",
        "L Yang",
        "K Zhu",
        "H Chen",
        "X Yi",
        "C Wang",
        "Y Wang"
      ],
      "year": "2024",
      "venue": "ACM transactions on intelligent systems and technology"
    },
    {
      "citation_id": "18",
      "title": "A comprehensive overview of large language models",
      "authors": [
        "H Naveed",
        "A Khan",
        "S Qiu",
        "M Saqib",
        "S Anwar",
        "M Usman",
        "N Akhtar",
        "N Barnes",
        "A Mian"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Intelligent Systems and Technology"
    },
    {
      "citation_id": "19",
      "title": "Wilddesed: an llm-powered dataset for wild domestic environment sound event detection system",
      "authors": [
        "Y Xiao",
        "R Das"
      ],
      "year": "2024",
      "venue": "Wilddesed: an llm-powered dataset for wild domestic environment sound event detection system"
    },
    {
      "citation_id": "20",
      "title": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "authors": [
        "Z Wu",
        "Z Gong",
        "L Ai",
        "P Shi",
        "K Donbekci",
        "J Hirschberg"
      ],
      "year": "2024",
      "venue": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances",
      "arxiv": "arXiv:2407.21315"
    },
    {
      "citation_id": "21",
      "title": "Dialoguellm: Context and emotion knowledge-tuned large language models for emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "M Wang",
        "Y Wu",
        "P Tiwari",
        "Q Li",
        "B Wang",
        "J Qin"
      ],
      "year": "2025",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "22",
      "title": "Bioserc: Integrating biography speakers supported by llms for erc tasks",
      "authors": [
        "J Xue",
        "M.-P Nguyen",
        "B Matheny",
        "L.-M Nguyen"
      ],
      "year": "2024",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "23",
      "title": "Be your own teacher: Improve the performance of convolutional neural networks via self distillation",
      "authors": [
        "L Zhang",
        "J Song",
        "A Gao",
        "J Chen",
        "C Bao",
        "K Ma"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Self-distillation: Towards efficient and compact neural networks",
      "authors": [
        "L Zhang",
        "C Bao",
        "K Ma"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Ucil: An unsupervised class incremental learning approach for sound event detection",
      "authors": [
        "Y Xiao",
        "R Das"
      ],
      "year": "2024",
      "venue": "Ucil: An unsupervised class incremental learning approach for sound event detection"
    },
    {
      "citation_id": "26",
      "title": "Teacher-free distillation via regularizing intermediate representation",
      "authors": [
        "L Li",
        "S.-N Liang",
        "Y Yang",
        "Z Jin"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "27",
      "title": "Deep learningbased multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "28",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference"
    },
    {
      "citation_id": "29",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition based on audio and text by using hybrid attention networks",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "R Liu",
        "X Tao",
        "W Guo",
        "Y Xu",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "31",
      "title": "Enhanced emotion recognition through dynamic restrained adaptive loss and extended multimodal bottleneck transformer",
      "authors": [
        "D.-K Nguyen",
        "E Lim",
        "S.-H Kim",
        "H.-J Yang",
        "S Kim"
      ],
      "year": "2025",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "32",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "33",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Mental chronometry of speaking in dialogue: Semantic interference turns into facilitation",
      "authors": [
        "A Kuhlen",
        "R Rahman"
      ],
      "year": "2022",
      "venue": "Cognition"
    },
    {
      "citation_id": "35",
      "title": "Mix-dann and dynamicmodal-distillation for video domain adaptation",
      "authors": [
        "Y Yin",
        "B Zhu",
        "J Chen",
        "L Cheng",
        "Y.-G Jiang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Multi teacher privileged knowledge distillation for multimodal expression recognition",
      "authors": [
        "M Aslam",
        "M Pedersoli",
        "A Koerich",
        "E Granger"
      ],
      "year": "2024",
      "venue": "Multi teacher privileged knowledge distillation for multimodal expression recognition",
      "arxiv": "arXiv:2408.09035"
    },
    {
      "citation_id": "37",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "38",
      "title": "Gatedxlstm: A multimodal affective computing approach for emotion recognition in conversations",
      "authors": [
        "Y Li",
        "Q Sun",
        "S Murthy",
        "E Alturki",
        "B Schuller"
      ],
      "year": "2025",
      "venue": "Gatedxlstm: A multimodal affective computing approach for emotion recognition in conversations",
      "arxiv": "arXiv:2503.20919"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition for multimodal information interaction",
      "authors": [
        "J Wang",
        "N Li",
        "L Zhang",
        "L Shan"
      ],
      "year": "2025",
      "venue": "2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)"
    },
    {
      "citation_id": "40",
      "title": "Enhancing multimodal emotion recognition through multi-granularity cross-modal alignment",
      "authors": [
        "X Wang",
        "S Zhao",
        "H Sun",
        "H Wang",
        "J Zhou",
        "Y Qin"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
      "authors": [
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "42",
      "title": "Enhancing speech emotion recognition in urdu using bi-gru networks: An in-depth analysis of acoustic features and model interpretability",
      "authors": [
        "M Adeel",
        "Z.-Y Tao"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Industrial Technology (ICIT)"
    },
    {
      "citation_id": "43",
      "title": "Memocmt: multimodal emotion recognition using cross-modal transformer-based feature fusion",
      "authors": [
        "M Khan",
        "P.-N Tran",
        "N Pham",
        "A El Saddik",
        "A Othmani"
      ],
      "year": "2025",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "44",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "45",
      "title": "Cross-modal features interaction-andaggregation network with self-consistency training for speech emotion recognition [c]",
      "authors": [
        "Y Hu",
        "H Yang",
        "H Huang",
        "L He"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech 2024"
    },
    {
      "citation_id": "46",
      "title": "Emotionnas: Two-stream neural architecture search for speech emotion recognition",
      "authors": [
        "H Sun",
        "Z Lian",
        "B Liu",
        "Y Li",
        "L Sun",
        "C Cai",
        "J Tao",
        "M Wang",
        "Y Cheng"
      ],
      "year": "2022",
      "venue": "Emotionnas: Two-stream neural architecture search for speech emotion recognition",
      "arxiv": "arXiv:2203.13617"
    },
    {
      "citation_id": "47",
      "title": "Multi-scale temporal transformer for speech emotion recognition",
      "authors": [
        "Z Li",
        "X Xing",
        "Y Fang",
        "W Zhang",
        "H Fan",
        "X Xu"
      ],
      "year": "2024",
      "venue": "Multi-scale temporal transformer for speech emotion recognition",
      "arxiv": "arXiv:2410.00390"
    },
    {
      "citation_id": "48",
      "title": "Multimodal emotion recognition using contextualized audio information and ground transcripts on multiple datasets",
      "authors": [
        "K Chauhan",
        "K Sharma",
        "T Varma"
      ],
      "year": "2024",
      "venue": "Arabian Journal for Science and Engineering"
    }
  ]
}