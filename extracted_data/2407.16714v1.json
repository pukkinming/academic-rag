{
  "paper_id": "2407.16714v1",
  "title": "Masked Graph Learning With Recurrent Alignment For Multimodal Emotion Recognition In Conversation",
  "published": "2024-07-23T02:23:51Z",
  "authors": [
    "Tao Meng",
    "Fuchen Zhang",
    "Yuntao Shou",
    "Hongen Shao",
    "Wei Ai",
    "Keqin Li"
  ],
  "keywords": [
    "Graph representation learning",
    "multimodal emotion recognition",
    "multimodal fusion",
    "recurrent alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Since Multimodal Emotion Recognition in Conversation (MERC) can be applied to public opinion monitoring, intelligent dialogue robots, and other fields, it has received extensive research attention in recent years. Unlike traditional unimodal emotion recognition, MERC can fuse complementary semantic information between multiple modalities (e.g., text, audio, and vision) to improve emotion recognition. However, previous work ignored the inter-modal alignment process and the intra-modal noise information before multimodal fusion but directly fuses multimodal features, which will hinder the model for representation learning. In this study, we have developed a novel approach called Masked Graph Learning with Recursive Alignment (MGLRA) to tackle this problem, which uses a recurrent iterative module with memory to align multimodal features, and then uses the masked GCN for multimodal feature fusion. First, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Second, we build a recurrent iteration module with a memory function, which can use communication between different modalities to eliminate the gap between modalities and achieve the preliminary alignment of features between modalities. Then, a cross-modal multi-head attention mechanism is introduced to achieve feature alignment between modalities and construct a masked GCN for multimodal feature fusion, which can perform random mask reconstruction on the nodes in the graph to obtain better node feature representation. Finally, we utilize a multilayer perceptron (MLP) for emotion recognition. Extensive experiments on two benchmark datasets (i.e., IEMOCAP and MELD) demonstrate that MGLRA outperforms state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E Motions affect every aspect of our lives through thoughts or actions, and conversation is the primary way to express them  [1] -  [7] . Therefore, it is crucial to understand emotions in conversation accurately, and the results can be widely used in fields such as intelligent dialogue  [8]  and intelligent recommendations  [9] . However, in actual dialogue scenes, the emotions expressed by the speaker are not only related to the content of the speech but also closely related to his tone and expression. Multimodal Emotion Recognition in Conversation (MERC) task aims to use the utterance (text, audio) and visual (expression) information in the conversation to identify the speaker's emotion. Compared with traditional unimodal emotion recognition in conversation, MERC can improve the instability of emotion analysis by fusing richer multimodal semantic information  [10] -  [17] . Therefore, the key to advancing MERC lies in the effective alignment and fusion of text, audio, and visual information to achieve a collaborative understanding of cross-modal emotional semantics.  [18] -  [22]  In response to the above challenges, many researchers have made significant efforts in the field of conversational emotion recognition. For instance, Liu et al.  [23]  first used a convolutional neural network (CNN) to learn local features of speech signals, then used a recurrent neural network (RNN) to capture long sequence features, and finally fused the two types of features to achieve emotion recognition. Lian et al.  [24]  propose a transformer-based dialogue emotion recognition model called CTNet, which can adaptively learn and capture important emotional features from input dialogues. Due to the excellent performance of graph neural networks (GNN) in relational modeling, Ghosal et al.  [10]  converted the conversation history into a graph data structure and effectively extracted the emotional features in the conversation history through GNN. This method can not only be used for emotion recognition tasks but also can be applied to other dialogue-related tasks. Hu et al.  [11]  use GNN to model speaker-to-speaker relationships, effectively exploiting multimodal dependencies and speaker information. Yuan et al.  [25]  used the relational bilevel GNN to model MERC, which reduced the redundancy of node information and improved the capture of long-distance dependencies.\n\nHowever, only considering multimodal fusion is not complete enough for MERC. The alignment of semantic features before multimodal fusion is also a difficult challenge for MERC, which affects the fusion performance to some extent. Alignment is often used to unify disparate data from multiple modalities  [26] ,  [27] . At the same time, noise reduction processing is indispensable during the alignment process. There are usually two types of noise.  (1)  As shown in Fig.  1  (a), features with different granularities may mean inconsistent emotional polarity. Character visual angles represent positive emotions, while different words represent neutral or negative emotions.  (2)  The original features extracted from the corresponding single modality using different pre-trained models may contain some missing, redundant, or even wrong information. Previous researchers have done a lot of work on this issue. For example, Chen et al.  [28]  proposed a gated multimodal embedding LSTM, which can filter noise information while processing noisy modality data and achieve finer fusion between input modalities. Xu et al.  [29]  use the attention mechanism to use an adaptive alignment strategy in the alignment layer, which can automatically learn alignment weights in the process of frame and word alignment in the time domain. Xue et al.  [30]  proposed a multi-level attention map network to filter intra-modal or inter-modal noise to achieve fine-grained feature alignment. However, as shown in Fig.  1 (b ), these methods have the following limitations:  (1)  The alignment process is often completed in one go, lacking an iterative alignment process, resulting in the model being unable to complete fine-grained alignment. (2) These methods do not allow the model to observe representations extracted from other modalities and realign them during the alignment process and do not consider contextual dialogue relationships during the alignment process, resulting in poor performance when dealing with the first type of noise.\n\nTo align and fuse semantic information from multiple modalities, we carry out a Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation (MGLRA), which uses an iterative alignment mechanism to strengthen the modalities' consistency gradually, as shown in Fig.  1 (c ). First, MGLRA uses different feature encoders to represent modality-specific features. Second, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Third, MGLRA employs a novel feature alignment method based on recursive memoryaugmented cross-modal attention, which iteratively refines and aligns unimodal features by observing memory modules from other modalities. Fourth, we introduce a variational GCNbased fusion method to fuse unimodal features to produce a robust multimodal representation. Finally, multimodal representation is directly used in MERC to generate emotion classification. The contributions of our work are summarized as follows:\n\n• We propose a novel Masked Graph Learning with a Recurrent Alignment (MGLRA) model to refine unimodal representations of semantic information from multiple modalities. MGLRA uses a memory mechanism to iteratively align semantic information from multiple modalities, making it more robust in noisy scenes and scenes lacking modal information. • We introduce a cross-modal multi-head attention mechanism to explore interactive semantic information among multiple modalities and expand the receptive field of contextual information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we mainly introduce the research in emotion recognition, the technology of alignment mechanism related to our research, and the latest related work of GCN.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Emotion Recognition In Conversation",
      "text": "Conversation analysis and speaker relationship modeling are crucial in emotion recognition tasks. As demonstrated in  [31]  and  [32] , previous studies have deeply explored the relationship between emotion and social relevance in conversations and highlighted the dynamic emotional issues arising in human interactions. These interactions form complex interrelationships that require dynamic consideration in model design. To this end, DialogueRNN  [33]  adopts a recurrent neural network (RNN) and an attention mechanism to automatically learn long-term dependencies and dynamic interaction patterns between speakers, which shows excellent performance. DialogueGCN  [10]  transfers and aggregates information on the nodes and edges of the graph structure through graph convolutional neural network (GCN), more effectively handles long-term dependencies and multi-round dialogue interactions, and better captures the global structure and context of dialogues. Inspired by the high-performance models in multimodal community tasks  [34] -  [37] , many current models focus on the fusion process to solve the multi-modal emotion recognition (MERC) task. For example, LR-GCN  [38]  introduces a latent relationship representation learning mechanism to better represent the interactive relationships between nodes and edges by learning the latent connections between nodes, thereby improving performance.\n\nHowever, these methods mainly focus on the fusion process of context information and speaker information to improve the performance of the fusion process but ignore the semantic information alignment process before fusion. This directly limits the effectiveness of the model during fusion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Alignment Mechanism",
      "text": "The alignment mechanism matches the semantic features from different modalities so that the emotional expressions of other modalities are consistent. Multimodal emotion recognition mainly uses context-order-based and enforced word-level alignments.\n\nThe alignment of the contextual order relationship can be used to compare the similarity or difference between two sentences while aligning the relationship between words and frames to achieve the fusion between modalities. Liu et al.  [39]  used the attention mechanism to establish the alignment between vision and audio modalities, and corresponding elements can be found in the two modalities. This correspondence is called bi-directional attention alignment. Li et al.  [40]  proposed an Inter-modality Excitement Encoder (IMEE), which can learn the refined excitability between modalities, such as vision and audio modalities. Chen et al.  [41]  introduced a cross-modal time consistency module, which used a Bi-LSTM model to learn the time dependence between vision and audio modalities, ensuring that the emotional prediction results of the two modalities at the same time step are consistent.\n\nEnforced word-level alignment in tasks with multiple modal inputs by providing information in the decoder corresponding to words or regions in different modalities; forced alignment between other modalities and output sequences is achieved. Gu et al.  [42]  proposed a method for multimodal emotion analysis using a hierarchical attention mechanism and wordlevel alignment. Romanian et al.  [43]  implemented a technique for detecting depression using word-level multimodal fusion, which mainly relies on time-based recursive methods to achieve word-level modal alignment, but word-level processing also brings high computational complexity, which is something to consider.\n\nDespite the success of the above methods, they are all local alignment mechanisms, ignoring the interaction of global context information, which leads to limited context awareness of the model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Graph Convolutional Network",
      "text": "The rise of graph neural networks (GNN) has attracted researchers over the past few years. It has achieved remarkable success in research areas such as semantic segmentation, object detection, and knowledge graphs  [44] ,  [45] . The graph convolutional network (GCN) proposed by Kipf et al.  [46]  is central to this success. The technology is similar to traditional convolutional neural networks (CNN), using convolutions to pass information through the network and capture comprehensive data set features. Its efficiency stems from leveraging unlabeled data for model augmentation, achieved using a simple similarity matrix. Veljkovic et al.  [47]  recently developed a graph attention network (GAT). This model innovatively uses the attention mechanism to dynamically understand the graph structure, thereby achieving accurate modeling and reasoning of complex relationships in the graph. This enables superior performance for tasks such as node classification and inference. The progress of GCN has achieved significant breakthroughs, motivating us to apply GCN to address challenges in MERC.\n\nAlthough the effectiveness of deep learning methods has been proven, the MERC task still needs to improve, such as semantic consistency between different modalities and the complexity of effectively fusing multi-modal features. To address these challenges, our paper introduces the MGLRA framework. This novel framework adopts a recurrent alignment strategy enhanced by a memory component to ensure semantic alignment across modalities before fusion. Furthermore, we utilize a multi-head attention mechanism to explore the relationships between modules in detail and combine it with a computationally efficient GCN for effective fusion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Preliminary",
      "text": "In this section, we introduce various details of our work from a data flow perspective and how we extract multimodal features of all utterances from the dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Multimodal Feature Extraction",
      "text": "The reason for unimodal-specific feature extraction methods is twofold. First, since each modality has its unique semantic features, it is best to use its special feature extractor for each modality to capture salient representations adequately. Second, this unimodal feature extraction method allows state-of-the-art feature extractors to obtain better unimodal semantic information for transfer learning. The feature extraction methods and different processing methods of all modalities used are as follows:\n\n1) Text Feature Extraction: Emotion keywords in text content play a crucial role in emotion recognition, so extracting rich lexical features from text content is a fundamental challenge in multimodal emotion recognition. In addition, the contextual semantic information composed of sentences as the basic unit also provides many guiding clues for emotion recognition. In multimodal emotion recognition research, RoBERTa often captures local semantic and global contextual features. In this paper, following previous work  [10] ,  [33] ,  [48] , we also adopt the RoBERTa model to extract and represent text features. The final processed text features are represented as x t , and x t ∈ R dt , d t = 100. The sequence of text features is denoted by X t .\n\n2) Audio Feature Extraction: In determining the speaker's emotional state, audio features play a crucial role in information. So we use openSMILE to extract audio features following previous work  [10] ,  [33] ,  [48] . It is a highly opensource software for extracting audio features, mainly used in emotion recognition, emotion computing, music information processing, etc. It can extract many vectors in audio, including MFCC, frame intensity, frame energy, pitch, etc. The final processed audio features are represented as x a , and x a ∈ R da , d a = 100. The audio feature sequence is denoted by X a .\n\n3) Vision Feature Extraction: Since the facial expression features can best reflect the emotional changes at a particular moment, we use 3D-CNN to extract the expressive facial features of the interlocutor to enhance the extraction of unimodal features of the vision to pursue better multimodal fusion effect following previous work  [10] ,  [33] ,  [48] . In addition to extracting the details of relevant features from many key image frames, 3D-CNN can extract spatiotemporal features jumping across multiple image frames. Doing so makes it easy to identify critical emotional states, such as smiling or depression. The final processed vision features are represented as x v , and x v ∈ R dv , d v = 512. The vision feature sequence is denoted by X v .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Problem Definition",
      "text": "Given a dataset S of multimodal dialogues with multiple characters, through our designed preprocessing process, we get input features X r m = x (m,1) , x (m,2) , . . . , x (m,l r m ) , m ∈ {a, v, t}, a, v, t represent audio, vision, and text, respectively. Here, r represents the original feature, and l r m means the sequence length. Given these sequences X m , the final task is to determine a deep fusion network F (X m ) such that the output ŷm is getting closer and closer to the target y m , which can be achieved by minimizing the loss function. The loss function of the model is defined as shown in Eq. (  1 ):\n\nwhere b represents the batch size, y m is the true emotion of the utterance and ŷm is the predicted emotion of the model.\n\nIV. METHODOLOGY This section proposes a novel Masked Graph Learning with Recurrent Alignment (MGLRA) to improve emotion classi-fication performance for multimodal emotion recognition in conversation. Fig.  2  shows the overall architecture of MGLRA. This method consists of the following parts: data preprocessing, multimodal iterative alignment, multimodal fusion with masked GCN, and emotion classifier. (1) Data preprocessing: Due to the different data structures of other modalities, we use RoBERTa, openSMILE, and 3D-CNN to extract text, audio, and visual features in the data preprocessing stage.\n\n(2) Multimodal iterative alignment: First, a graph attention filtering mechanism is developed in the multimodal feature alignment stage to adequately filter redundant noise in multimodal features. Second, to enhance the expressive ability of the original multimodal features, the memory-based recursive feature alignment (MRFA) was created, and this module was used to gradually realize the preliminary alignment of the three modalities using the memory iteration mechanism. Third, we develop cross-modal multi-head attention to discover shared information and complementary relationships between modalities to understand and express emotions better. (3) Multimodal fusion with masked GCN: For the fusion problem of multimodal emotion recognition in conversation, a simple and effective masked GCN is used for multimodal feature fusion, which achieves good performance without bringing more parameters. (4) Emotion classifier: We used MLP for emotion classification in the emotion classification stage.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Multimodal Iterative Alignment",
      "text": "A multimodal iterative alignment module is designed to improve the fusion effect of multimodal features, which provides aligned and robust unimodal representations for downstream fusion tasks. Specifically, due to the gap in the semantic information between the three modalities and the different peaks of their data distributions, it is difficult for the model to capture the complementary semantic information among the three modalities. Therefore, we cyclically align unimodal features before downstream task fusion to produce a significant unimodal representation. This unimodal feature alignment mechanism is also reflected in the multisensory cognitive system of animals. Technically, the multimodal iterative alignment module includes a graph attention filtering mechanism, a memory-based recursive feature alignment method, and cross-modal multi-head attention. Specifically, we first exclude redundant or wrong information within or between modalities through a graph attention filtering mechanism. Then, the memory-based recursive feature alignment is used to achieve the preliminary alignment of features between modalities. Finally, the final alignment of inter-modal features is achieved using cross-modal multi-head attention.\n\n1) Graph Attention Filtering Mechanism: Inspired by the Multi-Level Attention Graph Network (MAMN)  [30] , we use a graph attention filtering mechanism to filter out some noise in raw multimodal features, which may contain wrong, redundant, or missing information. The difference is that we assign higher weights to the more critical multimodal features rather than multi-granular features. After such a process, the representations of all multimodal features are re-optimized. Correspondingly, the weighted average based on the attention Fig.  2 . We propose the architecture of MGLRA. In the preprocessing stage, we use different feature extractors for the structural features of different modality data. In the multimodal feature alignment stage, we use a graph filtering mechanism for noise reduction and propose an alignment architecture with a memory iteration mechanism to enhance semantic features. Moreover, the speaker's information is incorporated into the construction process of the graph. Then the masked GCN is used to fuse the semantics to achieve the final emotion label classification.\n\nmechanism relatively emphasizes or weakens the role of a modality. This can lead to more accurate emotional judgments.\n\nThe graph structure must be constructed before utilizing the graph attention filtering mechanism for intra-modal or inter-modal noise reduction. For the input text, visual, and audio modality features, we first feed them into a long shortterm memory network (LSTM) to extract contextual semantic information. The formula for LSTM is defined as follows:\n\nwhere r represents the original feature, and c represents features with contextual information. Then, we construct a graph structure for the captured multimodal context information. The specific graph construction process is as follows. As shown in Fig.  2 , the central node of the graph is a multimodal feature node, represented by brown, and its features are generated by connecting text, audio, and visual features. The first-order neighbors of the central node are unimodal feature nodes, and purple, green, and yellow represent text, speech, and visual feature nodes, respectively. Moreover, the edges represent the relationships between each unimodality and multimodality, such as textmultimodal, audio-multimodal, and visual-multimodal relationships. Finally, the graph attention filtering mechanism filters noise or redundant information within and between modalities by learning information about nodes and edges and assigning different weights to different nodes.\n\nThe input of the graph attention filtering mechanism is the edge relation category matrix C ∈ R 3×T and the node eigenvalue matrix V ∈ R 4×P , where T is the dimension of each relation type embedding, and P is the dimension of each eigenvalue. The relation category C contains three ba-sic semantic features: text-multimodal, audio-multimodal, and visual-multimodal levels. The eigenvalue matrix V contains four kinds of semantic features: multimodal features X c m , text features X c t , audio features X c a , and visual features X c v . In order to filter the noise or redundant information of each node, the relationship degree between feature node pairs is represented by c ijk , which is defined in Eq. (  3 ):\n\nwhere W ijk represents the linear transformation matrix obtained by learning in the network, ∥ represents cascade operation. In particular, V i , V j , represents the i-th row and j-th column of the value matrix, and C k represents the k-th row of the edge relation category matrix. The purpose of filtering and enhancing the original semantic features is achieved by using the attention mechanism to assign different weights to different multimodal feature vectors. The attention weight calculation formula is as follows:\n\nwhere Q i represents the neighbors of feature node v i , and R iq represents the relationship type between feature node pair v i and v q . For the feature node v i , its filtering feature is the sum of each pair of representations weighted by their attention weights:\n\nEach feature vector is updated through all the above computation steps, resulting in three new unimodal features X h a , X h v , X h t . Furthermore, the noise in each mode has been reduced.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "2) Memory-Based Recursive Feature Alignment (Mrfa):",
      "text": "A specific flowchart illustrating how we utilize MRFA and crossmodal multi-head attention for alignment is depicted in Fig.  3 . In MRFA, each modality will create a memory block ϕ that stores unimodal semantic features. After recursive strengthening of the memory blocks, a feature sequence X ϕ   3 . Detailed pipeline for aligning multimodal data using MRFA and crossmodal multi-head attention. First, each modality has a corresponding memory block for information storage. Then, a single-modal attention mechanism is used to extract intra-modal information. Finally, cross-modal multi-head attention is used to achieve multi-modal feature fusion. Here we use two modes as examples, and the three modes in the paper cross each other in pairs.\n\nThen, MRFA uses the semantic features X h m extracted from different unimodal feature extractors to generate a memory block by the subsequent recurrent alignment augmentation network. As shown in Eq. (  6 ), X ϕ m uses the features X h m ′ and X h m ′′ of the other two modalities for alignment refinement.\n\nIn subsequent model runs, MRFA reads the content from the memory repository, continuously refines X ϕ m using X h m , and aligns the unimodal latent semantic information with the other two modalities. These refined unimodal semantic features obtained above will be stored in the memory block to replace the old content in the block.\n\nMoreover, MRFA uses an attention mechanism for each modality to extract significant semantic features X R m ∈ R b×F R m . Here, F R m represents the embedding size of the enhanced unimodal feature. Intra-modality memory attention performs refined feature extraction on the semantic features X ϕ m of each entry τ in the three modal memory blocks. The calculation formula of the attention weight of each memory entry τ in the memory blocks is as follows:\n\nThen, MRFA utilizes ω (m,τ ) to fuse memory block features to extract the salient unimodal representations in each modality. W R T m represents a learnable dynamic parameter. l m indicates the number of entries.\n\nIn MRFA, we use the lightweight model 1D-CNN to calculate the attention weight and set the filter size to 1 to speed up the attention calculation.\n\n3) Cross-modal Multi-head Attention: First, we apply linear projections to the query Q ∈ R T Q ×d Q , key K ∈ R T K ×d K , and value V ∈ R T V ×d V , n times using distinct linear transformations. Here, n represents the number of heads. T Q , T K , and T V denote the sequence lengths of the query, key, and value, respectively, representing the number of elements in each sequence. Similarly, d Q , d K , and d V are used to indicate the feature dimensions of the query, key, and value, respectively, representing the number of features or dimensions in each element of the sequences.\n\nAfter the basic description, to precisely illustrate how the cross-modal multi-head attention mechanism is applied to data of different modalities, we will use one modality as an example for illustration. Let m represent one of the modalities, such as text, and m ′ represent one of the other two modalities, such as visual or audio. In this case, we first process the text modality (m) data, applying n different linear transformations to generate the query Q. We then apply n different linear transformations to the other modality (m ′ ) data to generate K and V . This way, we can capture cross-modal interaction information between each modality and other modalities. The specific calculation is as follows:\n\nare learnable parameters in the fully connected layer, and d m is the output dimension.\n\nThen, we use the dot-product attention to compute queries\n\non each projection, and get attention scores for feature vectors composed of different relations. The formula is defined as follows:\n\nNext, the outputs of the attention function head i , i ∈ [1, n] are concatenated to form the final value X head m , which is calculated as follows:\n\nHere, head i ∈ R T Q ×dm and X head ∈ R T Q ×ndm . Finally, we employ a GRU model to capture the correlation between feature alignments at each iteration. The formula is defined as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Multimodal Fusion With Masked Gcn",
      "text": "In this section, we introduce the embedding of speaker information, the process of graph construction, and the masked graph mechanism.\n\n1) Speaker Embedding: Some existing GCN models do not consider embedding learning of speaker information when constructing graphs, resulting in the inability to use speaker information to model potential connections within or between speakers. To solve the above problems, this paper embeds speaker information into GCN. Assuming that there are N dialogue characters in the data set, then the size of our speaker embedding is also N . We show the speaker information embedding process in Fig.  2 . The original speaker information can be represented by the vector S i , and X S represents the speaker's embedding. The calculation process is as follows:\n\nHere, W s represents a learnable weight matrix. After the above process, we embed the speaker information in GCN modeling with additional information about the speaker.\n\n2) Graph Construction: The graph construction process consists of node representation, edge connection, and edge weight initialization. The following will introduce each in detail.\n\na) Node Representation: We form a graph of text, visual, and audio data, and it is expressed as\n\nWhere V m represents the node sets, A m ∈ R |Vm|×|Vm| is the adjacency matrix, and E m represents the edge sets. Any node v i m ∈ V m in the graph contains a sentence in modalities. At this time, each sentence node v i m in the fusion graph contains the semantic features X S ∈ R dm from the three modal fusions.\n\nb) Edge Connection: In the same dialogue, we assume that there are explicit or latent connections between arbitrary sentences. Therefore, in the graph constructed in this study, any two nodes in the same modality in the same dialogue are connected. Furthermore, each node is also connected to nodes in the same conversation in different modalities due to the complementarity of the same discussion among other modalities.\n\nc) Edge Weight Initialization: The graph designed in this study has two different types of edges. (1) the two nodes connected by the edge come from the exact modal; (2) the two nodes connected by the edge come from two different modes. In order to capture the similarity of node representations, we employ degree similarity to determine the edge weight. The significance of an edge connecting two nodes is directly proportional to their similarity. This implies that nodes with higher similarity exhibit more crucial information interaction between them.\n\nTo handle different types of edges, we use different edge weighting strategies. For edges from the same modality, our approach is computed as follows:\n\nwhere n i and n j represent the feature representations of the n-th and j-th nodes in the graph. For edges from the different modalities, our approach is computed as follows:\n\nwhere ℵ is a hyperparameter. 3) Graph Learning and Mask Mechanism: Through the analysis and research of GCNs used in the past for multimodal emotion recognition, we found that the fully connected graphs constructed by most previous models have more or less introduced redundant or noisy information in the fusion process. The node structure in graph information is overemphasized, and they usually build more edges on GCNs to exploit the topological closeness between adjacent nodes. Based on this situation, we utilize a random masked graph neural network, as shown in Fig.  4 , by randomly masking the adjacency matrix to remove excessive noise in the model and improve the robustness of GCN. In addition, our masked GCN can also reduce the number of network parameters of the model on a large scale so that the model avoids the problem of overfitting.\n\nWe achieve the above effect by exploiting a random mask on the adjacency matrix. Mathematically, a subset V ⊂ V of nodes is sampled, a subset E ⊂ E of nodes is sampled and a mask mark [M ] is defined.\n\nHere x i and e i , respectively, represent the set of nodes and edges after the mask.\n\nWe construct a masked GCN, which is used to exploit the semantic complementarity between different modalities further to encode context dependencies. Specifically, given an undirected graph G m = {V m , A m , E m }, P be the renormalized graph laplacian matrix of G m :\n\nHere, P represents a learnable weight matrix, A represents the neighbor weight matrix, A [M ] represents the mask matrix, D represents the diagonal matrix, and I represents the identity matrix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Emotion Classifier",
      "text": "Using the masked GCN, we fuse the semantic information from multiple modalities to obtain the semantic representation of various modalities.\n\nThen we input the feature P that combines multiple modal semantic information into an MLP with a fully connected layer, then use the RELU activation function for nonlinear activation, and normalize the feature information P i of the hidden layer through the Softmax function:\n\nHere, W l and W smax represent a learnable weight matrix. Finally, we use the argmax function to match P i with the emotional label ŷi of the utterance and use the formula to express the process of predicting the emotional label ŷi of the utterance as follows:\n\nThe entire inference process of the MGLRA pseudocode is contained in Algorithm 1.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Algorithm 1 Mglra",
      "text": "Input: X r t , X r a , X r v ← RoBERTa,OpenSMILE,3D-CNN T F : total MRFA iterations Output:\n\n▷ (Eq. 2)\n\n4: for k ← 1, 2, ..., T F do 7:\n\nRepeat steps 6-25 for each modality\n\n▷ Initialize memory block with X h m 11:\n\nelse 12:\n\nX ϕ m ←Memory Module (X m )\n\n13:\n\n▷ Use X m for subsequent rounds\n\n▷ Unimodal feature (Eq. 9)\n\n19:\n\nProject query ( Q), key ( K) and value ( V )",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Experiments",
      "text": "This section introduces two commonly used datasets for multimodal emotion recognition and the evaluation indicators of related experiments. We show our setup and experimental procedure on these two datasets and discuss and analyze our comparison methods and results. At the same time, we also checked the results of the ablation experiments. We use visualization experiments to study the distribution of semantic features to verify the effectiveness of our proposed method.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Benchmark Datasets",
      "text": "Based on the latest research results in the MERC field, we selected two benchmark datasets of different scales, IEMO-CAP  [49]  and MELD  [50] , to conduct experiments to verify the innovation and performance of our proposed algorithm model. The detailed visualization statistics of the two datasets are shown in Table  I .  IEMOCAP is a multimodal dataset of binary interactions between actors consisting of five males and five females developed at the University of Southern California. The dialogue process between them was recorded using a video camera and a motion capture device. The dialogue recording contained five groups, each group had a boy and a girl, and each group of interactive sessions lasted 5 to 10 minutes. IEMOCAP provides the actors' video, transcription, and audio in detail. In addition, the dataset also includes annotations of various other features, such as facial expressions and head and body movements. This consists of the script content and some improvisational performances to restore the multiple situations in which emotions occur more realistically. IEMOCAP divides emotion tags into six categories: happy, anger, frustration, sadness, excitement, and neutral. The distribution of emotion labels is shown in Fig.  5(a) . A total of 7433 samples were used for experimental verification.\n\nMELD is a collection of audiovisual clips designed for research in emotion recognition. The dataset contains 1,535 pins from the classic TV series Friends, each roughly 3-5 seconds long. The clips were labeled with seven basic emotions: anger, disgust, fear, happiness, sadness, and surprise, as well as a neutral category. MELD provides data in visual, audio, and text form for experiments. Unlike IEMOCAP, in addition to emotion labels, the dataset includes demographic information about the actors in the clips, such as their gender, age, and race. Another difference from IEMOCAP is that MELD divides the dataset into three parts for users: the development set, training set, and test set, thus providing a unified test method when analyzing the model. The distribution of emotion labels is shown in Fig.  5(b) . Furthermore, following previous work  [24] ,  [38] ,  [51] , we adopt a standard train/test split method to separate the training, validation, and test sets. Specifically, we use the first four sessions of the dataset as the training and validation sets and the last session as the test set. This approach ensures that the model's performance is evaluated on different sets, thereby maximizing the diversity of training and testing scenarios and ensuring that the specific characteristics of any single session do not bias the model's performance.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Our experiments were conducted on a GeForce RTX 3090 GPU server with 24 GB of memory using an Intel Core 12900K CPU. The experimental code was developed based on the PyTorch 1.8.1 deep learning framework and implemented using Python 3.7. To obtain the best performance of the MGLRA model, we set the number of heads of the multihead attention mechanism to 10 and the random masking rate of the masked GCN to 0.7. The batch size of the model was set to 32, and the model was trained for 70 epochs, each taking approximately 55 seconds. We use the Adam optimizer and set the learning rate to 0.0001 and weight decay to 0.00005. Due to the varying data sizes of the IEMOCAP and MELD datasets, we train the models for 3000 steps and 2500 steps, respectively. During training, the models are evaluated or tested every 20 steps, and the best-performing model is saved.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Baselines And Evaluation Metrics",
      "text": "To verify the effectiveness of our model on MGLRA, the paper compared the following baseline models with our model: text-CNN  [52] , MFN  [53] , bc-LSTM  [54] , CMN  [48] , DialogueRNN  [33] , DialogueGCN  [10] , ICON  [55] , A-DMN  [56] , CTnet  [24] , LR-GCN  [38] , GraphCFC  [51] . It should be noted that since the DialogueGCN and LR-GCN methods cannot directly fuse multi-modal information, we extended them through a linear fusion layer. The extended methods are represented by DialogueGCN* and LR-GCN*, respectively.\n\nTo compare with other studies, we uniformly use the accuracy and F1-score to evaluate the performance of our model and use the weighted average method to reduce the error.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Results And Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Comparison With State-Of-The-Art And Baseline Methods",
      "text": "We evaluate the performance of the proposed model and compare it with baseline methods and state-of-the-art methods. Table  II  represents the performance of all models on the IEMOCAP dataset, while Table  III  represents the performance of all models on the MELD dataset.\n\nIEMOCAP: Observing Table  II , GraphCFC has the best performance among all baseline models. GraphCFC extracts all the different edge types from the constructed graph for further encoding, enabling GCN to model the interaction between contexts in semantic information transfer accurately. We align semantic information using a recurrent alignment network and a more lightweight GCN to incorporate multimodal semantic details in our work, resulting in better performance than GraphCFC. Among all the compared models, LR-GCN* achieves excellent performance second only to our model and GraphCFC, with 68.8% accuracy and 68.6% F1-score, respectively, effectively utilizing the latent contextual semantic session information. LR-GCN introduced a multi-head attention mechanism to find potential connections between utterances. It then encoded the enhanced multimodal semantic information into a fully connected graph, and this approach also inspired our work. However, both GraphCFC and LR-GCN focus on the fusion of later models, ignoring the noise within and between multimodal features in the early alignment process, which limits their performance. Compared with LR-GCN* and GraphCFC, our model improves weighted accuracy and F1-score by 2.5%, 2.2%, 1.5%, and 1.6%, respectively. Our model achieves the maximum performance across six metrics on the IEMOCAP dataset, and has a more balanced performance on each emotion label than other models.\n\nMELD: As can be seen from Table  III , the overall performance of our MGLRA model is relatively close to that of LR-GCN*. However, it surpasses the *LR-GCN method in two emotion categories and overall weighted average performance. Specifically, across all methods listed, MGLRA achieved an accuracy of 66.4%, an F1 score of 64.9%, an accuracy of 59.8% for the surprise category, and an accuracy of 68.5% for the joy category. Using graph attention filtering mechanism and recurrent alignment architecture to align the three modes before fusion can achieve better results. However, *LR-GCN performs well in the Digust emotion category and achieves state-of-the-art performance .*LR-GCN uses multi-head attention to dynamically explore potential relationships between sentences and introduces densely connected graphs to further capture graph structure information. This is more effective  for the MELD data set with imbalanced data and can better capture the variety of small sample emotions.\n\nExperimental results show that our proposed MGLRA model has absolute advantages over the IEMOCAP data set. It is on par with or slightly ahead of the LR-GCN model on the MELD data set. In general, MGLRA can more effectively eliminate noise and enhance semantic features during the modal alignment process. It can also effectively utilize this information to improve the overall performance of the model. This improvement is mainly attributed to two reasons: (1) MGLRA aligns the three modalities using the graph attention filtering mechanism and iterative augmentation architecture, thereby capturing more emotional information for fusion. (2) During the fusion process, masked GCN randomly discards some nodes while incorporating speaker information. These advantages improve the experimental performance of our model on the MELD dataset.\n\nTo analyze the performance of MGLRA more comprehensively, we show the confusion matrices on the IEMCOAP and MELD datasets in Fig.  6 . By analyzing the confusion matrix of IEMOCAP, it can be seen that people's expressions of excitement and happiness are very similar. The reason is that the activation domain and valence domain of excitement and happiness are relatively close, which leads to the model  Compare MGLRA (Fig.  8-d ) with original data (Fig.  8-a ), LSTM model (Fig.  8-b ) and DialogueRNN (Fig.  8-c ). MGLRA exhibits a more concentrated feature distribution, indicating superior clustering ability. In contrast, the feature distribution of the LSTM model, especially the feature distribution in the upper right corner, blurs the distinction between Angry, Sad, and Neutral emotions to a certain extent, causing the overall distribution to be more dispersed. Likewise, DialogueRNN has difficulty distinguishing between Happy and Neutral emotions and merging their data distributions. MGLRA (Fig.  8-d ) demonstrates an impressive ability to cluster emotions tightly, and its emotion classification is more precise and effective.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "B. Comparison Of Time And Total Number Of Parameters",
      "text": "In Table  IV , we perform a comprehensive performance evaluation of the benchmark models on the IEMOCAP dataset, focusing on the number of parameters required for each model (megabytes) and the computation time (seconds). To make the comparison fair, we only select GCN-based baseline methods for comparison, such as DialogueGCN  [10] , LR-GCN  [38] , RGAT  [57] , GraphCFC  [51] , and DER-GCN  [58] . Specifically, MGLRA with the mask mechanism achieves the lowest memory footprint, with a parameter of only 13.21 MB, and the fastest training time of 55.5 seconds. This efficiency is attributed to our innovative masking mechanism, which effectively reduces redundancy in computing and memory usage without affecting the ability to capture relevant features from the data. This improves the efficiency of the model. In addition, from a performance perspective, using masks not only improves the efficiency of the model but also promotes the effect of the model. Acc and F1 are both improved by 0.6, respectively. The effectiveness of our method in handling complex conversation-based emotion recognition tasks makes it a highly competitive model in terms of efficiency and performance.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Importance Of The Modalities",
      "text": "To assess the significance of text, vision, and audio modalities in emotion recognition, we conducted experiments on the IEMOCAP and MELD datasets. Our objective was to evaluate the impact of unimodal, bimodal, and multimodal features on model performance. These experiments are detailed in Table  V . Considering the datasets' data imbalance issue, we selected Weighted F1 Score (WF1) as our primary metric for its balanced consideration of precision and recall rates. Weighted Accuracy (WA) is a secondary metric. Our findings reveal that text-based features outperform those derived from audio and vision among the unimodal approaches. Specifically, text modality achieved WA scores of 63.1% and 62.7% and WF1 scores of 63.9% and 61.8% on the IEMOCAP and MELD datasets, respectively. These results underscore the text modality's pivotal role in our model's emotion recognition capability. Audio features ranked second in effectiveness, with WA scores of 61.3% and 62.0% and WF1 scores of 61.5% and 61.2% on the respective datasets. Vision features demonstrated the minor utility for emotion recognition, evidenced by WA scores of 57.7% and 60.4% and WF1 scores of 57.2% and 60.5% on IEMOCAP and MELD, respectively. This suggests challenges in extracting valuable emotional cues from vision data. Overall, our analysis indicates that text features introduce the slightest noise, thus facilitating more effective learning of dynamic feature representations by the model.\n\nThe comparison between bi-modal and single-modal approaches shows a notable enhancement in performance, with WA seeing an increase of 2% to 9% and WF1 improving by 2% to 10%. This improvement underscores that the context of conversations and the variations in audio-useful signals and  Integrating the modal features of text, vision, and audio leads to superior emotion prediction performance, with multimodal features outperforming those of single-modal and bimodal configurations. This improvement suggests that the model leverages more than just the semantic content of dialogues. It also capitalizes on vision and audio cues to enrich the emotional feature vectors' representational capacity.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "D. Ablation Study",
      "text": "In this section, we present the experimental results of devoiding each part in MGLRA and analyze their performance on the IEMOCAP dataset to see their impact on performance. The corresponding results are shown in Table  VI .\n\n• Comparing the first and second rows shows the impact of the graph attention filtering mechanism (GAF) on performance. Compared with emotion classification using only LSTM's discourse representation, graph filtering can improve accuracy by 2.2% because the graph filtering mechanism considers the impact of noise generated in the dialogue. • Comparing the second and third rows shows the impact of memory-based recursive feature alignment (MRFA) on enhancing semantic features. We can see that using loop alignment improves accuracy by 2.6% compared to graph filtering alone, which shows that it effectively aligns semantic features from multiple modalities to facilitate late fusion performance. • The results in the third and third rows show the effect of adopting multi-head attention (MHA) on performance.\n\nCompared with the recurrent alignment without multihead attention, the accuracy is improved by 2.4%, which shows that the multi-head attention mechanism effectively captures the correlation between utterances from multiple modalities during the alignment process. • The impact of adopting multimodal fusion with masked GCN (MG) on emotion prediction is shown in the fourth and fifth rows. The use of masked GCN not only brought a 0.6% increase in accuracy but also effectively reduced the model's running memory and time, facilitating the deployment and operation of our model on low-performance machines. When we adopt GAF, MRFA, MHA, and MG at the same time, MGLRA achieves the best performance. These combinations constitute our final model, and the rationality of our model is proved by experiments.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "E. Analysis On Parameters",
      "text": "In this section, we analyze the impact of the head number M in multi-head attention and the masked rate in masked GCN on the model. The relevant experimental results are shown in Fig.  8  and Fig.  9   M in the multi-head attention mechanism is a vital hyperparameter for the experimental results because it relates to the depth of potential relationship exploration in the alignment process. The masked rate in masked GCN is another hyperparameter that determines the performance of MGLRA. It is directly related to memory usage, the time required for model deployment, and the propagation depth of features in the model. We choose M and masked rate through comparative experiments. Specifically, we initially choose the most suitable M from {2, 4, 6, 8, 10, 12, 14, 16, 20} and choose the most appropriate masked rate from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} . In the experiment, we set M to 10 to get the experimental results about the asking rate and put the masked rate to 0.7 to get the testing results about M .\n\nThe experimental results in Fig.  8  prove that MGLRA performance is proportional to M in the range of 2 to 10. When M = 10, MGLRA achieves the best performance, and the accuracy is 71.3%. However, when M > 10, the performance of MGLRA starts to drop significantly. It can be seen from the experiments that when exploring more potential relations, multi-head attention cannot provide useful semantic information, and brings more redundant information, resulting in a decline in model performance.\n\nFurthermore, increasing the masked rate from 0.1 to 0.7 gradually improves performance. As shown in Fig.  9 , we observed that MGLRA achieves the best performance when rate = 0.7; at this time, accuracy = 71.3%, and F1-score = 70.10%, which shows that shielding more nodes will only lead to the loss of much crucial emotional information. This process reduces the richness of emotional communication, which directly limits the performance of MGLRA.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this study, we propose a recurrent alignment method to enhance the features of each modality and make the semantic gap between modalities more minor, making up for some shortcomings of current SOTA methods such as GraphCFC. Simultaneously, we incorporate a directed graph-based masked GCN to enhance the model's generalization ability and reduce memory usage. Our proposed MGLRA approach consistently surpasses existing SOTA models through experimental evaluation on two public datasets. These results demonstrate the effectiveness of our work in aligning semantic information between modalities and enhancing self-representation features. Many recent studies have shown that MERC still faces challenges, such as semantic gaps between modalities and many noises within modalities. In future work, we plan to optimize multimodal fusion and semantic information alignment methods and evaluate whether masked GCN applies to other multimodal tasks.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), features with different granularities may mean incon-",
      "page": 1
    },
    {
      "caption": "Figure 1: An example to illustrate the importance of alignment before performing multi-modal fusion and the difference from existing methods. (a) The example",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), these methods have the following limitations: (1)",
      "page": 2
    },
    {
      "caption": "Figure 1: (c). First, MGLRA uses different",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the overall architecture of MGLRA.",
      "page": 4
    },
    {
      "caption": "Figure 2: We propose the architecture of MGLRA. In the preprocessing stage, we use different feature extractors for the structural features of different modality",
      "page": 5
    },
    {
      "caption": "Figure 2: , the central node",
      "page": 5
    },
    {
      "caption": "Figure 3: In MRFA, each modality will create a memory",
      "page": 6
    },
    {
      "caption": "Figure 3: Detailed pipeline for aligning multimodal data using MRFA and cross-",
      "page": 6
    },
    {
      "caption": "Figure 2: The original speaker information",
      "page": 7
    },
    {
      "caption": "Figure 4: Randomly mask the nodes on the graph, and use GCN for information",
      "page": 7
    },
    {
      "caption": "Figure 4: , by randomly masking the adjacency matrix",
      "page": 7
    },
    {
      "caption": "Figure 5: Emotion label distribution on IEMOCAP and MELD datasets.",
      "page": 8
    },
    {
      "caption": "Figure 5: (a). A total of 7433 samples were",
      "page": 8
    },
    {
      "caption": "Figure 6: By analyzing the confusion",
      "page": 10
    },
    {
      "caption": "Figure 6: The confusion matrices present the true and predicted labels of the",
      "page": 11
    },
    {
      "caption": "Figure 8: Compare MGLRA (Fig. 8-d) with original data (Fig. 8-a),",
      "page": 11
    },
    {
      "caption": "Figure 8: -b) and DialogueRNN (Fig. 8-c). MGLRA",
      "page": 11
    },
    {
      "caption": "Figure 7: Visualization of the feature embedding space on the IEMCOAP dataset using T-SNE. In this visualization, each emotion category is represented by a",
      "page": 12
    },
    {
      "caption": "Figure 8: and Fig. 9, respectively.",
      "page": 13
    },
    {
      "caption": "Figure 8: MGLRA experimental results of the number of heads in multi-head",
      "page": 13
    },
    {
      "caption": "Figure 9: Experimental results of MGLRA with different masked rate on the",
      "page": 13
    },
    {
      "caption": "Figure 8: prove that MGLRA",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Utterance Count": "train+val\ntest",
          "Dialogue Count": "train+val\ntest",
          "Number Of Speakers": "",
          "Total Duration(Hours)": "",
          "Evaluation Metrics": ""
        },
        {
          "Dataset": "IEMOCAP",
          "Utterance Count": "5810\n1623",
          "Dialogue Count": "120\n31",
          "Number Of Speakers": "10",
          "Total Duration(Hours)": "12.5",
          "Evaluation Metrics": "Accuracy/F1"
        },
        {
          "Dataset": "MELD",
          "Utterance Count": "11098\n2610",
          "Dialogue Count": "1153\n280",
          "Number Of Speakers": "5",
          "Total Duration(Hours)": "73.3",
          "Evaluation Metrics": "Accuracy/F1"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSadness\nNeutral\nAngry\nExcitement\nFrustration\nAverage(w)"
        },
        {
          "Methods": "",
          "IEMOCAP": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "bc-LSTM\nCMN\nICON\nMFN\nDialogueRNN\nA-DMN\nDialogueGCN\nDialogueGCN*\nCTnet\nLR-GCN\nLR-GCN*\nGraphCFC\nMGLRA",
          "IEMOCAP": "29.2 34.3\n57.2 60.9\n54.2 51.9\n57.1 56.8\n51.2 58.0\n67.2 58.0\n55.3 55.1\n25.2 30.4\n56.1 62.3\n52.9 52.4\n61.6 59.9\n55.4 60.1\n71.2 60.5\n56.6 56.0\n22.3 30.1\n58.9 64.7\n62.8 57.3\n64.8 63.1\n59.0 63.2\n67.3 60.9\n59.2 58.6\n23.9 34.2\n65.2 70.4\n55.4 51.9\n71.9 66.9\n64.2 61.9\n68.1 62.4\n60.2 60.1\n25.5 33.2\n75.2 78.7\n58.4 59.3\n64.8 65.3\n80.1 71.7\n61.2 59.0\n63.5 62.6\n43.2 50.7\n69.5 76.9\n63.2 63.1\n63.4 56.7\n88.2 77.8\n53.4 55.8\n64.8 64.3\n40.4 42.6\n89.2 84.5\n62.0 63.4\n67.4 64.2\n65.5 63.1\n64.0 66.8\n65.1 64.2\n89.6 85.0\n40.1 43.1\n61.5 62.9\n67.0 63.9\n66.0 63.6\n63.6 66.2\n65.5 63.9\n48.2 51.2\n78.1 79.9\n69.1 65.7\n72.8 67.3\n85.4 78.4\n52.3 58.9\n68.1 67.6\n54.3 55.6\n81.7 79.0\n59.3 63.9\n69.5 69.1\n76.2 74.1\n68.1 68.7\n68.4 68.2\n55.8 56.9\n82.4 79.5\n60.3 64.6\n70.2 69.8\n76.8 74.7\n68.9 69.3\n68.8 68.6\n43.2 54.2\n85.0 84.3\n64.6 62.1\n71.4 70.2\n78.9 74.1\n63.8 62.1\n69.1 68.5\n62.9 63.5\n70.9 71.5\n71.3 70.1\n81.1 81.5\n60.2 61.1\n74.4 76.3\n69.2 67.8"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset Modal": "DialogueGCN\nLR-GCN\nGraphCFC\nRGAT\nDER-GCN",
          "Number of Parameters (MB) Time (s)": "16.85\n15.77\n41.45\n16.89\n78.59",
          "Acc. F1": "65.1 64.2\n68.4 68.2\n69.1 68.5\n65.0 65.2\n69.7 69.4"
        },
        {
          "Dataset Modal": "MGLRA (no mask)\nMGLRA (mask)",
          "Number of Parameters (MB) Time (s)": "17.45\n13.21",
          "Acc. F1": "70.7 69.5\n71.3 70.1"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Row Number": "",
          "Components": "GAF",
          "Performance": "Acc."
        },
        {
          "Row Number": "1",
          "Components": "-",
          "Performance": "63.5"
        },
        {
          "Row Number": "2",
          "Components": "√",
          "Performance": "65.7"
        },
        {
          "Row Number": "3",
          "Components": "√",
          "Performance": "68.3"
        },
        {
          "Row Number": "4",
          "Components": "√",
          "Performance": "70.7"
        },
        {
          "Row Number": "5",
          "Components": "√",
          "Performance": "71.3"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Bridging text and video: A universal multimodal transformer for audio-visual scene-aware dialog",
      "authors": [
        "Z Li",
        "Z Li",
        "J Zhang",
        "Y Feng",
        "J Zhou"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "4",
      "title": "Prediction model of dow jones index based on lstm-adaboost",
      "authors": [
        "R Ying",
        "Y Shou",
        "C Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "5",
      "title": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Y Shou",
        "X Cao",
        "D Meng",
        "B Dong",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "arxiv": "arXiv:2306.17799"
    },
    {
      "citation_id": "7",
      "title": "Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng"
      ],
      "year": "2023",
      "venue": "Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "8",
      "title": "Designing precise and robust dialogue response evaluators",
      "authors": [
        "T Zhao",
        "D Lala",
        "T Kawahara"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Generating sequences with recurrent neural networks",
      "authors": [
        "A Graves"
      ],
      "year": "2013",
      "venue": "Generating sequences with recurrent neural networks",
      "arxiv": "arXiv:1308.0850"
    },
    {
      "citation_id": "10",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "13",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "14",
      "title": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "15",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "16",
      "title": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "K Li"
      ],
      "year": "2023",
      "venue": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.16778"
    },
    {
      "citation_id": "17",
      "title": "Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "Masked contrastive graph representation learning for age estimation",
      "authors": [
        "Y Shou",
        "X Cao",
        "D Meng"
      ],
      "year": "2023",
      "venue": "Masked contrastive graph representation learning for age estimation",
      "arxiv": "arXiv:2306.17798"
    },
    {
      "citation_id": "19",
      "title": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Y Shou",
        "T Meng",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "20",
      "title": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "W Ai",
        "J Du",
        "T Meng",
        "H Liu"
      ],
      "year": "2024",
      "venue": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "arxiv": "arXiv:2407.00119"
    },
    {
      "citation_id": "21",
      "title": "A twostage multimodal emotion recognition model based on graph contrastive learning",
      "authors": [
        "W Ai",
        "F Zhang",
        "T Meng",
        "Y Shou",
        "H Shao",
        "K Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "22",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "T Meng",
        "F Zhang",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang",
        "L Guo",
        "J Dang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Rba-gcn: Relational bilevel aggregation graph convolutional network for emotion recognition",
      "authors": [
        "L Yuan",
        "G Huang",
        "F Li",
        "X Yuan",
        "C.-M Pun",
        "G Zhong"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Multi-view clustering via deep matrix factorization and partition alignment",
      "authors": [
        "C Zhang",
        "S Wang",
        "J Liu",
        "S Zhou",
        "P Zhang",
        "X Liu",
        "E Zhu",
        "C Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "T2vlad: global-local sequence alignment for text-video retrieval",
      "authors": [
        "X Wang",
        "L Zhu",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "M Chen",
        "S Wang",
        "P Liang",
        "T Baltrušaitis",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "29",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Multi-level attention map network for multimodal sentiment analysis",
      "authors": [
        "X Xue",
        "C Zhang",
        "Z Niu",
        "X Wu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "31",
      "title": "The handbook of conversation analysis",
      "authors": [
        "T Stivers",
        "J Sidnell"
      ],
      "year": "2012",
      "venue": "The handbook of conversation analysis"
    },
    {
      "citation_id": "32",
      "title": "The cognitive consequences of concealing feelings",
      "authors": [
        "J Richards"
      ],
      "year": "2004",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "33",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "C-gcn: Correlation based graph convolutional network for audio-video emotion recognition",
      "authors": [
        "W Nie",
        "M Ren",
        "J Nie",
        "S Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Visual-texual emotion analysis with deep coupled video and danmu neural networks",
      "authors": [
        "C Li",
        "J Wang",
        "H Wang",
        "M Zhao",
        "W Li",
        "X Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Image-text multimodal emotion classification via multi-view attentional network",
      "authors": [
        "X Yang",
        "S Feng",
        "D Wang",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "38",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "40",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "41",
      "title": "Multimodal emotion recognition with temporal and semantic consistency",
      "authors": [
        "B Chen",
        "Q Cao",
        "M Hou",
        "Z Zhang",
        "G Lu",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "42",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "43",
      "title": "Detecting depression with word-level multimodal fusion",
      "authors": [
        "M Rohanian",
        "J Hough",
        "M Purver"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "44",
      "title": "Relational inductive biases, deep learning, and graph networks",
      "authors": [
        "P Battaglia",
        "J Hamrick",
        "V Bapst",
        "A Sanchez-Gonzalez",
        "V Zambaldi",
        "M Malinowski",
        "A Tacchetti",
        "D Raposo",
        "A Santoro",
        "R Faulkner"
      ],
      "year": "2018",
      "venue": "Relational inductive biases, deep learning, and graph networks",
      "arxiv": "arXiv:1806.01261"
    },
    {
      "citation_id": "45",
      "title": "Spectral networks and deep locally connected networks on graphs",
      "authors": [
        "J Bruna",
        "W Zaremba",
        "A Szlam",
        "Y Lecun"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations"
    },
    {
      "citation_id": "46",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "47",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Liò",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "48",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "49",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "50",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "51",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "52",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL"
    },
    {
      "citation_id": "53",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on Artificial Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "55",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "56",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "S Xing",
        "S Mai",
        "H Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "58",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    }
  ]
}