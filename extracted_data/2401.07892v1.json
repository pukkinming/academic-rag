{
  "paper_id": "2401.07892v1",
  "title": "Deep Fuzzy Framework For Emotion Recognition Using Eeg Signals And Emotion Representation In Type-2 Fuzzy Vad Space",
  "published": "2024-01-15T18:44:13Z",
  "authors": [
    "Mohammad Asif",
    "Noman Ali",
    "Sudhakar Mishra",
    "Anushka Dandawate",
    "Uma Shanker Tiwary"
  ],
  "keywords": [
    "EEG Emotion Recognition",
    "Affective Computing",
    "VAD",
    "Emotion Dimensions",
    "DENS Dataset",
    "Emotion Analysis",
    "Emotion Representation",
    "CNN",
    "LSTM",
    "Multimodal Fusion",
    "Deep Fuzzy Framework",
    "Emotion Modelling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, the representation of emotions in the Valence, Arousal and Dominance (VAD) space has drawn enough attention. However, the complex nature of emotions and the subjective biases in self-reported values of VAD make the emotion model too specific to a particular experiment. This study aims to develop a generic model representing emotions using a fuzzy VAD space and improve emotion recognition by utilizing this representation. We partitioned the crisp VAD space into a fuzzy VAD space using low, medium and high type-2 fuzzy dimensions to represent emotions. A framework that integrates fuzzy VAD space with EEG data has been developed to recognize emotions. The EEG features were extracted using spatial and temporal feature vectors from time-frequency spectrograms, while the subject-reported values of VAD were also considered. The study was conducted on the DENS dataset, which includes a wide range of twenty-four emotions, along with EEG data and subjective ratings. The study was validated using various deep fuzzy framework models based on type-2 fuzzy representation, cuboid probabilistic lattice representation and unsupervised fuzzy emotion clusters. These models resulted in emotion recognition accuracy of 96.09%, 95.75% and 95.31%, respectively, for the classes of 24 emotions. The study also included an ablation study, one with crisp VAD space and the other without VAD space. The result with crisp VAD space performed better, while the deep fuzzy framework outperformed both models. The model was extended to predict cross-subject cases of emotions, and the results with 78.37% accuracy are promising, proving the generality of our model. The generic nature of the developed model, along with its successful cross-subject predictions, gives direction for real-world applications in the areas such as affective computing, humancomputer interaction, and mental health monitoring.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "I DENTIFYING and understanding emotions has been a major challenge not only in the field of artificial intelligence but also in general science and psychology. We all experience a range of emotions in our daily lives. It is a complex phenomenon. According to the multidimensional theory of emotions, these psychological phenomena are complex constructs that consist of different dimensions  [1] , which together shape the human emotional experience. These multidimensional spaces are Valence, Arousal and Dominance (VAD). In this perspective, the complexity of emotions arises from the interplay of multiple dimensions. Basic emotions can be considered as primary building blocks within a multidimensional space  [2] . More subtle and complex emotions emerge through the blending, intensifying, or suppressing of these basic emotional dimensions  [3] -  [5] . A schematic depiction of the emotion dimensions is provided in Fig.  1 . We can depict the emotion-i representation in VAD space by\n\nwhere E ∈ {Set of Emotions}, f represents a function, and x V , x A , x D are the crisp values of valence, arousal and dominance provided by the participants. Emotions are not simple entities but complicated combinations of subjective feelings, physiological responses, cognitive evaluations, and behavioural tendencies  [6] . Moreover, the temporal dynamics of emotions should not be examined as isolated events but as evolving trajectories within this multidimensional framework. The subjective experience of an emotion is closely linked to the cognitive appraisal of a arXiv:2401.07892v1 [cs.HC] 15 Jan 2024 situation, the physiological arousal it induces, the subsequent behavioural expressions, and the element of perceived control or influence within the emotional experience  [6] . This study also aims to explore the importance of a multidimensional theory and why valence, arousal, and dominance are important in understanding a wide spectrum of emotions. A special emphasis will be placed on Dominance and empirically showing why it is so important while considering a complex phenomenon like emotions and how it is related to the user's subjective experience for controlling the emotions.\n\nHuman emotions frequently involve a degree of ambiguity, where individuals may experience mixed feelings or transitions between emotional states  [7] . Also, emotional expression and perception vary across cultures and individuals. Individuals vary in how they express and experience emotions. Some may express joy more exuberantly, while others may do so more subtly. Emotional intensity often varies gradually rather than abruptly  [8] . Also, due to a limited emotional vocabulary, it is challenging to convey the nuances of their feelings accurately. The lack of precise words for certain emotional states contributes to ambiguity in self-reports. The meanings of emotional terms can vary among individuals. What one person labels as \"anxiety,\" another might perceive as \"nervousness\" or \"apprehension\". This semantic variability introduces ambiguity in the interpretation of self-reports. This ambiguity also occurs from the individuals' appraisal and interpretation of emotional stimuli differently as they may use different terms to describe similar emotional responses based on their cognitive interpretations. Due to all these and because human emotions are inherently complex and often exist along a spectrum  [9] , crisp boundaries, being discrete and predefined, struggle to capture the full spectrum of emotional experiences. Emotions can manifest with varying intensities, shades, and combinations. For example, emotions like \"mixed feelings\" or \"ambivalence\" are challenging to represent within the confines of crisp categories. Individuals rarely experience emotions as isolated events but rather undergo fluid transitions from one emotional state to another  [10] . Fuzzy logic excels in handling such cases, and it can handle complex emotion combinations by allowing overlapping memberships. This flexibility enables the representation of mixed emotions or intricate blends, acknowledging that individuals often experience a combination of feelings simultaneously.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Eeg Based Emotion Recognition",
      "text": "Emotion recognition through EEG is a growing field in affective computing that involves studying the complex neural patterns associated with human emotions by examining brain activity during emotional experiences. EEG, a non-invasive method, offers a real-time window with high temporal precision, providing a valuable approach in diverse applications. This can range from improving human-computer interaction to contributing to mental health research and finding neural correlates of the brain for emotions and processing.\n\nEEG data analysis includes frequency bands, spectral power analysis, ERPs, connectivity measures, and time-frequency analysis. Emotion recognition is modelled using several methods, such as machine learning, ensemble learning, fuzzy logic, deep learning, and so on. Researchers use various models such as CNN, CNN-LSTM, CNN-GRU, attention-based hierarchical, and multi-head self-attention for emotion recognition  [11] -  [17] .\n\nHowever, there are challenges in this field, specifically defining and categorising emotions has always been a challenging task, including the complexity of emotions and the need to interpret intricate neural signals accurately  [15] . EEG-based emotion recognition provides an objective measure of emotional states, minimizing reliance on subjective selfreports. This objectivity is essential for avoiding biases and gaining a more accurate understanding of emotional responses. Additionally, there are other challenges that need to be overcome, including individual variability and integration with multimodal data like ECG, EMG, EOG and self-reported states  [17] . We will address and incorporate this issue in this study.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Research Gap-Mapping The Relationship Between Vad Space And Emotion:",
      "text": "In emotion studies, prevailing research often focuses on discrete emotion recognition  [18]  or continuous emotional dimensions recognition  [19]  and usually ignores the complex relationship between emotions and emotional dimensions. This study attempts to address this gap by incorporating the deep fuzzy framework grounded in a multidimensional VAD space of emotions. This study attempts to map the relationship between emotions and the dimensional space of emotions. The primary objective is to highlight the significance of multidimensions and emphasise why it is imperative not to overlook them when delving into the identification of complex emotions beyond discrete categories. Building upon the multidimensional theory of emotions, our approach acknowledges that emotions are multifaceted constructs encompassing various dimensions  [6] .\n\nAnother significant challenge in this field is the availability of datasets based on EEG. It is constrained in terms of quantity. Generating the required amount of datasets based on EEG or any other biomedical imaging is not an easy task. Therefore, managing and optimising a limited dataset provides a significant challenge. A deep learning model often requires a substantial amount of data to be effectively trained, which is not possible with a limited dataset. However, utilising all the information inherited in the datasets can help to cope with this problem. EEG is one of the features of the datasets that can be used, but generally, the ratings provided by the participants are omitted. In this work, we aim to overcome this issue by considering information about emotional experience in terms of three independent dimensions -Valence, Arousal, and Dominance (VAD) with a deep fuzzy framework consisting of fuzzy VAD space. Fuzzy logic can be seamlessly integrated with deep learning models  [20] , combining the strengths of both paradigms. This integration allows for the development of deep fuzzy frameworks that leverage the power of neural networks while benefiting from the flexibility and interpretability of fuzzy logic  [21] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Our Contributions:",
      "text": "Before going further, here is a brief mention of our contribution to this study in a nutshell:\n\n• An insight into a novel representation of emotions using type-2 fuzzy VAD space. • Deep Fuzzy Framework: Deep Fuzzy Framework:\n\nProposing a multimodal fusion framework for recognition of a wide spectrum of emotions using fuzzy VAD space and EEG data. • Significantly improving cross-subject emotion recognition with the integration of Deep Fuzzy Framework. • Representation of temporally localised twenty-four emotions (in DENS Dataset), developed and validated on Indian population in our lab  [15] ,  [22] . The paper is structured into several sections, starting with an introduction to the research topic. The following section covers the VAD space and provides an overview of the emotions that are included in it using the DENS dataset. The third section talks about the fuzzy representation of emotion. The fourth section is about the methodology, which discusses the complete methodology from the preprocessing of the EEG signal to the integration of fuzzy framework in the deep architecture. The next section presents the results of all the experiments done. Finally, the study concludes with a discussion in the last section.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Beyond Traditional Labels: Mapping Emotions",
      "text": "IN THE VAD SPACE USING DENS In this section, we give a description of the emotional dimensions and empirically analyse why considering emotions in an emotional dimension (VAD space) is crucial. We also present an overview of the emotions collected from the DENS dataset  [15] , which covers twenty-four different emotions. These emotions have concise information of the time and have EEG recordings with subjective ratings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Exploring The Dimensions Of Emotional Experiences",
      "text": "Emotions are often represented in a three-dimensional space defined by valence, arousal, and dominance, known as the VAD model  [23] ,  [24] . The representation in this space is a potential way to capture the multifaceted nature of human emotions  [25] ,  [26] . This section provides an in-depth overview of the VAD space dimensions, elucidating their individual significance and collective role in portraying the richness of emotional experiences. Understanding the interaction of these three dimensions allows for a subtle and comprehensive representation of emotions. The combination of valence, arousal, and dominance provides a geometric space where each point corresponds to a unique emotional state.\n\nWhen it comes to understanding emotions, researchers in affective neuroscience and psychology often explore how emotions are represented in the mind and how people subjectively experience them  [27] ,  [28] . Affective experiences are central to our understanding of emotions, and these experiences are not merely influenced by language or social desirability but are intrinsic to how emotions are mentally represented  [28] . Hence, it is important to understand the subjective experiences of emotions and how they are mentally represented in order to gain a deeper understanding of how emotions influence our thoughts, behaviours, and overall well-being. While core affect, or the basic experience of pleasure or displeasure, provides a foundational aspect of emotional experience, it is acknowledged that a complete understanding of emotions goes beyond this basic level. As mentioned, an experience of emotion is not just about feeling pleasure or displeasure; it is an intentional state that involves being directed towards or about something  [29] .\n\nThe level of physiological arousal (e.g., heart rate, breathing, muscle tension) is a crucial component of emotional experiences. Different emotions are associated with varying levels of arousal. For example, excitement or fear may be linked to high arousal, while calmness or sadness may be associated with lower arousal  [30] .\n\nHaving these two dimensions is not sufficient if we ignore the degree of control or influence one perceives they have over their emotional experience and expression. This is the Dominance dimension we refer to, and it is one of the key dimensions often used to characterise emotions, along with valence (pleasantness/unpleasantness) and arousal (intensity or activation level)  [31] .\n\nOn one end of the spectrum, there are emotions that are more dominant or controllable. These are the emotions where individuals feel like they have agency over their emotional experiences. They can regulate their emotions consciously and choose how to respond to a situation. On the other end of the spectrum, there are emotions that are less controllable or more dominant over an individual's subjective experience. In these cases, it might feel like the emotion takes control rather than being consciously regulated  [32] .\n\nThere are individual differences in how people perceive and experience control over their emotions. Overall, recognising the dominance or control dimension adds depth to our understanding of emotions and contributes to the development of strategies for adaptive functioning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Overview Of Twenty-Four Emotions And Dens Dataset",
      "text": "The research employed naturalistic stimuli  [33]  to elicit emotional responses from participants. Each participant was randomly exposed to nine emotional stimuli and two nonemotional stimuli selected from a pool of 16 emotional stimuli, each lasting 60 seconds. The dataset encompasses EEG recordings from forty participants (mean age: 23.3 ± 1.25, F=3) while viewing emotionally evocative films. Participants were instructed to perform a mouse click upon experiencing any emotional response, referred to as an Emotional Event. Post each video stimulus, participants assessed six dimensions using self-assessment scales measuring valence, arousal, dominance, liking, familiarity, and relevance, with valence, arousal, and dominance scales ranging from 1 to 9.\n\nParticipants selected emotions from a predefined list or specified an emotional category aligned with their experience, resulting in 24 labelled emotions. The dataset also includes 10 other emotions associated with fewer emotional events.\n\nA total of 465 emotional experiences were collected, with each participant clicking at least once, averaging 1.29 clicks  There are a total of 465 recorded emotional events. The distribution of valence, arousal and dominance is shown in Fig.  4  of all the 465 emotional events. The total number of labelled emotions is 24 in these 465 emotional events. The distribution of all the twenty-four emotions in VAD space is shown in Fig.  3 . Emotion percentage distribution of these twenty-four emotions in all the emotional events is presented in Fig.  5 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Crucial Empirical Insights: Significance Of Emotional Dimensions, With Emphasis On Dominance In Vad Space",
      "text": "To gain a better empirical analysis of the relationship between multidimensional space and emotions, we used Fuzzy C-Means (FCM) to divide the emotions in different clusters. We determined the optimal number of clusters using the fuzzy silhouette index  [34]  and found the optimum number of clusters is 4. FCM is applied to the VAD values for all the emotional events. We assigned all the emotional labels to these clusters based on the highest membership value in each clusterI 7a. Emotion distribution to these clusters is shown in Fig.  6 .  After determining the optimal number of clusters using the fuzzy silhouette index, fuzzy C-Means is applied to the VAD values of the emotional space, resulting in their division into four clusters. The assignment of emotional labels to these clusters was based on the highest membership value in each cluster.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Clusters",
      "text": "The analysis revealed that emotions like-Happy, Excited and Joyous predominantly belonged to cluster-0, which notably possessed a cluster centroid for valence 7.73, distinguishing it from other clusters whose centroid valence values fell within the range of 1 to 3. A clear distinction was observed between the Sad and Angry emotions. The sad emotions, which are less related to emotions causing agitation, were categorised into cluster 2. The discernible contrast in emotional agitation between the emotions within cluster 2 and those in clusters 1 and 3 is further evident in the disparity of centroid arousal values. Specifically, cluster 2 demonstrates a value of 2.63, while clusters 1 and 3 exhibit values of 7.17 and 7.05, respectively. Clusters 1 and 3 exhibited similar angry-afraid emotions, with varying combinations depending on the dominance of specific emotions. Emotion \"Afraid\" is found in two different clusters with almost equal numbers of emotional events. Upon analysis, it can be observed that the mean Valence and mean Arousal values are quite similar in these two clusters for this particular emotion. However, the mean Dominance value differs significantly between the two clusters. This is a similar case with many other emotions, as shown in Fig.  6 .\n\nFigure7 represent the four and six FCM clusters with respective emotions that appear in more than one group. These emotions have the same emotional labels, similar valence, and similar arousal but significantly different dominance values. In our previous study, we found that including the Dominance dimension improves accuracy when only considering the Valence and Arousal dimensions.  [35]  These differences are the results from the subjective experiences of the participants where the participant perceives emotion as other participants perceive, but the controlling mechanism of the participants differs a lot, which can only be examined with the Dominance value. We cannot explain why this emotion should be classified separately in different clusters if we don't consider the dominance value. In conclusion, it is crucial to map an emotion into VAD space instead of representing it as a discrete entity. Emotions are subjective and can be ambiguous. They are represented in three dimensions, but due to individual differences in perception, the crisp values of subjective feelings and reporting may not accurately reflect the true nature of the emotion. Applying fuzzification to these dimensions reflects the complexity and subjectivity of emotional experiences. The concept of fuzzification deals with uncertainty and ambiguity by allowing values to be flexible and fall within a range instead of precise points. Fuzzification can indeed be applied to the dimensions of valence, arousal, and dominance in representing the complexity and variability of emotional experiences  [36] ,  [37] .\n\nSubjects feel and try to respond to their subjective feelings in terms of crisp values. Still, it will be more realistic if they represent it in continuous values such as low, medium or high and in membership functions in defining the imprecise boundaries of emotional states such as emotion belonging to a particular group or nearness to a particular group or sharing multiple groups. This approach acknowledges the fluid and subjective nature of emotional experiences and accommodates the inherent uncertainty in emotional expression  [10] . This concept enables the continuous grading of emotional ratings, which traditional categorical models may struggle to capture  [38] .\n\nRepresenting emotions through fuzzy VAD introduces the concept of soft boundaries between emotional states. This acknowledges that transitions between emotions are gradual instead of abrupt, which aligns with the dynamic and continuous nature of emotional experiences  [6] ,  [28] ,  [38] . This soft transition allows for a more natural and realistic representation of the fluidity between different emotional states. Furthermore, it helps in reducing misclassifications in emotion recognition systems. By capturing the uncertainty and imprecision inherent in emotional expression, fuzzy concepts mitigate the risk of rigid categorisations that may lead to misinterpretations of complex emotional states  [39] . This is particularly valuable in real-world scenarios where emotions are multifaceted and context-dependent  [28] .\n\nInstead of strictly categorising emotions as purely positive or negative, fuzzification allows for shades of emotional experiences. Emotions may fall on a continuum of valence, acknowledging that some emotions may have mixed or nuanced valence. This idea aligns with the concept that emotions exist on a spectrum rather than being neatly categorised. Using fuzzy concepts in emotion dimensions can be a significant improvement in emotion representation methodologies, as it contains the margin for personal biases. Arousal can also be fuzzified to capture the variability in intensity experienced across different emotions. For example, an emotion could be moderately arousing rather than strictly categorised as high or low arousal  [29] . This can manage uncertainty and imprecision, enabling us to capture the subjective and context-dependent nature of human emotions in a more subtle way. Applying fuzzy concepts in emotional dimensions offers improved representational flexibility, allowing a deeper understanding of how emotions are experienced and expressed in different contexts. Fuzzifying the dominance dimension recognises that individuals may perceive varying degrees of control over their emotions  [40] . Some emotions may be perceived as partially controllable, and fuzzification allows this gradation. Additionally, it provides a flexible framework that can adapt to individual differences in emotional expression and interpretation.\n\nWe can depict the emotion-i representation in VAD space by\n\nwhere E ∈ {Set of Emotions}, f represents a function, and x V , x A , x D are the crisp values of valence, arousal and dominance provided by the participants. A more realistic and comprehensive emotion representation of Eq.1 depicting realworld scenarios can be represented by\n\nwhere E ∈ {Set of Emotions}, f represents a function, and µ V (x V ), µ A (x A ), µ D (x D ) are the membership degrees of valence, arousal and dominance dimensions for their respective crisp values of x V , x A , x D .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Type-2 Fuzzy Representation Of Emotions:",
      "text": "We defined the representation of emotion with interval type-2 fuzzy sets. The two membership functions-Lower and Upper (LMF and UMF), are based on the general meaning of the terms and the population-based meaning of the terms. UMF is defined as a generic term for all the dimensions, while LMF is set based on the statistical values of the population. In this representation, crisp values of the VAD are converted into phonetic representations of low, medium and high. Membership functions are drawn in Fig.  8 , and membership degrees are defined as follows:\n\nUpper Membership Functions (UMF):\n\nLower Membership Functions (LMF): The symbol X dim represents the input value for that dimension and X dim ∈  [1, 9] .\n\nM and σ represent the mean and standard deviation of the various ranges for Gaussian distributions.\n\nThe membership functions are shown in Fig.  8 . The corresponding mean, standard deviation and ranges are shown in TableII. These membership functions are modified where they intersect to define the Footprint of Uncertainty (FoU), and the means are adjusted accordingly.\n\nwhere F represents a function to determine the FoU and coincide the means to a single point. µ UMF and µ LMF represent the UMF and LMF memberships. A. Data Preparation 1) Emotional Events Extraction and Preprocessing: In the initial phase of EEG signal processing, the raw data, initially referenced to the Cz electrode, was imported. Subsequently, average re-referencing was applied. The EEG signal underwent filtering using a fifth-order Butterworth bandpass filter, with a low cutoff frequency of 1 Hz and a high cutoff frequency of 40 Hz. Segmentation of the filtered signal was performed to isolate event segments corresponding to the baseline state and specific emotional events (captured using a mouse clicks). The baseline state was defined as the period from 10 to 70 seconds, while the click events were extracted from -6 to +1 seconds relative to the click response for the emotional event by the participants. The duration of the extracted click events spanned from 6 seconds before to 1 second after the event.\n\nFollowing signal concatenation of extracted emotional events, a manual inspection was conducted to ensure data quality. Electrodes and samples displaying high amplitude,  potentially due to electrode separation, were excluded from the analysis.\n\nTo mitigate the impact of eyeblink activity, Independent Component Analysis (ICA) was employed. This step was crucial to prevent automatic channel rejection based on high amplitudes associated with eye artefacts. After the initial automatic channel rejection, a second ICA iteration was implemented to eliminate any residual artefacts, such as muscle and heart activity, line noise, and channel noise. The ICLabel tool categorized independent components into different labels  [41] , and components with a brain activity probability greater than 0.3 were retained. The probabilities for all labels equal to 1 were aggregated.\n\nUpon completion of the pre-processing steps, discrete occurrences corresponding to the baseline state and emotional responses were stored, facilitating subsequent data analysis.\n\n2) Feature Extraction and Input Formatting: The Short-Time Fourier Transform (STFT) is a widely used signal processing technique, particularly for analyzing time-varying signals like EEG data. It enables the analysis of a signal's frequency content as it evolves over time, which is critical for understanding dynamic processes such as changes in brain activity captured by EEG signals. Since EEG signals are nonstationary, meaning that their statistical properties change over time, STFT provides a local analysis that adapts to these changes, making it well-suited for analyzing signals with varying characteristics. STFT offers the ability to capture temporal and frequency information simultaneously. Its adaptability to non-stationary signals makes it particularly well-suited for studying dynamic brain activities.\n\nThe data received from the i th EEG channel is x i , The STFT of x i is represented as\n\nwhere STFT coefficient X i represented at a specific time index n and frequency index m for the signal x i [n]. A sliding window approach is used, where the input signal x i is shifted by k samples. The term w[k -m] denotes the value of the window function at the relative position k -m. The application of the window function is integral in minimizing spectral leakage and refining the precision of the STFT. The complex exponential modulation term e -j2πf k/N introduces a frequency-dependent weighting factor. Here, f denotes the frequency index, k represents the sample index within the window, and N stands for the total number of samples. This modulation is pivotal in elucidating the frequency content within the defined window. We used 50% overlapping to compute the STFT.\n\nThe spectrograms are computed by\n\nAfter this, spectrograms are stacked further to create an input to be given in the architecture as Input1,\n\nwhere l is the number of stacked spectrograms.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Architecture With Deep Fuzzy Framework",
      "text": "Rather than being distinct groups, emotions can be seen as continuous values in the VAD space. This suggests that affective states and emotions are interconnected and interdependent and connected to each other systematically. Our framework is based on this phenomenon.\n\nIn Fig.  9 , we have presented the overall framework of our architecture, consisting of three modules. The first module captures spatial features, while the second module captures temporal features. The third module utilizes a fuzzy framework, demonstrated in Eq. 3, to provide the membership degree of emotion to the VAD space. We built three distinct models based on this framework that contribute to three different representations of emotions, which are discussed in detail in this section.\n\n1) Role of Fuzzy VAD Representation in Enhancing Emotion Recognition: The integration of VAD space with multimodal inputs, such as physiological signals (e.g., EEG), further enhances the capabilities of recognition systems. This integration allows for a more holistic understanding of emotional states by leveraging a diverse set of features. VAD space mitigates ambiguity, a common challenge in emotion recognition. The multidimensional information of emotional experience provided by VAD dimensions reduces the likelihood of misinterpretations, enhancing the robustness of emotion recognition algorithms. VAD space might also contribute to improved personalisation in emotion recognition. Recognising that individuals may exhibit varied emotional responses to the same stimuli, VAD dimensions allow recognition systems to account for these individual differences, leading to more accurate and personalised emotion predictions. The consideration of valence, arousal, and dominance provides richer information for understanding emotional responses and empowering emotion recognition systems. 2) Modules in the Architecture: We have three modules in the architecture: First for spatial features, second for temporal features, and third for fuzzy representational block. Our emotion recognition architecture leverages both spatial and temporal information of EEG signals. To extract significant patterns and features from spectrograms, we used Convolutional Neural Networks (CNNs). Moreover, Long Short-Term Memory (LSTM) networks are highly effective in analyzing time-series data and modelling temporal dependencies. As a result, they are an ideal solution for this emotion recognition from EEG. Lastly, we have a parallel module for providing a VAD representational block.\n\na) Spatial Module: In this process, we use kernels in the convolution process to learn local patterns from the input data. These patterns are then combined and refined through multiple layers of CNNs to yield more complex and meaningful features. We utilized two layers of CNN in our models. All the layers have dropout masks with a 20% dropout rate. The output of the spatial module is calculated by the following set of equations:\n\ni,j,k = MaxPooling(ReLU(Conv(X, W [1] , b [1] )), p m , p n ) Y  [2]  i,j,k = MaxPooling(ReLU(Conv(Y [1] , W [2] , b [2] )), p m , p n ) Y  [2]  flattened = Flatten(Y [2] )\n\ni,j,k and Y  [2]  i,j,k represent the outputs of the first and second convolutional layers, respectively at position (i,j,k). X is the Input tensor. W [1] , W [2] represent weights of the convolutional layers. b [1] , b [2] are the biases of the convolutional layers. Y  [2]  flattened represents the flattened output tensor from the second convolutional layer. (p m , p n ) is the window size of the pooling layer. All the layers have dropout masks with a 20% dropout rate.\n\nThe convolution operation, denoted as Conv(X, W [1] , b [1] ), is applied to the input tensor X using weights W [1] and biases b [1] . Following the convolution, the Rectified Linear Unit (ReLU) activation function, ReLU(•), is applied element-wise to the result. Mathematically, ReLU is defined as ReLU(x) = max. Finally, a max-pooling operation, denoted as MaxPooling(•, p m , p n ), is performed on the output with a window size of p m × p n . We used a (2 × 2) pooling channel with a (3 × 3) kernel size of the convolution. We used 32 and 64 kernels for first and second CNN layers, respectively.\n\nConv operation we used as-\n\nThe symbol Y i,j,k denotes the element at position (i, j, k) in the output tensor. Similarly, W a,b,c,k represents the weight located at position (a, b, c, k) within the filter W . Moving to the input tensor, X i+a-1,j+b-1,c corresponds to the element situated at position (i + a -1, j + b -1, c). Additionally, b k is the bias term associated with the k-th filter of size (k m × k n ). Y flattened will be used for temporal module.\n\nb) Temporal Module: We use a hybrid architecture for emotion categorisation to take advantage of both Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM). The hybrid CNN-LSTM model combines the CNN's ability to extract features from data with the LSTM's ability to capture both long-term and short-term dependencies.\n\nThe output from the Flatten layer, denoted as\n\nflattened , is sequentially processed through two Long Short-Term Memory (LSTM) layers. Before the LSTM layers, a Repeat Sequence operation is applied. This operation, often used in sequence-tosequence tasks, repeats the output sequence from the Flatten Vector and prepares it for further processing. R represents the number of Repeat Sequence. We used R=4 for further processing of the Output received from the spatial module.\n\nflattened , R) H [1]  r , C [1]  r = LSTM(Y rep , H\n\nr-1 , C\n\nr-1 ) H [2]  r , C [2]  r = LSTM(H [1]  r , H\n\nr-1 , C\n\nr-1 )\n\nIn the equations, Y rep Repeat Sequence operation.\n\nr , C\n\n[1] r represent the hidden state and cell state of the first LSTM layer at repetition r, while\n\nr denote the corresponding states for the second LSTM layer. The LSTM operations involve gating mechanisms, such as input gates, forget gates, and output gates, influencing the flow of information and memory retention in the cell states. The first LSTM layer processes the repeated sequence, and its output, along with the previous hidden and cell states, becomes the input for the second LSTM layer.\n\nIn the provided equations, the final output of the second LSTM layer is denoted as\n\nr , representing the hidden state at the final time step r. This hidden state encapsulates the learned representation of the input sequence after processing through the first and second LSTM layers. H  [2]  r will be used as a feature representation and serves as the input for subsequent layers, which concatenates with the input from the deep fuzzy module. It captures the hierarchical dependencies and patterns in the sequential data, providing a meaningful representation for further analysis or decision-making.\n\n3) Integration of Deep Fuzzy Module: In our approach, we developed three models based on fuzzy representations of emotions. The aim is to use a computational approach for decision-making in the final classification. To achieve this, we utilized membership functions, unsupervised clustering, and supervised cubical probabilities. We developed three different models based on three distinct representations of emotion: firstly, degree of membership values (Type-1 and Type-2), secondly unsupervised fuzzy clusters, and thirdly, cuboid probabilistic lattice representation of the VAD dimensions. The models are schematically represented in Fig.  10 . The fuzzy module consists of a Fuzzifier sub-module, which converts the crisp VAD values into fuzzy membership degrees, a Model Selector, according to which membership degrees are assigned (more details about the membership degrees in individual models). and Fully Connected neural network. The last layer acts as an output for this module. The membership degree of the VAD Input is,\n\nwhere ϕ is a function Fuzzifier. This function is defined differently for each model, which is discussed later in this section.\n\nΓ is further processed through Dense layers with ReLU and Dropout layers inbuilt with (•) product in initial layers, while the final layer is processed differently with different models.\n\nwhere O [i]  FC and O [f]  FC are the outputs of the initial layers and final layers, respectively and W and b are the weight and bias, respectively.\n\na) Model-1: Model-1 uses the type-2 fuzzy membership function for ϕ in Eq.15. These membership functions are mentioned in Section-III with the equations from Eq.4 to Eq.9.\n\nb) Model-2: Model-2 is based on unsupervised clustering. We used the FCM algorithm to define the degrees of each clusters in the VAD space. Here ϕ in Eq.15 is updated as,\n\nwhere The membership degrees are represented by a matrix U where u ij denotes the degree of membership of data point i to cluster j. Here, i ranges from 1 to n (number of data points), and j ranges from 1 to c (number of clusters). c) Model-3: Model-3 uses the type-2 fuzzy membership function for ϕ in Eq.15 with the equations from Eq.4 to Eq.9. However, the output layer of the fuzzy framework module in this model is used as a dual output and optimized simultaneously with the main output layer. The second output is trained on the classification of VAD space in 27 classes, just like a cuboid is divided into low, medium, and high values of its dimension. We used Softmax in the last layer of this fuzzy module, i.e., Softmax is applied on O [f]  FC in Eq.  16 .\n\nwhere i represents the index of the element in the vector, and k is the number of dense units used in the final output of the layer, i.e., the number of VAD space classes used in the second output layer. And O [f]  FC = [o 1 , o 2 , . . . , o k ]. We took k=27. We call the vector [p 1 , p 2 , . . . , p k ] as cuboid probabilistic lattices as it contains the probability of the VAD space mapped to k cuboids. These lattices are the output of Model-3.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "C. Cross-Subject Study",
      "text": "In order to validate our model , we extended our work to test cross-subject emotion recognition. The cross-subject task is more complex in nature  [42] . Therefore, we simplified the task by grouping together a few emotions based on a tree-structured list  [43] ,  [44] . It is worth noticing here that we did not use VAD information to group these emotions to avoid creating any biases in the model. Three distinct groups are formed. Emotions considered in these three groups are mentioned in TableIII.\n\nWe split the samples in such a way that the training and validation sets do not have data from the same participants.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "We have created three models, as discussed earlier. We performed the experiments with all the models to verify the sanctity of our framework and methodology.\n\nModel-1 is tested with type-2 fuzzy membership degrees and 24 emotion classes. Model-2, which is based on the   We also explored cross-subject emotion classification, which is considered a relatively more challenging task. Results are shown in TableV. Results show that Group-1 vs Group-2 secures the best accuracy. Improvements when using a deep fuzzy framework are shown in Fig.  12 .\n\nAn ablation study with many experiments is also conducted to check the performance of our proposed deep fuzzy framework. Firstly, we evaluated the performance of our models under various conditions without the fuzzy framework and only the module working as a crisp VAD space. secondly, we removed all the VAD space and recognised the emotions with only spatial and temporal modules. Thirdly, individual UMF and individual LMF working as type-1 fuzzy are evaluated to check the performance of type-2 fuzzy sets with a comparison to type-1 sets. All the results of ablation study are shown in TableVI.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Discussion And Conclusion",
      "text": "The utilization of a fuzzy VAD space and the integration with the measured EEG data have helped in the development of a deep fuzzy framework for emotion representation and recognition. By introducing the fuzziness of type-2, the model becomes more adaptive to individual variations in emotional experience, enhancing its versatility across different experiments and diverse individual responses.\n\nThe DENS dataset, unlike others, has given an option to the subjects to choose the name and time of their emotional experience in the value of V,A and D. However, consideration of raw VAD values could be an oversimplification since mental activities may not be accurately represented using only three numbers. To address this issue, we fuzzified the VAD values   On the other hand, using type-1 representation, we obtained an accuracy of 95.82% and 94.66%, respectively, when we used these UMF and LMF separately working as type-1. This further supports our theory that this model works well as a generic model for emotion representation and recognition.\n\nCuboid probabilistic lattice-based representation (Model-3)(Section-IV-B3c)10 achieves an accuracy of 95.75% when input is given by a type-2 membership degree. However, the accuracy decreases to 95.16% and 95.22%, respectively, when independently using UMF and LMF as type-1 membership degrees. Nevertheless, this cuboid-based probability lattice is able to enhance the performance almost to the level of when UMF is used as type-1 in the previous model. This is due to the fact that this model is dual-output based, and cuboid probabilistic lattices are optimized for the next input to the EEG features, thereby improving performance.\n\nFinally, when it comes to unsupervised learning using a fuzzy C-means cluster-based model (Model-2)(Section-IV-B3b)10, the model's accuracy tends to fluctuate depending on the number of clusters used. To determine the optimal number of clusters, we can use the fuzzy Silhouette coefficient. In this case, the coefficient suggests that four clusters is the optimal number. Beyond five clusters, the accuracy increases a bit but again drops.\n\nWe conducted an ablation study to evaluate the performance of our model in two scenarios: with no VAD space integrated and with crisp VAD space integrated into the architecture. The results showed that when no VAD space was used, the model's performance was as low as 93.54%. However, with the crisp VAD space, the model's performance improved to 95.01%. Hence, supporting the need for a type-2 fuzzy framework. Additionally, incorporating both physiological data (EEG features) and subjective ratings enhances the model's ability to capture the multidimensional aspects of emotions, leading to more reliable recognition outcomes.\n\nIn cross-subject analysis, our model achieves an accuracy of 78.37%, which is also on par with the other recent works  [42] ,  [45] ,  [46] . Notably, our ablation study showed that changing no other parameters resulted in a significant improvement in accuracy from 72.97% to 78.37% (Fig.  12 ) if we add a fuzzy framework to the existing architecture. Generally, crosssubject studies do not yield significant accuracies  [45] ,  [47] ,  [48] . Also, a comparison of the other two groups shows a similar kind of improvement after adding the fuzzy framework to the existing architecture; however, they elicit a bit low accuracy but are still on par with the recent trends. This signifies the generalizability of the proposed architecture, showcasing its potential application across diverse circumstances. It is a critical step toward real-world applicability, and the study's success in this aspect further validates the robustness of the developed model.\n\nThe study deals with 24 emotions at a time, which is not only challenging but also the first of its kind. Typically, emotion recognition tasks involve identifying emotional dimensions such as low or high valence and arousal. Additionally, they often focus on a few basic emotions, such as happiness, sadness, anger, and joy. The number of emotional classes is also kept low. In this perspective, the study's outcomes have significant implications for emotion recognition research and applications. The generic nature of the developed model, along with its successful cross-subject predictions, gives direction for real-world applications in the field of affective computing, human-computer interaction, mental health monitoring, etc. Future research could explore the adaptation of the model to different cultural contexts and the integration of additional modalities for an even more comprehensive understanding of human emotions, including complex emotions.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotion Dimensions: A schematic diagram depicting",
      "page": 1
    },
    {
      "caption": "Figure 2: Emotion Distribution: Mean and Standard Deviation of all the emotions.",
      "page": 4
    },
    {
      "caption": "Figure 3: Emotion Distribution of DENS",
      "page": 4
    },
    {
      "caption": "Figure 4: VAD Distribution: A histogram of all emotional events in VAD space.",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotion Percent Distribution: Emotional Events Pie",
      "page": 5
    },
    {
      "caption": "Figure 6: Figure7 represent the four and six FCM clusters with",
      "page": 5
    },
    {
      "caption": "Figure 6: Emotion Distributions in All the four FCM partitions. Many emotions are partitioned in more than two clusters, having",
      "page": 6
    },
    {
      "caption": "Figure 7: Fuzzy C-Means with 4 and 6 Clusters: Analysis of Same Emotion in More than One Clusters.",
      "page": 7
    },
    {
      "caption": "Figure 8: , and membership degrees are",
      "page": 7
    },
    {
      "caption": "Figure 8: Membership",
      "page": 8
    },
    {
      "caption": "Figure 9: Deep Fuzzy Architecture: This architecture consists of three modules. The spatial module is based on CNNs, the",
      "page": 10
    },
    {
      "caption": "Figure 10: The fuzzy",
      "page": 11
    },
    {
      "caption": "Figure 10: A Schematic Representation of Developed Models: Models Based on Various Fuzzy Representations: The first model is",
      "page": 12
    },
    {
      "caption": "Figure 11: Comparison of Accuracies With Model-2 Based on Fuzzy C-Means",
      "page": 13
    },
    {
      "caption": "Figure 12: Cross Subject Accuracy Comparison- with and with-",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Group 1\nEmotions\n#": "Delighted\n11\nAmused\n14\nHappy\n5\nAdventurous\n20\nJoyous\n20\nExcited\n26",
          "Group 2\nEmotion\n#": "Melancholic\n5\nDepressed\n6\nDespondent\n10\nDissatisfied\n10\nMiserable\n19\nSad\n45",
          "Group 3\nEmotions\n#": "Taken Aback\n11\nStartled\n20\nDistress\n22\nAlarmed\n27\nAfraid\n46"
        },
        {
          "Group 1\nEmotions\n#": "Total\n108",
          "Group 2\nEmotion\n#": "Total\n95",
          "Group 3\nEmotions\n#": "Total\n126"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Groups": "G-1 vs G-2\nG-1 vs G-2",
          "Model": "Model-1\nModel-1",
          "Framework": "with fuzzy\nwithout\nfuzzy",
          "#Classes": "2\n2",
          "Accuracy": "78.35%\n72.97%"
        },
        {
          "Groups": "G-1 vs G-3\nG-1 vs G-3",
          "Model": "Model-1\nModel-1",
          "Framework": "with fuzzy\nwithout\nfuzzy",
          "#Classes": "2\n2",
          "Accuracy": "61.70%\n55.31%"
        },
        {
          "Groups": "G-2 vs G-3\nG-2 vs G-3",
          "Model": "Model-1\nModel-1",
          "Framework": "with fuzzy\nwithout\nfuzzy",
          "#Classes": "2\n2",
          "Accuracy": "64.44%\n60.00%"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Model-1",
          "Method": "without\nFuzzy\nframework\n(with\nCrisp VAD values)",
          "Classes": "24",
          "Accuracy": "95.01%"
        },
        {
          "Model": "Model-1",
          "Method": "without Fuzzy framework (without\nany VAD values)",
          "Classes": "24",
          "Accuracy": "93.54%"
        },
        {
          "Model": "Model-1\nModel-1",
          "Method": "UMF working as Type-1 Fuzzy\nLMF working as Type-1 Fuzzy",
          "Classes": "24\n24",
          "Accuracy": "95.82%\n94.65%"
        },
        {
          "Model": "Model-3\nModel-3",
          "Method": "UMF working as Type-1 Fuzzy\nLMF working as Type-1 Fuzzy",
          "Classes": "24\n24",
          "Accuracy": "95.16%\n95.22%"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional style questionnaire: A multidimensional measure of healthy emotionality",
      "authors": [
        "P Kesebir",
        "A Gasiorowska",
        "R Goldman",
        "M Hirshberg",
        "R Davidson"
      ],
      "year": "2019",
      "venue": "Psychological assessment"
    },
    {
      "citation_id": "2",
      "title": "Emotional dimensions of user experience: A user psychological analysis",
      "authors": [
        "P Saariluoma",
        "J Jokinen"
      ],
      "year": "2014",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "3",
      "title": "Are there basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions"
    },
    {
      "citation_id": "4",
      "title": "Basic emotions, complex emotions, machiavellian emotions",
      "authors": [
        "P Griffiths"
      ],
      "year": "2002",
      "venue": "Basic emotions, complex emotions, machiavellian emotions"
    },
    {
      "citation_id": "5",
      "title": "Are all \"basic emotions\" emotions? a problem for the (basic) emotions construct",
      "authors": [
        "A Ortony"
      ],
      "year": "2022",
      "venue": "Perspectives on psychological science"
    },
    {
      "citation_id": "6",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "7",
      "title": "Mixed emotions and coping: The benefits of secondary emotions",
      "authors": [
        "A Braniecka",
        "E Trzebińska",
        "A Dowgiert",
        "A Wytykowska"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "8",
      "title": "Modelling individual and crosscultural variation in the mapping of emotions to speech prosody",
      "authors": [
        "P Van Rijn",
        "P Larrouy-Maestri"
      ],
      "year": "2023",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "9",
      "title": "The social effects of emotions",
      "authors": [
        "G Van Kleef",
        "S Côté"
      ],
      "year": "2022",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "10",
      "title": "The roles of fluid intelligence and emotional intelligence in affective decision-making during the transition to early adolescence",
      "authors": [
        "D Li",
        "M Wu",
        "X Zhang",
        "M Wang",
        "J Shi"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "11",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "12",
      "title": "Application of deep belief networks in eeg-based dynamic music-emotion recognition",
      "authors": [
        "N Thammasan",
        "K -I. Fukui",
        "M Numao"
      ],
      "year": "2016",
      "venue": "2016 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Shalaby"
      ],
      "year": "2018",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "14",
      "title": "Recognizing emotions evoked by music using cnn-lstm networks on eeg signals",
      "authors": [
        "S Sheykhivand",
        "Z Mousavi",
        "T Rezaii",
        "A Farzamnia"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition using temporally localized emotional events in eeg with naturalistic context: Dens dataset",
      "authors": [
        "M Asif",
        "S Mishra",
        "M Vinodbhai",
        "U Tiwary"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Hierarchical attention-based temporal convolutional networks for eeg-based emotion recognition",
      "authors": [
        "C Li",
        "B Chen",
        "Z Zhao",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based emotion recognition using convolutional recurrent neural network with multi-head selfattention",
      "authors": [
        "Z Hu",
        "L Chen",
        "Y Luo",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "18",
      "title": "Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification",
      "authors": [
        "S Gannouni",
        "A Aledaily",
        "K Belwafi",
        "H Aboalsamh"
      ],
      "year": "2021",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition from eeg signals using empirical mode decomposition and second-order difference plot",
      "authors": [
        "N Salankar",
        "P Mishra",
        "L Garg"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "20",
      "title": "Hybrid fuzzy deep neural network toward temporal-spatial-frequency features learning of motor imagery signals",
      "authors": [
        "M Sorkhi",
        "M Jahed-Motlagh",
        "B Minaei-Bidgoli",
        "M Daliri"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "21",
      "title": "A comprehensive review of deep neuro-fuzzy system architectures and their optimization methods",
      "authors": [
        "N Talpur",
        "S Abdulkadir",
        "H Alhussian",
        "• Hasan",
        "N Aziz",
        "A Bamhdi"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "22",
      "title": "Dataset on emotions using naturalistic stimuli (dens)",
      "authors": [
        "S Mishra",
        "M Asif",
        "U Tiway"
      ],
      "year": "2021",
      "venue": "Dataset on emotions using naturalistic stimuli (dens)"
    },
    {
      "citation_id": "23",
      "title": "An approach to environmental psychology",
      "authors": [
        "A Mehrabian",
        "J Russell"
      ],
      "year": "1974",
      "venue": "An approach to environmental psychology"
    },
    {
      "citation_id": "24",
      "title": "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "25",
      "title": "Cardiac-brain dynamics depend on context familiarity and their interaction predicts experience of emotional arousal",
      "authors": [
        "S Mishra",
        "N Srinivasan",
        "U Tiwary"
      ],
      "year": "2022",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "26",
      "title": "Dynamic functional connectivity of emotion processing in beta band with naturalistic emotion stimuli",
      "year": "2022",
      "venue": "Brain sciences"
    },
    {
      "citation_id": "27",
      "title": "Independence and bipolarity in the structure of current affect",
      "authors": [
        "L Barrett",
        "J Russell"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "28",
      "title": "Are emotions natural kinds?",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "Perspectives on psychological science"
    },
    {
      "citation_id": "29",
      "title": "Arousal focus and interoceptive sensitivity",
      "authors": [
        "L Barrett",
        "K Quigley",
        "E Bliss-Moreau",
        "K Aronson"
      ],
      "year": "2004",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "30",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "S Kreibig"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "31",
      "title": "The relation between valence and arousal in subjective experience",
      "authors": [
        "P Kuppens",
        "F Tuerlinckx",
        "J Russell",
        "L Barrett"
      ],
      "year": "2013",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "32",
      "title": "The neural correlates of the dominance dimension of emotion",
      "authors": [
        "M Jerram",
        "A Lee",
        "A Negreira",
        "D Gansler"
      ],
      "year": "2014",
      "venue": "Psychiatry Research: Neuroimaging"
    },
    {
      "citation_id": "33",
      "title": "Affective film dataset from india (afdi): creation and validation with an indian sample",
      "authors": [
        "S Mishra",
        "N Srinivasan",
        "M Asif",
        "U Tiwary"
      ],
      "year": "2023",
      "venue": "Journal of Cultural Cognitive Science"
    },
    {
      "citation_id": "34",
      "title": "A fuzzy extension of the silhouette width criterion for cluster analysis",
      "authors": [
        "R Campello",
        "E Hruschka"
      ],
      "year": "2006",
      "venue": "Fuzzy Sets and Systems"
    },
    {
      "citation_id": "35",
      "title": "Emotion recognition in vad space during emotional events using cnngru hybrid model on eeg signals",
      "authors": [
        "M Asif",
        "M Vinodbhai",
        "S Mishra",
        "A Gupta",
        "U Tiwary"
      ],
      "year": "2022",
      "venue": "International Conference on Intelligent Human Computer Interaction"
    },
    {
      "citation_id": "36",
      "title": "Flame-fuzzy logic adaptive model of emotions",
      "authors": [
        "M El-Nasr",
        "J Yen",
        "T Ioerger"
      ],
      "year": "2000",
      "venue": "Autonomous Agents and Multi-agent systems"
    },
    {
      "citation_id": "37",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "2022",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "38",
      "title": "Continuous measurement of emotion",
      "authors": [
        "A Ruef",
        "R Levenson"
      ],
      "year": "2007",
      "venue": "Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "39",
      "title": "An emotional model based on fuzzy logic and social psychology for a personal assistant robot",
      "authors": [
        "G Fernández-Blanco Martín",
        "F Matía",
        "L García Gómez-Escalonilla",
        "D Galan",
        "M Sánchez-Escribano",
        "P De La Puente",
        "M Rodríguez-Cantelar"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "40",
      "title": "Same situation-different emotions: how appraisals shape our emotions",
      "authors": [
        "M Siemer",
        "I Mauss",
        "J Gross"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "41",
      "title": "Iclabel: An automated electroencephalographic independent component classifier, dataset, and website",
      "authors": [
        "L Pion-Tonachini",
        "K Kreutz-Delgado",
        "S Makeig"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "42",
      "title": "Inter subject emotion recognition using spatio-temporal features from eeg signal",
      "authors": [
        "M Asif",
        "D Srivastava",
        "A Gupta",
        "U Tiwary"
      ],
      "year": "2023",
      "venue": "2023 27th International Computer Science and Engineering Conference (ICSEC)"
    },
    {
      "citation_id": "43",
      "title": "Emotion knowledge: further exploration of a prototype approach",
      "authors": [
        "P Shaver",
        "J Schwartz",
        "D Kirson",
        "C O'connor"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "44",
      "title": "Emotions in social psychology: Key readings",
      "authors": [
        "W Parrott"
      ],
      "year": "2000",
      "venue": "Emotions in social psychology: Key readings"
    },
    {
      "citation_id": "45",
      "title": "Sect: A method of shifted eeg channel transformer for emotion recognition",
      "authors": [
        "Z Bai",
        "F Hou",
        "K Sun",
        "Q Wu",
        "M Zhu",
        "Z Mao",
        "Y Song",
        "Q Gao"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "46",
      "title": "Subject-independent eeg emotion recognition with hybrid spatio-temporal gru-conv architecture",
      "authors": [
        "G Xu",
        "W Guo",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Medical & Biological Engineering & Computing"
    },
    {
      "citation_id": "47",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "48",
      "title": "Locally robust eeg feature selection for individual-independent emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "J Chen",
        "B Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    }
  ]
}