{
  "paper_id": "2303.03186v1",
  "title": "Department: Affective Computing And Sentiment Analysis",
  "published": "2023-03-03T16:11:37Z",
  "authors": [
    "Mostafa M. Amin",
    "Erik Cambria",
    "Björn W. Schuller"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilise three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results, and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where Word2Vec models achieve worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialised training, however, it is not as good as a specialised model for a downstream task. WITH THE ADVENT of increasingly largedata trained general purpose machine learning models, a new era of 'foundation models' has started. According to  [1] , these are marked by having been trained on 'broad' data -often selfsupervised -at scale leading to a) homogenisation (i. e., most use the same model for fine-tuning and training for down-stream tasks as they are effective \n IEEE Intelligent Systems 38(1) This article is part of the IEEE IS ACSA series",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We focus on related work within the key research question of potential emergence (in the text domain) by foundation models. In particular,  [7]  explores the question if ChatGPT is a general NLP solver that works for all problems. They explore a wide range of tasks, like reasoning, text summarisation, named entity recognition, and sentiment analysis.  [8]  explores the capabilities of GPT language models (including ChatGPT) in Machine Translation.  [5]  explores the systematic errors of ChatGPT.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "The aim of this paper is to evaluate the generalisation capabilities of ChatGPT across a wide range of affective computing tasks. In order to assess this, we utilise three datasets corresponding to three different problems, as mentioned: bigfive personality prediction, sentiment analysis, and suicide tendency assessment. For these tasks, we utilise three datasets. against three baselines, namely a large language model, a word model with pre-trained embeddings, a basic bag-of-words model without making use of any external data. We describe the datasets, querying procedure of ChatGPT, and the baselines in this section. Figure  1  demonstrates the pipelines of all methods (ChatGPT and the three baselines).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "We introduce the three datasets in this Section. A summary of their statistics is presented in Table  1 . We utilise publicly available datasets for reproducibility.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Personality Dataset",
      "text": "We utilise the First Impressions (FI) dataset  [9] ,  [10]  for the personality task  1  . Personality is represented by the Big-five personality traits (OCEAN), namely, Openness to experience, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. The dataset consists of 15 seconds videos with one speaker, whose personality was manually labelled. Such labelling was conducted by relative comparisons between pairs of videos, by ranking which person scores higher on each one of the big-five personality traits. A statistical model was then used to reduce the labels into regression values within the range [0, 1]. The personality labels were given based on the multiple modalities of a video, namely, images, audio, and text (content). We utilise the transcriptions of these videos as the input to be used to predict personality from. We use the Train/Dev/Test split given by the publishers of the dataset  [9] ,  [10] . Like  [11] , we train the models on this dataset as a regression problem (by using Mean Absolute Error as loss function), since the labels can give a more granular estimation of the labels; then, we binarise these to positive or negative using the threshold 0.5.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sentiment Dataset",
      "text": "We adopt the Senti-ment140 dataset  [12]  for the sentiment analysis task  2  . The dataset is collected from tweets on Twitter, which makes the text very noisy, which can pose a challenge against many models (especially word models). The dataset consists of tweets and the corresponding sentiment labels (positive, or negative). We split the training portion with a ratio 9:1 to give the Train and Dev portions 3  listed in Table  1 . The Test portion consists of 497 tweets only, however, these were filtered down to 359 because the remaining have a neutral label which is not present in the training set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Suicide And Depression Dataset",
      "text": "The Suicide and Depression dataset  [13]  is collected from the Reddit platform, under different subreddits categories, namely \"SuicideWatch\", \"depression\", and \"teenagers\".  4  The texts of the posts from the \"teenagers\" category are labelled as negative, while the texts from the other two categories are labelled as positive. We excluded examples longer than 512 characters, then divided the dataset into three portions Train, Dev, and Test.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Chatgpt Querying Mechanism",
      "text": "We introduce the stages of querying ChatGPT as shown in Figure  1 . The general mechanism to collect for our experiments is achieved by the following procedure for each problem:\n\n1) Reformat all the texts of the Test portion of the dataset, by using a format that asks ChatGPT what is their guess about the label of the text. The formatting in the first step and the postprocessing in the last step are specified in the following two Subsections. We used the version of ChatGPT released on 30.01.2023 5 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Question Formulation",
      "text": "The formats that are used for the three problems are given by the following snippets. The example text is substituted in place of the {text} part, however, the quotation marks are kept since it specifies for ChatGPT that this a placeholder used by the question being asked. The formulations for the three problems are given by: 1) For the Big-five personality traits, we formulate the question:\n\n\"What is your guess for the big-five personality traits of someone who said \"{text}\", answer low or high with bullet points for the five traits? It does not have to be fully correct. You do not need to explain the traits. Do not show any warning after.\" 2) For the sentiment analysis, we formulate the question:\n\n\"What is your guess for the sentiment of the text \"{text}\", answer positive, neutral, or negative? it does not have to be correct. Do not show any warning after.\" 3) For the suicide problem, we formulate the following question: 5 ChatGPT release notes https://help.openai.com/en/articles/ 6825453-chatgpt-release-notes \"What is your guess if a person is saying \"{text}\" has a suicide tendency or not, answer yes or no? it does not have to be correct. Do not show any warning after.\" The formulation of the question is of crucial importance to the answer ChatGPT will give; we encountered the following aspects: 1) Asking the question directly without asking about a guess made ChatGPT in many instances to answer that there is little information provided to answer the question, and it cannot answer it exactly. Hence, we ask it to guess the answer, and we declare that it is acceptable to be not fully accurate.\n\n2) It is important to ask what the guess is and not Can you guess, because this can give a response similar to 1., where it responds with an answer that starts with No, I cannot accurately answer whether.... Therefore, the question needs to be assertive and specific.\n\n3) We need to specify the exact output format, because ChatGPT can get innovative otherwise about the formatting of the answer, which can make it hard to collect the answers for our experiment. Despite specifying the format, it still sometimes gave different formats. We elaborate in the next Subsection. 4) The questions for the suicide assessment task triggered warnings in the responses of ChatGPT due to its sensitive content. We elaborate on the terms of use in the Acknowledgement Section.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parsing Responses",
      "text": "The responses of Chat-GPT need to be parsed, since ChatGPT can give arbitrary formats for a given answer, even when the content is the same. This is predominant in the personality traits, since there are five traits. Sometimes the answers are listed as bullet points, other times they are all in one comma-separated line. Also, it used different delimiters or order, e. g. , \"Openness: Low\", or \"Low in Openness\", and \"Low: Openness\". Additionally, in all problems, it sometimes gives an introduction for the answer, for example, \"Here is my guess for ..\", or \"Based on the statement\". We encounter this issue by using regular expressions to capture the responses.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baselines",
      "text": "In order to compare the performance of Chat-GPT on the different tasks, we need to use baselines and train them on the Train portion (while validating on the Dev portion). We employ three baselines, which serve as the specialised models specifically tailored for the corresponding downstream task. The first baseline is a robust language model (RoBERTa) trained on a large amount of text. The second is a simple baseline which uses a word model by employing pretrained Word2Vec embeddings on the words of a sentence, with a simple classifier. The third baseline is a simple Bag-of-Words (BoW) model that utilises a linear classifier. The hyperparameters of all models are optimised by selecting the hyperparameters yielding the best performance on the Dev portion. The hyperparameters are tuned using the SMAC toolkit  [14] , which is based on Bayesian Optimisation. The selected hyperparameters are listed in Table  2 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Roberta Language Model",
      "text": "The baseline RoBERTa  [15]  is a pretrained BERT model, which has a transformer architecture.  [15]  trained two instances of RoBERTa; we use the smaller one, namely RoBERTa-base 6  , consisting of 110 M parameters. RoBERTa-base is pretrained on a mixture of several large datasets that included books, English Wikipedia, English news, Reddit posts, and stories  [15] . The model starts by tokenising a text using subword encoding, which is a hybrid representation between character-based and word-based encodings. The tokens are then fed to RoBERTa to obtain a sequence of embeddings. The pooling layer of RoBERTa is then used to reduce the embeddings into one embedding only, hence acquiring a static feature vector of size 768 representing the text. We train additionally a Multi-Layer Perceptron (MLP)  [16]  to predict the final label. The pipeline for the model is shown in Figure  1 .\n\nFor the training procedure, we use SMAC  [14]  to select the MLP specifications. We employ SMAC to sample a total of 100 models per task, and train them with a batch size 256 for 300 epochs with early stopping to prune the ineffective models. Eventually, the model with best performance on the Dev set is selected. The hyperparameter space consists of four hyperparameters: the number of hidden layers N ∈ [0, 3], the number of neurons in the first hidden layer U ∈ [64, 512] (log sampled), the optimisation algorithm (Adam  [17]  or Stochastic Gradient Descent (SGD)  [16] ), and the learning rate α ∈ [10 -6 , 10] (log sampled). The number of neurons in the hidden layers is specified by the first one as a hyperparameter, then, the number of neurons is halved for each subsequent hidden layer (clipped to be at least 32). The hidden layers have Recitified Linear Unit (ReLU) as an activation function. The final layer has a sigmoid activation function. The loss function for classification is crossentropy, and mean absolute error for regression.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Word2Vec Word Embeddings",
      "text": "The baseline Word2Vec  [18] ,  [19]  makes use of pretrained word embeddings  7  , which are trained on a large amounts of text from Google News. The model operates by tokenising a given text into words, each word is assigned an embedding from the pretrained embeddings. The embeddings are then averaged for all words to give a static feature vector of size 300 for the entire string. A Support Vector Machine (SVM)  [16]  is then used to predict the given task. The pipeline of this model is shown in Figure  1 .\n\nWe train the SVM model by tuning its hyperparmeter C using SMAC  [14] , by sampling 20 values within the range [10 -6 , 10 4 ] (log sampled), and choosing the model that yields the best score on the Dev set. We use Radial Basis Function (RBF) kernel for the SVMs, except for the sentiment dataset, where we apply a linear kernel as the sentiment dataset is much bigger (as shown in Table  1 ) which renders the RBF impractical due to the computational efficiency.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Bag Of Words",
      "text": "The Bag-of-Words (BoW) model is a very simple baseline, which does not rely on any knowledge transfer or large-scale training. In particular, it uses only in-domain data for training and no other data for either up-or downstreaming.\n\nWe utilise the classical technique term frequency -inverse document frequency (TF-IDF), which tokenises the sentences into words, then, a sentence is represented by a vector of the counts of the words it contains. The vector is then normalised by the term frequency across the entire Train set of the corresponding dataset. We restrict the words to the most common 10,000 words the Train set, then we scale each feature to be within [-1, 1], by dividing by the maximum absolute value of the feature across the Train set. We optimise a linear kernel SVM, we optimise using SGD  [16]  due to the high number of features (10,000 features). We tune the learning rate η of SGD using SMAC  [14] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "In this section, we review the results of our experiments. In summary, we evaluate the performance of ChatGPT against the three baselines RoBERTa, Word2Vec, and BoW on three downstream classification tasks, namely personality traits, sentiment analysis, and suicide tendency assessment. We measure classification accuracy and Unweighted Average Recall (UAR)  [20]  as performance measures. UAR has an advantage of exposing if a model is performing very well on a class on the expense of the other class, especially in imbalanced datasets. Additionally, we utilise randomised permutation test as a statistical significance test  [21] . The main results of the experiments are shown in Table  3 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "The RoBERTa model is achieving the best performance for the personality and suicide assessment tasks, with a statistically significant improvement of accuracy over ChatGPT. However, ChatGPT is the best in sentiment analysis, but only slightly better than RoBERTa. The UAR for the personality traits point to similar conclusions about the relative performance, however, they yield much lower values for all baselines on some of the traits (openness and agreeableness). The UAR measure generally yields similar results on all models for both sentiment analysis, and suicide assessment. The performance of ChatGPT on the personality assessment is inferior to the three baselines on the all traits. It is significantly worse than RoBERTa on all traits, and Word2Vec in three traits.\n\nChatGPT has the best performance in the sentiment analysis, where it is slightly better than RoBERTa and BoW and significantly better than Word2Vec. One of the potential reasons of the inferiority of Word2Vec and BoW on the sentiment dataset is not using subword encodings. The reason is that, the sentiment dataset is collected from Twitter, so it is very noisy, which can lead to many mistakes in identifying the words and hence assigning them the proper embeddings. Subword encoding avoids many of these issues, since few typos would still yield a meaningful subword representation of the given sentence.\n\nThe results on the suicide assessment problem show the contrast between the aforementioned analyses. The task is not as hard as the personality assessment problem, with a much bigger amount of training data. The suicide assessment can rather be thought of as classifying extreme negative sentiment, where  [7]  showed that ChatGPT is better at predicting negative sentiment than positive. However, the texts of the suicide dataset are much less noisy compared to the sentiment dataset. In that case, the performance of the Word2Vec and BoW models are more or less on par with the ChatGPT model, while RoBERTa is significantly better than all of them.\n\nOur experiments indicate that ChatGPT has a decent performance across many tasks (especially sentiment analysis, or similar), which is comparable to simple specialised models that solve a downstream task. However, it is not competent enough as compared to the best specialised model to solve the same downstream task (e. g. , finetuned RoBERTa). The performance of ChatGPT does not generally show statistically significant differences when compared to the simplest baseline BoW, which does not make any use of pretraining. This is further confirmed by  [8]  in machine translation, and other tasks  [7] . In summary, our study suggests that ChatGPT is a generalist model (in contrast to a specialised model), that can decently solve many different problems without specialised training. However, in order to achieve the best results on specific downstream tasks, dedicated training is still required. This might be enhanced in future versions of ChatGPT and alikes by including more diverse tasks for the Reinforcement Learning Human Feedback (RLHF) component in the training.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "The most crucial limitation of the presented results is the small amount of data for evaluation (497, 362, and 509 examples for the three tasks), since ChatGPT is only available for manual entries by the consumers and not for automated largescale testing. Additionally, it only responds to approximately 25-35 requests per hour, in order to reduce the computational cost and avoid brute forcing. Another issue that may limit future experi-ments is parsing the responses. In our experiments, ChatGPT responded with arbitrary formatting despite specifying the desired format explicitly in the question prompt.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we provided first insight into the potential 'full' emergence of tasks by broad-data trained foundation models. We approached this from the perspective of natural language tasks in the Affective Computing domain, and chose ChatGPT as exemplary foundation model. To this end, we introduced a framework to evaluate the performance of ChatGPT as a generalist foundation model against specialised models on a total of seven classification tasks from three affective computing problems, namely, personality assessment, sentiment analysis, and suicide tendency assessment.\n\nWe compared the results against three baselines, which reflect training the downstream tasks, and using or not using additional data for the upstream task. The first model was RoBERTa, a large-scale-trained transformer-based language model, the second was Word2Vec, a deep learning model trained to reconstruct linguistic contexts of words, and the third was a simple bag-of-words (BoW) model.\n\nThe experiments have shown that ChatGPT is a generalist model that has a decent performance on a wide range of problems without specialised training. ChatGPT showed superior performance in sentiment analysis, and poor performance on personality assessment, and average performance on suicide assessment. In other words, we could demonstrate genuine emergence properties potentially rendering future efforts to collect taskspecific databases increasingly obsolete.\n\nHowever, the performance of ChatGPT is not particularly impressive, since it did not show statistically significant differences with a simple BoW model in almost all cases. On the other hand, RoBERTa fine-tuned for a specific task had significantly better performance as compared to ChatGPT on the given tasks, which suggests that despite the generalisation abilities of ChatGPT, specialised models are still the best option for optimal performance. However, this can be taken into consideration in future developments of foundation models like ChatGPT, in order to yield wider exploration spaces for training.\n\nIn the near future, we will extend our experiments to more metrics, e. g. , explainability and computational efficiency, on top of accuracy and UAR. We also plan to expand our comparative evaluation to more sophisticated models, e. g. , prompt-based classification  [22]  and neurosymbolic AI  [23] , more advanced affective computing tasks, e. g. , sarcasm detection  [24]  and metaphor understanding  [25] , but also more complex sentiment datasets requiring commonsense reasoning and/or narrative understanding.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Acknowledgement",
      "text": "We would like to thank OpenAI for the usage of ChatGPT. We followed the policy of ChatGPT 8 . Our use of ChatGPT is purely for research purposes to assess emerging capabilities of foundation models, and does not promote the use of ChatGPT in any way that violates the aforementioned usage policy; in particular, with regards to the subject of self-harm; note that some of the examples in the datasets we use triggered a related warning by ChatGPT.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: demonstrates the pipelines",
      "page": 3
    },
    {
      "caption": "Figure 1: The general mechanism",
      "page": 3
    },
    {
      "caption": "Figure 1: Pipelines of the ChatGPT (top), RoBERTa baseline (second), Word2Vec baseline (third), and BoW",
      "page": 4
    },
    {
      "caption": "Figure 1: For the training procedure, we use SMAC [14]",
      "page": 5
    },
    {
      "caption": "Figure 1: We train the SVM model by tuning its hyper-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Statistics on the sizes of the datasets, with",
      "data": [
        {
          "Dataset": "O\nC\nE\nA\nN",
          "Train": "6,000",
          "Dev": "2,000",
          "Test": "509",
          "+ve": "333\n286\n214\n340\n274",
          "-ve": "176\n223\n295\n169\n235"
        },
        {
          "Dataset": "Sent",
          "Train": "1,440,144",
          "Dev": "159,856",
          "Test": "359",
          "+ve": "182",
          "-ve": "177"
        },
        {
          "Dataset": "Sui",
          "Train": "138,479",
          "Dev": "6,270",
          "Test": "496",
          "+ve": "165",
          "-ve": "331"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: p/word2vec/",
      "data": [
        {
          "RoBERTa": "N",
          "W2V": "C",
          "BoW": "η"
        },
        {
          "RoBERTa": "2",
          "W2V": "0.0378\n0.0472\n0.0069\n0.0218\n0.0657",
          "BoW": "2.47 × 10−3\n3.09 × 10−6\n1.09 × 10−5\n4.65 × 10−4\n2.21 × 10−6"
        },
        {
          "RoBERTa": "3",
          "W2V": "0.0144",
          "BoW": "5.25 × 10−6"
        },
        {
          "RoBERTa": "3",
          "W2V": "10.00",
          "BoW": "4.71 × 10−6"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: The classification accuracy and Unweighted Average Recall (in %) of ChatGPT against the baselines on",
      "data": [
        {
          "Accuracy": "ChatGPT",
          "Unweighted Average Recall": "ChatGPT"
        },
        {
          "Accuracy": "46.6\n57.4\n55.2\n44.8\n47.2",
          "Unweighted Average Recall": "50.1\n57.7\n54.0\n48.4\n49.1"
        },
        {
          "Accuracy": "85.5",
          "Unweighted Average Recall": "85.5"
        },
        {
          "Accuracy": "92.7",
          "Unweighted Average Recall": "91.2"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On the Opportunities and Risks of Foundation Models",
      "authors": [
        "R Bommasani"
      ],
      "year": "2021",
      "venue": "On the Opportunities and Risks of Foundation Models"
    },
    {
      "citation_id": "2",
      "title": "ChatGPT gained 1 million users in under a week. Here's why the AI chatbot is primed to disrupt search as we know it",
      "authors": [
        "S Mollman"
      ],
      "year": "2022",
      "venue": "ChatGPT gained 1 million users in under a week. Here's why the AI chatbot is primed to disrupt search as we know it"
    },
    {
      "citation_id": "3",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback"
    },
    {
      "citation_id": "4",
      "title": "Scaling Laws for Reward Model Overoptimization",
      "authors": [
        "L Gao",
        "J Schulman",
        "J Hilton"
      ],
      "year": "2022",
      "venue": "Scaling Laws for Reward Model Overoptimization"
    },
    {
      "citation_id": "5",
      "title": "A Categorical Archive of ChatGPT Failures",
      "authors": [
        "A Borji"
      ],
      "year": "2023",
      "venue": "A Categorical Archive of ChatGPT Failures"
    },
    {
      "citation_id": "6",
      "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
      "authors": [
        "C Zhou"
      ],
      "year": "2023",
      "venue": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"
    },
    {
      "citation_id": "7",
      "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
      "authors": [
        "C Qin"
      ],
      "year": "2023",
      "venue": "arxiv"
    },
    {
      "citation_id": "8",
      "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
      "authors": [
        "A Hendy"
      ],
      "year": "2023",
      "venue": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Chalearn lap 2016: First Round Challenge on First Impressions -Dataset and Results",
      "authors": [
        "V Ponce-L Ópez"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "10",
      "title": "Design of an Explainable Machine Learning Challenge for Video Interviews",
      "authors": [
        "H Escalante"
      ],
      "year": "2017",
      "venue": "International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "11",
      "title": "Multi-Modal Score Fusion and Decision Trees for Explainable Automatic Job Candidate Screening From Video CVs",
      "authors": [
        "H Kaya",
        "F Gurpinar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, IEEE"
    },
    {
      "citation_id": "12",
      "title": "Twitter Sentiment Classification using Distant Supervision",
      "authors": [
        "A Go",
        "R Bhayani",
        "L Huang"
      ],
      "year": "2009",
      "venue": "CS224N project report"
    },
    {
      "citation_id": "13",
      "title": "Suicide and Depression Detection in Social Media Forums",
      "authors": [
        "V Desu"
      ],
      "year": "2022",
      "venue": "Smart Intelligent Computing and Applications"
    },
    {
      "citation_id": "14",
      "title": "SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization",
      "authors": [
        "M Lindauer"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "15",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu"
      ],
      "year": "1907",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "citation_id": "16",
      "title": "Pattern Recognition and Machine Learning",
      "authors": [
        "C Bishop"
      ],
      "year": "2006",
      "venue": "Pattern Recognition and Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, Conference Track Proceedings, ICLR"
    },
    {
      "citation_id": "18",
      "title": "Efficient Estimation of Word Representations in Vector Space",
      "authors": [
        "T Mikolov"
      ],
      "year": "2013",
      "venue": "Efficient Estimation of Word Representations in Vector Space"
    },
    {
      "citation_id": "19",
      "title": "Distributed Representations of Words and Phrases and their Compositionality",
      "authors": [
        "T Mikolov"
      ],
      "venue": "C. Burges IEEE Intelligent Systems"
    },
    {
      "citation_id": "20",
      "title": "Advances in Neural Information Processing Systems",
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "The INTERSPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism",
      "authors": [
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings INTERSPEECH, ISCA"
    },
    {
      "citation_id": "22",
      "title": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses",
      "authors": [
        "P Good"
      ],
      "year": "1994",
      "venue": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses"
    },
    {
      "citation_id": "23",
      "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-based Sentiment Analysis and Emotion Detection",
      "authors": [
        "R Mao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "SenticNet 7: A Commonsense-based Neurosymbolic AI Framework for Explainable Sentiment Analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2022",
      "venue": "LREC"
    },
    {
      "citation_id": "25",
      "title": "Sentiment and Sarcasm Classification with Multitask Learning",
      "authors": [
        "N Majumder"
      ],
      "year": "2019",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "26",
      "title": "Explainable Metaphor Identification Inspired by Conceptual Metaphor Theory",
      "authors": [
        "M Ge",
        "R Mao",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th AAAI Conference on Artificial Intelligence"
    }
  ]
}