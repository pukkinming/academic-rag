{
  "paper_id": "2101.06634v1",
  "title": "Regional Attention Network (Ran) For Head Pose And Fine-Grained Gesture Recognition",
  "published": "2021-01-17T10:14:28Z",
  "authors": [
    "Ardhendu Behera",
    "Zachary Wharton",
    "Morteza Ghahremani",
    "Swagat Kumar",
    "Nik Bessis"
  ],
  "keywords": [
    "Gesture Recognition",
    "Head Pose Recognition",
    "Facial Expressions Recognition",
    "Attention Mechanism",
    "Convolutional Neural Network",
    "Fine-grained Gesture Recognition",
    "Computer Vision",
    "Human-Object Interaction",
    "Regional Attention Network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affect is often expressed via non-verbal body language such as actions/gestures, which are vital indicators for human behaviors. Recent studies on recognition of fine-grained actions/gestures in monocular images have mainly focused on modeling spatial configuration of body parts representing body pose, human-objects interactions and variations in local appearance. The results show that this is a brittle approach since it relies on accurate body parts/objects detection. In this work, we argue that there exist local discriminative semantic regions, whose \"informativeness\" can be evaluated by the attention mechanism for inferring fine-grained gestures/actions. To this end, we propose a novel end-to-end Regional Attention Network (RAN), which is a fully Convolutional Neural Network (CNN) to combine multiple contextual regions through attention mechanism, focusing on parts of the images that are most relevant to a given task. Our regions consist of one or more consecutive cells and are adapted from the strategies used in computing HOG (Histogram of Oriented Gradient) descriptor. The model is extensively evaluated on ten datasets belonging to 3 different scenarios: 1) head pose recognition, 2) drivers state recognition, and 3) human action and facial expression recognition. The proposed approach outperforms the state-of-the-art by a considerable margin in different metrics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A FFECT refers to the underlying experience of feeling, emotion or mood. Affect and its physical expression are an integral part of social interaction, informing others about how we are feeling and influencing social outcomes. It is often displayed via facial expressions, head pose/movements, hand gestures, body posture, voice characteristics, and other physical manifestations  [1] . Observers are capable of recognizing these affect displays, and often react to and draw inferences from them. The mapping of affective states onto behavioral cues is a complex problem involving numerous factors, and psychologists attempt to establish links between them without relying on subjective self-report as a primary measure. The mechanization of this process is fundamental in affective computing. Therefore, research on automatic recognition of nonverbal behavior/gestures is only the first step. This has significantly influenced the automatic recognition of nonverbal behavior in images and videos to address this fundamental problem in affective computing. Automatic recognition of human gestures/actions and nonverbal body language is wellresearched within computer vision community  [2] ,  [3]  and is instrumental for various applications such as socially assistive robots/AI, human-computer interactions, affectaware technologies, autonomous vehicles, sign language recognition, virtual reality, and many more.\n\nLearning to predict fine-grained gestures from a single monocular photographic image is arguably a more challenging problem and comparably less studied. The difficulty of the problem could be linked to the lack of temporal information, which often plays a key role in video-based action recognition. Recently, it has gained increased attentions in the research community due to the great success of deep learning methods. In this work, fine-grained gestures refer to the task of distinguishing sub-ordinate categories, such as head pose, facial expression and driver's in-vehicle activities recognition in which the difference between finegrained classes is very subtle. In such scenarios, the most discriminative cues are often not based on the global shape/appearance variation but contained in the misalignment of local parts or patterns. For example, recognition of head poses for human attention, holding versus playing a musical instrument and measuring driver's inattention by recognizing distraction activities of texting versus talking using a mobile phone.\n\nPrevious researches on action/gesture recognition from still images focus on body parts and their spatial configurations representing body pose and human-objects interactions  [4]  [5]  [6] . Therefore, most of these works aim at modeling contextual information involving human body pose and their interaction with objects/scenes for action recognition. This, in turn, requires explicit annotations of body pose (e.g. body parts/joints locations and bounding boxes) and objects on top of image-level action annotation (e.g. playing a flute and texting). Manual annotations of these bounding boxes are not only tedious, laborious, and time-consuming, but also demand special skills which are expensive and not readily available.\n\nOver the last couple of years, attention mechanism  [7]  has drawn increasing interest in machine translation  [8]    [9] , visual question answering (VQA)  [10] , image captioning  [11]  [12]  [13] , human activities recognition  [14]  [15]  [16] , and other applications. The aim is to imitate human perception by focusing on parts of the scenes/sentences to acquire information at specific places and times, resulting in improved accuracy, as the model can focus on parts of the data, which are most relevant to a given task. Such models usually take image-level labels (e.g. kicking, riding, and phoning) without requiring as input manual annotations of bounding-boxes for human and/or objects of interest.\n\nThe core goal of this work is to develop a simple yet a powerful network involving the attentional layer that can be added on top of the existing Convolutional Neural Networks (CNNs) to learn attention maps exploiting the effective spatial support of the visual information in making fine-grained action classification decisions. The proposed attentional module does not require additional annotation/supervision. It leads to significant improvements in classification accuracy over the baseline architectures and state-of-the-art approaches on three separate fine-grained action/gesture recognition tasks: 1) head pose, 2) driver's distraction activities, and 3) human actions and facial expressions in still images. The method is based on the hypothesis that there is a benefit to exploring salient regions and amplifying their influence while suppressing the potentially noisy and irrelevant information in other regions. In particular, we reveal that enforcing a more focused and parsimonious use of image information could efficiently aid in discriminating subtle changes that are often observed in fine-grained action recognition tasks. Therefore, the proposed end-to-end attention-aware fine-grained classification network uses a collection of regional CNN features, dynamically weighted by the compatibility scores in classifying fine-grained actions/gestures.\n\nThe proposed approach is inspired by R*CNN  [17] , attention  [8] , and Histogram of Oriented Gradient (HOG)  [18]  for combining multiple regions representing visual cues in a given image to solve the fine-grained action recognition problem. The main contributions of this paper are:\n\n• A novel approach is proposed for gesture/action recognition in still images, unlike current approaches, without requiring bounding-box annotations and/or body parts/object/people detection. The generalization and easy-to-implement capability of our approach is demonstrated by integrating it with the state-of-theart base CNNs that incorporate regional attentions to give a significant improvement in the fine-grained gesture/action recognition performance; • To the best of our knowledge, the proposed regionbased attentional module is the first of its kind that uses a hybrid approach to include hard attention, soft attention, and self-attention on pooled regional CNN features from a base network. We also introduce a skip connection to the Squeeze-and-Excitation (SE) block  [19]  to improve the gradient-flow from its output to the base CNN in modeling the interdependencies between channels of region-specific visual features; • The efficacy of our approach is demonstrated through in-depth analysis on 10 datasets comprising of three dif-ferent types of action/gestures: 1) head pose, 2) driver's distraction activities, and 3) human actions involving human-objects interaction and facial expressions; • Finally, ablation study and visual analysis to show the impact of our region-based attentional and SE module on the base network and its performance despite being trained with image-level classification label only. The rest of this paper is organized as follows: Section 2 discusses the related work on fine-grained gestures/actions recognition from monocular imagery. Section 3 describes the proposed approach for recognizing fine-grained activities. Experimental evaluations and results are presented and discussed in Section 4. Finally, the concluding remarks are given in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Human gestures/action is a well-studied problem  [2]  [3] with a wide range of approaches. In this section, we review several state-of-the-art approaches on head pose recognition, driver's distracting gestures/action recognition, and human action/gesture/facial expression recognition. We have also reviewed the role of attention in human action/gesture recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Head Pose Recognition",
      "text": "Head pose infers the orientation of a person's head relative to the camera view. Traditionally, head pose estimation is computed by locating 2D facial landmarks (also known as keypoints) in the target face and establishing the correspondence between landmarks and a head template by performing alignment  [20] . Recently, there has been a significant progress in detecting and localizing facial landmarks using modern deep learning models  [21]    [22]  [23]  [24] . These models aim to predict head poses and facial landmarks jointly. However, the primary goal of the head pose estimation is to improve the accuracy of the landmark prediction. As a result, head pose estimation itself is usually not sufficiently accurate on its own. In OpenFace 2.0  [24] , authors use simplified deep Convolutional Experts Constrained Local Model (CE-CLM) for facial landmarks detection. Head pose estimation is carried out using a 3D representation of facial landmarks. Hyperface  [22]  combines R-CNN  [25]  and AlexNet to perform four different sub-tasks (detect faces, determine gender, detect facial landmarks, and estimate head pose) simultaneously. KEPLER  [21]  uses Heatmap-CNN (H-CNN) to predict facial landmarks and pose jointly. To improve landmarks detection, it uses coarse pose supervision. All-In-One CNN  [23]  uses a multi-task learning concept for simultaneous face detection and alignment: face recognition, smile detection, pose estimation, gender recognition, and age estimation using a single CNN. Ruiz et al.  [26]  describe landmarks-free head pose estimation using image intensities. They regress head pose Euler angles by applying a multi-loss objective function. Similarly, FSA-Net  [27]  uses stage-wise regression, and feature aggregation for landmarks-free head pose estimation. Our work differs from the approaches above since we focus on the classification of head poses targeting the existing large-scale datasets for face recognition. Our work is also applicable to other tasks such as human actions/gesture recognition and can be easily integrated into most of the deep CNN architectures.\n\nAlthough significant advancement has been made in face detection, accurate estimation of head poses and landmarks is still a challenging task, particularly in unconstrained \"in the wild\" images. Uncertainty in head pose estimation seems to be a key factor for face recognition and landmarks estimation  [20]    [22] . In extreme poses, face detection is arguably still a difficult problem to address due to occlusion. We aim to recognize the coarse head pose directly from image intensities and is different from the head pose estimation regression problem  [27] [26] . This is necessary for inferencing human attention (which direction a person is looking), which is often explored in humanmachine/computer interactions, human-robot social interactions, and nonverbal communications. To address this, we introduce novel attention involving self-attention and co-attention that can be easily integrated with the existing state-of-the-art CNNs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Driver'S Gestures Recognition",
      "text": "There are two types of gestures associated with a driver: 1) driving gestures -primary activities involving the interactions between driver's body parts and vehicle controls, and 2) non-driving gestures -secondary activities are often known as distractions (e.g. eating, drinking, etc.) that often involve driver-objects interactions. The non-driving secondary activities are overwhelmingly to blame for the vast majority of accidents  [28]  [29], and thus, there is an urgent need for automatic monitoring of such activities. Moreover, it is found that such activities are most wanted in-vehicle activities in highly/fully automated driving systems when the driver is not in control  [30] . In such a scenario, there is a need for monitoring the driver's state and readiness for Take-Over-Request (TOR) when the vehicle is unable to make an appropriate decision.\n\nRecently, there has been significant progress in monitoring driver's gesture/state using monocular images  [28]    [31]  [32]  [33] . Most of these approaches focus on human-centric cues such as body pose  [28] , body-object interactions  [29] , and hand positions and movements  [32]    [33] . Behera et al.  [28]  propose a method for drivers state/gesture recognition by injecting latent body pose into the adapted DenseNet architecture  [34] . Baheti et al.  [31]  modify the VGG16 architecture  [35]  to improve the driver's state classification accuracy by reducing the number of parameters for faster execution. Abouelnaga et al.  [32]  achieve a high classification accuracy of driver's state/gesture by considering a genetically weighted ensemble of five different CNNs, making it too heavy for real-time applications, which are very much essential in autonomous/self-driving cars.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Human Action/Gesture/Facial Expression Recognition In Still Images",
      "text": "There is a wide range of work in the field of action/gesture recognition using monocular images  [6]  [36]  [37]  [38]  [39]  [40]  [41]  [42]  [43] . Recently, deep learning is making major advances in action recognition that has attracted the best attempts of the computer vision community for many years.\n\nZhao et al.  [6]  exploit the mid-level semantic actions by dividing the human body into seven semantic parts, which are combined with contextual cues to recognize the entire body action. The work described by Zhang et al.  [36]  segments the precise regions of underlying humanobject interactions with minimum annotation efforts. An Expanded Parts Model (EPM) is proposed by Sharma et al.  [37]  for recognizing human attributes (e.g. young and short hair) and actions (e.g. running and jumping) in still images. Zhao et al.  [38]  capture multi-scale cues involving semantic region candidates at multiple scales to highlight the optimal scale for each action. An action-specific person detection approach is presented by Khan et al.  [39]  by exploiting transfer learning to overcome the limited labeled action examples. A region-based model is proposed by Zhao et al.  [40]  for action classification in still images by introducing a discriminative region selection method.\n\nFacial expression recognition (FER) is widely used to determine the affective state of the subject, regardless of its identity. There has been a significant advancement for automatic FER, particularly in controlled laboratory settings  [44] . However, it remains a challenge in uncontrolled reallife situations involving unpredictable variability in head poses, lighting conditions, occlusions, and subjects. A deexpression residual learning procedure is proposed in  [41]  to recognize facial expressions by extracting information from the expressive components. In  [45] ,  Kim et al.  propose an approach to fuse information about non-aligned and aligned facial states to boost FER accuracy and efficiency. A deep model is proposed in  [46]  to learn a rich face representation to capture gender, expression, head pose, and age-related attributes, and then perform pairwise-face reasoning for relation prediction. Similar to the above approaches, we propose an attention-driven deep model that not only improves the FER accuracy but also generalizes its applicability to wider related tasks such as head pose recognition and human/driver actions/gestures recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attentions In Action/Gesture Recognition",
      "text": "Deep models over the full image have shown great promise, but it raises the question of whether the fine-grained recognition task can be treated as a general classification problem. Recently, human visual perception has been explored in machine learning and computer vision community to address this issue. It focuses selectively on parts of the scene to acquire information by exploring vital cues such as body parts involved, the identity of objects around them, and their interaction with these objects. Most of the recent attention-based approaches  [8]  [9] [10]  [11] [12]  [13] focus selectively on these vital cues to improve the performance of the recognition task by considering parts of the image, which are most relevant to a given task.\n\nGkioxari et al.  [17]  propose an action-specific model called R*CNN that uses a primary region containing the person in question and a secondary region consisting of contextual cues. An object relation transformer model is proposed in  [12]  for image captioning that explicitly incorporates information consisting of the spatial relationship between detected objects in a scene through geometric attention. Huang et al.  [13]  propose a two-layer attention Fig.  1 : Architecture of our RAN. Given an image, we select a set of candidate regions (bounding boxes). The image is passed through a base CNN (e.g. VGG  [35] , ResNet  [47] , Inception  [48] , etc.). The output activation of a given region r is computed using specialized Squeeze-and-Excitation  [19]  layer with skip connection. For each gesture g (head pose example), the most informative region is selected using the proposed attention layer consisting of self-attention and co-attention representing combined attention of regions and the whole image. The softmax operation transforms co-attention-focused activations C a into probabilities that form the final prediction.\n\n(attention on attention) model for image captioning, which generates an information vector and an attention gate using the attention result and the current context in order to obtain the attended information. On the other hand, Li et al.  [11]  introduce entangled attention that exploits the semantic and visual information simultaneously to enhance the image captioning performance. In order to recognize the human activity, Zeng et al.  [15]  propose LSTM-based two attention models (temporal and sensor) highlighting the important part of the time-series signals as well as sensor importance. Similarly, Wang et al.  [14]  propose an end-toend deep learning model called BANet to learn temporal and bodily parts that are more informative for the detection of protective behavior. To address the problem of video question answering, Li et al.  [10]  present an approach that consists of positional self-attention with co-attention and takes as inputs video frame and posed question textual features, and then compute attentions for them simultaneously. Vaswani et al.  [8]  describe a self-attention model by modifying traditional attention. The model calculates the response at a position in a sequence by attending to all the positions to perform the machine translation task.\n\nOur proposed attention is inspired by these latest developments. It is different from the above approaches since this work is focused on fine-grained recognition tasks involving subtle changes in images (e.g. playing versus holding a flute and talking vs texting on phone). Whereas, existing attentional models  [11]  [12]  [13]  are mostly focused on images with distinctive object categories and/or classes. Therefore, it is observed that such models often use Faster R-CNN  [49]  for object detection/proposal, whereas our approach focuses on subtle changes within a given object (e.g. facial expression and head pose). Therefore, we employ soft attention by considering the entire image, apply hard attention in which semantic regions are selected via hard decisions, and adapt self-attention by considering the positions of different semantic regions within a still image to address the fine-grained action/gesture recognition problem. The novelty of our approach is that a semantic region is not only conditioned on itself (soft attention) but also conditioned on the other regions and the whole image (self-attention) before applying attention to attentions, which is called combined attention or co-attention. The output of the co-attention is fed into the softmax layer for the final decision. The whole process is carried out in an end-to-end learning fashion without requiring component-level bounding box labeling and/or object/people/body parts detection. Moreover, the proposed method can be easily integrated into the state-ofthe-art CNN architectures.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Previous Work By Authors",
      "text": "This work builds on the published conference output  [50] , focusing on coarse head pose recognition from image intensities using ROIs. The proposed RAN makes a substantial advance to it in two aspects: (i) by integrating a novel attention mechanism to explore salient regions in images while making recognition decisions. This approach is also evaluated on the head pose dataset and the performance is significantly better than that of the published approach  [50] .\n\n(ii) We have also introduced a skip connection to the SE block to model the interdependencies between channels of ROI-specific visual features. The efficacy and generalizability of RAN is demonstrated through in-depth analysis and evaluation using three different tasks for action/gestures recognition: 1) head pose, 2) driver's distraction activities, 3) human actions involving human-objects interaction and facial expression.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Approach",
      "text": "The proposed deep architecture is inspired by the recent advances in attention-focused deep learning approaches to solve fine-grained gestures (e.g. head pose, human actions, and driver's activities) recognition problem. The overview of the architecture is shown in Fig.  1 . An image is fed into a base CNN, and its output is up-sampled and fed into a regions of interest (ROIs) pooling layer, which also takes as input a list of regions with information about spatial location (x, y) and size (width and height). The pooling provides a fixed-size feature map for each ROI by using bilinear interpolation. These ROIs are computed automatically (see Section 3.1). Therefore, our network does not require the cropped region or region annotations. Subsequently, the ROI-pooled feature maps are passed through the corresponding Squeeze-and-Excitation (SE)  [19]  layer (red layers with skip connection described in Section 3.2) in Fig.  1 . Similarly, the output of the convolutional layer is also passed through the proposed SE layer (green layer) for computing feature map of the whole image I. Regionspecific feature maps and the image-specific feature map are then fed into the proposed attention layer to compute region-specific attentions, and then combine them to construct single activation (see Section 3.3) for fine-grained action recognition.\n\nWe define feature map F(g; I) of a fine-grained gesture g in a given image I with ROIs R as:\n\nwhere r, r ∈ {R, I}; F(I; R) is a feature map representing the whole image I conditioned on all the ROIs in R; F(r; r , I) is a feature map representing the ROI r conditioned on the rest of the ROIs r ∈ R and the whole image I. Similarly, the weight matrices W I g and W r g correspond to the whole image I and ROIs R for a given gesture g, respectively. Given a feature map F(g; I) of each gesture g, we compute the probability of a given gesture g in image I by using a softmax layer:\n\nThe feature F(.) and weight matrices W I g and W r g are all trainable parameters and learned jointly for all the finegrained gestures g ∈ G using a CNN, trained with gradientbased optimization of a stochastic objective function. The above feature maps (F(I; R) and F(r; r , I)) and weight matrices (W I g and W r g ) are computed through our proposed attention model.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Candidates Region Selection",
      "text": "Computer vision research has a long history of patch-or component/region-based approaches to visual recognition problems. This is mainly due to 1) different objects partially share similar parts, 2) occlusions and cluttered scenes, and 3) changes in the geometrical relation between parts. Finegrained gestures often exhibit most of these characteristics.\n\nHand-crafted features such as HOG  [18]  once dominated in solving visual recognition problem due to their superior performance prior to the recent advances in deep learning. It often considers patches around keypoints or facial landmarks to extract features. Not long ago, this patchbased approach was adapted into the deep learning models such as R-CNN (Regions with CNN features)  [25] , which led to a significant impact on the simultaneous detection and localization problem involving objects and people. Our approach is inspired by this. In R-CNN, the selective search is used to find 2K region proposals per image. Each region is passed through the same network to compute its objectness and is more suitable for the detection of distinct objects. We aim to recognize fine-grained gestures, which can be seen as the deformation of the same object/body parts, and therefore, learning separate region-specific features is more suitable. To achieve this using a CNN, each region has to be modeled separately and will be difficult to fit a large number (e.g. 2K in R-CNN) of regions. Thus, we adapted the strategies (cells and blocks) used in HOG  [18]  for our region proposals. We divide a given image into C × C cells. Our region consists of one or more consecutive cells, resulting in regions of different aspect ratios and areas from all possible combinations within the entire image. A block in HOG consists of 2 × 2 cells. Our region is similar to the block but consists of different sizes (e.g. 1 × 2, 1 × 3, 2 × 1, and 2 × 2) to consider all possible semantic regions within the entire image instead of only square ones. As a result, there are |R| = 35 possible regions for C = 3. Moreover, the proposed ROI-specific computation layers are added towards the end layers of our network (Fig.  1 ), and therefore, the most computational time is spent in the base CNN, which considers the whole image. One of the main advantages of the proposed ROI-based approach is that it can be added onto the top of any existing CNN models. Our evaluation using various CNNs is presented in Section 4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Squeeze And Excitation (Se) Layer With Skip Connection",
      "text": "The motivation for using ROI-specific SE  [19]  layer is to improve the representational power of our architecture by explicitly modeling the inter-dependencies between the channels of ROI-pooled features. This is done by feature recalibration in which the network learns to use ROI-specific global information to selectively suppress less useful features and emphasize the more informative ones. As a result, our model will be able to emphasize ROIs with task-specific features. The feature re-calibration capability within the SE layer is computed as: Firstly, the ROI-pooled features are passed through a squeeze operation (channel-wise scaling), which aggregates the feature maps across ROIs spatial dimension (e.g. 7 × 7 for the ResNet  [47] ) to produce a channel descriptor. This embeds the global distribution of channel-wise feature responses. Secondly, this is followed by an excitation operation (element-wise summation) in which ROI-specific activations are learned for each channel by a self-gating mechanism based on channel dependence and governs the excitation of each channel. As a result, the SE layer becomes increasingly specialized and responds to different ROIs in a highly task-specific manner.\n\nWe adapted the existing SE  [19]  block by introducing a skip connection (Fig.  2 ). Skip connections are also known as identity shortcut connections, which are extra connections between nodes in different layers of a network that skip one or more layers. The introduction of this skip connection has improved recognition accuracy. An arguable conjecture for introducing skip connections is due to three factors: 1) we use the pre-trained base CNNs, which are trained on a large dataset (e.g. ImageNet  [51] ). Therefore, these connections provide easy access to the learned low-level features, making it easy to use this information if needed for a new task as part of transfer learning to smaller datasets, which is the case here. 2) It improves the gradient-flow from the output of the SE layer to the base network and is important when adjusting parameters during transfer learning. 3) The skip connections also improve the training of deep networks partly by eliminating the singularities inherent in the loss landscapes of deep networks  [52] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Attention Computation",
      "text": "Our attention consists of two layers: 1) self-attention and 2) co-attention (Fig  3 ). Let F = {f 1 , f 2 , . . . , f |R|+1 } the set CNN features f r (F ⊆ F(.)), where r ∈ {R, I} (|R| + 1 is the number of the ROIs plus the whole image I). Each f r is the vector of output activations of the ROI r. We introduce a self-attention layer with an attention matrix A that contains the weight matrices W I g and W r g in Eq. (  1 ). The aim is to capture the similarity between any ROI with respect to the rest of the ROIs and the whole image I. This is achieved via element α r,r ∈ A by considering the region activations f r and f r of ROIs r and r , respectively. The ultimate objective is to infer how much to attend a particular ROI r conditioned on all the other ROIs and the whole image to highlight the importance of a given region in the decisionmaking process. It is implemented using an LSTM cell as:\n\nwhere W h ⊂ A and W h ⊂ A are the weight matrices for the respective ROIs r and r ; W g is the weight matrix corresponding to their non-linear combinations, h r,r is computed from W h f r + W h f r + b h using the element-wise sigmoid function, and α r,r is calculated using the Softmax function; b h and b g are the bias vectors. The self-attentionfocused activation l r of ROI r is given by the weighted summation of region activations f r of all the other ROIs r and their similarity α r,r to the ROI r in focus.\n\nThe attention-focused activations L = {l 1 , l 2 , . . . , l |R|+1 } are then used to produce a single activation C a as shown in Fig.  3 . To achieve this, we apply another attention mechanism to L, i.e. attention of attentions, and we called it coattention representing a high-level encoding of the entire image. It allows the model to decide the importance of selfattention-focused activation l r for the prediction by weighting them when constructing the single combined activation C a . We use a simple approach and is computed as: Then it is normalized to construct a weight vector a r over the regions {R, I}. Finally, the co-attention C a is computed as a weighted summation over all the regions using the attention scores as weights. Our co-attention is similar to the Attention on Attention in  [13]  in which the module generates an \"information vector\" and an \"attention gate\" via two separate linear transformations, which are both conditioned on the previous attention result and the current context represented as a query. Whereas, in our co-attention, we use the self-attention principle and a single linear transformation in which the concept of the query, the key, and the value are all the same (i.e. previous attention l r ).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Training And Implementation Details Of Ran",
      "text": "The proposed approach is experimented with various stateof-the-art CNN as a base network, and our region-specific SE and attention layers are added on top of it. All the layers in the base CNNs are initialized with pre-trained ImageNet's  [51]  (1.2M natural images with 1K categories) weights. Our RAN is trained in an end-to-end fashion with the default image size of 224 × 224 and is randomly selected from 256 × 256. The data augmentation also includes random rotation of ±15 • and a random zoom of 1 ± 0.15 and the image center. The model is trained with a batch size of 16 using a Linux (Ubuntu) machine fitted with 16GB GPU (NVIDIA Quadro P5000) card. During training, we minimize the softmax probability P rob(g; I) representing that gesture g appears in the image I computed in Eq. (  2 ). The loss over a batch B = {I i , y i } M i=1 is given by loss\n\nwhere g are the gesture predictions, y are the actual labels, i denotes the training images, and G represents a set of gestures. We use the Adam optimizer  [53]  with a learning rate of 10 -5 to minimize the objective function in Eq. (  5 ) and train the model for 50 epochs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Evaluations",
      "text": "In order to validate our model, we consider 10 various datasets depicting three different scenarios: 1) head pose, 2) driver's distraction activities, and 3) human actions/gestures/facial expression recognition. Its performance is measured in two metrics: accuracy (ACC) and mean average precision (mAP) in percentage  [58]  [59]  [60] .\n\nThe higher these values, the better the method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Head Pose Recognition Datasets",
      "text": "MultiLab  [50] : It is a collection of a number of publicly available datasets for pose estimation and related research on face analysis. The dataset consists of 24,334 images from 1288 identities, and 5 coarse head poses: 1) frontal (0 • ), 2) half profile -left (-45 • ), 3) full profile -left (-90 • ), 4) half profile -right (+45",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multi-Task Facial Landmark (Mtfl) [20]:",
      "text": "The dataset consists of five different head poses (0, ±30, ±60) and contains 13, 466 faces in which 5, 590 are from LFW  [61] . We use the same training and test subset as in  [20] . Annotated Facial Landmarks in the Wild (AFLW)  [55] : It consists of 25K annotated face in real-world images with coarse head poses. It is very similar to the MTFL, but the head pose information is provided as three rotation angles yaw, pitch, and roll. We consider only yaw as in MTFL  [20] , MultiLab  [50]  and VGGFace2  [54]  datasets. For evaluation, we follow the training/test split as in  [55] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Driver'S State/Gesture Recognition Datasets",
      "text": "We validate our model using two challenging datasets: i) Distracted Driver V1  [32] , and ii) Distracted Driver V2  [33] . These datasets consist of 10 classes of actions: 1) safe driving, 2) texting -right, 3) talking on the phone -right,   In our experiment, we use the VIS camera (10,379 images) with a strong illumination condition. We follow the standard 10-fold subjectindependent cross-validation evaluation procedure in  [64] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Discussion",
      "text": "Head Pose Recognition: The coarse head pose recognition accuracy using 4 challenging datasets is presented in Table 1. We have compared the performance with baselines consisting of various state-of-the-art CNNs, as well as our previous method using ROI-CNN  [50] . The proposed RAN performed significantly better than the baselines and ROI-CNN for eight different base networks. In MultiLab, RAN's accuracy is over 99% for all the base networks except DenseNet-169. The highest accuracy (99.90%) is achieved using Inception-V3 as a base CNN and is ∼5% and ∼7% better than those of the respective best performance by ROI-CNN (Inception-V3  [48]  as a base CNN: 95.01%) and the baseline (VGG16  [35] : 92.84%).\n\nA common observation is that the overall performance of the baselines and ROI-CNN  [50]  is low in VGGFace2  [54] , MTFL  [20] , and AFLW  [55]  in comparison to MultiLab  [50] . This is mainly due to the clutter in images. For example, most images in the MultiLab dataset are captured in a laboratory setup and thus, often exhibit a clean background. The rest of the datasets contain images with mixed difficulty (e.g. occlusion, multiple faces, and hand-over-faces) since they are collected from the web. However, it is more often in MTFL than VGGFace2 and AFLW. Moreover, the size of VGGFace2 (∼63K) and AFLW (25K) is larger than that of the MTFL (10K), resulting in an impact on the performance because deep models learn more from large datasets. Nevertheless, the proposed RAN performs far better (VG-GFace2: 99.57%, MTFL: 97.88%, and AFLW: 99.39%) than the baselines and ROI-CNN irrespective of dataset size and complexity.\n\nTo the best of our knowledge, we are the first to provide the quantitative evaluation of coarse head pose recognition on VGGFace2, MTFL, and AFLW datasets. Coarse head poses have been used to improve the detection of facial landmarks  [20] , as well as the influence of head pose in identity recognition performance  [54] . We have also compared the coarse pose recognition accuracy with the stateof-the-art OpenFace 2.0  [24]  and FSA-Net  [27] . The results are presented in Table  2 . Test images from three datasets are used to estimate yaw angle, which is binned into five different poses: 1) 0  2 , it can be seen that both the OpenFace and FSA-Net perform nearly perfect (100%) for the frontal view (0 • ). However, the accuracy is significantly dropped for both half (±45 • ) and full (±90 • ) profile faces. This has impacted on the overall performance, which is significantly lower than those of the proposed approach as well as the baselines in Table  1 . The pose estimation in OpenFace 2.0 is carried out using a 3D representation of the detected facial landmarks. The model is unable to detect most of these landmarks, which are invisible in half/full profile images, resulting in inaccurate pose estimation. The FSA-Net  [27]  is a state-of-the-art landmark-free regression approach for head pose estimation. We use their pre-trained model for pose estimation without retraining on the target dataset since our RAN is a classification model. The FSA-Net's performance is better than that of the OpenFace for profile faces. Our RAN is a landmark-free classification approach and is most suitable for coarse head pose recognition.\n\nWe have also carried out a performance analysis involving individual coarse poses. The recognition accuracy of various poses using FaceNet  [62]  as a baseline and the proposed RAN using Inception-V3  [48]  as a base network is presented in Table  3 . The accuracy of the baseline as well as the proposed RAN is far better than that of the OpenFace in Table  2 . Driver's State/Gesture Recognition: The performance of RAN using eight different base networks and their comparison to the state-of-the-art approaches is presented in Table  4 . For the dataset V1  [32] , RAN with Inception-V3  [48]  as a base CNN is the best (99.47%) performer consistent with that for head pose recognition in the last section. Moreover, the proposed RAN with most base networks (except VGG16) has outperformed the approach in  [31] , which is the best among existing works. RAN with Inception-V3  [48]  as a base CNN is 3.16% better than that of  [31] . Similarly, RAN with different base CNNs significantly outperforms all the existing approaches on dataset V2  [33] . The best performer (98.13%) is the RAN with DenseNet-169  [34]  as a base CNN.\n\nThe accuracy of individual action/gesture is presented in Fig.  4 . For the dataset V1, the proposed RAN is compared with the VGG with regularization (R-VGG) and modified VGG (M-VGG) proposed in  [31] . We also compare the performance with that of the Genetic Algorithm Weighted Ensemble (GA-WE)  [32]  in Fig 4a . It can be seen that in all the categories, our approach is better than the state-of-theart ones. Similarly, in dataset V2, the proposed RAN with DenseNet-169 as a base CNN is compared with the only available method, Inception-V3  [33] , in class-wise accuracy. The RAN performs better in all the actions except \"Talking Right\" (Fig.  4b ). Moreover, if we add our proposed ROIpooling and attention layers to the Inception-V3 network, the performance is 6.8% better than that of the Inception-V3 alone. This justifies the benefits of the proposed RAN that incorporates region-based attention into a given CNN architecture.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Human Action/Gesture/Facial Expression Recognition:",
      "text": "The performance of the proposed RAN with different base CNNs is presented in Table  5  using Stanford-40 and PPMI datasets. In most of the previous approaches, the mean average precision (mAP) is used as a metric over both datasets. However, approaches in  [58]  [59]  [60]  have used Fig.  4 : Individual action/gesture accuracy of our RAN: a) evaluated on\"Distracted Driver\" V1 dataset  [32]  with Inception-V3  [48]  as a base CNN, compared with R-VGG and M-VGG  [31]  and GA-WE  [32]  (left); b) evaluated on\"Distracted Driver\" V2 dataset  [33]  and compared with Inception-V3  [48]  (best performer) as evaluated in  [33]  (right).  both accuracy and mAP. For a fairer comparison, we have also used them. In both datasets, the proposed RAN using any base CNN has outperformed the state-of-the-art.\n\nIn PPMI  [5] , the mAP of our approach using ResNet-50 (96.68%) is 30.83% higher than that of the best performer  [59]  (65.85%). Similarly, in Stanford-40  [4] , the mAP of our approach (ResNet-50: 96.12%) is 4.92% better than that of the approach by Zhao et al.  [6]  (91.20%). This significant improvement in performance proves the powerfulness of the proposed RAN in action/gesture recognition in still images.\n\nWe have also evaluated the mAP of individual actions over the Stanford-40 and PPMI datasets, and the results are presented in Fig 5 . In Stanford-40, overall mAP without bounding box (96.12%) is marginally better than that with bounding box (95.72%). However, the individual mAP with bounding box is better for actions 'blowing bubble', 'climbing', 'fixing bike', 'phoning', 'rowing a boat', 'shooting an arrow', 'watching TV', 'waving hands', 'writing on a board' and 'writing on a book' (Fig.  5 ). The reason could be due to: 1) specific body pose involved in these actions, and 2) human-objects interaction, and most of these objects appear within the bounding box. In PPMI, our approach has significantly outperformed the best approach  [59]  in all the actions.\n\nThe recognition accuracy using RAN outperforms the state-of-the-art with a significant margin on FER2013 dataset  [63]  consisting of facial expressions of seven emotions. The accuracy of RAN using six different base CNNs is shown in Table  5 . The best accuracy (97.21%) is achieved with the lightweight NasNet mobile  [57]  as a base CNN and is 21.4% higher than the highest accuracy (75.8%) in  [69] . We also evaluate our RAN on the Oulu-CAISA facial expression dataset  [64] . The accuracy of the proposed RAN with DenseNet-121 as a base CNN and those of the state-of-theart are presented in Table  6 . The accuracy of our approach (88.74%) is better than that of the best approach (88%), De-expression Residue Learning (DeRL)  [41] . This demonstrates the wider applicability of our proposed RAN in recognizing human action/gesture/facial expression from monocular RGB images by adding our proposed attentional module on top of the existing state-of-the-art base CNN 91.20  [6] , 83.25  [59] , 82.64  [36] , 81.20  [47] , 78.80  [38] , 77.80  [35] , 75.50  [39] , 72.30  [37]  FER2013    [64] .\n\nMethod Setting Accuracy (%) LBP-TOP  [72]  sequence-based 68.13 HOG 3D  [73]  sequence-based 70.63 STM-Explet  [74]  sequence-based 74.59 Atlases  [75]  sequence-based 75.52 DTAGN-Joint  [76]  sequence-based 81.46 FN2EN  [43]  image-based 87.71 PPDN  [42]  image-based 84.59 DeRL  [41]  image-based 88.00 Our RAN image-based 88.  74  architectures.\n\nIn our experiments, the proposed RAN is evaluated on 10 different datasets ranging from smallest size (PPMI: 4,209 images) to the largest size (VGGFace2: 73,766 images) with varied numbers of image categories. It is well known that deep models using smaller datasets often result in lower test accuracy, perhaps because the training set is not sufficiently representative of the problem and the model might over-fit. Similarly, their result is better on larger datasets, but perhaps slightly lower than ideal test accuracy because the dataset might over-represent the problem and might not have the capacity to learn. In order to avoid these, we have used transfer learning (pre-trained base CNNs that are trained over large diverse ImageNet dataset  [51]  (1.2M natural images with 1K categories) since our RAN includes a base CNN as a major component. Moreover, we also introduce a skip connection (see Section 3.2) that provides easy access to learned features from pre-trained layers in base CNN, making it easy for transfer learning to smaller datasets. It also improves the gradient flow from output layers to the base CNN and is vital when adjusting parameters during transfer learning. We also apply data augmentation (see Section 3.2). As a result, there is no significant performance difference of our model over either smaller or larger dataset, as demonstrated in Table  5 , Table  4  and Table  1 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we conduct an ablation study to understand the impact of various components: base CNN architectures, SE block, attention, and the number of regions in our RAN. Firstly, we evaluated the base CNNs by simply using the transfer learning (fine-tuning the pre-trained models) and then our ROIs were added with/without proposed attention (±Attn) and SE block (±SE). We also experimented RAN with SE block, ROI only SE (ROI-SE) and the whole image only SE (I-SE). Finally, we evaluated the model accuracy with varying number of regions. We were not able to experiment with more than 50 ROIs due to GPU memory limitations. The results are shown in Table  7  using Stanford-40 and PPMI datasets. For various base CNNs over both datasets, it can be seen that the addition of our novel ROIbased modeling has significantly enhanced the accuracy of the original base CNNs. Moreover, the highest gain is when our novel ROI-based attention is added (columns 4 and 12). The overall best accuracy is when both the attention and SE layer are used (columns 8 and 16). It is also observed that the performance improves with the increasing number of ROIs (columns 7-9 and 15-17). However, the model complexity and memory requirement also increase with the number of ROIs. The accuracy using 50 ROIs is not significantly higher than that using 35 ROIs. For 50 ROIs, the batch size is reduced to 8 since the model is unable to fit in 16GB GPU memory. The accuracy using 35 ROIs is significantly better than that using 8 ROIs. Considering our model's performance and complexity, the optimal number of ROIs was set as 35.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Visualization And Analysis",
      "text": "In this section, we investigate why the proposed RAN is so effective for human action/gesture/facial expression recognition through the \"visual explanations\", using Gradientweighted Class Activation Mapping (Grad-CAM)  [77]  to produce coarse localization map, highlighting the salient regions in the decision-making process. The Grad-CAM is applied to our RAN with ResNet-50  [47]  as a base CNN. The visualizations of randomly selected images from three different datasets (\"Distracted Driver\", PPMI and Stanford-40) are presented in Fig.  6 . The visual explanation using our RAN is compared with that of the ResNet-50  [47]  as a baseline. Therefore, we use the feature map from 5c branch2c convolution layer (just before the attention layer) of ResNet-50 since Grad-CAM requires a convolutional layer to produce localization maps. Each sub-figure in   be seen that the salient regions using the RAN are more appropriate and indicative for a visual explanation during decision-making in comparison to the respective baselines. Moreover, the visual explanation is similar to the \"saliency in context\" (SALICON)  [78]  in which human visual attention on the popular MS COCO dataset often focuses on interacted objects or objects of interests that humans look at frequently and rapidly during natural exploration. These results clearly explain why the proposed RAN is effective for human action recognition.\n\nWe also visualize the feature discrimination ability of the proposed RAN versus ResNet-50  [47]  using t-Distributed Stochastic Neighbor Embedding (t-SNE)  [79] . The t-SNE is known to visualize high-dimensional data by converting it to low-dimensional embedding using similarities between data points as joint probabilities. We extract features from ResNet-50 base CNN just before the Softmax layer. Similarly, features from our RAN are extracted at two different layers: before and after our proposed region-based attention layer (the same as ResNet-50 base CNN and that before Softmax). Test data in both PPMI  [5]  and Stanford-40  [4]  datasets are used for feature extraction and then used to visualize the class separability. Fig.  7  clearly shows that the class separability (the gap between clusters and compactness of data points within each cluster) in ResNet-50 (baseline) is low in both the datasets. Whereas, in RAN, the clusters are farther apart and more compact, resulting in a clear distinction of various clusters representing different actions. To understand the impact of the proposed regional attention, we extract features at the same layer (Fig.  7b  and 7e ) as in baseline (Fig.  7a  and 7d ) since our regional attention is added on top of the base CNNs. Such t-SNE analysis clearly shows that the addition of our regional attention layer not only improves the recognition accuracy but also significantly enhances the discrimination capability of the base CNN (Fig.  7b  and 7e ). The cluster separation after the attention layer (Fig.  7c  and 7f ) is clearer than that after the base CNN (Fig.  7b  and 7e ). These results further explain the effectiveness of the proposed RAN in discriminating subtle changes in images for fine-grained action recognition.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusions And Future Works",
      "text": "In this work, we have proposed a novel end-to-end RAN that uses a hybrid attention mechanism to combine ROI- pooled features by exploring multiple regions of different sizes. The proposed network learns to benefit from informative regions, while suppressing less useful ones. The innovative attention mechanism applies soft attention by considering the entire image, employs hard attention in which semantic regions are selected via hard decisions and engages self-attention by considering the spatial distribution of various semantic regions within an image to address the challenges associated with the fine-grained action/gesture recognition problem. We also adapted the existing Squeezeand-Excitation (SE) block by introducing a skip connection to model the interdependencies between channels of regionspecific CNN features. This has improved the representational power of the RAN. The proposed region-specific layers are added on top of the existing CNN models, and therefore, most computational processing is in the base CNN, which processes the whole images.\n\nThe proposed approach is evaluated on ten challenging datasets about three different scenarios: 1) head pose recognition, 2) driver state recognition, and 3) human action/gesture/facial expression recognition. The proposed method is shown to outperform the existing state-of-the-art methods in this field by a large margin, thereby establishing a new benchmark in the field and demonstrates the effectiveness of the proposed network. In future, we will extend the proposed model to recognize the fine-grained actions in videos.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of our RAN. Given an image, we select a set of candidate regions (bounding boxes). The image is passed",
      "page": 4
    },
    {
      "caption": "Figure 1: An image is fed into",
      "page": 4
    },
    {
      "caption": "Figure 1: Similarly, the output of the convolutional layer",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Skip connections are also known as",
      "page": 5
    },
    {
      "caption": "Figure 2: Squeeze-and-Excitation layer with skip connection",
      "page": 6
    },
    {
      "caption": "Figure 3: ). Let F = {f1, f2, . . . , f|R|+1} the set",
      "page": 6
    },
    {
      "caption": "Figure 3: To achieve this, we apply another attention mech-",
      "page": 6
    },
    {
      "caption": "Figure 3: Computation of the proposed self-attention and co-",
      "page": 6
    },
    {
      "caption": "Figure 4: For the dataset V1, the proposed RAN is compared",
      "page": 8
    },
    {
      "caption": "Figure 4: a. It can be seen that in all",
      "page": 8
    },
    {
      "caption": "Figure 4: b). Moreover, if we add our proposed ROI-",
      "page": 8
    },
    {
      "caption": "Figure 4: Individual action/gesture accuracy of our RAN: a) evaluated on“Distracted Driver” V1 dataset [32] with Inception-",
      "page": 9
    },
    {
      "caption": "Figure 5: The mAP of the proposed RAN with ResNet-50 as a base CNN in individual action/gesture recognition over",
      "page": 9
    },
    {
      "caption": "Figure 5: In Stanford-40, overall mAP with-",
      "page": 9
    },
    {
      "caption": "Figure 5: ). The reason could",
      "page": 9
    },
    {
      "caption": "Figure 6: The visual explanation using",
      "page": 10
    },
    {
      "caption": "Figure 6: consists of two outputs from: 1) baseline model",
      "page": 10
    },
    {
      "caption": "Figure 6: Visual explanation of decision using Gradient-weighted Class Activation Mapping (Grad-CAM) [77] for “Distracted",
      "page": 11
    },
    {
      "caption": "Figure 7: clearly shows that",
      "page": 11
    },
    {
      "caption": "Figure 7: a and 7d) since our regional attention",
      "page": 11
    },
    {
      "caption": "Figure 7: b and 7e). The cluster separation after the",
      "page": 11
    },
    {
      "caption": "Figure 7: c and 7f) is clearer than that after the",
      "page": 11
    },
    {
      "caption": "Figure 7: b and 7e). These results further explain the",
      "page": 11
    },
    {
      "caption": "Figure 7: Visualization of outputs (before Softmax) of ResNet-50 [47] as a baseline and our RAN with ResNet-50 as a base",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Head pose recognition accuracy in percentage of different methods over different datasets. For a given dataset",
      "data": [
        {
          "Used Base CNN": "",
          "MultiLab dataset [50]": "Base-\nline",
          "VGGFace2 dataset [54]": "Base-\nline",
          "MTFL dataset [20]": "Base-\nline",
          "AFLW dataset [55]": "Base-\nline"
        },
        {
          "Used Base CNN": "ResNet-50 [47]",
          "MultiLab dataset [50]": "91.50",
          "VGGFace2 dataset [54]": "85.40",
          "MTFL dataset [20]": "75.47",
          "AFLW dataset [55]": "84.54"
        },
        {
          "Used Base CNN": "Inception ResNet-V2 [56]",
          "MultiLab dataset [50]": "88.82",
          "VGGFace2 dataset [54]": "82.43",
          "MTFL dataset [20]": "69.12",
          "AFLW dataset [55]": "85.26"
        },
        {
          "Used Base CNN": "Inception-V3 [48]",
          "MultiLab dataset [50]": "90.97",
          "VGGFace2 dataset [54]": "85.54",
          "MTFL dataset [20]": "70.70",
          "AFLW dataset [55]": "83.55"
        },
        {
          "Used Base CNN": "DenseNet-121 [34]",
          "MultiLab dataset [50]": "90.89",
          "VGGFace2 dataset [54]": "85.57",
          "MTFL dataset [20]": "77.11",
          "AFLW dataset [55]": "85.28"
        },
        {
          "Used Base CNN": "DenseNet-169 [34]",
          "MultiLab dataset [50]": "91.56",
          "VGGFace2 dataset [54]": "85.23",
          "MTFL dataset [20]": "75.23",
          "AFLW dataset [55]": "84.80"
        },
        {
          "Used Base CNN": "DenseNet-201 [34]",
          "MultiLab dataset [50]": "90.75",
          "VGGFace2 dataset [54]": "85.85",
          "MTFL dataset [20]": "72.06",
          "AFLW dataset [55]": "85.67"
        },
        {
          "Used Base CNN": "VGG16 [35]",
          "MultiLab dataset [50]": "92.84",
          "VGGFace2 dataset [54]": "85.16",
          "MTFL dataset [20]": "71.50",
          "AFLW dataset [55]": "85.13"
        },
        {
          "Used Base CNN": "NASNet mobile [57]",
          "MultiLab dataset [50]": "91.06",
          "VGGFace2 dataset [54]": "85.44",
          "MTFL dataset [20]": "64.72",
          "AFLW dataset [55]": "84.47"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Head pose recognition accuracy in percentage of different methods over different datasets. For a given dataset",
      "data": [
        {
          "Dataset": "",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "OpenFace 2.0 [24]"
        },
        {
          "Dataset": "MultiLab [50]",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "12.31\n43.82\n99.25\n35.86\n16.87\n54.14"
        },
        {
          "Dataset": "VGGFace2 [54]",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "3.26\n25.03\n99.69\n20.87\n4.21\n42.47"
        },
        {
          "Dataset": "MTFL [20]",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "0.00\n23.56\n99.62\n31.42\n0.00\n68.45"
        },
        {
          "Dataset": "",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "FSA-Net [27]"
        },
        {
          "Dataset": "MultiLab [50]",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "10.06\n66.74\n98.88\n68.53\n21.12\n70.73"
        },
        {
          "Dataset": "VGGFace2 [54]",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "8.96\n63.15\n98.49\n52.96\n6.41\n59.60"
        },
        {
          "Dataset": "MTFL [20]",
          "−90◦\n−45◦\n0◦\n+45◦\n+90◦\nOverall": "000\n41.50\n96.09\n89.93\n0.00\n70.10"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: Facial expression recognition accuracy (%) of",
      "data": [
        {
          "Datasets/method": "PPMI [5] (Ours, ACC)",
          "ResNet-50\nInc. ResNet\nInception\nDenseNet-\nDenseNet-\nDenseNet-\nNASNet\n[47]\nV2 [56]\nV3 [48]\n121 [34]\n169 [34]\n201 [34]\nmobile [57]": "98.63\n97.58\n97.00\n97.08\n97.50\n96.54\n96.08"
        },
        {
          "Datasets/method": "PPMI [5] (SOTA, ACC)",
          "ResNet-50\nInc. ResNet\nInception\nDenseNet-\nDenseNet-\nDenseNet-\nNASNet\n[47]\nV2 [56]\nV3 [48]\n121 [34]\n169 [34]\n201 [34]\nmobile [57]": "65.94 [59],\n64.94 [59]"
        },
        {
          "Datasets/method": "PPMI [5] (Ours, mAP)",
          "ResNet-50\nInc. ResNet\nInception\nDenseNet-\nDenseNet-\nDenseNet-\nNASNet\n[47]\nV2 [56]\nV3 [48]\n121 [34]\n169 [34]\n201 [34]\nmobile [57]": "96.68\n95.67\n95.60\n96.24\n94.76\n94.71\n94.15"
        },
        {
          "Datasets/method": "PPMI [5] (SOTA, mAP)",
          "ResNet-50\nInc. ResNet\nInception\nDenseNet-\nDenseNet-\nDenseNet-\nNASNet\n[47]\nV2 [56]\nV3 [48]\n121 [34]\n169 [34]\n201 [34]\nmobile [57]": "65.85 [59],\n64.93 [59],\n51.70 [40],\n49.40 [65],\n47.00 [66],\n46.70 [67],\n45.30 [68],\n36.70 [5]"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 6: Facial expression recognition accuracy (%) of",
      "data": [
        {
          "Method": "LBP-TOP [72]\nHOG 3D [73]\nSTM-Explet [74]\nAtlases [75]\nDTAGN-Joint [76]\nFN2EN [43]\nPPDN [42]\nDeRL [41]",
          "Setting": "sequence-based\nsequence-based\nsequence-based\nsequence-based\nsequence-based\nimage-based\nimage-based\nimage-based",
          "Accuracy (%)": "68.13\n70.63\n74.59\n75.52\n81.46\n87.71\n84.59\n88.00"
        },
        {
          "Method": "Our RAN",
          "Setting": "image-based",
          "Accuracy (%)": "88.74"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Human action recognition accuracy of the proposed RAN with different components and parameters over",
      "data": [
        {
          "Base": "",
          "People Playing Musical Instruments (PPMI) [5]": "base",
          "Standford 40 without bounding-box [4]": "base"
        },
        {
          "Base": "ResNet-50 [47]",
          "People Playing Musical Instruments (PPMI) [5]": "77.6",
          "Standford 40 without bounding-box [4]": "78.8"
        },
        {
          "Base": "DenseNet-121 [34]",
          "People Playing Musical Instruments (PPMI) [5]": "81.9",
          "Standford 40 without bounding-box [4]": "82.2"
        },
        {
          "Base": "DenseNet-169 [34]",
          "People Playing Musical Instruments (PPMI) [5]": "83.5",
          "Standford 40 without bounding-box [4]": "83.5"
        },
        {
          "Base": "DenseNet-201 [34]",
          "People Playing Musical Instruments (PPMI) [5]": "83.2",
          "Standford 40 without bounding-box [4]": "83.6"
        },
        {
          "Base": "NASNet-M [57]",
          "People Playing Musical Instruments (PPMI) [5]": "71.6",
          "Standford 40 without bounding-box [4]": "77.6"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Nonverbal Dictionary of Gestures, Signs & Body Language Cues",
      "authors": [
        "D Givens",
        "C Studies"
      ],
      "year": "2006",
      "venue": "The Nonverbal Dictionary of Gestures, Signs & Body Language Cues"
    },
    {
      "citation_id": "2",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "Vision based hand gesture recognition for human computer interaction: a survey",
      "authors": [
        "S Rautaray"
      ],
      "year": "2015",
      "venue": "Artificial intelligence review"
    },
    {
      "citation_id": "4",
      "title": "Human action recognition by learning bases of action attributes and parts",
      "authors": [
        "B Yao",
        "X Jiang",
        "A Khosla",
        "A Lin",
        "L Guibas",
        "L Fei-Fei"
      ],
      "year": "2011",
      "venue": "IEEE ICCV"
    },
    {
      "citation_id": "5",
      "title": "Grouplet: A structured image representation for recognizing human and object interactions",
      "authors": [
        "B Yao",
        "L Fei-Fei"
      ],
      "year": "2010",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "6",
      "title": "Single image action recognition using semantic body part actions",
      "authors": [
        "Z Zhao",
        "H Ma",
        "S You"
      ],
      "year": "2017",
      "venue": "IEEE"
    },
    {
      "citation_id": "7",
      "title": "A model of saliency-based visual attention for rapid scene analysis",
      "authors": [
        "L Itti",
        "C Koch",
        "E Niebur"
      ],
      "year": "1998",
      "venue": "IEEE Trans. on PAMI"
    },
    {
      "citation_id": "8",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "9",
      "title": "Position-based content attention for time series forecasting with sequence-to-sequence rnns",
      "authors": [
        "Y Cinar",
        "H Mirisaee",
        "P Goswami",
        "E Gaussier",
        "A Aït-Bachir",
        "V Strijov"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "10",
      "title": "Beyond rnns: Positional self-attention with co-attention for video question answering",
      "authors": [
        "X Li",
        "J Song",
        "L Gao",
        "X Liu",
        "W Huang",
        "X He",
        "C Gan"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "11",
      "title": "Entangled transformer for image captioning",
      "authors": [
        "G Li",
        "L Zhu",
        "P Liu",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Image captioning: Transforming objects into words",
      "authors": [
        "S Herdade",
        "A Kappeler",
        "K Boakye",
        "J Soares"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Attention on attention for image captioning",
      "authors": [
        "L Huang",
        "W Wang",
        "J Chen",
        "X.-Y Wei"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Learning bodily and temporal attention in protective movement behavior detection",
      "authors": [
        "C Wang",
        "M Peng",
        "T Olugbade",
        "N Lane",
        "A Williams",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "Learning bodily and temporal attention in protective movement behavior detection",
      "arxiv": "arXiv:1904.10824"
    },
    {
      "citation_id": "15",
      "title": "Understanding and improving recurrent networks for human activity recognition by continuous attention",
      "authors": [
        "M Zeng",
        "H Gao",
        "T Yu",
        "O Mengshoel",
        "H Langseth",
        "I Lane",
        "X Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "16",
      "title": "On attention models for human activity recognition",
      "authors": [
        "V Murahari",
        "T Pl"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "17",
      "title": "Contextual action recognition with r* cnn",
      "authors": [
        "G Gkioxari",
        "R Girshick",
        "J Malik"
      ],
      "year": "2015",
      "venue": "IEEE ICCV"
    },
    {
      "citation_id": "18",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "19",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proc. of the IEEE CVPR"
    },
    {
      "citation_id": "20",
      "title": "Facial landmark detection by deep multi-task learning",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2014",
      "venue": "Proc. ECCV"
    },
    {
      "citation_id": "21",
      "title": "Kepler: Keypoint and pose estimation of unconstrained faces by learning efficient h-cnn regressors",
      "authors": [
        "A Kumar",
        "A Alavi",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE Face & Gesture Recognition"
    },
    {
      "citation_id": "22",
      "title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition",
      "authors": [
        "R Ranjan",
        "V Patel",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on PAMI"
    },
    {
      "citation_id": "23",
      "title": "An all-in-one convolutional neural network for face analysis",
      "authors": [
        "R Ranjan",
        "S Sankaranarayanan",
        "C Castillo",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "24",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. IEEE Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "25",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "authors": [
        "R Girshick",
        "J Donahue",
        "T Darrell",
        "J Malik"
      ],
      "year": "2014",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "26",
      "title": "Fine-grained head pose estimation without keypoints",
      "authors": [
        "N Ruiz",
        "E Chong",
        "J Rehg"
      ],
      "year": "2018",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "27",
      "title": "Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image",
      "authors": [
        "T.-Y Yang",
        "Y.-T Chen",
        "Y.-Y Lin",
        "Y.-Y Chuang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Latent body-pose guided densenet for recognizing driver's fine-grained secondary activities",
      "authors": [
        "A Behera",
        "A Keidel"
      ],
      "year": "2018",
      "venue": "IEEE AVSS"
    },
    {
      "citation_id": "29",
      "title": "Context-driven multistream lstm (m-lstm) for recognizing fine-grained activity of drivers",
      "authors": [
        "A Behera",
        "A Keidel",
        "B Debnath"
      ],
      "year": "2018",
      "venue": "GCPR"
    },
    {
      "citation_id": "30",
      "title": "Investigating user needs for non-driving-related activities during automated driving",
      "authors": [
        "B Pfleging",
        "M Rang",
        "N Broy"
      ],
      "year": "2016",
      "venue": "15th Int'l Cconf. on mobile and ubiquitous multimedia"
    },
    {
      "citation_id": "31",
      "title": "Detection of distracted driver using convolutional neural network",
      "authors": [
        "B Baheti",
        "S Gajre",
        "S Talbar"
      ],
      "year": "2018",
      "venue": "IEEE CVPRW"
    },
    {
      "citation_id": "32",
      "title": "Realtime distracted driver posture classification",
      "authors": [
        "Y Abouelnaga",
        "H Eraqi",
        "M Moustafa"
      ],
      "year": "2017",
      "venue": "Realtime distracted driver posture classification",
      "arxiv": "arXiv:1706.09498"
    },
    {
      "citation_id": "33",
      "title": "Driver distraction identification with an ensemble of convolutional neural networks",
      "authors": [
        "H Eraqi",
        "Y Abouelnaga",
        "M Saad",
        "M Moustafa"
      ],
      "year": "2019",
      "venue": "Driver distraction identification with an ensemble of convolutional neural networks",
      "arxiv": "arXiv:1901.09097"
    },
    {
      "citation_id": "34",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "K Weinberger",
        "L Van Der Maaten"
      ],
      "year": "2017",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "35",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "36",
      "title": "Action recognition in still images with minimum annotation efforts",
      "authors": [
        "Y Zhang",
        "L Cheng",
        "J Wu",
        "J Cai",
        "M Do",
        "J Lu"
      ],
      "year": "2016",
      "venue": "IEEE Trans. on Image Processing"
    },
    {
      "citation_id": "37",
      "title": "Expanded parts model for semantic description of humans in still images",
      "authors": [
        "G Sharma",
        "F Jurie",
        "C Schmid"
      ],
      "year": "2016",
      "venue": "IEEE Trans. PAMI"
    },
    {
      "citation_id": "38",
      "title": "Multi-scale region candidate combination for action recognition",
      "authors": [
        "Z Zhao",
        "H Ma",
        "X Chen"
      ],
      "year": "2016",
      "venue": "IEEE ICIP"
    },
    {
      "citation_id": "39",
      "title": "Recognizing actions through action-specific person detection",
      "authors": [
        "F Khan",
        "J Xu",
        "J Van De Weijer",
        "A Bagdanov",
        "R Anwer",
        "A Lopez"
      ],
      "year": "2015",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "40",
      "title": "Generalized symmetric pair model for action classification in still images",
      "authors": [
        "Z Zhao",
        "H Ma",
        "X Chen"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Facial expression recognition by deexpression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "X Zhao",
        "X Liang",
        "L Liu",
        "T Li",
        "Y Han",
        "N Vasconcelos",
        "S Yan"
      ],
      "year": "2016",
      "venue": "Peak-piloted deep network for facial expression recognition"
    },
    {
      "citation_id": "43",
      "title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition",
      "authors": [
        "H Ding",
        "S Zhou",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "44",
      "title": "Facial expression recognition using convolutional neural networks: state of the art",
      "authors": [
        "C Pramerdorfer",
        "M Kampel"
      ],
      "year": "2016",
      "venue": "Facial expression recognition using convolutional neural networks: state of the art",
      "arxiv": "arXiv:1612.02903"
    },
    {
      "citation_id": "45",
      "title": "Fusing aligned and non-aligned face information for automatic affect recognition in the wild: a deep learning approach",
      "authors": [
        "B.-K Kim",
        "S.-Y Dong",
        "J Roh",
        "G Kim",
        "S.-Y Lee"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "46",
      "title": "Learning social relation traits from face images",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C.-C Loy",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. of the IEEE CVPR"
    },
    {
      "citation_id": "48",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "49",
      "title": "Faster r-cnn: Towards realtime object detection with region proposal networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "A CNN model for head pose recognition using wholes and regions",
      "authors": [
        "A Behera",
        "A Gidney",
        "Z Wharton",
        "D Robinson",
        "K Quinn"
      ],
      "year": "2019",
      "venue": "IEEE Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "51",
      "title": "ImageNet Large Scale Visual Recognition Challenge",
      "authors": [
        "O Russakovsky"
      ],
      "year": "2015",
      "venue": "IJCV"
    },
    {
      "citation_id": "52",
      "title": "Skip connections eliminate singularities",
      "authors": [
        "E Orhan",
        "X Pitkow"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "53",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "54",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "IEEE Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "55",
      "title": "Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization",
      "authors": [
        "M Koestinger",
        "P Wohlhart",
        "P Roth",
        "H Bischof"
      ],
      "year": "2011",
      "venue": "IEEE ICCVW"
    },
    {
      "citation_id": "56",
      "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "57",
      "title": "Learning transferable architectures for scalable image recognition",
      "authors": [
        "B Zoph",
        "V Vasudevan",
        "J Shlens",
        "Q Le"
      ],
      "year": "2018",
      "venue": "Proc. IEEE CVPR"
    },
    {
      "citation_id": "58",
      "title": "Action classification via concepts and attributes",
      "authors": [
        "A Rosenfeld",
        "S Ullman"
      ],
      "year": "2018",
      "venue": "ICPR"
    },
    {
      "citation_id": "59",
      "title": "New color fusion deep learning model for large-scale action recognition",
      "authors": [
        "Y Lavinia",
        "H Vo",
        "A Verma"
      ],
      "year": "2019",
      "venue": "Int. Journal of Computational Vision and Robotics"
    },
    {
      "citation_id": "60",
      "title": "Visual concept recognition and localization via iterative introspection",
      "authors": [
        "A Rosenfeld",
        "S Ullman"
      ],
      "year": "2016",
      "venue": "ACCV"
    },
    {
      "citation_id": "61",
      "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "authors": [
        "G Huang",
        "M Mattar",
        "T Berg",
        "E Learned-Miller"
      ],
      "year": "2008",
      "venue": "Real-Life' Images: detection"
    },
    {
      "citation_id": "62",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "63",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "64",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "65",
      "title": "Discriminative spatial saliency for image classification",
      "authors": [
        "G Sharma",
        "F Jurie",
        "C Schmid"
      ],
      "year": "2012",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "66",
      "title": "Combining randomization and discrimination for fine-grained image categorization",
      "authors": [
        "B Yao",
        "A Khosla",
        "L Fei-Fei"
      ],
      "year": "2011",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "67",
      "title": "Coloring action recognition in still images",
      "authors": [
        "F Khan",
        "R Anwer",
        "J Van De Weijer",
        "A Bagdanov",
        "A Lopez",
        "M Felsberg"
      ],
      "year": "2013",
      "venue": "IJCV"
    },
    {
      "citation_id": "68",
      "title": "Localityconstrained linear coding for image classification",
      "authors": [
        "J Wang",
        "J Yang",
        "K Yu",
        "F Lv",
        "T Huang",
        "Y Gong"
      ],
      "year": "2010",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "69",
      "title": "Facial expression recognition with deep learning",
      "authors": [
        "A Khanzada",
        "C Bai",
        "F Celepcikay"
      ],
      "year": "2020",
      "venue": "Facial expression recognition with deep learning",
      "arxiv": "arXiv:2004.11823"
    },
    {
      "citation_id": "70",
      "title": "Hierarchical committee of deep convolutional neural networks for robust facial expression recognition",
      "authors": [
        "B.-K Kim",
        "J Roh",
        "S.-Y Dong",
        "S.-Y Lee"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "71",
      "title": "Deep learning using linear support vector machines",
      "authors": [
        "Y Tang"
      ],
      "year": "2013",
      "venue": "ICML Workshop on Challenges in Representation Learning"
    },
    {
      "citation_id": "72",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "73",
      "title": "A spatio-temporal descriptor based on 3d-gradients",
      "authors": [
        "A Klaser",
        "M Marszałek",
        "C Schmid"
      ],
      "year": "2008",
      "venue": "BMVC 2008-19th British Machine Vision Conference"
    },
    {
      "citation_id": "74",
      "title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition",
      "authors": [
        "M Liu",
        "S Shan",
        "R Wang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "75",
      "title": "Dynamic facial expression recognition using longitudinal facial expression atlases",
      "authors": [
        "Y Guo",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2012",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "76",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "77",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "IEEE ICCV"
    },
    {
      "citation_id": "78",
      "title": "Salicon: Saliency in context",
      "authors": [
        "M Jiang",
        "S Huang",
        "J Duan",
        "Q Zhao"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "79",
      "title": "Accelerating t-sne using tree-based algorithms",
      "authors": [
        "L Van Der Maaten"
      ],
      "year": "2014",
      "venue": "The Journal of Machine Learning Research"
    }
  ]
}