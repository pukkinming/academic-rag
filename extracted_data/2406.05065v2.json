{
  "paper_id": "2406.05065v2",
  "title": "Emo-Bias: A Large Scale Evaluation Of Social Bias On Speech Emotion Recognition",
  "published": "2024-06-07T16:36:50Z",
  "authors": [
    "Yi-Cheng Lin",
    "Haibin Wu",
    "Huang-Cheng Chou",
    "Chi-Chun Lee",
    "Hung-yi Lee"
  ],
  "keywords": [
    "social bias",
    "self-supervised learning",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics. However, SER models might contain social bias toward gender, leading to unfair outcomes. This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it. SSL-based SER models are chosen for their cutting-edge performance. Our research pioneering research gender bias in SER from both upstream model and data perspectives. Our findings reveal that females exhibit slightly higher overall SER performance than males. Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias. Moreover, models trained with Mandarin datasets display a pronounced bias toward valence. Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims to detect and interpret emotional states conveyed through speech signals. However, SER models may capture and learn social bias, leading to potential social harm. Biased SER systems may exacerbate existing inequalities by disproportionately affecting marginalized communities. For example, if a system is less accurate in recognizing emotions in individuals with disabilities, non-native speakers, or a specific gender, it could further marginalize these groups by denying them equitable access to services or opportunities.\n\nWhile extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR)  [1, 2, 3] , Speech Translation  [4, 5] , Facial Emotion Recognition  [6] , and Automatic Speaker Verification (ASV)  [7] , limited attention has been paid to social bias within SER systems. For instance, Gorrostieta et al.  [8]  evaluated gender bias within a specific model and dataset, proposing two adversarial debiasing approaches. However, their analysis was confined to a singular model and only one dataset, potentially limiting its applicability to broader contexts. Similarly, Chien et al.  [9]  investigated gender-specific emotion perception using the IEMOCAP dataset  [10] , presenting a perceptual emotion learning framework. Yet, they overlooked the impact of training dataset selection on emotional bias. This underscores the need for comprehensive investigations into gender bias across diverse SER models and datasets to ensure robust and generalizable results.\n\nRecognizing such research gaps, our study delves into two inquiries: Firstly, do contemporary SER models exhibit gender bias? Secondly, what are the primary factors contributing to such bias? Specifically, we investigate whether upstream representations and downstream training data play a crucial role in shaping bias within these models.\n\nLeveraging the cutting-edge advancements in speech selfsupervised learning (SSL)  [11, 12] , we employ 15 SSL models and classical speech features like FBank to train SER systems. Through rigorous and comprehensive experimentation across six diverse emotion datasets, we carefully train and assess a total of 96 SER models.\n\nOur work yields the following contributions: • We conduct a large-scale evaluation of 15 SER models on six emotion datasets. Notably, females exhibit slightly superior overall emotion recognition performance to males, with a substantial gender-wise performance gap evident across individual emotions. Our analysis highlights two models, Modified CPC  [13]  and XLS-R  [14] , as exhibiting the highest bias in gender-wise SER F1-score differences. • We found that downstream training data distribution significantly affects gender bias for models trained with acted datasets while having a medium correlation with SER performance bias for real-world datasets.\n\n• We analyze the gender-wise F1-score difference on valence.\n\nWe observe that \"females have higher F1-score on positive valence while males have higher F1-score on negative valence\" is apparent in models trained with Chinese datasets. • We analyze the correlation between gender-wise upstream representation bias and SER performance gap between two genders. We find that SSL upstream representations barely influence the bias in SER performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "2. Evaluation Design",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "2.1. Ssl Model And Downstream Model",
      "text": "We chose SSL-based SER systems to evaluate gender bias, as these SOTA models are commonly preferred for emotion evaluation  [15, 16] , potentially amplifying the impact of any bias present. In the framework of speech SSL, model training consists of two stages. The first stage pre-trains a model (or upstream model) using self-supervised learning with a predefined pretext objective. The second stage uses representation from the upstream model to train a downstream task such as SER.\n\nWe use the SSL-based SER models in  [17] , which uses publicly available speech SSL models collected by the S3PRL toolkit  [18] . The upstream models include models trained with generative approach: DeCoAR 2  [19] , Autoregressive Predictive Coding (APC)  [20] , Vector-Quantized APC (VQ-APC)   [21] , Nonautoregressive Predictive Coding (NPC)  [22] , TERA  [23] , Mockingjay (Mock)  [24] ; models trained with contrastive approach: Wav2Vec2-XLS-R-1B (XLS-R)  [14] , Wav2Vec 2.0 (W2V2)  [25] , Wav2Vec2 Large Robust (W2V2 R)  [26] , vq-wav2vec (VQ-W2V)  [27] , wav2vec (W2V)  [28] , and Contrastive Predictive Coding (M CPC)  [13] ; models trained with predictive approach: HuBERT  [29] , WavLM  [30] , Data2Vec (D2V)  [31] .\n\nWe use the same downstream model architecture as the SER task in the S3PRL toolkit  [18] , using three Conv1d, a self-attention pooling, and two linear layers. To capture the high-dimensional nature of emotion  [32] , we formulate emotion recognition as a multilabel classification problem. We first transform the emotion annotations to emotion distribution by frequency and then apply label smoothing by  [33]  using a smoothing parameter of 0.05 to obtain a soft label. We use the F1 score as the evaluation metric for performance, which aligns with SER challenges  [34]  and benchmarks  [17] . A label prediction is successful if the output emotion probability distribution is higher than 1 n for n emotion classification task. Our models are trained on Nvidia Tesla V100 GPUs with 32 GB of memory. The cumulative GPU runtime amounts to approximately 3,300 hours.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Bias Evaluation",
      "text": "In our study, we employ three evaluation metrics to assess gender bias. We hope our evaluation plan can serve as a valuable reference for others.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "F1 Score Difference",
      "text": "We quantify the gender bias de i on each emotion ei by the difference of F1 score on gender because F1 score reflects both precision and recall. Furthermore, we define the corpus-level gender bias dc by averaging the absolute value of de i for all emotions ei in the set of all emotion E in the corpus:\n\nWe further evaluate the gender bias on valence by Eq. 3. 1 The intuition is calculating the difference of de i from positive valence and negative valence:\n\nE+, E-, and E b are the set of emotions in the corpus that belong to positive valence, negative valence, and both valence, respectively. p+ and pare the portions of speech belonging to positive and negative valence. de i ,+ and de i ,-are the de i of positive and negative valence speech, respectively. The detail of valence categorization is shown in Table  1 , following the study  [35] . 1 We don't use absolute value on dv as dc, because we mean to compare it with upstream bias in section 2.2.2",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Upstream Representation Bias On Valence",
      "text": "We would like to know whether gender bias in the upstream model embedding propagates to downstream applications. To evaluate how the upstream model's embedding of stimuli representing females and males relates to its embedding of stimuli representing positive and negative valence, we use the Speech Embedding Association Test (SpEAT)  [36]  for detecting bias. SpEAT measures the relative cosine similarity between 4 groups of stimuli. Let X and Y represent sets of embeddings from female and male, and let A and B be sets of embeddings for positive and negative valence, respectively. SpEAT effect size d is the difference between sums over the respective target concept, normalized by the standard deviation to compute the magnitude of association:\n\nwhere each term is the difference between the mean of cosine similarity of gender to each valence:\n\nIn SpEAT, the embedding of speech segments is first done by averaging embedding in each layer and then averaging the aggregated embeddings across all layers, which we call the Mean aggregation. We further incorporate representation weights from SER models to achieve compatibility and comparability with the SSL paradigm. Assume SSL SER models with n layers are trained to use ci as the weight for the input representation weighted sum of i th layer. We use ci to weighted sum embeddings across layers, which is called Weighted aggregation. This enhances the similarity of our representations to those generated by ER models trained via SSL. We average ci from SER models trained in different folds for cross-fold validation datasets defined in  [17] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Training Data Bias On Emotion Distribution",
      "text": "We evaluate the bias in downstream training data distribution d d by examining the difference in mean emotion distribution in training data (soft-label) for males and females. Take an emotion dataset with 4 emotions (neutral (N), anger (A), sadness (S), and happiness (H)) for example. We compute average training data distribution, (N, A, S, H) = (0.2, 0.3, 0.4, 0.1) for females and (N, A, S, H) = (0.1, 0.2, 0.4, 0.3) for males.\n\nTheir training data bias is then d d = (0.1, 0.1, 0.0, -0.2). For datasets with cross-fold validation, we compute the emotion distribution difference of the whole dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "To make our evaluation comprehensive, we evaluate SER performance parity using six emotion datasets with diverse languages, speaker sources, and emotion types. The datasets include real-world datasets MSP-PODCAST (POD)  [37] , BIIC-PODCAST (BPO)  [38] , and actor performance datasets MSP-IMPROV (IMP)  [39] , IEMOCAP (IEM)  [10] , BIIC-NNIME (NNI)  [40] , CREMA-D (CRE)  [41] . BIIC-NNIME and BIIC-Podcast are in Mandarin, and other datasets are in English. We follow SpEAT, using the Morgan Emotional Speech Set (MESS)  [42]  for valence stimuli in upstream bias evaluation. We use the emotion datasets with both valence and speaker ID annotation in SER training (NNI, IMP, POD) as more valence .0 0.0 0.5 1.0 -7.0 0.0 7.5 0.9 VQ-APC -1.2 5.0 0.0 -6.3 0.0 -24.7 4.0 29.7 0.9 -1.9 6.9 -5.2 0.7 -3.1 10.7 1.3 -0.2 3.3 2.5 0.0 1.9 9.7 -4.0 -1.1 8.0 2.2 HuBERT 3.8 13.4 0.0 -6.3 2.7 -26.1 4.4 29.3 2.6 2.0 8.0 -1.5 1.9 -2.3 7.9 2.7 -0.3 6.4 2.5 -7.6 -2.0 2.7 -3.0 3.9 7.5 1.1 DeCoAR 2 3.9 2.4 0.0 -8.5 0.0 -24.9 1.7 29.3 0.5 1.9 6.2 -3.3 3.6 -2.6 10.1 2.7 -0.9 3.7 2.8 0.0 1.7 4.6 -4.  4   stimuli for fair comparison. We use the Speech Accent Archive  [43]  for male and female stimuli. The selection criteria for stimuli are the same as those for SpEAT, with the goal of ensuring that differences in association with positive and negative valence do not stem from variations among the speakers.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Result And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Downstream Performance Difference",
      "text": "Table  2  shows the difference in F1 score between females and males on each dataset and each emotion. Due to space limitations, we report the result for models across 3 different training objectives and achieve top-9 performance in EMO-SUPERB. It shows that despite most ER models only exhibiting a slightly high macro-F1 score for females, a high parity exists between the F1 score for each emotion. For instance, in the BPO dataset, males exhibit approximately a 25% higher F1 score than females for neutral emotion, whereas they demonstrate a 29% lower F1 score for happy emotion. The bias observed in emotion F1 scores varies across datasets but shows a consistent trend across emotion recognition models. Specifically, we take BPO and POD as examples since they have identical emotion categories. While the POD dataset exhibits a slight SER bias de in the happy and neutral emotion, the BPO dataset displays a significant F1 score disparity in these categories, indicating substantial dissimilarity. Furthermore, all SER models trained with BPO consistently show the largest de on happy and the smallest de on neutral. Conversely, all SER models trained with the POD dataset consistently exhibit the highest de values for emotion angry and the lowest for contempt. These observations underscore the potential correlation between gender bias in SER and the characteristics of the emotion dataset, with less influence from the upstream model. We conduct further analysis in section 3.2 and 3.3.\n\nWe further compare the average F1 score parity dc across models and datasets in  Table 3(a) . It shows that gender bias is closely related to datasets. SER models trained with BPO exhibit higher dc than those trained with other datasets, while SER models trained with IEM are less biased. Furthermore, the mean dc across datasets shows Modified CPC is more biased than all  the other models, while XLS-R shows the most biased result on a majority of emotion datasets. Conversely, the baseline feature, FBank, shows less bias across corpora. This is caused by its low classification accuracy and recall of models trained with this feature.\n\nAlso, we analyze the gender bias on valence dv by observing whether females have higher F1 scores on positive valence and males have higher F1 scores on negative valence. The result in Table  3  (b) indicates that SER models trained with Chinese datasets (BPO and NNI) have higher dv than SER models trained with English datasets, and these models associate female speech with positive valence and male speech with negative valence. In contrast, SER models trained with English datasets demonstrate positive and negative dv values across different datasets, suggesting a more varied impact.\n\nSince the downstream model architecture is the same across all SER models trained with SSL fashion, the bias may only come from two possible sources: training data and upstream representation. We try to find out the most influential factor of gender bias via the following experiments. This observation underscores the importance of considering the variance in emotion distribution across diverse social groups, as it might propagate to SER performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Upstream Representation",
      "text": "We first evaluate the upstream representation bias using SpEAT.\n\nAs showcased in Table  5 , our findings unveil substantial levels of representation bias across most models, especially VQ-APC, NPC, and APC, which have larger ds values. In contrast, D2V and M CPC have smaller gender biases among these models. Moreover, our analysis reveals that aggregating layerwise representations via weighted sum yields consistent outcomes akin to mean pooling, underscoring the robustness and stability of our bias evaluation metric across diverse SER tasks. We further evaluate the Pearson correlation coefficient between the downstream F1 score gap on valence dv and upstream bias on valence ds among all models. Table  6  shows low or no correlation between upstream and downstream valence bias on different stimuli and aggregation methods. This implies gen-  der bias in upstream representation might hardly propagate to downstream emotion classification tasks, which contradicts the conclusion in SpEAT. Two possible reasons might contribute to the difference between our work and SpEAT: (1) SpEAT trains the downstream valence prediction model with only 1800 speech samples, while we train multilabel emotion classification models and then calculate the difference between gender F1 parity of positive and negative valence emotions, with at least 4000 speech samples per model. (2) SpEAT asserts that the group identified as positive valence by the pre-trained model tends to exhibit a similar association with positive valence in the downstream SER model, framing the challenge as a binary classification problem. However, our analysis transcends binary classification by discussing the correlation between metrics representing continuous scales that reflect the extent of association between upstream and downstream bias. Our analysis suggests that we should use upstream bias metrics carefully as the bias might not reflect in application-level performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion And Limitation",
      "text": "Gender is a spectrum rather than a solely male/woman binary  [44] . However, existing speech emotion classification dataset datasets only have labels on binary gender. Including a broader range of gender identities in the dataset would better reflect the reality of human diversity and improve the performance and fairness of the models, which can be our future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This work provides extensive insights into the gender bias in SER models trained with the SSL paradigm. We identify that females exhibit superior overall SER performance compared to males. Also, a substantial gender-wise performance gap exists across individual emotions. Furthermore, our investigation underscores the influence of dataset characteristics, revealing Mandarin datasets to exhibit a pronounced bias toward valence compared to their English counterparts. Importantly, we find that downstream training data distribution plays a pivotal role in exacerbating gender bias, while upstream representation exerts minimal influence. These findings have far-reaching implications for developing ER technologies. Our forthcoming efforts aim to rectify biases inherent in these ER systems. Our research sets the stage for more inclusive and ethical approaches to designing and deploying AI systems.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "perspectives. Our ﬁndings reveal\nthat\nfemales exhibit slightly"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "higher overall SER performance than males. Modiﬁed CPC"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "and XLS-R, two well-known SSL models, notably exhibit sig-"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "niﬁcant bias. Moreover, models trained with Mandarin datasets"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "display a pronounced bias toward valence. Lastly, we ﬁnd that"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "gender-wise emotion distribution differences\nin training data"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "signiﬁcantly affect gender bias, while upstream model\nrepre-"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "sentation has a limited impact."
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "Index Terms:\nsocial bias,\nself-supervised learning,\nemotion"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "recognition"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "1.\nIntroduction"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "Speech emotion recognition (SER) aims to detect and interpret"
        },
        {
          "search gender bias in SER from both upstream model and data": "emotional\nstates conveyed through speech signals.\nHowever,"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "SER models may capture and learn social bias,\nleading to po-"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "tential social harm. Biased SER systems may exacerbate ex-"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "isting inequalities by disproportionately affecting marginalized"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "communities. For example,\nif a system is less accurate in rec-"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "ognizing emotions\nin individuals with disabilities, non-native"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "speakers, or a speciﬁc gender, it could further marginalize these"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "groups by denying them equitable access to services or oppor-"
        },
        {
          "search gender bias in SER from both upstream model and data": ""
        },
        {
          "search gender bias in SER from both upstream model and data": "tunities."
        },
        {
          "search gender bias in SER from both upstream model and data": "While extensive research has addressed bias in various ma-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "Recognizing such research gaps, our study delves into two"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "inquiries: Firstly, do contemporary SER models exhibit gender"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "bias?\nSecondly, what are the primary factors contributing to"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "such bias?\nSpeciﬁcally, we investigate whether upstream rep-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "resentations and downstream training data play a crucial role in"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "shaping bias within these models."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "Leveraging the cutting-edge advancements in speech self-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "supervised learning (SSL) [11, 12], we employ 15 SSL models"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "and classical speech features like FBank to train SER systems."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "Through rigorous and comprehensive\nexperimentation across"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "six diverse emotion datasets, we carefully train and assess a to-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "tal of 96 SER models."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "Our work yields the following contributions:"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "• We conduct a large-scale evaluation of 15 SER models on"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "six emotion datasets. Notably, females exhibit slightly supe-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "rior overall emotion recognition performance to males, with a"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "substantial gender-wise performance gap evident across indi-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "vidual emotions. Our analysis highlights two models, Modi-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "ﬁed CPC [13] and XLS-R [14], as exhibiting the highest bias"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "in gender-wise SER F1-score differences."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "• We found that downstream training data distribution signif-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "icantly affects gender bias\nfor models\ntrained with acted"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "datasets while having a medium correlation with SER per-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "formance bias for real-world datasets."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "• We analyze the gender-wise F1-score difference on valence."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "We observe that “females have higher F1-score on positive"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "valence while males have higher F1-score on negative va-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "lence” is apparent in models trained with Chinese datasets."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "• We analyze the correlation between gender-wise upstream"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "representation bias and SER performance gap between two"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "genders. We ﬁnd that SSL upstream representations barely"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "inﬂuence the bias in SER performance."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "2. Evaluation design"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "2.1.\nSSL model and downstream model"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": ""
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "We chose SSL-based SER systems to evaluate gender bias, as"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "these SOTA models are commonly preferred for emotion eval-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "uation [15, 16], potentially amplifying the impact of any bias"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "present.\nIn the framework of speech SSL, model\ntraining con-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "sists of\ntwo stages.\nThe ﬁrst stage pre-trains a model\n(or up-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "stream model) using self-supervised learning with a predeﬁned"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "pretext objective. The second stage uses representation from the"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "upstream model to train a downstream task such as SER."
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "We use the SSL-based SER models\nin [17], which uses"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "publicly available speech SSL models collected by the S3PRL"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "toolkit [18]. The upstream models include models trained with"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "generative approach: DeCoAR 2 [19], Autoregressive Predic-"
        },
        {
          "{r12942075,hungyilee}@ntu.edu.tw, cclee@ee.nthu.edu.tw": "tive Coding (APC)\n[20], Vector-Quantized APC (VQ-APC)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: Overview of mapping between valence and emotion. 2.2.2. Upstreamrepresentationbiasonvalence",
      "data": [
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "Surprise can have a positive or negative valence."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "Valence\nEmotion"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "positive\nHappiness, Excitement, Relax, Joy"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "negative Anger, Disgust, Contempt, Frustration, Disappointment, Sadness, Fear"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "both\nSurprise"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "[21], Nonautoregressive Predictive Coding (NPC) [22], TERA"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "[23], Mockingjay (Mock) [24]; models trained with contrastive"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "approach: Wav2Vec2-XLS-R-1B (XLS-R) [14], Wav2Vec 2.0"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "(W2V2)\n[25], Wav2Vec2 Large Robust\n(W2V2 R)\n[26], vq-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "wav2vec\n(VQ-W2V)\n[27], wav2vec\n(W2V)\n[28],\nand Con-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "trastive Predictive Coding (M CPC) [13]; models trained with"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "predictive approach: HuBERT [29], WavLM [30], Data2Vec"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "(D2V) [31]."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "We use\nthe same downstream model\narchitecture\nas\nthe"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "SER task in the S3PRL toolkit\n[18], using three Conv1d,\na"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "self-attention pooling,\nand two linear\nlayers.\nTo capture the"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "high-dimensional nature of emotion [32], we formulate emo-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "tion recognition as\na multilabel\nclassiﬁcation problem.\nWe"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "ﬁrst\ntransform the emotion annotations to emotion distribution"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "by frequency and then apply label smoothing by [33] using a"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "smoothing parameter of 0.05 to obtain a soft\nlabel. We use the"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "F1 score as the evaluation metric for performance, which aligns"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "with SER challenges[34] and benchmarks [17]. A label predic-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "tion is successful\nif the output emotion probability distribution"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "is higher than 1"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "n for n emotion classiﬁcation task. Our models"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "are trained on Nvidia Tesla V100 GPUs with 32 GB of memory."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "The cumulative GPU runtime amounts to approximately 3,300"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "hours."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "2.2. Bias evaluation"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "In our study, we employ three evaluation metrics to assess gen-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "der bias. We hope our evaluation plan can serve as a valuable"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "reference for others."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "2.2.1. F1 score difference"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "We quantify the gender bias dei on each emotion ei by the dif-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "ference of F1\nscore on gender because F1\nscore reﬂects both"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "precision and recall.\nFurthermore, we deﬁne the corpus-level"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "for all\ngender bias dc by averaging the absolute value of dei"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "emotions ei in the set of all emotion E in the corpus:"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "(1)\ndei = F1,eif emale − F1,eimale"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "(2)\ndc = meanei∈E|dei |."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "We further evaluate the gender bias on valence by Eq. 3.1 The"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "from positive va-\nintuition is calculating the difference of dei"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "lence and negative valence:"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "dv = Σei∈E+dei − Σei ∈E−dei"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "(3)\n+ Σei∈Eb (p+dei,+ − p−dei,−)."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "E+, E−, and Eb are the set of emotions in the corpus that be-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "long to positive valence, negative valence,\nand both valence,"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "respectively.\np+ and p− are the portions of speech belonging"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "to positive and negative valence. dei,+ and dei,− are the dei of"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "positive and negative valence speech, respectively. The detail of"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "valence categorization is shown in Table 1, following the study"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "[35]."
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": ""
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "1We don’t use absolute value on dv as dc, because we mean to com-"
        },
        {
          "Table 1: Overview of mapping between valence and emotion.": "pare it with upstream bias in section 2.2.2"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": ""
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": ""
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": ""
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "Ang"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "6.3"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "1.8"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "6.6"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "-3.3"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "-1.2"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "3.8"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "3.9"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "0.0"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "3.1"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "0.7"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": ""
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "Ang"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "6.7"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "1.7"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "1.5"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "1.3"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "-3.5"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "4.4"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "2.1"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "2.0"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "-0.4"
        },
        {
          "Table 2: SER bias on emotion de across 6 emotion datasets and 9 models, in %. The emotions are abbreviated as follows. Angry: Ang,": "0.6"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "stimuli are the same as those for SpEAT, with the goal of en-"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "suring that differences in association with positive and negative"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "valence do not stem from variations among the speakers."
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "3. Result and Discussion"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "3.1. Downstream performance difference"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "Table 2 shows the difference in F1 score between females and"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "males on each dataset and each emotion. Due to space limita-"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "tions, we report the result for models across 3 different training"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "objectives and achieve top-9 performance in EMO-SUPERB. It"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "shows that despite most ER models only exhibiting a slightly"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "high macro-F1 score for females, a high parity exists between"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": ""
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "the F1 score for each emotion. For instance, in the BPO dataset,"
        },
        {
          "[43]\nfor male and female stimuli.\nThe selection criteria for": "males exhibit approximately a 25% higher F1\nscore than fe-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "Model"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "XLS-R"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "WavLM"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "W2V2 R"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "W2V2"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "VQ-APC"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "HuBERT"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "DeCoAR 2"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "D2V"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "APC"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "mean"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "XLS-R"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "WavLM"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "W2V2 R"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "W2V2"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "VQ-APC"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "HuBERT"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "DeCoAR 2"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "D2V"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "APC"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "mean"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": "[43]"
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        },
        {
          "Relax: Rel. Mac represents the macro-F1 score over all emotions": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 6: Pearson correlation coefficient between downstream",
      "data": [
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "F1 score gap on valence dv and upstream representation bias"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "using"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "BPO",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "(Aggr.)."
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.31",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.43",
          "Table 6: Pearson correlation coefﬁcient between downstream": "Stimuli"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.40",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.34",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "MESS"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.53",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.34",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.47",
          "Table 6: Pearson correlation coefﬁcient between downstream": "NNIME"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.44",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.52",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "IMPROV"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.62",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.48",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.39",
          "Table 6: Pearson correlation coefﬁcient between downstream": "PODCAST"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.47",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.36",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.48",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "0.40",
          "Table 6: Pearson correlation coefﬁcient between downstream": "der bias in upstream representation might hardly propagate to"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "downstream emotion classiﬁcation tasks, which contradicts the"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "conclusion in SpEAT. Two possible reasons might contribute"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "3.2. Downstream training data distribution",
          "Table 6: Pearson correlation coefﬁcient between downstream": ""
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "",
          "Table 6: Pearson correlation coefﬁcient between downstream": "to the difference between our work and SpEAT:"
        },
        {
          "Table 4: Pearson correlation coefﬁcient between F1-score gap": "We evaluate the Pearson correlation coefﬁcient between train-",
          "Table 6: Pearson correlation coefﬁcient between downstream": "trains the downstream valence prediction model with only 1800"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 6: Pearson correlation coefficient between downstream",
      "data": [
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "columns named by datasets denote using the weighted sum of",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "embedding from ER models trained with the dataset.",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "Model",
          "layers. Other": "POD"
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "XLS-R",
          "layers. Other": "1.08"
        },
        {
          "denotes taking the average embedding over all": "WavLM",
          "layers. Other": "1.48"
        },
        {
          "denotes taking the average embedding over all": "W2V",
          "layers. Other": "1.35"
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "W2V2 R",
          "layers. Other": "0.83"
        },
        {
          "denotes taking the average embedding over all": "W2V2",
          "layers. Other": "0.57"
        },
        {
          "denotes taking the average embedding over all": "VQW2V",
          "layers. Other": "0.66"
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "VQ-APC",
          "layers. Other": "1.77"
        },
        {
          "denotes taking the average embedding over all": "TERA",
          "layers. Other": "1.43"
        },
        {
          "denotes taking the average embedding over all": "NPC",
          "layers. Other": "1.70"
        },
        {
          "denotes taking the average embedding over all": "M CPC",
          "layers. Other": "0.59"
        },
        {
          "denotes taking the average embedding over all": "Mock",
          "layers. Other": "1.06"
        },
        {
          "denotes taking the average embedding over all": "HuBERT",
          "layers. Other": "1.02"
        },
        {
          "denotes taking the average embedding over all": "DeCoAR 2",
          "layers. Other": "1.46"
        },
        {
          "denotes taking the average embedding over all": "D2V",
          "layers. Other": "0.50"
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        },
        {
          "denotes taking the average embedding over all": "APC",
          "layers. Other": "1.69"
        },
        {
          "denotes taking the average embedding over all": "",
          "layers. Other": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "review,” in International Conference on Human-Computer Inter-": "action, 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ing with Deep Bidirectional Transformer Encoders,” in ICASSP"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2020 - 2020 IEEE International Conference on Acoustics, Speech"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[2]\nS. Feng et al., “Quantifying bias in automatic speech recognition,”",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "and Signal Processing (ICASSP), 2020."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "arXiv preprint arXiv:2103.15122, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[25] A. Baevski et al., “wav2vec 2.0: A framework for self-supervised"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[3] G. Attanasio et al., “Multilingual Speech Models for Automatic",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "informa-\nlearning of speech representations,” Advances in neural"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Speech Recognition Exhibit Gender Performance Gaps,” arXiv",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "tion processing systems, 2020."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "preprint arXiv:2402.17954, 2024.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "et\n[26] W.-N. Hsu\nal.,\n“Robust wav2vec\n2.0:\nAnalyzing Domain"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[4] B. Savoldi et al., “Under\nthe Morphosyntactic Lens: A Multi-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Shift in Self-Supervised Pre-Training,” in Proc. Interspeech 2021,"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "faceted Evaluation of Gender Bias in Speech Translation,” in Pro-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2021."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ceedings of\nthe 60th Annual Meeting of the Association for Com-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[27] A. Baevski et al., “vq-wav2vec: Self-Supervised Learning of Dis-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "putational Linguistics (Volume 1: Long Papers), 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "crete Speech Representations,”\nin International Conference on"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[5] M. Gaido et al., “How to Split:\nthe Effect of Word Segmentation",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Learning Representations, 2020."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "on Gender Bias in Speech Translation,” in Findings of the Associ-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[28]\nS. Schneider\net al.,\n“wav2vec:\nUnsupervised\npre-training\nfor"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ation for Computational Linguistics: ACL-IJCNLP 2021, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "speech recognition,” arXiv preprint arXiv:1904.05862, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[6] A. Domnich and G. Anbarjafari,\n“Responsible AI: Gender bias",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[29] W.-N. Hsu et al., “Hubert: Self-supervised speech representation"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "assessment in emotion recognition,” 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "learning by masked prediction of hidden units,” IEEE/ACM Trans-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[7] G. Fenu et al., “Exploring algorithmic fairness\nin deep speaker",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "actions on Audio, Speech, and Language Processing, 2021."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Science\nand\nIts Applications–\nveriﬁcation,”\nin Computational",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "et\n[30]\nS. Chen\nal.,\n“WavLM: Large-Scale\nSelf-Supervised\nPre-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ICCSA 2020: 20th International Conference, Cagliari, Italy, July",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Training for Full Stack Speech Processing,” IEEE Journal of Se-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "1–4, 2020, Proceedings, Part IV 20, 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "lected Topics in Signal Processing, 2022."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[8] C. Gorrostieta\net al.,\n“Gender De-Biasing in Speech Emotion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[31] A. Baevski\net al.,\n“data2vec:\nA General Framework for Self-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Recognition.” in Interspeech, 2019.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "supervised Learning in Speech, Vision and Language,”\nin Pro-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[9] W.-S. Chien and C.-C. Lee,\n“Achieving Fair Speech Emotion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ceedings of the 39th International Conference on Machine Learn-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Recognition via Perceptual Fairness,”\nin ICASSP 2023 - 2023",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ing, 2022."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "IEEE International Conference on Acoustics, Speech and Signal",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[32] A. S. Cowen and D. Keltner, “Semantic Space Theory: A Com-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Processing (ICASSP), 2023.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "putational Approach to Emotion,” Trends in Cognitive Sciences,"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[10] C. Busso et al., “IEMOCAP: Interactive emotional dyadic motion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2021."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "capture database,” Journal of Language Resources and Evalua-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[33] C. Szegedy\net al.,\n“Rethinking\nthe\nInception Architecture\nfor"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "tion, 2008.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "the IEEE Conference on\nComputer Vision,”\nin Proceedings of"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[11] A. Mohamed et al., “Self-supervised speech representation learn-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Computer Vision and Pattern Recognition (CVPR), 2016."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ing: A review,” IEEE Journal of Selected Topics in Signal Pro-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[34]\nL. Goncalves and C. Busso, “Improving Speech Emotion Recog-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "cessing, 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "nition Using Self-Supervised Learning with Domain-Speciﬁc Au-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "diovisual Tasks,” in Proc. Interspeech 2022, 2022, pp. 1168–1172."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[12] B. T. Atmaja and A. Sasou, “Evaluating self-supervised speech",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "representations\nfor\nspeech emotion recognition,”\nIEEE Access,",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[35] A. S. Cowen et al., “Mapping 24 Emotions Conveyed by Brief"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Human Vocalization,” American Psychologist, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[13] M. Riviere et al., “Unsupervised pretraining transfers well across",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[36]\nI. Slaughter et al., “Pre-trained Speech Processing Models Con-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "languages,” in ICASSP 2020-2020 IEEE International Conference",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "tain Human-Like Biases\nthat\nPropagate\nto\nSpeech\nEmotion"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "on Acoustics, Speech and Signal Processing (ICASSP), 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "the Association for Computational\nRecognition,”\nin Findings of"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Linguistics: EMNLP 2023, 2023."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[14] A. Babu et al., “XLS-R: Self-supervised cross-lingual speech rep-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "resentation learning at scale,” arXiv preprint arXiv:2111.09296,",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[37] R. Lotﬁan and C. Busso, “Building Naturalistic Emotionally Bal-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "anced Speech Corpus by Retrieving Emotional Speech From Ex-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "isting Podcast Recordings,” IEEE Transactions on Affective Com-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[15]\nJ. Wagner et al., “Dawn of the Transformer Era in Speech Emotion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "puting, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Recognition: Closing the Valence Gap,” IEEE Transactions on",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Pattern Analysis &amp; Machine Intelligence, 2023.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[38]\nS. G. Upadhyay et al., “An Intelligent Infrastructure Toward Large"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Scale Naturalistic Affective Speech Corpora Collection,” in 2023"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "et\n[16]\nE. Morais\nal.,\n“Speech Emotion Recognition Using Self-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "11th International Conference on Affective Computing and Intel-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Supervised Features,”\nin ICASSP 2022 - 2022 IEEE Interna-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ligent Interaction (ACII), 2023."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "tional Conference on Acoustics, Speech and Signal Processing",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "(ICASSP), 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[39] C. Busso et al., “MSP-IMPROV: An Acted Corpus of Dyadic In-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "teractions\nto Study Emotion Perception,” IEEE Transactions on"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[17] H. Wu et al., “EMO-SUPERB: An In-depth Look at Speech Emo-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Affective Computing, 2017."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "tion Recognition,” 2024.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[40] H.-C. Chou et al., “NNIME: The NTHU-NTUA Chinese interac-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[18]\nS. wen Yang et al., “SUPERB: Speech Processing Universal PER-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "tive multimodal emotion corpus,” in 2017 Seventh International"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "formance Benchmark,” in Proc. Interspeech 2021, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Conference on Affective Computing and Intelligent\nInteraction"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[19]\nS. Ling and Y. Liu, “DeCoAR 2.0: Deep Contextualized Acoustic",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "(ACII), 2017."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Representations with Vector Quantization,” 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[41] H. Cao et al.,\n“CREMA-D: Crowd-Sourced Emotional Multi-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "et\n[20] Y.-A. Chung\nal.,\n“An Unsupervised Autoregressive Model",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "modal Actors Dataset,” IEEE Transactions on Affective Comput-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "for Speech Representation Learning,” in Proc. Interspeech 2019,",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ing, 2014."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "2019.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[42]\nS. D. Morgan, “Categorical and dimensional ratings of emotional"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "speech: Behavioral ﬁndings from the Morgan emotional speech"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[21] ——,\n“Vector-Quantized Autoregressive Predictive Coding,”\nin",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "set,” Journal of Speech, Language, and Hearing Research, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Proc. Interspeech 2020, 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[43]\nS. H. Weinberger and S. A. Kunath, “The Speech Accent Archive:"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "et\n[22] A. H. Liu\nal.,\n“Non-autoregressive\npredictive\ncoding\nfor",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "towards a typology of English accents,” in Corpus-based studies"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "learning speech representations\nfrom local dependencies,” arXiv",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "in language use, language learning, and language documentation,"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "preprint arXiv:2011.00406, 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2011."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[23] A. T. Liu et al., “TERA: Self-Supervised Learning of Transformer",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[44] M. Bass et al., “Rethinking gender:\nThe nonbinary approach,”"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Encoder Representation for Speech,” IEEE/ACM Transactions on",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "American Journal of Health-System Pharmacy, 2018."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Audio, Speech, and Language Processing, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "review,” in International Conference on Human-Computer Inter-": "action, 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ing with Deep Bidirectional Transformer Encoders,” in ICASSP"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2020 - 2020 IEEE International Conference on Acoustics, Speech"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[2]\nS. Feng et al., “Quantifying bias in automatic speech recognition,”",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "and Signal Processing (ICASSP), 2020."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "arXiv preprint arXiv:2103.15122, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[25] A. Baevski et al., “wav2vec 2.0: A framework for self-supervised"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[3] G. Attanasio et al., “Multilingual Speech Models for Automatic",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "informa-\nlearning of speech representations,” Advances in neural"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Speech Recognition Exhibit Gender Performance Gaps,” arXiv",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "tion processing systems, 2020."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "preprint arXiv:2402.17954, 2024.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "et\n[26] W.-N. Hsu\nal.,\n“Robust wav2vec\n2.0:\nAnalyzing Domain"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[4] B. Savoldi et al., “Under\nthe Morphosyntactic Lens: A Multi-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Shift in Self-Supervised Pre-Training,” in Proc. Interspeech 2021,"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "faceted Evaluation of Gender Bias in Speech Translation,” in Pro-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2021."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ceedings of\nthe 60th Annual Meeting of the Association for Com-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[27] A. Baevski et al., “vq-wav2vec: Self-Supervised Learning of Dis-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "putational Linguistics (Volume 1: Long Papers), 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "crete Speech Representations,”\nin International Conference on"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[5] M. Gaido et al., “How to Split:\nthe Effect of Word Segmentation",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Learning Representations, 2020."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "on Gender Bias in Speech Translation,” in Findings of the Associ-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[28]\nS. Schneider\net al.,\n“wav2vec:\nUnsupervised\npre-training\nfor"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ation for Computational Linguistics: ACL-IJCNLP 2021, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "speech recognition,” arXiv preprint arXiv:1904.05862, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[6] A. Domnich and G. Anbarjafari,\n“Responsible AI: Gender bias",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[29] W.-N. Hsu et al., “Hubert: Self-supervised speech representation"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "assessment in emotion recognition,” 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "learning by masked prediction of hidden units,” IEEE/ACM Trans-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[7] G. Fenu et al., “Exploring algorithmic fairness\nin deep speaker",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "actions on Audio, Speech, and Language Processing, 2021."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Science\nand\nIts Applications–\nveriﬁcation,”\nin Computational",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "et\n[30]\nS. Chen\nal.,\n“WavLM: Large-Scale\nSelf-Supervised\nPre-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ICCSA 2020: 20th International Conference, Cagliari, Italy, July",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Training for Full Stack Speech Processing,” IEEE Journal of Se-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "1–4, 2020, Proceedings, Part IV 20, 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "lected Topics in Signal Processing, 2022."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[8] C. Gorrostieta\net al.,\n“Gender De-Biasing in Speech Emotion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[31] A. Baevski\net al.,\n“data2vec:\nA General Framework for Self-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Recognition.” in Interspeech, 2019.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "supervised Learning in Speech, Vision and Language,”\nin Pro-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[9] W.-S. Chien and C.-C. Lee,\n“Achieving Fair Speech Emotion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ceedings of the 39th International Conference on Machine Learn-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Recognition via Perceptual Fairness,”\nin ICASSP 2023 - 2023",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ing, 2022."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "IEEE International Conference on Acoustics, Speech and Signal",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[32] A. S. Cowen and D. Keltner, “Semantic Space Theory: A Com-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Processing (ICASSP), 2023.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "putational Approach to Emotion,” Trends in Cognitive Sciences,"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[10] C. Busso et al., “IEMOCAP: Interactive emotional dyadic motion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2021."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "capture database,” Journal of Language Resources and Evalua-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[33] C. Szegedy\net al.,\n“Rethinking\nthe\nInception Architecture\nfor"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "tion, 2008.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "the IEEE Conference on\nComputer Vision,”\nin Proceedings of"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[11] A. Mohamed et al., “Self-supervised speech representation learn-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Computer Vision and Pattern Recognition (CVPR), 2016."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "ing: A review,” IEEE Journal of Selected Topics in Signal Pro-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[34]\nL. Goncalves and C. Busso, “Improving Speech Emotion Recog-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "cessing, 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "nition Using Self-Supervised Learning with Domain-Speciﬁc Au-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "diovisual Tasks,” in Proc. Interspeech 2022, 2022, pp. 1168–1172."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[12] B. T. Atmaja and A. Sasou, “Evaluating self-supervised speech",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "representations\nfor\nspeech emotion recognition,”\nIEEE Access,",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[35] A. S. Cowen et al., “Mapping 24 Emotions Conveyed by Brief"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Human Vocalization,” American Psychologist, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[13] M. Riviere et al., “Unsupervised pretraining transfers well across",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[36]\nI. Slaughter et al., “Pre-trained Speech Processing Models Con-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "languages,” in ICASSP 2020-2020 IEEE International Conference",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "tain Human-Like Biases\nthat\nPropagate\nto\nSpeech\nEmotion"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "on Acoustics, Speech and Signal Processing (ICASSP), 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "the Association for Computational\nRecognition,”\nin Findings of"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Linguistics: EMNLP 2023, 2023."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[14] A. Babu et al., “XLS-R: Self-supervised cross-lingual speech rep-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "resentation learning at scale,” arXiv preprint arXiv:2111.09296,",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[37] R. Lotﬁan and C. Busso, “Building Naturalistic Emotionally Bal-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "anced Speech Corpus by Retrieving Emotional Speech From Ex-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "isting Podcast Recordings,” IEEE Transactions on Affective Com-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[15]\nJ. Wagner et al., “Dawn of the Transformer Era in Speech Emotion",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "puting, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Recognition: Closing the Valence Gap,” IEEE Transactions on",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Pattern Analysis &amp; Machine Intelligence, 2023.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[38]\nS. G. Upadhyay et al., “An Intelligent Infrastructure Toward Large"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Scale Naturalistic Affective Speech Corpora Collection,” in 2023"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "et\n[16]\nE. Morais\nal.,\n“Speech Emotion Recognition Using Self-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "11th International Conference on Affective Computing and Intel-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Supervised Features,”\nin ICASSP 2022 - 2022 IEEE Interna-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ligent Interaction (ACII), 2023."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "tional Conference on Acoustics, Speech and Signal Processing",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "(ICASSP), 2022.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[39] C. Busso et al., “MSP-IMPROV: An Acted Corpus of Dyadic In-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "teractions\nto Study Emotion Perception,” IEEE Transactions on"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[17] H. Wu et al., “EMO-SUPERB: An In-depth Look at Speech Emo-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Affective Computing, 2017."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "tion Recognition,” 2024.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[40] H.-C. Chou et al., “NNIME: The NTHU-NTUA Chinese interac-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[18]\nS. wen Yang et al., “SUPERB: Speech Processing Universal PER-",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "tive multimodal emotion corpus,” in 2017 Seventh International"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "formance Benchmark,” in Proc. Interspeech 2021, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "Conference on Affective Computing and Intelligent\nInteraction"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[19]\nS. Ling and Y. Liu, “DeCoAR 2.0: Deep Contextualized Acoustic",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "(ACII), 2017."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Representations with Vector Quantization,” 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[41] H. Cao et al.,\n“CREMA-D: Crowd-Sourced Emotional Multi-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "et\n[20] Y.-A. Chung\nal.,\n“An Unsupervised Autoregressive Model",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "modal Actors Dataset,” IEEE Transactions on Affective Comput-"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "for Speech Representation Learning,” in Proc. Interspeech 2019,",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "ing, 2014."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "2019.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[42]\nS. D. Morgan, “Categorical and dimensional ratings of emotional"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "speech: Behavioral ﬁndings from the Morgan emotional speech"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[21] ——,\n“Vector-Quantized Autoregressive Predictive Coding,”\nin",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "set,” Journal of Speech, Language, and Hearing Research, 2019."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Proc. Interspeech 2020, 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[43]\nS. H. Weinberger and S. A. Kunath, “The Speech Accent Archive:"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "et\n[22] A. H. Liu\nal.,\n“Non-autoregressive\npredictive\ncoding\nfor",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "towards a typology of English accents,” in Corpus-based studies"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "learning speech representations\nfrom local dependencies,” arXiv",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "in language use, language learning, and language documentation,"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "preprint arXiv:2011.00406, 2020.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "2011."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "[23] A. T. Liu et al., “TERA: Self-Supervised Learning of Transformer",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "[44] M. Bass et al., “Rethinking gender:\nThe nonbinary approach,”"
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Encoder Representation for Speech,” IEEE/ACM Transactions on",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": "American Journal of Health-System Pharmacy, 2018."
        },
        {
          "review,” in International Conference on Human-Computer Inter-": "Audio, Speech, and Language Processing, 2021.",
          "[24] ——, “Mockingjay: Unsupervised Speech Representation Learn-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Hey ASR system! Why aren't you more inclusive? Automatic speech recognition systems' bias and proposed bias mitigation techniques. A literature review",
      "authors": [
        "M Ngueajio",
        "G Washington"
      ],
      "year": "2022",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "3",
      "title": "Quantifying bias in automatic speech recognition",
      "authors": [
        "S Feng"
      ],
      "year": "2021",
      "venue": "Quantifying bias in automatic speech recognition",
      "arxiv": "arXiv:2103.15122"
    },
    {
      "citation_id": "4",
      "title": "Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps",
      "authors": [
        "G Attanasio"
      ],
      "year": "2024",
      "venue": "Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps",
      "arxiv": "arXiv:2402.17954"
    },
    {
      "citation_id": "5",
      "title": "Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation",
      "authors": [
        "B Savoldi"
      ],
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation",
      "authors": [
        "M Gaido"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "7",
      "title": "Responsible AI: Gender bias assessment in emotion recognition",
      "authors": [
        "A Domnich",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "Responsible AI: Gender bias assessment in emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "Exploring algorithmic fairness in deep speaker verification",
      "authors": [
        "G Fenu"
      ],
      "year": "2020",
      "venue": "Computational Science and Its Applications-ICCSA 2020: 20th International Conference"
    },
    {
      "citation_id": "9",
      "title": "Gender De-Biasing in Speech Emotion Recognition",
      "authors": [
        "C Gorrostieta"
      ],
      "year": "2019",
      "venue": "Gender De-Biasing in Speech Emotion Recognition"
    },
    {
      "citation_id": "10",
      "title": "Achieving Fair Speech Emotion Recognition via Perceptual Fairness",
      "authors": [
        "W.-S Chien",
        "C.-C Lee"
      ],
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "12",
      "title": "Self-supervised speech representation learning: A review",
      "authors": [
        "A Mohamed"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Unsupervised pretraining transfers well across languages",
      "authors": [
        "M Riviere"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu"
      ],
      "year": "2021",
      "venue": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "16",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Speech Emotion Recognition Using Self-Supervised Features",
      "authors": [
        "E Morais"
      ],
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition",
      "authors": [
        "H Wu"
      ],
      "year": "2024",
      "venue": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition"
    },
    {
      "citation_id": "19",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Yang"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "20",
      "title": "DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization",
      "authors": [
        "S Ling",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization"
    },
    {
      "citation_id": "21",
      "title": "An Unsupervised Autoregressive Model for Speech Representation Learning",
      "authors": [
        "Y.-A Chung"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Vector-Quantized Autoregressive Predictive Coding",
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "authors": [
        "A Liu"
      ],
      "year": "2020",
      "venue": "Non-autoregressive predictive coding for learning speech representations from local dependencies",
      "arxiv": "arXiv:2011.00406"
    },
    {
      "citation_id": "24",
      "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech",
      "authors": [
        "A Liu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders",
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "27",
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "28",
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "30",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "data2vec: A General Framework for Selfsupervised Learning in Speech, Vision and Language",
      "authors": [
        "A Baevski"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning"
    },
    {
      "citation_id": "33",
      "title": "Semantic Space Theory: A Computational Approach to Emotion",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2021",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "34",
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "authors": [
        "C Szegedy"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "Mapping 24 Emotions Conveyed by Brief Human Vocalization",
      "authors": [
        "A Cowen"
      ],
      "year": "2019",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "37",
      "title": "Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition",
      "authors": [
        "I Slaughter"
      ],
      "year": "2023",
      "venue": "Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition"
    },
    {
      "citation_id": "38",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "An Intelligent Infrastructure Toward Large Scale Naturalistic Affective Speech Corpora Collection",
      "authors": [
        "S Upadhyay"
      ],
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "authors": [
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "NNIME: The NTHU-NTUA Chinese interactive multimodal emotion corpus",
      "authors": [
        "H.-C Chou"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "42",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Categorical and dimensional ratings of emotional speech: Behavioral findings from the Morgan emotional speech set",
      "authors": [
        "S Morgan"
      ],
      "year": "2019",
      "venue": "Journal of Speech, Language, and Hearing Research"
    },
    {
      "citation_id": "44",
      "title": "The Speech Accent Archive: towards a typology of English accents",
      "authors": [
        "S Weinberger",
        "S Kunath"
      ],
      "year": "2011",
      "venue": "Corpus-based studies in language use, language learning, and language documentation"
    },
    {
      "citation_id": "45",
      "title": "Rethinking gender: The nonbinary approach",
      "authors": [
        "M Bass"
      ],
      "year": "2018",
      "venue": "American Journal of Health-System Pharmacy"
    }
  ]
}