{
  "paper_id": "2501.09502v1",
  "title": "Omni-Emotion: Extending Video Mllm With Detailed Face And Audio Modeling For Multimodal Emotion Analysis",
  "published": "2025-01-16T12:27:05Z",
  "authors": [
    "Qize Yang",
    "Detao Bai",
    "Yi-Xing Peng",
    "Xihan Wei"
  ],
  "keywords": [
    "https :",
    "github . com",
    "HumanMLLM",
    "Omni-Emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding emotions accurately is essential for fields like human-computer interaction. Due to the complexity of emotions and their multi-modal nature (e.g., emotions are influenced by facial expressions and audio), researchers have turned to using multi-modal models to understand human emotions rather than single-modality. However, current video multi-modal large language models (MLLMs) encounter difficulties in effectively integrating audio and identifying subtle facial micro-expressions. Furthermore, the lack of detailed emotion analysis datasets also limits the development of multimodal emotion analysis. To address these issues, we introduce a self-reviewed dataset and a human-reviewed dataset, comprising 24,137 coarsegrained samples and 3,500 manually annotated samples with detailed emotion annotations, respectively. These datasets allow models to learn from diverse scenarios and better generalize to real-world applications. Moreover, in addition to the audio modeling, we propose to explicitly integrate facial encoding models into the existing advanced Video MLLM, enabling the MLLM to effectively unify audio and the subtle facial cues for emotion understanding. By aligning these features within a unified space and employing instruction tuning in our proposed datasets, our Omni-Emotion achieves state-of-the-art performance in both emotion recognition and reasoning tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion analysis plays a crucial role in areas such as educational tools, avatars, and human-computer interaction. Traditional single-modal methods such as audio-based emotion recognition  [16, 17, 20] , text sentiment analysis  [15, 18, 24] , and facial expression recognition methods [19,  40, 52]  have shown their strengths on single modality emotion perception. However, beyond single modality, real-world emotional expressions often involve textual, visual, and auditory",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Explainable Reason:",
      "text": "The video sentiment analysis report shows that the picture focuses on a simple but dull indoor environment, with a woman wearing black clothes showing an sad and painful expression. Her facial expression underwent significant changes, with her mouth wide open, her eyebrows furrowed, and the corners of her eyes wet with tears, showing extreme pain and anger. In the voice information provided in the video, you can hear the woman's voice mixed with sobs and an excited tone, which are typical expressions of strong negative emotions. As she spoke, she appeared very excited along with her hand movements.\n\nOpen-vocabulary: Sadness, pain, anger Figure  1 . An example from MAFW  [34]  with our self-reviewed annotation shows the integration of multimodal explainable reasons for emotion analysis, involving background information, detailed facial expressions, and audio cues. Besides, the open vocabulary labels provide a more accurate description of the character's emotions in the video.\n\ninformation, but these important cues of human emotions are overlooked in traditional methods.\n\nTo utilize the information from different modalities, previous multimodal methods have focused on feature interaction and modality completion  [13, 29, 48, 57, 60, 63, 65, 66] , but lack of reasoning abilities. Despite significant advancements in multimodal large language models (MLLMs)  [9, 12, 25, 42, 53] , which have demonstrated impressive performance in visual-language and audiolanguage analysis, challenges persist in accurately recognizing emotions.\n\nAs shown in Figure  3 , these methods often fall short in precise emotional analysis even with step-by-step analyses. Recently, Emotion-LLaMA  [10] , AffectGPT  [32] , and EmoLLM  [32]  tackled these issues by integrating universal vision encoders, an audio encoder, and an LLM decoder; however, they lack facial modeling and struggle to capture fine-grained facial information effectively. As a result, these limitations lead to sub-optimal performance in real-world scenarios.\n\nWe argue that another factor hindering the effectiveness of MLLMs is the absence of multimodal emotion instruction datasets with diverse data distributions and detailed annotations. The inconsistency in labeling standards among individuals and varying definitions of expressions also pose significant challenges. Different cultural contexts influence how emotions are expressed, which complicates the acquisition of emotion data and necessitates expert annotation.\n\nTo acquire high-quality multimodal emotion instruction tuning data, we curate two types of emotion datasets from existing labeled emotion datasets. For each video clip, we utilize a general video MLLM with strong performance to describe the general visual cues, including surroundings and characters. Next, we extract the face tracklets and use an age/gender estimator along with general video MLLM to capture facial attributes and fine-grained expressions, respectively. Then, we use GPT-3.5  [41]  to ensure consistency and discard descriptions that do not align with other clues, generating the final multimodal emotion reasoning and open-vocabulary labels. As shown in Figure  1 , the annotations generated by our processing pipeline are highly detailed. By aligning the detailed annotations with the coarse-grained ground-truth classification labels and eliminating the inconsistency, the quality of the annotations is significantly enhanced. Finally, we have collected 24,137 self-reviewed videos with high alignment scores, naming this the self-reviewed emotion (SRE) dataset. Additionally, we evenly selected 3,500 videos for manual verification, resulting in a human-reviewed emotion (HRE) dataset.\n\nWith the high-quality detailed-annotated datasets, we adopt an effective open-source video MLLM as our base model and integrate the audio and face encoder to capture fine-grained auditory clues and facial movement for emotion analysis. Specifically, we first align the audio feature from whisper-large-v3  [46] , and the facial feature from FaceXFormer  [39]  to the general video MLLM embedding space. Subsequently, we leverage our high-quality data to simultaneously train the projectors of three encoders along with the LLM decoder. Our proposed model, named Omni-Emotion, shows strong performance in multiple tasks and is capable of processing information from various modalities. Our main contributions are as follows. High-quality dataset construction. We develop an effective and scalable approach to construct high-quality benchmark for emotion reason analysis and open-vocabulary emotion recognition based on existing emotion recognition datasets. The SRE dataset includes 24,137 videos filtered by GPT scoring, and the HRE dataset includes 3,500 videos that have been manually verified.\n\nIntegrating video MLLM with auditory and facial information. We propose to integrate additional face and audio encoders with the existing video MLLM model for better emotion analysis. We also build a three-stage training process to effectively unify audio encoder and fine-grained face encoder into Video MLLM. State-of-the-Art performance. Our model achieves stateof-the-art (SOTA) results across various emotion analysis tasks. For in-the-wild emotion recognition, the unweighted average recall (UAR) on DFEW  [19]  and MAFW  [34]  of our model are 68.80/53.81 respectively. In open-vocabulary emotion recognition task, our average score in EMER-OV  [30]  is 65.9. In the emotion reason analysis dataset EMER  [30]  , our clue overlap score is 8.22. Our model outperforms existing methods by a clear margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Multimodal Large Language Models (MLLMs). Recently, MLLMs  [1, 4, 8, 11, 26, 43, 44, 54]  have garnered significant attention for their impressive inferential capabilities. Image-based MLLMs  [7, 33, 51, 67]  transformed image-text pairs into instruction-following data and study alignment problems between text and image features, demonstrating impressive multimodal capabilities. Despite these advancements, while most video MLLMs  [25, 28, 62]  utilize pretrained general vision encoders like CLIP  [45] , SigLIP  [61] , and InternVideo2  [56]  for general domain applications, they encounter difficulties in understanding emotions from audio cues and subtle facial expressions. The limitation is also due to the lack of specialized training on datasets that integrate multimodal emotional cues and comprehensive emotion-related knowledge. Compared to these methods, our proposed method can extend strong video MLLM to audio modality with a strong audio encoder, and enable MLLM to capture fine-grained facial cues, thereby facilitating the effective analysis of human emotions in video. Emotion analysis. Emotion and facial expression recognition have predominantly depended on datasets collected under controlled conditions, often leading to suboptimal performance in real-world applications. Some large dynamic facial expressions in-the-wild datasets [19,  34, 55]  facilitate the availability. While the previous methods  [13, 48, 65]  focus on emotion recognition, human emotions are complex and often involve multiple emotions in various situations. Additionally, understanding the underlying causes of these emotions is also crucial. EmoVIT  [59]  generated visual emotion instruction data for training; however, it lacks audio information. On the other hand, EMRE  [30]  offered detailed annotations with explainable reasons for emotions but included only 332 samples. AffectGPT  [30]  and Emotion-LLaMA  [10]  collected a large volume of inference annotations, yet these annotations lack quality assurance. Com-Open Vocabulary (GPT-3.5)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contentment, Concern, Warmth, Optimism",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emer (Gpt-3.5)",
      "text": "The man in the video is wearing a plaid shirt, with white hair and beard, sitting indoors, leisurely sipping coffee. His brows are slightly furrowed, his eyes are sometimes half-closed, revealing a thoughtful expression, and sometimes slightly raised, showing a relieved smile… the whispers in his voice reveal a kind of concern and acceptance of the status quo. Overall, he shows a gentle, tolerant and slightly relieved emotional state, full of optimism about life. Figure  2 . The processing pipeline of our proposed datasets consists of three aspects, including the extraction of global visual information, fine-grained facial details, and audio cues. First, we conduct sparse sampling at 1 FPS for each video, using Qwen2-VL to capture global visual information. For detailed facial analysis, a face detector and tracking method are employed to extract face tracklets, followed by dense sampling and analysis using an age/gender estimator and Qwen2-VL. Lastly, we use Whisper-large-v3 to analyze the audio cues.\n\nWe then employ GPT-3.5 to ensure consistency and accuracy, discarding descriptions that do not align with other clues. Based on these results, we extract explainable reasons, open vocabulary descriptions, and intensity for emotion of each video. Finally, we utilize GPT-3.5 to review the generated data referring to existing coarse-grained classification labels and obtain the self-reviewed emotion (SRE) dataset. We select partial data from SRE dataset and perform manual verification to build the human-reviewed emotion (HRE) dataset.\n\npared to Emotion-LLaMA, our collected data with selfreview by GPT-3.5 using ground-truth labels and manual reviews. Additionally, we extend existing video MLLMs to capture detailed facial and auditory information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Reviewed Emotion Dataset",
      "text": "In this section, we introduce the processing of emotional data and the curation of our proposed datasets. Existing emotion datasets  [19, 34, 55]  predominantly use categorical labels, which often fail to capture the nuanced emotions and expressions in videos. Although initiatives like EMER  [30]  attempt to use open vocabulary and detailed reasons for annotation, their scale is limited to only 332 entries because the process of emotion annotation demands highly skilled emotion knowledge, making it prohibitively expensive. Emotion-LLaMA  [10]  introduces the MERR dataset with model-inferred descriptions but lacks thorough quality assessments. Besides, their explanations for the emotions are not detailed enough to fully capture emotional clues.\n\nTo collect a substantial and reliable dataset for accurately analyzing emotions in videos, we propose to leverage existing MLLM and advanced facial analysis model to provide detailed, comprehensive annotations to enrich existing emotion datasets, and perform self-reviewing by comparing the ground-truth classification labels and the inferring detailed annotations. Our approach entails reasoning based on datasets including the training set from DFEW [19], MAFW  [34] , MER24  [31] , CAER  [23] , AFEW-VA  [21] , RAVDESS  [35] , and FERV39K  [55] , exploring global information, detailed facial features, and audio cues to extract emotional clues.\n\nOur method involves three aspects, as illustrated in Figure  2 . First, we perform low-density sampling at 1 FPS for each video and use Qwen2-VL-72B for video description. Simultaneously, we use face detection [14] and tracking  [58]  methods to extract each facial tracklet and conduct fine-grained analysis of age, gender, and facial expressions. Specifically, MiVOLO  [22]  is employed for age and gender estimation, and Qwen2-VL-72B  [53]  is employed to describe the detailed facial movement. Finally, we extract audio tracks and utilize Whisper-large-v3  [46]  for captioning, automatic speech recognition (ASR), and emotion recognition. The consistency of the information obtained from these steps is verified using GPT-3.5  [41] . Inconsistent or uncertain data is discarded, ensuring high-quality intermediate results.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Self-Reviewed Emotion Dataset Curation",
      "text": "Building on these results, we derive explainable reasoning clues for emotions by analyzing them from visual, auditory, and textual perspectives, and obtain open-vocabulary annotations. To ensure quality, we utilize the original labels from these datasets and prompt GPT-3.5 to assess the alignment degree between the ground-truth labels and explainable reason statements on a 0-10 scoring scale. In Figure  3 , we visualize the score statistics on DFEW [19] and MAFW  [34] . Despite rigorous processing, achieving high agreement scores remains challenging, underscoring the complexity of emotion recognition. Accordingly, we filter out samples with a score below 5.\n\nAfter filtering with a score threshold, our self-reviewed annotation process extends the traditional emotion classification to emotion analysis from multiple perspectives while maintaining a high level of agreement. We further remove the RAVDESS dataset considering the video clips are collected from the indoor studio. Finally, the self-reviewed dataset comprises 24,137 samples, as shown in Figure  4 , and we refer to this dataset as the Self-Reviewed Emotion (SRE) dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Human-Reviewed Emotion Dataset Curation",
      "text": "In addition, we have constructed a human-annotated dataset by randomly selecting 700 samples from each training set of these datasets, ensuring no overlap with the SRE dataset, namely human-reviewed emotion (HRE) dataset. Ultimately, we assembled a higher-quality training dataset consisting of 3500 samples. Our SRE and HRE dataset includes a wide range of samples from diverse sources.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Omni-Emotion Mllm",
      "text": "While existing open-source video MLLMs have achieved significant improvements in video perception tasks, they fall short in emotion analysis due to their limited ability to process audio inputs and capture fine-grained facial information. To mitigate these limitations, we propose to extend the video MLLM to audio modality first and then introduce the facial detailed modeling ability to capture fine-grained facial clues. An illustration of our proposed Omni-Emotion MLLM is in Figure  5 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Extending Video Mllm To Audio Modality",
      "text": "We use the audio encoder from Whisper-large-v3  [46]  as our audio encoder, which is a state-of-the-art model for automatic speech recognition (ASR) and speech translation.\n\nGiven the paired data (a, x), where the a and x denote the audio sequences and text sequences, the training objective focuses on maximizing the probability of the next text token, formulated as:\n\nwhere θ and ϕ denote the parameters of the LLM decoder and audio encoder, respectively. The probability is conditioned on the audio representations and preceding text sequence x < t, aiming to bridge audio features with language features. Specifically, the initialization of the audio encoder is loaded from the encoder weights of the Whisper-large-v3 model. We resample each audio data to a frequency of 16KHz and convert the raw waveform into 128 channel melspectrogram using a window size of 25ms and a hop size of 10ms. We use a pooling layer with a stride 3 to reduce to token length, and each token approximately corresponds to a 60ms segment of the original audio signal. Each audio feature is fed into the audio projector, which consists of two linear layers. The output dimension of the projector is designed to match the hidden embedding size of LLM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Detail Facial Modeling",
      "text": "Most video MLLMs utilize general visual encoders, such as SigLip  [61] , CLIP  [45] , or InternVideo2  [56] , to extract general visual features from videos. However, these encoders do not specifically focus on extracting person-related or face-related features, and the features they capture often contain many other objects or background cues. While these cues are valuable for general tasks, they are not particularly important for analyzing human emotions. One of the crucial clues for analyzing emotions is facial information. Hence, we introduce an additional facial feature encoder in our Omni-Emotion MLLM and adopt the encoder in FaceXFormer  [39]  as our facial feature encoder. FaceXFormer is an end-to-end encoder-decoder transformer architecture designed for a wide range of facial analysis tasks, including face parsing, landmark detection, head pose estimation, and attribute recognition. We believe its encoder could provide robust and generalized face representation capable of handling images in the wild.\n\nFor an input video frame, we first extract the finegrained multi-scale features; and then a lightweight MLPfusion module generates a fused face representation from the multi-scale features. The transformed features are finally concatenated together, flattened, and fed into the facial projector, which consists of two linear layers. The output dimension of the projector is equal to the hidden embedding size of LLM.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Feature Fusion Module",
      "text": "To maintain general knowledge while capturing finegrained facial details, we develop a vision fusion module to integrate the facial features with general visual features. Given the different lengths of these features, approaches like the weighted sum or concatenation along the channel dimension are not applicable.\n\nAs shown in Figure  6 , we examine three methods: con- shows the first approach that directly concatenates facial and visual features for each frame, ensuring that each frame contains both visual and facial features. The modeling of the relation between facial and visual information is left to the LLM decoder. The second approach uses crossattention, with facial features act as the query and visual features as the keys and values, to extract person-related in-formation from the visual features, as shown in Figure  6  (b). The third approach is to separately extracts the visual and facial features for the entire video and then concatenating them. This fusion method has the least effect on the original video MLLM. The fusion features serve as our final visual tokens, which are then combined with embeddings from other modalities and fed into the LLM decoder.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Detail",
      "text": "Our proposed Omni-emotion MLLM is built on LLaVA-OneVision-7B  [25] . We keep the general visual encoder and projector from LLaVA-OneVision-7B and introduce an additional facial and audio encoder with the corresponding projectors. To effectively unify the information from three encoders, we first align the audio and face features to the original MLLM feature space. Phase 1: Audio Feature Alignment. To align the audio features with the text embeddings, we use CommonVoice-15  [2] , WavCaps  [38] , and VGGSound  [5]  as audio-text alignment datasets. We format the original labels into diverse instruction tuning formats instead of task tagging as in  [46] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Example Of Audio Qa Pair",
      "text": "Question:Listen to this audio clip and provide its caption in English.\n\nAnswer: Wind and a man speaking are heard, accompanied by buzzing and ticking.\n\nDuring this training phase, we freeze the rest of the model and only train the audio projector for one epoch, using a batch size of 256 and a learning rate of 1e-3. As a result, the audio features are aligned with the text embedding space. Phase 2: Facial Feature Alignment. In the second phase, we reformat the origin emotion recognition data into the instruction-tuning format. We use the training set from seven emotion classification datasets and ask MLLM to choose the most matching option from the option list. The classification annotation is formatted as follows:\n\nExample of emotion recognition QA pair Question: As an emotional recognition expert, when you observe the video, what is the primary emotion exhibited by the characters? surprise, neutral, fear, angry, happy, sad, disgust.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Answer: Neutral.",
      "text": "In this training phase, we only use the general visual and facial features, freeze the rest of the model, and only train the facial projector for 1 epoch. The learning rate is set to 1e-3, and the total batch size is 256. Phase 3: Multimodal Instruction Tuning. In this training phase, we leverage the general visual and facial features of each frame. For the features of each frame, we add a learnable parameter to help the model distinguish between different frame sequences, thereby improving temporal modeling. Additionally, for both visual and audio features, we use specific tokens to label them. The multimodal template is denoted as:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Example Of Modality Special Token <Vi Start><Vision Feature><Vi End><Au Start> <Audio Feature><Au End>",
      "text": "The wrapped multimodal embedding is further wrapped by the original chat template of the LLM. If one modality is absent, we use a zero tensor to replace it. This approach helps the model differentiate between features from different modalities, preventing information confusion in cases where one modality may be absent (i.e., preventing the model from being uncertain whether an embedding originates from the visual, auditory, or textual domain).\n\nIn this phase, our goal is to develop an omnimodal emotion model for open-vocabulary emotion recognition and multimodal emotion reasoning using the proposed SRE and HRE datasets. We fine-tune three projectors and the LLM decoder for a total of 3 epochs. We sample 8 frames for each video and use the concatenation of the general visual features and facial features at the video level as the default fusion feature. The learning rate and batch size are 1e-5 and 128, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "We conduct experiments across distinct settings to evaluate the effectiveness of our proposed method and dataset. Since human emotions are often mixed or extend beyond these predefined categories, (1) we follow AffectGPT  [32]  and employ all the open-vocabulary labeled data provided in the EMER dataset to assess open-vocabulary emotion recognition capabilities, which include 332 samples in total. We refer to this setting as \"EMER-OV\" in the following text. (2) Additionally, to further analyze the underlying reasons for emotions, we leverage the explainable multimodal emotion recognition descriptions in EMER to evaluate the model's emotion reasoning abilities. (3) Finally, we report additional results in emotion recognition task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Open-Vocabulary Emotion Recognition",
      "text": "Following AffectGPT  [32] , we use the open-vocabulary labels in EMER-OV  [30]  to evaluate the open-set prediction performance of our model. The EMER-OV includes 332 open-vocabulary-labeled samples. In this experiment, we compare our method with AffectGPT  [32] .\n\nWe use the same method as  [32]  to group all labels and convert the emotion grouping information into a function that can map each label to its group ID. And then map each label into its group ID. Finally, we can compute the recall, precision, and average score between the ground-truth group Y and the predicted group Ŷ as follows:\n\nFor more computation details, please refer to  [30, 32] .\n\nAs shown in Table  1 , our model significantly outperforms AffectGPT, especially in terms of recall. Besides, our model can return a richer set of emotional descriptors, such as \"Excited\", \"Calm\", \"Frustration\", and others. This experiment confirms the quality and diversity of our collected datasets and highlights the potential and effectiveness of our method as a promising solution for real-world applications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Emotion Reasoning",
      "text": "The explainable reason description in EMER dataset is significantly different from traditional emotion recognition datasets due to the inclusion of additional multimodal emotion clues, such as facial micro-expressions, speech tone, and contextual video information, alongside the emotion categories. To evaluate the emotional reasoning abilities of various MLLMs on the EMER dataset, we use the same setting as the Emotion-LLaMA  [10]  and use ChatGPT to score their predictions, focusing on two metrics:  (1)  the overlap between the emotion-related cues, and (2) the overlap in the summarized emotional states. As shown in Table 2, our proposed method achieves the highest on both metrics. These experiments validate the effectiveness of our proposed method and the quality of SRE and HRE datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis",
      "text": "Ablation study on each modality. In Table  1 , we perform modality analysis in EMER-OV. We can observe that the auditory modality significantly improves recall and accuracy. Furthermore, the combination of \"vision+audio+face\" achieves the best average performance, while the accuracy is slightly lower than the \"vision+audio\" combination.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Models",
      "text": "Clue These experimental results reveal that the fine-grained face clue is also important for multimodal open-vocabulary emotion recognition.\n\nAnalysis on visual feature fusion. We analyze the visual fusion modules on EMER-OV and EMER datasets, and present the results in Table  3 . While the concatenation on the video level achieves the highest score in both settings, we believe that this design is most compatible with the original video MLLM. However, given the significant gap in our training data compared to the general video MLLM, we will further explore frame fusion methods.\n\nQualitative result We present an example of qualitative comparison with Emotion-LLaMA in Table  4 . Our model provides more accurate descriptions of surroundings, the character, and a more detailed understanding of facial expressions, enabling precise inference of the reasons behind the emotions. More examples are provided in the supplementary materials.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Recognition Evaluation.",
      "text": "We utilized two in-the-wild datasets DFEW [19] and MAFW  [34]  containing audio information to evaluate the emotion recognition performance of our model. The test sets for DFEW and MAFW contain 7 and 11 emotion categories, respectively. In this task, we reuse question-answer pairs data as discussed in phase 2 to perform instruction tuning. We then applied instruction tuning for projectors and the LLM decoder using this training data for 3 epochs. We compared our model with other MLLMs and state-ofthe-art (SOTA) methods using unweighted average recall (UAR) and weighted average recall (WAR) as metrics. The experimental results are shown in Table  5 . Compared to single-modality approaches, multimodal methods show a\n\nThe reasoning results of \"sample 00000342.mp4\" In EMER  [30]  dataset Subtitle (En) Hao Zhi, are you the Hao Zhi from Fuwai Hospital?",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ground Truth",
      "text": "In the video, it is impossible to determine the emotional state of the person. In the audio, when expressing \"Hao Zhi\", the tone rises and the tone sounds surprised. In the text, the subtitle says, \"Hao Zhi, are you the Hao Zhi from Fuwai Hospital?\" Based on the description of the rising tone and the surprised tone in the audio clue, as well as the inability to determine the emotional state of the person in the video clue, we can infer that this sentence may carry a sense of surprise or suspicion. The woman may be surprised or uncertain about Hao Zhi's identity, hence expressing this question.\n\nEmotion-LLaMA in the video, the young woman's facial expression and body language suggest a sense of surprise, as she is standing in front of a mirror, looking at herself, possibly admiring her appearance or reflecting on her thoughts her eyebrows are slightly raised, which can indicate a moment of curiosity or interest her mouth is open, which may be a result of her surprise or simply an expression of concentration her posture appears relaxed, with her arms at her sides, and her hands not visible, indicating a lack of tension or anxiety her overall demeanor suggests a sense of calmness and introspection, as she is engaging in self-reflection or admiring her own appearance, which is often associated with a feeling of surprise",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Omni-Emotion",
      "text": "In the video, a young woman dressed in a pink dress stands in a cozy indoor setting. Her expression shifts from surprise to confusion, ultimately revealing a look of concern. Her brows are slightly furrowed, her eyes wide open, and her lips slightly parted, as if she is trying to comprehend or respond to some unexpected information. This emotional transition suggests that she may have received shocking news or encountered an unforeseen situation, leaving her feeling puzzled and worried. The entire process showcases her immediate reaction to external stimuli and her inner turmoil.   5 . Comparison to SOTA methods in two large in-the-wild emotion datasets (DFEW and MAFW). M denotes the modality significant advantage. Our proposed method achieves stateof-the-art performance on both datasets, with a notable margin on the MAFW dataset. The MAFW dataset includes 11 emotion classes, presenting a greater challenge that requires capturing fine-grained facial expressions and audio cues.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations",
      "text": "Our SRE and HRE datasets are curated from existing datasets, yet the video diversity remains far from that en-countered in real-world applications. For instance, in most scenarios, users tend to capture data using the front-facing camera, leading to a different data distribution with existing sources. Moreover, despite our proposed method attaining SOTA performance, its accuracy in handling mixed emotions still falls short. Furthermore, the size of our model and the input token length also pose substantial challenges for practical deployment.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "Current MLLMs also struggle with integrating audio and identifying facial micro-expressions due to insufficient detailed training data. To address this, we introduce a selfreviewed dataset with 24,137 samples and 3,500 manually annotated with detailed emotion annotations, enhancing model learning and generalization. We propose a novel method that incorporates audio and facial encoders into existing Video MLLMs, effectively capturing facial and audio cues. By aligning features in a unified space and using instruction tuning, our method, Omni-Emotion, achieves SOTA results in emotion recognition and reasoning.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "More Qualitative Results Details Of Datasets",
      "text": "In Table  6 , 7, and 8, we provide more visualization comparisons. Our model not only captures overall visual information and subtle facial expressions but also analyzes audio within the video. Our scores are significantly higher than those of Emotion-LLaMA  [10] , closely aligning with the details in the ground truth.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Instructions Details Of Training",
      "text": "Audio feature alignment. In our audio training, we used a total of 1.5M audio instructions tuning data. Below, we present more instruction examples.\n\nExamples of instructions for audio alignment.\n\n• Listen to this audio clip and provide its caption in English.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "• Could You Summarise What'S Happening In This Audio?",
      "text": "• Please describe the audio in English.\n\n• Describe the following audio in a caption.\n\n• Write down the content of the speech you heard.\n\n• Give me the transcription of the speech you heard.\n\n• lease transcribe the speech into a written format.\n\n• Recognize the speech and write it down in a written format. • Recognize the speech and give me the transcription.\n\nFacial feature alignment. In our facial alignment training, we utilized about 130K data from 7 datasets (including the training sets from DFEW [19], MAFW  [34] , MER24  [31] , CAER  [23] , AFEW-VA  [21] , RAVDESS  [35] , and FERV39K  [55] ) for alignment training of the facial projector.\n\nHere are examples of instructions for facial alignment:\n\nExamples of instructions for facial alignment.\n\n• Which emotion exhibited by the characters can you confirm as the primary emotion? disgust, angry, sad, surprise, neutral, fear, happy Examples of instructions for multimodal tuning.\n\n• What insights can we gain about the character's emotional state from their actions and facial expressions, as well as the accompanying audio and visual cues in the video? Please provide a detailed analysis. • Based on the character's physical actions and emotional expressions, along with the video's sound and visual context, what can we deduce about their emotional state? Please elaborate thoroughly.\n\n• What can we interpret about the character's emotional state through their expressive actions and visual cues, as well as the audio elements present in the video? Please provide an in-depth analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Annotation Comparison With Other Datasets",
      "text": "In Table  9 , we compare our dataset with EMER  [30]  and MERR  [10] . Our annotations are more aligned with EMER and more detailed compared to MERR. Additionally, we ensure the quality of our dataset through self-review against the original ground-truth labels and human review, which also allows for a larger scale of annotations. Additionally, we would like to correct the citation regarding the source of the EMER  [30]  data in Section 3 of the main text; the correct source is MER24  [31] . We apologize for this mistake.\n\nThe reasoning results of \"sample 00000342.mp4\" In EMER  [30]  dataset Subtitle (En) Hao Zhi, are you the Hao Zhi from Fuwai Hospital?",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ground Truth",
      "text": "In the video, it is impossible to determine the emotional state of the person. In the audio, when expressing \"Hao Zhi\", the tone rises and the tone sounds surprised. In the text, the subtitle says, \"Hao Zhi, are you the Hao Zhi from Fuwai Hospital?\" Based on the description of the rising tone and the surprised tone in the audio clue, as well as the inability to determine the emotional state of the person in the video clue, we can infer that this sentence may carry a sense of surprise or suspicion. The woman may be surprised or uncertain about Hao Zhi's identity, hence expressing this question.\n\nEmotion-LLaMA Output: in the video, the young woman's facial expression and body language suggest a sense of surprise, as she is standing in front of a mirror, looking at herself, possibly admiring her appearance or reflecting on her thoughts her eyebrows are slightly raised, which can indicate a moment of curiosity or interest her mouth is open, which may be a result of her surprise or simply an expression of concentration her posture appears relaxed, with her arms at her sides, and her hands not visible, indicating a lack of tension or anxiety her overall demeanor suggests a sense of calmness and introspection, as she is engaging in self-reflection or admiring her own appearance, which is often associated with a feeling of surprise Score: 2 Reason: The predicted description does not align with the actual description of the character's emotional state.\n\nThe actual description indicates a sense of surprise or suspicion, while the predicted description infers a somber or melancholic mood, sadness, or depression. The overlap between the predicted and actual emotional states is low, resulting in a lower score.",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Omni-Emotion",
      "text": "Output: In the video, a young woman dressed in a pink dress stands in a cozy indoor setting. Her expression shifts from surprise to confusion, ultimately revealing a look of concern. Her brows are slightly furrowed, her eyes wide open, and her lips slightly parted, as if she is trying to comprehend or respond to some unexpected information. This emotional transition suggests that she may have received shocking news or encountered an unforeseen situation, leaving her feeling puzzled and worried. The entire process showcases her immediate reaction to external stimuli and her inner turmoil. Score: 10 Reason: The predicted description accurately captures the emotional state of the character, reflecting her initial surprise, transition to confusion, and ultimate concern, aligning with the actual description of the character's emotional state of surprise or suspicion. The overlap between the predicted and actual emotional states is high, resulting in a higher score.\" Table  6 . Examples of multimodal emotion reasoning comparing Omni-Emotion with other emotion MLLMs. We highlight the incorrect reasoning with red color.\n\nThe reasoning results of \"sample 00000927.mp4\" In EMER  [30]  dataset joy in the audio clues, we can infer that this sentence is a greeting or expectation from the lady to an intimate person. The lady's expression and tone both show positive emotions, indicating her happiness and anticipation for the person's return.\", \"p\": \"In the video, the woman's emotional state is one of warmth and anticipation. She is wearing a white top with black polka dot accents and has long, dark hair. Her facial expression is gentle and her eyes are filled with warmth and hope. She is smiling and her body language is relaxed, indicating that she is enjoying the conversation on the other end of the phone. As the conversation progresses, her smile deepens, showing her inner joy and anticipation. When she mentions \\\"Xiao Qiu Yin, when are you coming back?\\\", her tone is calm and slightly excited, revealing her eagerness for the return of a loved one. Overall, her emotional state is one of warmth, happiness, and hopeful anticipation.\" in the video, the woman's facial expressions and body language suggest a neutral emotional state her facial micro expressions, such as brow lowerer, lip corner depressor, and slight frown, do not convey any strong emotions her tone and intonation are also neutral, which further supports the inference of a neutral emotional state the fact that she is holding a cell phone and talking on it, possibly engaging in a conversation or checking her messages, suggests that she is focused on a task or activity rather than experiencing any intense emotions",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Subtitle (En)",
      "text": "Little worm, when will you come back?",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ground Truth",
      "text": "In the video, in the opening scene, we see a lady in an indoor environment. Her facial expression appears slightly serious, with a furrowed brow and tightly closed lips, as if she is attentively listening to a conversation on the phone. As time passes, in the middle scene, there is a noticeable change in the lady's expression. Her lips curve upwards, revealing a smile, and her eyes show relaxation and joy. This change may indicate that the conversation on the phone has become positive or that she has heard news that made her happy. The entire conversation process seems positive, making her feel happy. In the audio, the tone is gentle, with a slight rising intonation, and there is a sense of anticipation and joy in the speech. In the text, the subtitle says, \"Little worm, when will you come back?\" This sentence is likely what the lady said to the person on the phone. Based on the lady's facial expression change from seriousness to a smile in the video clues, as well as the description of a gentle tone with anticipation and joy in the audio clues, we can infer that this sentence is a greeting or expectation from the lady to an intimate person. The lady's expression and tone both show positive emotions, indicating her happiness and anticipation for the person's return.\n\nEmotion-LLaMA Output: in the video, the woman's facial expressions and body language suggest a neutral emotional state her facial micro expressions, such as brow lowerer, lip corner depressor, and slight frown, do not convey any strong emotions her tone and intonation are also neutral, which further supports the inference of a neutral emotional state the fact that she is holding a cell phone and talking on it, possibly engaging in a conversation or checking her messages, suggests that she is focused on a task or activity rather than experiencing any intense emotions Score: 3 Reason: The predicted description does not align with the actual description of the character's emotional state. The actual description indicates happiness and anticipation, while the predicted description infers a neutral emotional state. The overlap between the predicted and actual emotional states is low, resulting in a lower score.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Omni-Emotion",
      "text": "Output: In the video, the woman's emotional state is one of warmth and anticipation. She is wearing a white top with black polka dot accents and has long, dark hair. Her facial expression is gentle and her eyes are filled with warmth and hope. She is smiling and her body language is relaxed, indicating that she is enjoying the conversation on the other end of the phone. As the conversation progresses, her smile deepens, showing her inner joy and anticipation. When she mentions \"Xiao Qiu Yin, when are you coming back?\", her tone is calm and slightly excited, revealing her eagerness for the return of a loved one. Overall, her emotional state is one of warmth, happiness, and hopeful anticipation. Score: 10 Reason:The predicted description accurately captures the emotional state of the character, reflecting her warmth, happiness, and hopeful anticipation, aligning with the actual description of the character's joyful and anticipatory emotional state. The overlap between the predicted and actual emotional states is high, resulting in a higher score. The reasoning results of \"sample 00002292.mp4\" In EMER  [30]  dataset\n\n\"reason\": \"The predicted description focuses more on confusion, deep contemplation, and internal conflict, which deviates from the actual description that emphasizes nervousness, concern, seriousness, and admonishment. There is a lack of alignment in the emotional states described, leading to a lower score.\"\n\n\"a\": \"In the video, the screen shows a male character in what appears to be an indoor environment, as curtains can be seen in the background. In the video, he frowns and his facial expression looks serious, indicating some nervousness and concern. He maintains direct eye contact with the other person and his mouth moves, indicating that he is engaged in a serious conversation. Towards the end of the video, he accompanies his speech with head movements, which often signify emphasis. Overall, he is engaged in a conversation that is not easygoing.In the audio, the character's tone is serious. Combined with the text content, it seems to carry a sense of admonishment towards the other person.In the text, the subtitle reads: \\\"You forgot what the doctor said, the doctor said if you have a child, there will be a risk to your life.\\\" This sentence may be an expression of advice or warning from the male character to the other person. Based on the serious facial expression and direct eye contact of the male character in the video, as well as the emphasized head movements, it can be inferred that he is engaged in a serious conversation and his tone is serious. Combined with the description of the serious tone and admonishing meaning in the audio clues, it can be speculated that this sentence is the male character reminding the other person about the potential life-threatening risks of having a child, expressing his concern and nervousness.\", \"p\": \"In the video, we can infer that the man is experiencing a complex emotional state. His facial expressions show a mix of confusion and deep contemplation, with furrowed brows and a slightly open mouth indicating his inner turmoil. The subtle changes in his facial muscles suggest that he is struggling to understand or process information. Additionally, his voice carries a tone of uncertainty and hesitation, with a slower pace and a slight tremor in his speech, reflecting his internal conflict and anxiety about the unknown future. Overall, the man appears to be grappling with a significant decision or dilemma, as evidenced by his intense focus and emotional fluctuations.\" \"in the video, the man's facial expressions, including brow lowerer, upper lid raiser, and lip corner puller, suggest a state of anger his furrowed brow and tightened lips further reinforce this emotion his tone and intonation are also angry, which is evident from his raised voice and aggressive tone the physical cues, such as his clenched fists and tense posture, indicate that he is physically prepared for confrontation or aggression the combination of these facial expressions, tone, and physical cues suggests that the man is experiencing a strong negative emotion, likely anger, due to his intense focus on the object in front of him, which may be a threat or a source of frustration\"",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Subtitle (En)",
      "text": "You forgot what the doctor said, the doctor said if you have a child, there will be a risk to your life Ground Truth In the video, the screen shows a male character in what appears to be an indoor environment, as curtains can be seen in the background. In the video, he frowns and his facial expression looks serious, indicating some nervousness and concern. He maintains direct eye contact with the other person and his mouth moves, indicating that he is engaged in a serious conversation. Towards the end of the video, he accompanies his speech with head movements, which often signify emphasis. Overall, he is engaged in a conversation that is not easygoing. In the audio, the character's tone is serious. Combined with the text content, it seems to carry a sense of admonishment towards the other person. In the text, the subtitle reads: \"You forgot what the doctor said, the doctor said if you have a child, there will be a risk to your life.\" This sentence may be an expression of advice or warning from the male character to the other person. Based on the serious facial expression and direct eye contact of the male character in the video, as well as the emphasized head movements, it can be inferred that he is engaged in a serious conversation and his tone is serious. Combined with the description of the serious tone and admonishing meaning in the audio clues, it can be speculated that this sentence is the male character reminding the other person about the potential life-threatening risks of having a child, expressing his concern and nervousness.\n\nEmotion-LLaMA Output: in the video, the man's facial expressions, including brow lowerer, upper lid raiser, and lip corner puller, suggest a state of anger his furrowed brow and tightened lips further reinforce this emotion his tone and intonation are also angry, which is evident from his raised voice and aggressive tone the physical cues, such as his clenched fists and tense posture, indicate that he is physically prepared for confrontation or aggression the combination of these facial expressions, tone, and physical cues suggests that the man is experiencing a strong negative emotion, likely anger, due to his intense focus on the object in front of him, which may be a threat or a source of frustration Score: 4 Reason: The predicted description only focuses on the emotional state of the character, which is anger, while the actual description indicates a serious and concerned emotional state. There is a mismatch between the predicted and actual emotional states, resulting in a lower score.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Omni-Emotion",
      "text": "Output: In the video, we can infer that the man is experiencing a complex emotional state. His facial expressions show a mix of confusion and deep contemplation, with furrowed brows and a slightly open mouth indicating his inner turmoil. The subtle changes in his facial muscles suggest that he is struggling to understand or process information. Additionally, his voice carries a tone of uncertainty and hesitation, with a slower pace and a slight tremor in his speech, reflecting his internal conflict and anxiety about the unknown future. Overall, the man appears to be grappling with a significant decision or dilemma, as evidenced by his intense focus and emotional fluctuations.\n\nScore: 8 Reason: The predicted description accurately captures the emotional state of the character, reflecting his inner turmoil, anxiety, uncertainty, and deep contemplation, which aligns with the actual description of the character's nervousness, concern, and serious tone. The overlap between the predicted and actual emotional states is high, resulting in a higher score. EMER  [30]  One-hot label: surprise EMER description: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment.\n\nOpen-vocabulary labels: surprise, nervous, dissatisfied 332 None MERR  [10]  Coarse-grained annotation: The woman in the video is talking to a man, possibly discussing something important or sharing her thoughts and feelings. The woman's expression and actions include eyes widened and mouth wide open, and the woman in the video speaks with an excited voice. Saying: Oh my! Fine-grained description: In the video, a woman is conversing with a man. Her facial expressions, with eyes widened and mouth wide open, clearly show surprise. She amplifies this visual cue with an excited tone as she exclaims, \"Oh my.\" This combination of voice and expression indicates that she is experiencing surprise, likely in response to unexpected news shared during the conversation.\n\n28,618/4,487 None Ours Original label: angry EMER description: The scene in the video is set in a private space, with walls painted bright red, creating a warm and slightly oppressive atmosphere. The focus of the picture is on a woman, who is wearing a yellow top with a black watch as an embellishment. The woman's facial expression shows emotional fluctuations of surprise, confusion, struggle and finally anger, with her eyes wide open, brows furrowed, and mouth slightly open to show teeth. From the video, we can clearly feel the rapid emotional evolution of the woman's heart, which seems to be an emotional reaction of psychological tension and anger caused by receiving unexpected news. According to voice recognition technology, her voice is accompanied by shouts of surprise or rapid breathing, which will echo her facial expressions and body language, further confirming that she is experiencing a strong emotional reaction. Comprehensive analysis shows that the woman in the video has experienced an emotional transition from surprise to obvious anger and dissatisfaction. Her facial expressions and body language clearly show this emotional journey, showing an immediate emotional reaction of being violated or dissatisfied.\n\nOpen-vocabulary labels: angry, surprised, dissatisfied 24,137/3,500 Self-review against the original labels and human review.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example from MAFW [34] with our self-reviewed",
      "page": 1
    },
    {
      "caption": "Figure 3: , these methods often fall short",
      "page": 1
    },
    {
      "caption": "Figure 2: The processing pipeline of our proposed datasets consists of three aspects, including the extraction of global visual information,",
      "page": 3
    },
    {
      "caption": "Figure 3: , we visualize the score statistics on DFEW [19]",
      "page": 3
    },
    {
      "caption": "Figure 3: Statistics of the GPT-based scores assessing the align-",
      "page": 4
    },
    {
      "caption": "Figure 4: The distribution of sample sizes across several emo-",
      "page": 4
    },
    {
      "caption": "Figure 5: Illustration of our proposed Omni-Emotion MLLM. Our model includes an LLM decoder, a general vision encoder for extracting",
      "page": 5
    },
    {
      "caption": "Figure 6: , we examine three methods: con-",
      "page": 5
    },
    {
      "caption": "Figure 6: Illustration of the visual feature fusion methods: (a) The",
      "page": 5
    },
    {
      "caption": "Figure 6: (b). The third approach is to separately extracts the visual",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Methods",
          "DFEW [19]\nMAFW [34]": "UAR\nWAR\nUAR\nWAR\nM"
        },
        {
          "Dataset": "Wav2Vec2.0 [3]\nHuBERT [17]\nDFER-CLIP [65]\nSVFAP [50]\nMAE-DFER [48]\nFinecliper [6]\nTMEP [64]\nHiCMAE [49]\nEmotion-LLaMA [10]\nMMA-DFER [13]",
          "DFEW [19]\nMAFW [34]": "36.15\n43.05\n21.59\n29.69\nA\n35.98\n43.24\n25.00\n32.60\nA\n59.61\n71.25\n38.89\n52.55\nV\n62.83\n74.27\n41.19\n54.28\nV\n63.41\n74.43\n41.62\n54.31\nV\n65.98\n76.21\n45.01\n56.91\nV\n57.16\n68.85\n37.17\n51.15\nAV\n63.76\n75.01\n42.65\n56.17\nAV\n64.21\n77.06\n-\n-\nAV\n66.01\n77.51\n44.11\n58.52\nAV"
        },
        {
          "Dataset": "Omni-Emotion (Ours)",
          "DFEW [19]\nMAFW [34]": "68.80\n78.35\n53.81\n64.23\nAV"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "5",
      "title": "Vggsound: A large-scale audio-visual dataset",
      "authors": [
        "Honglie Chen",
        "Weidi Xie",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters",
      "authors": [
        "Haodong Chen",
        "Haojian Huang",
        "Junhao Dong",
        "Mingzhe Zheng",
        "Dian Shao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yunyang Xiong",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "arxiv": "arXiv:2310.09478"
    },
    {
      "citation_id": "8",
      "title": "Shikra: Unleashing multi",
      "authors": [
        "Keqin Chen",
        "Zhao Zhang",
        "Weili Zeng",
        "Richong Zhang",
        "Feng Zhu",
        "Rui Zhao"
      ],
      "venue": "Shikra: Unleashing multi"
    },
    {
      "citation_id": "9",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Jingdong Sun",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann",
        "; Wei-Lin",
        "Zhuohan Chiang",
        "Zi Li",
        "Ying Lin",
        "Zhanghao Sheng",
        "Hao Wu",
        "Lianmin Zhang",
        "Siyuan Zheng",
        "Yonghao Zhuang",
        "Joseph Zhuang",
        "Ion Gonzalez",
        "Eric Stoica",
        "Xing"
      ],
      "year": "2023",
      "venue": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "10",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "11",
      "title": "Alexandros Iosifidis, and Moncef Gabbouj. Mma-dfer: Multimodal adaptation of unimodal models for dynamic facial expression recognition in-the",
      "authors": [
        "Kateryna Chumachenko"
      ],
      "venue": "Alexandros Iosifidis, and Moncef Gabbouj. Mma-dfer: Multimodal adaptation of unimodal models for dynamic facial expression recognition in-the"
    },
    {
      "citation_id": "12",
      "title": "Large raw emotional dataset with aggregation mechanism",
      "authors": [
        "Vladimir Kondratenko",
        "Artem Sokolov",
        "Nikolay Karpov",
        "Oleg Kutuzov",
        "Nikita Savushkin",
        "Fyodor Minkin"
      ],
      "year": "2022",
      "venue": "Large raw emotional dataset with aggregation mechanism",
      "arxiv": "arXiv:2212.12266"
    },
    {
      "citation_id": "13",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "14",
      "title": "Mivolo: Multiinput transformer for age and gender estimation",
      "authors": [
        "Maksim Kuprashevich",
        "Irina Tolstykh"
      ],
      "year": "2023",
      "venue": "International Conference on Analysis of Images, Social Networks and Texts"
    },
    {
      "citation_id": "15",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "16",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "17",
      "title": "Llava-onevision: Easy visual task transfer",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Dong Guo",
        "Renrui Zhang",
        "Feng Li",
        "Hao Zhang",
        "Kaichen Zhang",
        "Yanwei Li",
        "Ziwei Liu",
        "Chunyuan Li"
      ],
      "year": "2006",
      "venue": "Llava-onevision: Easy visual task transfer",
      "arxiv": "arXiv:2408.03326"
    },
    {
      "citation_id": "18",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "20",
      "title": "Mvbench: A comprehensive multimodal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Yong Li",
        "Yuanzhi Wang",
        "Zhen Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "23",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "24",
      "title": "Affectgpt: Dataset and framework for explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Affectgpt: Dataset and framework for explainable multimodal emotion recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "25",
      "title": "Visual instruction tuning. NeurIPS",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Visual instruction tuning. NeurIPS"
    },
    {
      "citation_id": "26",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2009",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "28",
      "title": "Video assistant with large language model enhanced ability",
      "authors": [
        "Ruipu Luo",
        "Ziwang Zhao",
        "Min Yang",
        "Junwei Dong",
        "Minghui Qiu",
        "Pengcheng Lu",
        "Tao Wang",
        "Zhongyu Wei",
        "Valley"
      ],
      "year": "2023",
      "venue": "Video assistant with large language model enhanced ability",
      "arxiv": "arXiv:2306.07207"
    },
    {
      "citation_id": "29",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "30",
      "title": "Wavcaps: A chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "Xinhao Mei",
        "Chutong Meng",
        "Haohe Liu",
        "Qiuqiang Kong",
        "Tom Ko",
        "Chengqi Zhao",
        "Mark Plumbley",
        "Yuexian Zou",
        "Wenwu Wang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Facexformer: A unified transformer for facial analysis",
      "authors": [
        "Kartik Narayan",
        "V Vibashan",
        "Rama Chellappa",
        "M Vishal",
        "Patel"
      ],
      "year": "2024",
      "venue": "Facexformer: A unified transformer for facial analysis",
      "arxiv": "arXiv:2403.12960"
    },
    {
      "citation_id": "32",
      "title": "Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition",
      "authors": [
        "Jia Le Ngwe",
        "Ming Kian",
        "Chin Lim",
        "Thian Poo Lee",
        "Ong"
      ],
      "year": "2023",
      "venue": "Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition",
      "arxiv": "arXiv:2306.09626"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "Openai",
        "Chatgpt"
      ],
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "35",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "Kosmos-2: Grounding multimodal large language models to the world",
      "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding multimodal large language models to the world",
      "arxiv": "arXiv:2306.14824"
    },
    {
      "citation_id": "37",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "38",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2006",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "39",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "40",
      "title": "Maedfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2008",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Hicmae: Hierarchical contrastive masked autoencoder for selfsupervised audio-visual emotion recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "42",
      "title": "Svfap: Selfsupervised video facial affect perceiver",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Kexin Wang",
        "Yu He",
        "Mingyu Xu",
        "Haiyang Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
      "authors": [
        "Shengbang Tong",
        "Ellis Brown",
        "Penghao Wu",
        "Sanghyun Woo",
        "Manoj Middepogu",
        "Charitha Sai",
        "Jihan Akula",
        "Shusheng Yang",
        "Adithya Yang",
        "Xichen Iyer",
        "Pan"
      ],
      "year": "2024",
      "venue": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
      "arxiv": "arXiv:2406.16860"
    },
    {
      "citation_id": "44",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "45",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "46",
      "title": "Large language model is also an open-ended decoder for vision-centric tasks",
      "authors": [
        "Wenhai Wang",
        "Zhe Chen",
        "Xiaokang Chen",
        "Jiannan Wu",
        "Xizhou Zhu",
        "Gang Zeng",
        "Ping Luo",
        "Tong Lu",
        "Jie Zhou",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Large language model is also an open-ended decoder for vision-centric tasks",
      "arxiv": "arXiv:2305.11175"
    },
    {
      "citation_id": "47",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "48",
      "title": "Internvideo2: Scaling video foundation models for multimodal video understanding",
      "authors": [
        "Yi Wang",
        "Kunchang Li",
        "Xinhao Li",
        "Jiashuo Yu",
        "Yinan He",
        "Guo Chen",
        "Baoqi Pei",
        "Rongkun Zheng",
        "Jilan Xu",
        "Zun Wang"
      ],
      "year": "2024",
      "venue": "Internvideo2: Scaling video foundation models for multimodal video understanding",
      "arxiv": "arXiv:2403.15377"
    },
    {
      "citation_id": "49",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Yuanzhi Wang",
        "Yong Li",
        "Zhen Cui"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Simple online and realtime tracking with a deep association metric",
      "authors": [
        "Nicolai Wojke",
        "Alex Bewley",
        "Dietrich Paulus"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on image processing (ICIP)"
    },
    {
      "citation_id": "51",
      "title": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "authors": [
        "Hongxia Xie",
        "Chu-Jun Peng",
        "Yu-Wen Tseng",
        "Hung-Jen Chen",
        "Chan-Feng Hsu",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "year": "2024",
      "venue": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "arxiv": "arXiv:2404.16670"
    },
    {
      "citation_id": "52",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "53",
      "title": "Sigmoid loss for language image pre-training",
      "authors": [
        "Xiaohua Zhai",
        "Basil Mustafa",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "54",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "55",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Transformer-based multimodal emotional perception for dynamic facial expression recognition in the wild",
      "authors": [
        "Xiaoqin Zhang",
        "Min Li",
        "Sheng Lin",
        "Hang Xu",
        "Guobao Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "57",
      "title": "Prompting visuallanguage models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2008",
      "venue": "Prompting visuallanguage models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "58",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "Hengshun Zhou",
        "Debin Meng",
        "Yuanyuan Zhang",
        "Xiaojiang Peng",
        "Jun Du",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "2019 International conference on multimodal interaction"
    },
    {
      "citation_id": "59",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}