{
  "paper_id": "2507.00055v1",
  "title": "Leveraging Unlabeled Audio-Visual Data In Speech Emotion Recognition Using Knowledge Distillation",
  "published": "2025-06-26T04:13:47Z",
  "authors": [
    "Varsha Pendyala",
    "Pedro Morgado",
    "William Sethares"
  ],
  "keywords": [
    "speech emotion recognition",
    "facial expression recognition",
    "multimodal knowledge distillation",
    "audio-visual emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Voice interfaces integral to the human-computer interaction systems can benefit from speech emotion recognition (SER) to customize responses based on user emotions. Since humans convey emotions through multi-modal audio-visual cues, developing SER systems using both the modalities is beneficial. However, collecting a vast amount of labeled data for their development is expensive. This paper proposes a knowledge distillation framework called LightweightSER (LiSER) that leverages unlabeled audio-visual data for SER, using large teacher models built on advanced speech and face representation models. LiSER transfers knowledge regarding speech emotions and facial expressions from the teacher models to lightweight student models. Experiments conducted on two benchmark datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence on extensive labeled datasets for SER tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human-computer interaction systems equipped with voice interfaces are increasing in popularity. Detecting emotional states through spoken language, called speech emotion recognition (SER), is critical to effectively implement these systems. However, accurate SER is challenging due to the differences in accents, age, gender, and voice characteristics of the users. Human facial expressions and body language are closely linked to emotional states. Recent research  [1, 2, 3]  has shown that these visual cues can be used to enhance the accuracy of SER systems. However, collecting large volumes of manually labeled emotion data to develop accurate SER systems is both costly and time-consuming, largely due to the inherent ambiguity in humans' perception of emotions.\n\nRecently, there has been significant progress in the field of audio, vision, and text, particularly in developing selfsupervised learning (SSL) models such as HuBERT  [4] , Video-MAE  [5] , and BERT  [6] . These models can be pre-trained on vast amounts of unlabeled data and subsequently fine-tuned using a limited quantity of task-specific labeled data, to yield remarkable performance in applications like facial expression recognition (FER)  [7]  and SER  [8] . However, their large size makes these SSL models challenging to deploy in lowresource environments, such as mobile devices with computing and memory constraints. To overcome these challenges, knowledge distillation techniques  [9]  are used to transfer the knowledge from large and accurate \"teacher\" models to lightweight \"student\" models. In these techniques, the student models are trained by aligning their intermediate feature representations or softmax distributions with those of the teacher.\n\nVarious distillation techniques have been explored in SER research, utilizing teacher models from the speech modality and other modalities such as vision and text. In  [10, 11] , the authors developed distillation techniques for speech SSL models that have been fine-tuned for SER. The authors in  [12]  utilized crossmodal distillation from prosodic and linguistic teachers to boost the accuracy of their SER model. Another approach in  [13]  trained a student model on unlabeled audio-text pairs through cross-modal distillation from a strong BERT-based teacher that was fine-tuned on a text emotion corpus. In  [3] , SER models were developed using ground-truth labels and distillation from video models trained from scratch on labeled audio-visual data. However, no reported literature investigates distillation using unlabeled audio-visual data for SER.\n\nThe use of unlabeled audio-visual data to boost the performance of SER models has been reported in  [1, 2] . In  [1] , the authors introduced an SSL framework, proposing new audiovisual pretext tasks to enhance speech representations for SER tasks. These cross-modal pretext tasks involve using acoustic features to predict the temporal variance of facial landmark positions, and multi-class pseudo-emotional labels derived from a combination of facial action units (AUs). However, relying solely on landmark variance prediction tasks or employing hand-engineered rules for generating pseudo-labels from AUs may not adequately capture the intricate changes in facial expressions over time. The authors in  [2]  train SER models through visual self-supervision via a face reconstruction task. In that approach, a speech encoder is jointly trained with a face encoder-decoder network to reconstruct video from a still face image paired with the corresponding speech utterance. However, the compute-intensive nature of this task presents significant challenges when attempting to scale this framework to large volumes of audio-visual data from everyday interactions. This paper introduces LiSER, a knowledge distillation framework that utilizes unlabeled audio-visual data alongside a limited amount of labeled speech emotion data to build lightweight SER models. Our framework integrates state-ofthe-art speech and face representation models to enhance the performance of lightweight SER models. We leverage unlabeled audio-visual data through the distillation of speech emotion knowledge from the HuBERT model, which has been finetuned for the SER task, while also incorporating insights from S2D  [14] , a dynamic facial expression recognition (DFER) model. The DFER model is capable of recognizing facial expressions from raw pixel data in dynamic face image sequences or videos. As a result, our approach can efficiently leverage large-scale audio-visual data available on video-sharing platforms, employing the standard preprocessing pipeline typically associated with face recognition systems  [15] .\n\nWe use the MSP-Face corpus  [16]  containing audio-visual data to extract emotion-related knowledge from HuBERT and S2D models. We train a lightweight SER model by employing both uni-modal and cross-modal distillation. In addition, we propose a novel training objective that incorporates instance-level confidence pertaining to emotion predictions of the teacher models. Systematic evaluations conducted on the RAVDESS  [17]  and CREMA-D  [18]  benchmarks yield several key findings: 1) Distillation from both audio and visual modalities of unlabeled data enhances the accuracy of the lightweight SER model 2) Utilizing both audio and visual modalities during the distillation process provides greater performance improvements compared to relying solely on one modality. 3) The integration of instance-level confidence related to the emotion predictions of teacher models shows promise for further enhancing the SER accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "This section outlines our approach, called LiSER, for training a lightweight SER model by leveraging unlabeled audio-visual data and a limited amount of labeled speech emotion data. Figure  1  depicts the overall framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Teacher",
      "text": "To develop a teacher model capable of identifying emotions from speech with high accuracy, LiSER starts with a HuBERT model  [4]  trained using self-supervised learning on a large corpus of unlabeled speech data. HuBERT's representations have proven to be beneficial across various applications, including speech recognition  [4, 19] , speaker verification  [20] , and emotion recognition  [21, 8] . The model utilizes a convolutional encoder to capture local temporal features from raw speech inputs, along with a transformer encoder that generates global contextualized representations. We selected the base variant of the pretrained HuBERT (hubert-base-ls960) from the HuggingFace library  [22]  and fine-tuned it for SER using the available labeled speech emotion data. The resulting speech teacher model processes raw speech waveforms as inputs and outputs the softmax probabilities corresponding to the emotion categories in the labeled speech.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Video Teacher",
      "text": "LiSER uses a state-of-the-art dynamic facial expression recognition model (DFER) known as S2D  [14]  as a second teacher to extract emotional knowledge from videos. The architecture of S2D is based on the Vision Transformer (ViT)  [23] . In  [14] , the authors pre-train a ViT model to recognize facial expressions from static images, utilizing features derived from Mo-bileFaceNet  [24] , a network designed for facial landmark detection. Subsequently, they adapt the static FER model for the dynamic FER task by training spatio-temporal adapters with face image sequence data obtained from the DFER dataset. S2D processes 16-frame face image sequences as inputs and produces softmax probabilities across the emotion categories found in the DFER dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Student Model",
      "text": "The LiSER student model accepts log Mel-spectrograms of speech waveforms as inputs. Its architecture is inspired by the 2D CNN LSTM network proposed in  [25] . LiSER's student model consists of three two-dimensional convolution blocks, each with 64 filters, to capture local spatio-temporal features from the spectrograms, followed by an LSTM layer to capture the global context. Additionally, the model includes three multilayer perceptrons (MLPs) to facilitate the training of the student using various loss functions, which will be detailed in the next subsection. The exact configuration of each component of the model is outlined in Table  1 . In the table, KS represents the number of emotion categories seen in the available labeled speech, while KV indicates the number of emotion categories in the DFER dataset which was utilized to train the video teacher. We train the student model with parameters θ, by employing standard supervised learning on D L and softmax-level distillation-based learning using D U . The student model consists of three distinct MLP heads namely, g sup , g sd and g vd , for the tasks of supervised learning, speech distillation, and video distillation, respectively. Let ts and tv represent the networks of speech and video teachers. The loss terms associated with the three tasks are defined as:\n\nL CE in equation (  1 ) represents the cross-entropy loss and L MAE in equation (  2 ) and (3) refers to the mean absolute error (MAE) between the softmax outputs of the student and teacher models.\n\nThe parameters θ are learned by minimizing the mini-batch loss defined in the following subsections. Finally, after the model is trained, we utilize g sup MLP head to make emotion predictions for any given speech signal.\n\nMini-batch loss Let L sup i represent the supervised loss term for the i th data point from the labeled dataset D L and L sd j , L vd j denote the distillation loss terms for the j th data point from the unlabeled dataset D U . The overall loss for a mini-batch containing N l labeled data points and Nu unlabeled data points is defined as follows:\n\nwhere λ sd , λ vd are the hyperparameters denoting the weights for the sound and visual distillation loss terms.\n\nConfidence-enhanced mini-batch loss In the mini-batch loss defined in (4), we utilize constant weights (i.e., λ sd , λ vd ) across all unlabeled data points. This approach leads to the student model emphasizing both modalities uniformly across the entire dataset. However, since data point contain varying amounts of emotional information in the two modalities, we enhance the mini-batch loss computation by incorporating the confidence of emotion predictions from the teacher models. Specifically, we introduce instance-level weights denoted as w sd j , w vd j .\n\nThe instance-level confidence weights are computed as the maximum probability values associated with the softmax outputs of the respective teacher models for each data point.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "This work utilizes audio-visual data from MSP-Face  [16]  corpus and speech emotion data from SER benchmark datasets namely, RAVDESS  [17]  and CREMA-D  [18] .\n\nMSP-Face is an audio-visual dataset with recordings collected in-the-wild from video-sharing websites. Each recording features an individual facing the camera and discussing various topics from their daily life in a natural and spontaneous manner.\n\nThe data was gathered from a diverse group of individuals, conveying a wide range of emotions. The dataset includes YouTube links to these videos, although some of them are no longer available. We successfully downloaded 46.55 hours of data from 386 speakers, with 55% of them being male. Each video has a frame rate of 30 fps, with an average duration of 9.25 seconds. While some videos included emotion annotations, we do not utilize those annotations and treat all available data as unlabeled.\n\nWe extracted and stored the facial regions from each frame of the recordings using the DeepFace toolkit  [26]  for face detection, alignment, and extraction. To reduce the computational load when training the student model, we pre-computed the softmax outputs of the DFER model for all the videos. The video frames are fed to the DFER model using a sliding window with a length and stride of 16.\n\nRAVDESS dataset comprises 1,440 audio-visual recordings from 24 professional actors, of whom 12 are male. The actors vocalize two sentences across eight different emotions including neutral, calm, happy, sad, angry, fearful, surprise, and disgust. For our study, we utilize only the speech portion of this dataset to train and evaluate our student model.\n\nCREMA-D dataset consists of 7,442 audio-visual clips from a diverse group of 91 actors with 48 of them being male. Each actor spoke from a selection of 12 sentences multiple times, conveying emotions from six categories: anger, disgust, fear, happy, neutral, and sad. As with RAVDESS, we focus only on the speech portion of this dataset in the current study.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Development Of Teacher Models",
      "text": "We developed the speech teacher model by fine-tuning the pretrained HuBERT for the SER task, utilizing labeled speech samples from the same dataset used to train the student model. The fine-tuning of HuBERT is achieved by applying Low-Rank Adaptation (LoRA)  [27]  to the weight matrices of the selfattention modules. We utilized 80% of the labeled speech to fine-tune it for a maximum of 50 epochs and chose the checkpoint corresponding to the epoch with the best SER performance on the remaining 20% data. This selected checkpoint serves as the speech teacher.\n\nOur video teacher is an S2D model trained in  [14] , using video samples from the FERV39k corpus  [28] . The FERV39k dataset comprises videos with a frame rate of 30 fps, spanning seven emotion categories: angry, disgust, fear, happy, neutral, sad, surprise. The S2D model was trained to predict emotions based on any randomly selected 16 consecutive face image frames (equivalent to 0.5s) extracted from these video samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input To The Student Model",
      "text": "The student model receives log Mel-spectrograms derived from speech signals of 3s in duration as its inputs. The Melspectrogram is calculated using 64 Mel bands, with a window size of 128 ms and a stride of 32ms. For speech signals shorter than 3s, zero padding is applied before inputting them into the model. For signals exceeding 3s, a random 3-second segment is selected from the entire signal during training and fed into the model. In the evaluation phase, multiple 3s segments are extracted from the entire signal using a sliding window of 3s with a stride of 0.1s. Note that a single prediction is generated for the entire signal by averaging the g sup logits (ref. section 2.4) corresponding to these smaller segments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mini-Batch Loss Computation",
      "text": "For labeled data points in the mini-batch, LiSER computes the cross-entropy loss between the emotion labels and the logits from g sup MLP. For unlabeled data points, the loss terms are computed for both speech and video distillation. As outlined in section 3.3, we feed a 3s speech signal from the unlabeled data point to the student model, obtaining outputs from the relevant MLP heads for the distillation tasks. We then obtain the softmax outputs from the two teacher models by feeding the respective 3s audio and video inputs into them. Since the S2D model can only predict from 0.5s-duration video clips, we calculate the softmax prediction for the entire 3s video by averaging the outputs from all corresponding 0.5s clips. In contrast, HuBERT can handle speech signals of any length. However, to ensure a fair comparison between the knowledge distillation from both modalities, we similarly average the outputs from the 0.5s segments of the 3s speech signal. After computing the relevant loss terms for each data point, we utilize the mini-batch loss defined in equations 4 and 5 to train the student model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Configurations",
      "text": "The student models are trained under various configurations to assess the effectiveness of different components within our training framework. The performance of these models is presented in Table  2 . no-dstl indicates the training of the student using only labeled speech data. vid-dstl and sp-dstl refer to the training with supervised learning over labeled speech in conjunction with distillation from either video or speech teacher, respectively. vid-sp-dstl and conf-vid-sp-dstl refers to the training using mini-batch loss specified in equations (  4 ) and (  5 ), respectively, with λ vd = 0, λ sd = 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "We evaluate the LiSER framework on RAVDESS and CREMA-D using Unweighted Average Recall (UAR) and Weighted Average Recall (WAR). We follow a five-fold cross-validation protocol to divide the labeled dataset into train, validation and test sets, ensuring no overlap in speakers across these sets. The resulting training set is augmented with samples from MSP-Face when training with distillation loss terms. In each training configuration, we train the student model for a maximum of 50 epochs, selecting the checkpoint corresponding to the epoch with best validation set performance. The validation set is used to determine the optimal values for λ vd , λ sd over {0.1, 0.5, 1, 5, 10}. The student models are trained using AdamW optimizer with a learning rate of 1e-4, batch size of 25.\n\nThe results in Table  2  show that using knowledge from speech and video teachers enhances the performance of the student model. In RAVDESS, when comparing with the no-dstl scenario, we see improvements of 3.29% and 5.42% in UAR from the video and speech teachers, respectively. For CREMA-D, both teachers lead to a 4.54% improvement. Combining both speech and video distillation in the vid-sp-dstl approach gives even better results, with increases of 7.54% in RAVDESS and 5.99% in CREMA-D. We also looked at how incorporating teacher models' confidence in emotion predictions affects results. In RAVDESS, this integration improves UAR by 15.09% compared to the no-dstl approach. However, in CREMA-D, the improvement slightly decreases from 5.99% to 4.9%.\n\nWe also conduct an ablation study to examine the effects of different loss functions and training methodologies for knowl-  edge transfer from the teacher models, as shown in Table  3 . In distill-ce, we replaced the MAE distillation loss with crossentropy (CE) loss. We compared LiSER's training method, which uses both labeled and unlabeled data at the same time, with the two-stage training method used in  [1, 2, 13] . The twostage method first trains on unlabeled data, followed by finetuning with labeled data. Our results show that MAE outperforms CE loss, which aligns with related research such as  [29]  which finds MAE more resilient to noisy labels. Additionally, LiSER's training method outperforms the two-stage training.\n\nFinally, we assessed the impact of using less labeled speech data in the vid-sp-dstl scenario by training the student model on smaller subsets of the labeled data. Figure  2  displays these results for CREMA-D. The findings indicate that the student model trained with all data from MSP-Face and only half of the labeled data performs better than the model that used the whole labeled dataset in the no-dstl scenario.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper developed a knowledge distillation framework called LiSER that improves lightweight models for recognizing emotions in speech by using unlabeled audio-visual data. We validated this framework with an unlabeled audio-visual dataset collected in-the-wild. Our results show significant improvements of up to 15.09% and 5.99% in unweighted average recall on RAVDESS and CREMA-D benchmarks, respectively. The findings indicate that the knowledge gained from teacher models which understand speech emotions and facial expressions, enhances the performance of the student models. Moreover, simultaneous distillation from both audio and visual modalities yields better results than using a single modality. The results from RAVDESS also suggest that integrating confidence measures from teachers' predictions can help each data point to effectively utilize the varying levels of information offered by different teacher models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In LiSER, the student is trained using labeled speech",
      "page": 2
    },
    {
      "caption": "Figure 2: Ablation study over CREMA-D on the impact of using",
      "page": 4
    },
    {
      "caption": "Figure 2: displays these",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "softmax distributions with those of the teacher."
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "Various distillation techniques have been explored in SER"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "research, utilizing teacher models from the speech modality and"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "other modalities such as vision and text. In [10, 11], the authors"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "developed distillation techniques\nfor speech SSL models\nthat"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "have been ﬁne-tuned for SER. The authors in [12] utilized cross-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "modal distillation from prosodic and linguistic teachers to boost"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "the accuracy of\ntheir SER model.\nAnother approach in [13]"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "trained a student model on unlabeled audio-text pairs through"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "cross-modal distillation from a strong BERT-based teacher that"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "was ﬁne-tuned on a text emotion corpus.\nIn [3], SER models"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "were developed using ground-truth labels and distillation from"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "video models trained from scratch on labeled audio-visual data."
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "However, no reported literature investigates distillation using"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "unlabeled audio-visual data for SER."
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "The use of unlabeled audio-visual data to boost\nthe perfor-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "mance of SER models has been reported in [1, 2].\nIn [1],\nthe"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "authors introduced an SSL framework, proposing new audio-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "visual pretext\ntasks to enhance speech representations for SER"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "tasks. These cross-modal pretext\ntasks involve using acoustic"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "features to predict the temporal variance of facial landmark po-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": ""
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "sitions, and multi-class pseudo-emotional\nlabels derived from"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "a combination of\nfacial\naction units\n(AUs).\nHowever,\nrely-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "ing solely on landmark variance prediction tasks or employ-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "ing hand-engineered rules\nfor generating pseudo-labels\nfrom"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "AUs may not adequately capture the intricate changes in facial"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "expressions over\ntime.\nThe authors\nin [2]\ntrain SER models"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "through visual self-supervision via a face reconstruction task."
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "In that approach, a speech encoder is jointly trained with a face"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "encoder-decoder network to reconstruct video from a still face"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "image paired with the corresponding speech utterance. How-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "ever,\nthe compute-intensive nature of this task presents signif-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "icant challenges when attempting to scale this\nframework to"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "large volumes of audio-visual data from everyday interactions."
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "This\npaper\nintroduces LiSER,\na\nknowledge\ndistillation"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "framework that utilizes unlabeled audio-visual data alongside"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "a\nlimited\namount\nof\nlabeled\nspeech\nemotion\ndata\nto\nbuild"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "lightweight SER models.\nOur\nframework integrates state-of-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "the-art speech and face representation models to enhance the"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "performance of\nlightweight SER models. We leverage unla-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "beled audio-visual data through the distillation of speech emo-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "tion knowledge from the HuBERT model, which has been ﬁne-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "tuned for the SER task, while also incorporating insights from"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "S2D [14],\na dynamic\nfacial\nexpression recognition\n(DFER)"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "model. The DFER model\nis capable of\nrecognizing facial ex-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "pressions from raw pixel data in dynamic face image sequences"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "or videos.\nAs a result, our approach can efﬁciently leverage"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "large-scale audio-visual data available on video-sharing plat-"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "forms, employing the standard preprocessing pipeline typically"
        },
        {
          "pendyala@wisc.edu, pmorgado@wisc.edu, sethares@wisc.edu": "associated with face recognition systems [15]."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: In the table, K represents",
      "data": [
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "data to extract emotion-related knowledge from HuBERT and",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "LiSER uses a state-of-the-art dynamic facial expression recog-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "S2D models.\nWe\ntrain\na\nlightweight SER model\nby\nem-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "nition model\n(DFER) known as S2D [14] as a second teacher"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "ploying both uni-modal and cross-modal distillation.\nIn addi-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "to extract emotional knowledge from videos. The architecture"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "tion, we propose a novel\ntraining objective that\nincorporates",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "of S2D is based on the Vision Transformer (ViT)\n[23]. In [14],"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "instance-level conﬁdence pertaining to emotion predictions of",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "the authors pre-train a ViT model\nto recognize facial expres-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "the teacher models.\nSystematic evaluations conducted on the",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "sions from static images, utilizing features derived from Mo-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "RAVDESS [17] and CREMA-D [18] benchmarks yield several",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "bileFaceNet [24], a network designed for facial landmark detec-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "key ﬁndings: 1) Distillation from both audio and visual modal-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "tion. Subsequently,\nthey adapt the static FER model for the dy-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "ities of unlabeled data enhances the accuracy of the lightweight",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "namic FER task by training spatio-temporal adapters with face"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "SER model 2) Utilizing both audio and visual modalities during",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "image sequence data obtained from the DFER dataset. S2D pro-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "the distillation process provides greater performance improve-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "cesses 16-frame face image sequences as inputs and produces"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "ments compared to relying solely on one modality. 3) The inte-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "softmax probabilities across the emotion categories found in the"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "gration of instance-level conﬁdence related to the emotion pre-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "DFER dataset."
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "dictions of teacher models shows promise for further enhancing",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "the SER accuracy.",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "2.3.\nStudent model"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "The LiSER student model\naccepts\nlog Mel-spectrograms of"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "2. Method",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "speech waveforms as inputs.\nIts architecture is inspired by the"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "This section outlines our approach, called LiSER, for training",
          "2.2. Video teacher": "2D CNN LSTM network proposed in [25].\nLiSER’s student"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "a lightweight SER model by leveraging unlabeled audio-visual",
          "2.2. Video teacher": "model consists of\nthree two-dimensional convolution blocks,"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "data and a limited amount of labeled speech emotion data. Fig-",
          "2.2. Video teacher": "each with 64 ﬁlters,\nto capture local spatio-temporal\nfeatures"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "ure 1 depicts the overall framework.",
          "2.2. Video teacher": "from the spectrograms, followed by an LSTM layer to capture"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "the global context. Additionally, the model includes three multi-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "layer perceptrons\n(MLPs)\nto facilitate the training of\nthe stu-"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "2.1.\nSpeech teacher",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "dent using various loss functions, which will be detailed in the"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "To develop a teacher model capable of\nidentifying emotions",
          "2.2. Video teacher": "next subsection. The exact conﬁguration of each component of"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "from speech with high accuracy, LiSER starts with a HuBERT",
          "2.2. Video teacher": "the model\nis outlined in Table 1.\nIn the table, KS represents"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "model [4] trained using self-supervised learning on a large cor-",
          "2.2. Video teacher": "the number of emotion categories seen in the available labeled"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "pus of unlabeled speech data. HuBERT’s representations have",
          "2.2. Video teacher": "speech, while KV indicates the number of emotion categories in"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "proven to be beneﬁcial across various applications,\nincluding",
          "2.2. Video teacher": "the DFER dataset which was utilized to train the video teacher."
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "speech recognition [4, 19], speaker veriﬁcation [20], and emo-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "Table 1: Details of the LiSER student architecture."
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "tion recognition [21, 8]. The model utilizes a convolutional en-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "coder to capture local temporal features from raw speech inputs,",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "Block\nType\nConﬁguration"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "along with a transformer encoder that generates global contex-",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "tualized representations. We selected the base variant of the pre-",
          "2.2. Video teacher": "Conv2D\nKernel:(3,3) MaxPool:(2,2)"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "trained HuBERT (hubert-base-ls960) from the HuggingFace li-",
          "2.2. Video teacher": "1-3\n+ BatchNorm\nKernel:(3,3) MaxPool:(4,2)"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "brary [22] and ﬁne-tuned it for SER using the available labeled",
          "2.2. Video teacher": "+ ReLU\nKernel:(3,1) MaxPool:(4,1)"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "speech emotion data. The resulting speech teacher model pro-",
          "2.2. Video teacher": "4\nLSTM\nHidden size: 64"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "cesses raw speech waveforms as inputs and outputs the softmax",
          "2.2. Video teacher": "5\nsupervised-mlp\nlayers: 1, nodes: KS"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "probabilities corresponding to the emotion categories in the la-",
          "2.2. Video teacher": "6\nspeech-distill-mlp\nlayers: 2, nodes: 32, KS"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "beled speech.",
          "2.2. Video teacher": ""
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "7\nvideo-distill-mlp\nlayers: 2, nodes: 32, KV"
        },
        {
          "We use the MSP-Face corpus [16] containing audio-visual": "",
          "2.2. Video teacher": "# Parameters: 105K"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "in equation (2) and (3) refers to the mean absolute error (MAE)",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "video frames are fed to the DFER model using a sliding win-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "between the softmax outputs of the student and teacher models.",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "dow with a length and stride of 16."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "The parameters θ are learned by minimizing the mini-batch loss",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "deﬁned in the following subsections. Finally, after the model is",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "RAVDESS\ndataset comprises 1,440 audio-visual\nrecordings"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "trained, we utilize gsup MLP head to make emotion predictions",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "from 24 professional actors, of whom 12 are male. The actors"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "for any given speech signal.",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "vocalize two sentences across eight different emotions includ-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "ing neutral, calm, happy, sad, angry,\nfearful, surprise, and dis-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "Mini-batch loss\nrepresent\nthe supervised loss term\nLet Lsup\ni",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "gust. For our study, we utilize only the speech portion of\nthis"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "for the ith data point from the labeled dataset DL and Lsd\nj , Lvd",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "dataset to train and evaluate our student model."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "denote the distillation loss terms for the jth data point from the",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "loss for a mini-batch con-\nunlabeled dataset DU . The overall",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "CREMA-D\ndataset consists of 7,442 audio-visual clips from"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "labeled data points and Nu unlabeled data points is\ntaining Nl",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "a diverse group of 91 actors with 48 of them being male. Each"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "deﬁned as follows:",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "actor\nspoke from a selection of 12 sentences multiple times,"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "conveying emotions from six categories:\nanger, disgust,\nfear,"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "j + λvd · Lvd\nj (cid:1)\n+ P\nP\nj=1 (cid:0)λsd · Lsd\ni=1 Lsup",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "happy, neutral, and sad. As with RAVDESS, we focus only on"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "(4)\nLbatch =",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "Nl + Nu",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "the speech portion of this dataset in the current study."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "where λsd, λvd are the hyperparameters denoting the weights for",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "3.2. Development of teacher models"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "the sound and visual distillation loss terms.",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "We developed the speech teacher model by ﬁne-tuning the pre-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "trained HuBERT for the SER task, utilizing labeled speech sam-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "Conﬁdence-enhanced mini-batch loss\nIn the mini-batch loss",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "ples\nfrom the same dataset used to train the student model."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "deﬁned in (4), we utilize constant weights (i.e., λsd, λvd) across",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "The ﬁne-tuning of HuBERT is achieved by applying Low-Rank"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "all unlabeled data points.\nThis approach leads to the student",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "Adaptation (LoRA)\n[27]\nto the weight matrices of\nthe\nself-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "model emphasizing both modalities uniformly across\nthe en-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "attention modules. We utilized 80% of\nthe labeled speech to"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "tire dataset. However, since each data point may contain vary-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "ﬁne-tune it for a maximum of 50 epochs and chose the check-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "ing amounts of emotional\ninformation in the two modalities,",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "point\ncorresponding to the epoch with the best SER perfor-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "we enhance the mini-batch loss computation by incorporating",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "mance on the remaining 20% data.\nThis selected checkpoint"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "the conﬁdence of emotion predictions from the teacher mod-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "serves as the speech teacher."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "els.\nSpeciﬁcally, we introduce instance-level weights denoted",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "Our video teacher\nis an S2D model\ntrained in [14], using"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "as wsd\n.\nj , wvd",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "video samples from the FERV39k corpus [28]. The FERV39k"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "dataset comprises videos with a frame rate of 30 fps, spanning"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "· Lsd\n· Lvd\nj + λvd · wvd\nj (cid:1)\n+ P\nP\nj=1 (cid:0)λsd · wsd\ni=1 Lsup",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "Lbatch",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "seven emotion categories:\nangry, disgust,\nfear, happy, neutral,"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "conf =",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "Nl + Nu",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "sad,\nsurprise.\nThe S2D model was\ntrained to predict\nemo-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "(5)",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "tions based on any randomly selected 16 consecutive face image"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "frames (equivalent to 0.5s) extracted from these video samples."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "The\ninstance-level\nconﬁdence weights\nare\ncomputed\nas\nthe",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "maximum probability values associated with the softmax out-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "3.3.\nInput to the student model"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "puts of the respective teacher models for each data point.",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "The student model receives log Mel-spectrograms derived from"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "3. Experiments",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "speech\nsignals\nof\n3s\nin\nduration\nas\nits\ninputs.\nThe Mel-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "spectrogram is calculated using 64 Mel bands, with a window"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "3.1. Datasets",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "size of 128 ms and a stride of 32ms. For speech signals shorter"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "than 3s, zero padding is applied before inputting them into the"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "This work utilizes audio-visual data from MSP-Face [16] cor-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "model. For signals exceeding 3s, a random 3-second segment is"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "pus and speech emotion data from SER benchmark datasets",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "selected from the entire signal during training and fed into the"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "namely, RAVDESS [17] and CREMA-D [18].",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "model.\nIn the evaluation phase, multiple 3s segments are ex-"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "tracted from the entire signal using a sliding window of 3s with"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "MSP-Face\nis\nan\naudio-visual\ndataset with recordings\ncol-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "a stride of 0.1s. Note that a single prediction is generated for"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "lected in-the-wild from video-sharing websites. Each recording",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "the entire signal by averaging the gsup\nlogits (ref. section 2.4)"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "features an individual facing the camera and discussing various",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "corresponding to these smaller segments."
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "topics from their daily life in a natural and spontaneous manner.",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "The data was gathered from a diverse group of individuals, con-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "3.4. Mini-batch loss computation"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "veying a wide range of emotions. The dataset includes YouTube",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": ""
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "links to these videos, although some of them are no longer avail-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "For labeled data points in the mini-batch, LiSER computes the"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "able. We successfully downloaded 46.55 hours of data from 386",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "cross-entropy loss between the emotion labels and the logits"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "speakers, with 55% of them being male. Each video has a frame",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "from gsup MLP. For unlabeled data points,\nthe loss terms are"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "rate of 30 fps, with an average duration of 9.25 seconds. While",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "computed for both speech and video distillation. As outlined in"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "some videos included emotion annotations, we do not utilize",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "section 3.3, we feed a 3s speech signal from the unlabeled data"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "those annotations and treat all available data as unlabeled.",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "point\nto the student model, obtaining outputs from the relevant"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "We extracted and stored the facial regions from each frame",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "MLP heads for the distillation tasks. We then obtain the softmax"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "of\nthe recordings using the DeepFace toolkit\n[26]\nfor face de-",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "outputs from the two teacher models by feeding the respective"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "tection, alignment, and extraction. To reduce the computational",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "3s audio and video inputs into them. Since the S2D model can"
        },
        {
          "LCE in equation (1) represents the cross-entropy loss and LMAE": "load when training the student model, we pre-computed the",
          "softmax outputs of\nthe DFER model\nfor all\nthe videos.\nThe": "only predict\nfrom 0.5s-duration video clips, we calculate the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: show that using knowledge from",
      "data": [
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "dent models on RAVDESS and CREMA-D.",
          "Table 3: Ablation study of distillation loss and training method-": ""
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "",
          "Table 3: Ablation study of distillation loss and training method-": ""
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "Conﬁguration",
          "Table 3: Ablation study of distillation loss and training method-": "UAR"
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "no-dstl",
          "Table 3: Ablation study of distillation loss and training method-": "0.534"
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "vid-dstl",
          "Table 3: Ablation study of distillation loss and training method-": "0.523"
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "sp-dstl",
          "Table 3: Ablation study of distillation loss and training method-": "0.464"
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "vid-sp-dstl",
          "Table 3: Ablation study of distillation loss and training method-": ""
        },
        {
          "Table 2: Speech emotion recognition performance of LiSER stu-": "conf-vid-sp-dstl",
          "Table 3: Ablation study of distillation loss and training method-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "the\nA natural audiovisual emotional database,” in Proceedings of"
        },
        {
          "5. References": "[1]\nL. Goncalves and C. Busso, “Improving speech emotion recog-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "ACM International Conference on Multimodal Interaction, 2020,"
        },
        {
          "5. References": "nition using self-supervised learning with domain-speciﬁc audio-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "p. 397–405."
        },
        {
          "5. References": "visual\ntasks,” in Proc. INTERSPEECH 2022 – 23rd Annual Con-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "ference of\nthe International Speech Communication Association,",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[17]\nS. R. Livingstone\nand F. A. Russo,\n“The\nryerson audio-visual"
        },
        {
          "5. References": "2022, pp. 1168–1172.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "database of emotional\nspeech and song (ravdess): A dynamic,"
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "multimodal set of facial and vocal expressions in north american"
        },
        {
          "5. References": "[2] A.\nShukla,\nS.\nPetridis,\nand M.\nPantic,\n“Does\nvisual\nself-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "english,” PloS one, vol. 13, no. 5, p. e0196391, 2018."
        },
        {
          "5. References": "supervision improve learning of speech representations for emo-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[18] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,"
        },
        {
          "5. References": "tion recognition?”\nIEEE Transactions on Affective Computing,",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal"
        },
        {
          "5. References": "vol. 14, no. 1, pp. 406–420, 2023.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "actors dataset,” IEEE Transactions on Affective Computing, vol. 5,"
        },
        {
          "5. References": "[3] A. Hajavi and A. Etemad, “Audio representation learning by dis-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "no. 4, pp. 377–390, 2014."
        },
        {
          "5. References": "tilling video as privileged information,” IEEE Transactions on Ar-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[19] W. Wang and Y. Qian, “Hubert-agg: Aggregated representation"
        },
        {
          "5. References": "tiﬁcial Intelligence, vol. 5, no. 1, pp. 446–456, 2024.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "distillation of hidden-unit bert\nfor robust speech recognition,” in"
        },
        {
          "5. References": "[4] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "ICASSP 2023 - 2023 IEEE International Conference on Acous-"
        },
        {
          "5. References": "dinov, and A. Mohamed, “Hubert:\nSelf-supervised speech rep-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "tics, Speech and Signal Processing (ICASSP), 2023, pp. 1–5."
        },
        {
          "5. References": "resentation\nlearning\nby masked\nprediction\nof\nhidden\nunits,”",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[20]\nZ. Chen, S. Chen, Y. Wu, Y. Qian, C. Wang, S. Liu, Y. Qian,"
        },
        {
          "5. References": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "and M. Zeng, “Large-scale self-supervised speech representation"
        },
        {
          "5. References": "cessing, vol. 29, pp. 3451–3460, 2021.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "learning for automatic\nspeaker veriﬁcation,”\nin ICASSP 2022 -"
        },
        {
          "5. References": "[5]\nZ. Tong, Y. Song,\nJ. Wang, and L. Wang, “Videomae: masked",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "2022 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "5. References": "autoencoders are data-efﬁcient\nlearners for self-supervised video",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "Signal Processing (ICASSP), 2022, pp. 6147–6151."
        },
        {
          "5. References": "pre-training,” in Proceedings of the 36th International Conference",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[21]\nE. Morais, R. Hoory, W. Zhu,\nI. Gat, M. Damasceno,\nand"
        },
        {
          "5. References": "on Neural Information Processing Systems, 2024.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "H. Aronowitz, “Speech emotion recognition using self-supervised"
        },
        {
          "5. References": "[6]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "features,” in ICASSP 2022 - 2022 IEEE International Conference"
        },
        {
          "5. References": "training of deep bidirectional\ntransformers\nfor\nlanguage under-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp."
        },
        {
          "5. References": "the 2019 Conference of\nthe North\nstanding,”\nin Proceedings of",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "6922–6926."
        },
        {
          "5. References": "American Chapter of the Association for Computational Linguis-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[22]\nT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,"
        },
        {
          "5. References": "tics: Human Language Technologies, Volume 1 (Long and Short",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer,"
        },
        {
          "5. References": "Papers), 2019, pp. 4171–4186.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gug-"
        },
        {
          "5. References": "[7]\nL. Sun, Z. Lian, B. Liu, and J. Tao, “Mae-dfer: Efﬁcient masked",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "ger, M. Drame, Q. Lhoest, and A. Rush, “Transformers: State-of-"
        },
        {
          "5. References": "autoencoder for self-supervised dynamic facial expression recog-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "the 2020\nthe-art natural\nlanguage processing,” in Proceedings of"
        },
        {
          "5. References": "nition,” in Proceedings of the 31st ACM International Conference",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "Conference on Empirical Methods in Natural Language Process-"
        },
        {
          "5. References": "on Multimedia, 2023, pp. 6110–6121.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "ing: System Demonstrations, 2020, pp. 38–45."
        },
        {
          "5. References": "[8]\nE. Goron, L. Asai, E. Rut, and M. Dinov, “Improving domain",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[23] A. Dosovitskiy,\nL. Beyer,\nA. Kolesnikov,\nD. Weissenborn,"
        },
        {
          "5. References": "generalization in speech emotion recognition with whisper,”\nin",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,"
        },
        {
          "5. References": "ICASSP 2024 - 2024 IEEE International Conference on Acous-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16"
        },
        {
          "5. References": "tics, Speech and Signal Processing (ICASSP), 2024, pp. 11 631–",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "words: Transformers for image recognition at scale,” in Interna-"
        },
        {
          "5. References": "11 635.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "tional Conference on Learning Representations, 2021."
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[24]\nS. Chen, Y. Liu, X. Gao, and Z. Han, “Mobilefacenets: Efﬁcient"
        },
        {
          "5. References": "[9] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "cnns for accurate real-time face veriﬁcation on mobile devices,” in"
        },
        {
          "5. References": "a neural network,”\nin NIPS Deep Learning and Representation",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "Biometric Recognition.\nSpringer International Publishing, 2018,"
        },
        {
          "5. References": "Learning Workshop, 2015.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "pp. 428–438."
        },
        {
          "5. References": "[10]\nZ. Lou, S. Otake, Z. Li, R. Kawakami,\nand N.\nInoue,\n“Cu-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[25]\nJ. Zhao, X. Mao, and L. Chen, “Speech emotion recognition using"
        },
        {
          "5. References": "bic knowledge distillation\nfor\nspeech emotion recognition,”\nin",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "deep 1d & 2d cnn lstm networks,” Biomedical Signal Processing"
        },
        {
          "5. References": "ICASSP 2024 - 2024 IEEE International Conference on Acous-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "and Control, vol. 47, pp. 312–323, 2019."
        },
        {
          "5. References": "tics, Speech and Signal Processing (ICASSP), 2024, pp. 5705–",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "5709.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[26]\nS.\nI. Serengil and A. Ozpinar,\n“Lightface: A hybrid deep face"
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "in Intelligent Sys-\nrecognition framework,” in 2020 Innovations"
        },
        {
          "5. References": "[11] Y. Liu, H. Sun, G. Chen, Q. Wang, Z. Zhao, X. Lu, and L. Wang,",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "tems and Applications Conference (ASYU), 2020, pp. 1–5."
        },
        {
          "5. References": "“Multi-level knowledge distillation for speech emotion recogni-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "INTERSPEECH 2023 – 24th\ntion in noisy conditions,”\nin Proc.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[27]\nE.\nJ. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,"
        },
        {
          "5. References": "Annual Conference of\nthe International Speech Communication",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "L. Wang, and W. Chen, “LoRA: Low-rank adaptation of large lan-"
        },
        {
          "5. References": "Association, 2023, pp. 1893–1897.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "guage models,” in International Conference on Learning Repre-"
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "sentations, 2022."
        },
        {
          "5. References": "[12] D. Shome and A. Etemad, “Speech emotion recognition with dis-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "tilled prosodic and linguistic affect\nrepresentations,”\nin ICASSP",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[28] Y. Wang, Y. Sun, Y. Huang, Z. Liu, S. Gao, W. Zhang, W. Ge, and"
        },
        {
          "5. References": "2024 - 2024 IEEE International Conference on Acoustics, Speech",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "W. Zhang, “Ferv39k: A large-scale multi-scene dataset for facial"
        },
        {
          "5. References": "and Signal Processing (ICASSP), 2024, pp. 11 976–11 980.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "expression recognition in videos,” in 2022 IEEE/CVF Conference"
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "on Computer Vision and Pattern Recognition (CVPR), 2022, pp."
        },
        {
          "5. References": "[13] R. Li, J. Zhao, and Q. Jin, “Speech emotion recognition via multi-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "20 890–20 899."
        },
        {
          "5. References": "INTERSPEECH 2021 –\nlevel cross-modal distillation,”\nin Proc.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "22nd Annual Conference of the International Speech Communica-",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "[29] A. Ghosh, H. Kumar, and P. S. Sastry, “Robust\nloss functions un-"
        },
        {
          "5. References": "tion Association, 2021, pp. 4488–4492.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "the\nder\nlabel noise for deep neural networks,” in Proceedings of"
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "Thirty-First AAAI Conference on Artiﬁcial\nIntelligence, 2017, p."
        },
        {
          "5. References": "[14] Y. Chen,\nJ. Li, S. Shan, M. Wang, and R. Hong, “From static",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": "1919–1925."
        },
        {
          "5. References": "to dynamic: Adapting landmark-aware image models for\nfacial",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "expression recognition in videos,” IEEE Transactions on Affective",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "Computing, pp. 1–15, 2024.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "[15] H. Du, H. Shi, D. Zeng, X.-P. Zhang, and T. Mei, “The elements of",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "end-to-end deep face recognition: A survey of recent advances,”",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "ACM Computing Surveys\n(CSUR), vol. 54, no. 10s, pp. 1–42,",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        },
        {
          "5. References": "2022.",
          "[16] A. Vidal, A. Salman, W.-C. Lin, and C. Busso, “Msp-face corpus:": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. INTERSPEECH 2022 -23 rd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "3",
      "title": "Does visual selfsupervision improve learning of speech representations for emotion recognition?",
      "authors": [
        "A Shukla",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Audio representation learning by distilling video as privileged information",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Videomae: masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Z Tong",
        "Y Song",
        "J Wang",
        "L Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 36th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "8",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Improving domain generalization in speech emotion recognition with whisper",
      "authors": [
        "E Goron",
        "L Asai",
        "E Rut",
        "M Dinov"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "NIPS Deep Learning and Representation Learning Workshop"
    },
    {
      "citation_id": "11",
      "title": "Cubic knowledge distillation for speech emotion recognition",
      "authors": [
        "Z Lou",
        "S Otake",
        "Z Li",
        "R Kawakami",
        "N Inoue"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Multi-level knowledge distillation for speech emotion recognition in noisy conditions",
      "authors": [
        "Y Liu",
        "H Sun",
        "G Chen",
        "Q Wang",
        "Z Zhao",
        "X Lu",
        "L Wang"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023 -24 th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition with distilled prosodic and linguistic affect representations",
      "authors": [
        "D Shome",
        "A Etemad"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition via multilevel cross-modal distillation",
      "authors": [
        "R Li",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proc. INTERSPEECH 2021 -22 nd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "15",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Y Chen",
        "J Li",
        "S Shan",
        "M Wang",
        "R Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "The elements of end-to-end deep face recognition: A survey of recent advances",
      "authors": [
        "H Du",
        "H Shi",
        "D Zeng",
        "X.-P Zhang",
        "T Mei"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "17",
      "title": "Msp-face corpus: A natural audiovisual emotional database",
      "authors": [
        "A Vidal",
        "A Salman",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "18",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "19",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Hubert-agg: Aggregated representation distillation of hidden-unit bert for robust speech recognition",
      "authors": [
        "W Wang",
        "Y Qian"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Large-scale self-supervised speech representation learning for automatic speaker verification",
      "authors": [
        "Z Chen",
        "S Chen",
        "Y Wu",
        "Y Qian",
        "C Wang",
        "S Liu",
        "Y Qian",
        "M Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Transformers: State-ofthe-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "24",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "25",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "27",
      "title": "Lightface: A hybrid deep face recognition framework",
      "authors": [
        "S Serengil",
        "A Ozpinar"
      ],
      "year": "2020",
      "venue": "2020 Innovations in Intelligent Systems and Applications Conference (ASYU)"
    },
    {
      "citation_id": "28",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "Robust loss functions under label noise for deep neural networks",
      "authors": [
        "A Ghosh",
        "H Kumar",
        "P Sastry"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"
    }
  ]
}