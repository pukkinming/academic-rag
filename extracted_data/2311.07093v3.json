{
  "paper_id": "2311.07093v3",
  "title": "On The Effectiveness Of Asr Representations In Real-World Noisy Speech Emotion Recognition",
  "published": "2023-11-13T05:45:55Z",
  "authors": [
    "Xiaohan Shi",
    "Jiajun He",
    "Xingfeng Li",
    "Tomoki Toda"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Automatic Speech Recognition",
    "Self-Supervised Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose an efficient approach to noisy speech emotion recognition (NSER). Although conventional NSER methods effectively handle some types of noise, such as stationary noise, they struggle with more complex noise encountered in real-world environments owing to its complexity and unpredictability. To address this issue, we introduce a novel NSER method that leverages automatic speech recognition (ASR) models as noise-robust feature extractors, filtering out non-vocal information from noisy speech. Specifically, we extract intermediate layer representations from the ASR model to capture emotional speech features for the NSER task. In this study, we systematically investigate the effectiveness of ASR representations by comparing them with conventional noise reduction methods and fine-tuned self-supervised learning (SSL) approaches. We categorize ASR representations into Encoder and Decoder groups and analyze the last layer, the mean of all layers, and a proposed adapter method. Additionally, we assess the robustness of ASR representations under various noise intensities and types, explore their effects across different modalities, and compare the model's performance with ASR transcripts. Our experimental results demonstrate that (1) our method significantly outperforms conventional noise reduction and SSL approaches, and (2) our method optimizes ASR representations with a proposed adapterbased method, resulting in significant gains for NSER tasks under various noise intensities. However, (3) its performance diminishes with increasing noise intensity, particularly in complex scenarios involving human speech noise, and (4) this decline mirrors the degradation observed in ASR performance as noise intensity increases. Notably, (5) it also surpasses text-based approaches using ASR or ground-truth transcription of noisy speech and (6) exhibits robust performance under cross-lingual conditions compared with mainstream self-supervised representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH emotion recognition (SER) has attracted con- siderable attention over the past two decades, particularly within the domain of human-computer interaction  [1] -  [3] . SER has enabled a variety of applications, fundamentally transforming customer service interactions by allowing chatbots to provide empathetic and contextually appropriate responses  [4] ,  [5] . Additionally, SER has been integrated into in-vehicle dashboard systems, improving driver experience and safety through real-time understanding and responsiveness to emotional states  [6] ,  [7] . Moreover, the utility of this technology extends to speech-to-speech translation platforms, enhancing the accuracy and naturalness of cross-linguistic communication by incorporating emotional context  [8] ,  [9] . Despite these promising applications, SER systems face a significant challenge: the prevalence of unknown noise in various real-world scenarios. This noise severely hampers the performance of SER models, preventing them from effectively distinguishing and accurately recognizing emotions amidst environmental disturbances. Such limitations pose a considerable barrier to the widespread adoption and effective implementation of SER across diverse domains  [10] ,  [11] .\n\nEmpirical investigations have demonstrated the efficacy of established methodologies in mitigating common noise sources, such as white Gaussian noise, in speech-related tasks  [12] -  [17] . However, translating such successes to real-world settings faces challenges posed by a distinct category of noise, which includes ambient sounds such as the clicking of highheeled shoes or the abrupt knocking of doors. Unlike Gaussian noise, these real-world noises exhibit non-Gaussian distributions and possess inherent unpredictability, complicating their effective reduction or elimination through conventional noise reduction techniques  [10] ,  [18] . Consequently, the performance of SER systems, already vulnerable to noise, faces greater limitations in accurately recognizing emotional cues amidst such disturbances  [19] . Therefore, effectively adapting SER systems to tackle these real-world noise challenges is crucial for enhancing their robustness and applicability across diverse contexts.\n\nAutomatic speech recognition (ASR) serves as a cornerstone in contemporary communication systems, facilitating the conversion of spoken language into written text with exceptional precision  [20] . Its applicability spans a broad spectrum of domains, including virtual assistants, transcription services, and voice-controlled devices, among others  [21] . Central to ASR models is their proficiency in discerning and interpreting human speech amidst diverse environmental challenges, such as background noise and reverberations  [22] . Notably, ASR systems are meticulously engineered to differentiate between vocal and non-vocal elements of speech, thereby enhancing their robustness against noise interference  [23] . Thus, we expect that ASR can effectively serve as a noise-robust feature extractor by prioritizing the extraction of vocal features while attenuating extraneous acoustic signals.\n\nCapitalizing on this intrinsic capability, we propose an arXiv:2311.07093v3 [cs.SD] 12 Jan 2025\n\ninnovative approach to address the challenge of noisy speech emotion recognition (NSER) in real-world scenarios. In our framework, ASR representations play a pivotal role in capturing fundamental vocal cues that underpin emotional expressions. By leveraging the inherent robustness of ASR-derived features to noise, we aim to mitigate the harmful effects of environmental disturbances on emotion recognition accuracy.\n\nIn this study, we explore the impact of ASR representations on NSER by comparing them with conventional noise reduction techniques, specifically CONVTAS  [24]  and DCCRN  [25] , as well as fine-tuning methods based on self-supervised learning (SSL) models such as Wav2vec 2.0  [26] , HuBERT  [27] , and WavLM  [28] . A comprehensive analysis is conducted to assess the effectiveness of various ASR representations. This includes a comparative evaluation of different levels of encoder and decoder representations within the ASR model for NSER. We also employ a layer adapter method to integrate representations from different levels. The robustness of ASR representations is further examined under various noise intensities by testing our model across different signal-to-noise ratio (SNR) conditions. We investigate the correlation between ASR performance and NSER performance by testing the ASR model under various noise intensities and types. To evaluate the effectiveness of ASR representations in NSER, we assess their performance across different modalities. Finally, we test the robustness of our proposed model in cross-lingual NSER scenarios.\n\nThe contributions of this paper are summarized as follows:\n\n• Introduction of a novel approach to addressing real-world noise challenges in NSER by utilizing ASR. The remaining sections of this paper are organized as follows: In Section II, we provide an overview of NSER and the application of ASR in SER, forming the foundation for the proposed approach. In Section III, we present the proposed method and model structure. In Section IV, we describe the three databases used in this study. In Section V, we outline the experimental setup, including the control group design, implementation details, and evaluation metrics. In Section VI, we present the experimental results, validating the effectiveness of the proposed model. Finally, in Section VII, we provide a discussion and suggest directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Noisy Speech Emotion Recognition",
      "text": "In real-world scenarios, various types of noise present themselves, each with distinct characteristics and sources  [29] . Gaussian white noise is a typical example, representing a fundamental manifestation of signal randomness. It is characterized as stationary noise with a uniform distribution of power spectral density across all frequencies, symbolizing an essential element of signal variability  [30] . Conversely, impulse noise consists of sudden, high-amplitude bursts of pulses or spikes, often arising from equipment malfunctions or electromagnetic interference incidents  [31] . Additionally, environmental noise encompasses a broad spectrum of background disturbances from the surrounding environment, including sources such as vehicular noise, industrial machinery operations, and human activities, all contributing to real-world noise  [32] .\n\nTo address the multifaceted challenges posed by the diverse noise patterns encountered in real-world scenarios, numerous research initiatives have been undertaken to mitigate the detrimental impact of noise on the accuracy of speech-related tasks, particularly in the domain of NSER. These endeavors typically employ three primary strategies: signal-level, featurelevel, and model-level interventions, as elucidated by Tiwari et al.  [12] . For example, Pandharipande et al.  [13]  proposed an unsupervised method that uses a front-end voice activity detector to selectively extract frames with higher SNRs from spoken utterances, aiming to enhance accuracy under noisy conditions by improving signal quality at the signal level. Similarly, Vasquez-Correa et al.  [33]  proposed a method for emotion recognition using wavelet packet transform features. They classified negative emotions and distinguished them from neutral and positive states by analyzing voiced and unvoiced segments. Features such as log energy and mel-frequency cepstral coefficients were calculated, and the method was tested on the Berlin Emotional Database and the eNTERFACE05 Database under various noise conditions. They also evaluated two speech enhancement techniques, noting improved accuracy in noisy environments. At the feature level, Chenchah and Lachiri  [15]  introduced a novel approach involving MFCCshifted-delta-cepstral coefficients. On the other hand, Sekkate et al.  [16]  proposed an approach combining baseline melfrequency cepstral coefficients, discrete wavelet transformderived MFCCs, and pitch-based features to create a feature set that effectively captures relevant information in a relatively low-dimensional space. At the model level, Tiwari et al.  [12]  devised multi-conditioning and data augmentation using an utterance-level parametric generative noise model. This strategy aims to encompass the entire noise space in the melfilterbank energy domain, rendering the model robust against unseen noise conditions. Additionally, Nam and Lee  [17]  introduced a novel approach employing a cascaded denoising CNN architecture for SER under noisy conditions. This architecture consists of two stages: DnCNN for denoising using residual learning and CNN for subsequent classification at the model level.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Automatic Speech Recognition For Speech Emotion Recognition",
      "text": "ASR is a critical tool for transcribing spoken language into written text, with broad applicability across various realworld domains  [34] . Significant progress has been made in the field of SER through the use of ASR-generated transcripts. Yoon et al.  [35]  pioneered a deep dual recurrent encoder model that simultaneously leveraged both audio signals and text data from the Google Cloud Speech API. Subsequently, Sahu et al.  [36]  used transcripts from two commercial ASR systems for a bimodal SER approach (audio and text), noting a relative decline in unweighted accuracy compared with ground-truth transcripts. Li et al.  [37]  introduced a temporal alignment mean-max pooling mechanism to capture subtle, fine-grained emotions within utterances, as well as a cross-modality excitement module to make sample-specific adjustments to embeddings. Santoso et al.  [38]  proposed a confidence measure to adjust the importance weights of ASR transcripts based on the likelihood of recognition errors in each word, effectively mitigating the impact of ASR errors on SER performance. Wu et al.  [39]  developed a dual-branch model to enhance robustness against ASR errors, featuring a time-synchronous branch that combines speech and text modalities, alongside a time-asynchronous branch that integrates sentence text embeddings from contextual utterances. Shon et al.  [40]  generated pseudo labels for ASR transcripts in semi-supervised speech sentiment analysis. On the basis of human perception mechanisms, Li et al.  [41]  proposed the hierarchical attention fusion of audio features, ASR hidden states, and ASR transcripts, achieving SER performance comparable to that using ground-truth text. Lin and Wang  [42]  explored complementary semantic information from audio to mitigate ASR errors, employing an attention mechanism to calculate weighted acoustic representations on the basis of ASR hypotheses. He et al.  [43]  introduced two auxiliary tasks-ASR error detection and correction-to improve the semantic coherence of ASR text, and proposed a novel multimodal fusion method to learn shared representations across modalities. Finally, Feng and Narayanan  [44]  fused audio with ASR transcripts from an advanced ASR model, demonstrating that ASR-generated outputs deliver competitive SER performance relative to ground-truth transcripts.\n\nASR models inherently prioritize capturing human vocals while effectively filtering out non-vocal components from speech  [23] . Building on the efficacy of ASR models in capturing human speech, in this study, we propose leveraging ASR representations as robust features for emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "In this section, we describe in detail our proposed NSER model, which integrates a large-scale ASR model. As depicted in Fig.  1 , the network comprises two primary components: an embedding module tasked with encoding noisy speech utilizing the ASR model and an emotion recognition module responsible for identifying the emotion label. These components will be further elaborated upon in subsequent sections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Model Description",
      "text": "As depicted, raw noisy audio utterances are fed into dedicated encoder and decoder networks designed to extract speech representations. The layer adapter module then processes these representations at various levels to extract multi-level emotional representations. These representations are subsequently input into the speech emotion classification model to obtain the emotion label.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Asr Embedding Module",
      "text": "Different from most existing models that recognize emotional states using SSL and text features, the proposed model utilizes ASR representations to offer advancements in emotion recognition from several perspectives. Whereas SSL presents the advantage of leveraging extensive unsupervised data for model training, recent ASR systems such as Whisper have primarily benefited from supervised training with substantial labeled datasets. Generally, when ample labeled data is available, supervised learning methods tend to exhibit superior performance. Additionally, ASR representations can capture more speech temporal structures than text features. Therefore, we opt for ASR as our feature extractor.\n\nIn this study, we use Whisper  [45] , a robust supervised speech recognition model known for its noise robustness. Whisper is designed to transcribe spoken language into text and has been trained on a diverse dataset of approximately 680,000 hours of speech in 60 languages  [46] . The model employs an encoder-decoder transformer architecture, where the encoder processes Mel-spectrogram inputs, and the decoder generates text sequences.\n\nWe define the mapping from the audio input x to the speech representation y using Whisper as follows:\n\nwhere F θ denotes the mapping function implemented by Whisper. The audio input x ∈ R t×f is a 2D feature matrix, where t represents the number of time steps and f denotes the feature dimension. The speech representation y ∈ R m×d is a sequence of hidden representations produced by Whisper, where m is the sequence length and d is the feature dimension.\n\nThe speech representation y undergoes initial preprocessing through a convolutional neural network (CNN) layer to obtain the preliminary representation H 0 :\n\nSubsequently, the initial hidden states H 0 are processed through L encoder layers. Each layer updates its hidden states as follows:\n\nwhere m is the length of the input sequence (i.e., the number of time steps or frames in the input), and H l = (h 1 l , h 2 l , . . . , h m l ) ∈ R m×d represents the hidden states at layer l. The final encoder output, H L , encapsulates the contextual information of the entire input sequence, preparing it for the decoding process.\n\nFor the decoder, the initialization begins with a sequence of discrete tokens, starting with a special start-of-sequence token. During inference, the decoder generates tokens autoregressively, where each newly generated token is appended to the input sequence for the next decoding step. At each step, the decoder conditions the previously generated tokens and the contextual information H L from the encoder to produce the next token:\n\nwhere D l represents the decoder's hidden states at step l, and n corresponds to the length of the generated output sequence. Once the decoding process is complete, the final hidden states\n\nare passed through a fully connected layer and a softmax activation to produce the probability distribution over the vocabulary.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Layer Adapter Module (L-Adapter)",
      "text": "To leverage intermediate representations from the initial fine-tuning stages, layer adapters create pathways from each of the encoder and decoder layers to the emotion recognition classification module. Each layer adapter consists of a bidirectional gated recurrent unit (BiGRU) layer, followed by 1D max pooling, a non-linear ReLU activation function, and a normalization layer. The BiGRU network is an advanced variant of the RNN designed to address the vanishing gradient problem  [47] , which is common in sequential data analysis. It has been widely used in emotion-related tasks  [48] -  [50] . Additionally, following a previous study  [41] , as illustrated in Fig  1(b) :\n\nfor l = 1, 2, . . . , L in the encoder, and\n\nfor l = 1, 2, . . . , L in the decoder. The weighted sum of the adapted representations is computed as\n\nThis is fed into the emotion recognition classification, where w",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Emotion Recognition Classification",
      "text": "As shown in Fig.  1 (c), we use a fully connected layer as the classifier for emotion recognition.\n\nThe loss function L used for final emotion classification computes the softmax cross-entropy between the predicted probabilities P (y emo | x) and the ground truth labels y emo :\n\nwhere P (y i emo | x) represents the predicted probability distribution over emotion classes given the speech x, and y i emo denotes the ground truth label for the i-th sample.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Settings",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Database",
      "text": "In this section, we introduce the databases used in this study: the MELD dataset, which contains mixed real-world scenario recordings; the IEMOCAP dataset, offering clean recordings; and the CASIA dataset, designed for investigating cross-lingual scenarios. The emotional distribution across these datasets is illustrated in Fig.  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Meld:",
      "text": "The MELD dataset is an extension of a sentiment-focused dataset, comprising 1,433 dialogues from the American drama \"Old Friends,\" totaling 13,708 sentences and incorporating various data types, including video, text, and speech  [51] . Various real-world noises are recorded within the MELD dataset, such as car horn sounds, knocking noises, plate rattling sounds, and more. Each dialogue in MELD is annotated with seven emotions: anger, disgust, sadness, happiness, neutrality, surprise, and fear. Additionally, the dataset provides standard training, validation, and test sets to facilitate comparative experimental results. Our study focused on four emotions: happiness, anger, sadness, and neutrality.\n\n2) IEMOCAP: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset is widely used in affective computing  [52] . It consists of approximately 12 hours of audio-visual recordings designed specifically for two-person dialogues. Each dialogue is segmented into utterances and annotated with continuous labels in the Valence-Arousal dimension, along with categorical labels for emotional states.\n\nFor this study, we focus on four categorical emotions: neutral, happiness, sadness, and anger, aligning with experimental protocols used in previous research  [49] ,  [50] ,  [53] . Additionally, we combine happiness and excitement into a single category. Given that the IEMOCAP recordings were primarily captured in clean environments, we introduce noise from the DEMAND corpus to simulate real-world noisy conditions. The DEMAND corpus includes six noise categories: Street, Domestic, Office, Public, Nature, and Transportation  [54] . For our study, we selected Office and Nature noises to represent distinct environmental conditions.\n\n2.1 IEMOCAP-Environment: To simulate real-world scenarios, we combined the IEMOCAP data with the Nature category from the DEMAND corpus. Specifically, we used the NPARK segment of the Nature data, recorded in a bustling city park, to augment the IEMOCAP dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iemocap-Human:",
      "text": "To replicate human speech noise in real-world environments, we merged the IEMOCAP data with the Office category from the DEMAND corpus. We specifically used the OMEETING segment of the Office data, recorded in a busy office cafeteria, to augment the IEMOCAP dataset.\n\n3) CASIA: The CASIA Mandarin Chinese Emotional Corpus was meticulously designed in 2005, featuring both parallel and non-parallel transcript recordings conducted in a professional studio  [55] . The set includes 100 parallel sentences intended to mitigate the effect of textual content on emotion recognition. In this study, we leveraged a parallel transcript section containing six emotional states from four speakers: anger, fear, happiness, neutrality, sadness, and surprise, amounting to 7200 utterances.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experimental Procedure",
      "text": "In this study, we conducted six experiments to investigate various aspects of ASR representations in NSER.\n\nIn Experiment 1, we examined the impact of ASR representations on NSER across the MELD, IEMOCAP, IEMOCAP-Environment, and IEMOCAP-Human datasets at an SNR of 0 dB. We explored two main strategies for NSER: noise reduction methods and fine-tuning of SSL models. For noise reduction, we employed the speech separation method (Conv-TasNet) and speech enhancement method (DCCRN). In the SSL fine-tuning category, we fine-tuned models such as Wav2vec 2.0 1  , HuBERT 2  , and WavLM 3  on noisy speech data, which had seen widespread use in tasks involving both SER and NSER  [56] ,  [57] . Furthermore, we compared the performance of conventional ASR models with large-scale ASR models in NSER tasks. In the conventional ASR, we evaluated a Conformer-based ASR model, which had been extensively utilized in tasks combining emotion recognition and ASR  [58] -  [60] . For large-scale ASR, we selected Whisper owing to its robust multi-language support, multi-task processing capabilities, strong generalization from large-scale pre-training, and robustness in complex acoustic environments.\n\nIn Experiment 2, we examined the effectiveness of different ASR representations on NSER across the MELD, IEMOCAP, IEMOCAP-Environment, and IEMOCAP-Human datasets at an SNR of 0 dB. We performed a comparative analysis of the ASR encoder and ASR decoder, evaluating the performance of models using the last layer and the mean of all layer representations. Additionally, we proposed the layer adapter method to fuse representations from different levels, obtaining a more comprehensive ASR representation for NSER.\n\nIn Experiment 3, we examined the robustness of ASR representations in NSER under various noise intensities. A comparative analysis was carried out using the IEMOCAP-Environment and IEMOCAP-Human datasets, systematically controlling noise at SNR levels of -5, 0, and 5 dB. We compared different ASR representations, including the encoder with the adapter, the decoder with the adapter, and a combination of the encoder and decoder with the adapter, alongside representations from SSL models.\n\nIn Experiment 4, we examined the correlation between ASR performance and NSER outcomes under various noise intensities. A comparative analysis was conducted using the MELD, IEMOCAP, IEMOCAP-Environment, and IEMOCAP-Human datasets. We evaluated ASR representations from two models: a Conformer-based ASR model, representing the conventional ASR approach, and Whisper, representing the large-scale ASR approach.\n\nIn Experiment 5, we examined the effectiveness of ASR representations across different modalities in NSER at an SNR of 0 dB. A comparative analysis was carried out between speech-and text-based modalities, leveraging the SSL method on both ASR transcriptions and ground-truth transcriptions.\n\nFor the text modality, we employed a BERT model, following prior studies on emotion recognition  [61] ,  [62] . For the speech modality, we compared different ASR representations from Whisper, including the encoder with the adapter, the decoder with the adapter, and a combination of the encoder and decoder with the adapter.\n\nIn Experiment 6, we examined the impact of cross-lingual settings on the performance of ASR representations in NSER. Although ASR systems effectively captured speech and suppressed background noise, their performance degraded when handling languages not included in their training data. Using the CASIA dataset (Mandarin Chinese) as a monolingual scenario, we compared SSL representations under cross-lingual",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Implementation",
      "text": "Our deep learning models were developed using Python 3.7 and PyTorch 1.11.0. We performed model training and evaluation on a machine equipped with an Intel(R) Xeon(R) Gold 6248 CPU @ 2.50 GHz, 32 GB of RAM, and one NVIDIA Tesla V100 GPU.\n\nIn the speech modality, we employed the Whisper-small 4  as a large-scale ASR model, which is characterized by its architecture comprising 13 layers in both encoder and decoder configurations, each with a hidden layer size of 768. Additionally, we utilized the Conformer-small 6  as the conventional ASR model for this study. To maintain fairness in model sizes, the SSL model utilized in this study is also of the Base size, with a hidden layer size of 768. For the text modality, we employed the BERT-base 7  model, featuring a hidden size of 768, 12 attention layers, and 12 attention heads. Consistent with previous studies, the Whisper and Conformer models were frozen during the training process in this study. For the layer adapter module, the embedding size was kept constant at 512 across all layer adapters. For the SER model, each GRU in the architecture consists of two layers with 256 units each and a dropout rate of 0.5. We used the Adam optimizer with a learning rate of 0.0001 and employed the cross-entropy loss function.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "In evaluating our results on IEMOCAP, where a standard train, dev, and test split is not predefined, we employed a five-fold cross-validation approach, consistent with previous studies  [48] ,  [49] . For MELD, following established practices in prior research  [48] ,  [63] , we utilized the Train and Validation sets for training, with the Test set used for evaluation. As for CASIA, we adopted a four-fold cross-validation approach, aligning with previous works  [64] -  [66] .\n\nFor ASR performance evaluation, we used unweighted average recall (UAR) and F1 scores, which are widely recognized metrics for assessing models on imbalanced datasets  [67] . These metrics were applied across four distinct emotional categories. To evaluate the ASR performance, we used word error rate (WER), which is commonly employed to gauge recognition accuracy in speech-related tasks  [34] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Results And Evaluation A. Effect Of Asr Representations On Nser",
      "text": "In this study, we conducted a comprehensive comparative analysis to assess the efficacy of ASR representations in NSER, benchmarking them against conventional noise reduction methods and fine-tuning strategies using SSL models. The synthesized results of this analysis are systematically presented in Table  I   In summary, ASR representations, particularly Whisper, consistently outperform both conventional denoising and SSL methods across datasets. Whisper demonstrates superior robustness in handling both environmental and speech noise, leveraging rich acoustic and semantic features to achieve significant performance gains. These results underscore its effectiveness for NSER, particularly in challenging noise scenarios.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Effect Of Different Asr Representations",
      "text": "A meticulous comparative analysis was conducted to assess the impact of various ASR representation approaches on the task of NSER, focusing on different layers within the ASR encoder and decoder. The results of this examination are systematically shown in Table  II , which illustrates the performance variations across distinct ASR representation strategies (Last, Mean, and Adapter) within both the encoder and decoder.\n\nThe results provide evidence for the effectiveness of our proposed method (Encoder + Decoder), which integrates representations from both the encoder and decoder through the adapter method. This approach consistently achieves better UAR and F1 performances across all datasets and noise conditions, surpassing standalone encoder and decoder methods.\n\nFor the MELD dataset, compared with using only the Encoder with the Adapter, the proposed approach yields improvements of 2.24% in UAR and 0.86% in F1. Similarly, compared with using only the Decoder with the Adapter, it provides gains of 2.58% in UAR and 1.32% in F1. These results highlight the importance of jointly leveraging representations from both components to maximize performance.\n\nOn the IEMOCAP dataset, the proposed approach surpasses the standalone Encoder with the Adapter by 1.48% in UAR and 2.43% in F1 and the standalone Decoder with the Adapter by 3.75% in UAR and 4.14% in F1. The improvements underscore the advantage of fusing encoder and decoder representations in clean recording environments.\n\nFor the IEMOCAP-Environment subset under noisy conditions, the proposed approach yields increases of 2.44% in UAR and 2.99% in F1 compared with the Encoder alone with the Adapter, and 3.28% in UAR and 4.03% in F1 compared with the Decoder alone with the Adapter. The capability to handle environmental noise more effectively demonstrates the robustness of our proposed design.\n\nIn the IEMOCAP-Human subset, which contains the most challenging noise conditions, the proposed approach shows improvements of 1.75% in UAR and 2.03% in F1 over the Encoder alone with the Adapter, and 1.80% in UAR and 1.78% in F1 over the Decoder alone with the Adapter. These results indicate that integrating encoder and decoder representations enables the proposed method to better address the complexities of human-induced noise.\n\nOverall, our proposed approach consistently demonstrates superior performance by integrating features from both the encoder and decoder via the Adapter method. This strategy captures complementary information across multiple layers of the ASR architecture, leading to significant gains in NSER performance under a variety of datasets and noise conditions. The findings highlight the critical importance of leveraging both representations for emotion recognition tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Robustness Of Asr Representations On Nser",
      "text": "To evaluate the robustness of ASR representations for noisy emotion recognition, we conducted a comparative analysis of various ASR representations, including the encoder with the Adapter (Whisper-Encoder), the decoder with the Adapter (Whisper-Decoder), and a combination of both encoder and decoder with the Adapter (Whisper-Encoder+Decoder), alongside SSL model representations, across various noise intensities. The results are summarized in Table  III .\n\nThe results reveal that as noise intensity increases, the performance of all models declines, with human noise causing greater fluctuations than environmental noise. Conversely, reducing noise intensity leads to consistent performance improvements across the board, demonstrating the robustness of ASR representations.\n\nIn the Environment subset of IEMOCAP, the performance of the Whisper-Encoder+Decoder decreases as noise intensifies. When the SNR drops from 0 to -5 dB, the UAR decreases by 4.22%, whereas the F1 score declines by 6.08%. Conversely, an increase in SNR from 0 to 5 dB results in improvements of 0.84% in UAR and 0.99% in F1. These results demonstrate that although performance is affected under severe noise conditions, the Whisper-Encoder+Decoder maintains notable robustness. Compared with the best-performing SSL representation (Wav2vec 2.0), the Whisper-Encoder+Decoder achieves significant improvements, with gains of up to 11.08% in UAR and 12.11% in F1 at 0 dB. Furthermore, although the Whisper-Encoder shows slightly better stability than the Whisper-Decoder under challenging noise conditions, the integration of both representations ensures the most consistent and robust performance across all SNR levels.\n\nIn the Human subset, which presents more complex human noise challenges, performance declines sharply with increasing noise intensity. For the Whisper-Encoder+Decoder, the UAR decreases by 5.73% and the F1 score drops by 6.81% as the SNR decreases from 0 to -5 dB. Conversely, when the SNR increases to 5 dB, the Whisper-Encoder+Decoder achieves improvements of 2.02% in UAR and 2.32% in F1 relative to 0 dB. Compared with the best-performing SSL representation (Hubert), the Whisper-Encoder+Decoder achieves significant improvements, with gains of up to 8.3% in UAR and 8.37% in F1 at 0 dB. This highlights the superior capability of the Whisper-Encoder+Decoder to address complex human noise effectively.\n\nOverall, the findings confirm the superior robustness of ASR representations, particularly under noisy conditions. Compared with SSL-based models, the proposed method (Whisper-Encoder+Decoder) achieves consistently better performance across all noise levels, with significant improvements observed in both environmental and human noise scenarios.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Correlation Between Asr Performance And Nser Performance",
      "text": "Fig.  3  illustrates the correlation between ASR performance and NSER performance, examining the effects of different noise intensities and types on the effectiveness of ASR. The analysis incorporates representations extracted from Whisper, representing a large-scale ASR approach, and from a Conformer-based ASR model, representing a conventional ASR approach, to compare their ASR and NSER performances under various noisy conditions.  In the IEMOCAP-Environment subset, the ASR performance gradually deteriorates as the SNR decreases. For Whisper, at an SNR of 5 dB, the WER increases to 0.32. At an SNR of 0 dB, this metric rises to 0.39. Under the most challenging condition of -5 dB, the WER peaks at 0.54. Additionally, the F1 score for NSER tasks decreases by 7.07% as the noise level increases from 5 to -5 dB, indicating degraded performance in NSER tasks under higher noise conditions. For Conformer, performance shows even greater sensitivity to noise, with WER increasing to 0.65 at 5 dB SNR, rising further to 0.75 at 0 dB SNR, and reaching 0.86 at -5 dB SNR. The F1 score for NSER using the Conformer model decreases by 6.01% as the noise level increases from 5 to -5 dB, indicating that the Conformer model's performance deteriorates under higher-noise-level conditions.\n\nIn the IEMOCAP-Human subset, performance degradation is more pronounced. For the Whisper model, at SNR of 5 dB, the WER is 0.36. This WER increases to 0.57 at SNR of 0 dB and further increases to 0.86 at -5 dB, indicating a substantial decline in performance with a decreasing SNR. Concurrently, the F1 score for NSER decreases by 9.13% as the noise level increases from 5 to -5 dB. For the Conformer model, performance worsens consistently across all noise levels, with the WER increasing to 0.75 at 5 dB, rising further to 0.87 at 0 dB, and peaking at 0.95 at -5 dB. Similarly, NSER sees an 8.4% decrease in F1 as noise levels increase from 5 to -5 dB.\n\nIn summary, these results emphasize the significant impact of noise on ASR performance, particularly in scenarios with complex human noise. A consistent correlation between ASR performance and NSER is evident within the same type of datasets, i.e., IEMOCAP-Human subset and IEMOCAP-Environment subset, compared with different ASR systems. Furthermore, NSER strongly depends on the type of dataset, i.e., the difference between IEMOCAPs and MELD. Moreover, as noise levels increase, both ASR models exhibit deteriorating performance on NSER tasks. This decrease is slightly less pronounced in Whisper, indicating its comparative effectiveness in environmental and human noise.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Impact Of Asr Representations In Speech And Text Modalities On Nser",
      "text": "To investigate the impact of ASR representations in speech and text modalities on NSER, we conduct a comparative analysis, contrasting different text inputs for BERT in the text modality with various ASR representations in the speech modality, as detailed in Table  IV .\n\nIn the text modality, BERT applied to transcripts consistently outperforms BERT on Whisper-processed text, with notable performance disparities, particularly on the MELD dataset. However, across the MELD, IEMOCAP, and IEMOCAP-Environment datasets, the speech modality demonstrates superior performance over the text modality, highlighting the added value of ASR representations for NSER.\n\nIn the speech modality, the Encoder+Decoder consistently yields the best results, outperforming the individual contributions of the encoder or decoder. This indicates the synergistic potential of integrating comprehensive ASR strategies for NSER. Specifically, on the MELD dataset, the En-coder+Decoder achieves improvements of 5.77% in UAR and 3.37% in F1 compared with BERT on transcripts. Even the Encoder and Decoder individually provide substantial gains, with the Encoder alone improving UAR and F1 by 3.18% and 0.96%, and the Decoder contributing gains of 3.19% and 2.05% in UAR and F1, respectively.\n\nOn the IEMOCAP dataset, similar trends are observed. The Encoder+Decoder demonstrates a significant advantage over BERT on transcripts, with improvements of 6.41% in UAR and 6.94% in F1. The Encoder independently contributes gains of 4.93% in UAR and 4.51% in F1, whereas the Decoder provides improvements of 2.76% in UAR and 2.80% in F1.\n\nIn the Environment subset, the Encoder+Decoder outperforms BERT on transcripts by 2.91% in UAR and 2.83% in F1. In the Human subset, the most challenging noise condition, the Encoder+Decoder achieves slightly lower performance than BERT on transcripts, with decreases of 1.3% in UAR and 1.2% in F1.\n\nOverall, these findings emphasize the overall superiority of the speech modality compared with the text modality in NSER tasks, especially under environmental and mixed noise conditions.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "F. Robustness Of Asr Representations In Scenarios Involving Cross-Lingual Nser",
      "text": "To evaluate the robustness of ASR representations in crosslingual NSER tasks, we conducted experiments in which neither the ASR nor SSL models were exposed to the target language during training. On the other hand, a monolingual ASR model was used as a baseline for comparison. Whisper served as the ASR model, generating both encoder and decoder representations. Table  V  summarizes the performance of SSL models, Mel-Spectrogram, and Whisper-based ASR representations. The abbreviation \"FZ\" indicates frozen models, \"FT\" denotes fine-tuned models, \"EN\" refers to Whisper trained on the English dataset, and \"CN\" refers to Whisper trained on the Mandarin Chinese dataset. The results reveal substantial differences in the performance of various representations. Mel-Spectrogram results serve as a benchmark for comparison. In contrast, SSL models under a frozen configuration show marked performance degradation. For instance, frozen HuBERT's performance is lowered by 27.04% in UAR and 30.03% in F1 compared with the Mel-Spectrogram. These findings highlight the inability of frozen SSL representations to generalize effectively in cross-lingual scenarios. However, through fine-tuning, the performance of SSL models improves significantly. For example, WavLM shows increases of 32.42% in UAR and 33.37% in F1 compared with its frozen configuration.\n\nWhisper-based ASR representations exhibit notably stronger robustness, particularly in cross-lingual (EN) settings. Compared with the Mel-Spectrogram baseline, Whisper-Decoder yields additional gains of 1.77% in UAR and 1.35% in F1. The Whisper-Encoder+Decoder achieves its largest gains with a 2.37% increase in UAR and a 1.61% improvement in F1 over the baseline, illustrating the effectiveness of leveraging complementary features from both modules for cross-lingual emotion recognition.\n\nTo further clarify the performance impact of language mismatch, we compared Whisper-EN and Whisper-CN representations under cross-lingual and monolingual scenarios, respectively. Compared with Whisper-CN, Whisper-EN shows in UAR and F1 declines across all components. For the Encoder, there is a decrease of 3.53% in UAR and 2.41% in F1, whereas the Decoder experiences reductions of 2.11% in UAR and 2.45% in F1. The Encoder+Decoder shows the largest decline, with 2.64% lower UAR and 3.61% lower F1, highlighting the compounded challenges of language mismatch in cross-lingual settings.\n\nOverall, Whisper-based ASR representations consistently surpass both SSL and Mel-Spectrogram baselines in crosslingual and monolingual contexts, demonstrating remarkable robustness against language mismatch and noise.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusions And Future Work",
      "text": "In this paper, we presented a novel approach to NSER by leveraging ASR models as robust feature extractors in noisy environments. Our investigation demonstrated the effectiveness of this method in addressing the limitations of conventional NSER techniques when faced with real-world noise challenges. By utilizing intermediate layer representations from ASR models, we captured emotional speech features with greater accuracy, resulting in superior NSER performance compared with conventional noise reduction methods and SSL approaches.\n\nIn our thorough analysis, we examined the effects of ASR representations across several factors, including noise intensity, types of noise, and modality differences. The experimental results revealed key insights: the proposed method consistently outperforms conventional and SSL approaches, showing robustness against various noise intensities and types, and even surpasses text-based approaches using ASR transcriptions. Notably, our study highlighted the relevance of cross-lingual scenarios, where our method sustains robust performance compared with self-supervised representations.\n\nLooking ahead, future work could further refine our approach by incorporating advanced ASR models or exploring alternative feature extraction techniques to boost NSER performance. Expanding evaluations to a broader range of datasets and exploring the method's robustness across different languages and cultural contexts will enhance our understanding and practical application of SER.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the network comprises two primary components:",
      "page": 3
    },
    {
      "caption": "Figure 1: Structure of the NSER via ASR representations framework, where (a)",
      "page": 3
    },
    {
      "caption": "Figure 1: (c), we use a fully connected layer as",
      "page": 4
    },
    {
      "caption": "Figure 2: Fig. 2. Amount of emotion distribution in IEMOCAP, MELD, and CASIA.",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the correlation between ASR performance",
      "page": 8
    },
    {
      "caption": "Figure 3: Correlation between ASR and NSER performances using Whisper",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "SER",
          "UAR(%) F1(%)": "53.71 53.34"
        },
        {
          "Column_1": "Conv-TasNet + SER\nDCCRN + SER",
          "UAR(%) F1(%)": "55.03 53.84\n53.84 52.82"
        },
        {
          "Column_1": "Wav2Vec 2.0 + SER\nHubert + SER\nWavLM + SER",
          "UAR(%) F1(%)": "61.92 60.32\n61.24 60.26\n60.31 58.49"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP\n(Environment)": "SNR-5",
          "Column_2": "SNR0",
          "Column_3": "SNR5",
          "Column_4": "SNR-5",
          "Column_5": "SNR0"
        },
        {
          "IEMOCAP\n(Environment)": "UAR(%) F1(%)",
          "Column_2": "UAR(%) F1(%)",
          "Column_3": "UAR(%) F1(%)",
          "Column_4": "UAR(%) F1(%)",
          "Column_5": "UAR(%) F1(%)"
        },
        {
          "IEMOCAP\n(Environment)": "57.35 56.41\n57.71 56.21\n56.17 54.40",
          "Column_2": "61.92 60.32\n61.24 60.26\n60.31 58.49",
          "Column_3": "65.02 64.19\n64.53 63.94\n63.24 62.49",
          "Column_4": "50.26 49.26\n54.62 54.57\n56.63 54.33",
          "Column_5": "59.77 57.85\n60.49 60.02\n59.10 57.01"
        },
        {
          "IEMOCAP\n(Environment)": "68.36 67.58\n67.20 65.99\n68.78 66.34",
          "Column_2": "70.56 69.43\n69.72 68.39\n73.00 72.42",
          "Column_3": "73.14 72.66\n70.07 68.99\n73.84 73.41",
          "Column_4": "61.45 60.77\n60.42 60.19\n63.06 61.58",
          "Column_5": "67.04 66.36\n66.99 66.61\n68.79 68.39"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "BERT\nBERT",
          "UAR(%) F1(%)": "62.91 63.03\n70.09 69.59"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "SER"
        },
        {
          "Model": "Wav2Vec2.0(FZ)+SER\nHubert(FZ)+SER\nWavLM(FZ)+SER"
        },
        {
          "Model": "Wav2Vec2.0(FT)+SER\nHubert(FT)+SER\nWavLM(FT)+SER"
        },
        {
          "Model": "Whisper-Encoder(EN)+SER\nWhisper-Decoder(EN)+SER\nWhisper-Encoder+Decoder(EN)+SER"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "3",
      "title": "Emotion in human-computer interaction",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2007",
      "venue": "The human-computer interaction handbook"
    },
    {
      "citation_id": "4",
      "title": "End-to-end speech emotion recognition: challenges of real-life emergency call centers data recordings",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "5",
      "title": "Emma: An emotion-aware wellbeing chatbot",
      "authors": [
        "A Ghandeharioun",
        "D Mcduff",
        "M Czerwinski",
        "K Rowan"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "Emotion regulation for frustrating driving contexts",
      "authors": [
        "H Harris",
        "C Nass"
      ],
      "year": "2011",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "7",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "8",
      "title": "Toward affective speech-to-speech translation: Strategy for emotional speech recognition and synthesis in multiple languages",
      "authors": [
        "M Akagi",
        "X Han",
        "R Elbarougy",
        "Y Hamada",
        "J Li"
      ],
      "year": "2014",
      "venue": "Signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "9",
      "title": "Asia-Pacific. IEEE",
      "year": "2014",
      "venue": "Asia-Pacific. IEEE"
    },
    {
      "citation_id": "10",
      "title": "Toward relaying an affective speech-to-speech translator: Cross-language perception of emotional state represented by emotion dimensions",
      "authors": [
        "R Elbarougy",
        "H Xiao",
        "M Akagi",
        "J Li"
      ],
      "year": "2014",
      "venue": "the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from noisy speech",
      "authors": [
        "M You",
        "C Chen",
        "J Bu",
        "J Liu",
        "J Tao"
      ],
      "year": "2006",
      "venue": "2006 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in the noise applying large acoustic feature sets",
      "authors": [
        "B Schuller",
        "D Arsic",
        "F Wallhoff",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "Emotion recognition in the noise applying large acoustic feature sets"
    },
    {
      "citation_id": "13",
      "title": "Multi-conditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "An unsupervised frame selection technique for robust emotion recognition in noisy speech",
      "authors": [
        "M Pandharipande",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2018",
      "venue": "2018 26th European Signal Processing Conference"
    },
    {
      "citation_id": "15",
      "title": "An approach to blind source separation based on temporal structure of speech signals",
      "authors": [
        "N Murata",
        "S Ikeda",
        "A Ziehe"
      ],
      "year": "2001",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "16",
      "title": "A bio-inspired emotion recognition system under real-life conditions",
      "authors": [
        "F Chenchah",
        "Z Lachiri"
      ],
      "year": "2017",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "17",
      "title": "An investigation of a feature-level fusion for noisy speech emotion recognition",
      "authors": [
        "S Sekkate",
        "M Khalil",
        "A Adib",
        "S Jebara"
      ],
      "year": "2019",
      "venue": "Computers"
    },
    {
      "citation_id": "18",
      "title": "Cascaded convolutional neural network architecture for speech emotion recognition in noisy conditions",
      "authors": [
        "Y Nam",
        "C Lee"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition in noisy and reverberant environments",
      "authors": [
        "P Heracleous",
        "K Yasuda",
        "F Sugaya",
        "A Yoneyama",
        "M Hashimoto"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "20",
      "title": "A review on emotion recognition using speech",
      "authors": [
        "S Basu",
        "J Chakraborty",
        "A Bag",
        "M Aftabuddin"
      ],
      "year": "2017",
      "venue": "2017 International conference on inventive communication and computational technologies (ICICCT)"
    },
    {
      "citation_id": "21",
      "title": "Automatic speech recognition and speech variability: A review",
      "authors": [
        "M Benzeghiba",
        "R Mori",
        "O Deroo",
        "S Dupont",
        "T Erbes",
        "D Jouvet",
        "L Fissore",
        "P Laface",
        "A Mertins",
        "C Ris"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "22",
      "title": "Usability of automatic speech recognition systems for individuals with speech disorders: Past, present, future, and a proposed model",
      "authors": [
        "M Jefferson"
      ],
      "year": "2019",
      "venue": "Usability of automatic speech recognition systems for individuals with speech disorders: Past, present, future, and a proposed model"
    },
    {
      "citation_id": "23",
      "title": "Robust automatic speech recognition: a bridge to practical applications",
      "authors": [
        "J Li",
        "L Deng",
        "R Haeb-Umbach",
        "Y Gong"
      ],
      "year": "2015",
      "venue": "Robust automatic speech recognition: a bridge to practical applications"
    },
    {
      "citation_id": "24",
      "title": "Ageing voices: The effect of changes in voice parameters on asr performance",
      "authors": [
        "R Vipperla",
        "S Renals",
        "J Frankel"
      ],
      "year": "2010",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "25",
      "title": "Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation",
      "authors": [
        "Y Luo",
        "N Mesgarani"
      ],
      "year": "2019",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "26",
      "title": "Dccrn: Deep complex convolution recurrent network for phaseaware speech enhancement",
      "authors": [
        "Y Hu",
        "Y Liu",
        "S Lv",
        "M Xing",
        "S Zhang",
        "Y Fu",
        "J Wu",
        "B Zhang",
        "L Xie"
      ],
      "year": "2020",
      "venue": "Dccrn: Deep complex convolution recurrent network for phaseaware speech enhancement",
      "arxiv": "arXiv:2008.00264"
    },
    {
      "citation_id": "27",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Auditory processing of speech signals for robust speech recognition in real-world noisy environments",
      "authors": [
        "D.-S Kim",
        "S.-Y Lee",
        "R Kil"
      ],
      "year": "1999",
      "venue": "IEEE Transactions on speech and audio processing"
    },
    {
      "citation_id": "31",
      "title": "Analysis of physiological systems: The white-noise approach",
      "authors": [
        "V Marmarelis"
      ],
      "year": "2012",
      "venue": "Analysis of physiological systems: The white-noise approach"
    },
    {
      "citation_id": "32",
      "title": "Impulse noise: critical review",
      "authors": [
        "D Henderson",
        "R Hamernik"
      ],
      "year": "1986",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "33",
      "title": "Environmental noise and the cardiovascular system",
      "authors": [
        "T Münzel",
        "F Schmidt",
        "S Steven",
        "J Herzog",
        "A Daiber",
        "M Sørensen"
      ],
      "year": "2018",
      "venue": "Journal of the American College of Cardiology"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition from speech under environmental noise conditions using wavelet decomposition",
      "authors": [
        "J Vásquez-Correa",
        "N García",
        "J Orozco-Arroyave",
        "J Arias-Londoño",
        "J Vargas-Bonilla",
        "E Nöth"
      ],
      "year": "2015",
      "venue": "2015 International Carnahan Conference on Security Technology (ICCST)"
    },
    {
      "citation_id": "35",
      "title": "Emotions, speech and the asr framework",
      "authors": [
        "L Bosch"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "36",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE spoken language technology workshop (SLT)"
    },
    {
      "citation_id": "37",
      "title": "Multi-modal learning for speech emotion recognition: An analysis and comparison of asr outputs with ground truth transcription",
      "authors": [
        "S Sahu",
        "V Mitra",
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "38",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "arxiv": "arXiv:2010.12733"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "J Santoso",
        "T Yamada",
        "S Makino",
        "K Ishizuka",
        "T Hiramura"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Leveraging pretrained language model for speech sentiment analysis",
      "authors": [
        "S Shon",
        "P Brusco",
        "J Pan",
        "K Han",
        "S Watanabe"
      ],
      "year": "2021",
      "venue": "Leveraging pretrained language model for speech sentiment analysis",
      "arxiv": "arXiv:2106.06598"
    },
    {
      "citation_id": "42",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Robust multi-modal speech emotion recognition with asr error adaptation",
      "authors": [
        "B Lin",
        "L Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Mf-aed-aec: Speech emotion recognition by leveraging multimodal fusion, asr error detection, and asr error correction",
      "authors": [
        "J He",
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Whisper-at: Noiserobust automatic speech recognizers are also strong general audio event taggers",
      "authors": [
        "Y Gong",
        "S Khurana",
        "L Karlinsky",
        "J Glass"
      ],
      "year": "2023",
      "venue": "Whisper-at: Noiserobust automatic speech recognizers are also strong general audio event taggers",
      "arxiv": "arXiv:2307.03183"
    },
    {
      "citation_id": "47",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "48",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "49",
      "title": "Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation",
      "authors": [
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "50",
      "title": "Dimensional emotion prediction based on interactive context in conversation",
      "authors": [
        "X Shi",
        "S Li",
        "J Dang"
      ],
      "year": "2020",
      "venue": "Dimensional emotion prediction based on interactive context in conversation"
    },
    {
      "citation_id": "51",
      "title": "Music theory-inspired acoustic representation for speech emotion recognition",
      "authors": [
        "X Li",
        "X Shi",
        "D Hu",
        "Y Li",
        "Q Zhang",
        "Z Wang",
        "M Unoki",
        "M Akagi"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "53",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "54",
      "title": "Two-stage finetuning of wav2vec 2.0 for speech emotion recognition with asr and gender pretraining",
      "authors": [
        "Y Gao",
        "C Chu",
        "T Kawahara"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "55",
      "title": "The diverse environments multichannel acoustic noise database (demand): A database of multichannel environmental noise recordings",
      "authors": [
        "J Thiemann",
        "N Ito",
        "E Vincent"
      ],
      "year": "2013",
      "venue": "Proceedings of Meetings on Acoustics"
    },
    {
      "citation_id": "56",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "57",
      "title": "Enhancing two-stage finetuning for speech emotion recognition using adapters",
      "authors": [
        "Y Gao",
        "H Shi",
        "C Chu",
        "T Kawahara"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Finegrained disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "H Sun",
        "S Zhao",
        "X Wang",
        "W Zeng",
        "Y Chen",
        "Y Qin"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "59",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition",
      "arxiv": "arXiv:2005.08100"
    },
    {
      "citation_id": "60",
      "title": "Improving speech emotion recognition via fine-tuning asr with speaker information",
      "authors": [
        "B Ta",
        "T Nguyen",
        "D Dang",
        "N Le"
      ],
      "venue": "Improving speech emotion recognition via fine-tuning asr with speaker information"
    },
    {
      "citation_id": "61",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": "2022",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "62",
      "title": "Multi-task conformer with multi-feature combination for speech emotion recognition",
      "authors": [
        "J Seo",
        "B Lee"
      ],
      "year": "2022",
      "venue": "Symmetry"
    },
    {
      "citation_id": "63",
      "title": "Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)"
    },
    {
      "citation_id": "64",
      "title": "A bert based dual-channel explainable text emotion recognition system",
      "authors": [
        "P Kumar",
        "B Raman"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "65",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "66",
      "title": "Speech emotion recognition using multichannel parallel convolutional recurrent neural networks based on gammatone auditory filterbank",
      "authors": [
        "Z Peng",
        "Z Zhu",
        "M Unoki",
        "J Dang",
        "M Akagi"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "67",
      "title": "Speech emotion recognition based on three-channel feature fusion of cnn and bilstm",
      "authors": [
        "L Huang",
        "J Dong",
        "D Zhou",
        "Q Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Gf-capsnet: Using gabor jet and capsule networks for facial age, gender, and expression recognition",
      "authors": [
        "S Hosseini",
        "N Cho"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "69",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    }
  ]
}