{
  "paper_id": "2304.07958v1",
  "title": "Recursive Joint Attention For Audio-Visual Fusion In Regression Based Emotion Recognition",
  "published": "2023-04-17T02:57:39Z",
  "authors": [
    "R Gnana Praveen",
    "Eric Granger",
    "Patrick Cardinal"
  ],
  "keywords": [
    "Emotion Recognition",
    "Audio-Visual Fusion",
    "Attention Mechanisms",
    "Long Short-Term Memory"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In video-based emotion recognition (ER), it is important to effectively leverage the complementary relationship among audio (A) and visual (V) modalities, while retaining the intramodal characteristics of individual modalities. In this paper, a recursive joint attention model is proposed along with long short-term memory (LSTM) modules for the fusion of vocal and facial expressions in regression-based ER. Specifically, we investigated the possibility of exploiting the complementary nature of A and V modalities using a joint cross-attention model in a recursive fashion with LSTMs to capture the intramodal temporal dependencies within the same modalities as well as among the A-V feature representations. By integrating LSTMs with recursive joint cross-attention, our model can efficiently leverage both intra-and inter-modal relationships for the fusion of A and V modalities. The results of extensive experiments 1 performed on the challenging Affwild2 and Fatigue (private) datasets indicate that the proposed A-V fusion model can significantly outperform state-of-art-methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition (ER) is a challenging problem due to the complex and extremely diverse nature of expressions across individuals and cultures. In most of the realworld applications, emotions are exhibited over a wide range of emotional states besides the six basic categorical expressions -anger, disgust, fear, happy, sad, and surprise. For instance, emotional states can be expressed as intensities of fatigue, stress, and pain over discrete levels. Similarly, the wide range of continuous emotional states are often formulated as dimensional ER, where the diverse and complex human emotions are represented along the dimensions of valence and arousal. Valence denotes the range of continuous emotional states pertinent to pleasantness, spanning from being very sad (negative) to very happy (positive). Similarly, arousal spans the range of emotional states related to intensity, from being very passive (sleepiness) to extremely active (high excitement). In this paper, we have focused on developing a robust model for regression-based ER in valence-arousal space, as well as for fatigue.\n\nA and V modalities often carry complementary relationships among themselves, which is crucial to be exploited in order to build an efficient A-V fusion system for regressionbased ER. In addition to the inter-modal relationships across A and V modalities, temporal dynamics in videos carry significant information pertinent to the evolution of facial and vocal expressions over time. Therefore, effectively leveraging both the inter-modal association across the A and V modalities and temporal dynamics (intra-modal) within A and V modalities plays a major role in building a robust A-V recognition system. In this paper, we have investigated the prospect of leveraging these inter-and intra-modal characteristics of A and V modalities in a unified framework. In most of the existing approaches for regression-based ER, LSTMs has been used in order to model the intra-modal temporal dynamics in videos  [1, 2]  due to their efficiency in capturing the long-term temporal dynamics  [3] . On the other hand, cross-attention models  [4]  have been explored to model the inter-modal characteristics of A and V modalities for dimensional ER.\n\nIn this work, we have proposed a unified framework for A-V fusion, which effectively leverages both the intra-and intermodal information in videos using LSTMs and joint cross attention respectively. In order to further improve the A-V feature representations of the joint cross-attention model, we have also explored the recursive attention mechanism. Training the joint cross-attention model recursively allows refining the A and V feature representations, thereby improving the system performance. The main contributions of the paper are as follows. (1) A recursive joint cross-attentional model for A-V fusion is introduced to effectively exploit the complementary relationship across modalities while deploying a recursive mechanism to further refine the A-V feature representations.  (2)  LSTMs are further integrated to effectively capture the temporal dynamics within the individual modalities, as well as within the A-V feature representations. (3) An extensive set of experiments are conducted on the challenging Affwild2 and Fatigue (private) datasets, showing that the proposed A-V fusion model outperforms the related state-ofthe-art models for regression-based ER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "An early DL approach for A-V fusion-based dimensional ER was proposed by Tzirakis et al.  [5] , where the deep features (obtained with Resnet-50 for V and 1D CNN for A) are concatenated and fed to an LSTM. Recently, Vincent et al.  [3]  investigated the effectiveness of attention models and compared them with recurrent networks. They have shown that LSTMs are quite efficient in capturing the temporal dependencies when compared to attention models for dimensional ER. Kuhnke et al.  [2]  proposed a two-stream A-V network, where deep models are used to extract A and V features, and further concatenated for dimensional ER. Most of these approaches fail to effectively capture the intermodal semantics across A and V modalities. In  [6]  and  [7] , authors focused on cross-modal attention using transformers to exploit the inter-modal relationships of A and V modalities for dimensional ER. Rajasekhar et al  [4]  explored cross-attention models to leverage the inter-modal characteristics based on cross-correlation across the A and V features. They improved their approach by introducing joint feature representation into the cross-attention model to retain the intra-modal characteristics  [8, 9] . In most of these approaches, they cannot effectively leverage intra-modal relationships. Chen et al.  [10]  modeled A and V features using LSTMs, and the unimodal predictions are combined using attention weights from conditional attention based on LSTMs. Priyasad et al.  [11]  also explored LSTMs for V features, and used DNN-based attention on the concatenated features of A and V modalities for the final output predictions. Beard et al.  [12]  proposed a recursive recurrent attention model, where LSTMs are augmented using an additional shared memory state in order to capture the multi-modal relationships in a recursive manner. In contrast with these approaches, we focus on modeling the A-V relationships by allowing the A and V features to interact and measure the semantic relevance across and within the modalities in a recursive fashion before feature concatenation. LSTMs are employed for temporal modeling of both uni-modal and multimodal features to further enhance the proposed framework. Therefore, our proposed model effectively leverages the intra-and complementary inter-modal relationships, resulting in a higher level of performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "A) Problem Formulation: Given an input video sub-sequence S, L non-overlapping video clips are uniformly sampled and deep feature vectors X a and X v are extracted for the individual A and V modalities respectively from pre-trained networks. Let\n\nwhere d a and d v represent the dimensions of the A and V feature representations, respectively, and x l a and x l v denotes the A and V feature vectors of the video clips, respectively, for l = 1, 2, ..., L clips. The objective of the problem is to estimate the regression model F : X → Y from the training data X, where X denotes the set of A and V feature vectors of the input video clips and Y represents the regression labels of the corresponding video clips. B) Audio and Visual Networks: Spectrograms has been found to be promising with various 2D-CNNs (Resnet-18  [13] ) for ER  [14, 15] . Therefore, we have explored spectrograms in the proposed framework. In order to effectively leverage the temporal dynamics within A modality, we have also explored LSTMs across the temporal segments of the A sequences. Finally, the A feature vectors of L video clips are shown as X a = (x 1 a , x 2 a , ..., x L a ) ∈ R da×L . Facial expressions exhibit significant information pertinent to both visual appearance and temporal dynamics in videos. LSTMs are found to be efficient in capturing the long-term temporal dynamics while 3DCNNs are effective in capturing the short-term temporal dynamics  [16] . Therefore, we have used LSTMs with 3D CNNs (R3D  [17] ) to obtain the V features for the fusion model. In most of the existing approaches, the output of the last convolution layer is 512 x 7 x 7, which is further passed through a pooling operation in order to reduce the spatial dimensions to 1 (7 → 1). This reduction in spatial dimension was found to leave out useful information as the stride is big. Therefore, inspired by the idea of  [18] , we use the A feature representation to smoothly reduce the spatial dimensions of raw V features for each video clip similar to that of  [18] . Finally, we obtain a matrix of V feature vectors of the video clips as\n\nRecursive Joint Attention Model: Given the A and V features, X a and X v , the joint feature representation is obtained by concatenating the A and V feature vectors J = [X a ; X v ] ∈ R d×L , where d = d a + d v denotes the dimensionality of concatenated features. The concatenated A-V feature representations (J ) of a video sub-sequence (S) are now used to attend to unimodal feature representations X a and X v . The joint correlation matrix C a across the A features X a , and the combined A-V features J are given by:\n\nwhere W ja ∈ R L×L represents learnable weight matrix across the A and combined A-V features, and T denotes transpose operation. Similarly, the joint correlation matrix for V features are given by: The joint correlation matrices capture the semantic relevance across the A and V modalities as well as within the same modalities among consecutive video clips, which helps in effectively leveraging intra-and inter-modal relationships.\n\nAfter computing the joint correlation matrices, the attention weights of the A and V modalities are estimated. For the A modality, the joint correlation matrix C a and the corresponding A features X a are combined using the learnable weight matrices W ca to compute the attention weights of A modality, which is given by H\n\nwhere W ca ∈ R da×da and H a represents the attention maps of the A modality. Similarly, the attention maps (H v ) of V modality are obtained as\n\nwhere W cv ∈ R dv×dv . Then, the attention maps are used to compute the attended features of A and V modalities as:\n\nwhere W ha ∈ R d×da and W hv ∈ R d×dv denote the learnable weight matrices for A and V respectively. After obtaining the attended features they are fed again to the joint crossattentional model to compute the new A and V feature representations as:\n\nwhere\n\nhv ∈ R d×dv denote the learnable weight matrices of t th iteration for A and V respectively. Finally, the attended A and V features after t iterations are further concatenated and fed to BLSTM to obtain the temporal dependencies within the refined A-V feature representations, which are fed to fully connected layers for final prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results And Discussion",
      "text": "A) Datasets: Affwild2 is among the largest public dataset in affective computing, consisting of 564 videos collected from YouTube, all captured in-the-wild  [19] . The annotations are provided by four experts using a joystick and the final annotations are obtained as the average of the four raters. In total, there are 2, 816, 832 frames with 455 subjects, out of which 277 are male and 178 female. The annotations for valence and arousal are provided continuously in the range of [-1, 1]. The dataset is partitioned in a subject-independent manner so that every subject's data will be present in only one subset. The partitioning produces 341, 71, and 152 videos for the training, validation, and test sets respectively.\n\nThe Fatigue dataset (private) is obtained from 18 participants in a Rehabilitation center, suffering from degenerative diseases inducing fatigue. A total of 27 video sessions are captured with a duration of 40 -45 minutes and labeled at sequence level on a scale of 0 to 10 for every 10 to 15 minutes. We have considered 80% of data as training data (50,845 samples) and 20% as validation data (21,792 samples). B) Ablation Study: Table  1  presents the results of the experiments conducted on the validation set for the ablation study. The performance of the approach is evaluated using Concordance Correlation Coefficient (CCC). In this section, we have analyzed the contribution of BLSTMs, where we have performed experiments with and without BLSTMs. Firstly, we have conducted experiments without using BLSTM for both the individual A and V representations as well as the A-V feature representations. Then, we included BLSTMs only for the individual A and V modalities before feeding to the joint attention fusion model i.e., only Unimodal-BLSTMs (U-BLSTMs). By including U-BLSTMs to capture the temporal dependencies within the individual modalities, we can observe improvements in performance. Therefore, BLSTMs are found to be promising in capturing the intra-modal temporal dynamics better than that of correlation-based intramodeling in the joint attention model. After that, we have also included joint BLSTM (J-BLSTM) in order to capture the temporal dynamics across the joint A-V feature representations, which further improved the performance of the system. Note that in the above experiments, we have not performed recursive attention. In addition to the impact of U-BLSTM and J-BLSTMs, we have also conducted a few more experiments to investigate the impact of the recursive behavior of the joint attention model. First, we did recursion without LSTMs and found some improvement due to recursion. Then we included LSTMs and conducted several experiments by varying the number of recursions (iterations) in the fusion model. As we increase the number of recursive times, the model performance increases and starts to decrease after a certain recursion number. A similar trend of the model performance is also observed in the test set. Therefore, this can be attributed to the fact that recursion also works as a regularizer which improves the generalization ability of the model. In our experiments, we found that t = 2 gives the best performance i.e, we have achieved the best performance of our model with two recursive iterations.   2  shows our results against relevant state-of-the-art A-V fusion models on the Af-fwild2 dataset. The Affwild2 dataset has recently been widely used for Affective Behavior Analysis in-the-Wild (ABAW) challenges  [20, 21] . Therefore, we compare our approach with relevant state-of-art approaches in the ABAW challenges. Kuhnke et al  [2]  used a simple feature concatenation using Resnet-18 for A and R3D for V modalities, showing better performance for arousal than valence. Zhang et al  [22]  proposed a leader-follower attention model for fusion, and improved the performance (arousal) of the model proposed by Kuhnke et al  [2] . Rajasekhar et al.  [4]  explored the cross-attention model by leveraging only the inter-modal relationships of A and V, and showed improvement for valence but not for arousal. Rajasekhar et al.  [8]  further improved the performance of the model by introducing joint feature representation to the cross-attention model. The proposed model performs even better than that of vanilla JCA  [8]  by introducing LSTMs as well as a recursive attention mechanism. For test set results, the winner of the latest ABAW challenge  [23]  has shown improvement using A-V fusion, however using three external datasets and multiple backbones. We have also compared the performance of our A and V backbones with the ensembling of LSTMs and transformers  [23]  on the validation set. Vincent  [3]  used LSTMs to capture intra-modal dependencies and explored transformers for cross-modal attention, however, they fail to effectively capture the inter-modal relationships across the consecutive video clips. Rajasekhar et al  [8]  further improved the performance (valence) using joint cross-attention. The proposed model outperforms both  [3]  and  [8] . Table  3  shows the performance of the proposed approach on the Fatigue dataset. We have shown the performance of individual modalities along with feature concatenation and cross-attention  [4] . The proposed approach outperforms cross-attention  [4]  and baseline feature concatenation.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a recursive joint attention model along with BLSTMs that allows effective spatiotemporal A-V fusion for regression-based ER. In particular, the joint attention model is trained in a recursive fashion, allowing to refine the A-V features. We further investigated the impact of BLSTMs for capturing the intra-modal temporal dynamics of individual A and V modalities, as well as A-V features for regressionbased ER. By effectively capturing the intra-modal relationships using BLSTMs, and inter-modal relationships using recursive joint attention, the proposed approach is able to outperform the related state-of-the-art approaches.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed recursive joint attention model with BLSTMs.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance of our approach with components Ours RecursiveJA+BLSTM 0.467 0.405",
      "data": [
        {
          "Validation Set": "Kuhnke et al. [2]"
        },
        {
          "Validation Set": "Zhang et al. [22]"
        },
        {
          "Validation Set": "Rajasekhar et al [4]"
        },
        {
          "Validation Set": "Rajasekhar et al. [8]"
        },
        {
          "Validation Set": "Ours"
        },
        {
          "Validation Set": "Ours"
        },
        {
          "Validation Set": "Test Set"
        },
        {
          "Validation Set": "Meng et al. [23]"
        },
        {
          "Validation Set": "Vincent et al. [3]"
        },
        {
          "Validation Set": "Rajasekhar et al [8]"
        },
        {
          "Validation Set": "Ours"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance of our approach with components Ours RecursiveJA+BLSTM 0.467 0.405",
      "data": [
        {
          "Audio only (2D-CNN: Resnet18)": "Visual only (3D-CNN: R3D)",
          "0.312": "0.415"
        },
        {
          "Audio only (2D-CNN: Resnet18)": "Feature Concatenation",
          "0.312": "0.378"
        },
        {
          "Audio only (2D-CNN: Resnet18)": "Cross Attention [4]",
          "0.312": "0.421"
        },
        {
          "Audio only (2D-CNN: Resnet18)": "Recursive JA + BLSTM (Ours)",
          "0.312": "0.447"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance of our approach with components Ours RecursiveJA+BLSTM 0.467 0.405",
      "data": [
        {
          "JA Fusion w/o Recursion": "Fusion w/o U-BLSTM\n0.670\n0.590"
        },
        {
          "JA Fusion w/o Recursion": "Fusion w/o J-BLSTM\n0.691\n0.646"
        },
        {
          "JA Fusion w/o Recursion": "Fusion w/ U-BLSTM and J-BLSTM\n0.715\n0.688"
        },
        {
          "JA Fusion w/o Recursion": "JA Fusion w/ Recursion"
        },
        {
          "JA Fusion w/o Recursion": "JA Fusion w/o BLSTMs, t = 2\n0.703\n0.623"
        },
        {
          "JA Fusion w/o Recursion": "0.721\n0.694\nJA Fusion with BLSTMs, t = 2"
        },
        {
          "JA Fusion w/o Recursion": "JA Fusion with BLSTMs, t = 3\n0.706\n0.652"
        },
        {
          "JA Fusion w/o Recursion": "JA Fusion with BLSTMs, t = 4\n0.685\n0.601"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "Liam Schoneveld",
        "Alice Othmani",
        "Hazem Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "3",
      "title": "Two-stream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "FG Workshop"
    },
    {
      "citation_id": "4",
      "title": "Timecontinuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition",
      "authors": [
        "Vincent Karas",
        "Mani Kumar Tellamekala",
        "Adria Mallol-Ragolta",
        "Michel Valstar",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Timecontinuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition"
    },
    {
      "citation_id": "5",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "Eric Gnana Praveen Rajasekhar",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "6",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE J. of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiaxin Chen",
        "Stefanos Zafeiriou",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "9",
      "title": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention",
      "authors": [
        "Patrick Gnana Praveen",
        "Eric Cardinal",
        "Granger"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "10",
      "title": "A joint crossattention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "Wheidima Gnana Praveen",
        "Nasib Carneiro De Melo",
        "Haseeb Ullah",
        "Osama Aslam",
        "Théo Zeeshan",
        "Marco Denorme",
        "Alessandro Pedersoli",
        "Simon Koerich",
        "Patrick Bacon",
        "Eric Cardinal",
        "Granger"
      ],
      "year": "2022",
      "venue": "A joint crossattention model for audio-visual fusion in dimensional emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Multi-modal conditional attention fusion for dimensional emotion prediction",
      "authors": [
        "S Chen",
        "Q Jin"
      ],
      "year": "2016",
      "venue": "Proc. of ACM'M"
    },
    {
      "citation_id": "12",
      "title": "Learning salient features for multimodal emotion recognition with recurrent neural networks and attention based fusion",
      "authors": [
        "Priyasad Darshana",
        "S Tharindu",
        "Simon",
        "Clinton"
      ],
      "year": "2019",
      "venue": "Proc. of AVSP"
    },
    {
      "citation_id": "13",
      "title": "Multimodal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "Beard",
        "R Das",
        "P Ng",
        "L Gopalakrishnan",
        "P Eerens",
        "Swietojanski",
        "Miksik"
      ],
      "year": "2018",
      "venue": "CoNLL"
    },
    {
      "citation_id": "14",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from speech using spectrograms and shallow neural networks",
      "authors": [
        "Anwer Slimi",
        "Mohamed Hamroun",
        "Mounir Zrigui",
        "Henri Nicolas"
      ],
      "year": "2020",
      "venue": "ICAMCM"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "Arsha Samuel Albanie",
        "Andrea Nagrani",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Emotion recognition in speech using cross-modal transfer in the wild"
    },
    {
      "citation_id": "17",
      "title": "Videobased emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Yin Fan",
        "Xiangju Lu",
        "Dian Li",
        "Yuanliu Liu"
      ],
      "year": "2016",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "18",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann Lecun",
        "Manohar Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Audio-visual event localization via recursive fusion by joint co-attention",
      "authors": [
        "Bin Duan",
        "Hao Tang",
        "Wei Wang",
        "Ziliang Zong",
        "Guowei Yang",
        "Yan Yan"
      ],
      "year": "2021",
      "venue": "Audio-visual event localization via recursive fusion by joint co-attention"
    },
    {
      "citation_id": "20",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Mihalis Nicolaou",
        "Athanasios Papaioannou",
        "Guoying Zhao",
        "Bjorn Schuller",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "IJCV"
    },
    {
      "citation_id": "21",
      "title": "Analysing affective behavior in the first abaw competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "Analysing affective behavior in the first abaw competition"
    },
    {
      "citation_id": "22",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "23",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Su Zhang",
        "Yi Ding",
        "Ziquan Wei",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "24",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild"
    }
  ]
}