{
  "paper_id": "2509.21747v1",
  "title": "Incorporating Scene Context And Semantic Labels For Enhanced Group-Level Emotion Recognition",
  "published": "2025-09-26T01:25:39Z",
  "authors": [
    "Qing Zhu",
    "Wangdong Guo",
    "Qirong Mao",
    "Xiaohua Huang",
    "Xiuyan Shao",
    "Wenming Zheng"
  ],
  "keywords": [
    "Group-level emotion recognition",
    "feature fusion",
    "semantic label information",
    "large language models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Group-level emotion recognition (GER) aims to identify holistic emotions within a scene involving multiple individuals. Current existed methods underestimate the importance of visual scene contextual information in modeling individual relationships. Furthermore, they overlook the crucial role of semantic information from emotional labels for complete understanding of emotions. To address this limitation, we propose a novel framework that incorporates visual scene context and labelguided semantic information to improve GER performance. It involves the visual context encoding module that leverages multiscale scene information to diversely encode individual relationships. Complementarily, the emotion semantic encoding module utilizes group-level emotion labels to prompt a large language model to generate nuanced emotion lexicons. These lexicons, in conjunction with the emotion labels, are then subsequently refined into comprehensive semantic representations through the utilization of a structured emotion tree. Finally, similarity-aware interaction is proposed to align and integrate visual and semantic information, thereby generating enhanced group-level emotion representations and subsequently improving the performance of GER. Experiments on three widely adopted GER datasets demonstrate that our proposed method achieves competitive performance compared to state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A UTOMATIC recognition of human emotions, a well- explored area within multimedia computing that includes image, audio, text, and video analysis, has significantly advanced our understanding of human behavior  [1] [2] [3] . In recent decades, substantial progress has been made in recognizing emotions at the individual level  [4] [5] [6] . Social behavior research  [7, 8]  indicates that people often change their reactions and behaviors based on their perceptions of others' emotions. As a result, there has been increased interest in group-level emotion recognition (GER). GER seeks to classify the collective emotion of multiple individuals into three categories: positive, neutral, and negative (as illustrated in Fig.  1 ), which has significant societal implications. It is relevant to various fields such as social behavior analysis, public security, and humanrobot interactions  [9, 10] .\n\nGiven a group-level emotion image, the essence of GER is in modeling the collaborative relationships among individuals and comprehending the contextual information within the scene. Previous works  [11] [12] [13]  have employed deep learning techniques to explore diverse emotion-related features and effectively aggregate them for group-level emotion inference. We revisit these methods through the lens of a causal graph, as illustrated in Fig.  2(a) . In this graph, I → C represents the generation of global scene features from the given image, I → X signifies the extraction of individual features (e.g., face, object) with the help of corresponding detectors from the input image, C → Z and X → Z depict the integration of scene features and aggregated individual features to generate group-level representations for emotion classification, respectively. However, these methods primarily focus on localizing individual features while neglecting the broader global scene context, which is crucial to enhancing GER.\n\nTo resolve the above issue, recent works  [14] [15] [16] [17]  have leveraged the global scene context to facilitate interactions among individuals, as illustrated by the corresponding causal graph in Fig.  2(b ). The primary distinctions are C → Y : generates the context-encoded individual features through weighted fusion with the global scene feature and primordial individual features, and Y → Z: the context-encoded individual features are then utilized to derive context-aware representations, rather than reasoning directly on the individual features. Several researchers  [14, 15]  straightforwardly concatenated the scene information with individual characteristics and then used attention mechanisms to generate fusion weights, while achieving notable results. Furthermore, Wang et al.  [17]  employed cosine similarity between the global scene and individual features in the subspace to determine the fusion weights, achieving stateof-the-art performance in recognition accuracy. The shortcomings of these methods are reflected in two aspects:  (1)  treating scene information and individuals on a one-to-one basis, which weakens the encoding of global scene context into individual interaction modeling; (2) struggling to capture comprehensive context from complex scenes due to insufficient semantic information.\n\nIn fact, given that all individuals coexist within the same scene, it is crucial to model interactions among individuals, guided by the global scene context. However, existing methods primarily rely on visual modality alone, neglecting the potential benefits of incorporating semantic-level information associated with emotion labels. This limitation hinders the advancement of GER. Effective GER requires not only an understanding of the visual scene but also the integration of explicit semantic guidance to refine emotional interpretation. For example, when identifying a \"positive\" image, as illustrated in Fig.  1 , it is imperative to analyze individuals, their interactions, and the surrounding environment holistically. The semantic information linked to the \"positive\" label provides valuable contextual cues, such as smiles, expressive body language, traditional attire, and festive decorations. These factors collectively enhance the perception of conveyed emotions, thereby improving the accuracy of group-level emotion inference.\n\nTo address the aforementioned drawbacks, we propose a novel method guided by a newly designed causal graph, as illustrated in Fig.  2(c ). A key distinction of our method from existing context-based approaches is its ability to incorporate multi-scale scene context information C to enhance the encoding of individual features and effectively model their relationships Y → Y . Furthermore, we introduce a novel branch I → T that leverages image labels to generate emotion semantic embeddings, thereby refining the overall emotion representation Z.\n\nIn practice, we propose a novel reasoning paradigm that integrates visual scene context with label-guided semantic information to enhance GER. Unlike previous methods that rely heavily on the entire complex scene, our approach utilizes multi-scale scene features to encode interactions among individual features in a diversified manner, leveraging a crossattention mechanism. Specifically, we align individual features X with global scene features C across multiple scales, aggregating and refining contextual information to generate diverse context-encoded representations Y . This process effectively mitigates the ambiguity arising from scene complexity. Furthermore, we derive label-guided semantic embeddings T to enhance group-level emotion representation. To enrich semantic understanding beyond simple class labels, we employ a Large Language Model (LLM) such as ChatGPT, to generate an extensive collection of nuanced emotion lexicons pertinent to group attribute. Subsequently, we utilize a Graph Convolutional Network (GCN) to learn discriminative semantic representations by traversing a structured emotion tree, effectively linking emotion lexicons to their corresponding emotion labels. Finally, we align and integrate visual and semantic feature to contruct a more robust representation of grouplevel emotions for emotion prediction Z. Experimental results demonstrate that our proposed method outperforms state-ofthe-art methods on three widely adopted GER datasets.\n\nThe main contributions of this paper are summarized as follows:\n\n• We propose a novel reasoning paradigm that effectively integrates rich visual scene context and label-related semantics within a new causal graph framework, meticulously crafted to enhance GER. • It is the first time in GER that label-related semantics have been embedded, overcoming the limitations of conventional approaches that relied purely on visual features. By prompting a LLM with group-level emotion labels to generate nuanced emotion lexicons, a comprehensive semantic understanding is facilitated, thereby enhancing the discriminability of predicted group-level emotion representations. • Extensive evaluations on three public GER datasets, i.e., GAFF2, GAFF3, and GroupEmoW, demonstrate that our approach achieves competitive performance compared to existing methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Group-Level Emotion Recognition",
      "text": "In recent years, GER has received increasing attention, leading to the development of numerous advanced approaches. Much of the research in this domain focuses on integrating diverse emotional cues to infer group-level emotions  [12, 15, [17] [18] [19] [20] . As GER research evolves, recent studies have emphasized the significance of incorporating supplementary contextual factors, such as objects and scenes, to improve recognition accuracy. Fujii et al.  [14]  proposed a hierarchical model that first classifies facial features and subsequently merges object and scene information. Similarly, Dai et al.  [21]  employed Long Short Term Memory (LSTM) networks to model individual features, enhancing group emotion representation. These advances show the pivotal role of context integration in improving GER accuracy and robustness.\n\nThe rapid progress in deep learning have facilitated methods that encode global context into individual features while refining aggregated information. Several studies have utilized Graph Neural Networks (GNNs) to model interactions among contextually enriched individual features, leading to improved group-level emotion understanding. For instance, Guo et al.  [18]  utilized GNNs to model relationships among face, object, and skeleton features based on holistic scene comprehension. More recently, attention-based methods have been employed to enhance the modeling of individual interactions within a scene, yielding significant improvements in GER. Khan et al.  [15]  introduced regional attention and contextaware fusion to assign appropriate weights to emotion streams. Wang et al.  [17]  mapped scene and individual features into a shared subspace for similarity-based fusion. Additionally, Xie et al.  [22]  designed cross-patch attention within vision transformers to integrate scene contexts and facial regions effectively.\n\nDespite these advances, existing methods primarily integrate global scene and individual features in a one-to-one manner, limiting the effectiveness of global context efficacy and neglecting semantic emotion cues. This constraint hinders a comprehensive understanding of group emotions. Effective GER requires a synergistic approach that models both visual contexts and emotion-guided semantics. Our approach addresses these limitations by jointly leveraging multi-scale scene contexts and label-aligned emotion semantics, thereby enhancing GER performance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Semantic Encoding Based On Labels",
      "text": "Emotion label information plays a pivotal role in emotion analysis tasks by providing semantic guidance, enriching emotion embeddings, and facilitating multi-modal feature fusion  [23, 24] . Labels enable models capture features relevant to each emotion category, enhance the alignment of multi-modal data, and serve as a foundation for constructing loss functions or regularization terms  [25] [26] [27] . Moreover, they contribute to a more granular recognition of emotions and support the construction of emotion knowledge graphs, which capture relationships among various emotions, ultimately leading to more accurate and context-aware predictions  [28, 29] .\n\nThe semantic encoding of labels has been widely applied in various image and video understanding tasks, including video caption  [30] , image classification  [31] [32] [33] , facial expression recognition  [34]  and group activity recognition  [35, 36] . For instance, Liu et al.  [35]  directly utilized group activity labels to construct a semantic graph, refining visual representations. Song et al.  [30]  introduced an emotion-prior awareness network that integrates catalog-level psychological categories with lexical-level common words to achieve precise and detailed emotion recognition. More recently, the use of LLMs for obtain semantic information to support research tasks has seen rapid development  [37, 38] . Zhao et al.  [34]  proposed a visionlanguage model that leverages expression descriptors related to facial behavior, generated by LLMs based on emotion labels, as textual input to enhance expression understanding.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "The proposed framework (Fig.  3 ) consists of three key modules: Visual Context Encoding (VCEM), Emotion Semantic Encoding (ESEM), and Visual-Semantic Interaction (VSIM). Given an input image, the framework first detects facial and object regions and extracts their features extracted using CNNbased encoders. To enhance contextual understanding, scene and face features are refined through a Context-Aware Mixer (CAM), which models interactions between these elements. The refined facial features are then fused with object features via a transformer encoder. In parallel, ESEM encodes emotionrelated semantic information. It processes emotion labels and LLM-generated lexicons, converting them into structured semantic embeddings. These embeddings are organized as an emotion tree and further refined using GCN-based Emotion Semantic Learning (ESL) to enhance representation quality. Finally, VSIM integrates the learned visual-semantic features through Similarity-aware Feature Fusion (SFF). The alignment of these representations is optimized through a Similarity Alignment Matching (SAM) loss, ensuring robust feature correspondence for the final emotion classification task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Visual Context Encoding Module",
      "text": "The VCEM is designed to effectively integrate visual features relevant to emotions by combining both local individual features (encompassing faces and objects) with global context (incorporating the overall scene). It begins by encoding context-aware details to refine individual features, ensuring a more comprehensive representation. These enriched feature are then fused through a fusion mechanism, allowing the model to capture intricate relationships between individuals and their surrounding environment. This structure integration enhance the precision of GER.\n\n1) Visual Representation Extraction: The input image is divided into three primary visual cues: face, object, and scene. For face features, a VGG16 network is employed to extract local features from the detected face regions using an off-the-shelf face detector  [39] , resulting in the set {x f i } I i=1 . Object features are extracted applied VGG16 to regions proposed by an object detector  [40] , yielding {x o j } J j=1 . Scene features are obtained using ResNet50 with Feature Pyramid Networks (FPN)  [41]  to capture multi-scale representations, denoted as {x s k } K k=1 . These multi-scale features are aligned via RoIAlign  [42]  and subsequently projected into a latent space through fully connected layers.\n\n2) Context-Aware Mixer: To enhance context integration, the CAM is designed to fuse multi-scale scene context with face individual representations. Given the face features\n\nwe prepend the kth scale scene feature x s k to the face individual features. The CAM is formulated as follows:\n\nwhere [; ] denotes feature concatenation, W (•) represents trainable weight matrices and h f k denotes the context-enhanced face individual feature at the kth scale. Subsequently, each scale undergoes weighted fusion:\n\nwhere w k represents scale-specific fusion weights derived from the concatenation of scene and face individual features, σ is the sigmoid activation, and W (ε) is a learned weight matrix. The final aggregated face individual representation v f , encapsulates multi-scale context information. Within the CAM, the aggregated face individual representations v f are concatenated with the scene features\n\nfollowed by processing through multi-head attention (MHA) and a feed-forward network (FFN), ensuring cohesive integration of contextual information:\n\nwhere v f ;s represents the refined fusion of scene and face features, effectively leveraging scene context to enhance individual relationship modeling and interactions.\n\nIn the final stage, the refined face-scene representation v f ;s is fused with object features x o to produce the overall visual embedding:\n\nwhere the fusion layer leverages a standard transformer encoder (4-layer transformer blocks within multi-head attention and feed-forward operations) to derive the unified representation F v , effectively capturing the comprehensive visual emotion representation essential for downstream GER.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Emotion Semantic Encoding Module",
      "text": "This subsection describes the construction of an emotion tree and the structured representations of emotion semantics. By prompting a LLM to generate lexicons associated with predefined emotion labels, our approach effectively captures fine-grained emotional semantics. To model the interconnections and relationships between these lexicons and labels, we employ GCNs to jointly refine their embedding. This process ensures that multi-granularity semantic information is seamlessly integrated, resulting in a unified and comprehensive emotion semantic representation.\n\n1) Emotion Tree Construction: In GER, existing approaches often struggle to capture the nuanced dynamics of group emotions due to the overly broad and generalized nature of emotion labels (e.g., positive, neutral, or negative). This limitation hinders the model's ability to accurately interpret subtle emotional cues and complex interpersonal interactions within scene contexts. To address this issue, we leverage LLM (e.g., ChatGPT) to automatically generate fine-grained emotion lexicons, eliminating the constraints of manual design. This systematic approach produces rich, context-aware lexicons that encapsulate detailed group-level emotional dynamics, thereby enhancing the model's ability to represent and differentiate intricate emotional variations effectively. The LLM is prompted with queries such as: Q: Please provide me with some words that convey grouplevel emotion of {class}.\n\nA: Here are some words that convey {class} group-level emotions:. . . where {class} is sequentially replaced with each group-level emotion class: positive, neutral, and negative.\n\nThe generated lexicons for each group-level emotion class are used to construct an emotion tree, providing a structured and hierarchical framework that organizes emotional semantic associated with each label. Following the hierarchical emotion structure outlined in  [30] , the emotion tree categorizes finegrained emotion lexicons such as \"Joy\", \"Unity\", and \"Solidarity\" under broader emotion classes like \"Positive\". This hierarchical representation enhance the model's ability to capture nuanced emotional cues at multiple levels of granularity, improving its interpretability and classification performance.   v(e c  m ), are extracted using GloVe  [43] , while the emotion class embedding v(E c ) is obtained in a similar manner.\n\nTo capture the relationships among emotion lexicons and their corresponding emotion class, we construct a fully connected graph G c = (V c , A c ), where A c is the adjacency matrix constructed using the cosine similarity among lexicon embeddings and the class embedding. The node set V c includes both the lexicon embeddings and the corresponding class label embedding:\n\nThe constructed graph G c is processed through a GCN, which updates the node embeddings. This operation can be compactly expressed as:\n\nwhere H represents the final node embeddings after two graph convolution layers, with intermediate activations and regularization (such as ReLU and dropout) applied within the GCN function.\n\nNext, we fuse these embeddings with the emotion class embedding v(E c ) using a simple attention mechanism. Specifically, the attention weights are obtained via a softmax operation, and the final emotion class embedding is obtained as a weighted average of the node embeddings:\n\nFurthermore, to effectively capture both the overall distribution and the most salient features of each emotion class, the overall class embedding is derived by concatenating the mean and max pooling of the fused embeddings:\n\nFinally, the embeddings for the positive, neutral, and negative emotion classes are concatenated to generate the final emotion semantic representation F t .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Visual-Semantic Interactive Module",
      "text": "Given the complexity of group environments, we propose a VSIM to enhance the alignment and interaction between visual and semantic information, thereby improving recognition performance. This module incorporates two key strategies: SFF and SAM loss function. The SFF mechanism selectively emphasizes meaningful visual-semantic correlations in the final representation, while the SAM loss reinforces alignment across different emotional features by ensuring bidirectional consistency between visual and semantic information.\n\n1) Similarity-aware Feature Fusion: In some cases, particularly where the visual-textual relationship is weak (e.g., ambiguous or uncertain emotions), direct concatenation can introduce noise. To effectively integrate visual and semantic features, we introduce distinct projection heads, each consisting of a fully connected layer. The projected features are then used to compute the cosine similarity between the visual and semantic representations. This similarity is leveraged to modulate the intensity of the fused feature F v;t , enhancing the integration of two branches:\n\nThen, we further refine the fusion process through similarity-weighted adjustment. During training, the similarity scores sim are standardized by calculating their mean and standard deviation. A Sigmoid function is then applied to map the standardized scores to the range [0, 1]. These re-scaled similarity scores are subsequently used to reweight the fused features for enhanced context integration:\n\nThe adjusted feature F ′ v;t is then processed through 4-layer transformer blocks to derive the final group-level emotion representation F group .\n\n2) Similarity Alignment Matching Loss: To achieve robust alignment between visual and semantic features, we introduce a similarity alignment matching loss, motivated by the bidirectional cross-modal matching method proposed in  [44] . The SAM loss accounts for both visual-to-semantic and semanticto-visual interactions.\n\nGiven a mini-batch of N samples, i.e., N × N visualtext pairs, the similarity between each visual feature F vi and all semantic features {F tj } N j=1 is evaluated using cosine similarity:\n\nEqually, the similarity between each semantic feature F tj and all visual features {F vi } N i=1 is also computed. Matching probabilities are obtained by applying a softmax function with a temperature scaling factor α:\n\nThe SAM loss combines both visual-to-text and text-tovisual components. For visual-to-text alignment, the loss is defined as:\n\nwhere ϵ is a small constant to avoid numerical instability. q i,j denotes the ground truth matching probability. The matching probability is typically normalized as follows:\n\nwhere y i,j denote the true matching label if i and j represent a true matching pair from the same emotion label, y i,j = 1; otherwise, y i,j = 0. The value y i,k represents the probability that the pair is correctly matched. Symmetrically, for text-to-visual alignment, the same operation is applied by swapping the roles of visual and semantic features. The final SAM loss is computed as:",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Training And Optimization",
      "text": "Our framework adopts an end-to-end training strategy that jointly optimizes classification and emotional feature alignment. To achieve effective GER, we define two main objectives: classification loss and visual-semantic alignment loss. The classification loss comprises four components: L group , which supervises the fused group-level feature F group (derived from both semantic and visual branches), and component losses L s , L f , and L o , which correspond to the classification of different visual emotion cues. Additionally, we introduce a similarity alignment matching loss, L SAM , which enforces consistent alignment between visual and semantic features. The overall optimization objective for training is defined as:\n\nwhere L cls represents the cross-entropy loss series. During inference, the model uses the refined group-level representation for emotion prediction, capturing both classification cues and cross-branch consistency to enhance GER performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "A. Experiment Settings GAFF Datasets. The GAFF datasets comprise two benchmark datasets: Group AFFective 2.0 (GAFF2)  [45]  and Group AFFective 3.0 (GAFF3)  [46] . These datasets were curated from online sources using targeted keywords such as \"protest,\" \"violence,\" and \"festival\" to ensure that each image contains at leasttwo individuals.    I . Importantly, the labels for all sets are publicly available.\n\nFor training and evaluation, we followed the protocols outlined in  [15, 17] , using using the training set for model training and the test set for performance evaluation. Implementation Details. For all datasets, face regions are first detected and cropped using the MTCNN detector  [39] , then resized to 224 × 224 pixels. Object proposals are generated by Faster R-CNN  [40]  pretrained on the MSCOCO dataset, while scene samples are uniformly resized to 256×256 pixels. The extracted features from each emotion cue and branch are standardized to a 512-dimensional vector. The hidden size and the number of heads for each layer in the multi-head attention and feed-forward network are set to 512 and 8, respectively. During training, we adopt the ADAM optimizer with hyperparameters β 1 = 0.9, β 2 = 0.999, and ϵ = 10 -8 , with an initial learning rate of 0.001 that decays by a factor of 0.9 per iteration. The momentum is fixed at 0.9, and the batch size is set to 4. The temperature parameter α in the SAM loss is set to 0.02. The entire framework is implemented in PyTorch, with training accelerated using an NVIDIA RTX 3090 GPU.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Comparison With State-Of-The-Art Methods",
      "text": "We compared the proposed framework with several stateof-the-art GER approaches, reflecting the latest advancements in this field. These methods can be grouped into two categories: (1) independently learning emotion representations from different visual emotion cues  [11-13, 18, 20, 47] ; and (2) enhancing individual representations by encoding global scene contextual information using different techniques, such as GNNs  [18]  or attention mechanisms  [14-17, 19, 22] . To ensure a fair comparison, we assess performance under consistent    conditions in the visual branch, which can be broadly classified into two types: (1) methods that integrate face and scene cues, and (2) methods that integrate face, scene, and object cues. 1) Results on the GAFF2 dataset: The performance comparisons of various methods are summarized in Table  II . In the scenario where the visual branch uses both face and scene cues, our proposed method achieves 79.50% on the overall result, surpassing state-of-the-art methods on the GAFF2 dataset. However, the recognition accuracy of the proposed method for the positive and neutral categories is lower than that of the Zhang et al.  [16] . This is can be attributed to the fact that Zhang et al.  [16]  adopts a contrastive learning strategy, which not only extracts effective features from labeled images but also leverages information from unlabeled images, significantly boosting the model's ability to recognize emotions in the positive and neutral categories. Nevertheless, the recognition accuracy of the negative category is higher in our proposed method compared to Zhang et al.  [16] . This may be because Zhang et al.  [16]  struggles to effectively address the confusion between neutral and negative categories. In contrast, our approach enhances recognition by integrating global contextual cues with individual features while leveraging labeldriven semantics to refine emotion embeddings, effectively boosting GER performance.\n\nFurthermore, when compared to the results obtained using all three emotion cues (face, object, and scene) in the visual branch, our method achieves competitive performance, recording an overall result of 80.25%. Although our approach slightly underperforms the hierarchical classification strategy in  [14] , it is worth noting that even the top-performing method by Wang et al.  [17]  does not surpass  [14]  under similar conditions. Fujii et al.initially conducted binary classification using aggregated facial features and then extended their approach to a three-class classification, incorporating object and scene features while harnessing computational resources to capture intricate object details. In contrast, our method utilizes a lightweight CNN-based encoder for direct threeclass classification of object information, optimizing computational efficiency without sacrificing performance. Additionally, Table  III  presents a comparative analysis of the method complexity between our approach and that of Fujii et al.  [14] , demonstrating that our method achieves competitive recognition performance while significantly reducing computational complexity. Moreover, our method notably improves accuracy in positive category, highlighting its efficacy in capturing positive emotion via using LLMs with emotion labels to generate fine-grained emotion lexicons. However, in neutral category, where emotional cues are less pronounced, this approach may cause the model to deviate since it struggles to fully capture their characteristics, leading to a noticeable performance drop in these categories.\n\n2) Results on the GAFF3 dataset: Table  IV  presents a comparison with various methods on the GAFF3 dataset. Our method outperforms the state-of-the-art methods, achieving the highest overall result. Specifically, when comparing the performance of methods that utilize face and scene cues in the visual branch, our model demonstrates significant improvements, reaching the highest overall result of 81.33%. This outperforms two current notably proficient methods, Wang et al.  [17]  and Xie et al.  [22] , which yielded overall accuracies of 79.08% and 79.20%, respectively. However, the proposed method performs relatively poorly in identifying neutral category. On the other hand, it achieves a significant performance boost in both the positive and negative categories. This can be attributed to 3) Results on the GroupEmoW dataset: Table V presents a detailed comparison of our approach with several state-ofthe-art methods on the GroupEmoW dataset. When utilizing only face and scene features, our model achieves notable performance across all emotion categories, with a particularly significant improvement in the neutral (87.25%) and negative (90.02%) classes, surpassing the best-performing prior method by 1.62% and 3.25%, respectively. This leads to an overall accuracy increase of 1.44%, indicating better overall class balance. Incorporating object features alongside face and scene cues results in further performance enhancements. Specifically, in the positive class, our method achieves a new high of 96.68%, outperforming all previous methods. Notably, the neutral class improves to 86.03%, while the negative class remains competitive at 88.63%. As a result, our method achieves the highest overall accuracy of 91.18%. Collectively, these results highlight the effectiveness of our method in improving GER performance. They demonstrate the robustness and generalization capability of our model in handling complex emotional scenarios that involve multiple emotion cues and branches. This improvements can be attributes to the integration of comprehensive scene context and labelrelated semantics, which together enhance the model's ability to interpret nuanced emotional signals.\n\nV. ABLATION STUDIES 1) Effects of the Proposed Modules: To better understand the contribution of each module in our method, we conduct the following ablation studies.\n\n• B1 (Base model): This base model includes backbones corresponding to the face, object, and scene cues, along with a final softmax classification layer. This is the only visual branch.\n\n• B2 (B1+VCEM): This method integrates the B1 with VCEM. This variant integrates the CAM to enhance individual representations by leveraging multi-scale context information. Preceding the final softmax classification layer, both emotion cues in the visual branch undergo interaction through a standard transformer encoder. • B3 (B2+ESEM): This method consists of the network B2, and ESEM. This variant, while employing LLM, it generates fine-grained emotion lexicons and models their associations with the emotion labels, thereby enhancing the semantic information. • B4 (Ours): The method encompasses the network B3, complemented by the VSIM. Within VSIM, both the visual and the semantic information are incorporated to gradually obtain enhanced group-level emotion representation. The results in Table VI demonstrate the effectiveness of each core module through comparisons across key model variants. Specifically, integrating the VCEM into the baseline (\"No. 3\" vs. \"No. 1\") leads to a significant improvement in overall performance, increasing from 74.41% to 78.66%. This demonstrates the importance of capturing rich visual contextual information in GER. However, incorporating the ESEM (\"No. 4\" vs. \"No. 3\") results in a slight performance drop of 0.84%. This decline can be attributed to the direct concatenation of features from the visual and semantic branches, which lacks proper alignment and fusion of the two modalities, potentially introducing noise that offsets the benefits of lexicon-based semantic representations generated by LLM. The most significant improvement occurs with the introduction of VSIM (No. 7\" vs. \"No. 4), yielding a gain of 2.43% and achieving an overall accuracy of 80.25%. These results emphasize that integrating both visual scene context and label-based semantic features, alongside a similarity-aware alignment and fusion strategy, significantly enhances grouplevel emotion representation, leading to notable improvements in GER performance.\n\n2) Variants of Semantic and Visual Information Fusion: To evaluate the effectiveness of our VSIM, we conducted ablation studies on different fusion strategies, as shown in experiments \"No. 5\" to \"No. 7\" in Table  VI . The comparison between \"No. 4\" and \"No. 5\" indicates the importance of adopting a similarity-weighted adjustment mechanism rather than straightforward concatenation for better integration of visual and semantic information. Further enhancing this align-   ment, experiment \"No. 6\" incorporates the L SAM loss, resulting in a notable improvement in recognition accuracy, achieving an overall performance of 79.50%. By combining both SFF and L SAM in \"No. 7\", the model demonstrates a stronger ability to capture complex interactions, leading to a 2.43% increase in overall performance compared to \"No. 4\".\n\n3) Effect of Visual Context Encoding and Semantic Embedding Variants: The results in Table VI illustrate the performance improvements achieved by incorporating visual context encoding and semantic embeddings. Specifically, for the VCEM, the comparison between \"No. 3\" and \"No. 2\" shows that integrating the CAM mechanism increases the overall performance from 77.67% to 78.66%. This enhancement highlights the effectiveness of encoding multi-scale visual context information, enabling richer and more nuanced emotion representations compared to using only a standard transformer encoder for fusing different emotion cues.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Visualization And Analysis",
      "text": "1) The visualization of confusion matrices. Fig.  4  illustrates a comparative analysis of classification performance between the baseline and our proposed methods on the GAFF2, GAFF3, and GroupEmoW datasets, focusing on three emotion categories: negative, neutral, and positive. As shown in Fig.  4 (a), the baseline model on the GAFF2 dataset achieves high accuracy for the negative class (84.42%) but struggles to differentiate between negative and neutral emotions, resulting in a high misclassification rate of31.45%. While the positive class attains a reasonable accuracy of 82.25%, the neutral class remains challenging, with an accuracy of only 58.25%. In contrast, Fig.  4 (b) demonstrates that our proposed method on the GAFF2 dataset significantly mitigates this confusion. While maintaining a strong recognition rate for the negative class (75.32%), it is notably enhances the neutral class accuracy, increasing it to 74.89%. Fig.  4 (c) highlights that the effectiveness of our method on GAFF3, achieving high accuracies for both negative (78.64%) and positive (93.64%) classes, demonstrating improved differentiation between these categories. Similarly, in Fig.  4 (d), our proposed method on the GroupEmoW dataset achieves 96.68% accuracy for the positive class, with minimal confusion between categories. Additionally, the neutral and negative classes attain accuracies of 86.03% and 88.63%, respectively, effectively reducing misclassification compared to the GAFF2 and GAFF3 datasets.\n\nOverall, these results demonstrate the robustness of our method across multiple datasets, consistently improving classification accuracy, particularly by reducing confusion between similar emotional states.\n\n2) The t-SNE visualization of learned representation. The t-SNE  [48]  visualizations in Fig.  5  compare the feature distribution learned by the baseline model (a) and our proposed method (b) for GER on the GAFF2 dataset. In the baseline model, the features representing the three emotion categories (positive, neutral, and negative) overlap significantly, indicating poor class separation and weak discriminative power. In contrast, our method demonstrates a clear and distinct separation between the feature clusters, especially enhancing the differentiation between neutral and positive emotions. This improved feature segregation highlights the effectiveness of our approach in capturing and encoding relevant emotional cues, ultimately boosting classification performance.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we propose a novel framework for grouplevel emotion recognition that effectively integrates multi-scale visual scene context and label-guided semantic information. Our approach introduces two key modules: the Visual Context Encoding Module (VCEM) and Emotion Semantic Encoding Module (ESEM), addressing the critical limitations of existing GER methods, which often underexploit contextual and semantic cues essential for accurate emotion inference. The VCEM enhances individual features by incorporating multi-scale global context, while the ESEM module refines semantic representations through label-guided emotion lexicons generated by a LLM. To further bridge the gap between visual and semantic representations, we propose the Visual-Semantic Interaction Module (VSIM), which aligns and fuses these features to produce a comprehensive group-level emotion representation. Extensive experiments on three public benchmarks demonstrate the effectiveness of our approach, yielding significant performance improvements over state-of-theart methods. For future work, we will explore extending our framework to more complex and diverse real-world scenarios and exploring advanced multi-modal feature fusion techniques to further enhance GER performance.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), which has",
      "page": 1
    },
    {
      "caption": "Figure 1: Examples corresponding to the three categories from the GER dataset.",
      "page": 1
    },
    {
      "caption": "Figure 2: (a) Causal graph of early models. (b) Causal graph of existing context-",
      "page": 1
    },
    {
      "caption": "Figure 2: (a). In this graph, I →C represents",
      "page": 1
    },
    {
      "caption": "Figure 2: (b). The primary distinctions are C →Y : gener-",
      "page": 1
    },
    {
      "caption": "Figure 1: , it is imperative to analyze individuals, their interactions,",
      "page": 2
    },
    {
      "caption": "Figure 2: (c). A key distinction of our method from",
      "page": 2
    },
    {
      "caption": "Figure 3: ) consists of three key mod-",
      "page": 3
    },
    {
      "caption": "Figure 3: Framework of our proposed method. The framework comprises three key modules: (1) a visual context encoding module, (2) an emotion semantic",
      "page": 4
    },
    {
      "caption": "Figure 4: Comparison of the confusion matrices on the GAFF2, GAFF3, and",
      "page": 9
    },
    {
      "caption": "Figure 5: The t-SNE visualization of learned representation by different models",
      "page": 9
    },
    {
      "caption": "Figure 4: (a), the baseline model on the GAFF2 dataset achieves",
      "page": 9
    },
    {
      "caption": "Figure 4: (b) demonstrates that our proposed method",
      "page": 9
    },
    {
      "caption": "Figure 4: (c) highlights that",
      "page": 9
    },
    {
      "caption": "Figure 4: (d), our proposed method on",
      "page": 9
    },
    {
      "caption": "Figure 5: compare the feature",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Positive": "Neutral"
        },
        {
          "Positive": "Negative"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition of playing musicians from eeg, ecg, and acoustic signals",
      "authors": [
        "L Turchet",
        "B O'sullivan",
        "R Ortner",
        "C Guger"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Hum. Mach. Syst"
    },
    {
      "citation_id": "2",
      "title": "Emotional video captioning with vision-based emotion interpretation network",
      "authors": [
        "P Song",
        "D Guo",
        "X Yang",
        "S Tang",
        "M Wang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "3",
      "title": "Exploring the role and potential of probiotics in the field of mental health: major depressive disorder",
      "authors": [
        "D Johnson",
        "S Thurairajasingam",
        "V Letchumanan",
        "K.-G Chan",
        "L.-H Lee"
      ],
      "year": "2021",
      "venue": "Nutrients"
    },
    {
      "citation_id": "4",
      "title": "MSA-GCN: multiscale adaptive graph convolution network for gait emotion recognition",
      "authors": [
        "Y Yin",
        "L Jing",
        "F Huang",
        "G Yang",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "5",
      "title": "Individual performance in women's grassroots football: A physical and emotional perspective",
      "authors": [
        "L Rodríguez",
        "R Fernández",
        "D Palacio"
      ],
      "year": "2025",
      "venue": "IEEE Trans. Hum. Mach. Syst"
    },
    {
      "citation_id": "6",
      "title": "Objective classbased micro-expression recognition under partial occlusion via regioninspired relation reasoning network",
      "authors": [
        "Q Mao",
        "L Zhou",
        "W Zheng",
        "X Shao",
        "X Huang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "Group affect",
      "authors": [
        "S Barsade",
        "A Knight"
      ],
      "year": "2015",
      "venue": "Annu. Rev. Organ. Psychol. Organ. Behav"
    },
    {
      "citation_id": "8",
      "title": "Group affect: Its influence on individual and group outcomes",
      "authors": [
        "S Barsade",
        "D Gibson"
      ],
      "year": "2012",
      "venue": "Curr. Dir. Psychol"
    },
    {
      "citation_id": "9",
      "title": "Revisiting crowd behaviour analysis through deep learning: Taxonomy, anomaly detection, crowd emotions, datasets, opportunities and prospects",
      "authors": [
        "F Sánchez",
        "I Hupont",
        "S Tabik",
        "F Herrera"
      ],
      "year": "2020",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "10",
      "title": "Automatic emotion recognition for groups: A review",
      "authors": [
        "E Veltmeijer",
        "C Gerritsen",
        "K Hindriks"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "11",
      "title": "Analyzing group-level emotion with global alignment kernel based approach",
      "authors": [
        "X Huang",
        "A Dhall",
        "R Goecke",
        "M Pietikäinen",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "12",
      "title": "Grouplevel emotion recognition using deep models with A four-stream hybrid network",
      "authors": [
        "A Khan",
        "Z Li",
        "J Cai",
        "Z Meng",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2018",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in the wild using deep neural networks and bayesian classifiers",
      "authors": [
        "L Surace",
        "M Patacchiola",
        "E Battini",
        "W Sönmez",
        "A Spataro",
        "Cangelosi"
      ],
      "year": "2017",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "14",
      "title": "Hierarchical group-level emotion recognition",
      "authors": [
        "K Fujii",
        "D Sugimura",
        "T Hamamoto"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multi"
    },
    {
      "citation_id": "15",
      "title": "Regional attention networks with context-aware fusion for group emotion recognition",
      "authors": [
        "A Khan",
        "Z Li",
        "J Cai",
        "Y Tong"
      ],
      "venue": "Proc. WACV, 2021"
    },
    {
      "citation_id": "16",
      "title": "Semi-supervised group emotion recognition based on contrastive learning",
      "authors": [
        "J Zhang",
        "X Wang",
        "D Zhang",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "17",
      "title": "A self-fusion network based on contrastive learning for group emotion recognition",
      "authors": [
        "X Wang",
        "D Zhang",
        "H Tan",
        "D Lee"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Comput. Soc. Syst"
    },
    {
      "citation_id": "18",
      "title": "Graph neural networks for image understanding based on multiple cues: Group emotion recognition and event recognition as use cases",
      "authors": [
        "X Guo",
        "L Polania",
        "B Zhu",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2020",
      "venue": "Proc. WACV"
    },
    {
      "citation_id": "19",
      "title": "Grouplevel emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions",
      "authors": [
        "X Guo",
        "B Zhu",
        "L Polanía",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2018",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "20",
      "title": "Nonvolume preserving-based fusion to group-level emotion recognition on crowd videos",
      "authors": [
        "K Quach",
        "N Le",
        "C Duong",
        "I Jalata",
        "K Roy",
        "K Luu"
      ],
      "year": "2022",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "21",
      "title": "Group emotion recognition based on global and local features",
      "authors": [
        "Y Dai",
        "X Liu",
        "D Shuzhan",
        "L Yang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "22",
      "title": "Most important person-guided dual-branch cross-patch attention for group affect recognition",
      "authors": [
        "H Xie",
        "M Lee",
        "T Chen",
        "H Chen",
        "H Liu",
        "H Shuai",
        "W Cheng"
      ],
      "year": "2023",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "23",
      "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
      "authors": [
        "Y Zhang",
        "M Chen",
        "J Shen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proc. AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Multimodal adaptive emotion transformer with flexible modality inputs on A novel dataset with continuous labels",
      "authors": [
        "W Jiang",
        "X Liu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2023",
      "venue": "Proc. ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Proc. AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Label-aware hyperbolic embeddings for fine-grained emotion classification",
      "authors": [
        "C Chen",
        "T Hung",
        "Y Hsu",
        "L Ku"
      ],
      "year": "2023",
      "venue": "Proc. Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Multi-label emotion detection via emotion-specified feature extraction and emotion correlation learning",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "28",
      "title": "Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "29",
      "title": "Emotional reaction analysis based on multi-label graph convolutional networks and dynamic facial expression recognition transformer",
      "authors": [
        "K Wang",
        "Z Lian",
        "L Sun",
        "B Liu",
        "J Tao",
        "Y Fan"
      ],
      "year": "2022",
      "venue": "Proc. International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "30",
      "title": "Emotionprior awareness network for emotional video captioning",
      "authors": [
        "P Song",
        "D Guo",
        "X Yang",
        "S Tang",
        "E Yang",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "31",
      "title": "Improving few-shot remote sensing scene classification with class name semantics",
      "authors": [
        "J Chen",
        "Y Guo",
        "J Zhu",
        "G Sun",
        "D Qin",
        "M Deng",
        "H Liu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Geosci. Remote. Sens"
    },
    {
      "citation_id": "32",
      "title": "Semanticformer: Hyperspectral image classification via semantic transformer",
      "authors": [
        "Y Liu",
        "X Wang",
        "B Jiang",
        "L Chen",
        "B Luo"
      ],
      "year": "2024",
      "venue": "Pattern Recognit. Lett"
    },
    {
      "citation_id": "33",
      "title": "Sgva-clip: Semanticguided visual adapting of vision-language models for few-shot image classification",
      "authors": [
        "F Peng",
        "X Yang",
        "L Xiao",
        "Y Wang",
        "C Xu"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "34",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Z Zhao",
        "I Patras"
      ],
      "year": "2023",
      "venue": "Proc. British Machine Vision Conference"
    },
    {
      "citation_id": "35",
      "title": "Multimodal-semantic context-aware graph neural network for group activity recognition",
      "authors": [
        "T Liu",
        "R Zhao",
        "K Lam"
      ],
      "year": "2021",
      "venue": "Proc. IEEE ICME"
    },
    {
      "citation_id": "36",
      "title": "Learning label semantics for weakly supervised group activity recognition",
      "authors": [
        "L Wu",
        "M Tian",
        "Y Xiang",
        "K Gu",
        "G Shi"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "37",
      "title": "Constraint embedding for prompt tuning in vision-language pre-trained model",
      "authors": [
        "K Cheng",
        "L Wei",
        "J Tang",
        "Y Zhan"
      ],
      "year": "2025",
      "venue": "Multim. Syst"
    },
    {
      "citation_id": "38",
      "title": "Augsumm: Towards generalizable speech summarization using synthetic labels from large language models",
      "authors": [
        "J Jung",
        "R Sharma",
        "W Chen",
        "B Raj",
        "S Watanabe"
      ],
      "year": "2024",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "39",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "40",
      "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "41",
      "title": "Feature pyramid networks for object detection",
      "authors": [
        "T Lin",
        "P Dollár",
        "R Girshick",
        "K He",
        "B Hariharan",
        "S Belongie"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "42",
      "title": "Mask R-CNN",
      "authors": [
        "K He",
        "G Gkioxari",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2017",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "43",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "44",
      "title": "Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval",
      "authors": [
        "D Jiang",
        "M Ye"
      ],
      "year": "2023",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "45",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Ghosh",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2017",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "46",
      "title": "Emotiw 2018: Audiovideo, student engagement and group-level affect prediction",
      "authors": [
        "A Dhall",
        "A Kaur",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2018",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "47",
      "title": "Hierarchical group-level emotion recognition in the wild",
      "authors": [
        "K Fujii",
        "D Sugimura",
        "T Hamamoto"
      ],
      "year": "2019",
      "venue": "Proc. IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "48",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}