{
  "paper_id": "2503.01750v1",
  "title": "Ecg-Emotionnet: Nested Mixture Of Expert (Nmoe) Adaptation Of Ecg-Foundation Model For Driver Emotion Recognition",
  "published": "2025-03-03T17:19:45Z",
  "authors": [
    "Nastaran Mansourian",
    "Arash Mohammadi",
    "M. Omair Ahmad",
    "M. N. S. Swamy"
  ],
  "keywords": [
    "Autonomous Driving",
    "ECG Signals",
    "Emotion Recognition",
    "Foundation Models",
    "Mixture of Experts"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Driver emotion recognition plays a crucial role in driver monitoring systems, enhancing human-autonomy interactions and the trustworthiness of Autonomous Driving (AD). Various physiological and behavioural modalities have been explored for this purpose, with Electrocardiogram (ECG) emerging as a standout choice for real-time emotion monitoring, particularly in dynamic and unpredictable driving conditions. Existing methods, however, often rely on multi-channel ECG signals recorded under static conditions, limiting their applicability in real-world dynamic driving scenarios. To address this limitation, the paper introduces ECG-EmotionNet, a novel architecture designed specifically for emotion recognition in dynamic driving environments. ECG-EmotionNet is constructed by adapting a recently introduced ECG Foundation Model (FM) and uniquely employs single-channel ECG signals, ensuring both robust generalizability and computational efficiency. Unlike conventional adaptation methods such as full finetuning, linear probing, or low-rank adaptation, we propose an intuitively pleasing alternative, referred to as the nested Mixture of Experts (MoE) adaptation. More precisely, each transformer layer of the underlying FM is treated as a separate expert, with embeddings extracted from these experts fused using trainable weights within a gating mechanism. This approach enhances the representation of both global and local ECG features, leading to a 6% improvement in accuracy and a 7% increase in the F1 score, all while maintaining computational efficiency. The effectiveness of the proposed ECG-EmotionNet architecture is evaluated using a recently introduced and challenging driver emotion monitoring dataset. The proposed architecture outperforms its counterparts, achieving an average classification accuracy of 82.45% and an F1 score of 77.11% across five emotional states: anger, fear, neutral, sadness, and surprise.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recently, presence of partially/semi-Autonomous Vehicles (AVs) on the roads has increased considerably. Equipped with unparalleled capabilities in perceiving their surroundings, AVs aim to provide safer and more efficient Autonomous Driving (AD). Despite recent advancements in AD, however, building trust in the AI-driven decisionmaking processes of AVs remains a significant barrier to their widespread adoption. Recognizing driver emotion is one crucial factor to improve trust in AD, as emotional states such as stress, anger, or fatigue can impair decisionmaking and reaction times, significantly increasing the risk of accidents  [1] . By monitoring driver emotions in real-time, Advanced Driver-Assistance Systems (ADAS) can detect hazardous states and provide interventions such as calming alerts or break suggestions  [2] . Emotion-aware systems also improve Human-Autonomy Teaming (HAT) interactions in fully/semi AVs, ensuring smoother transitions and personalized driving experiences  [3] . While several studies  [4] -  [6]  have explored this domain, the application of recent advancements in Foundational Modelling (FM)  [7]  for emotion recognition in AD remains in its infancy, particularly in terms of accuracy, efficiency, and generalizability. This paper addresses this gap by proposing a novel approach that achieves comparable accuracy with significantly reduced complexity and improved generalizability to unseen data through the use of transfer learning.\n\nLiterature Review: Generally speaking, emotion recognition methods in dynamic driving scenario can be classified based on the input signals into physiological and non-physiological categories. Non-physiological signals, such as facial expressions, often face challenges due to individual differences, lighting conditions, and camera angles, leading to potential inaccuracies  [8] . In contrast, physiological signals are involuntarily produced by the nervous and endocrine systems  [9] , making them less susceptible to external factors and providing a more accurate reflection of emotional states  [10] . Various physiological signals, including Eelectroencephalograms (EEGs)  [11] , Electrodermal activity (EDA)  [12] , Electrocardiograms (ECGs)  [13] , and Electromyography (EMG)  [14] , are commonly utilized to assess human psychological states across diverse contexts, such as driving. Among these, ECGs are particularly advantageous for emotion recognition in dynamic environments due to their robustness against motion artifacts, continuous non-invasive monitoring capabilities, and the computational efficiency afforded by single-channel ECG usage  [13] .\n\nRecently, there has been a surge of interest in using Machine Learning (ML) and Deep Learning (DL) techniques for ECG-based emotion recognition  [15] ,  [16] . Traditional ML approaches rely on manually extracting ECG features, such as Interbeat Interval (IBI), Heart Rate Variability (HRV), and Power Spectral Densities (PSD), which requires domain expertise and is time-intensive  [17] ,  [18] . In contrast, DL enables end-to-end emotion recognition from raw ECG signals, removing the need for manual feature engineering  [16] . DL approaches, such as Temporal Convolutional Neural Networks (TCNNs)  [19] , have demonstrated high accuracy in classifying arousal and valence levels from ECG signals. However, several challenges persist. On the one hand, to the arXiv:2503.01750v1 [cs.LG] 3 Mar 2025 best of our knowledge, most existing ECG-based emotion recognition methods  [19] -  [21]  are designed for static environments, where no secondary tasks are involved. This makes such methods impractical for real-world dynamic driving scenarios. On the other hand is the significant limitation of reliance on large, labelled datasets for training. Acquiring large datasets is, typically, infeasible and time-consuming especially in the domain of AVs. Additionally, models trained in a fully supervised fashion may develop representations that are overly specific to the training data, resulting in limited generalizability to new, unseen data. To address these issues, self-supervised learning frameworks have been proposed  [22]  in other domains, enabling models to learn robust ECG representations without the need for extensive labelled data. However, the self-supervised learning process often involves multiple sequential training steps, making it both time-intensive and computationally demanding.\n\nContributions: Foundation Models (FM) have revolutionized Natural Language Processing (NLP)  [23] , computer vision  [24] , and speech recognition  [25] , demonstrating the effectiveness of pretraining on massive datasets. Models such as GPT  [23]  and CLIP  [24]  enable fine-tuning for diverse tasks with high accuracy and efficiency. Despite their success in other domains, however, foundation modeling for physiological signal analysis, particularly ECGbased emotion recognition in driving monitoring, remains largely unexplored  [26] . To bridge this gap, we leverage a recently introduced ECG-FM  [27] , a transformer-based FM pre-trained on 2.5 million ECG samples, for driver emotion recognition, and introduce the ECG-EmotionNet architecture.\n\nTo preserve the pre-trained model's generalization capability while enhancing computational efficiency, ECG-FM is adapted via an intuitively pleasing approach, referred to as the nested Mixture of Experts (NMoE) adaptation. More specifically, we keep the FM's parameters frozen, but instead of using only the final transformer's embedding as input to a linear layer, we treat each transformer layer of the underlying ECG-FM as an independent expert. In summary, the paper makes the following two main contributions: The remainder of the paper is organized as follows: Section II provides the required material and methods. The proposed ECG-EmotionNet framework is introduced in Section III. Experimental results and comparisons are presented in Section IV. Finally, Section V concludes the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Materials And Methods",
      "text": "This section provides an overview of the backbone ECG-FM, the dataset used for training and evaluation, and preprocessing and data augmentation steps.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Ecg-Fm Architecture",
      "text": "The ECG-FM model  [27]  is a self-supervised, transformerbased foundation model designed for ECG signal analysis. It includes a feature extractor with four convolutional blocks that converts raw ECG signals into latent representations, incorporating relative positional embeddings for temporal awareness. In addition, it includes a transformer encoder, inspired by BERT-Large  [28] , which processes the extracted representations through self-attention mechanisms within a high-dimensional embedding space.\n\nPretraining incorporates multiple objectives, including the masking objective from wav2vec 2.0  [25] , the Contrastive Multi-segment Coding (CMSC) objective from Contrastive Learning of Cardiac Signals (CLOCS)  [29] , and Random Lead Masking (RLM)  [30] , ensuring robust feature learning. Trained on 2.5 million ECG samples, ECG-FM effectively captures both local and global patterns, making it well-suited for downstream tasks such as emotion recognition in realworld settings, including driving scenarios explored in this study. Please refer to  [27]  for further details on the ECG-FM model's architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset",
      "text": "This study utilizes the manD 1.0 dataset  [31] , a recently released multimodal benchmark dataset for driver monitoring in autonomous driving. The manD 1.0 dataset includes synchronized physiological signals (ECG, EEG, EDA), vehicle dynamics, driver activities, and environmental factors from 50 participants (balanced by gender, and aged between 21-65) in a controlled setting. Participants drove through five scenarios designed to elicit neutral, anger, fear, sadness, and surprise, simulating real-world driving conditions.\n\nFor this study, we focused on ECG data to classify emotional states. A visual inspection was conducted to ensure signal quality, leading to the exclusion of specific low-quality signals, i.e., the 4th ECG from anger, the 17th and 23rd from fear, the 7th and 16th from neutral, the 29th from sadness, and the 14th, 22nd, and 30th from surprise.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Pre-Processing And Data Augmentation",
      "text": "A structured pre-processing pipeline was applied to ensure high-quality ECG data for emotion recognition. First, a  second-order Butterworth high-pass filter with a 0.8 Hz cutoff was used to remove baseline wander and low-frequency noise while preserving essential signal components. Subsequently, the signals were standardized using z-score normalization to minimize variability across recordings. Next, 10-second windows sampled at 256 Hz were extracted to preserve both temporal dynamics and spatial relationships crucial for emotion recognition.\n\nData augmentation is used to enhance the training dataset. For augmentation, one can rely on overlapping (sliding window) or Generative Adversarial Networks (GAN). Given the size of the available dataset, we applied the overlappingwindow technique, generating additional samples by leveraging repeated patterns within each trial  [32] . This approach expanded the training set by creating multiple, slightly shifted representations of the same data. Consequently, this method augments the dataset by progressively incorporating new temporal segments while partially discarding previous ones. As illustrated in Fig.  2 , the overlapping window technique segments the ECG signal of length L into smaller overlapping windows W i of fixed size N . The stride between consecutive windows determines the overlap percentage, ensuring that each segment captures both unique and shared patterns from the signal through positional changes in the previous signal. Such an approach increases the diversity of the training dataset, providing n = L-N stride + 1 augmented samples while preserving temporal and spatial features crit-ical for emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. The Ecg-Emotionnet",
      "text": "In this section, we present details of the proposed ECG-EmotionNet architecture to adopt the ECG-FM for the task of driver emotion recognition. Reference  [27] , which introduced the ECG-FM, introduced the following two approaches for its adaptation to downstream tasks: (i) Full Fine-Tuning, where the ECG-FM is initialized with the pretrained weights and then all model weights are updated using the dataset associated with the downstream task, and; (ii) Linear Probing, where the ECG-FM pretrained weights are frozen, instead the extracted embeddings from the last transformer layer of the pretrained model are provided as inputs to a single linear layer to generate predictions.\n\nWe propose to use an alternative approach, we refer to as the Nested Mixture of Experts (NMoE) adaptation inspired by  [33] . Intuitively speaking, the idea is to preserve the pretrained model's generalization capability while optimizing its computational efficiency. For this purpose, we retain the FM's parameters frozen (similar to the aforementioned linear probing mechanism), however, instead of feeding only the last transformer's embedding to a linear layer, we treat each of the underlying transformer layers of the ECG-FM as a separate expert. Extracted embedding from these experts are then fused (mixed) using trainable weights. In other words, by introduction of such a multi-model fusion architecture, hidden layer embeddings from all transformers' outputs are leveraged to capture richer contextual representations.\n\nIntuitively speaking, NMoE offers several advantages over its traditional counterparts. On the one hand is its enhanced feature refinement through hierarchical representation learning. More specifically, each expert in the nested structure receives the transformed output from the previous expert. This allows for a progressive abstraction of features, where early experts focus on low-level patterns, while later experts extract higher-level representations, leading to a deeper contextual understanding of the data. Furthermore, NMoE reduces redundancy and improves parameter efficiency. In conventional MoE setups, experts may redundantly learn overlapping features since they all process the same raw input. By contrast, the nested structure forces specialization among experts, ensuring that each expert contributes uniquely to feature extraction. This diversification in learning improves the expressiveness of representations while maintaining computational efficiency.\n\nWe constructed a Nested MoE settings, i.e., the extracted feature vector x is provided as input to the first expert. The output embedding of the first expert h i (x) is used both as an output and as input to the second expert. This continues in a sequential fashion, where the output of each expert is both an output embedding and the input to the next expert. More specifically,\n\n• Let xß be the input feature vector to the i th expert (transformer layer), for (1\n\nextracted from the i th transformer layer. Here, d represents the embedding dimension, and L denotes the total number of Transformer layers. • Let G(x) = (α 1 (x 1 ), α 2 (x 2 ), . . . , α L (x L ) be the gating function, where each αß(xß) represents the weight assigned to expert i based on its input. The final aggregated embedding h agg (x) is computed as\n\nwhere α i (x i ) represents the trainable gating function output for expert i to determine its contribution to the final representation, ensuring that L i=1 α i (x i ) = 1. The gating function for the i th expert is computed as a softmax layer over a learned function W h given by\n\nwhere W g ∈ R L×d is the trainable gating weight matrix. The transformer-based contextualized encoder extracts global temporal patterns across the entire input sequence due to its global receptive field, whereas the local encoder, constrained by a limited receptive field, focuses on detailed localized features. By integrating embeddings from multiple transformer layers, our method balances local and global feature extraction, enhancing emotion recognition from ECG signals while minimizing overfitting and improving efficiency. By freezing the pretrained model parameters and employing a trainable weighted averaging strategy, we optimize feature selection for emotion recognition while preserving the pretrained model's knowledge. This domainadaptive approach effectively refines both global and local features without altering the encoder's expressive capacity. Furthermore, this method significantly reduces computational complexity, leading to faster convergence and more efficient training.\n\nTo summarize, Fig.  1  illustrates the proposed ECG-EmotionNet architecture, which consists of a pretrained transformer backbone followed by a trainable weighted averaging mechanism that aggregates hidden outputs from all transformer layers. The resulting weighted embeddings undergo average pooling, reducing the temporal dimension to generate a fixed-size feature vector. This vector is then processed through a dense layer (128 units, ReLU activation), batch normalization, and dropout (0.3 probability) to improve generalization and mitigate overfitting. Finally, a fully connected layer maps the refined features to five emotion classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Result And Discussion",
      "text": "In this section, we evaluate performance of the proposed ECG-EmotionNet architecture through a comprehensive set of experiments. For evaluation purposes, the dataset described in Section II-A was divided into 80% for training and 20% for testing, with 5-fold cross-validation. Models were trained for 10 epochs using a batch size of 32, the CrossEntropy objective function, and the Adam optimizer with a learning rate of 10 -3 . A. Full vs. Partial vs. NMoE-based Fine-Tuning Table  I  summarizes the results of the proposed algorithm for emotion recognition using single-channel ECG under different fine-tuning strategies and overlapping percentages. For the augmented dataset (75% overlapping), both CNN fine-tuning and the NMoE-based model achieved superior performance while requiring significantly fewer trainable parameters compared to full or encoder fine-tuning. However, as the overlapping percentage decreases and data availability becomes more constrained, the NMoE-based model consistently outperforms CNN fine-tuning and other strategies. Notably, the performance gap is significantly larger in low-data scenarios than in augmented settings, further highlighting the NMoE model's effectiveness in dynamic environments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Robustness And Efficiency Evaluation",
      "text": "To evaluate robustness, which is critical factor in driving scenarios, Gaussian noise with varying SNR levels was added to the data. All four fine-tuning strategies were tested under these conditions. As shown in Fig.  3 , the NMoE-based approach consistently outperforms other strategies in terms of accuracy and F1 score, even in noisy environments.\n\nTo evaluate model efficiency, we compared the number of trainable parameters. Full fine-tuning, encoder fine-tuning, and CNN fine-tuning involve approximately 312 million, 302 million, and 1.6 million parameters, respectively. In contrast, ECG-EmotionNet architecture trains fewer than 200, 000 parameters per epoch, significantly improving computational efficiency without compromising performance. In summary, the advantages of NMoE-based model over fine-tuning it are: (i) Superior Performance: It consistently outperforms other fine-tuning approaches, especially in limited-data scenarios; (ii) Reduced Parameters: It requires significantly fewer trainable parameters compared to other strategies, and; (ii)   Robustness: It performs better in the presence of noise, making it suitable for real-world driving scenarios.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Nmoe Vs. Final Transformer Layer",
      "text": "We evaluated model performance using the final transformer layer outputs against the NMoE that uses embeddings from all transformer layers. As shown in Fig.  4 , leveraging all transformer embeddings consistently improved the accuracy across all fine-tuning strategies. Analyzing the learned weights of α i (xß) (Fig.  5 ) revealed that middle layers (e.g., layers 6 -8) contribute most significantly to ECG-based emotion recognition, while the local encoder (layer 0) and deeper layers (layers 10 -12) are less impactful. This underscores the middle layers' role in balancing local signal features and global contextual information.  Methodology Accuracy F1 score TCNN  [19]  41.81% 29.15% Self-Supervised Learning  [22]  58.21% 33.64% ECG-EmotionNet (ours) 82.45% 77.20%",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Comparison With Existing Methods",
      "text": "To compare the proposed ECG-EmotionNet architecture with state-of-the-art, we have implemented the TCNN  [19]  and the self-supervised learning framework of Reference  [22]  on our dataset for driver emotion recognition. These are proposed for ECG-based emotion recognition in static environments. Although these methods perform well in static environments, as shown in Table  II , our proposed architecture significantly outperforms them in scenarios involving secondary tasks such as driving.\n\nMoreover, the proposed model's use of single-channel ECG for five-class emotion classification offers significant computational efficiency. This is while comparable performance is achieved compared to existing methods proposed for driver emotion recognition relying on alternative modalities, such as multi-channel EEG, or facial expressions. For example, Chen et al. achieved 75.26% accuracy in a threeclass task using 32-channel EEG signals, with a maximum F1 score of 76%  [4] . Similarly, Gursesli et al. utilized imagebased facial expressions from multiple datasets, including FER-2013, RAF-DB, and AffectNet, achieving an average accuracy of 67% across seven emotion classes  [5] . Additionally, Xiang et al.  [6]  explored multi-modal data fusion for driver emotion recognition Their findings showed that Blood Volume Pulse (BVP) signals alone achieved 77.01% accuracy and an F1-score of 76.43%, surpassing facial video, which achieved 72.56% accuracy and a 71.76% F1-score.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion And Future Works",
      "text": "In this paper, we introduced ECG-EmotionNet, a novel deep learning framework tailored for driver emotion recognition using single-channel ECG signals. Unlike conventional approaches that rely on multi-channel ECG data recorded in static environments, our proposed model adapts a pretrainedfondation model to dynamic driving conditions, ensuring improved robustness, generalizability and computational efficiency. Through the NMoE adaptation, we effectively utilize all transformer layers as independent experts, enhancing both global and local feature representations. The experimental results demonstrated that ECG-EmotionNet achieves an average classification accuracy of 82.45% and an F1 score of 77.11%, outperforming state-of-the-art approaches while maintaining a significantly lower computational cost. These findings suggest that ECG-EmotionNet can serve as a practical solution for ADAS and AD applications, contributing to enhanced driver monitoring and human-autonomy interaction. Despite its promising results, ECG-EmotionNet presents several opportunities for future improvements. Since variations in ECG signals across individuals and driving conditions may impact recognition performance, real-time evaluations can provide deeper insights into its adaptability. Moreover, while ECG-EmotionNet effectively captures physiological signals, integrating multimodal emotion recognition, combining ECG with EDA, EEG, and/or facial expressions, can lead to an improved emotion detection model.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A graphical representation of the proposed ECG-EmotionNet methodology for emotion recognition in dynamic driving scenarios. (a) The method",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the overlapping window technique for data augmen-",
      "page": 3
    },
    {
      "caption": "Figure 2: , the overlapping window",
      "page": 3
    },
    {
      "caption": "Figure 1: illustrates",
      "page": 4
    },
    {
      "caption": "Figure 3: , the NMoE-based",
      "page": 4
    },
    {
      "caption": "Figure 3: Comparison of Accuracy: (a) and F1-Score. (b) under different",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparison of Accuracy and F1 Score for Models Using All",
      "page": 5
    },
    {
      "caption": "Figure 4: , leveraging",
      "page": 5
    },
    {
      "caption": "Figure 5: ) revealed that middle layers (e.g.,",
      "page": 5
    },
    {
      "caption": "Figure 5: Final learned weights of αi of the encoder layers after training,",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "2",
      "title": "Leveraging Context-Aware Emotion and Fatigue Recognition Through Large Language Models for Enhanced Advanced Driver Assistance Systems (ADAS)",
      "authors": [
        "V Tavakkoli",
        "K Mohsenzadegan",
        "K Kyamakya"
      ],
      "year": "2024",
      "venue": "Recent Advances in Machine Learning Techniques and Sensor Applications for Human Emotion, Activity Recognition and Support"
    },
    {
      "citation_id": "3",
      "title": "Review and Perspectives on Human Emotion for Connected Automated Vehicles",
      "authors": [
        "W Li",
        "G Li",
        "R Tan",
        "C Wang",
        "Z Sun",
        "Y Li",
        "G Guo",
        "D Cao",
        "K Li"
      ],
      "year": "2024",
      "venue": "Automotive Innovation"
    },
    {
      "citation_id": "4",
      "title": "EEG-based emotion recognition for road accidents in a simulated driving environment",
      "authors": [
        "J Chen",
        "X Lin",
        "W Ma",
        "Y Wang",
        "W Tang"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "5",
      "title": "Facial emotion recognition (FER) through custom lightweight CNN model: performance evaluation in public datasets",
      "authors": [
        "M Gursesli"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "A multi-modal driver emotion dataset and study: Including facial expressions and synchronized physiological signals",
      "authors": [
        "G Xiang"
      ],
      "year": "2024",
      "venue": "Eng. Applications of Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "A survey for foundation models in autonomous driving",
      "authors": [
        "H Gao",
        "Z Wang",
        "Y Li",
        "K Long",
        "M Yang",
        "Y Shen"
      ],
      "year": "2024",
      "venue": "A survey for foundation models in autonomous driving",
      "arxiv": "arXiv:2402.01105"
    },
    {
      "citation_id": "8",
      "title": "Advances in Facial Expression Recognition: A Survey of Methods, Benchmarks, Models, and Datasets",
      "authors": [
        "T Kopalidis",
        "V Solachidis",
        "N Vretos",
        "P Daras"
      ],
      "year": "2024",
      "venue": "Advances in Facial Expression Recognition: A Survey of Methods, Benchmarks, Models, and Datasets"
    },
    {
      "citation_id": "9",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "11",
      "title": "EEG-based emotion recognition for road accidents in a simulated driving environment",
      "authors": [
        "J Chen",
        "X Lin",
        "W Ma",
        "Y Wang",
        "W Tang"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "12",
      "title": "Non-Linear Signal Processing Methods for Automatic Emotion Recognition using Electrodermal Activity",
      "authors": [
        "Y Veeranki"
      ],
      "year": "2024",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "13",
      "title": "Horizons in single-lead ECG analysis from devices to data",
      "authors": [
        "A Abdou",
        "S Krishnan"
      ],
      "year": "2022",
      "venue": "Frontiers in Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Evaluating the effectiveness of machine learning in identifying the optimal facial electromyography location for emotion detection",
      "authors": [
        "V Barigala"
      ],
      "year": "2025",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "15",
      "title": "A new emotion detection algorithm using extracted features of the different time-series generated from ST intervals poincaré map",
      "authors": [
        "M Baghizadeh",
        "K Maghooli",
        "F Farokhi",
        "N Dabanloo"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "16",
      "title": "A new data augmentation convolutional neural network for human emotion recognition based on ECG signals",
      "authors": [
        "S Nita",
        "S Bitam",
        "M Heidet",
        "A Mellouk"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "17",
      "title": "DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "18",
      "title": "A novel two-level interactive action recognition model based on inertial data fusion",
      "authors": [
        "S Qiu"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "19",
      "title": "Ecg-based automated emotion recognition using temporal convolution neural networks",
      "authors": [
        "T Sweeney-Fanelli",
        "M Imtiaz"
      ],
      "year": "2024",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "20",
      "title": "Graph enhanced low-resource ECG representation learning for emotion recognition based on wearable internet of things",
      "authors": [
        "J Chen"
      ],
      "year": "2024",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "21",
      "title": "A new deep convolutional neural network incorporating attentional mechanisms for ECG emotion recognition",
      "authors": [
        "T Fan",
        "S Qiu",
        "Z Wang",
        "H Zhao",
        "J Jiang",
        "Y Wang",
        "J Xu",
        "T Sun",
        "N Jiang"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "22",
      "title": "Self-supervised ECG representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown"
      ],
      "year": "2020",
      "venue": "Advances in neural inf. processing systems"
    },
    {
      "citation_id": "24",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "25",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Applications of self-supervised learning to biomedical signals: A survey",
      "authors": [
        "F Del Pup",
        "M Atzori"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Ecg-fm: An open electrocardiogram foundation model",
      "authors": [
        "K Mckeen",
        "L Oliva",
        "S Masood",
        "A Toma",
        "B Rubin",
        "B Wang"
      ],
      "year": "2024",
      "venue": "Ecg-fm: An open electrocardiogram foundation model",
      "arxiv": "arXiv:2408.05178"
    },
    {
      "citation_id": "28",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "29",
      "title": "Clocs: Contrastive learning of cardiac signals across space, time, and patients",
      "authors": [
        "D Kiyasseh",
        "T Zhu",
        "D Clifton"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Lead-agnostic self-supervised learning for local and global representations of electrocardiogram",
      "authors": [
        "J Oh"
      ],
      "year": "2022",
      "venue": "Conference on Health, Inference, and Learning"
    },
    {
      "citation_id": "31",
      "title": "A multimodal driver monitoring benchmark dataset for driver modeling in assisted driving automation",
      "authors": [
        "K Nobari",
        "T Bertram"
      ],
      "year": "2024",
      "venue": "Scientific data"
    },
    {
      "citation_id": "32",
      "title": "Efficient classification of motor imagery electroencephalography signals using deep learning methods",
      "authors": [
        "I Majidov",
        "T Whangbo"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    }
  ]
}