{
  "paper_id": "2409.10545v2",
  "title": "Resemotenet: Bridging Accuracy And Loss Reduction In Facial Emotion Recognition",
  "published": "2024-09-01T16:50:24Z",
  "authors": [
    "Arnab Kumar Roy",
    "Hemant Kumar Kathania",
    "Adhitiya Sharma",
    "Abhishek Dey",
    "Md. Sarfaraj Alam Ansari"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Convolutional Neural Network",
    "Squeeze and Excitation Network",
    "Residual Network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The human face is a silent communicator, expressing emotions and thoughts through it's facial expressions. With the advancements in computer vision in recent years, facial emotion recognition technology has made significant strides, enabling machines to decode the intricacies of facial cues. In this work, we propose ResEmoteNet, a novel deep learning architecture for facial emotion recognition designed with the combination of Convolutional, Squeeze-Excitation (SE) and Residual Networks. The inclusion of SE block selectively focuses on the important features of the human face, enhances the feature representation and suppresses the less relevant ones. This helps in reducing the loss and enhancing the overall model performance. We also integrate the SE block with three residual blocks that help in learning more complex representation of the data through deeper layers. We evaluated ResEmoteNet on four open-source databases: FER2013, RAF-DB, AffectNet-7 and ExpW, achieving accuracies of 79.79%, 94.76%, 72.39% and 75.67% respectively. The proposed network outperforms state-of-the-art models across all four databases. The source code for ResEmoteNet is available at https://github.com/ArnabKumarRoy02/ResEmoteNet.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial Emotion Recognition (FER) is a specialized task within Image Recognition, focusing on identifying emotions from facial images or videos. Facial emotions change with subtle movements of facial features such as lips, teeth, skin, hair, cheekbones, nose, face shape, eyebrows, eyes, jawline, and mouth, making it difficult to design models that can accurately capture these intricate details. Additionally, data collection for FER is a labor-intensive process that requires significant funding and careful annotation by humans. Despite its challenges, FER is a crucial task in image recognition. Facial emotions provide valuable insights into a person's mental health, helping to identify signs of depression, anxiety, and other psychiatric disorders  [1] . Consequently, FER plays a significant role in mental health and therapy. It can also be instrumental in creating responsive and adaptive systems for human-computer interaction. In educational settings, FER can help teachers understand the emotions of their students in a classroom, allowing them to use this feedback to enhance the learning experience  [2] .\n\nArnab Kumar Roy is with Sikkim Manipal Institute of Technology (SMIT), Sikkim, India -737136 (e-mail: arnab 202000152@smit.smu.edu.in), Hemant Kumar Kathania, Adhitiya Sharma, and Md. Sarfaraj Alam Ansari are with National Institute of Technology Sikkim, India -737139 (e-mail: hemant.ece@nitsikkim.ac.in, b180078@nitsikkim.ac.in, sarfaraj@nitsikkim.ac.in), Abhishek Dey is with Bay Area Advanced Analytics India (P) Ltd, A Kaliber.AI company, Guwahati, India -781039 (e-mail: abhishek@kaliberlabs.com)\n\nIn recent years, FER has been dominated by deep learning systems, primarily leveraging deep Convolutional Networks such as ResNets  [3]  and AlexNets  [4] . Vision Transformers  [5] , widely used in Natural Language Processing (NLP), have also been applied to FER, significantly enhancing performance. In  [6] , MobileFaceNet  [7]  was employed as a backbone for feature extraction from images, and a Dual Direction Attention Network (DDAN) was proposed. DDAN generates attention maps from both vertical and horizontal orientations, guiding the model to focus on specific facial features and providing detailed feature representation. An attention loss mechanism was introduced to ensure that different attention heads concentrate on distinct features.\n\nIn  [8] , the Local Multi-Headed Channel (LHC) module was introduced to add channel-wise self-attention to existing CNN architectures. Pecoraro et al. incorporated LHC into ResNet34, creating LHC-Net for facial emotion classification. A similar approach is found in  [9] , where ResNet is combined with spatial and channel attention mechanisms and two loss functions. In  [10] , a two-stream feature extraction method using an image backbone and facial landmark detector was proposed, utilizing cross-attention for classification. This method is more computationally efficient than  [11] .\n\nIn  [12] , Vision Transformers (ViTs) with Multi-View Complementary Prompters (MCPs) were proposed for FER, where ViT extracts facial features and MCP enhances performance by combining landmark features. An ensemble approach is introduced in  [13] , using the ResMaskingNetwork with other networks. In  [14] , a multi-task learning approach integrates the EmoAffectNet and EffNet-B2 models, using a coupling loss to improve facial feature learning across datasets. In  [15] , an attention mechanism with self-supervised learning, diffusionbased denoising, and restoration improves performance in noisy environments like under-display cameras (UDC). Group-GAN, proposed in  [16] , includes an extractor, generator, and two discriminators to handle weak emotions by converting them to peak emotions, improving performance. Finally,  [17]  introduces HTNet for micro-expression recognition, leveraging local temporal, local, and global semantic features, demonstrating enhanced results across four publicly available datasets.\n\nIn this paper, we propose ResEmoteNet, a novel neural network architecture for facial emotion recognition. ResE-moteNet integrates Convolutional Neural Networks, Residual connections, and the Squeeze and Excitation network  [18]  to effectively capture facial emotions. We evaluated the proposed network using four open-source databases: FER2013  [19] , RAF-DB  [20] , AffectNet-7  [21]  and ExpW (Expressions in",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Convolutional Network",
      "text": "Our architecture includes a CNN module with three convolutional layers for hierarchical feature extraction, followed by batch normalization to stabilize learning and enhance training efficiency. Max-pooling is applied after each layer to reduce spatial dimensions, lowering computational costs and introducing translational invariance for improved robustness. These layers form the foundation of our feature extraction process. Mathematically, the Convolutional Network's (CNet) feature extraction process is expressed as follows:\n\nwhere X is the raw image sample and X F E is the output of the feature map from the CNet.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Squeeze And Excitation Network",
      "text": "Squeeze and Excitation Network (SENet) is incorporated into our methodology to boost the representational power of convolutional neural networks. At the heart of SENet lies the SE block, a key component that models the relationships between convolutional channels. It performs two primary functions: Squeeze, which uses global average pooling to condense spatial data from each channel into a global descriptor, and Excitation, which employs a sigmoid-activated gating mechanism to capture channel dependencies. SENet's approach allows the network to learn a series of attention weights, highlighting the importance of each input element for the network's output. The architecture of the Squeeze and Excitation block is shown in Fig.  1 (a) .\n\nLet X F E be the input for the squeeze operation which can be expressed as:\n\nHere, H and W are the height and widths of the feature maps, respectively, and X F E (c, i, j) denotes the activation at position (i, j) in channel c.\n\nThe squeezed output z is then processed through two fully connected layers: a dimensionality-reduction layer followed by a dimensionality-expansion layer, with a Rectified Linear Unit (ReLU) activation in between. The excitation operation can be formulated as:\n\nHere, W 1 is the weight matrix that reduces the dimensionality. X S1 is the output from first dimension-reduction layer. W 2 is the weight matrix that expands it back to the original number of channels i.e., c.X S2 is the output from first dimensionexpansion layer. The per-channel modulation weights, derived from the excitation phase output s, are employed to adjust the original input feature maps X F E . This scaling operation is done on each element individually and is represented by Y:\n\nResidual Networks (ResNets) are a significant innovation in deep learning, particularly in fields that involve training extremely deep neural networks. He et al.  [3]  introduced ResNets, which efficiently tackle the common issues of vanishing and exploding gradients in neural networks. ResNets' main innovation is the addition of the residual block, which includes a shortcut connection to skip one or more layers. Mathematically, the function in a residual block can be defined as:\n\nIn a residual block, x is the input, H(x) is the output from stacked layers, and F (x) is the final output. Adding x to",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Adaptive Average Pooling",
      "text": "Adaptive Average Pooling (AAP) is a technique that was first introduced in 2018  [24] . AAP is a type of pooling layer used in CNNs that enables the aggregation of input information into a constant output size, regardless of the original input dimensions. AAP adjusts kernel size and stride to reach a specific output size, instead of reducing spatial dimensions like traditional pooling methods. It ensures consistent output dimensions in various datasets and layers.\n\nConsidering X RB as the output feature map from the residual block and XF E be the output from the AAP operation, this operation can be expressed as:\n\nXF E is finally fed to the classifier that outputs a probability distribution over the possible facial expressions.\n\nP ∈ R N , where N is the number of facial emotion classes. Classif ier is the Linear layer that helps to classify image based on output of the network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Dataset And Experimental Details A. Dataset",
      "text": "In this sub-section, we provide a brief overview of the datasets used in this study. We conducted our experimental studies on four popular Facial Emotion Recognition datasets namely FER2013  [19] , RAF-DB  [20] , AffectNet-7  [21]  and ExpW (Expressions in the Wild)  [22] . A comparison of the four datasets is presented in Table  I , detailing their number of channels, image sizes, number of samples and number of classes. Our facial emotion recognition task involves identifying seven fundamental emotions: Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise. Visual examples of each emotion are displayed in Fig.  2 . To ensure a comprehensive analysis, we have also included a class-wise breakdown of the train-test distribution for each dataset in Table  II .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Details",
      "text": "In this study, we employed a consistent experimental setup across four facial emotion datasets: FER2013, RAF-DB, Af-fectNet and ExpW. ResEmoteNet was trained on each dataset using carefully selected hyperparameters, based on sensitivity analysis experiments. We evaluated batch sizes of 16, 32, 64, and 128, and found that a batch size of 16 consistently yielded superior performance, likely because more frequent weight updates facilitated faster convergence during training. Similarly, we experimented with epochs ranging from 40 to 80, with 80 epochs being the optimal choice, as the model's performance saturated around this point, and further increases yielded minimal gains in accuracy. Thus, the final configuration employed a batch size of 16 and 80 training epochs across all datasets, using Cross-Entropy Loss  [25]  as the cost function and Stochastic Gradient Descent (SGD)  [26]  as the optimizer. As the classes in all the datasets are not balanced as seen from Table  II , we used a class-weight biased method for training, where the classes with fewer samples were given more weightage in the loss function. We applied Random Horizontal Flip for data augmentation and used a learning rate of 1 × 10 -3 , with a scheduler that reduced the rate by 0.1 when performance plateaued. Our experiments were executed on two distinct hardware configurations: a MacBook Pro (M2 Pro-Chip with 10-core CPU and 16-core GPU) and a NVIDIA Tesla P100 GPU provided by Kaggle. The implementation was done using PyTorch  [27] . For our facial emotion recognition tasks, we employ Accuracy (%) as the evaluation metric, defined as:\n\nwhere TP: True Positive, TN: True Negative, FP: False Positive, FN: False Negative. ResEmoteNet was trained for 3 hours for FER2013, 2 hours for RAF-DB, 6 hours for AffectNet-7 and 4.5 hours for ExpW, with inference times of 1.4ms, 3.2ms, 3.6ms and 1.9ms per image respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results And Discussion",
      "text": "In this section, we present the experimental results conducted on four widely used benchmark datasets, FER2013, RAF-DB, AffectNet-7 and ExpW. We evaluated our proposed method, ResEmoteNet, on these datasets and tabulated in Table  III . We also present the confusion matrices of the ResEmoteNet for each of the datasets depicting their class wise confusions on the respective test sets in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Model Performance Across Datasets: Comparison With Prior Studies",
      "text": "In Table  III , we compared the performance of the proposed method with other state-of-the-art methods, the results demonstrate that our proposed method outperforms the current state-of-the-art techniques. Working with FER2013 is challenging due to inaccurate labeling, absence of faces in some images, and imbalanced data distribution, yet ResEmoteNet achieved a classification accuracy of 79.79%, a 2.97% absolute improvement over Ensemble ResMaskingNet  [13] . On RAF-DB, which presents real-life challenges such as pose, lighting, and occlusion, ResEmoteNet attained a classification accuracy of 94.76%, improving by 2.19% over S2D  [12] . On AffectNet-7, a large-scale dataset with 7 emotions and diverse annotations, our model achieved a classification accuracy of 72.93%, showing a 3.53% improvement over EmoAffectNet  [14] . Lastly, on ExpW, which presents a broad range of facial expressions in uncontrolled, real world settings, ResEmoteNet achieved a classification accuracy of 75.67%, showing a 2.19% improvement over APViT  [28] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Ablation Study",
      "text": "We conducted an ablation study on the RAF-DB dataset to evaluate the contribution of key components: Convolutional layers, SE blocks, and Residual Blocks within the ResE-moteNet architecture. The results in Table  IV  demonstrate the impact on accuracy and loss when these components are removed.\n\nRemoving the SE block resulted in a minor accuracy drop from 94.76% to 94.18%, while eliminating Residual Block 1 (with 256 channels) led to a substantial reduction in accuracy, from 94.76% to 75.39%, highlighting the critical role of residual connections. Similarly, removing convolutional layers, such as CNN 64 and CNN 64, 128, caused accuracy decreases of 11.25% and 6.49%, respectively. V. CONCLUSION In this paper, we introduced ResEmoteNet, a novel neural network architecture designed to address the challenging task of facial emotion recognition. Our model integrates a combination of three distinct networks: Convolutional Neural Network, Squeeze and Excitation network and residual network. The integration of these networks allows ResEmoteNet to effectively capture and interpret complex emotional expressions from facial images. To evaluate the performance of our proposed ResEmoteNet, we conducted extensive experiments using four widely recognized benchmark datasets: FER2013, RAF-DB, AffectNet-7 and ExpW. Our experimental results demonstrate that ResEmoteNet consistently outperforms existing stateof-the-art models across all these datasets. Additionally, an ablation study on the RAF-DB dataset confirmed the critical role of the residual and convolutional layers in maintaining the model's performance. These results highlight the effectiveness of our approach in accurately recognizing facial emotions, offering significant advancements in the field of facial emotion recognition. Future work will explore further enhancements to ResEmoteNet and its applications in real-world scenarios.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Acknowledgement",
      "text": "This work was supported by the iHub DivyaSampark IIT Roorkee.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Architecture of Squeeze and Excitation Block, (b) Overall Architecture of Proposed ResEmoteNet (c) Architecture",
      "page": 2
    },
    {
      "caption": "Figure 1: (b). The framework integrates a simple Convolutional Neural",
      "page": 2
    },
    {
      "caption": "Figure 2: Representative images of the 7 emotion classes: Angry, Disgust, Fear, Happy, Neutral, Sad, and Surprise, showcasing",
      "page": 3
    },
    {
      "caption": "Figure 1: (c) shows",
      "page": 3
    },
    {
      "caption": "Figure 2: To ensure a comprehensive analysis,",
      "page": 3
    },
    {
      "caption": "Figure 3: Confusion matrices of ResEmoteNet across four databases: (a) for FER2013, (b) for RAF-DB (c) for AffectNet-7 and",
      "page": 4
    },
    {
      "caption": "Figure 3: A. Model Performance Across Datasets: Comparison with",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Class": "",
          "FER2013": "Train",
          "RAF-DB": "Train",
          "AffectNet-7": "Train",
          "ExpW": "Train"
        },
        {
          "Class": "Angry",
          "FER2013": "3995",
          "RAF-DB": "705",
          "AffectNet-7": "24882",
          "ExpW": "2569"
        },
        {
          "Class": "Disgust",
          "FER2013": "436",
          "RAF-DB": "717",
          "AffectNet-7": "3803",
          "ExpW": "2796"
        },
        {
          "Class": "Fear",
          "FER2013": "4097",
          "RAF-DB": "281",
          "AffectNet-7": "6378",
          "ExpW": "761"
        },
        {
          "Class": "Happy",
          "FER2013": "7215",
          "RAF-DB": "4772",
          "AffectNet-7": "134415",
          "ExpW": "21375"
        },
        {
          "Class": "Neutral",
          "FER2013": "4965",
          "RAF-DB": "2524",
          "AffectNet-7": "74874",
          "ExpW": "24418"
        },
        {
          "Class": "Sad",
          "FER2013": "4830",
          "RAF-DB": "1982",
          "AffectNet-7": "25459",
          "ExpW": "7391"
        },
        {
          "Class": "Surprise",
          "FER2013": "3171",
          "RAF-DB": "1290",
          "AffectNet-7": "14090",
          "ExpW": "4943"
        },
        {
          "Class": "Total",
          "FER2013": "28709",
          "RAF-DB": "12271",
          "AffectNet-7": "283901",
          "ExpW": "64253"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Config": "",
          "Components": "CNN",
          "Removed\nComponent(s)": "",
          "Accuracy (%)": "",
          "Loss (%)": ""
        },
        {
          "Config": "C1",
          "Components": "64, 128, 256",
          "Removed\nComponent(s)": "-",
          "Accuracy (%)": "94.76",
          "Loss (%)": "3.73"
        },
        {
          "Config": "C2",
          "Components": "64, 128, 256",
          "Removed\nComponent(s)": "SE (256)",
          "Accuracy (%)": "94.18 (-0.58)",
          "Loss (%)": "16.83 (+13.1)"
        },
        {
          "Config": "C3",
          "Components": "128, 256",
          "Removed\nComponent(s)": "CNN (64)",
          "Accuracy (%)": "83.51 (-11.25)",
          "Loss (%)": "7.19 (+3.46)"
        },
        {
          "Config": "C4",
          "Components": "256",
          "Removed\nComponent(s)": "CNN (64, 128)",
          "Accuracy (%)": "88.27 (-6.49)",
          "Loss (%)": "5.32 (+1.59)"
        },
        {
          "Config": "C5",
          "Components": "64, 128, 256",
          "Removed\nComponent(s)": "RB (256)",
          "Accuracy (%)": "75.39 (-19.37)",
          "Loss (%)": "10.2 (+6.47)"
        },
        {
          "Config": "C6",
          "Components": "64, 128, 256",
          "Removed\nComponent(s)": "RB (256, 512)",
          "Accuracy (%)": "82.77 (-11.99)",
          "Loss (%)": "8.8 (+5.07)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "Accuracy in %": "FER2013"
        },
        {
          "Method": "Seg. VGG-19 [29]",
          "Accuracy in %": "75.97"
        },
        {
          "Method": "EmoNeXt\n[30]",
          "Accuracy in %": "76.12"
        },
        {
          "Method": "En. ResMaskingNet\n[13]",
          "Accuracy in %": "76.82"
        },
        {
          "Method": "SEResNet\n[31]",
          "Accuracy in %": "-"
        },
        {
          "Method": "Arm [32]",
          "Accuracy in %": "-"
        },
        {
          "Method": "APVit\n[28]",
          "Accuracy in %": "-"
        },
        {
          "Method": "ARBEx [33]",
          "Accuracy in %": "-"
        },
        {
          "Method": "S2D [12]",
          "Accuracy in %": "-"
        },
        {
          "Method": "C MT EmoAffectNet\n[14]",
          "Accuracy in %": "-"
        },
        {
          "Method": "AGLRLS [34]",
          "Accuracy in %": "-"
        },
        {
          "Method": "SchiNet\n[35]",
          "Accuracy in %": "-"
        },
        {
          "Method": "Proposed ResEmoteNet",
          "Accuracy in %": "79.79"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automated facial action coding system for dynamic analysis of facial expressions in neuropsychiatric disorders",
      "authors": [
        "J Hamm",
        "C Kohler",
        "R Gur",
        "R Verma"
      ],
      "year": "2011",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "2",
      "title": "Detecting student emotions in computer-enabled classrooms",
      "authors": [
        "N Bosch",
        "S D'mello",
        "R Baker",
        "J Ocumpaugh",
        "V Shute",
        "M Ventura",
        "L Wang",
        "W Zhao"
      ],
      "year": "2016",
      "venue": "International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "4",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations ICLR"
    },
    {
      "citation_id": "6",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "S Zhang",
        "Y Zhang",
        "Y Zhang",
        "Y Wang",
        "Z Song"
      ],
      "year": "2023",
      "venue": "MDPI Electronics"
    },
    {
      "citation_id": "7",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition: 13th Chinese Conference"
    },
    {
      "citation_id": "8",
      "title": "Local multi-head channel selfattention for facial expression recognition",
      "authors": [
        "R Pecoraro",
        "V Basile",
        "V Bono"
      ],
      "year": "2022",
      "venue": "MDPI Information"
    },
    {
      "citation_id": "9",
      "title": "Distract your attention: Multihead cross attention network for facial expression recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2023",
      "venue": "MDPI Biomimetics"
    },
    {
      "citation_id": "10",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "J Mao",
        "R Xu",
        "X Yin",
        "Y Chang",
        "B Nie",
        "A Huang"
      ],
      "year": "2023",
      "venue": "Poster++: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12149"
    },
    {
      "citation_id": "11",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "C Zheng",
        "M Mendieta",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Y Chen",
        "J Li",
        "S Shan",
        "M Wang",
        "R Hong"
      ],
      "year": "2023",
      "venue": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "arxiv": "arXiv:2312.05447"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition using residual masking network",
      "authors": [
        "L Pham",
        "T Vu",
        "T Tran"
      ],
      "year": "2021",
      "venue": "2020 25Th international conference on pattern recognition (ICPR)"
    },
    {
      "citation_id": "14",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Lrdif: Diffusion models for under-display camera emotion recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "Lrdif: Diffusion models for under-display camera emotion recognition",
      "arxiv": "arXiv:2402.00250"
    },
    {
      "citation_id": "16",
      "title": "Four-player groupgan for weak expression recognition via latent expression magnification",
      "authors": [
        "W Niu",
        "K Zhang",
        "D Li",
        "W Luo"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "17",
      "title": "Htnet for microexpression recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "W Luo",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "20",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Path aggregation network for instance segmentation",
      "authors": [
        "S Liu",
        "L Qi",
        "H Qin",
        "J Shi",
        "J Jia"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "An overview of gradient descent optimization algorithms",
      "authors": [
        "S Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms",
      "arxiv": "arXiv:1609.04747"
    },
    {
      "citation_id": "27",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Vision transformer with attentive pooling for robust facial expression recognition",
      "authors": [
        "F Xue",
        "Q Wang",
        "Z Tan",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "A novel facial emotion recognition model using segmentation vgg-19 architecture",
      "authors": [
        "S Vignesh",
        "M Savithadevi",
        "M Sridevi",
        "R Sridhar"
      ],
      "year": "2023",
      "venue": "International Journal of Information Technology"
    },
    {
      "citation_id": "30",
      "title": "Emonext: an adapted convnext for facial emotion recognition",
      "authors": [
        "Y Boudouri",
        "A Bohi"
      ],
      "year": "2023",
      "venue": "2023 IEEE 25th International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "A study on computer vision for facial emotion recognition",
      "authors": [
        "Z.-Y Huang",
        "C.-C Chiang",
        "J.-H Chen",
        "Y.-C Chen",
        "H.-L Chung",
        "Y.-P Cai",
        "H.-C Hsu"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "32",
      "title": "Learning to amend facial expression representation via de-albino and affinity",
      "authors": [
        "J Shi",
        "S Zhu",
        "Z Liang"
      ],
      "year": "2021",
      "venue": "Learning to amend facial expression representation via de-albino and affinity",
      "arxiv": "arXiv:2103.10189"
    },
    {
      "citation_id": "33",
      "title": "Arbex: Attentive feature extraction with reliability balancing for robust facial expression learning",
      "authors": [
        "A Wasi",
        "K Šerbetar",
        "R Islam",
        "T Rafi",
        "D.-K Chae"
      ],
      "year": "2023",
      "venue": "Arbex: Attentive feature extraction with reliability balancing for robust facial expression learning",
      "arxiv": "arXiv:2305.01486"
    },
    {
      "citation_id": "34",
      "title": "Adaptive global-local representation learning and selection for cross-domain facial expression recognition",
      "authors": [
        "Y Gao",
        "Y Xie",
        "Z Hu",
        "T Chen",
        "L Lin"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Schinet: Automatic estimation of symptoms of schizophrenia from facial behaviour analysis",
      "authors": [
        "M Bishay",
        "P Palasek",
        "S Priebe",
        "I Patras"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}