{
  "paper_id": "2403.12609v1",
  "title": "Sun Team'S Contribution To Abaw 2024 Competition: Audio-Visual Valence-Arousal Estimation And Expression Recognition",
  "published": "2024-03-19T10:24:15Z",
  "authors": [
    "Denis Dresvyanskiy",
    "Maxim Markitantov",
    "Jiawei Yu",
    "Peitong Li",
    "Heysem Kaya",
    "Alexey Karpov"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on finetuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "This paper presents out contribution to the 2024 edition  [28]  of the Affective Behavior Analysis in-the-wild (ABAW) challenge series  [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] 46] . The challenges in the field of affective computing have boosted the development of stateof-the-art methods, while also ensuring the reproducibility and comparability of the developed methods under a common experimental protocol. This year, sub-challenges of ABAW include 8-class categorical emotion recognition (Expression Challenge-EXPR), featuring Ekman's six basic emotions (anger, disgust, fear, happiness, sadness, surprise), plus neutral and others classes as well as emotion primitives (arousal and valence) regression challenge (VA). We participated in these two sub-challenges. For the challenge data, its annotation process and further details, we refer the reader to  [28]  and the former editions of ABAW. We continue with the details of the proposed approach in the next section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2.1. Acoustic Emotion Recognition System",
      "text": "We proposed three slightly different models. The backbone of all models is based on the PDEM. The Public Dimensional Emotion Model (PDEM), designed for predicting arousal, valence, and dominance, is the first publicly available transformer-based dimensional Speech Emotion Recognition (SER) model  [45] . It is fine-tuned on the pretrained wav2vec2-large-robust model, which is one of the variants of Wav2Vec 2.0  [1] .\n\nOn top of each model, we stack two Gated Recurrent Unit (GRU) layers with 256 neurons (AudioModelV1) or two transformer layers with self-attention mechanisms, each with 32 and 16 heads (AudioModelV1 and AudioMod-elV2). After the last transformer layer, we aggregate the information along the time axis using Convolutional Neural Networks (CNN) and apply two subsequent Fully Connected Layers (FCLs) for feature compression and for the classification or regression layer, depending on the challenge. We fine-tune all the layers from the top to the last two (AudioModelV1 and AudioModelV2) or four (AudioMod-elV3) encoding layers of the backbone model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Emotion Recognition System",
      "text": "For the visual emotion recognition system, we have trained the final model in several stages. First of all, we have pre-trained the modified static models (EfficientNet-B1, -B4, and ViT-B-16) on the pre-training datasets introduced in 3.1.1. Next, to further enhance the efficacy and robustness of the static Facial Expression Recognition (FER) models, they have been fine-tuned on the AffWild2 dataset. Finally, the fine-tuned static models have been frozen and used as accurate feature extractors that provide valuable affective features for consecutive temporal aggregation using the dynamic FER model. In the next subsections, we describe the parts of the pipeline of the final visual dynamic FER system.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Static Models",
      "text": "To construct an accurate emotion recognition model, especially for the visual data, a robust and efficient feature extractor is needed. Those models are called static as they are trained on and provide emotion predictions as well as informative facial features per-frame, not paying attention to the temporal context. In the context of FER, such state-of-theart models are based either on CNN or recently introduced Visual Transformer (ViT) neural network architectures. In this work, we experimented with both approaches, as various models can demonstrate different performances given in-the-wild nature of the data. Specifically, we have employed the EfficientNet  [41]  (B1 and B4 versions, compris-  ing 7.8M and 19.3M parameters, respectively) and Visual Transformer-B-16  [9]  architectures that are pre-trained on ImageNet  [6, 40] .\n\nHowever, before fine-tuning those models on various emotion recognition datasets (including AffWild2), we have slightly modified them as depicted in Figure  1 . Thus, we removed the last layer responsible for the classification and stacked on top of it (1) the dropout followed by deep embeddings layer (feed-forward) with 256 neurons, batch normalization, and Tanh activation function, (2) the new classification or regression layer with the corresponding number of neurons (8 for the classification task, 2 for arousalvalence regression). For classification task, a softmax activation function is used, while for regression (Valence and Arousal) we used the Tanh as it transforms the output values into the [-1, 1] range.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Models",
      "text": "It is well-known that emotions are temporal phenomena. They last for a certain period and are reflected in the facial, vocal, and body dynamics. To exploit this aspect, we developed dynamic FER models that take into account the temporal context during the decision-making process. An overview of these model architectures is given in Figures  2  and 3 .\n\nIn the emotion recognition literature, there are many different approaches for temporal information aggregation, including functionals-aggregation (calculation of statistics over period of time), recurrent neural networks  [10] , and recently introduced Transformers-based architectures  [4, 34, 44] . Although recurrent neural networks have been most popular in FER domain so far, the Transformers-based architectures are taking the lead in the last years.\n\nTo leverage the most effective architectures and build a robust FER model, we employed the Transformer-based temporal aggregation method as well. The implemented FER dynamic model is schematically depicted in Figure  3 . Thus, the dynamic model consists of a static feature extractor and the temporal part that consists of three consecutive Transformer-encoder-based layers inspired by  [43] . Lastly, the classification or regression head completes the decisionmaking process. For the regression case, a Tanh activation function is used.\n\nWe should note that the feature extractor is firstly pretrained on several FER datasets, then fine-tuned on the Af-fWild2 dataset frames, and is finally frozen so that it does not change its weights during the training of the dynamic model. As we fixed the number of static embeddings by modifying respective static models, the feature extractor always outputs 256 features per frame. For the Transformerbased layers, we set the number of heads equal to 8 and dropout equals 0.1. Additionally, the positional encoding employed in  [43]  is applied to visual embeddings.\n\nFor the comparison and as an alternative, we developed a simpler temporal aggregation method: the statistical-based model that calculates functionals over a fixed period. Here, based on previous work on an earlier version of ABAW challenge  [10] , we fix the analysis window to 2 seconds. We apply mean, minimum, and maximum functional statistics to non-overlapping 2-second windows and apply Kernel Extreme Learning Machine (KELM)  [13] . KELM aims to solve a regularized least squares regression problem between a kernel (instance similarity) matrix K and a target vector (or matrix) T from the training dataset, and hence is very fast to train given the kernel. The set of weights (β) in KELM is calculated via:\n\nwhere I is the identity matrix, and C is the regularization coefficient optimized via cross-validation on the challenge development set. The prediction for a test instance x is obtained via ŷ = K(D, x)β, where K(, ) and D denote the kernel function and the training dataset, respectively. KELM is used both for regression and classification in this study. For the classification (Expr) task, considering the challenge performance measure and the class imbalance, we use the Weighted KELM  [48]  that weights each instance inversely proportional to the training set instance count of that class. This extension is not only more memory-efficient compared to upsampling but also found to yield more accurate results in challenging audio  [15]  and video-based  [2]  recognition tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Fusion Schemes",
      "text": "Fusion, particularly to leverage complementary multimodal information is an important stage in video-based emotion recognition systems. In our experiments, we experimented with decision (late) and model-based fusion strategies. In the latter, the features from audio and video models are concatenated and fed to attention mechanisms (selfattention and cross-attention).\n\nFor late fusion, we experimented with two schemes. First, we used Dirichlet-based Random Weighted Fusion (DWF), where fusion matrices containing weights per model-class combination are randomly sampled from the Dirichlet distribution. A large pool of such matrices is generated and the matrix that gives the best performance in terms of the task-wise challenge measure is selected for the test set submission. This approach is shown to generalize well to in-the-wild emotion recognition in former studies  [10, 16] .\n\nThe second decision fusion approach is based on Random Forests (RF)  [3] , where the concatenated probability vectors from the base models are stacked to RF. To avoid over-fitting, out-of-bag predictions are probed to optimize the number of trees.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Data",
      "text": "For all experiments presented in this work, the AffWild2 dataset and corresponding labels from the 6th ABAW challenge  [28]  have been used.\n\nThe AffWild2 dataset is an audiovisual in-the-wild corpus, that serves as a comprehensive benchmark for multiple affective behavior analysis tasks and is utilized in ABAW Competition series. Comprising 594 videos with approximately 3 million frames from 584 subjects, it is annotated in terms of Valence and Arousal emotional continuous labels ranging from -1 to 1. Additionally, a subset of 548 videos (approximately 2.7 million frames) is annotated for expression recognition across 8 classes, including 6 Ekman's basic expressions, the Neutral state, and Other.\n\nThe dataset is subject-independently partitioned into training, validation, and test sets, ensuring no subject overlap across partitions. Thus, the AffWild2 dataset poses several emotion recognition challenges to be solved, advancing affective computing research in unconstrained settings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pre-Training Data",
      "text": "To pre-train our static FER models, we used a range of publicly available datasets, namely, Remote Collaborative and Affective Interactions (RECOLA)  [39] ,  [30] , SEMAINE  [37] , AFEW-VA  [29]  (which is based on AFEW  [8] ), AffectNet  [38] , SAVEE  [14] , EMOTIC  [31] , ExpW  [47] , FER+  [11] , RAF-DB  [33] . To pre-train those models, Ekman's six basic emotions were selected from the aforementioned datasets along with Valence and Arousal values. We summarized the information about all used corpora in Table  1 .\n\nAfter the pre-training phase, the static FER models have been fine-tuned on the AffWild2 dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Preprocessing",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio",
      "text": "Before training an audio model, in addition to extracting audio signals from multimedia files, we perform voice activity detection. Due to the specific nature of the acoustic data provided by the ABAW24 challenge organizers, audio data may include background noise and multiple speakers, making it difficult to identify the target speaker, methods based only on audio analysis are not suitable. Therefore, we rely on video modality by analyzing the video data frame by frame. For this purpose, facial landmarks are extracted using the MediaPipe framework  [35] , then mouth landmarks are detected, and the corresponding region of interest is extracted. It is used to determine whether the target speaker's mouth is open or closed. Simultaneously, we filter the acoustic data from noise using Spleeter by deezer * .\n\nAfter that, we downsample all videos with varying frame rates to 5 Frames Per Second (FPS), which helps to fix the window length. Then, 4-second windows with a step of two seconds are formed on the filtered detected voice segments. In case of the EXPR challenge, to obtain the target label at each second, we compute the most frequent frame-wise label. In case of the VA challenge, we utilize all downsampled values. Therefore, each window has four labels and 20 valence/arousal values for the EXPR and VA challenges, respectively.\n\nTo enhance the generalizability of the audio models, we employ several augmentation techniques, including polarity inversion, the addition of white noise or variation in audio volume, and Label Smoothing  [36] . These techniques help to reduce the confidence level of the models in their emotion predictions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Video",
      "text": "Depending on the model type (static or dynamic), several preprocessing steps have been applied. For the static FER model, we have depicted the preprocessing pipeline in Figure  5 . We first detect faces and crop them, adding 15 pixels to all bounding box boundaries to include the chin and other human facial features. we have utilized the Reti-naFace model  [5, 7]  based on the MobileNet  [12]  architecture, namely the MobileNet-0.25 version. We have chosen this model because it is one of the most effective face recognition models known nowadays, yet very computationally efficient, since it has only around 1.7 million parameters. The next step in the static data preprocessing pipeline is Audio, Visual 3:50 hours A, V In-the-wild SEWA  [30]  Audio, Visual 9:10 hours A, V In-the-wild SEMAINE  [37]  Audio, Visual 6:30 hours A, V Lab. AFEW-VA  [29]  Visual ≈30,000 frames A, V In-the-wild AffectNet  [38]  Visual 420,299 images C, A, V In-the-wild SAVEE  [14]  Audio, Visual ≈24 minutes C Lab. EMOTIC  [31]  Visual 23,571 images C, A, V In-the-wild ExpW  [47]  Visual 91,793 images C In-the-wild FER+  [11]  Visual In the context of the dynamic FER model, the data preprocessing pipeline is depicted in Figure  5 . It closely mirrors the static methodology except for one important step: we use additional normalization of embeddings to avoid the gradient explosion that can arise in the early stages of training. Two different normalization methods have been used: MinMax scaling and Per-video-MinMax scaling. The difference in methods is that MinMax scaling computes the corresponding min and max values across the whole training set (and then applied to every instance), while the Pervideo-MinMax scaling computes min and max values for every video separately (and applies those values only within the corresponding video).\n\nIt is well-known that Transformer-based models can operate with sequences of arbitrary length. However, different video frame rates (FPS) of videos and varying lengths of the video sequences can significantly harm the training process. Therefore, to stabilize it and ensure convergence, we downsampled all videos with varying frame rates to 5 FPS and fixed the window length. It is experimentally shown that the size of a temporal window can significantly influence the efficacy of the FER model  [10] . That is why we have done experiments with different temporal context lengths, namely: 1, 2, 3, 4, 6, and 8 seconds. Finally, the model with the highest competition measure value is used for test set submissions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Post-Processing",
      "text": "After getting class probabilities or Valence and Arousal values for every frame, several post-processing steps are applied. Since dynamic models are trained using reduced FPS, we first interpolated the models' predictions to align them with the ground truth labels on the validation set and the actual video FPS on the test set. We used linear interpolation, filling in missing values between two consecutive predictions. After interpolation, we applied the smoothing using Hamming window  [42] . The size of the window has been chosen to be 0.5 seconds.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "The challenge measures for expression (EXPR) and dimensional emotion (VA) challenges are F1 and Concordance Correlation Coefficient (CCC), respectively. CCC is recently popularly used in regression tasks over the Pearson's Correlation (PC), as it also considers the difference in means  [32] :\n\nwhere µ t and µ p denote the averaged ground truth and predicted scores for all test clips, respectively; σ t and σ p denote the respective standard deviations; σ t,p is the covariance between t and p.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Audio-Based Models",
      "text": "For acoustic modality, we generated three base models via End-to-End (E2E) fine-tuning of the PDEM model. All of the top approaches used data augmentation and Spleeter for background noise separation. Note that the results reported here are on 4-second windows excluding the unvoiced segments, rather than frame-wise over which the ground truth annotations are provided. The best development set performance for the EXPR and VA challenges are presented in Tables  2  and 3 , respectively. On both tasks, the best development set performance is obtained with AudioModelV3. We therefore use (and fuse) these models' predictions for our test set submissions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Video-Based Models",
      "text": "We experimented with EfficientNet-B1 and ViT models for extracting features for subsequent modeling. Using  Extensive experiments with end-to-end models showed the sensitivity of these models to hyperparameters. These models' predictions are post-processed to match the ground truth label frequency. Our best performance (0.387 F1 score) on the EXPR challenge was obtained with ViT as an embedding extractor, followed by three Transformer layers for temporal modeling. The best E2E video model on the VA task was, on the other hand, EfficientNet-B1, reaching an average CCC performance of 0.574, with corresponding arousal and valence CCC scores of 0.626 and 0.523, respectively. These along with the best functional-based system's predictions are later used for test set predictions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Models And Test Submissions",
      "text": "We selected the best-performing audio and visual models, extracted embeddings, and experimented with the attention mechanism. This approach did not outperform the video-only system on the development set. Therefore, we used decision-based fusion schemes described earlier for the fusion. For our test set probes, we used one unimodal (best face-based E2E system) and three multimodal systems, based on the development set performances. By the time of writing, the test set results are not available to the challenge participants.    10 ) is used for fusion. For the sake of completeness, we submit predictions using both RF and DWF late fusion schemes.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "The results of our research highlight the potential of deep learning models for audiovisual emotion recognition in unconstrained, \"in-the-wild\" settings. The face-based endto-end dynamic models, leveraging salient embeddings extracted by Visual Transformer and EfficientNet-B1 model, achieved a competitive efficacy, outperforming the traditional functional-based approaches on the AffWild2 dataset. However, optimizing video models still remains computationally very demanding, posing additional challenges in deploying these solutions for \"in-the-wild\" scenarios.\n\nWhile our experiments on the development set suggested that combining audio and video modalities through fusion techniques could significantly enhance performance, we do not have an opportunity to draw definitive conclusions about the advantages of multi-modal fusion approaches for the 6th ABAW challenge yet, as the test set results have not yet been revealed.\n\nLooking ahead, further research could explore other novel fusion strategies that take into account additional contextual information such as background sound, participants' postures and gestures, linguistics, and other valuable complementary sources of information. Ultimately, progress in this field holds promise for enhancing human-computer interaction and enabling a natural unconstrained usage of computer systems and robots in human society.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The neural network architecture of modified frame-level",
      "page": 2
    },
    {
      "caption": "Figure 2: The architecture of Kernel ELM based dynamic emotion",
      "page": 2
    },
    {
      "caption": "Figure 1: . Thus, we",
      "page": 2
    },
    {
      "caption": "Figure 3: Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window",
      "page": 3
    },
    {
      "caption": "Figure 3: Thus, the dynamic model consists of a static feature extrac-",
      "page": 3
    },
    {
      "caption": "Figure 4: Pipeline of the image preprocessing for the static FER",
      "page": 4
    },
    {
      "caption": "Figure 5: Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.",
      "page": 5
    },
    {
      "caption": "Figure 5: It closely mir-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "karpov@iias.spb.su"
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "1. Introduction"
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "of\nthe Affective Behavior Analysis"
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "mon experimental protocol."
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "This\nyear,\nsub-challenges"
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "categorical\nemotion\nrecognition"
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "gust,\nfear, happiness,\nsadness,"
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": ""
        },
        {
          "of the Russian Academy of Sciences (SPC RAS), Russia": "two sub-challenges. For the challenge data,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. Methodology": "2.1. Acoustic Emotion Recognition System"
        },
        {
          "2. Methodology": "We proposed three slightly different models.\nThe back-"
        },
        {
          "2. Methodology": "bone of all models is based on the PDEM. The Public Di-"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "mensional Emotion Model\n(PDEM), designed for predict-"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "ing arousal, valence, and dominance,\nis the first publicly"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "available transformer-based dimensional Speech Emotion"
        },
        {
          "2. Methodology": "Recognition (SER) model [45].\nIt\nis fine-tuned on the pre-"
        },
        {
          "2. Methodology": "trained wav2vec2-large-robust model, which is one of\nthe"
        },
        {
          "2. Methodology": "variants of Wav2Vec 2.0 [1]."
        },
        {
          "2. Methodology": "On\ntop\nof\neach model, we\nstack\ntwo Gated Recur-"
        },
        {
          "2. Methodology": "rent Unit (GRU) layers with 256 neurons (AudioModelV1)"
        },
        {
          "2. Methodology": "or\ntwo transformer\nlayers with self-attention mechanisms,"
        },
        {
          "2. Methodology": "each with 32 and 16 heads (AudioModelV1 and AudioMod-"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "elV2). After\nthe last\ntransformer\nlayer, we aggregate the"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "information along the time axis using Convolutional Neu-"
        },
        {
          "2. Methodology": "ral Networks (CNN) and apply two subsequent Fully Con-"
        },
        {
          "2. Methodology": "nected Layers (FCLs)\nfor\nfeature compression and for\nthe"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "classification or\nregression layer, depending on the chal-"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "lenge. We fine-tune all the layers from the top to the last two"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "(AudioModelV1 and AudioModelV2) or four (AudioMod-"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "elV3) encoding layers of the backbone model."
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "2.2. Visual Emotion Recognition System"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "For the visual emotion recognition system, we have trained"
        },
        {
          "2. Methodology": "the final model\nin several\nstages.\nFirst of\nall, we have"
        },
        {
          "2. Methodology": "pre-trained the modified static models (EfficientNet-B1,\n-"
        },
        {
          "2. Methodology": "B4, and ViT-B-16) on the pre-training datasets introduced"
        },
        {
          "2. Methodology": "in 3.1.1.\nNext,\nto further\nenhance\nthe\nefficacy and ro-"
        },
        {
          "2. Methodology": "bustness of the static Facial Expression Recognition (FER)"
        },
        {
          "2. Methodology": "models, they have been fine-tuned on the AffWild2 dataset."
        },
        {
          "2. Methodology": "Finally,\nthe fine-tuned static models have been frozen and"
        },
        {
          "2. Methodology": "used as accurate feature extractors that provide valuable af-"
        },
        {
          "2. Methodology": "fective features for consecutive temporal aggregation using"
        },
        {
          "2. Methodology": "the dynamic FER model.\nIn the next subsections, we de-"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "scribe the parts of the pipeline of the final visual dynamic"
        },
        {
          "2. Methodology": "FER system."
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "2.2.1\nStatic Models"
        },
        {
          "2. Methodology": ""
        },
        {
          "2. Methodology": "To construct an accurate emotion recognition model, espe-"
        },
        {
          "2. Methodology": "cially for the visual data, a robust and efficient feature ex-"
        },
        {
          "2. Methodology": "tractor is needed. Those models are called static as they are"
        },
        {
          "2. Methodology": "trained on and provide emotion predictions as well as infor-"
        },
        {
          "2. Methodology": "mative facial features per-frame, not paying attention to the"
        },
        {
          "2. Methodology": "temporal context.\nIn the context of FER, such state-of-the-"
        },
        {
          "2. Methodology": "art models are based either on CNN or recently introduced"
        },
        {
          "2. Methodology": "Visual Transformer (ViT) neural network architectures.\nIn"
        },
        {
          "2. Methodology": "this work, we experimented with both approaches, as var-"
        },
        {
          "2. Methodology": "ious models can demonstrate different performances given"
        },
        {
          "2. Methodology": "in-the-wild nature of\nthe data.\nSpecifically, we have em-"
        },
        {
          "2. Methodology": "ployed the EfficientNet [41] (B1 and B4 versions, compris-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "size (in the number of frames), N – the number of neurons in the decision-making head (either 8 for classification or 2 for regression task)."
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "a robust FER model, we employed the Transformer-based"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "temporal aggregation method as well.\nThe implemented"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "FER dynamic model\nis schematically depicted in Figure 3."
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "Thus, the dynamic model consists of a static feature extrac-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "tor and the temporal part\nthat consists of three consecutive"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "Transformer-encoder-based layers inspired by [43]. Lastly,"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "the classification or regression head completes the decision-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "making process. For the regression case, a Tanh activation"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "function is used."
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "We should note that\nthe feature extractor\nis firstly pre-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "trained on several FER datasets,\nthen fine-tuned on the Af-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "fWild2 dataset frames, and is finally frozen so that\nit does"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "not change its weights during the training of\nthe dynamic"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "model. As we fixed the number of static embeddings by"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "modifying respective static models, the feature extractor al-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "ways outputs 256 features per frame. For the Transformer-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "based layers, we set\nthe number of heads equal\nto 8 and"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "dropout equals 0.1. Additionally,\nthe positional encoding"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "employed in [43] is applied to visual embeddings."
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": ""
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "For the comparison and as an alternative, we developed a"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "simpler temporal aggregation method:\nthe statistical-based"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "model that calculates functionals over a fixed period. Here,"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "based on previous work on an earlier version of ABAW"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "challenge [10], we fix the analysis window to 2 seconds."
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "We apply mean, minimum, and maximum functional statis-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "tics to non-overlapping 2-second windows and apply Ker-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "nel Extreme Learning Machine (KELM) [13]. KELM aims"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "to solve a regularized least squares regression problem be-"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "tween a kernel\n(instance similarity) matrix K and a target"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "vector (or matrix) T from the training dataset, and hence is"
        },
        {
          "Figure 3. Pipeline and architecture of the transformer-based sequence to one video emotion recognition model. W – the temporal window": "very fast to train given the kernel. The set of weights (β) in"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "the test\nset\nsubmission.\nThis approach is\nshown to gen-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "eralize well\nto in-the-wild emotion recognition in former"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "studies [10, 16]."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "The second decision fusion approach is based on Ran-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "dom Forests (RF)\n[3], where the concatenated probability"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "vectors from the base models are stacked to RF. To avoid"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "over-fitting, out-of-bag predictions are probed to optimize"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "the number of trees."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "3. Experimental Setup"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "3.1. Experimental Data"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "For all experiments presented in this work,\nthe AffWild2"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "dataset and corresponding labels from the 6th ABAW chal-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "lenge [28] have been used."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "The AffWild2 dataset\nis an audiovisual\nin-the-wild cor-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "pus, that serves as a comprehensive benchmark for multiple"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "affective behavior analysis tasks and is utilized in ABAW"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "Competition series. Comprising 594 videos with approxi-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "mately 3 million frames from 584 subjects, it is annotated in"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "terms of Valence and Arousal emotional continuous labels"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "ranging from -1 to 1. Additionally, a subset of 548 videos"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "(approximately 2.7 million frames) is annotated for expres-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "sion recognition across 8 classes, including 6 Ekman’s basic"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "expressions, the Neutral state, and Other."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "The\ndataset\nis\nsubject-independently\npartitioned\ninto"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "training, validation, and test sets, ensuring no subject over-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "lap across partitions. Thus, the AffWild2 dataset poses sev-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "eral emotion recognition challenges to be solved, advancing"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "affective computing research in unconstrained settings."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "3.1.1\nPre-training Data"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "To\npre-train\nour\nstatic\nFER models, we\nused\na\nrange"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "of publicly available datasets,\nnamely, Remote Collabo-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "rative and Affective Interactions\n(RECOLA)\n[39],\n[30],"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "SEMAINE\n[37],\nAFEW-VA [29]\n(which\nis\nbased\non"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "AFEW [8]), AffectNet\n[38], SAVEE [14], EMOTIC [31],"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "ExpW [47], FER+ [11], RAF-DB [33]. To pre-train those"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "models, Ekman’s six basic emotions were selected from the"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "aforementioned datasets along with Valence and Arousal"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "values. We summarized the information about all used cor-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "pora in Table 1."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "After the pre-training phase, the static FER models have"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "been fine-tuned on the AffWild2 dataset."
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "3.2. Data Preprocessing"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "3.2.1\nAudio"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "Before training an audio model,\nin addition to extracting"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": ""
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "audio signals from multimedia files, we perform voice ac-"
        },
        {
          "terms of\nthe task-wise challenge measure is\nselected for": "tivity detection. Due to the specific nature of the acoustic"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": "."
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        },
        {
          "Table 1. The summary of pre-training corpora used in this work. Note that in the “Data volume” column, only the volume of data utilized": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "MinMax scaling and Per-video-MinMax scaling. The dif-"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "ference in methods is that MinMax scaling computes the"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "corresponding min and max values across the whole train-"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "ing set (and then applied to every instance), while the Per-"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "video-MinMax scaling computes min and max values for"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "every video separately (and applies those values only within"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "the corresponding video)."
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "It is well-known that Transformer-based models can op-"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "erate with sequences of arbitrary length. However, different"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "video frame rates (FPS) of videos and varying lengths of the"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "video sequences can significantly harm the training process."
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "Therefore, to stabilize it and ensure convergence, we down-"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": "sampled all videos with varying frame rates to 5 FPS and"
        },
        {
          "Figure 5. Pipeline of preprocessing of the video data for dynamic emotion recognition modeling.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Best development set results using Functionals-based",
      "data": [
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "the VA challenge."
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "Model\nArousal\nValence\nAverage"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "0.345\nAudioModelV3\n0.400\n0.290"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "AudioModelV1\n0.377\n0.282\n0.329"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "AudioModelV2\n0.375\n0.241\n0.308"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "the functionals-based approach, where the embeddings are"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "summarized over 2-second non-overlapping windows, we"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "obtained decent results on both EXPR (see Table 4) and VA."
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "Based on the results of the EXPR challenge, we optimized"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "the VA model using the combination of mean, min, and max"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "functionals. The best development set CCC (0.489 average"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "over\ntwo dimensions) was obtained using EfficientNetB1"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "model, with corresponding CCC performances of 0.398 and"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "0.581 for valence and arousal,\nrespectively. Note that\nthe"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "results reported for\nthe functionals-based approach are on"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "2-second windows, rather than frame-wise."
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "Table 4.\nBest development\nset\nresults using Functionals-based"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "video summarization approach.\nEmbeddings are extracted from"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "fine-tuned ViT model."
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "Functionals\nF1\nAcc.\nPrecision\nRecall"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "0.354\nmean, max, min\n0.478\n0.338\n0.416"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "mean, max\n0.344\n0.462\n0.328\n0.412"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "mean, min\n0.352\n0.469\n0.336\n0.419"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "Extensive experiments with end-to-end models showed"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "the sensitivity of\nthese models to hyperparameters. These"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "models’ predictions are post-processed to match the ground"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "truth label\nfrequency.\nOur best performance\n(0.387 F1"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "score) on the EXPR challenge was obtained with ViT as an"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "embedding extractor, followed by three Transformer layers"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "for temporal modeling. The best E2E video model on the"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "VA task was, on the other hand, EfficientNet-B1, reaching"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "an average CCC performance of 0.574, with corresponding"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "arousal and valence CCC scores of 0.626 and 0.523, respec-"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "tively. These along with the best functional-based system’s"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "predictions are later used for test set predictions."
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "4.3. Multimodal Models and Test Submissions"
        },
        {
          "Table 3. Best development set results per acoustic base-model on": ""
        },
        {
          "Table 3. Best development set results per acoustic base-model on": "We selected the best-performing audio and visual models,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: Development set performances of the submitted systems for the EXPR challenge. DWF: Dirichlet-based Random Weighted",
      "data": [
        {
          "Table 5. Development set CCC performances of the submitted systems for the VA challenge. DWF: Dirichlet-based Random Weighted": "Fusion, RF: Random Forest-based fusion."
        },
        {
          "Table 5. Development set CCC performances of the submitted systems for the VA challenge. DWF: Dirichlet-based Random Weighted": "Sys #"
        },
        {
          "Table 5. Development set CCC performances of the submitted systems for the VA challenge. DWF: Dirichlet-based Random Weighted": "1"
        },
        {
          "Table 5. Development set CCC performances of the submitted systems for the VA challenge. DWF: Dirichlet-based Random Weighted": "2"
        },
        {
          "Table 5. Development set CCC performances of the submitted systems for the VA challenge. DWF: Dirichlet-based Random Weighted": "3"
        },
        {
          "Table 5. Development set CCC performances of the submitted systems for the VA challenge. DWF: Dirichlet-based Random Weighted": "4"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: Development set performances of the submitted systems for the EXPR challenge. DWF: Dirichlet-based Random Weighted",
      "data": [
        {
          "Table 6. Development set performances of": "Fusion, RF: Random Forest-based fusion.",
          "the submitted systems for": "",
          "the EXPR challenge. DWF: Dirichlet-based Random Weighted": ""
        },
        {
          "Table 6. Development set performances of": "Sys #",
          "the submitted systems for": "Method",
          "the EXPR challenge. DWF: Dirichlet-based Random Weighted": "F1"
        },
        {
          "Table 6. Development set performances of": "1",
          "the submitted systems for": "Face-based E2E model (ViT + Transformer layers)",
          "the EXPR challenge. DWF: Dirichlet-based Random Weighted": "0.397"
        },
        {
          "Table 6. Development set performances of": "2",
          "the submitted systems for": "Sys1 + Best Audio (DWF)",
          "the EXPR challenge. DWF: Dirichlet-based Random Weighted": "0.458"
        },
        {
          "Table 6. Development set performances of": "3",
          "the submitted systems for": "Sys1 + Best Audio + Best Functional (DWF)",
          "the EXPR challenge. DWF: Dirichlet-based Random Weighted": "0.457"
        },
        {
          "Table 6. Development set performances of": "4",
          "the submitted systems for": "Sys1 + Best Audio + Best Functional (RF)",
          "the EXPR challenge. DWF: Dirichlet-based Random Weighted": "0.999"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: Development set performances of the submitted systems for the EXPR challenge. DWF: Dirichlet-based Random Weighted",
      "data": [
        {
          "4\nAudio-visual": "set performances of the VA challenge, whereas Table 6 re-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "References"
        },
        {
          "4\nAudio-visual": "ports the corresponding development set performances of",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and"
        },
        {
          "4\nAudio-visual": "the EXPR challenge submissions. From 6, we see that\nthe",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Michael Auli. wav2vec 2.0: A framework for self-supervised"
        },
        {
          "4\nAudio-visual": "RF-based fusion overfits, even though a small number of",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "learning of speech representations. Advances in neural infor-"
        },
        {
          "4\nAudio-visual": "trees (10) is used for fusion. For the sake of completeness,",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "mation processing systems, 33:12449–12460, 2020. 2"
        },
        {
          "4\nAudio-visual": "we submit predictions using both RF and DWF late fusion",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[2]\nPınar Baki, Heysem Kaya, Elvan C¸ iftc¸i, H¨useyin G¨ulec¸, and"
        },
        {
          "4\nAudio-visual": "schemes.",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Albert Ali Salah.\nA multimodal approach for mania level"
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "IEEE Transactions on Affec-\nprediction in bipolar disorder."
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "tive Computing, 13(4):2119–2131, 2022. 3"
        },
        {
          "4\nAudio-visual": "5. Conclusion and Future Work",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[3] Leo Breiman.\nBagging predictors. Machine learning, 24:"
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "123–140, 1996. 4"
        },
        {
          "4\nAudio-visual": "The results of our research highlight\nthe potential of deep",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[4] Aayushi Chaudhari, Chintan Bhatt, Achyut Krishna,\nand"
        },
        {
          "4\nAudio-visual": "learning models for audiovisual emotion recognition in un-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Pier Luigi Mazzeo. Vitfer:\nfacial emotion recognition with"
        },
        {
          "4\nAudio-visual": "constrained,\n”in-the-wild” settings.\nThe face-based end-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "vision transformers.\nApplied System Innovation, 5(4):80,"
        },
        {
          "4\nAudio-visual": "to-end dynamic models,\nleveraging salient embeddings ex-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "2022. 2"
        },
        {
          "4\nAudio-visual": "tracted by Visual Transformer and EfficientNet-B1 model,",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[5]\nJiankang Deng,\nJia Guo, Y Zhou,\nJ Yu,\nI Kotsia,\nand S"
        },
        {
          "4\nAudio-visual": "achieved a competitive efficacy, outperforming the tradi-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Zafeiriou. Retinaface: Single-stage dense face localisation"
        },
        {
          "4\nAudio-visual": "tional functional-based approaches on the AffWild2 dataset.",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "in the wild. arxiv 2019.\narXiv preprint arXiv:1905.00641,"
        },
        {
          "4\nAudio-visual": "However, optimizing video models still remains computa-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "1905. 4"
        },
        {
          "4\nAudio-visual": "tionally very demanding, posing additional challenges\nin",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[6]\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,"
        },
        {
          "4\nAudio-visual": "deploying these solutions for ”in-the-wild” scenarios.",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "and Li Fei-Fei.\nImagenet: A large-scale hierarchical\nimage"
        },
        {
          "4\nAudio-visual": "While our experiments on the development set suggested",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "database. In 2009 IEEE Conference on Computer Vision and"
        },
        {
          "4\nAudio-visual": "that combining audio and video modalities through fusion",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Pattern Recognition, pages 248–255, 2009. 2"
        },
        {
          "4\nAudio-visual": "techniques could significantly enhance performance, we do",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[7]\nJiankang Deng, Jia Guo, Evangelos Ververas,\nIrene Kotsia,"
        },
        {
          "4\nAudio-visual": "not have an opportunity to draw definitive conclusions about",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "and Stefanos Zafeiriou. RetinaFace: Single-shot multi-level"
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "face localisation in the wild.\nIn 2020 IEEE/CVF Conference"
        },
        {
          "4\nAudio-visual": "the advantages of multi-modal fusion approaches for the 6th",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "on Computer Vision and Pattern Recognition (CVPR), pages"
        },
        {
          "4\nAudio-visual": "ABAW challenge yet, as\nthe test\nset\nresults have not yet",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "5202–5211, 2020. 4"
        },
        {
          "4\nAudio-visual": "been revealed.",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[8] Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon,"
        },
        {
          "4\nAudio-visual": "Looking\nahead,\nfurther\nresearch\ncould\nexplore\nother",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "et al.\nCollecting large,\nrichly annotated facial-expression"
        },
        {
          "4\nAudio-visual": "novel fusion strategies that take into account additional con-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "databases from movies.\nIEEE multimedia, 19(3):34, 2012."
        },
        {
          "4\nAudio-visual": "textual information such as background sound, participants’",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "4"
        },
        {
          "4\nAudio-visual": "postures and gestures,\nlinguistics, and other valuable com-",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,"
        },
        {
          "4\nAudio-visual": "plementary sources of\ninformation.\nUltimately, progress",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": ""
        },
        {
          "4\nAudio-visual": "",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Dirk Weissenborn,\nXiaohua\nZhai,\nThomas Unterthiner,"
        },
        {
          "4\nAudio-visual": "in this field holds promise for enhancing human-computer",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-"
        },
        {
          "4\nAudio-visual": "interaction and enabling a natural unconstrained usage of",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is"
        },
        {
          "4\nAudio-visual": "computer systems and robots in human society.",
          "Sys1 + Best Audio + Best Functional (RF)\n0.999": "worth 16x16 words: Transformers for image recognition at"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "scale.\nIn International Conference on Learning Representa-": "tions, 2021. 2, 5",
          "ings of\nthe IEEE/CVF International Conference on Com-": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[10] Denis Dresvyanskiy, Elena Ryumina, Heysem Kaya, Maxim",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[23] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Markitantov, Alexey Karpov, and Wolfgang Minker. End-to-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "end modeling and transfer learning for audiovisual emotion",
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "recognition in-the-wild. Multimodal Technologies and Inter-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv:1910.11111, 2019."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "action, 6(2), 2022. 2, 3, 4, 5",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[24] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[11]\nIan J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Courville, Mehdi Mirza,\nBen Hamner, Will Cukierski,",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Chal-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "lenges\nin representation learning: A report on three ma-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "chine learning contests.\nIn Neural Information Processing:",
          "ings of\nthe IEEE/CVF International Conference on Com-": "pages 1–23, 2019."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "20th International Conference,\nICONIP 2013, Daegu, Ko-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[25] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "rea, November 3-7, 2013. Proceedings, Part\nIII 20, pages",
          "ings of\nthe IEEE/CVF International Conference on Com-": "affective behavior\nin the first abaw 2020 competition.\nIn"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "117–124. Springer, 2013. 4, 5",
          "ings of\nthe IEEE/CVF International Conference on Com-": "2020\n15th\nIEEE International Conference\non Automatic"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[12] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Face and Gesture Recognition (FG 2020)(FG), pages 794–"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "800, 2020."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "dreetto, and Hartwig Adam. MobileNets: Efficient convolu-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[26] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "tional neural networks for mobile vision applications. CoRR,",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "abs/1704.04861, 2017. 4",
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[13] Guang-Bin Huang, Hongming Zhou, Xiaojian Ding,\nand",
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv:2105.03790, 2021."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Rui Zhang. Extreme Learning Machine for Regression and",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[27] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Systems, Man, and Cybernetics,\nMulticlass Classification.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Cowen,\nand Stefanos Zafeiriou.\nABAW: Valence-arousal"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Part B: Cybernetics, IEEE Transactions on, 42(2):513–529,",
          "ings of\nthe IEEE/CVF International Conference on Com-": "estimation, expression recognition, action unit detection &"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "2012. 3",
          "ings of\nthe IEEE/CVF International Conference on Com-": "emotional reaction intensity estimation challenges.\nIn Pro-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[14]\nPhilip Jackson and SJUoSG Haq.\nSurrey audio-visual ex-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "ceedings of\nthe IEEE/CVF Conference on Computer Vision"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "University of Surrey:\npressed emotion (SAVEE) database.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "and Pattern Recognition, pages 5888–5897, 2023. 1"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Guildford, UK, 2014. 4, 5",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[28] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[15] Heysem Kaya and Alexey A. Karpov.\nIntroducing Weighted",
          "ings of\nthe IEEE/CVF International Conference on Com-": "fanos Zafeiriou, Chunchang Shao, and Guanyu Hu. The 6th"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Kernel Classifiers\nfor Handling Imbalanced Paralinguistic",
          "ings of\nthe IEEE/CVF International Conference on Com-": "affective behavior analysis in-the-wild (abaw) competition."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Corpora: Snoring, Addressee and Cold. In Proc. Interspeech",
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv preprint arXiv:2402.19344, 2024. 1, 4"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "2017, pages 3527–3531, 2017. 3",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[29]\nJean Kossaifi, Georgios Tzimiropoulos, Sinisa Todorovic,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[16] Heysem Kaya,\nFurkan G¨urpınar,\nand Albert Ali\nSalah.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "and Maja Pantic. Afew-va database for valence and arousal"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Video-based emotion recognition in the wild using deep",
          "ings of\nthe IEEE/CVF International Conference on Com-": "estimation in-the-wild. Image and Vision Computing, 65:23–"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Image and Vision Com-\ntransfer learning and score fusion.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "36, 2017. 4, 5"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "puting, 65:66–75, 2017. Multimodal Sentiment Analysis and",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[30]\nJean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Mining in the Wild Image and Vision Computing. 4",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Maximilian Schmitt, Fabien Ringeval,\nJing Han, Vedhas"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[17] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Pandit, Antoine Toisoul, Bj¨orn Schuller,\net al.\nSewa db:"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "ings of\nthe IEEE/CVF International Conference on Com-": "A rich database for audio-visual emotion and sentiment re-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of",
          "ings of\nthe IEEE/CVF International Conference on Com-": "IEEE Transactions on Pattern Analysis\nsearch in the wild."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "ference on Computer Vision and Pattern Recognition, pages",
          "ings of\nthe IEEE/CVF International Conference on Com-": "and Machine Intelligence, 43(3):1022–1040, 2019. 4, 5"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "2328–2336, 2022. 1",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[31] Ronak Kosti, Jose M. Alvarez, Adria Recasens, and Agata"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[18] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Lapedriza. EMOTIC: Emotions in context dataset.\nIn Pro-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "multi-task learning challenges.\nIn European Conference on",
          "ings of\nthe IEEE/CVF International Conference on Com-": "ceedings of\nthe IEEE Conference on Computer Vision and"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Computer Vision, pages 157–172. Springer, 2023.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Pattern Recognition (CVPR) Workshops, 2017. 4, 5"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[19] Dimitrios Kollias. Multi-label compound expression recog-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[32]\nI Lawrence and Kuei Lin. A concordance correlation coeffi-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "nition:\nC-expr database & network.\nIn Proceedings of",
          "ings of\nthe IEEE/CVF International Conference on Com-": "cient to evaluate reproducibility. Biometrics, pages 255–268,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "the IEEE/CVF Conference on Computer Vision and Pattern",
          "ings of\nthe IEEE/CVF International Conference on Com-": "1989. 6"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "Recognition, pages 5589–5598, 2023.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[33]\nShan Li, Weihong Deng, and JunPing Du. Reliable crowd-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[20] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,",
          "ings of\nthe IEEE/CVF International Conference on Com-": "sourcing and deep locality-preserving learning for expres-"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "action unit\nrecognition: Aff-wild2, multi-task learning and",
          "ings of\nthe IEEE/CVF International Conference on Com-": "the IEEE\nsion recognition in the wild.\nIn Proceedings of"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "arcface. arXiv preprint arXiv:1910.04855, 2019.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Conference on Computer Vision and Pattern Recognition"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[21] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis",
          "ings of\nthe IEEE/CVF International Conference on Com-": "(CVPR), 2017. 4, 5"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "in-the-wild: Valence-arousal, expressions, action units and a",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[34] Zheng Lian, Bin Liu, and Jianhua Tao. Ctnet: Conversational"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "unified framework. arXiv preprint arXiv:2103.15792, 2021.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "IEEE/ACM\ntransformer network for emotion recognition."
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "[22] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Transactions on Audio, Speech, and Language Processing,"
        },
        {
          "scale.\nIn International Conference on Learning Representa-": "tive behavior in the second abaw2 competition.\nIn Proceed-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "29:985–1000, 2021. 2"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-",
          "and Pattern Recognition Workshops\nIn Computer Vision": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Medi-",
          "and Pattern Recognition Workshops\nIn Computer Vision": "IEEE, 2017. 1"
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "apipe: A framework for building perception pipelines. arXiv",
          "and Pattern Recognition Workshops\nIn Computer Vision": "[47] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou"
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "preprint arXiv:1906.08172, 2019. 4",
          "and Pattern Recognition Workshops\nIn Computer Vision": "Tang. From facial expression recognition to interpersonal re-"
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[36] Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and",
          "and Pattern Recognition Workshops\nIn Computer Vision": "lation prediction.\nInternational Journal of Computer Vision,"
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Sanjiv Kumar. Does label smoothing mitigate label noise?",
          "and Pattern Recognition Workshops\nIn Computer Vision": "126:550–569, 2018. 4, 5"
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "In International Conference on Machine Learning,\npages",
          "and Pattern Recognition Workshops\nIn Computer Vision": "[48] Weiwei\nZong,\nGuang-Bin Huang,\nand Yiqiang\nChen."
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "6448–6458, 2020. 4",
          "and Pattern Recognition Workshops\nIn Computer Vision": "Weighted extreme learning machine for imbalance learning."
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[37] Gary McKeown, Michel Valstar, Roddy Cowie, Maja Pantic,",
          "and Pattern Recognition Workshops\nIn Computer Vision": "Neurocomputing, 101:229 – 242, 2013. 3"
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "and Marc Schroder. The semaine database: Annotated multi-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "modal records of emotionally colored conversations between",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "a person and a limited agent. IEEE Transactions on Affective",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Computing, 3(1):5–17, 2011. 4, 5",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[38] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "hoor. Affectnet: A database for facial expression, valence,",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "IEEE Transactions on\nand arousal computing in the wild.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Affective Computing, 10(1):18–31, 2017. 4, 5",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[39]\nFabien Ringeval, Andreas Sonderegger, Juergen Sauer, and",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Denis Lalanne.\nIntroducing the recola multimodal corpus of",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "remote collaborative and affective interactions.\nIn 2013 10th",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "IEEE International Conference and Workshops on Automatic",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Face and Gesture Recognition (FG), pages 1–8. IEEE, 2013.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "4, 5",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Aditya Khosla, Michael Bernstein,\net al.\nImagenet\nlarge",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "International Journal of\nscale visual recognition challenge.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Computer Vision, 115:211–252, 2015. 2",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[41] Mingxing Tan and Quoc V. Le.\nEfficientNet: Rethinking",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "model\nscaling for convolutional neural networks.\nIn Pro-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "ceedings of\nthe 36th International Conference on Machine",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Learning,\nICML 2019, 9-15 June 2019, Long Beach, Cali-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "fornia, USA, pages 6105–6114. PMLR, 2019. 2, 5",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[42]\nJohn W Tukey. The measurement of power spectra: from the",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "point of view of communications engineering. Dover, 1958.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "6",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Polosukhin. Attention is all you need.\nIn Advances in Neu-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "ral Information Processing Systems. Curran Associates, Inc.,",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "2017. 3",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[44]\nJohannes Wagner, Andreas Triantafyllopoulos, Hagen Wier-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "storf, Maximilian Schmitt, Felix Burkhardt, Florian Eyben,",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "and Bj¨orn W. Schuller. Dawn of the transformer era in speech",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "IEEE Trans-\nemotion recognition: Closing the valence gap.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "actions on Pattern Analysis and Machine Intelligence, 45(9):",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "10745–10759, 2023. 2",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[45]\nJohannes Wagner, Andreas Triantafyllopoulos, Hagen Wier-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "storf, Maximilian Schmitt, Felix Burkhardt, Florian Eyben,",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "and Bj¨orn W Schuller. Dawn of the transformer era in speech",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "IEEE Trans-\nemotion recognition: closing the valence gap.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "actions on Pattern Analysis and Machine Intelligence, 2023.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "2",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "[46]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        },
        {
          "[35] Camillo Lugaresi,\nJiuqiang Tang, Hadon Nash, Chris Mc-": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge.",
          "and Pattern Recognition Workshops\nIn Computer Vision": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "A multimodal approach for mania level prediction in bipolar disorder",
      "authors": [
        "Pınar Baki",
        "Heysem Kaya",
        "Hüseyin Gülec",
        "Albert Salah"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Bagging predictors",
      "authors": [
        "Leo Breiman"
      ],
      "year": "1996",
      "venue": "Machine learning"
    },
    {
      "citation_id": "4",
      "title": "Vitfer: facial emotion recognition with vision transformers",
      "authors": [
        "Aayushi Chaudhari",
        "Chintan Bhatt",
        "Achyut Krishna",
        "Pier Mazzeo"
      ],
      "year": "2022",
      "venue": "Applied System Innovation"
    },
    {
      "citation_id": "5",
      "title": "Single-stage dense face localisation in the wild. arxiv 2019",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Y Zhou",
        "J Yu",
        "I Kotsia",
        "S Zafeiriou",
        "Retinaface"
      ],
      "year": "1905",
      "venue": "Single-stage dense face localisation in the wild. arxiv 2019",
      "arxiv": "arXiv:1905.00641"
    },
    {
      "citation_id": "6",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "RetinaFace: Single-shot multi-level face localisation in the wild",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Evangelos Ververas",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "8",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "9",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "End-toend modeling and transfer learning for audiovisual emotion recognition in-the-wild",
      "authors": [
        "Denis Dresvyanskiy",
        "Elena Ryumina",
        "Heysem Kaya",
        "Maxim Markitantov",
        "Alexey Karpov",
        "Wolfgang Minker"
      ],
      "year": "2005",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "11",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "12",
      "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Andrew Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient convolutional neural networks for mobile vision applications"
    },
    {
      "citation_id": "13",
      "title": "Extreme Learning Machine for Regression and Multiclass Classification. Systems, Man, and Cybernetics, Part B: Cybernetics",
      "authors": [
        "Guang-Bin Huang",
        "Hongming Zhou",
        "Xiaojian Ding",
        "Rui Zhang"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on"
    },
    {
      "citation_id": "14",
      "title": "Surrey audio-visual expressed emotion (SAVEE) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (SAVEE) database"
    },
    {
      "citation_id": "15",
      "title": "Introducing Weighted Kernel Classifiers for Handling Imbalanced Paralinguistic Corpora: Snoring, Addressee and Cold",
      "authors": [
        "Heysem Kaya",
        "Alexey Karpov"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "Heysem Kaya",
        "Furkan Gürpınar",
        "Albert Salah"
      ],
      "year": "2017",
      "venue": "Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "21",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "22",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceed-ings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "24",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "26",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "27",
      "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "29",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "30",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "EMOTIC: Emotions in context dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "32",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "Kuei Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "33",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "Camillo Lugaresi",
        "Jiuqiang Tang",
        "Hadon Nash",
        "Chris Mc-Clanahan",
        "Esha Uboweja",
        "Michael Hays",
        "Fan Zhang",
        "Chuo-Ling Chang",
        "Ming Yong",
        "Juhyun Lee"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "36",
      "title": "Does label smoothing mitigate label noise",
      "authors": [
        "Michal Lukasik",
        "Srinadh Bhojanapalli",
        "Aditya Menon",
        "Sanjiv Kumar"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "40",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "V Quoc",
        "Le",
        "Efficientnet"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "42",
      "title": "The measurement of power spectra: from the point of view of communications engineering",
      "authors": [
        "John Tukey"
      ],
      "year": "1958",
      "venue": "The measurement of power spectra: from the point of view of communications engineering"
    },
    {
      "citation_id": "43",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "47",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "48",
      "title": "Weighted extreme learning machine for imbalance learning",
      "authors": [
        "Weiwei Zong",
        "Guang-Bin Huang",
        "Yiqiang Chen"
      ],
      "year": "2013",
      "venue": "Neurocomputing"
    }
  ]
}