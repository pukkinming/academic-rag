{
  "paper_id": "2509.18706v1",
  "title": "M4Ser: Multimodal, Multirepresentation, Multitask, And Multistrategy Learning For Speech Emotion Recognition",
  "published": "2025-09-23T06:45:07Z",
  "authors": [
    "Jiajun He",
    "Xiaohan Shi",
    "Cheng-Hung Hu",
    "Jinyi Mi",
    "Xingfeng Li",
    "Tomoki Toda"
  ],
  "keywords": [
    "speech emotion recognition",
    "multimodal fusion",
    "modality-invariant representations Modality Discriminator Classification Contrastive Learning Positive Negative Cross-modal Interaction Learning Contextual Information Learning Concatenation Classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human-machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on humanannotated text data, ASR errors possibly degrade emotion recognition performance. To address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modalityspecific and modality-invariant representations across different modalities. Building on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features. We refer to our proposed method as M 4 SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Multimodal Ser",
      "text": "SER aims to identify the speaker's emotional state from spoken utterances. Early SER models primarily rely on audio signals, extracting prosodic and acoustic features such as Melfrequency cepstral coefficients, filter banks, or other handcrafted descriptors  [14] . With the advent of deep learning, models based on RNNs, CNNs and Transformer architectures  [15] [16] [17] [18]  have achieved significant improvements by modeling complex temporal and hierarchical patterns in speech. More recently, the rise of SSL has led to the development of speech-based pretrained models such as wav2vec  [3] , HuBERT  [4] , and WavLM  [5] , which provide rich contextual representations and deliver SOTA performance on various SER benchmarks  [19] .\n\nIn parallel, text-based SER has emerged as a complementary paradigm, where textual transcripts-either manually curated or obtained via ASR-serve as input for emotion prediction. These approaches benefit from high-level semantic understanding and often leverage contextual modeling techniques such as RNNs or graph neural networks (GNNs)  [20] . However, textual information alone lacks prosodic and paralinguistic cues, limiting its effectiveness in capturing nuanced emotional states.\n\nTo address these limitations, multimodal SER often integrates both speech and text modalities, capturing both acoustic and semantic cues. A common setting involves using speech signals alongside textual transcripts (typically from manual annotations or ASR outputs)  [21] [22] [23] [24] . Fan et al.  [21]  proposed multi-granularity attention-based transformers (MGAT) to address emotional asynchrony and modality misalignment. Sun et al.  [22]  introduced a method integrating shared and private encoders, projecting each modality into a separate subspace to capture modality consistency and diversity while ensuring label consistency in the output. These methods typically assume access to high-quality manual transcriptions for the text modality.\n\nRecently, growing efforts have focused on reducing the dependency on annotated text by directly leveraging ASR hypotheses as input  [9, [25] [26] [27] [28] . Santoso et al.  [25]  introduced confidence-aware self-attention mechanisms that downweight unreliable ASR tokens to mitigate error propagation. Lin and Wang  [9]  proposed a robust multimodal SER framework that adaptively fuses attention-weighted acoustic representations with ASR-derived text embeddings, compensating for recognition noise in the transcript.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multirepresentation: Modality-Specific And Modalityinvariant Representations",
      "text": "Learning modality-specific representations (MSRs) and modality-invariant representations (MIRs) has proven effective for multimodal SER, as it enables models to capture both shared semantics across modalities and complementary modality-unique cues. Hazarika et al.  [10]  proposed MISA, which disentangles each modality into invariant and specific subspaces through factorization, facilitating effective fusion for emotion prediction. Liu et al.  [29]  proposed a modalityrobust MER framework that introduces a contrastive learning module to extract MIRs from full-modality inputs, and an imagination module that reconstructs missing modalities based on the MIRs, enhancing robustness under incomplete input conditions. To explicitly reduce distribution gaps and mitigate feature redundancy, Yang et al.  [11]  proposed FD-MER, which extracts both MSRs and MIRs, combined with consistency-disparity constraints and a modality discriminator to encourage disentangled learning. Liu et al.  [30]  presented NORM-TR, introducing a noise-resistant generic feature (NRGF) extractor trained with noise-aware adversarial objectives, followed by a multimodal transformer that fuses NRGFs with raw modality features for robust representation learning.\n\nIn contrast to factorization-based approaches which explicitly disentangle modality inputs into shared and private spaces, our M 4 SER framework adopts a structured encoding-generation strategy to implicitly model MSRs and MIRs. Specifically, we utilize a stack of cross-modal encoder blocks to extract token-aware and speech-aware features (MSRs), and then apply a dedicated generator composed of hybridmodal attention and masking mechanisms to derive the MIRs. In addition, unlike prior works that incorporate adversarial learning at a global or representation-level, our M 4 SER employs a frame-level discriminator that operates on each temporal representation. A frame-level discriminator attempts to distinguish the modality origin of each temporal representation, while the MIR generator is optimized to deceive the discriminator by producing modality-agnostic outputs that are hard to classify as either text or speech. This fine-grained, perframe adversarial constraint leads to more robust and aligned cross-modal representations. Moreover, M 4 SER integrates this disentanglement framework into a multitask learning setup with ASR-aware auxiliary supervision, enabling more effective adaptation to noisy real-world speech-text scenarios.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Multitask: Asr Error Detection And Correction",
      "text": "ASR error correction (AEC) techniques remain effective in optimizing the ASR hypotheses. AEC has been jointly trained with ER tasks  [9, 13, 27]  in multitask settings. Li et al.  [27]  proposed an AEC method that improves transcription quality on low-resource out-of-domain data through cross-modal training and the incorporation of discrete speech units, and they validated its effectiveness in downstream ER tasks. Lin and Wang  [9]  introduced a robust ER method that leverages complementary semantic information from audio, adapts to ASR errors through an auxiliary task for ASR error detection, and fuses text and acoustic representations for ER. On the basis of  [31] [32] [33] , we propose a partially autoregressive AEC method that uses a label predictor to restrict decoding to only desirable parts of the input sequence embeddings, thereby reducing inference latency. D. Multistrategy Learning 1) Adversarial Network: The concept of adversarial network was first introduced using GAN  [34] , which rapidly attracted extensive research interest owing to their strong capability to generate high-quality novel samples on the basis of existing data. As research progressed, the applications of GAN expanded to multimodal tasks. In a multimodal SER task, Ren et al.  [35]  combined speaker characteristics with single-modal features using an adversarial module to capture both the commonality and diversity of single-modal features, and they finally fused different modalities to generate refined utterance representations for emotion classification.\n\n2) Supervised Contrastive Learning: In recent years, to fully utilize supervised information, some researchers have proposed supervised contrastive learning (SCL)  [36] , which leverages label information to construct positive pairs, making the distance between samples of the same class closer than that between samples of different classes. Zhang et al.  [37]  introduced label embeddings to better understand the language. Li et al.  [38]  unified the target setting on a hypersphere and forced the data representations to be close to these targets. Zhu et al.  [39]  regarded classifier weights as prototypes in the representation space and incorporated them into the contrastive loss.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology A. Problem Formulation",
      "text": "Our proposed M 4 SER framework can be formalized as the function f (S, T ) = (L, Z). Here, the speech modality S = (s 1 , s 2 , • • • , s m ) ∈ R m consists of m frames extracted from an utterance, whereas the text modality T = (t 1 , t 2 , • • • , t n ) ∈ R n comprises n tokens from the original ASR hypotheses of the utterance. All tokens are mapped to a predefined WordPiece vocabulary  [40] . Additionally, within our multitask learning framework, the primary task is ER, yielding an emotion classification output L ∈ {l 1 , l 2 , • • • , l e }, where e represents the number of emotional categories. Concurrently, the auxiliary tasks include ASR error detection (AED) and ASR error correction (AEC). The output of these auxiliary tasks is Z ∈ {Z 1 , Z 2 , • • • , Z k }, representing all the ground truth (GT) sequences where the ASR transcriptions differ from the human-annotated transcriptions.  Meaning of the icons: AEC module, MF module, and ER module, as shown in Fig.  2 . In the following section, we provide a detailed explanation of each module.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Embedding Module",
      "text": "Our embedding module consists of acoustic and token embeddings, as illustrated in Fig.  2(a) .\n\nContextual Speech Representations. To obtain comprehensive contextual representations of acoustic features, we utilize a pretrained SSL model, HuBERT  [4] , as our acoustic encoder. HuBERT integrates CNN layers with a transformer encoder to effectively capture both speech features and contextual information. We denote the acoustic hidden representations of the speech modality inputs S generated by HuBERT as H S = (h\n\nwhere d is the dimension of hidden representations.\n\nContextual Token Representations. We use the pretrained language model BERT  [6]  as our text encoder to obtain the token representations H T = (h\n\nwhere TE(•) and PE(•) denote the token and position embeddings, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Asr Error Detection (Aed) Module",
      "text": "The first subtask we introduce is AED, which is designed to detect the positions of ASR errors. Similarly to  [31] , we align T and C by determining the longest common subsequence between them. The aligned tokens are labeled KEEP (K), whereas the remaining tokens are labeled DELETE (D) or CHANGE (C). A specific example is illustrated in Fig.  2(b ). The label prediction layer is a straightforward fully connected (FC) layer with three classes.\n\nwhere\n\nThe label prediction loss is then defined as the negative loglikehood.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Asr Error Correction (Aec) Module",
      "text": "We introduce a second subtask called AEC, which aims to correct the errors determined by AED. Unlike conventional autoregressive decoders that start decoding from scratch, our decoder operates in parallel to the tokens predicted as C.\n\nMore specifically, once the erroneous positions are identified, the decoder constructs a separate generation sequence for each token labeled as C. Each sequence begins with a special start token <BOS> and generates the corrected text in an autoregressive manner. At each decoding step, the decoder input for a given correction sequence consists of all previously generated tokens, each concatenated with the representation of the corresponding erroneous token. All correction sequences are processed in parallel by a shared transformer decoder, which attends to the full ASR representation H T as memory.\n\nFor the k th C position, the generated sequence of length c by the transformer decoder can be represented as\n\n) , where z 1,k is initialized by a special start token <BOS>. We compute the decoder input embeddings for the first t steps as follows:\n\nwhere \"∥\" denotes a concatenation operation along the feature dimension. Here, h (k) T is the representation of the k th erroneous token, repeated at each decoding position 1:t. Then, a generic transformer decoder  [41]  is applied to obtain the decoder layer output, where the query is H\n\nwhere O (gen) t+1,k is the decoder layer output. Finally, the generation output is calculated as\n\nwhere d vocab is the size of the vocabulary of BERT. Therefore, the next generated token is z t+1,k = argmax(P (gen) t+1,k ). Finally, the transformer-based generation loss is defined as",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Multimodal Fusion (Mf) Module",
      "text": "On the basis of  [13] , our MF module is composed of three cross-modal encoder (CME) blocks and one MIR generator. The objective is to facilitate the learning of modality-specific and modality-invariant representations. In this section, we provide an in-depth explanation of the operation of each CME block and the MIR generator.\n\nCME is structured akin to a standard transformer layer, featuring an h-head cross-attention module  [42] , residual connections, and FC layers, as shown in Fig.  3 . To acquire tokenaware speech representations, we first feed H T as the query and H S as the key and value into a CME block:\n\nwhere Cross-Attn is the cross-attention calculation. Next, to address the issue where each representation generated in",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Linear",
      "text": "Cross-Attention the previous block corresponds to token embeddings rather than acoustic embeddings, we input Ĥ(spe) S into another CME module. In this block, the original H S serves as the query and Ĥ(spe) S serves as the key and value. Ultimately, we obtain the final token-aware speech representation:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "S",
      "text": ")) ∈ R m×d . (9) Similarly, we feed H S as the query and H T as the key and value into a CME block to obtain speech-aware token representations:\n\nThe MIR generator utilizes a hybrid-modal attention (HMA) module to extract the shared information from each modality-specific representation that pertains to both modalities, as shown in Fig.  4(a) :  (11)  where i denotes either the speech or text modality and H ST represents the concatenation of H S and H T , which is then fed into a FC layer to map it back to the same dimension as H S . This process integrates both speech and text information into a unified representation. The resulting features are subsequently summed with H ST , culminating in the ultimate modalityinvariant representation:\n\nwhere \"Norm\" represents layer normalization  [43]  and \"Conv 1d \" denotes 1×1 convolution followed by PReLU activation  [44] . We uniformly express Eqs.  (11)  and  (12)  as\n\nwhere G is the MIR generator.\n\nHMA initiates with a cross-attention layer aimed at extracting the shared information from each modality-specific representation pertinent to both modalities, as shown in Fig.  4(b) :\n\nConcat. Sigmoid Element-wise multi. Element-wise add. To enhance the feature's modality invariance, a parallel convolutional network is employed to learn a mask that filters out modality-specific information:\n\nwhere \"σ\" denotes Sigmoid activation and \"⊗\" indicates element-wise multiplication. Finally, the modality-specific and modality-invariant representations are concatenated together to obtain the final multimodal fusion representation H\n\nST :",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Emotion Recognition (Er) Module",
      "text": "Emotion classification is performed by applying the temporal average pooling layer to the output feature H (fus) ST of the MF module, followed by an FC layer and a SoftMax activation function.\n\nwhere y emo is the predicted emotion classification and e is the number of emotion categories. The corresponding loss function can be defined as",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "G. Modality Discriminator",
      "text": "We further develop the modality discriminator D utilizing the modality-invariant representations produced by the MIR generator. This discriminator illustrated in Fig.  2 (f) is composed of two linear layers with a ReLU activation function in between, followed by a Sigmoid activation function. To enhance the MIR's capability to remain modality-agnostic, we employ adversarial learning techniques. Specifically, the discriminator D outputs a scalar value between 0 and 1 for each frame, where values close to 0 indicate the text modality, values close to 1 indicate the speech modality, and values near 0.5 represent an ambiguous modality. Here,\n\nST }. On one hand, we aim for the discriminator that correctly classifies frames in the modality-specific representations H\n\nwhere\n\n, H ST ) is the output of the MIR generator as shown in Eq. (  13 ) and E denotes the expectation over all temporal frames in a batch.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "H. Label-Based Contrastive Learning (Lcl)",
      "text": "To enhance the model's capability to learn emotion features from multimodal data, we employ a label-based contrastive learning task to complement the cross-entropy loss, as shown in Fig.  2(g ). This task aids the model in extracting emotionrelated features when the MF module integrates speech and text data. As depicted in the LCL task in Fig.  5 , we categorize data in each batch into positive and negative samples on the basis of emotion labels. For instance, in a batch containing eight samples, we compute the set of positive samples for each sample, where those with the same label are considered positive samples (yellow squares), and those with different labels are considered negative samples (white squares). We then calculate the label-based contrastive loss (LCL) using Eq. 21, which promotes instances of a specific label to be more similar to each other than instances of other labels. ST ) a represent the i th sample, the p th positive sample, and the a th sample, respectively. We define I as the index set of samples, P (i) as the set of positive samples for the i th sample, and A(i) as the set of all samples.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "I. Joint Training",
      "text": "During the training stage, the learning process is optimized using three loss functions that correspond to ER, AED, and AEC (i.e., L ER , L AED , and L AEC ), and two training strategies (i.e., L GAN and L LCL ). The specific optimization process of M 4 SER is detailed in Alg. 1. These five loss functions are linearly combined as the overall training objective of M 4 SER:\n\nwhere α is the hyperparameter for adjusting the weight between the main task and the auxiliary tasks, and β is the hyperparameter for adjusting the weight between the two auxiliary tasks, and γ, λ are the hyperparameters for balancing different training strategies.\n\nFollowing the GAN training strategy  [34] , we divide the backpropagation process into two steps. Firstly, we maximize L GAN to update the discriminator, during which the generator is detached from the optimization. According to Eq. (  20 ), on one hand, maximizing the first term L D of L GAN essentially trains the discriminator to correctly distinguish between text and audio features by making H (spe) S close to 1 and H (spe) T close to 0 1  . On the other hand, maximizing the second term L G (i.e., minimizing -L G ) will make H (inv)  ST approach 0 or 1  2  , indicating that the discriminator recognizes H (inv)  ST as modality-specific, which can be either text or audio, contrary to our desired modality invariance. Secondly, we freeze the discriminator parameters and update the remaining parameters by minimizing L G , which makes the output of the discriminator D for the modality-invariant features H (inv)  ST approach 0.5, thereby blurring these features between audio and text modalities to achieve modality agnosticism. Additionally, L ER optimizes the downstream emotion recognition model, L AED and L AEC optimize the auxiliary tasks of ASR error detection and ASR error correction models, respectively, and L LCL implements the LCL strategy. The entire system is trained in an end-to-end manner and optimized by adjusting weight parameters.\n\nDuring the inference stage, the AED and AEC modules are excluded. The remaining modules accept speech and ASR transcripts as input and output emotion classification results. for (S, T ) ∈ D do 7:",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Forward Propagation:",
      "text": "8:\n\n10:\n\nTraining Objectives: Update the Rest Network:\n\nend for 27: end while IV. EXPERIMENTAL SETUP In this section, we present the datasets, evaluation metrics, and implementation details used in our paper.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Dataset And Evaluation Metrics",
      "text": "To investigate the effectiveness of our proposed model, we carried out experiments on two public datasets: the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [45]  and the Multimodal Emotion Lines Dataset (MELD)  [46] . The statistics of the two datasets are shown in Fig.  6 .\n\nIEMOCAP comprised roughly 12 hours of speech from ten speakers participating in five scripted sessions. Following prior work  [9, 47]  on IEMOCAP, we employed 5531 utterances that were categorized into four emotion categories: \"Neutral\" (1,708), \"Angry\" (1,103), \"Happy\" (including \"Excited\") (595 + 1,041 = 1,636), and \"Sad\" (1,084). We performed five-fold  leave-one-session-out (LOSO) cross-validation to evaluate the emotion classification performance using weighted accuracy (WA) and unweighted accuracy (UA). MELD included a total of 13708 samples extracted from the TV series Friends, divided into 9,989 for training, 1,109 for validation, and 2,610 for testing. The dataset contained seven labels: \"Neutral\" (6167), \"Happy\" (2198), \"Fear\" (350), \"Sad\" (985), \"Disgust\" (355), \"Angry\" (1541), and \"Surprise\" (1499). We tuned hyperparameters on the validation set and reported final results on the test set using the best checkpoint, evaluated by accuracy (ACC) and weighted F1-score (W-F1).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Our method was implemented using Python 3.10.0 and Pytorch 1.11.0. The model was trained and evaluated on a system equipped with an Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, 32GB RAM, and one NVIDIA Tesla V100 GPU. The detailed parameter settings are presented in Table  I .\n\nThe acoustic encoder was initialized using the hubert-base-ls960 3  model, producing acoustic representations d S with a dimensionality of 768. The text encoder employed the bert-base-uncased 4 model for initialization, yielding token representations d T with a dimensionality of 768. The vocabulary size for word tokenization d vocab was set to 30,522. We set the hidden size d to 768, the number of attention layers to 12, and the number 3 https://huggingface.co/facebook/hubert-base-ls960 4 https://huggingface.co/google-bert/bert-base-uncased of attention heads to 12. Both the HuBERT and BERT models were fine-tuned during the training stage. The transformer decoder in Fig.  2 (c) adopted a single-layer transformer with a hidden size of 768. Additionally, we obtained corresponding ASR hypotheses using the Whisper ASR model  [48] . We used the openai/whisper-medium.en 5  and openai/whisper-large-v2 6  models, achieving word error rates (WERs) of 20.48% and 37.87% on the IEMOCAP and MELD datasets, respectively.\n\nWe used Adam  [49]  as the optimizer with a batch size of 16. On the basis of empirical observations and prior work  [50, 51] , we kept the learning rate constant at 1e -5 for IEMOCAP and 1e -4 for MELD during training. For our multitask learning setup, we set β to 3 and α to 0.1. For our multistrategy learning setup, we set γ to 0.01 and λ to 0.1. The temperature parameter τ was set to 0.07. To validate these choices, we additionally performed a one-at-a-time sensitivity analysis on the hyperparameters α, β, γ, and λ, as presented in Section V-C and illustrated in Fig.  7 .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Experimental Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Comparisons Of State-Of-The-Art (Sota) Methods",
      "text": "In the experiments, we compared various multimodal speech emotion recognition SOTA methods with our proposed method, M 4 SER. The comparison was conducted by the following methods on the IEMOCAP dataset:\n\n• SAWC  [47]  adjusts importance weights using confidence measures, reducing ASR error impact by emphasizing the corresponding speech segments. • RMSER-AEA  [9]  uses complementary semantic information, adapting to ASR errors through an auxiliary task and fusing text and acoustic representations for SER.\n\n• SAMS  [52]  leverages high-level emotion representations as supervisory signals to build a multi-spatial learning framework for each modality, enabling cross-modal semantic learning and fusion representation exploration.\n\n• MGAT  [21]  tackles emotional asynchrony and modality misalignment problems by employing a multi-granularity attention mechanism.\n\n• MMER  [14]  employs early fusion and cross-modal selfattention between text and acoustic modalities, and addresses three novel auxiliary tasks to enhance SER. For fairness, we selected the result that did not introduce augmented text for comparison.\n\n• IMISA  [53]  involves mapping a speech-text pair to a shared subspace, extracting modality-specific features and employing contrastive learning to align features at a sample level.\n\n• MAF-DCT  [54]  introduces a dual approach utilizing SSL representation and spectral features for comprehensive speech feature extraction, along with a dual cross-modal transformer to handle interactions. • FDRL  [22]  integrates modality-shared and modality-private encoders, incorporating fine-grained alignment and disparity components to enhance modal consistency and diversity.\n\n• In MF-AED-AEC  [13] , as in our previous work, we consider two auxiliary tasks to improve semantic coherence in ASR text and introduce a MF method to learn shared representations.  SAMS and MF-AED-AEC methods are also utilized as baselines for the MELD dataset. Additionally, the following methods were included for comparison:\n\n• Full  [55]  uses contextual cross-modal transformers and graph convolutional networks for enhanced emotion representations and modality fusion.\n\n• HCAM  [56]  combines wav2vec audio and BERT text inputs with co-attention and recurrent neural networks for multimodal emotion recognition.\n\n• DIMMN  [57]  fuses multimodal data using multiview attention layers, temporal convolutional networks, gated recurrent units, and memory networks to model dynamic dependencies and contextual information. • MER-HAN  [58]  integrates local intramodal, cross-modal, and global intermodal attention mechanisms to effectively learn emotional features through a structured process.\n\n• MLCCT  [59]  involves feature extraction, interaction, and fusion using SSL embedding models, Bi-LSTM, crossmodal transformers, and self-attention blocks.\n\nTables II and III compare the performance of the M 4 SER method with those of recent multimodal SER methods on the IEMOCAP and MELD datasets. Our proposed M 4 SER achieves a WA of 79.2% and a UA of 80.1% on IEMOCAP,",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Ablation Study",
      "text": "Our proposed M 4 SER model is composed of four types of learning: multimodal learning, multirepresentation learning, multitask learning, and multistrategy learning. To determine the impact of different types of learning on the performance of the emotion recognition task, ablation experiments were conducted on the IEMOCAP and MELD datasets, as presented in Table  IV .\n\nImpact of Multimodalities. To assess the necessity of multimodality, we conduct single-modal comparison experiments involving text and speech modalities. These experiments operate under a baseline framework excluding the MF module, LCL loss, and GAN loss, precluding intermodal interaction. As shown in Table  IV (A), in the single-modal experiments, the performance of speech modality in the IEMOCAP dataset is better than that of text modality, whereas the reverse is observed in the MELD dataset. This divergence suggests that emotional information is more distinctly conveyed through speech in IEMOCAP, whereas MELD leans towards text for emotional expression. Moreover, the multimodal baseline model, which simply concatenates speech and text features for recognition (see Table  IV  A(3)), significantly enhances performance compared with single-modal baselines, highlighting the essential role of multimodal information. Impact of Multirepresentations. We study the importance of modality-invariant and modality-specific representations by discarding each type. Removing modality-specific representations from multimodal fusion significantly reduces downstream emotion recognition performance on both datasets, confirming their effectiveness in capturing and utilizing unique information from each modality. Similarly, omitting refined modality-invariant representations from multimodal fusion also significantly decreases emotion recognition performance on both datasets, demonstrating their importance in bridging modality gaps. Clearly, removing both representations further decreases performance.\n\nImpact of Multitasks. First, we discuss the impact of the AED module. Specifically, we remove the AED module and introduce only the AEC module as an auxiliary task. This requires the model to correct all errors in each utterance from scratch, rather than only the detected errors. Evidently, the absence of the AED module leads to a significant drop in emotion recognition performance, demonstrating the necessity of the AED module. Similarly, we observe that the AEC module also plays an important role. Moreover, we note that the WA results using only the AEC module (see Table  IV (C)(1)) are even worse than the results with neither the AED nor AEC module (see Table IV(C)(3)) in both datasets. This phenomenon occurs because directly using the neural machine translation (NMT) model for AEC can even increase the WER  [60] . Unlike NMT tasks, which typically require modifying almost all input tokens, AEC involves fewer but more challenging corrections. For instance, if the ASR model's WER is 10%, then only about 10% of the input tokens require correction in the AEC model. However, these tokens are often difficult to recognize and correct because they have already been misrecognized by the ASR model. Therefore, we should consider the characteristics of ASR outputs and carefully design the AEC model, which is why we introduce both AED and AEC as auxiliary tasks.\n\nImpact of Multistrategies. To validate the effectiveness of the adversarial training strategy outlined in Alg. 1, we remove the adversarial training strategy from M 4 SER. The results in Table IV(D)  (1)  show a decrease in performance on both datasets, demonstrating the critical role of this strategy in learning modality-invariant representations. The proposed modality discriminator effectively enhances the modality agnosticism of the refined representations from the generator. Additionally, omitting the LCL strategy described in Section III-H also results in similar performance degradation (see Table  IV (D)(2)). We visualize the results before and after introducing this strategy to demonstrate its effectiveness, with specific analysis also detailed in the following t-SNE analysis section. Moreover, removing both of these strategies further diminishes emotion recognition performance, confirming that both learning strategies contribute significantly to performance improvement.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Table V Trade-Off Between Computational Cost And Performance On Iemocap. Parameter Size (Params) Is Measured In Millions (M). Training Time Is Reported Over 100 Epochs In Hours (H), And Inference Time Is Averaged Per Utterance In Milliseconds (Ms/U).",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Sensitivity Analysis",
      "text": "We conduct a one-at-a-time sensitivity analysis on four key hyperparameters: the auxiliary task weight α, the AED/AEC balancing factor β, the GAN loss weight γ, and the LCL loss weight λ. As shown in Fig.  7 , model performance is moderately sensitive to α and λ, with optimal results achieved at α = 0.1 and λ = 0.1. Excessively small values reduce the effectiveness of auxiliary guidance, while overly large values interfere with the main task.\n\nIn contrast, the model exhibits relative robustness to variations in β, indicating stable synergy between the AED and AEC objectives. Conversely, performance is highly sensitive to γ; large values result in unstable training due to adversarial objectives, while a small weight (e.g., γ = 0.01) ensures stable convergence and optimal results.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Convergence Of Multitask And Multistrategy Losses",
      "text": "Fig.  8  shows that the multitask objectives (L ER , L AED , L AEC ) rapidly decrease and stabilize, confirming that the auxiliary tasks provide useful supervision for ER. The multistrategy objectives also exhibit stable convergence: L LCL quickly decreases under label supervision, while for the adversarial component, L D rises from negative values to near zero as the discriminator learns, L GAN steadily increases and plateaus, and L G exhibits fluctuations, reflecting the adversarial interplay, and eventually converges as D(H (inv) ST ) → 0.5, which corresponds to about 1.386 in the L G term of Eq. (  20 ), indicating that an adversarial equilibrium is achieved. Overall, both multitask and multistrategy objectives are optimized in a stable manner, validating the effectiveness of the M 4 SER.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E. Computational Complexity Analysis",
      "text": "To assess the trade-off between model performance and computational cost, we report the number of parameters,  training time, and inference time for each model on the IEMOCAP dataset in Table  V , with all experiments conducted on a single NVIDIA Tesla V100 GPU. Among the single-modal models, HuBERT exhibits higher computational cost than BERT owing to the nature of speech encoders, but also delivers better recognition performance. In the multi-modal setting, each additional module introduces a moderate increase in parameter size and training cost, while consistently improving recognition accuracy.\n\nOur full model M 4 SER achieves the best performance with WA of 79.2% and UA of 80.1%. Although its training time increases to 21.5 hours owing to the inclusion of GAN and LCL strategies and AED and AEC subtasks, these components are used only during training. Consequently, the inference latency of M 4 SER remains nearly identical to the baseline with only MSR and MIR. Considering the significant gains in performance, M 4 SER demonstrates a favorable trade-off between computational cost and recognition accuracy, making it suitable for practical deployment.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Visualization Analysis",
      "text": "t-SNE Analysis. To intuitively demonstrate the advantages of our proposed M 4 SER model on IEMOCAP and MELD datasets, we utilize the t-distributed stochastic neighbor embedding (t-SNE) tool  [61]  to visualize the learned emotion features using all samples from session 3 of the IEMOCAP and test set of the MELD. We compare these features across the following models: the speech model (Fig.  9  From the visualization results in Fig.  9 , we observe that the emotion label distributions for the two single-modal baselines on the IEMOCAP and MELD datasets show a significant overlap among the emotion categories, indicating that they are often confused with each other. In contrast, the emotion label distributions for the two multimodal models are more distinguishable, demonstrating the effectiveness of multimodal models in capturing richer emotional features.\n\nFurthermore, compared with the M 4 SER model without the LCL strategy, our M 4 SER model achieves better clustering for each emotion category, making them as distinct as possible. For instance, on the IEMOCAP dataset, the intra-class clustering of samples learned by the M 4 SER model (Fig.  9(d) ) is more pronounced than that learned by the M 4 SER model without the LCL strategy (Fig.  9(c )), especially with clearer boundaries for sad and angry samples. Additionally, the distances between the four types of emotion in the emotion vector space increase. Although distinguishing MELD samples is more challenging than distinguishing IEMOCAP samples, the overall trend remains similar. This indicates that the proposed M 4 SER model effectively leverages both modalityspecific and modality-invariant representations to capture highlevel shared feature representations across speech and text modalities for emotion recognition.\n\nTo further explore the effectiveness of adversarial learning in M 4 SER, we visualize the distribution of modality-invariant and modality-specific representations before and after adversarial learning using the t-SNE tool, as shown in Fig.  10 . It can be observed that after adversarial learning, different modality-specific representations become more separated with increasing intraclass clustering. This indicates that with the introduction of adversarial learning, M 4 SER enhances the diversity of modality-specific representations, generating superior features for the downstream emotion recognition task.\n\nRepresentation Analysis. We find that our M 4 SER method performs better when using ASR text than when using GT     11 (a) illustrates that during single-modal learning with ASR text, representation weights in the text modality mainly focus on the word \"oh\", whereas in the speech modality, representation weights concentrate towards the sentence's end. After cross-modal learning, we observed a significant decrease in \"oh\" weight in the text modality. This change occurs as the model detects errors or inaccuracies in ASR transcription using AED and AEC modules, shifting representation weights to other words such as \"that's\". Owing to limited emotional cues in the text, the model relies more on speech features, especially anger-related cues such as intonation and volume, critical for accurate anger recognition. Conversely, with GT text, after cross-modal learning, attention in the text modality focuses on words conveying positive emotions such as \"amusing\", whereas speech modality distribution remains largely unchanged. Without ASR error signals, the model leans towards these textual features, leading to a bias towards happiness in emotion recognition.\n\nSimilarly,  Fig. 11(b)  shows inconsistency between GT text and speech features. GT text contains words (e.g., \"don't know\") indicating confusion and uncertainty, whereas speech features exhibit significant emotional fluctuations. This mismatch complicates feature alignment in cross-modal learning, resulting in erroneous anger classification. In contrast, ASR text shows higher consistency with speech features, highlighting emotional words (\"freaking out\", \"what the hell\", and \"all of a sudden\") despite errors and simplifications. Representation weights confirm high consistency between ASR text and speech features across multiple regions, enhancing crossmodal learning accuracy and correct emotion identification as \"Happy\" (\"Excited\"). We demonstrate with dynamic time warping (DTW) that ASR transcriptions are more consistent with speech than GT transcriptions, as shown in Table  VII .\n\nIn summary, compared with ASR text, GT text exhibits two primary differences:\n\n1) Lack of ASR error signals: GT text lacks ASR errors, missing additional cues beneficial for emotion recognition. This limitation possibly hinders capturing intense emotional signals when relying solely on GT text. 2) Differences in feature weight distribution: the absence of ASR errors alters feature weight distribution in GT text modality compared with ASR text, potentially leading to inaccurate emotion classification. Confusion Matrix Analysis. To investigate the impact of different modules on class-wise prediction, we visualize the averaged confusion matrices over 5-folds on IEMOCAP in Fig.  12 . Compared with the baseline, introducing MSR and MIR significantly enhances the prediction of \"Happy\" class, which often suffer from ambiguous acoustic-textual alignment. By further incorporating GAN and LCL strategies, we observe overall improvements across most emotion classes, particularly in \"Neutral\". However, the performance on the \"Happy\" class slightly drops, potentially owing to the acoustictextual heterogeneity introduced by merging \"Excited\" into \"Happy\", which possibly reduces the effectiveness of local contrastive learning in capturing class-specific discriminative features. Finally, integrating AED and AEC modules yields the most accurate and balanced predictions overall. We observe further improvements in \"Neutral\" and \"Sad\" classes, while the performance of \"Happy\" slightly recovers compared with the previous setting. These results highlight the effectiveness of multi-task learning in mitigating ASR-induced errors and enhancing emotional robustness across modalities.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "G. Cross-Corpus Generalization Ability",
      "text": "To evaluate the performance of M 4 SER in real-world environments, we conduct cross-corpus emotion recognition experiments, which simulate the practical scenario of domain shift between different datasets. Following the experimental settings of previous works  [62] [63] [64] , we adopt a transfer evaluation strategy where one corpus is used entirely for training, and 30% of the target corpus is reserved for validation for parameter tuning, while the remaining 70% is used for final testing. In addition, as the IEMOCAP dataset contains only four emotion classes (\"Neutral\", \"Happy\", \"Angry\", \"Sad\"), we restrict MELD to the same set of overlapping emotions for a fair comparison and discard the rest (\"Suprise\", \"Fear\", \"Disgust\"). This ensures consistent label space across corpora during both training and evaluation.\n\nTable  VIII  presents the cross-corpus generalization results. Compared with the reimplemented SAMS and MF-AED-AEC baselines, our full model M 4 SER achieves the best performance in both IE→ME and ME→IE directions. Specifically, M 4 SER outperforms the strong multimodal baseline by 4.3% in ACC and 3.1% in W-F1 under the IE→ME setting, and by 5.1% WA and 4.8% UA in the ME→IE setting.\n\nTo further evaluate the adaptability of M 4 SER under limited supervision, we conduct few-shot domain adaptation experiments by sampling 5, 10, 20, and 60 labeled instances per class from the target-domain validation set. The remaining validation data is still used for model selection, while the test set remains fixed across all settings. As shown in Table  IX , M 4 SER consistently outperforms SAMS and MF-AED-AEC across all shot settings. For example, with only 5 labeled samples per class, M 4 SER achieves ACC of 57.9% in the IE→ME setting, already surpassing MF-AED-AEC with 20 shots. As the number of target samples increases, our model scales well and reaches ACC of 62.1% and W-F1 61.5% at 60-shot. In the ME→IE setting, M 4 SER reaches UA of 75.9%, demonstrating excellent cross-domain adaptability.\n\nThese results demonstrate that M 4 SER not only generalizes robustly across corpora with minimal tuning, but also effectively adapts to new domains under few-shot conditions.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose M 4 SER, a novel emotion recognition method that combines multimodal, multirepresentation, multitask, and multistrategy learning. M 4 SER leverages an innovative multimodal fusion module to learn modality-specific and modality-invariant representations, capturing unique features of each modality and common features across modalities.\n\nWe then introduce a modality discriminator to enhance modality diversity through adversarial learning. Additionally, we design two auxiliary tasks, AED and AEC, aimed at enhancing the semantic consistency within the text modality. Finally, we propose a label-based contrastive learning strategy to distinguish different emotional features. Results of experiments on the IEMOCAP and MELD datasets demonstrate that M 4 SER surpasses previous baselines, proving its effectiveness. In the future, we plan to extend our approach to the visual modality and introduce disentangled representation learning to further enhance emotion recognition performance. We also plan to investigate whether AED and AEC modules remain necessary when using powerful LLM-based ASR systems.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the difference between previous multimodal SER",
      "page": 1
    },
    {
      "caption": "Figure 1: M4SER mitigates the impact of ASR errors,",
      "page": 2
    },
    {
      "caption": "Figure 2: Overall architecture of the proposed M4SER model. Specific illustrations of CME and MIR blocks in the (d) MF module are shown in Figs. 3 and",
      "page": 4
    },
    {
      "caption": "Figure 2: In the following section, we provide a detailed explanation",
      "page": 4
    },
    {
      "caption": "Figure 3: To acquire token-",
      "page": 5
    },
    {
      "caption": "Figure 3: Illustration of the CME block.",
      "page": 5
    },
    {
      "caption": "Figure 4: Illustration of the MIR generator and HMA blocks.",
      "page": 6
    },
    {
      "caption": "Figure 2: (f) is com-",
      "page": 6
    },
    {
      "caption": "Figure 5: Example of constructing positive and negative samples for label-based",
      "page": 6
    },
    {
      "caption": "Figure 2: (g). This task aids the model in extracting emotion-",
      "page": 6
    },
    {
      "caption": "Figure 5: , we categorize",
      "page": 6
    },
    {
      "caption": "Figure 6: IEMOCAP comprised roughly 12 hours of speech from ten",
      "page": 7
    },
    {
      "caption": "Figure 6: Amount of data for each type of emotion in IEMOCAP and MELD.",
      "page": 8
    },
    {
      "caption": "Figure 2: (c) adopted a single-layer transformer",
      "page": 8
    },
    {
      "caption": "Figure 7: V. EXPERIMENTAL RESULTS",
      "page": 8
    },
    {
      "caption": "Figure 7: Sensitivity analysis of key hyperparameters on the IEMOCAP dataset.",
      "page": 10
    },
    {
      "caption": "Figure 8: Evolution of optimization objectives during multitask multistrategy training.",
      "page": 10
    },
    {
      "caption": "Figure 9: t-SNE visualization using IEMOCAP and MELD datasets. We visualize all samples from IEMOCAP and MELD test sets.",
      "page": 11
    },
    {
      "caption": "Figure 10: t-SNE visualizations of the distribution of the modality-specific and",
      "page": 11
    },
    {
      "caption": "Figure 7: , model performance is",
      "page": 11
    },
    {
      "caption": "Figure 8: shows that the multitask objectives (LER, LAED,",
      "page": 11
    },
    {
      "caption": "Figure 11: Representation weights of temporal-level features under different text input conditions in different types of modal learning. Brighter colors (tending",
      "page": 12
    },
    {
      "caption": "Figure 9: (a)), the text",
      "page": 12
    },
    {
      "caption": "Figure 9: (b)), the proposed M4SER model without label-",
      "page": 12
    },
    {
      "caption": "Figure 9: (c)), and the proposed",
      "page": 12
    },
    {
      "caption": "Figure 9: , we observe that the",
      "page": 12
    },
    {
      "caption": "Figure 9: (d)) is more pronounced than that learned by the M4SER",
      "page": 12
    },
    {
      "caption": "Figure 9: (c)), especially with",
      "page": 12
    },
    {
      "caption": "Figure 10: It can be observed that after adversarial learning, different",
      "page": 12
    },
    {
      "caption": "Figure 12: Confusion matrices obtained using IEMOCAP datasets. We utilize the results from five-fold cross-validation. Columns represent predicted labels and",
      "page": 13
    },
    {
      "caption": "Figure 11: We display representations",
      "page": 13
    },
    {
      "caption": "Figure 11: (a) illustrates that during",
      "page": 13
    },
    {
      "caption": "Figure 11: (b) shows inconsistency between GT text",
      "page": 13
    },
    {
      "caption": "Figure 12: Compared with the baseline, introducing MSR",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) IEMOCAP": "Angry\n1103\nNeutral\n1708\nSad\n1084\nHappy\n1636"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b) MELD": "Suprise\n1499\nAngry\n1541\nNeutral\nDisgust 355\n6167\nSad 985\nFear\nHappy\n350\n2198"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "Semi-supervised multimodal emotion recognition with consensus decision-making and label correction",
      "authors": [
        "J Tian",
        "D Hu",
        "X Shi",
        "J He",
        "X Li",
        "Y Gao",
        "T Toda",
        "X Xu",
        "X Hu"
      ],
      "year": "2023",
      "venue": "Proc. MRAC"
    },
    {
      "citation_id": "3",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "HuBERT: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "WavLM: Largescale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "RoBERTa: a robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: a robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition based on self-attention weight correction for acoustic and text features",
      "authors": [
        "J Santoso",
        "T Yamada"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Robust multi-modal speech emotion recognition with ASR error adaptation",
      "authors": [
        "B Lin",
        "L Wang"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "10",
      "title": "MISA: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proc. ACMMM"
    },
    {
      "citation_id": "11",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "H Kuang",
        "Y Du",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Proc. ACMMM"
    },
    {
      "citation_id": "12",
      "title": "Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences",
      "authors": [
        "D Yang",
        "H Kuang",
        "S Huang",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Proc. ACMMM"
    },
    {
      "citation_id": "13",
      "title": "MF-AED-AEC: Speech emotion recognition by leveraging multimodal fusion, ASR error detection, and ASR error correction",
      "authors": [
        "J He",
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2024",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "14",
      "title": "MMER: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Speech-Former: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "ISNet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "A study on multimodal fusion and layer adapter in emotion recognition",
      "authors": [
        "X Shi",
        "Y Gao",
        "J He",
        "J Mi",
        "X Li",
        "T Toda"
      ],
      "year": "2024",
      "venue": "Proc. APSIPA ASC"
    },
    {
      "citation_id": "18",
      "title": "Two-stage framework for robust speech emotion recognition using target speaker extraction in human speech noise conditions",
      "authors": [
        "J Mi",
        "X Shi",
        "D Ma",
        "J He",
        "T Fujimura",
        "T Toda"
      ],
      "year": "2024",
      "venue": "Proc APSIPA ASC"
    },
    {
      "citation_id": "19",
      "title": "SeeNet: A soft emotion expert and data augmentation method to enhance speech emotion recognition",
      "authors": [
        "Q Li",
        "Y Gao",
        "Y Wen",
        "Z Zhao",
        "Y Li",
        "B Schuller"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "S Bharti",
        "S Varadhaganapathy",
        "R Gupta",
        "P Shukla",
        "M Bouye",
        "S Hingaa",
        "A Mahmoud"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "21",
      "title": "MGAT: Multigranularity attention based transformers for multi-modal emotion recognition",
      "authors": [
        "W Fan",
        "X Xing",
        "B Cai",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Fine-grained disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "H Sun",
        "S Zhao",
        "X Wang",
        "W Zeng",
        "Y Chen",
        "Y Qin"
      ],
      "year": "2024",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Coordination attention based transformers with bidirectional contrastive loss for multimodal speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "G Zhou",
        "X Deng",
        "X Xing"
      ],
      "year": "2025",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "24",
      "title": "GIA-MIC: Multimodal emotion recognition with gated interactive attention and modality-invariant learning constraints",
      "authors": [
        "J He",
        "J Mi",
        "T Toda"
      ],
      "year": "2025",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "J Santoso",
        "T Yamada",
        "S Makino",
        "K Ishizuka",
        "T Hiramura"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition with ASR transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2024",
      "venue": "Proc. SLT"
    },
    {
      "citation_id": "27",
      "title": "Crossmodal ASR error correction with discrete speech units",
      "authors": [
        "Y Li",
        "P Chen",
        "P Bell",
        "C Lai"
      ],
      "year": "2024",
      "venue": "Proc. SLT"
    },
    {
      "citation_id": "28",
      "title": "Revise, reason, and recognize: LLM-based emotion recognition via emotion-specific prompts and ASR error correction",
      "authors": [
        "Y Li",
        "Y Gong",
        "C.-H Yang",
        "P Bell",
        "C Lai"
      ],
      "year": "2025",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "29",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Noise-resistant multimodal transformer for emotion recognition",
      "authors": [
        "Y Liu",
        "H Zhang",
        "Y Zhan",
        "Z Chen",
        "G Yin",
        "L Wei",
        "Z Chen"
      ],
      "year": "2024",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "ED-CEC: Improving rare word recognition using ASR postprocessing based on error detection and context-aware error correction",
      "authors": [
        "J He",
        "Z Yang",
        "T Toda"
      ],
      "year": "2023",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "32",
      "title": "PMF-CEC: Phoneme-augmented multimodal fusion for context-aware asr error correction with error-specific selective decoding",
      "authors": [
        "J He",
        "T Toda"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Enhancing recognition of rare words in ASR through error detection and contextaware error correction",
      "authors": [
        "J He",
        "Z Yang",
        "T Toda"
      ],
      "year": "2023",
      "venue": "IEICE Tech. Rep"
    },
    {
      "citation_id": "34",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "35",
      "title": "MALN: multimodal adversarial learning network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "J Liu",
        "M Liu",
        "X Li",
        "A.-A Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "36",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "Proc. NIPS"
    },
    {
      "citation_id": "37",
      "title": "Label anchored contrastive learning for language understanding",
      "authors": [
        "Z Zhang",
        "Y Zhao",
        "M Chen",
        "X He"
      ],
      "year": "2022",
      "venue": "Proc. NAACL"
    },
    {
      "citation_id": "38",
      "title": "Targeted supervised contrastive learning for long-tailed recognition",
      "authors": [
        "T Li",
        "P Cao",
        "Y Yuan",
        "L Fan",
        "Y Yang",
        "R Feris",
        "P Indyk",
        "D Katabi"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "39",
      "title": "Balanced contrastive learning for long-tailed visual recognition",
      "authors": [
        "J Zhu",
        "Z Wang",
        "J Chen",
        "Y.-P Chen",
        "Y.-G Jiang"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "40",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Y Wu",
        "M Schuster",
        "Z Chen",
        "Q Le",
        "M Norouzi",
        "W Macherey",
        "M Krikun",
        "Y Cao",
        "Q Gao",
        "K Macherey"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "arxiv": "arXiv:1609.08144"
    },
    {
      "citation_id": "41",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "42",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "43",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "44",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "45",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "46",
      "title": "MELD: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "47",
      "title": "Multimodal transformer with learnable frontend and self attention for emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "48",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "49",
      "title": "ADAM: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "50",
      "title": "Enhancing modal fusion by alignment and label matching for multimodal emotion recognition",
      "authors": [
        "Q Li",
        "Y Gao",
        "Y Wen",
        "C Wang",
        "Y Li"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "51",
      "title": "CFN-ESA: A crossmodal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Y Liu",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Semantic alignment network for multi-modal emotion recognition",
      "authors": [
        "M Hou",
        "Z Zhang",
        "C Liu",
        "G Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "53",
      "title": "Inter-modality and intrasample alignment for multi-modal emotion recognition",
      "authors": [
        "Y Wang",
        "D Li",
        "J Shen"
      ],
      "year": "2024",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "54",
      "title": "Multi-modal emotion recognition using multiple acoustic features and dual cross-modal transformer",
      "authors": [
        "Y Wu",
        "P Yue",
        "L Qu",
        "T Li",
        "Y.-P Ruan"
      ],
      "year": "2024",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "55",
      "title": "Contextual and cross-modal interaction for multi-modal speech emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "Y Liu",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "56",
      "title": "HCAM-hierarchical cross attention model for multi-modal emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2023",
      "venue": "HCAM-hierarchical cross attention model for multi-modal emotion recognition",
      "arxiv": "arXiv:2304.06910"
    },
    {
      "citation_id": "57",
      "title": "Dynamic interactive multiview memory network for emotion recognition in conversation",
      "authors": [
        "J Wen",
        "D Jiang",
        "G Tu",
        "C Liu",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "58",
      "title": "Multimodal emotion recognition based on audio and text by using hybrid attention networks",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "R Liu",
        "X Tao",
        "W Guo",
        "Y Xu",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "59",
      "title": "A multi-level circulant cross-modal transformer for multimodal speech emotion recognition",
      "authors": [
        "P Gong",
        "J Liu",
        "Z Wu",
        "B Han",
        "Y Wang",
        "H He"
      ],
      "year": "2023",
      "venue": "Computers, Materials & Continua"
    },
    {
      "citation_id": "60",
      "title": "Fastcorrect: Fast error correction with edit alignment for automatic speech recognition",
      "authors": [
        "Y Leng",
        "X Tan",
        "L Zhu",
        "J Xu",
        "R Luo",
        "L Liu",
        "T Qin",
        "X Li",
        "E Lin",
        "T.-Y Liu"
      ],
      "venue": "Proc. NeurIPS, 2021"
    },
    {
      "citation_id": "61",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "62",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "S Latif",
        "M Usama",
        "M Malik",
        "B Schuller"
      ],
      "year": "2025",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "64",
      "title": "Speech emotion recognition based on syllablelevel feature extraction",
      "authors": [
        "A Rehman",
        "Z.-T Liu",
        "M Wu",
        "W.-H Cao",
        "C.-S Jiang"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    }
  ]
}