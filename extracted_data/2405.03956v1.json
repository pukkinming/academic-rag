{
  "paper_id": "2405.03956v1",
  "title": "Adaptive Speech Emotion Representation Learning Based On Dynamic Graph",
  "published": "2024-05-07T02:42:17Z",
  "authors": [
    "Yingxue Gao",
    "Huan Zhao",
    "Zixing Zhang"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Dynamic graph",
    "Node similarity matrix",
    "Adaptive graph presentation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Graph representation learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings. However, for sequential data such as speech signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data. This may reduce the efficiency of graph representation learning for sequential data. For this reason, we propose an adaptive graph representation learning method based on dynamically evolved graphs, which are consecutively constructed on a series of subsequences segmented by a sliding window. In doing this, it is better to capture local and global context information within a long sequence. Moreover, we introduce a weighted approach to update the node representation rather than the conventional average one, where the weights are calculated by a novel matrix computation based on the degree of neighboring nodes. Finally, we construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to optimize the graph structure. To verify the effectiveness of the proposed method, we conducted experiments for speech emotion recognition on the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed method outperforms the latest (non-)graph-based models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Graph representation learning has demonstrated tremendous promise due to its powerful capability of mining graph structure information and data relationships  [1] . Graph convolutional network (GCN), a concrete and popular implementation in graph representation learning, has been widely used since it fully considers the relationships between target nodes and neighboring nodes to learn efficient node representations. For example, Compact-GCN  [2]  constructs a lightweight GCN architecture, which can perform accurate graph convolution for speech emotion recognition (SER). To model dynamic data, L-GrIN  [3]  proposes a learnable graph structure, which is designed to adapt across modalities.\n\nDespite great progress made in graph representation learning for SER, they primarily focused on a static graph constructed on an entire utterance. This may fail to capture the trivial variation of emotion in a small region. Moreover, for most previous methods, the dominant paradigm to update the node representations is by averaging the information of neighboring nodes, without considering the importance of different neighboring nodes. Although some efforts have been made to explore a weighted average approach by considering the attention mechanism  [4, 5] , the calculation process is time-consuming and highly computationally complex.\n\nTo address these shortcomings, this paper proposes an adaptive graph representation learning model based on dynamically evolved graphs. Specifically, we consecutively construct the graphs on a set of subsequences segmented by a sliding window, where each node of the graph corresponds to a frame (frame-to-node), and extract the feature vector of the frame as its node representation. Then, the node representation is updated by our proposed matrix calculation method. Finally, we construct a learnable graph convolutional layer to optimize the graph structure. The contributions of this paper are summarized as follows:\n\n• We introduce a weighted method to update the node representations rather than the traditional methods of averaging the information of neighboring nodes, where the weights are calculated by the proposed matrix computation based on the degree of neighboring nodes.\n\n• We construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to jointly optimize the graph structure during the training.\n\n• Experimental results show that our SER model has better performance than the state-of-the-art (SOTA) graph-based networks and several widely used non-graph-based models on the IEMOCAP and RAVDESS datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "There are two kinds of graph representation learning that are used to deal with static graphs and dynamic graphs. For the former, these methods focus on mining the connectivity of graphs. A wellknown connectivity way is to connect the first-order or second-order neighboring nodes  [3] , which provide the structure information of a graph at different levels. Commonly used approaches, including random walk (DeepWalk  [6]  and Node2Vec  [7] ), graph convolution (GCNs  [8] ), sampling (GraphSAGE  [9]  and GraphSAINT  [10] ), non-negative matrix decomposition (M-NMF  [11] ), and attention mechanism (GAT  [12] ). However, these methods largely ignore the evolution of graph structures and the temporal properties of graphs.\n\nRecently, several dynamic representation learning approaches have been proposed. Specifically, to dig deeper into the local structure of the graph, StudentLSP  [13]  utilizes a knowledge distillation method to learn the node representations. DySAT  [14]  learns node representations with the help of a self-attention mechanism by modeling both neighboring nodes and temporal attributes. FADGC  [5]  captures the temporal properties of dynamic graphs by using a finegrained attention mechanism to focus on the node changes. In addition, to learn the dynamic graph representations of a set of nodes, EvolveGCN  [15]  is implemented through a long short-term memory.\n\nHowever, like most GCN-based methods, the node aggregation of EvolveGCN is realized by averaging the information of neighboring nodes, which may not effectively take into account the importance of different neighboring nodes. To this end, we propose a new matrix computation method to update node representations based on the degree of neighboring nodes.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "This section elaborates on our proposed architecture, which consists of three parts: the graph construction, the computation of relations between nodes, and the learnable graph convolutional layer. The overall framework of our model is shown in Fig.  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph Construction",
      "text": "a dynamic graph, different from the traditional methods that only construct a static graph for an entire sequence. G s is an observed graph specific to an audio segment s (1 ≤ s ≤ S) and the structure of G s varies with different audio segments.\n\neach audio segment s has m nodes, E s is an edge set at audio segment s, and X s is a feature matrix for all nodes at audio segment s. Given a dynamic graph G, the goal of graph representation learning is to study a function f : R m×m → R m×q for each G s in G. Specifically, based on the function f , the given G can be output as low-dimensional representations, e.g.,\n\nOur graph construction follows the frame-to-node transformation, as shown in Fig.  2 . A frame represents a 25ms audio. To encode the temporal information, neighboring nodes (frames) need to be connected. Meanwhile, to aggregate the information of distant nodes, we also randomly connect the nodes. The aij ∈ A s represents the weight corresponding to the edge eij ∈ E s between vi and vj nodes. Note that the graph structure is not naturally defined here, i.e., the elements in A are unknown. The common methods to determine the A include cosine similarity function  [1] , manual definition  [2] , and distance function  [16] . However, these may lead to a sub-optimal graph  [17] . Therefore, we propose a new matrix method to initially the A and optimize the graph structure during the training by combining graph structure loss and classification loss. This loss function will be presented in Subsection 3.3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A Novel Matrix Computation",
      "text": "Calculating the similarity between each node and its neighboring nodes is crucial in graph-based analysis. Dice similarity is a wellknown node similarity measurement function  [18] . Since Dice similarity is calculated directly based on the network topology, it is relatively interpretable and saves computing resources. Given two nodes vi and vj, the Dice similarity score is calculated as follows:\n\nwhere Con (vi, vj) indicates the number of common neighbors. N (vi) and N (vj) denote the number of neighbors of vi and vj (not contain the node itself), respectively. As can be seen from Eq. (  1 ), when calculating the similarity between the nodes, the degree of neighboring nodes does not affect the final result. However, nodes with higher degrees are more likely to have more influence and higher weight values  [19] . For example, in the social networks of NBA players, players with more followers may bring more attention to their teams than players with similar physical conditions and skills. Take Fig.  3  as an example, the common neighbors number of node v2 with node v1 is equal to the node v3 with node v1. The influence of node v2 with smaller degree is SDice (v2, v1) = 0.33. However, the influence of node v3 with larger degree is SDice (v3, v1) = 0.29, which is not expected in the real world. This is because graph-based methods usually update node representations by transferring and aggregating information from neighboring nodes. Thus, the higher-degree nodes may aggregate more information.\n\nTherefore, we propose a new Dice matrix calculation method to solve the problem that the neighboring node degree has no positive effect on the target node. The proposed method is defined as follows:\n\nwhere Con (vi, vj) represents the number of neighbors shared by node vi and node vj, and D (vi, vj) indicates the degree of neighbor node vi (the node itself is contained). According to Eq. (  4 ), as expected, the influence of node v2 on v1 and the influence of node v3 on v1 in Fig.  3  are A s Dice (v2, v1) = 0.56 and A s Dice (v3, v1) = 0.57. That is to say, the nodes with relatively large degrees have higher node weight values.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Learnable Graph Convolution Network",
      "text": "Traditional GCN layer usually takes the node feature matrix X ∈ R m×p and the graph adjacency matrix A ∈ R m×m as inputs to generate the node-level representation matrix Z ∈ R m×q . The GCN layer can be described as:\n\nwhere σ is a ReLU activation function that implements nonlinearity.\n\n, D is a the degree matrix of A, and I is a m × m identity matrix. H (0) = X, H (L) = Z, and W (l) is the weight matrix for the l th layer.\n\nWe present a new graph convolution structure for SER. It consists of the following two novel components:\n\n• Segment-specific graph convolution. For each audio segment s, we generate a node representation matrix H s . The graph convolution layer is represented as:\n\nwhere D is a degree matrix of A s and A s = Ãs + φA s Dice + I. The Ãs i,j = (i -j) 2 , A s Dice is a novel matrix calculated by the proposed Eq. (  4 ), and I is an identity matrix. X s is a feature matrix and W s is a weight matrix, where W 0 is obtained by random and W s+1 = σ (W s + A s * X s ). Different from the traditional GCN that employs A + I to guide the aggregate of node representations. We additionally add the ADice to further guide the aggregation and use a parameter φ = 0.6 to control its contribution. Under the guidance of the new aggregation strategy, a higher-quality node representation can be obtained.\n\nOverview of our proposed model. The feature matrix X s , the graph structure (i.e., adjacency matrix A s ), and the weight matrix W s of the graph are dynamically changed with different audio segments s. Among them, the graph structure is mainly obtained by our proposed matrix calculation method (A s Dice ), and combined with the graph structure loss and classification loss to jointly optimize the graph structure. The relationship between nodes is jointly determined by the proposed matrix computation method and learning during the training. Each node has a node feature vector xi associated with it.\n\n• Learnable adjacency matrix (ALearn). We combine graph classification loss (LGC ) and graph structure loss (LGL) to jointly optimize the graph structure during training. The LGC is defined using the cross-entropy loss:\n\nwhere ŷn represents the prediction label for the n-th sample. The\n\nLGL is implemented as follows:\n\nLGL = λ1e T (ADice ⊙ ALearn) e + λ2 ∥ADice∥ 2\n\nwhere λ1 and λ2 control the proportion of these items respectively, e is an all-ones vector, ∥•∥ F indicates Frobenious norm, and ADice refers to Eq. (  4 ). The overall loss function is as follows: min\n\nwhere Θ represents all learnable parameters on all graph convolution layers. Each term in the overall loss function L is differentiable, thus allowing end-to-end optimization.\n\n, 0.33\n\n, 0.29\n\n, 0.56\n\n, 0.57\n\nComparison of traditional Dice similarity (SDice) and proposed Dice similarity (ADice). v2 and v3 have the same number of neighbors regarding the target node whilst the node with a larger degree (v3) has a higher influence.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "The IEMOCAP database contains 12 hours of data. To keep consistent with previous studies, only four emotions are used. We utilize the OpenSMILE tool to extract features from the audio. For each sample, we use a sliding window of length 25ms (with a step length of 10ms) to locally extract the low-level descriptors (LLDs), such as signal strength, power spectral density, and base frequency. Each speech sample contains 120 nodes, where each node represents an (overlapping) audio frame of length 25ms. The RAVDESS database includes 1,500 speech samples, performed by 24 professional actors (12 females and 12 males). Each actor simulated eight different emotional states. We use the Fourier transform to convert the speech signal into frequency domain representations. Then, we extract frequency domain features, such as the Mel frequency cepstrum coefficient (MFCC), where the sampling rate is 22,050 Hz and the number of MFCCs is 40. Each audio sample contains 40 nodes, where the node corresponds to a 25ms audio frame. Appropriately increasing the number of MFCCs can help perceive changes in sound signals at different frequencies, and these detailed features may play a positive role in emotional sensitivity. The reason for extracting MFCC features from the RAVDESS dataset is that it provides representative spectral information.\n\nTable  1 . Performance comparison between our model with latest (non-)graph models. Ablation experiments are constructing graph structures in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node similarity matrix (ADice). \"-\" represents no results provided in the references.\n\n(a) Performance comparison on the IEMOCAP dataset.\n\nModels IEMOCAP Acc(%) F1(%) Graph-based GCN (2017)  [8]  56.14 -PATCHY-Diff (2018)  [20]  63.23 -HSGCF (2023)  [21]  65.13 65.18 DialogueGCN (2019)  [22]  65.25 64.18 L-GrIN (2022)  [3]  65.50 -Non-graph based SpecMAE-12 (2023)  [23]  46.70 45.90 CNN-LSTM (2019)  [24]  50.17 -DialogueRNN (2019)  [25]  63.40 62.75 DualTransformer (2023)  [26]  64 Models RAVDESS Acc(%) F1(%) Graph-based Synch-Graph (2020)  [27]  42.60 -Esma et al. (2019)  [28]  45.10 -GCN (2023) 51.67 50.69 TSP-INCA (2021)  [29]  53.00 57.00 Riccardo et al (2022)  [30]  58.50 57.00 Non-graph based SpecMAE-12 (2023)  [23]  52.20 52.00 CNN-LSTM (2019)  [24]  53.08 -VGG-Transformer (2023)  [31]  61.60 -GResNet (2019)  [32]  64 GPUs, and obtained the final experimental data by using the 10-fold cross-validation averaging method.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "Comparison with SOTA methods. Table  1  presents all results on two datasets. On the IEMOCAP, compared with the graph-based SOTA approaches, the performance of our model seems only slightly better (1.44% of accuracy absolutely) than L-GrIN  [3] , but the adjacency matrix A of L-GrIN is obtained by a distance function, which tends to locally optimal graphs during the training. When compared with other SOTA non-graph methods, we find that our method outperforms popular Transformer-based methods. This is mainly attributed to the fact that we connect neighboring nodes while randomly connecting distant nodes for information transfer. The relationship between the nodes is calculated by the proposed matrix calculation method to guide the aggregation of node information, thus capturing the long-distance information. Table  1 (b) also shows that our model performs better than the graph-based and non-graphbased approaches on the RAVDESS database. For example, compared with the classic GCN model, our model has great performance advantages (15.83% of accuracy absolutely). This indicates that the proposed matrix computation method can alleviate the sub-optimal graph in traditional GCN caused by averaging the information of neighboring nodes. Ablation experiment. We perform ablation studies on two datasets, including the following variations of our model: Binary: manually defining the structure of the graph, the matrix A only contains 0 and 1; Weighted: the relationship between nodes is determined based on the distance; ALearn: the structure of the graph is learned during the training; ADice: the loss function only contains the proposed matrix computation method. The results of the study are displayed in the \"Adjacency matrix\" of Table  1 . We have the following observations. Firstly, both Binary and Weighted determine the relationship between nodes through customized form, which makes them easy to form the local optimal graphs during the training process, so the performance is the worst. Secondly, the proposed matrix calculation method ADice has enhanced performance compared to the learnable ALearn, which shows that our method can effectively distinguish the significance of different neighboring nodes. Finally, we optimize the structure of the graph by jointing the graph structure loss and classification loss to achieve the best performance. This indicates that the proposed matrix computation method can update the node representation and that the structure of the graph can be continuously optimized during the training process.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We propose an adaptive graph representation learning method based on dynamically evolved graphs rather than the traditional methods that only construct a static graph for an entire sequence. Our graphs are consecutively constructed on a series of subsequences segmented by a sliding window. To compute the edge weights, we propose a new matrix calculation method that updates the node representations based on the degree of neighboring nodes. Moreover, we combine the graph structure loss and classification loss to jointly optimize the graph structure during the training. In the future, we will simultaneously consider combining structural similarity with feature similarity to jointly measure the similarity of nodes, and work on multimodal data with graph structures.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 3.1. Graph construction",
      "page": 2
    },
    {
      "caption": "Figure 2: A frame represents a 25ms audio. To en-",
      "page": 2
    },
    {
      "caption": "Figure 3: as an example, the common",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of our proposed model. The feature matrix Xs, the graph structure (i.e., adjacency matrix As), and the weight matrix Ws",
      "page": 3
    },
    {
      "caption": "Figure 2: Graph Construction. Given an audio segment s, frame it in",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison of traditional Dice similarity (SDice) and pro-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "a series of subsequences segmented by a sliding window.\nIn doing"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "this, it is better to capture local and global context information within"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "a long sequence. Moreover, we introduce a weighted approach to"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "update the node representation rather than the conventional average"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "one, where the weights are calculated by a novel matrix computa-"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "tion based on the degree of neighboring nodes.\nFinally, we con-"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "struct a learnable graph convolutional layer that combines the graph"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "structure loss and classification loss to optimize the graph structure."
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "To verify the effectiveness of\nthe proposed method, we conducted"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "experiments for speech emotion recognition on the IEMOCAP and"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "RAVDESS datasets.\nExperimental\nresults show that\nthe proposed"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "method outperforms the latest (non-)graph-based models."
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "Index Terms— Speech emotion recognition, Dynamic graph,"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "Node similarity matrix, Adaptive graph presentation learning"
        },
        {
          "namically evolved graphs, which are consecutively constructed on": ""
        },
        {
          "namically evolved graphs, which are consecutively constructed on": "1.\nINTRODUCTION"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "have been made to explore a weighted average approach by con-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "sidering the attention mechanism [4, 5],\nthe calculation process is"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "time-consuming and highly computationally complex."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "To address these shortcomings,\nthis paper proposes an adaptive"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "graph representation learning model based on dynamically evolved"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "graphs. Specifically, we consecutively construct\nthe graphs on a set"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "of subsequences segmented by a sliding window, where each node"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "of the graph corresponds to a frame (frame-to-node), and extract the"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "feature vector of\nthe frame as\nits node representation.\nThen,\nthe"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "node representation is updated by our proposed matrix calculation"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "method. Finally, we construct a learnable graph convolutional layer"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "to optimize the graph structure. The contributions of this paper are"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "summarized as follows:"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "• We introduce a weighted method to update the node repre-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "sentations\nrather\nthan the traditional methods of averaging"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "the information of neighboring nodes, where the weights are"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "calculated by the proposed matrix computation based on the"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "degree of neighboring nodes."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "• We construct a learnable graph convolutional layer that com-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "bines the graph structure loss and classification loss to jointly"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "optimize the graph structure during the training."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "• Experimental results show that our SER model has better per-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "formance than the state-of-the-art\n(SOTA) graph-based net-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "works and several widely used non-graph-based models on"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "the IEMOCAP and RAVDESS datasets."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "2. RELATED WORK"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "There are two kinds of graph representation learning that are used"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "to deal with static graphs and dynamic graphs.\nFor\nthe former,"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "these methods focus on mining the connectivity of graphs. A well-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "known connectivity way is to connect the first-order or second-order"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "neighboring nodes [3], which provide the structure information of"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "a graph at different\nlevels. Commonly used approaches,\nincluding"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "random walk (DeepWalk [6] and Node2Vec [7]), graph convolution"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "(GCNs\n[8]),\nsampling (GraphSAGE [9]\nand GraphSAINT [10]),"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "non-negative matrix decomposition (M-NMF [11]),\nand attention"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "mechanism (GAT [12]). However,\nthese methods largely ignore the"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "evolution of graph structures and the temporal properties of graphs."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "Recently,\nseveral dynamic representation learning approaches"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "have been proposed. Specifically,\nto dig deeper into the local struc-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "ture of the graph, StudentLSP [13] utilizes a knowledge distillation"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "method to learn the node representations. DySAT [14] learns node"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "representations with the help of a self-attention mechanism by mod-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "eling both neighboring nodes and temporal attributes. FADGC [5]"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "captures the temporal properties of dynamic graphs by using a fine-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "grained attention mechanism to focus on the node changes.\nIn ad-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "dition,\nto learn the dynamic graph representations of a set of nodes,"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "EvolveGCN [15] is implemented through a long short-term memory."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "EvolveGCN is realized by averaging the information of neighboring",
          "social networks of NBA players, players with more followers may": "bring more attention to their teams than players with similar physi-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "nodes, which may not effectively take into account\nthe importance",
          "social networks of NBA players, players with more followers may": "cal conditions and skills. Take Fig. 3 as an example,\nthe common"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "of different neighboring nodes. To this end, we propose a new ma-",
          "social networks of NBA players, players with more followers may": "is equal\nto the node\nneighbors number of node v2 with node v1"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "trix computation method to update node representations based on the",
          "social networks of NBA players, players with more followers may": "v3 with node v1. The influence of node v2 with smaller degree is"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "degree of neighboring nodes.",
          "social networks of NBA players, players with more followers may": "However,\nSDice (v2, v1) = 0.33.\nthe influence of node v3 with"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "larger degree is SDice (v3, v1) = 0.29, which is not expected in"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "the real world.\nThis is because graph-based methods usually up-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "3. PROPOSED APPROACH",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "date node representations by transferring and aggregating informa-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "tion from neighboring nodes.\nThus,\nthe higher-degree nodes may"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "This section elaborates on our proposed architecture, which consists",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "aggregate more information."
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "of three parts:\nthe graph construction,\nthe computation of relations",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "Therefore, we propose a new Dice matrix calculation method to"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "between nodes, and the learnable graph convolutional\nlayer.\nThe",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "solve the problem that\nthe neighboring node degree has no positive"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "overall framework of our model is shown in Fig. 1.",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "effect on the target node. The proposed method is defined as follows:"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "3.1. Graph construction",
          "social networks of NBA players, players with more followers may": "(2)\nCon (vi, vj) = N (vi) ∩ N (vj) ,"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "Let G = (cid:8)G1, G2, · · ·\n, GS(cid:9) indicate a dynamic graph, different",
          "social networks of NBA players, players with more followers may": "|N (vi) ∪ {vi}|"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": ",\n(3)\nD (vi, vj) ="
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "from the traditional methods that only construct a static graph for",
          "social networks of NBA players, players with more followers may": "|N (vi) ∪ {vi}| + |N (vj) ∪ {vj}|"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "an entire sequence. Gs\nis an observed graph specific to an audio",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "Con (vi, vj) + D (vi, vj)"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "segment s (1 ≤ s ≤ S) and the structure of Gs varies with differ-",
          "social networks of NBA players, players with more followers may": "As\n,\n(4)\nDice (vi, vj) ="
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "|N (vj) ∪ {vj}| + 1"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": ", AS(cid:9) denote the struc-\nent audio segments. A = (cid:8)A1, A2, · · ·",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "ture of G. More specifically, Gs = {V s, E s, Xs}, where V s =",
          "social networks of NBA players, players with more followers may": "where Con (vi, vj) represents the number of neighbors shared by"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "{v1, v2, · · ·\n, vm} indicate that each audio segment s has m nodes,",
          "social networks of NBA players, players with more followers may": "node vi and node vj, and D (vi, vj) indicates the degree of neigh-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "E s is an edge set at audio segment s, and Xs is a feature matrix for",
          "social networks of NBA players, players with more followers may": "(the node itself is contained). According to Eq. (4), as\nbor node vi"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "all nodes at audio segment s. Given a dynamic graph G, the goal of",
          "social networks of NBA players, players with more followers may": "expected,\nthe influence of node v2 on v1 and the influence of node"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": ": Rm×m →\ngraph representation learning is to study a function f",
          "social networks of NBA players, players with more followers may": "v3 on v1 in Fig. 3 are As\nDice (v2, v1) = 0.56 and As\nDice (v3, v1) ="
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "Rm×q\nfor each Gs\nin G.\nSpecifically, based on the function f ,",
          "social networks of NBA players, players with more followers may": "0.57.\nThat\nis to say,\nthe nodes with relatively large degrees have"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "the given G can be output as low-dimensional representations, e.g.,",
          "social networks of NBA players, players with more followers may": "higher node weight values."
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "H = (cid:8)H1, H2, · · ·\n, HS(cid:9), where Hs ∈ Rm×q.",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "Our graph construction follows the frame-to-node transforma-",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "3.3. Learnable graph convolution network"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "tion, as shown in Fig. 2. A frame represents a 25ms audio. To en-",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "code the temporal\ninformation, neighboring nodes (frames) need to",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "Traditional GCN layer usually takes the node feature matrix X ∈"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "be connected. Meanwhile,\nto aggregate the information of distant",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "Rm×p and the graph adjacency matrix A ∈ Rm×m as inputs to"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "nodes, we also randomly connect\nthe nodes.\nrep-\nThe aij ∈ As",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "generate the node-level representation matrix Z ∈ Rm×q. The GCN"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "resents the weight corresponding to the edge eij ∈ E s between vi",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "layer can be described as:"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "the graph structure is not naturally defined\nand vj nodes. Note that",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "here, i.e., the elements in A are unknown. The common methods to",
          "social networks of NBA players, players with more followers may": "H(l+1) = σ\n(cid:16) ˆAH(l)W(l)(cid:17)\n,\n(5)"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "determine the A include cosine similarity function [1], manual defi-",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "nition [2], and distance function [16]. However, these may lead to a",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "where σ is a ReLU activation function that implements nonlinearity."
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "sub-optimal graph [17]. Therefore, we propose a new matrix method",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "A = D− 1\n2 (A + I) D− 1\n2 , D is a the degree matrix of A, and I is"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "to initially the A and optimize the graph structure during the training",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "a m × m identity matrix. H(0) = X, H(L) = Z, and W(l)\nis the"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "by combining graph structure loss and classification loss. This loss",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "weight matrix for the lth layer."
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "function will be presented in Subsection 3.3.",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "We present a new graph convolution structure for SER. It con-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "sists of the following two novel components:"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "3.2. A novel matrix computation",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "• Segment-specific graph convolution.\nFor each audio seg-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "Calculating the similarity between each node and its neighboring",
          "social networks of NBA players, players with more followers may": "ment s, we generate a node representation matrix Hs.\nThe graph"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "nodes is crucial\nin graph-based analysis. Dice similarity is a well-",
          "social networks of NBA players, players with more followers may": "convolution layer is represented as:"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "known node similarity measurement function [18]. Since Dice simi-",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "(cid:19)\n(cid:18)"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "− 1\n− 1"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "ˆ"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "larity is calculated directly based on the network topology, it is rela-",
          "social networks of NBA players, players with more followers may": "2\n2\nHs = σ\nD\nAs ˆD\nXsWs\n,\n(6)\ns\ns"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "tively interpretable and saves computing resources. Given two nodes",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "vi and vj, the Dice similarity score is calculated as follows:",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "where ˆD is a degree matrix of As and As = ˜As + φAs\nDice + I."
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "2Con (vi, vj)",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": ",\n(1)\nSDice (vi, vj) =",
          "social networks of NBA players, players with more followers may": "As\nThe\nis a novel matrix calculated by the\ni,j = (i − j)2, As\nDice"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "N (vi) + N (vj)",
          "social networks of NBA players, players with more followers may": ""
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "",
          "social networks of NBA players, players with more followers may": "proposed Eq. (4), and I is an identity matrix. Xs is a feature ma-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "indicates\nthe number of\ncommon neighbors.\nwhere Con (vi, vj)",
          "social networks of NBA players, players with more followers may": "trix and Ws is a weight matrix, where W0 is obtained by random"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "N (vi) and N (vj) denote the number of neighbors of vi and vj (not",
          "social networks of NBA players, players with more followers may": "and Ws+1 = σ (Ws + As ∗ Xs). Different\nfrom the traditional"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "contain the node itself), respectively. As can be seen from Eq. (1),",
          "social networks of NBA players, players with more followers may": "GCN that employs A + I to guide the aggregate of node representa-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "when calculating the similarity between the nodes,\nthe degree of",
          "social networks of NBA players, players with more followers may": "tions. We additionally add the ADice to further guide the aggrega-"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "neighboring nodes does not affect the final result.",
          "social networks of NBA players, players with more followers may": "tion and use a parameter φ = 0.6 to control its contribution. Under"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "However, nodes with higher degrees are more likely to have",
          "social networks of NBA players, players with more followers may": "the guidance of the new aggregation strategy, a higher-quality node"
        },
        {
          "However,\nlike most GCN-based methods,\nthe node aggregation of": "more influence and higher weight values [19]. For example,\nin the",
          "social networks of NBA players, players with more followers may": "representation can be obtained."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": ""
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "(a) Performance comparison on the IEMOCAP dataset."
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "IEMOCAP"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": ""
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "Acc(%)"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": ""
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "56.14"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "63.23"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "65.13"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "65.25"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "65.50"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": ""
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "46.70"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "50.17"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "63.40"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "64.80"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": ""
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "53.46"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "58.69"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "63.58"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "65.64"
        },
        {
          "in different ways, such as the manually defined (Binary), the distance-based (Weighted), the learnable (ALearn), and the proposed node": "66.94"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "65.64\n65.28\nOurs (ADice)": "Ours\n66.94\n66.54",
          "65.69\n65.34\nOurs (ADice)": "Ours\n67.50\n67.05"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "4.2.\nImplementation Details",
          "65.69\n65.34\nOurs (ADice)": "manually defining the\nstructure of\nthe graph,\nthe matrix A only"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "contains 0 and 1; Weighted:\nthe relationship between nodes is"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "we trained the model\nto have up to 1000 epochs, with 150 itera-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "the structure of\nthe\ndetermined based on the distance; ALearn:"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "tions per epoch, and used an early stop strategy. The batch size of",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "the loss function only\ngraph is learned during the training; ADice:"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "the model was set\nto 64. Meantime,\nthe RAdam optimizer with a",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "contains the proposed matrix computation method.\nThe results of"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "learning rate of 0.001 was employed and we set\nthe decay rate to",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "the study are displayed in the “Adjacency matrix” of Table 1. We"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "0.5 after every 150 epochs. We conducted experiments on GeForce",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "have the following observations."
        },
        {
          "65.64\n65.28\nOurs (ADice)": "GTX 3090 Ti, NVIDIA-SMI\n460.39,\nand CUDA Version\n11.2",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "Firstly, both Binary and Weighted determine the relation-"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "GPUs, and obtained the final experimental data by using the 10-fold",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "ship between nodes through customized form, which makes them"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "cross-validation averaging method.",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "easy to form the local optimal graphs during the training process,"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "so the performance is the worst. Secondly, the proposed matrix cal-"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "4.3. Results and Analysis",
          "65.69\n65.34\nOurs (ADice)": "culation method ADice has enhanced performance compared to the"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "learnable ALearn, which shows that our method can effectively dis-"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "Comparison with SOTA methods. Table 1 presents all results on",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "tinguish the significance of different neighboring nodes. Finally, we"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "two datasets. On the IEMOCAP, compared with the graph-based",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "optimize the structure of\nthe graph by jointing the graph structure"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "SOTA approaches, the performance of our model seems only slightly",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "loss and classification loss to achieve the best performance.\nThis"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "better (1.44% of accuracy absolutely) than L-GrIN [3], but the adja-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "indicates that\nthe proposed matrix computation method can update"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "cency matrix A of L-GrIN is obtained by a distance function, which",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "the node representation and that\nthe structure of\nthe graph can be"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "tends to locally optimal graphs during the training. When compared",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "continuously optimized during the training process."
        },
        {
          "65.64\n65.28\nOurs (ADice)": "with other SOTA non-graph methods, we find that our method out-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "performs popular Transformer-based methods.\nThis is mainly at-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "5. CONCLUSION"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "tributed to the fact\nthat we connect neighboring nodes while ran-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "domly connecting distant nodes for\ninformation transfer.\nThe re-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "We propose an adaptive graph representation learning method based"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "lationship between the nodes is calculated by the proposed matrix",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "on dynamically evolved graphs rather\nthan the traditional methods"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "calculation method to guide the aggregation of node information,",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "that only construct a static graph for an entire sequence. Our graphs"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "thus capturing the long-distance information. Table 1(b) also shows",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "are consecutively constructed on a series of subsequences segmented"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "that our model performs better than the graph-based and non-graph-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "by a sliding window. To compute the edge weights, we propose a"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "based approaches on the RAVDESS database.\nFor example, com-",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "new matrix calculation method that updates the node representations"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "pared with the classic GCN model, our model has great performance",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "based on the degree of neighboring nodes. Moreover, we combine"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "advantages (15.83% of accuracy absolutely). This indicates that the",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "the graph structure loss and classification loss to jointly optimize the"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "proposed matrix computation method can alleviate the sub-optimal",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "graph structure during the training. In the future, we will simultane-"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "graph in traditional GCN caused by averaging the information of",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "ously consider combining structural similarity with feature similarity"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "neighboring nodes.",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "to jointly measure the similarity of nodes, and work on multimodal"
        },
        {
          "65.64\n65.28\nOurs (ADice)": "Ablation experiment. We perform ablation studies on two",
          "65.69\n65.34\nOurs (ADice)": ""
        },
        {
          "65.64\n65.28\nOurs (ADice)": "",
          "65.69\n65.34\nOurs (ADice)": "data with graph structures."
        },
        {
          "65.64\n65.28\nOurs (ADice)": "datasets, including the following variations of our model: Binary:",
          "65.69\n65.34\nOurs (ADice)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "munity discovery in networks with deep sparse filtering,” Pat-"
        },
        {
          "6. REFERENCES": "[1] Dou Hu, Xiaolong Hou, Lingwei Wei, Lian-Xin Jiang,\nand",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "tern Recognit., vol. 81, pp. 50–59, 2018."
        },
        {
          "6. REFERENCES": "Yang Mo,\n“MM-DFN: multimodal dynamic fusion network",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[19] Zemin Liu, Trung-Kien Nguyen, and Yuan Fang, “On general-"
        },
        {
          "6. REFERENCES": "for emotion recognition in conversations,”\nin ICASSP, 2022,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "ized degree fairness in graph neural networks,” in AAAI, 2023,"
        },
        {
          "6. REFERENCES": "pp. 7037–7041.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "pp. 4525–4533."
        },
        {
          "6. REFERENCES": "[2] Amir Shirian and Tanaya Guha,\n“Compact graph architecture",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[20] Zhitao Ying,\nJiaxuan You, Christopher Morris, Xiang Ren,"
        },
        {
          "6. REFERENCES": "for speech emotion recognition,” in ICASSP, 2021, pp. 6284–",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "William L. Hamilton,\nand\nJure Leskovec,\n“Hierarchical"
        },
        {
          "6. REFERENCES": "6288.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "graph representation learning with differentiable pooling,”\nin"
        },
        {
          "6. REFERENCES": "[3] Amir Shirian, Subarna Tripathi, and Tanaya Guha,\n“Dynamic",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "NeurIPS, 2018, pp. 4805–4815."
        },
        {
          "6. REFERENCES": "emotion modeling with learnable graphs and graph inception",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[21] Binqiang Wang, Gang Dong, Yaqian Zhao, Rengang Li,"
        },
        {
          "6. REFERENCES": "network,” IEEE Trans. Multim., vol. 24, pp. 780–790, 2022.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "Qichun Cao, Kekun Hu, and Dongdong Jiang, “Hierarchically"
        },
        {
          "6. REFERENCES": "[4] Huan Zhao, Zhengwei Li, Zhu-Hong You, Ru Nie, and Tangbo",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "stacked graph convolution for emotion recognition in conver-"
        },
        {
          "6. REFERENCES": "Zhong, “Predicting mirna-disease associations based on neigh-",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "sation,” Knowl. Based Syst., vol. 263, pp. 110285, 2023."
        },
        {
          "6. REFERENCES": "IEEE ACM Trans.\nbor\nselection graph attention networks,”",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[22] Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niy-"
        },
        {
          "6. REFERENCES": "Comput. Biol. Bioinform., vol. 20, no. 2, pp. 1298–1307, 2023.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "ati Chhaya, and Alexander F. Gelbukh, “Dialoguegcn: A graph"
        },
        {
          "6. REFERENCES": "[5] Bo Wu, Xun Liang, Xiangping Zheng, Yuhui Guo, and Hui",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "convolutional neural network for emotion recognition in con-"
        },
        {
          "6. REFERENCES": "Tang,\n“Improving dynamic graph convolutional network with",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "versation,” in EMNLP, 2019, pp. 154–164."
        },
        {
          "6. REFERENCES": "fine-grained attention mechanism,”\nin ICASSP, 2022,\npp.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "3938–3942.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[23]\nSamir Sadok, Simon Leglaive, and Renaud S´eguier,\n“A vec-"
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "tor quantized masked autoencoder for speech emotion recog-"
        },
        {
          "6. REFERENCES": "[6] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena, “Deepwalk:",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "nition,” in ICASSP, 2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "online learning of social representations,”\nin KDD, 2014, pp.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "701–710.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[24]\nJack Parry, Dimitri Palaz, Georgia Clarke, Pauline Lecomte,"
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "Rebecca Mead, Michael Berger, and Gregor Hofer,\n“Analysis"
        },
        {
          "6. REFERENCES": "[7] Aditya Grover and Jure Leskovec, “node2vec: Scalable feature",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "of deep learning architectures for cross-corpus speech emotion"
        },
        {
          "6. REFERENCES": "learning for networks,” in KDD, 2016, pp. 855–864.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "recognition,” in INTERSPEECH, 2019, pp. 1656–1660."
        },
        {
          "6. REFERENCES": "[8] Thomas N. Kipf and Max Welling, “Semi-supervised classifi-",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "cation with graph convolutional networks,” in ICLR, 2017.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[25] Navonil Majumder,\nSoujanya Poria, Devamanyu Hazarika,"
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "Rada Mihalcea, Alexander F. Gelbukh, and Erik Cambria, “Di-"
        },
        {
          "6. REFERENCES": "[9] William L. Hamilton, Zhitao Ying, and Jure Leskovec,\n“In-",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "aloguernn: An attentive RNN for emotion detection in conver-"
        },
        {
          "6. REFERENCES": "ductive representation learning on large graphs,”\nin NeurIPS,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "sations,” in AAAI, 2019, pp. 6818–6825."
        },
        {
          "6. REFERENCES": "2017, pp. 1024–1034.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[26] Zheng Liu, Xin Kang, and Fuji Ren,\n“Dual-tbnet:\nImproving"
        },
        {
          "6. REFERENCES": "[10] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "the robustness of speech features via dual-transformer-bilstm"
        },
        {
          "6. REFERENCES": "Kannan, and Viktor K. Prasanna, “Graphsaint: Graph sampling",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "IEEE ACM Trans. Audio\nfor\nspeech emotion recognition,”"
        },
        {
          "6. REFERENCES": "based inductive learning method,” in ICLR, 2020.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "Speech Lang. Process., vol. 31, pp. 2193–2203, 2023."
        },
        {
          "6. REFERENCES": "[11] Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[27] Esma Mansouri-Benssassi and Juan Ye,\n“Synch-graph: Mul-"
        },
        {
          "6. REFERENCES": "Shiqiang Yang, “Community preserving network embedding,”",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "tisensory emotion recognition through neural\nsynchrony via"
        },
        {
          "6. REFERENCES": "in AAAI, 2017, pp. 203–209.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "graph convolutional networks,” in AAAI, 2020, pp. 1351–1358."
        },
        {
          "6. REFERENCES": "[12]\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adri-",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "ana Romero, Pietro Li`o, and Yoshua Bengio, “Graph attention",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[28] Esma Mansouri-Benssassi\nand Juan Ye,\n“Speech emotion"
        },
        {
          "6. REFERENCES": "networks,” in ICLR, 2018.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "recognition with early visual cross-modal enhancement using"
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "spiking neural networks,” in IJCNN, 2019, pp. 1–8."
        },
        {
          "6. REFERENCES": "[13] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xin-",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "chao Wang,\n“Distilling knowledge from graph convolutional",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[29] T¨urker Tuncer, Seng¨ul Dogan, and U. Rajendra Acharya, “Au-"
        },
        {
          "6. REFERENCES": "networks,” in CVPR, 2020, pp. 7072–7081.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "tomated accurate\nspeech emotion recognition system using"
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "twine shuffle pattern and iterative neighborhood component"
        },
        {
          "6. REFERENCES": "[14] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "analysis techniques,” Knowl. Based Syst., vol. 211, pp. 106547,"
        },
        {
          "6. REFERENCES": "Hao Yang,\n“Dysat: Deep neural\nrepresentation learning on",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "2021."
        },
        {
          "6. REFERENCES": "dynamic graphs via self-attention networks,” in WSDM, 2020,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "pp. 519–527.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[30] Riccardo Franceschini, Enrico Fini, Cigdem Beyan, Alessan-"
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "dro Conti, Federica Arrigoni, and Elisa Ricci,\n“Multimodal"
        },
        {
          "6. REFERENCES": "[15] Aldo Pareja, Giacomo Domeniconi,\nJie Chen, Tengfei Ma,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "emotion recognition with modality-pairwise unsupervised con-"
        },
        {
          "6. REFERENCES": "Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao B.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "trastive loss,” in ICPR, 2022, pp. 2589–2596."
        },
        {
          "6. REFERENCES": "Schardl,\nand Charles E. Leiserson,\n“Evolvegcn:\nEvolving",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "graph convolutional networks for dynamic graphs,”\nin AAAI,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[31] Esam Ghaleb, Jan Niehues, and Stylianos Asteriadis,\n“Joint"
        },
        {
          "6. REFERENCES": "2020, pp. 5363–5370.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "modelling of audio-visual cues using attention mechanisms for"
        },
        {
          "6. REFERENCES": "[16] Yonghua Zhu, Junbo Ma, Changan Yuan, and Xiaofeng Zhu,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "emotion recognition,” Multim. Tools Appl., vol. 82, no. 8, pp."
        },
        {
          "6. REFERENCES": "“Interpretable learning based dynamic graph convolutional net-",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "11239–11264, 2023."
        },
        {
          "6. REFERENCES": "works for alzheimer’s disease analysis,”\nInf. Fusion, vol. 77,",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "[32] Yuni Zeng, Hua Mao, Dezhong Peng, and Zhang Yi,\n“Spec-"
        },
        {
          "6. REFERENCES": "pp. 53–61, 2022.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "trogram based multi-task audio classification,” Multim. Tools"
        },
        {
          "6. REFERENCES": "[17] Amir Shirian, Mona Ahmadian, Krishna Somandepalli,\nand",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": "Appl., vol. 78, no. 3, pp. 3705–3722, 2019."
        },
        {
          "6. REFERENCES": "Tanaya Guha,\n“Heterogeneous graph learning for\nacoustic",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        },
        {
          "6. REFERENCES": "event classification,” in ICASSP, 2023, pp. 1–5.",
          "[18] Yu Xie, Maoguo Gong, Shanfeng Wang, and Bin Yu,\n“Com-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "3",
      "title": "Compact graph architecture for speech emotion recognition",
      "authors": [
        "Amir Shirian",
        "Tanaya Guha"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Dynamic emotion modeling with learnable graphs and graph inception network",
      "authors": [
        "Subarna Amir Shirian",
        "Tanaya Tripathi",
        "Guha"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "5",
      "title": "Predicting mirna-disease associations based on neighbor selection graph attention networks",
      "authors": [
        "Huan Zhao",
        "Zhengwei Li",
        "Zhu-Hong You",
        "Ru Nie",
        "Tangbo Zhong"
      ],
      "year": "2023",
      "venue": "IEEE ACM Trans. Comput. Biol. Bioinform"
    },
    {
      "citation_id": "6",
      "title": "Improving dynamic graph convolutional network with fine-grained attention mechanism",
      "authors": [
        "Bo Wu",
        "Xun Liang",
        "Xiangping Zheng",
        "Yuhui Guo",
        "Hui Tang"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Deepwalk: online learning of social representations",
      "authors": [
        "Bryan Perozzi",
        "Rami Al-Rfou",
        "Steven Skiena"
      ],
      "year": "2014",
      "venue": "KDD"
    },
    {
      "citation_id": "8",
      "title": "node2vec: Scalable feature learning for networks",
      "authors": [
        "Aditya Grover",
        "Jure Leskovec"
      ],
      "year": "2016",
      "venue": "KDD"
    },
    {
      "citation_id": "9",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "Thomas Kipf",
        "Max Welling"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "10",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "William Hamilton",
        "Zhitao Ying",
        "Jure Leskovec"
      ],
      "year": "2017",
      "venue": "Inductive representation learning on large graphs"
    },
    {
      "citation_id": "11",
      "title": "Graphsaint: Graph sampling based inductive learning method",
      "authors": [
        "Hanqing Zeng",
        "Hongkuan Zhou",
        "Ajitesh Srivastava",
        "Rajgopal Kannan",
        "Viktor Prasanna"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "12",
      "title": "Community preserving network embedding",
      "authors": [
        "Xiao Wang",
        "Peng Cui",
        "Jing Wang",
        "Jian Pei",
        "Wenwu Zhu",
        "Shiqiang Yang"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Graph attention networks",
      "authors": [
        "Petar Velickovic",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "14",
      "title": "Distilling knowledge from graph convolutional networks",
      "authors": [
        "Yiding Yang",
        "Jiayan Qiu",
        "Mingli Song",
        "Dacheng Tao",
        "Xinchao Wang"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "Dysat: Deep neural representation learning on dynamic graphs via self-attention networks",
      "authors": [
        "Aravind Sankar",
        "Yanhong Wu",
        "Liang Gou",
        "Wei Zhang",
        "Hao Yang"
      ],
      "year": "2020",
      "venue": "Dysat: Deep neural representation learning on dynamic graphs via self-attention networks"
    },
    {
      "citation_id": "16",
      "title": "Evolvegcn: Evolving graph convolutional networks for dynamic graphs",
      "authors": [
        "Aldo Pareja",
        "Giacomo Domeniconi",
        "Jie Chen",
        "Tengfei Ma",
        "Toyotaro Suzumura",
        "Hiroki Kanezashi",
        "Tim Kaler",
        "B Tao",
        "Charles Schardl",
        "Leiserson"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "17",
      "title": "Interpretable learning based dynamic graph convolutional networks for alzheimer's disease analysis",
      "authors": [
        "Yonghua Zhu",
        "Junbo Ma",
        "Changan Yuan",
        "Xiaofeng Zhu"
      ],
      "year": "2022",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "18",
      "title": "Heterogeneous graph learning for acoustic event classification",
      "authors": [
        "Mona Amir Shirian",
        "Krishna Ahmadian",
        "Tanaya Somandepalli",
        "Guha"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Community discovery in networks with deep sparse filtering",
      "authors": [
        "Yu Xie",
        "Maoguo Gong",
        "Shanfeng Wang",
        "Bin Yu"
      ],
      "year": "2018",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "20",
      "title": "On generalized degree fairness in graph neural networks",
      "authors": [
        "Zemin Liu",
        "Trung-Kien Nguyen",
        "Yuan Fang"
      ],
      "year": "2023",
      "venue": "AAAI"
    },
    {
      "citation_id": "21",
      "title": "Hierarchical graph representation learning with differentiable pooling",
      "authors": [
        "Zhitao Ying",
        "Jiaxuan You",
        "Christopher Morris",
        "Xiang Ren",
        "William Hamilton",
        "Jure Leskovec"
      ],
      "year": "2018",
      "venue": "Hierarchical graph representation learning with differentiable pooling"
    },
    {
      "citation_id": "22",
      "title": "Hierarchically stacked graph convolution for emotion recognition in conversation",
      "authors": [
        "Binqiang Wang",
        "Gang Dong",
        "Yaqian Zhao",
        "Rengang Li",
        "Qichun Cao",
        "Kekun Hu",
        "Dongdong Jiang"
      ],
      "year": "2023",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "23",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "24",
      "title": "A vector quantized masked autoencoder for speech emotion recognition",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "25",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "Michael Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "Analysis of deep learning architectures for cross-corpus speech emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "27",
      "title": "Dual-tbnet: Improving the robustness of speech features via dual-transformer-bilstm for speech emotion recognition",
      "authors": [
        "Zheng Liu",
        "Xin Kang",
        "Fuji Ren"
      ],
      "year": "2023",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "28",
      "title": "Synch-graph: Multisensory emotion recognition through neural synchrony via graph convolutional networks",
      "authors": [
        "Esma Mansouri",
        "- Benssassi",
        "Juan Ye"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition with early visual cross-modal enhancement using spiking neural networks",
      "authors": [
        "Esma Mansouri",
        "- Benssassi",
        "Juan Ye"
      ],
      "year": "2019",
      "venue": "IJCNN"
    },
    {
      "citation_id": "30",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "Türker Tuncer",
        "Sengül Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "31",
      "title": "Multimodal emotion recognition with modality-pairwise unsupervised contrastive loss",
      "authors": [
        "Riccardo Franceschini",
        "Enrico Fini",
        "Cigdem Beyan",
        "Alessandro Conti",
        "Federica Arrigoni",
        "Elisa Ricci"
      ],
      "year": "2022",
      "venue": "ICPR"
    },
    {
      "citation_id": "32",
      "title": "Joint modelling of audio-visual cues using attention mechanisms for emotion recognition",
      "authors": [
        "Esam Ghaleb",
        "Jan Niehues",
        "Stylianos Asteriadis"
      ],
      "year": "2023",
      "venue": "Multim. Tools Appl"
    },
    {
      "citation_id": "33",
      "title": "Spectrogram based multi-task audio classification",
      "authors": [
        "Yuni Zeng",
        "Hua Mao",
        "Dezhong Peng",
        "Zhang Yi"
      ],
      "year": "2019",
      "venue": "Multim. Tools Appl"
    }
  ]
}