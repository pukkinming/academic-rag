{
  "paper_id": "2005.08642v1",
  "title": "Atom Search Optimization With Simulated Annealinga Hybrid Metaheuristic Approach For Feature Selection",
  "published": "2020-05-10T07:56:58Z",
  "authors": [
    "Kushal Kanti Ghosh",
    "Ritam Guha",
    "Soulib Ghosh",
    "Suman Kumar Bera",
    "Ram Sarkar"
  ],
  "keywords": [
    "Atom Search Optimization",
    "Feature selection",
    "UCI data",
    "Handwritten digit recognition",
    "Text/non-text classification",
    "Facial emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Hybrid meta-heuristics' is one of the most interesting recent trends in the field of optimization and feature selection (FS). In this paper, we have proposed a binary variant of Atom Search Optimization (ASO) and its hybrid with Simulated Annealing called ASO-SA techniques for FS. In order to map the real values used by ASO to the binary domain of FS, we have used two different transfer functions: S-shaped and V-shaped. We have hybridized this technique with a local search technique called, SA We have applied the proposed feature selection methods on 25 datasets from 4 different categories: UCI, Handwritten digit recognition, Text/non-text separation, and Facial emotion recognition. We have used 3 different classifiers (K-Nearest Neighbor, Multi-Layer Perceptron and Random Forest) for evaluating the strength of the selected featured by the binary ASO, ASO-SA and compared the results with ‚â•4 recent wrapper-based algorithms. The experimental results confirm the superiority of the proposed method both in terms of classification accuracy and number of selected features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent times, due to the wide availability of data and the crucial need to extract useful information from these data, in information industry data mining is one of the fastest growing research topics (M.  Mafarja et al., 2018) . Data mining is an integral part of knowledge discovery (M. M.  Mafarja & Mirjalili, 2017) , which consists of data pre-processing, data mining, pattern evaluation and knowledge representation. The pre-processing step has a huge impact on the data mining technique. Now, all the extracted features are not useful for data mining, as some of those are irrelevant or redundant  (Ghaemi & Feizi-Derakhshi, 2016) , which affects the data mining tasks (e.g., classification). Not only that these irrelevant or redundant features become burden in process of knowledge discovery, but also they result in increased computational complexity, incomprehensibility of the results and augmented storage requirement  (Ghaemi & Feizi-Derakhshi, 2016) . Here comes the importance of feature selection (FS) techniques. FS  (Anaraki & Usefi, 2019)  is a pre-processing step that helps in dimensionality reduction of the dataset being processed by eliminating the irrelevant/redundant features. FS is a proper subset of feature weighting  (Wettschereck, Aha, & Mohri, 1997) , in which a feature is assigned a weight according to their importance in solving a problem using some machine learning approach. FS is subset of feature weighting, where weights are either '1' or '0', i.e., a feature is either selected or rejected.\n\nCoarsely, the main steps of FS  (DASH & LIU, 1997)  are: subset generation, subset evaluation, setting end criteria and validation. Based upon the subset evaluation criteria, FS methods can be divided in two types: filter and wrapper. Wrapper methods use a learning algorithm to measure the worth of a feature subset (M.  Ghosh, Adhikary, et al., 2019) . In filter methods, no learning algorithm is used, instead, the importance of a feature or a subset of features is determined based on some specific characteristics of the dataset under consideration (M.  Ghosh, Adhikary, et al., 2019) .\n\nThe subset generation is basically a searching technique that selects a subset from the original set using complete, random or heuristic search. In complete search, all possible subsets are evaluated to find the best one. Though this process guarantees the finding of best feature subset, for n number of features we need to evaluate 2 n subsets. Obviously the computational complexity of this method is too high, thus making a complete search non-feasible option (M.  Ghosh, Adhikary, et al., 2019) . Random search is another strategy for generating feature subset (M.  Mafarja et al., 2018) , but in worst case it may perform an exhaustive search. Heuristic search is another alternative to the mentioned search strategies. It can also be called depth first search guided by heuristics (M.  Mafarja et al., 2018) . Various meta-heuristics such as Genetic Algorithm  (Yang & Honavar, 1998) , Ant Colony Optimization (ACO)  (Dorigo & Di Caro, n.d.) , Particle Swarm Optimization (PSO)  (Lee, Soak, Oh, Pedrycz, & Jeon, 2008) , Dragonfly Algorithm (M.  Mafarja, Heidari, Faris, Mirjalili, & Aljarah, 2020) , Grasshopper Optimization Algorithm (GOA)  (Saremi, Mirjalili, & Lewis, 2017) , Forest Optimization Algorithm  (Ghaemi & Feizi-Derakhshi, 2016) , Grey Wolf Optimizer (GWO)  (Emary, Zawbaa, Grosan, & Hassenian, 2015) , etc. have shown quite good results in tackling FS problems. Meta-heuristic algorithms can be classified mainly in two categories (M.  Mafarja et al., 2018) : single solution and population based algorithms. In single solution-based methods, one state is manipulated and transformed throughout the search process. Methods in this category show more exploitative nature, which implies focusing on the space around a possible solution. In populationbased algorithms, a set of solutions is evolved during the search process. These methods are more explorative in nature, which means looking around different regions of the space. While designing a meta-heuristic algorithm, both of these criteria must be considered. High exploitation may result in the optimizer being trapped in local optima whereas high exploration may lead to selection of outliers.\n\nAtom Search Optimization (ASO)  (Zhao, Wang, & Zhang, 2019b)  is the newest addition to the domain of meta-heuristic algorithms. ASO tries to mimic the movement of atoms according to Lennard-Jones potential  (Stone, 2013)  developed among interacting atoms. In recent past, the algorithm has gained popularity as a nature-inspired optimization algorithm and has been applied to solve various interesting optimization problems like hydro-geologic parameter estimation  (Zhao et al., 2019b) , dispersion coefficient estimation  (Zhao, Wang, & Zhang, 2019a)  etc. But to the best of our knowledge, the optimization procedure used in ASO has not ever been applied to solve any FS problems. But the very characteristics of ASO makes it a suitable choice for FS problems. This has motivated us to develop the version of ASO which is applicable to FS. In addition, to improve the exploitation ability of the basic ASO, we have combined a popular local search technique called Simulated Annealing (SA) with the existing ASO version. Finally, to prove the robustness and applicability of ASO in the FS domain, we have made a case study by applying our ASO-SA based FS model over varied datasets which include UCI, text/non-text classification, digit recognition and facial emotion recognition.\n\nIn this paper, we have presented our experimental findings regarding the newly developed FS version of ASO. Our primary contributions to this paper are listed below:\n\n‚Ä¢ Development of the FS version of ASO for the first time.\n\n‚Ä¢ Hybridization of ASO with SA denoted by ASO-SA to improve exploitation ability of basic ASO.\n\n‚Ä¢ A detailed case study to show the usefulness of ASO and ASO-SA in the FS domain by applying them over UCI, text/non-text classification, digit recognition and facial emotion recognition datasets.\n\nThe rest of the paper is organized as follows: Section 2 provides a brief overview of the other works done by researchers in the domain of FS. Section 3 describes the proposed work in detail. The results and findings of our case study are presented in section 4. Section 5 concludes the paper and provides the scope of future works.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Over the years, researchers have found that simple natural phenomena have gifted abilities to provide solutions to many hard optimization problems. Motivated from these findings, people have implemented and used various nature and bio-inspired meta-heuristic algorithms to solve the optimization problems. One such highly addressed optimization problem is FS where researchers have proposed many algorithms to search for a near-optimal subset of features within a limited time span.\n\nOne of the most fundamental optimization algorithms applied to solve FS problems is Genetic Algorithm (GA). After its inception as an optimization algorithm  (Holland, 1992) , GA has been used to solve multiple optimization problems like gene selection in cancer classification  (Alba, Garcia-Nieto, Jourdan, & Talbi, 2007) , stock market data mining optimization  (Lin, Cao, Wang, & Zhang, 2004) , component designs  (Husbands, Jermy, McIlhagga, & Ives, 1996)  etc. A more comprehensive set of applications of GA can be found in  (Tang, Man, Kwong, & He, 1996) . After the introduction of GA to the domain of FS by  Leardi et al. in (Leardi, 1996) , it has been used extensively to solve various FS problems  (Basiri & Nemati, 2009; De Stefano, Fontanella, Marrocco, & Di Freca, 2014; Il-Seok Oh, Jin-Seon Lee, & Byung-Ro Moon, 2004; Malakar, Ghosh, Bhowmik, Sarkar, & Nasipuri, 2019; Nemati, Ehsan, Ghasem-aghaee, & Hosseinzadeh, 2009; Prasad, Biswas, & Jain, 2010) . GA uses the concepts of chromosome crossover and mutation to achieve a balance in exploration and exploitation of the search space. Inspired from the success of GA in the FS domain, researchers have made efforts to adapt other popular metaheuristic optimization algorithms to FS. As a result, some highly used optimization algorithms like PSO  (Chen, Zhou, & Yuan, 2019; C. Huang & Dun, 2008; Wang, Yang, Teng, Xia, & Jensen, 2007; Xue, Zhang, & Browne, 2012) , ACO  (Basiri & Nemati, 2009; Forsati, Moayedikia, Jensen, Shamsfard, & Meybodi, 2014; C. L. Huang, 2009; Kabir, Shahjahan, & Murase, 2012; Kashef & Nezamabadi-pour, 2015) , GSA  (Papa et al., 2011; E. Rashedi & Nezamabadi-Pour, 2014; Esmat Rashedi & Nezamabadi-pour, 2012; Esmat Rashedi, Nezamabadi-Pour, & Saryazdi, 2010b)  has been re-introduced to the research community with their binary versions to solve FS problems. These binary versions have performed well in the domain of FS and have become quite famous in the research community. But each such algorithm suffers from a fundamental problem i.e. finding a proper trade-off for exploration and exploitation. The next generation of researchers have made efforts to solve this problem by hybridizing various algorithms together  (Anter, Azar, & Fouad, 2019; Badawi, Khalil, & Alsmadi, 2013; Trivedi, Srivastava, Mishra, Shukla, & Tiwari, 2018) . Although some algorithms have performed better than the others to find a proper balance between exploitation and exploration, the problem still remained. In the search of a solution to this problem, researchers have started to exploit the searching behaviour of other natural objects and tried to mimic them which resulted in the introduction of some recently proposed metaheuristic algorithms in FS like Whale Optimization Algorithm (WOA)  (Hussien, Hassanien, Houssein, Bhattacharyya, & Amin, 2019; M. M. Mafarja & Mirjalili, 2017; Sharawi, Zawbaa, & Emary, 2017) ,  GWO (Abdel-Basset, El-Shahat, El-henawy, de Albuquerque, & Mirjalili, 2020; Emary, Zawbaa, & Hassanien, 2016) , GOA (M.  Mafarja et al., 2019) , Artificial Bee Colony (ABC)  (Zhang, Cheng, Shi, Gong, & Zhao, 2019) , Dragonfly algorithm (M. M.  Mafarja, Eleyan, Jaber, Hammouri, & Mirjalili, 2017)  etc. The applicability of these algorithms in the FS domain have inspired us to develop a binary version of a recently proposed optimization technique named ASO and to apply it over various datasets.  Zhao et al. proposed  the ASO model in  (Zhao et al., 2019a)  to estimate dispersion coefficient in ground water. Through the proposed model, the authors have tried to mimic the characteristics of atomic motion. According to the concepts of dynamics of atoms, Lennard-Jones potential and bondlength potential among atoms produce the interaction forces among atoms which guides their movements. Using these concepts, the ASO model has been developed to provide solution to optimization problems. The authors have applied ASO for dispersion coefficient estimation of ground water. The experimental outcomes prove that ASO is able to outperform other popular optimization approaches like PSO, GSA and Bacterial Foraging Optimization (BFO)  (Passino, 2010) . Zhao et al. have used the same procedure again to solve hydrogeological parameter estimation problem in  (Zhao et al., 2019b) . In this paper, they have provided the experimental outcomes of the application of ASO over 37 benchmark unimodal, multimodal, hybrid, low-dimensional and composite functions. The results obtained by ASO have been tallied against PSO, GA, SA, GSA and Wind Driven Optimization (WDO)  (Bayraktar, Komurcu, Bossard, & Werner, 2013) . The excellent results of ASO clearly establishes its applicability in the optimization domain. But ASO has never been used to solve FS problems. The searching prowess of ASO has motivated us to apply it to search for important feature subsets from feature sets in various contexts. That is why we have developed a binary version of ASO with the help of transformation functions to map real values in ASO to binary values. In addition, to prove the robustness of this version of ASO, we have applied it over datasets and features with different backgrounds.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Present Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Brief Overview Of Atom Search Optimization (Aso)",
      "text": "ASO is inspired by basic molecular dynamics. Molecular dynamics was first proposed in the field of theoretical physics  (Alder & Wainwright, 1959)  but afterwards it has been used also in other fields of science like biology, chemistry and materials science. The atomic motion follows the classical mechanics  (Goldstein, Poole, Safko, & Addison, 2002) . According to second law of Newton, if F i is the interaction force and G i is the constraint force working on the i th atom and the atom has mass m i then the acceleration of the atom is  (Ryckaert, Ciccotti, & Berendsen, 1977) :\n\n(1)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interaction Force",
      "text": "Lennard-Jones (LJ) potential  (Stone, 2013)  is a simple mathematical model that approximates the interaction force between a pair of atoms. The LJ potential between i th and j th atom is given by\n\nWhere, Œµ is the depth of the potential well  (Zhao et al., 2019b) , œÉ is the finite distance at which the inter-particle potential is zero, r ij = x j -x i where x i = (x i1 , x i2 , ‚Ä¶ , x in ) and x j = (x j1 , x j2 , ‚Ä¶ , x jn ) are the positions of ith and jth atom in the nth dimension. So the Euclidian distance between x i and\n\nx j is given by r ij = ‚Äñx j -x i ‚Äñ = ‚àö (x i1 -x j1 ) 2 + (x i2 -x j2 ) 2 + ‚ãØ + (x in -x jn ) 2 . The terms ( Figure  1 : LJ potential curve with attraction and repulsion region. In the attractive zone, the attraction force gradually decreases to zero as the distance between the atoms increases. In the repulsive zone, the repulsive force rapidly increases as the distance decreases. At the equilibrium distance (r=1.2œÉ), the interaction force between the atoms is zero. At that point, the minimum bonding potential energy of the atoms is reached.\n\nThe interaction force working on i th atom from j th atom in the d th dimension at the t th time is given by  (Zhao et al., 2019a) :\n\nAnd\n\nThe atom keeps a relative distance (from each other) that varies in a certain range all the time due to attraction or repulsion, and the change of amplitude in repulsion relative to equilibrium distance is much greater than that in attraction. Repulsion is positive and attraction is negative, so the atoms cannot converge to a specific position. Equation (  4 ) cannot be directly used for optimization purpose. So the authors of  (Zhao et al., 2019b)  have proposed a modified version of this for optimization problems:\n\nwhere Œ∑(t) is depth function to adjust the attraction or repulsion region. It is defined as:\n\nwhere T is the maximum number of iteration and Œ± is the depth weight.\n\nh min and h max are lower limit and upper limit of h ij respectively. The length scale œÉ(t) is defined as\n\n‚Äñ, where KBest is a subset of K atoms with the best function fitness values.\n\nwhere u is the upper limit and g is a drift function that helps the algorithm to drift from exploration to exploitation and is given by:\n\nNow the total force working on the ith atom from all other atoms is the weighted sum of components of the forces in dth dimension:\n\nwhere random j is a random number in [0,1].",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Geometric Constraints:",
      "text": "In the motion of atoms, the geometric constraint in molecular dynamics plays a vital role. For simplicity, in ASO an atom is assumed  (Zhao et al., 2019b)  to have a covalent bond with the best atom, so each atom is acted upon by a constraint force by the best atom. This constraint force acting on the i th atom in the d th dimension is defined as:\n\nwhere, Œª(t) is the Lagrangian multiplier, defined as\n\nwhere, Œ≤ is the multiplier weight.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Atomic Motion:",
      "text": "Considering both interaction force and geometric constraints, the acceleration of the i th atom at time t in d th dimension is\n\nm i (t) is the mass of the i th atom at t th iteration. It is determined as:\n\nThe position and velocity of the i th atom at time (t + 1) are given by:\n\nIn order to apply it to the optimization problem, exploration ability needs to be enhanced. Hence in the beginning, each atom needs to interact with large number of atoms with better fitness values, as its neighbors, so the value of K must be high. To enhance exploitation towards the ending of the algorithm, number of neighbors K must be decreased. So, K is calculated as:",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Binary Aso For Feature Selection:",
      "text": "In this section, we describe the customizations we made to use ASO for FS. The proposed methodology is wrapper-based FS method. In wrapper methods, the subsets of features are evaluated using a classifier to select the optimum subset. We have used ASO as the search method to find the optimal subset from the original feature set. We have used Multi-layer Perceptron, Random-Forest and K-Nearest Neighbor classifiers for the subset evaluation purpose. To use ASO for FS purpose, we need to define two things accordingly: representing an atom and defining a fitness function.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Atom Representation:",
      "text": "FS is considered as a binary optimization problem, i.e., solutions are restricted to binary values. To represent a FS problem, we need a vector of 0s and 1s, where 0 represents the corresponding feature is not selected and 1 represents selected. The vector length is equal to the number of features in the original dataset. In this case, a binary feature vector is considered as an atom.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fitness Function:",
      "text": "The fitness function consists of classification error and number of selected features. This justifies the fact that FS can be considered a multi-objective problem that aims to find smallest subset with best accuracy. The fitness function is defined as:\n\n|X| and |D| represent respectively number of selected features and total number of features, Œµ is the classification error rate of the corresponding classifier and œâœµ[0,1] is the weight.\n\nSince in FS problem the knowledge representation is binary in nature, the atoms can move by flipping various numbers of bits. So, we need to modify the position updating process. In continuous version of ASO, the atoms can move around the search space utilizing the position vectors within the continuous real domain. So, position can be easily updated by adding velocity using equation (  17 ). But in binary space, since we deal with only 0 and 1, the same formula to update position will not work. So, we need a way to map velocity and position to fit into binary positions. Now, according to  (Kennedy & Eberhart, 1997) , we can change the position of an atom based on the probability of velocity of that atom. To do this, a transfer function is required to map velocity values to probability values. After converting velocity values to probability values, we can update the position vectors using the corresponding probability of velocities as:\n\nwhere, T is the transfer function. For this work we have set rand = 0.5. Actually, transfer functions force atoms to move in a binary space  (Mirjalili & Lewis, 2013) . According to (Esmat Rashedi, Nezamabadi-Pour, & Saryazdi, 2010a), to select a transfer function for mapping velocity values to probability values some points must be maintained:\n\n1. Since transfer functions indicate probability, the range must be [0,1] 2. For a large absolute value of the velocity, transfer function should output high probability for position change (0 to 1 or vice-versa). Similarly, for small absolute value of the velocity, the output should be low. 3. With the increase of velocity, the value of the transfer function must rise. If an atom is moving away from the best solution, it must have a higher probability of changing its position to previous one. Similarly, with the decrease of the velocity the value of the transfer function must decrease.\n\nIn this work, we have used two different types of transfer functions  (Mirjalili & Lewis, 2013 ):\n\n1. S-shaped Transfer Function:",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Parameters Of Aso:",
      "text": "As parameters, ASO algorithm needs: number of atoms in a population, maximum number of iterations and dimension of the problem. In FS, this dimension is the number of features representing the corresponding dataset. In equation (  2 ), we set u = 1.24 following the work  (Zhao et al., 2019a) . The other parameters to be determined are depth (Œ±) and multiplier weight (Œ≤). Following the works of  (Zhao et al., 2019b) , we set Œ± and Œ≤ as Œ± = 50 and Œ≤ = 0.2 for our experiments. We have set number of atoms in the population as 20 and maximum number of iterations as 30.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Simulated Annealing-Based Aso (Aso-Sa):",
      "text": "One of the main challenges in FS is to search for a proper trade-off between exploration and exploitation. Each FS algorithm has its own way to find such a balance. In ASO, this trade-off is managed by the value of K(t) which is computed by equation (  16 ). In order to ensure exploration at the initial stage, the value of K(t) is kept large (for small t values). Large value of K(t) lets each atom interact with many of its neighbors having better fitness values. With increasing iteration (t), the value of K(t) decreases and the atoms interact with a smaller number of its neighbors having better fitness values which, in turn, enhances exploitation in the later stages. Thus, ASO ensures exploration during initial iterations and exploitation during final iterations. But, the value of K(t) may become as low as 2 which cannot lead to proper exploitation of the search space and makes it almost impossible to avoid local optima. To increase the exploitation ability of ASO and bypass the stagnation of atoms in local optima, we have embedded a local search technique named Simulated Annealing (SA) into ASO. Proposed by Kirkpatric et al. in  (Kirkpatrick, Gelatt, & Vecchi, 1983) , SA is hill climbing based single-solution meta-heuristic procedure which accepts worse solutions with some probability in the hope to circumvent local optima. After each iteration of ASO, each atom in the final set of population undergoes SA to improve the quality of the solution and to overcome local optima. The algorithm begins with creating a neighbor of an atom through perturbation of the atom. If the fitness of the neighbor exceeds that of the atom, then the neighbor is always accepted as a solution else it is accepted with certain probability determined by Boltzmann probability which is defined in equation (  21 ) where curFitness is the fitness of the generated neighbor and bestFitness is the fitness of the best solution found so far among all the generated neighbors. Temp is an important parameter in the process known as the temperature which determines the number of times neighbors needs to be evaluated for a particular atom. Temp is initialized to 2 * |N| where |N| is the total number of features present in the dataset and decreased as Temp = 0.93 * Temp (cooling schedule) as used in  (Jensen & Shen, 2003) .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Boltzmann Probability P = E -(Curfitness-Bestfitness) Temp",
      "text": "(21)\n\nIn this way, we have made an attempt to combine the concepts of SA with ASO to improve the exploitation ability of ASO. The entire flow of the ASO-SA algorithm is presented in Figure  4 . The basic ASO and SA are separately marked. These two processes interact with each other to form solutions of ASO-SA.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experimentation",
      "text": "In order to evaluate the effectiveness of the proposed FS method, we have applied it to different pattern classification problems. We have shown how our proposed method ensures the same accuracy (as obtained with the original feature vector), if not better, using much smaller number of features selected by the FS algorithm. Again, to prove its effectiveness, we have compared it with other well-known FS algorithms. We have used four case studies considering four different categories of pattern classification problems -UCI data, handwritten digit recognition, text non-text separation and FER. All the case studies, dataset descriptions and the feature extraction methods are explained in the following subsections.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Case Study:",
      "text": "Here, we have described the datasets we have used and the corresponding feature extraction techniques. It is to be noted that for UCI datasets, feature sets are already given, but for the rest, we have applied some standard feature extraction methods.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Case Study 1: Uci Datasets",
      "text": "Seven different benchmark datasets from UCI repository  (Dua & Graff, 2017) , have been used to evaluate the proposed FS method. The datasets are described in terms of number of features, number of instances, number of classes and application domain in Table  1 . For these datasets, we have used K-Nearest Neighbor (K = 5) classifier as an evaluator in the wrapper FS framework. For each dataset, 80% of the instances are used for training and rest 20% used for testing following the works found in the literature (M.  Mafarja et al., 2019)   (Alsaafin & Elnagar, 2017) .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Case Study 2: Handwritten Digit Recognition",
      "text": "The following case study includes the database descriptions and feature extraction methods of handwritten digit recognition. For these datasets we have used K-Nearest Neighbor (K=5) classifier as an evaluator in the proposed wrapper FS framework.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Database Description",
      "text": "For the assessment purpose, six handwritten digit databases are used. Of these, five are offline digit databases written in Bangla, Arabic, Telugu, Devanagari and Gurumukhi, and one online digit database written in Assamese. Bangla, Arabic, Telugu and Devanagari digit databases are taken from CMATERdb (\"CMATERDB,\" n.d.). The dataset size of Telugu, Arabic and Devanagari is 3000 (= 10 √ó 300) i.e. 300 samples per digit class, whereas Bangla database comprises 6000(= 10 √ó 600) samples i.e. 600 samples per digit class. On the other hand, Gurumukhi database contains a total of 2300 (10 √ó 230) samples taken from the work reported in  (Sharma & Jain, 2010) . The online Assamese digit database is taken from the work developed by  Baruah et al. (Baruah & Hazarika, 2015) . As online digits are written on digital pads, these are represented as the sequence of two-dimensional co-ordinate points along with the pen up and pen down information. In this work, all the (X, Y) coordinate points are connected consecutively using DDA line drawing algorithm to form a stroke. Considering pen up and pen down information, all the strokes are generated accordingly to obtain of the final digit image. After that each image is normalized to capture the text part only. Sample images per class is shown in Table  2 . The dataset contains only 45-digit sample per class which are further augmented by rotating each image by an angle X. Where X takes only integer value and ranges from -5 to 5 degree. This results 500 samples per class that is further used in the feature extraction.\n\nInitially, all the input images are normalized to 32 √ó 32 dimension to make the data of consistent resolution that facilitates us to implement any feature extraction technique uniformly.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": ".2 Feature Extraction",
      "text": "In this section, we have adopted Histogram of Oriented Gradients (HOG) for extracting features from the handwritten digit samples. HOG is a texture-based feature descriptor which adopts histogram of the gradients as a statistical measure. Primary idea behind the concept is that, any local shape and object can be demonstrated using the gradient intensity distribution or edge direction (S. Ghosh, Bhowmik, Ghosh, Sarkar, & Chakraborty, 2019). As HOG is invariant to geometric transformation, it is widely used in pattern recognition domain. In the computation part, first of all the entire image is divided into cells. After that, gradients are calculated according to equation 22.\n\nIn this work, one dimensional gradient is taken. A matrix M = [1 0 -1] is used to calculate the gradient in the X direction, Grad X . Subsequently, M T is adopted for gradient in Y direction, Grad Y . The final gradient direction Grad Dir is calculated using the equation 22.\n\nIn the further processing, the entire gradient direction domain (0 to 360 degree) is divided into 8 bins. For each cell, the histogram of the obtained gradient directions is calculated based on the 8 bins. Finally, those histograms from each cell are concatenated to get the final feature vector.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Case Study 3: Text / Non-Text Classification:",
      "text": "In the following two subsections, database description and the feature extraction methods of text non-text classification are explained. We have used Random Forest (no. of trees = 300) classifier for these datasets.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Database Description:",
      "text": "The   (Ojala, Pietik√§inen, & M√§enp√§√§, 2002) . LBP is a very useful texture-based feature descriptor which tends to find the edge property efficiently. It has been used for text non text classification in the recent past. The detailed formulation of LBP is described here.\n\nSuppose P cen = X cen , Y cen be the center pixel and it is surrounded by P surr number of pixels in a radius R. Each surrounding pixel can be described according to equation 23.\n\nLBP feature (LBP Feature ) is calculated using those surrounding pixel by comparing their intensity values. If the surrounding pixel is greater than equal to the center pixel, then we assign 1 else 0 is assigned. The formula is given in equation 24.\n\nLBP Feature = [LBPf(P 0 , P cen ), LBPf(P 1 , P cen ), ‚Ä¶ ‚Ä¶ . . , LBPf(P P surr -1 , P cen )] (24)\n\nThe function LBPf is a comparator which is explained in equation 25.\n\nIn our proposed work, we have considered R = 1. As a result, the LBP feature will be a binary string of 8 bit which is further converted to its equivalent decimal value. The final LBP value is formulated in equation 26.\n\nLBP val (P cen , P 1 , ‚Ä¶ ‚Ä¶ ‚Ä¶ . , P P surr -1 ) = ‚àë LBPf(P n , P cen ) √ó 2 n P surr -1 n=0 (26)\n\nRotation Invariant LBP: Ojala et al.  (Ojala et al., 2002)  have also proposed a modified version of LBP known as rotation invariant LBP (RILBP), which is invariant to image rotation. Firstly, the binary pattern in calculated using LBP. Then, the pattern is rotated bit wise with the carry and new patterns are obtained after each rotation. Finally, the binary pattern that has minimum decimal value, is taken as the feature. The process can be devised using the equation 29.\n\nRILBP (P surr -1) (P cen , P 1 , ‚Ä¶ ‚Ä¶ ‚Ä¶ . , P P surr -1 ) = min{Rotate(LBP Feature , i | 0 ‚â§ i ‚â§ P surr -1)}\n\n(27)\n\nThen we calculate the equivalent decimal value of the rotation invariant binary string using equation 26. The centre pixel is replaced with the obtained value. After replacing all the pixels, we get the RILBP image. Finally, we take the histogram of the values to get the final feature vector.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Local Ternary Pattern (Ltp):",
      "text": "In case of LBP, we only consider whether the surrounding pixel is greater or small compared to the centre pixel. But in case of LTP, it considers the greater and smaller both the cases as well as a threshold is set for the comparison (Xiaoyang  Tan & Triggs, 2010) . LTP is a three valued code containing 0, +1 and -1. LTP is formulated as follows.\n\nLTP Feature = [LTPf(P 0 , P cen , thr), LTPf(P 1 , P cen , thr), ‚Ä¶ . . LTPf(P P surr-1 , P cen , thr)] (28)\n\nWhere, LTPf is the function explained in equation 29, P i is neighboring pixels, P cen is the center pixel and P surr is the number of surrounding pixels.\n\nAfter obtaining the three valued code, it is converted to two binary strings containing only 1s and 0s. One binary string is obtained by replacing all the non 1 (i.e. -1 and 0) with 0 and keeping all the 1 as it is. We get hold of the second binary string by replacing all the non -1s (i.e. 1 and 0) with 0 and converting all the -1 to 1. An example of this conversion is shown.\n\nLet the three valued code be 1100(-1)(-1)00. The LTP string can be decomposed into two binary strings -11000000 and 00001100.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Rotation Invariant Ltp (Riltp):",
      "text": "RILTP is same as RILBP as explained in equation 31. After obtaining the two binary strings, rotation invariant property is applied on both strings to obtain RILTP. Similarly, the center pixel is replaced with the decimal equivalent of the string. Consequently, we shall get two channels of LTP image due to two strings for each center pixel. At the end, histogram of both the channels are taken as the final feature vector.\n\nUniform LTP: A transition in a binary string is defined as the change of 0 to 1 or change of 1 to 0.\n\nThe strings that contain less than or equal to two transitions are known as uniform strings and others are known as non-uniform strings. Main aim of using uniform pattern is to remove the redundant features and capture the information properly. Uniform property is incorporated with the two strings produced from LTP. In this case, we only consider the uniform strings and others are ignored. Finally, histogram of the uniform strings is considered as feature vector.\n\nLocal Tetra Pattern (LTRP): LTRP was first introduced by Murala et al.  (Murala, Maheshwari, & Balasubramanian, 2012)  in the domain of content based image retrieval. As LTRP is capable of capturing edge information prominently, it can be used in text, non-text separation purpose. The methodology of LTRP is described as follow.\n\nLet us consider S be an image segment of dimension 5 √ó 5. To start with, the first order derivative along vertical (i.e. 90¬∞) and horizontal (i.e. 0¬∞) directions are calculated and denoted by S Œ± 1 (G c )|Œ± = 0¬∞, 90¬∞ where\n\nWhere, G 0‚óã denotes the gray level intensity value of the pixel along 0¬∞ and G 90‚óã indicates the intensity value along 90¬∞. G c stands for the center pixel. Horizontal and vertical derivatives can be positive or negative. So, we shall get four different combinations due to their sign. The direction for the center pixel is given according to equation 32.\n\nSimilarly, we can get the direction for each of the neighboring pixels. After considering the direction of all the neighboring pixels, we get the second order LTrP 2 (G c ). This is formulated as equation 33.\n\nWhere, LTRPf is defined in equation 34.\n\nAs there are total 8 neighboring pixels, it will be an 8-bit pattern. Now, the string is four values code.\n\nFor each direction three binary patterns are generated. As there are total four possible directions for the center pixel, total 12 (= 3 √ó 4) binary patterns will be generated. An example is shown below where the center pixel direction is 4 (Dir 1 (G c ) = 4 according to equation 32).\n\nThese strings are generated using the derivative direction. The final string is generated from the gradient magnitude. The gradient magnitude is defined in equation 37.\n\nThe gradient magnitude for all the neighbors and center pixels is calculated. Now, the gradient magnitude is compared among the center and neighbor pixels. The generation of the 13 th binary string is formulated in equation 38.\n\nWhere,\n\nWe get 13 channels of LTRP image due to the 13 different strings for each center pixel. In case of LTP we got 2 channels, whereas, in LBP we got one channel.\n\nUniform ùêãùêìùêëùêè (ùêîùêãùêìùêëùêè): The uniform property is described in the previous section. In ULTRP, we consider only the uniform strings among those 13 obtained strings. Finally, histogram of those uniform strings is taken to get the final feature vector.\n\nRotation Invariant ùêãùêìùêëùêè (RILTRP): Previously, rotation invariant property is explained. After getting those 13 strings, rotation invariant property is applied on those strings. Finally, the histogram is considered for the final feature vector.\n\nNumber of features obtained by each of the above-mentioned methods is shown in Table  4 .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Case Study 4: Facial Emotion Recognition",
      "text": "This section contains the detailed explanation of our fourth case study which is FER. First subsection comprises database description followed by the feature extraction method in the later subsection. For these datasets, we have used Multi-Layer Perceptron classifier as an evaluator in the wrapper FS framework",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Database Description:",
      "text": "In the present work, two FER datasets are considered, namely -JAFEE and RaFD. The dataset description along with some pre-processing steps are described in the following sections. Pre-processing: As facial images are captured in different environmental conditions, hence images may be very noisy, therefore, it hampers the feature extraction process. To overcome this, a proper pre-processing technique is very essential. Viola Jones Haar  (Viola & Jones, 2004)  has been adopted\n\nto capture the important region from the entire image. Point of interest or the important regions mainly contains the area containing facial expressions such as lips, eyebrows, nose, eyes etc. Viola\n\nJones algorithm returns the bounding box coordinates containing the region of interest. To make the method more robust and comparable with real-world scenario, the technique is appraised with various image dimensions. In our work, three image dimensions 32 √ó 32, 48 √ó 48 and 64 √ó 64 are considered for the evaluation purpose. After pre-processing, the images are resized to the corresponding resolutions.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Feature Extraction",
      "text": "In the present work, Gabor filter-based feature descriptor which is a frequency-based feature descriptor is used. Gabor filter is a linear filter which is mainly used for texture study. Gabor filter predominantly analyzes if there is any explicit frequency content in the image or not in a precise direction within a localized region around the point or region of interest. Gabor filter is invariant to scale, translation and rotation. Accordingly, it is also robust towards any kind of photometric disorders, mainly occur as illumination changes and image noise (Chengjun  Liu & Wechsler, 2002) .\n\nIn the spatial domain, two dimensional Gabor filter involves the sinusoidal plane modulated Gaussian kernel function  (Haghighat, Zonouz, & Abdel-Mottaleb, 2015) . Equation for the kernel function in the spatial domain is shown in equations  (40 -42) . The Gabor features are directly calculated from a gray scale image using these formulas.\n\nGabor(x, y) = f 2 œÄŒ≥Œ∑ exp (-\n\nx ‚Ä≤ = x cos Œ∏ + y sin Œ∏ (41)\n\nThe standard deviation of the Gaussian envelope is expressed as œÉ. Œ≥ is the spatial aspect ratio and the ellipticity of the support of the Gabor function. is formulated according to equation 43  (Ou, Bai, Pei, Ma, & Liu, 2010) .\n\nFinally, the Gabor features are down sampled by a factor of 8. The feature vector size varies with the image dimension. In the present work, image dimensions which are taken into account are -32 √ó 32, 48 √ó 48 and 64 √ó 64. Table  5  describes the feature dimension corresponds to the previously mentioned image size.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Results",
      "text": "This section describes in detail the results obtained by the proposed methods. First of all, it is required to fix the population size and number of iterations. The number of iterations can be selected",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Comparison",
      "text": "Since we have applied our methods to various domains, we have compared them with different methods present in the literature align to the corresponding domains. This subsection presents a comparison between our proposed FS methods and other state-of-the-art meta-heuristic FS methods.\n\nWe have used different classifiers in different cases to show the classifier-independent nature of the proposed methods. For UCI and Handwritten digit datasets, we have used KNN, for Text/Non-text dataset, we have used Random Forest and for FER datasets, we have used MLP classifiers.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Uci Datasets",
      "text": "We have compared the results of our methods obtained over UCI datasets with 6 recent FS methods. For comparison, we have used BGWOPSO (Al-Tashi, Kadir, Rais,  Mirjalili, & Alhussian, 2019) ,  BPSO (Al-Tashi et al., 2019) , BGA  (Al-Tashi et al., 2019) , BGOA (M.  Mafarja et al., 2019) , BGSA  (Taradeh et al., 2019) , HGSA  (Taradeh et al., 2019) . The results of BGWOPSO, BPSO and BGA are obtained from  (Al-Tashi et al., 2019)  since they have used the same datasets. The results of BGOA, BGSA and HGSA are obtained from (M.  Mafarja et al., 2019) ,  (Taradeh et al., 2019)  and  (Taradeh et al., 2019)  respectively. In all the cases, used classifier is KNN (K=5). The accuracies achieved and number of features selected using different methods are shown in Figure  15 . We can easily see that both ASOs-SA and ASOv-SA outperform all the other methods considered here for comparison in terms of classification accuracy. Although, in case of number of selected features, the result is quite different. For most cases, the accuracies achieved by ASOs-SA and ASOv-SA are quite higher than ASOs and ASOv respectively. This justifies the usage of SA as local search. Number of selected features is not always lowest for ASOs-SA or ASOv-SA. In case of waveform and tic-tac-toe datasets, the number of selected features is lowest with BGWOPSO method.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Handwritten Digit Recognition Datasets",
      "text": "For handwritten digit recognition datasets, we have compared our methods with BGSA (Esmat  Rashedi et al., 2010a) , BPSO  (Mirjalili & Lewis, 2013) , GA  (Yang & Honavar, 1998) , WFACOFS (M.  Ghosh, Guha, Sarkar, & Abraham, 2019) . For the other methods, number of agents (number of genes in GA, number of particles in PSO etc.) is taken as 20, since number of atoms is taken as 20 in the proposed methods. Maximum number of iterations has been set to 30 for all the methods. In all cases, used classifier is KNN (K=5). The accuracies achieved and number of features selected using different methods are shown in Figure  16 . Again, ASOs-SA or ASOv-SA outperform all other methods in terms of classification accuracy. Another thing to be noticed is that, ASOs-SA and ASOv-SA perform quite better than ASOs and ASOv respectively. So, we can say that using SA for local search is quite effective. Now, considering the number of selected features, the proposed methods perform significantly better than others. Outperforming all the methods, ASO based methods have selected least number of features. ASOv selects least number of features 3 times, ASOs 2 times and ASOv-SA 1 time.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Text/Non-Text Classification Dataset",
      "text": "For Text/Non-text classification dataset, we have compared our methods with BGSA (Esmat  Rashedi et al., 2010a) , BPSO  (Mirjalili & Lewis, 2013) , GA  (Yang & Honavar, 1998) , WFACOFS (M.  Ghosh, Guha, et al., 2019) . For the other methods, number of agents (number of genes in GA, number of particles in PSO etc.) is taken as 20, since number of atoms is taken as 20 in the proposed methods. Number of maximum iterations has been set to 30 for all the methods. In all cases, we have used Random Forest Classifier (number of trees = 300). The accuracies achieved and number of features selected using different methods are shown in Figure  17 . In this case too, either ASOs-SA or ASOv-SA clearly beats other methods by a significant margin, although the results of ASOs and ASOs-SA are quite closer. The same can be stated for ASOv and ASOv-SA. In case of number of features, except the RILTRP, ASO based methods select lowest number of features. ASOv-SA selects the least number of features 3 times.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Fer Datasets",
      "text": "For FER datasets, we have compared our method with GA, ME-BPSO  (Wei et al., 2017) , WOA-CM (M.  Mafarja & Mirjalili, 2018)  and LHCMA (M.  Ghosh, Kundu, Ghosh, & Sarkar, 2019) . Amongst those, ME-BPSO, WOA-CM and LHCMA are recently published methods for FS. The accuracies achieved and number of features selected using different methods are shown in Figure  18 . Considering classification accuracy, in all six cases, ASO based methods have achieved higher accuracy than others. ASOs, ASOs-SA and ASOv-SA have achieved the highest accuracy in multiple cases. In terms of number of features selected, ASOv has performed consistently well. It has selected lowest number of features in 5 cases whereas in another case ASOs has selected lowest number of features.\n\nConsidering all the 25 accuracy results, it can be observed that ASO based methods have achieved highest accuracy in all 25 cases. ASOs, ASOs-SA, ASOv and ASOv-SA have achieved highest accuracies respectively in 6, 15, 3 and 16 cases. Now, considering all the 25 results in terms of number of features selected, ASO based methods have selected lowest number of features in 23 cases i.e., 92% of the cases. ASOs, ASOs-SA, ASOv and ASOv-SA have outperformed others respectively in 5, 2, 9 and 5 cases.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Conclusion",
      "text": "FS is an important pre-processing technique in enhancing classifiers' ability in classification process. In this paper, we have proposed the FS variant of pre-existing ASO algorithm. While mapping the continuous values in ASO to the binary space of FS, we have used two different transfer functions: one V-shaped (ASOv) and one S-shaped (ASOs). In addition, we have also included SA to enhance exploitation in ASO and named then ASOv-SA and ASOs-SA. To the best of our knowledge, this is the first time ASO has been applied for FS problem.\n\nThe performances of the proposed approaches are assessed by applying them on 25 datasets, which are from 4 different categories. We have applied the proposed methods on UCI, Handwritten digit recognition, Text/Non-text separation and FER datasets. These datasets are from completely belong to different domains, which give us clear idea about robustness of the proposed FS techniques. Two criteria are considered, classification accuracy and number of features for measuring the strength of the proposed methods. By using FS, we are able to achieve much higher accuracy with less number of features, irrespective of the domain of the datasets.\n\nTo compare the proposed methods with other FS techniques, we have considered different recently published meta-heuristic FS methods. The proposed methods have beat all the methods in comparison in terms of classification accuracy and beat most of the methods in terms of number of selected features. ASOs-SA and ASOv-SA have achieved higher accuracy than ASOs and ASOv respectively, in multiple cases. This justifies the use of SA. Another thing to be noticed, ASOv outperforms ASOs when it comes to number of selected features.\n\nFor future studies, we can apply different local search technique with ASO or use different classifiers. Along with that we can try other domains with large feature dimension (> 10000). It would also be really interesting if we are able to hybridize ASO with another population based metaheuristic algorithms.",
      "page_start": 32,
      "page_end": 33
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: explains the nature of this LJ",
      "page": 5
    },
    {
      "caption": "Figure 1: LJ potential curve with attraction and repulsion region. In the attractive zone, the attraction force gradually",
      "page": 5
    },
    {
      "caption": "Figure 2: S-shaped transfer function",
      "page": 9
    },
    {
      "caption": "Figure 3: V-shaped transfer function",
      "page": 10
    },
    {
      "caption": "Figure 4: The entire flowchart of ASO-SA. The Basic ASO and Simulated Annealing portions are marked separately",
      "page": 11
    },
    {
      "caption": "Figure 4: The basic ASO and SA are",
      "page": 11
    },
    {
      "caption": "Figure 5: Images taken from JAFEE dataset showing various emotions, (a) anger, (b) disgust, (c) happy, (d) fear, (e) sad,",
      "page": 20
    },
    {
      "caption": "Figure 6: contains some sample images taken from",
      "page": 20
    },
    {
      "caption": "Figure 6: Sample images taken from RaFD dataset displaying various emotions (a) anger, (b) disgust, (c) happy, (d)",
      "page": 21
    },
    {
      "caption": "Figure 7: Convergence graph of the proposed FC methods when applied on UCI datasets",
      "page": 24
    },
    {
      "caption": "Figure 8: Convergence graph of the proposed FC methods when applied on handwritten digit recognition datasets",
      "page": 25
    },
    {
      "caption": "Figure 9: Convergence curve of the proposed approaches for text/non-text separation datasets",
      "page": 26
    },
    {
      "caption": "Figure 10: Convergence graph of the proposed approaches for fer datasets",
      "page": 27
    },
    {
      "caption": "Figure 4: Plot of accuracy vs population size for fixed number of iterations (30) over UCI datasets.",
      "page": 28
    },
    {
      "caption": "Figure 52: Plot of accuracy vs population size for fixed number of iterations (30) over HANDWRITTEN DIGIT datasets.",
      "page": 28
    },
    {
      "caption": "Figure 63: Plot of accuracy vs population size for fixed number of iterations (30) over TEXT/NON-TEXT datasets.",
      "page": 29
    },
    {
      "caption": "Figure 74: Plot of accuracy vs population size for fixed number of iterations (30) over FER datasets.",
      "page": 29
    },
    {
      "caption": "Figure 15: Figure 15: Comparison of our FS methods with some state-of-the-art FS methods in terms of classification accuracy",
      "page": 30
    },
    {
      "caption": "Figure 8: Comparison of our FS methods with some state-of-the-art FS methods in terms of classification accuracy",
      "page": 31
    },
    {
      "caption": "Figure 17: Figure 9: comparison of our FS methods with some state-of-the-art FS methods in terms of classification accuracy and",
      "page": 31
    },
    {
      "caption": "Figure 10: Comparison of our FS methods with some state-of-the-art FS methods in terms of classification accuracy",
      "page": 32
    }
  ],
  "tables": [
    {
      "caption": "Table 1: For these datasets, we",
      "data": [
        {
          "Dataset": "IonosphereEW",
          "No. of \nInstances": "351",
          "No. of \nFeatures": "34",
          "No. of \nClasses": "2",
          "Application \nDomain": "Electromagnetic"
        },
        {
          "Dataset": "Lymphography",
          "No. of \nInstances": "148",
          "No. of \nFeatures": "18",
          "No. of \nClasses": "2",
          "Application \nDomain": "Biology"
        },
        {
          "Dataset": "SonarEW",
          "No. of \nInstances": "208",
          "No. of \nFeatures": "60",
          "No. of \nClasses": "2",
          "Application \nDomain": "Biology"
        },
        {
          "Dataset": "SpectEW",
          "No. of \nInstances": "267",
          "No. of \nFeatures": "22",
          "No. of \nClasses": "2",
          "Application \nDomain": "Biology"
        },
        {
          "Dataset": "waveformEW",
          "No. of \nInstances": "5000",
          "No. of \nFeatures": "40",
          "No. of \nClasses": "3",
          "Application \nDomain": "Physics"
        },
        {
          "Dataset": "WineEW",
          "No. of \nInstances": "178",
          "No. of \nFeatures": "13",
          "No. of \nClasses": "3",
          "Application \nDomain": "Chemistry"
        },
        {
          "Dataset": "Zoo",
          "No. of \nInstances": "101",
          "No. of \nFeatures": "16",
          "No. of \nClasses": "6",
          "Application \nDomain": "Artificial"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 2: The dataset",
      "data": [
        {
          "Class \nLabel": "0",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "1",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "2",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "3",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "4",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "5",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "6",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "7",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "8",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        },
        {
          "Class \nLabel": "9",
          "Bangla": "",
          "Arabic": "",
          "Telugu": "",
          "Devanagari": "",
          "Gurumukhi": "",
          "Assamese": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 4: TABLE 2: NUMBER OF FEATURES OBTAINED FROM EACH IMAGE USING EACH OF",
      "data": [
        {
          "Method": "RILBP",
          "Number of features": "36"
        },
        {
          "Method": "ULTP",
          "Number of features": "2 √ó 59  =  118"
        },
        {
          "Method": "ULTRP",
          "Number of features": "13 √ó 59   =  767"
        },
        {
          "Method": "RILTP",
          "Number of features": "2 √ó 36  =  72"
        },
        {
          "Method": "RILTRP",
          "Number of features": "13 √ó 36  =  468"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Image Dimensions": "32 √ó 32",
          "Feature Dimension": "40960 (= 40 √ó 32 √ó 32)",
          "Feature \ndimension \nafter \ndown \nsampling": "40960\n640(=\n) \n8 √ó 8)"
        },
        {
          "Image Dimensions": "48 √ó 48",
          "Feature Dimension": "92160 (= 40 √ó 48 √ó 48)",
          "Feature \ndimension \nafter \ndown \nsampling": "92160\n1440(=\n) \n8 √ó 8"
        },
        {
          "Image Dimensions": "64 √ó 64",
          "Feature Dimension": "163840 (= 40 √ó 64 √ó 64)",
          "Feature \ndimension \nafter \ndown \nsampling": "163840\n2560(=\n ) \n8 √ó 8"
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A new fusion of grey wolf optimizer algorithm with a two-phase mutation for feature selection",
      "authors": [
        "M Abdel-Basset",
        "D El-Shahat",
        "I El-Henawy",
        "V De Albuquerque",
        "S Mirjalili"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "2",
      "title": "Binary Optimization Using Hybrid Grey Wolf Optimization for Feature Selection",
      "authors": [
        "Q Al-Tashi",
        "S Kadir",
        "H Rais",
        "S Mirjalili",
        "H Alhussian"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Gene selection in cancer classification using PSO/SVM and GA/SVM hybrid algorithms",
      "authors": [
        "E Alba",
        "J Garcia-Nieto",
        "L Jourdan",
        "E.-G Talbi"
      ],
      "year": "2007",
      "venue": "Evolutionary Computation"
    },
    {
      "citation_id": "4",
      "title": "Studies in Molecular Dynamics. I. General Method",
      "authors": [
        "B Alder",
        "T Wainwright"
      ],
      "year": "1959",
      "venue": "The Journal of Chemical Physics",
      "doi": "10.1063/1.1730376"
    },
    {
      "citation_id": "5",
      "title": "A minimal subset of features using feature selection for handwritten digit recognition",
      "authors": [
        "A Alsaafin",
        "A Elnagar"
      ],
      "year": "2017",
      "venue": "Journal of Intelligent Learning Systems and Applications"
    },
    {
      "citation_id": "6",
      "title": "A Feature Selection based on perturbation theory",
      "authors": [
        "J Anaraki",
        "H Usefi"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "7",
      "title": "Intelligent Hybrid Approach for Feature Selection",
      "authors": [
        "A Anter",
        "A Azar",
        "K Fouad"
      ],
      "year": "2019",
      "venue": "International Conference on Advanced Machine Learning Technologies and Applications"
    },
    {
      "citation_id": "8",
      "title": "ICDAR2015 competition on recognition of documents with complex layouts -RDCL2015",
      "authors": [
        "A Antonacopoulos",
        "C Clausner",
        "C Papadopoulos",
        "S Pletschacher"
      ],
      "year": "2015",
      "venue": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)",
      "doi": "10.1109/ICDAR.2015.7333941"
    },
    {
      "citation_id": "9",
      "title": "A Hybrid Memetic Algorithm ( Genetic Algorithm and Great Deluge Local Search ) With Back-Propagation Classifier for Fish Recognition",
      "authors": [
        "U Badawi",
        "M Khalil",
        "S Alsmadi"
      ],
      "year": "2013",
      "venue": "A Hybrid Memetic Algorithm ( Genetic Algorithm and Great Deluge Local Search ) With Back-Propagation Classifier for Fish Recognition"
    },
    {
      "citation_id": "10",
      "title": "A dataset of online handwritten Assamese characters",
      "authors": [
        "U Baruah",
        "S Hazarika"
      ],
      "year": "2015",
      "venue": "Journal of Information Processing Systems",
      "doi": "10.3745/JIPS.02.0008"
    },
    {
      "citation_id": "11",
      "title": "A Novel Hybrid ACO-GA Algorithm for Text Feature Selection",
      "authors": [
        "M Basiri",
        "S Nemati"
      ],
      "year": "2009",
      "venue": "A Novel Hybrid ACO-GA Algorithm for Text Feature Selection"
    },
    {
      "citation_id": "12",
      "title": "The wind driven optimization technique and its application in electromagnetics",
      "authors": [
        "Z Bayraktar",
        "M Komurcu",
        "J Bossard",
        "D Werner"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Antennas and Propagation"
    },
    {
      "citation_id": "13",
      "title": "Hybrid particle swarm optimization with spiral-shaped mechanism for feature selection",
      "authors": [
        "K Chen",
        "F.-Y Zhou",
        "X.-F Yuan"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "14",
      "title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition",
      "authors": [
        "Chengjun Liu",
        "H Wechsler"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2002.999679"
    },
    {
      "citation_id": "15",
      "title": "ICDAR2017 Competition on Recognition of Documents with Complex Layouts -RDCL2017",
      "authors": [
        "C Clausner",
        "A Antonacopoulos",
        "S Pletschacher"
      ],
      "year": "2017",
      "venue": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)",
      "doi": "10.1109/ICDAR.2017.229"
    },
    {
      "citation_id": "16",
      "title": "",
      "authors": [
        "Cmaterdb"
      ],
      "venue": ""
    },
    {
      "citation_id": "17",
      "title": "Feature selection for classification",
      "authors": [
        "M Dash",
        "H Liu"
      ],
      "year": "1997",
      "venue": "Intelligent Data Analysis",
      "doi": "10.1016/S1088-467X(97)00008-5"
    },
    {
      "citation_id": "18",
      "title": "A GA-based feature selection approach with an application to handwritten character recognition",
      "authors": [
        "C De Stefano",
        "F Fontanella",
        "C Marrocco",
        "A Di Freca"
      ],
      "year": "2014",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "19",
      "title": "Ant colony optimization: a new meta-heuristic",
      "authors": [
        "M Dorigo",
        "G Di Caro"
      ],
      "venue": "Proceedings of the 1999 Congress on Evolutionary Computation-CEC99",
      "doi": "10.1109/CEC.1999.782657"
    },
    {
      "citation_id": "20",
      "title": "{UCI} Machine Learning Repository",
      "authors": [
        "D Dua",
        "C Graff"
      ],
      "year": "2017",
      "venue": "{UCI} Machine Learning Repository"
    },
    {
      "citation_id": "21",
      "title": "Feature Subset Selection Approach by Gray-Wolf Optimization",
      "authors": [
        "E Emary",
        "H Zawbaa",
        "C Grosan",
        "A Hassenian"
      ],
      "year": "2015",
      "venue": "Feature Subset Selection Approach by Gray-Wolf Optimization",
      "doi": "10.1007/978-3-319-13572-4_1"
    },
    {
      "citation_id": "22",
      "title": "Binary grey wolf optimization approaches for feature selection",
      "authors": [
        "E Emary",
        "H Zawbaa",
        "A Hassanien"
      ],
      "year": "2016",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2015.06.083"
    },
    {
      "citation_id": "23",
      "title": "Enriched ant colony optimization and its application in feature selection",
      "authors": [
        "R Forsati",
        "A Moayedikia",
        "R Jensen",
        "M Shamsfard",
        "M Meybodi"
      ],
      "year": "2014",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2014.03.053"
    },
    {
      "citation_id": "24",
      "title": "Feature selection using Forest Optimization Algorithm",
      "authors": [
        "M Ghaemi",
        "M.-R Feizi-Derakhshi"
      ],
      "year": "2016",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2016.05.012"
    },
    {
      "citation_id": "25",
      "title": "Genetic algorithm based cancerous gene identification from microarray data using ensemble of filter methods",
      "authors": [
        "M Ghosh",
        "S Adhikary",
        "K Ghosh",
        "A Sardar",
        "S Begum",
        "R Sarkar"
      ],
      "year": "2019",
      "venue": "Medical & Biological Engineering & Computing"
    },
    {
      "citation_id": "26",
      "title": "A wrapper-filter feature selection technique based on ant colony optimization",
      "authors": [
        "M Ghosh",
        "R Guha",
        "R Sarkar",
        "A Abraham"
      ],
      "year": "2019",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-019-04171-3"
    },
    {
      "citation_id": "27",
      "title": "Feature selection for facial emotion recognition using late hill-climbing based memetic algorithm",
      "authors": [
        "M Ghosh",
        "T Kundu",
        "D Ghosh",
        "Sarkar"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-019-07811-x"
    },
    {
      "citation_id": "28",
      "title": "A filter ensemble feature selection method for handwritten numeral recognition",
      "authors": [
        "S Ghosh",
        "S Bhowmik",
        "K Ghosh",
        "R Sarkar",
        "S Chakraborty"
      ],
      "year": "2019",
      "venue": "A filter ensemble feature selection method for handwritten numeral recognition"
    },
    {
      "citation_id": "29",
      "title": "Classical Mechanics, 3rd ed",
      "authors": [
        "H Goldstein",
        "C Poole",
        "J Safko",
        "S Addison"
      ],
      "year": "2002",
      "venue": "American Journal of Physics",
      "doi": "10.1119/1.1484149"
    },
    {
      "citation_id": "30",
      "title": "CloudID: Trustworthy cloud-based and cross-enterprise biometric identification",
      "authors": [
        "M Haghighat",
        "S Zonouz",
        "M Abdel-Mottaleb"
      ],
      "year": "2015",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2015.06.025"
    },
    {
      "citation_id": "31",
      "title": "Genetic Algorithms",
      "authors": [
        "J Holland"
      ],
      "year": "1992",
      "venue": "Scientific American"
    },
    {
      "citation_id": "32",
      "title": "A distributed PSO -SVM hybrid system with feature selection and parameter optimization",
      "authors": [
        "C Huang",
        "J Dun"
      ],
      "year": "2008",
      "venue": "A distributed PSO -SVM hybrid system with feature selection and parameter optimization",
      "doi": "10.1016/j.asoc.2007.10.007"
    },
    {
      "citation_id": "33",
      "title": "ACO-based hybrid classification system with feature subset selection and model parameters optimization",
      "authors": [
        "C Huang"
      ],
      "year": "2009",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2009.07.014"
    },
    {
      "citation_id": "34",
      "title": "Two applications of genetic algorithms to component design",
      "authors": [
        "P Husbands",
        "G Jermy",
        "M Mcilhagga",
        "R Ives"
      ],
      "year": "1996",
      "venue": "AISB workshop on evolutionary computing"
    },
    {
      "citation_id": "35",
      "title": "S-shaped binary whale optimization algorithm for feature selection",
      "authors": [
        "A Hussien",
        "A Hassanien",
        "E Houssein",
        "S Bhattacharyya",
        "M Amin"
      ],
      "year": "2019",
      "venue": "Recent trends in signal and image processing"
    },
    {
      "citation_id": "36",
      "title": "Hybrid genetic algorithms for feature selection",
      "authors": [
        "Il-Seok Oh",
        "Jin-Seon Lee",
        "Byung-Ro Moon"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2004.105"
    },
    {
      "citation_id": "37",
      "title": "Finding rough set reducts with ant colony optimization",
      "authors": [
        "R Jensen",
        "Q Shen"
      ],
      "year": "2003",
      "venue": "Proceedings of the 2003 UK workshop on computational intelligence"
    },
    {
      "citation_id": "38",
      "title": "A new hybrid ant colony optimization algorithm for feature selection",
      "authors": [
        "M Kabir",
        "M Shahjahan",
        "K Murase"
      ],
      "year": "2012",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2011.09.073"
    },
    {
      "citation_id": "39",
      "title": "An advanced ACO algorithm for feature subset selection",
      "authors": [
        "S Kashef",
        "H Nezamabadi-Pour"
      ],
      "year": "2015",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2014.06.067"
    },
    {
      "citation_id": "40",
      "title": "A discrete binary version of the particle swarm algorithm",
      "authors": [
        "J Kennedy",
        "R Eberhart"
      ],
      "year": "1997",
      "venue": "1997 IEEE International conference on systems, man, and cybernetics. Computational cybernetics and simulation"
    },
    {
      "citation_id": "41",
      "title": "Optimization by simulated annealing",
      "authors": [
        "S Kirkpatrick",
        "C Gelatt",
        "M Vecchi"
      ],
      "year": "1983",
      "venue": "Science"
    },
    {
      "citation_id": "42",
      "title": "Presentation and validation of the Radboud Faces Database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition & Emotion",
      "doi": "10.1080/02699930903485076"
    },
    {
      "citation_id": "43",
      "title": "Genetic algorithms in feature selection",
      "authors": [
        "R Leardi"
      ],
      "year": "1996",
      "venue": "Genetic algorithms in molecular modeling"
    },
    {
      "citation_id": "44",
      "title": "Modified binary particle swarm optimization",
      "authors": [
        "S Lee",
        "S Soak",
        "S Oh",
        "W Pedrycz",
        "M Jeon"
      ],
      "year": "2008",
      "venue": "Progress in Natural Science",
      "doi": "10.1016/j.pnsc.2008.03.018"
    },
    {
      "citation_id": "45",
      "title": "The applications of genetic algorithms in stock market data mining optimisation",
      "authors": [
        "L Lin",
        "L Cao",
        "J Wang",
        "C Zhang"
      ],
      "year": "2004",
      "venue": "The applications of genetic algorithms in stock market data mining optimisation"
    },
    {
      "citation_id": "46",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition",
      "doi": "10.1109/AFGR.1998.670949"
    },
    {
      "citation_id": "47",
      "title": "Binary grasshopper optimisation algorithm approaches for feature selection problems",
      "authors": [
        "M Mafarja",
        "I Aljarah",
        "H Faris",
        "A Hammouri",
        "A Al-Zoubi",
        "S Mirjalili"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2018.09.015"
    },
    {
      "citation_id": "48",
      "title": "Evolutionary Population Dynamics and Grasshopper Optimization approaches for feature selection problems",
      "authors": [
        "M Mafarja",
        "I Aljarah",
        "A Heidari",
        "A Hammouri",
        "H Faris",
        "A Al-Zoubi",
        "S Mirjalili"
      ],
      "year": "2018",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2017.12.037"
    },
    {
      "citation_id": "49",
      "title": "Dragonfly Algorithm: Theory, Literature Review, and Application in Feature Selection",
      "authors": [
        "M Mafarja",
        "A Heidari",
        "H Faris",
        "S Mirjalili",
        "I Aljarah"
      ],
      "year": "2020",
      "venue": "Dragonfly Algorithm: Theory, Literature Review, and Application in Feature Selection",
      "doi": "10.1007/978-3-030-12127-3_4"
    },
    {
      "citation_id": "50",
      "title": "Binary dragonfly algorithm for feature selection",
      "authors": [
        "M Mafarja",
        "D Eleyan",
        "I Jaber",
        "A Hammouri",
        "S Mirjalili"
      ],
      "year": "2017",
      "venue": "2017 International Conference on New Trends in Computing Sciences (ICTCS)"
    },
    {
      "citation_id": "51",
      "title": "Hybrid Whale Optimization Algorithm with simulated annealing for feature selection",
      "authors": [
        "M Mafarja",
        "S Mirjalili"
      ],
      "year": "2017",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2017.04.053"
    },
    {
      "citation_id": "52",
      "title": "Whale optimization approaches for wrapper feature selection",
      "authors": [
        "M Mafarja",
        "S Mirjalili"
      ],
      "year": "2018",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "53",
      "title": "A GA based hierarchical feature selection approach for handwritten word recognition",
      "authors": [
        "S Malakar",
        "M Ghosh",
        "S Bhowmik",
        "R Sarkar",
        "M Nasipuri"
      ],
      "year": "2019",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "54",
      "title": "S-shaped versus V-shaped transfer functions for binary Particle Swarm Optimization",
      "authors": [
        "S Mirjalili",
        "A Lewis"
      ],
      "year": "2013",
      "venue": "Swarm and Evolutionary Computation",
      "doi": "10.1016/j.swevo.2012.09.002"
    },
    {
      "citation_id": "55",
      "title": "Local Tetra Patterns: A New Feature Descriptor for Content-Based Image Retrieval",
      "authors": [
        "S Murala",
        "R Maheshwari",
        "R Balasubramanian"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2012.2188809"
    },
    {
      "citation_id": "56",
      "title": "Expert Systems with Applications A novel ACO -GA hybrid algorithm for feature selection in protein function prediction",
      "authors": [
        "S Nemati",
        "M Ehsan",
        "N Ghasem-Aghaee",
        "M Hosseinzadeh"
      ],
      "year": "2009",
      "venue": "Expert Systems With Applications",
      "doi": "10.1016/j.eswa.2009.04.023"
    },
    {
      "citation_id": "57",
      "title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns",
      "authors": [
        "T Ojala",
        "M Pietik√§inen",
        "T M√§enp√§√§"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "58",
      "title": "Automatic Facial Expression Recognition Using Gabor Filter and Expression Analysis",
      "authors": [
        "J Ou",
        "X.-B Bai",
        "Y Pei",
        "L Ma",
        "W Liu"
      ],
      "year": "2010",
      "venue": "2010 Second International Conference on Computer Modeling and Simulation",
      "doi": "10.1109/ICCMS.2010.45"
    },
    {
      "citation_id": "59",
      "title": "Feature selection through gravitational search algorithm",
      "authors": [
        "J Papa",
        "A Pagnin",
        "S Schellini",
        "A Spadotto",
        "R Guido",
        "M Ponti",
        "A Falc√£o"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Bacterial foraging optimization",
      "authors": [
        "K Passino"
      ],
      "year": "2010",
      "venue": "International Journal of Swarm Intelligence Research (IJSIR)"
    },
    {
      "citation_id": "61",
      "title": "SVM Classifier Based Feature Selection Using GA , ACO and PSO for siRNA Design",
      "authors": [
        "Y Prasad",
        "K Biswas",
        "C Jain"
      ],
      "year": "2010",
      "venue": "SVM Classifier Based Feature Selection Using GA , ACO and PSO for siRNA Design"
    },
    {
      "citation_id": "62",
      "title": "Feature subset selection using improved binary gravitational search algorithm",
      "authors": [
        "E Rashedi",
        "H Nezamabadi-Pour"
      ],
      "year": "2014",
      "venue": "Journal of Intelligent and Fuzzy Systems",
      "doi": "10.3233/IFS-130807"
    },
    {
      "citation_id": "63",
      "title": "Improving the precision of CBIR systems by feature selection using binary gravitational search algorithm",
      "authors": [
        "Esmat Rashedi",
        "H Nezamabadi-Pour"
      ],
      "year": "2012",
      "venue": "The 16th CSI International Symposium on Artificial Intelligence and Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "BGSA: binary gravitational search algorithm",
      "authors": [
        "Esmat Rashedi",
        "H Nezamabadi-Pour",
        "S Saryazdi"
      ],
      "year": "2010",
      "venue": "Natural Computing"
    },
    {
      "citation_id": "65",
      "title": "BGSA: Binary gravitational search algorithm",
      "authors": [
        "Esmat Rashedi",
        "H Nezamabadi-Pour",
        "S Saryazdi"
      ],
      "year": "2010",
      "venue": "Natural Computing",
      "doi": "10.1007/s11047-009-9175-3"
    },
    {
      "citation_id": "66",
      "title": "Numerical integration of the cartesian equations of motion of a system with constraints: molecular dynamics of n-alkanes",
      "authors": [
        "J.-P Ryckaert",
        "G Ciccotti",
        "H Berendsen"
      ],
      "year": "1977",
      "venue": "Journal of Computational Physics",
      "doi": "10.1016/0021-9991(77)90098-5"
    },
    {
      "citation_id": "67",
      "title": "Grasshopper Optimisation Algorithm: Theory and application",
      "authors": [
        "S Saremi",
        "S Mirjalili",
        "A Lewis"
      ],
      "year": "2017",
      "venue": "Advances in Engineering Software",
      "doi": "10.1016/j.advengsoft.2017.01.004"
    },
    {
      "citation_id": "68",
      "title": "Feature selection approach based on whale optimization algorithm",
      "authors": [
        "M Sharawi",
        "H Zawbaa",
        "E Emary"
      ],
      "year": "2017",
      "venue": "2017 Ninth International Conference on Advanced Computational Intelligence (ICACI)"
    },
    {
      "citation_id": "69",
      "title": "Recognition of Isolated Handwritten Characters of Gurumukhi Script using Neocognitron",
      "authors": [
        "D Sharma",
        "U Jain"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Applications",
      "doi": "10.5120/1503-2021"
    },
    {
      "citation_id": "70",
      "title": "The theory of intermolecular forces",
      "authors": [
        "A Stone"
      ],
      "year": "2013",
      "venue": "The theory of intermolecular forces"
    },
    {
      "citation_id": "71",
      "title": "Genetic algorithms and their applications",
      "authors": [
        "K.-S Tang",
        "K.-F Man",
        "S Kwong",
        "Q He"
      ],
      "year": "1996",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "72",
      "title": "An evolutionary gravitational search-based feature selection",
      "authors": [
        "M Taradeh",
        "M Mafarja",
        "A Heidari",
        "H Faris",
        "I Aljarah",
        "S Mirjalili",
        "H Fujita"
      ],
      "year": "2019",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2019.05.038"
    },
    {
      "citation_id": "73",
      "title": "Hybrid evolutionary approach for Devanagari handwritten numeral recognition using Convolutional Neural Network",
      "authors": [
        "A Trivedi",
        "S Srivastava",
        "A Mishra",
        "A Shukla",
        "R Tiwari"
      ],
      "year": "2018",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "74",
      "title": "Robust Real-Time Face Detection",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2004",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1023/B:VISI.0000013087.49260.fb"
    },
    {
      "citation_id": "75",
      "title": "Feature selection based on rough sets and particle swarm optimization",
      "authors": [
        "X Wang",
        "J Yang",
        "X Teng",
        "W Xia",
        "R Jensen"
      ],
      "year": "2007",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2006.09.003"
    },
    {
      "citation_id": "76",
      "title": "A BPSO-SVM algorithm based on memory renewal and enhanced mutation mechanisms for feature selection",
      "authors": [
        "J Wei",
        "R Zhang",
        "Z Yu",
        "R Hu",
        "J Tang",
        "C Gui",
        "Y Yuan"
      ],
      "year": "2017",
      "venue": "Applied Soft Computing Journal",
      "doi": "10.1016/j.asoc.2017.04.061"
    },
    {
      "citation_id": "77",
      "title": "A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms",
      "authors": [
        "D Wettschereck",
        "D Aha",
        "T Mohri"
      ],
      "year": "1997",
      "venue": "Artificial Intelligence Review",
      "doi": "10.1023/A:1006593614256"
    },
    {
      "citation_id": "78",
      "title": "Enhanced Local Texture Feature Sets for Face Recognition Under Difficult Lighting Conditions",
      "authors": [
        "Xiaoyang Tan",
        "B Triggs"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2010.2042645"
    },
    {
      "citation_id": "79",
      "title": "Particle swarm optimization for feature selection in classification: A multi-objective approach",
      "authors": [
        "B Xue",
        "M Zhang",
        "W Browne"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "80",
      "title": "Feature subset selection using a genetic algorithm",
      "authors": [
        "J Yang",
        "V Honavar"
      ],
      "year": "1998",
      "venue": "IEEE Intelligent Systems and Their Applications"
    },
    {
      "citation_id": "81",
      "title": "Cost-sensitive feature selection using two-archive multi-objective artificial bee colony algorithm",
      "authors": [
        "Y Zhang",
        "S Cheng",
        "Y Shi",
        "D Gong",
        "X Zhao"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "82",
      "title": "A novel atom search optimization for dispersion coefficient estimation in groundwater",
      "authors": [
        "W Zhao",
        "L Wang",
        "Z Zhang"
      ],
      "year": "2019",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "83",
      "title": "Atom search optimization and its application to solve a hydrogeologic parameter estimation problem. Knowledge-Based Systems",
      "authors": [
        "W Zhao",
        "L Wang",
        "Z Zhang"
      ],
      "year": "2019",
      "venue": "Atom search optimization and its application to solve a hydrogeologic parameter estimation problem. Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2018.08.030"
    }
  ]
}