{
  "paper_id": "2502.18186v1",
  "title": "Steering Language Model To Stable Speech Emotion Recognition Via Contextual Perception And Chain Of Thought",
  "published": "2025-02-25T13:26:25Z",
  "authors": [
    "Zhixian Zhao",
    "Xinfa Zhu",
    "Xinsheng Wang",
    "Shuiyuan Wang",
    "Xuelong Geng",
    "Wenjie Tian",
    "Lei Xie"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of comprehending diverse audio signal, performing audio analysis and generating textual responses. However, in speech emotion recognition (SER), ALMs often suffer from hallucinations, resulting in misclassifications or irrelevant outputs. To address these challenges, we propose C 2 SER, a novel ALM designed to enhance the stability and accuracy of SER through Contextual perception and Chain of Thought (CoT). C 2 SER integrates the Whisper encoder for semantic perception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S extends Emotion2Vec with semi-supervised learning to enhance emotional discrimination. Additionally, C 2 SER employs a CoT approach, processing SER in a step-by-step manner while leveraging speech content and speaking styles to improve recognition. To further enhance stability, C 2 SER introduces self-distillation from explicit CoT to implicit CoT, mitigating error accumulation and boosting recognition accuracy. Extensive experiments show that C 2 SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap, delivering more stable and precise emotion recognition. We release the training code, checkpoints, and test sets to facilitate further research 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Audio is a multifaceted medium for communication, conveying speech prosody, vocal tone, and paralinguistic cues through its acoustic features. Large-scale audio-language models (ALMs)  (Wang et al., 2024)  have demonstrated substantial progress in understanding diverse forms of audio signals, which is crucial for advancing Artificial General Intelligence  (AGI) . With increasing data availability, * Equal contribution. † Corresponding authors. 1 Hugging Face Collection, GitHub Repository Please describe emotion of this speech.\n\nThis speech expresseses the feeling of sadness. Maybe he is preparing for an exam.\n\nPlease describe the speaking style and content, and infer emotion of this speech.\n\nSpoken by a young woman with a slow pace, normal pitch, and low energy, the phrase 'Dear! dear!' conveys a happy tone.\n\nPlease consider the speaking style and content, and directly describe emotion of this speech.\n\nThis speech conveys a happy tone.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "''Explicit To Implicit'' Distillation Previous Approach Proposed Approach",
      "text": "Figure  1 : Overview of C 2 SER. C 2 SER overcomes the issue of hallucinations through contextual perception and chain of thought.\n\ncomputational power, and model size, significant strides have been made in speech signal comprehension, analysis, and reasoning, leading to more natural and human-like text responses.\n\nSpeech emotions are crucial in communication, influencing how individuals interact and respond through variations in tone, rhythm, and intensity. However, ALMs still face challenges in speech emotion recognition (SER)  (Bellver-Soler et al., 2024; Akman et al., 2025) , which aims to accurately detect emotional states from spoken language. Speech emotions are inherently complex, encompassing diverse affective states, and their precise recognition is essential for enhancing communication effectiveness across diverse domains. Despite advancements in general audio understanding, ALMs often struggle with hallucinations in the SER task due to the intricate context of speech. These hallucinations lead to misclassifications, where the model may incorrectly interpret the emotional tone or generate irrelevant responses, undermining the model's reliability in real-world appli-cations.\n\nIn this work, we address the hallucination problem in ALM-based SER by incorporating detailed speech information and expanding the model's reasoning length and depth. As illustrated in Figure  1 , we introduce C 2 SER, a novel ALM designed to improve both the stability and accuracy of SER. C 2 SER integrates two critical components: contextual perception and a chain of thought (CoT), leveraging both speech content and speaking styles (e.g., speaking rate, pitch, energy) to facilitate emotion recognition. The encoder of Whisper  (Radford et al., 2023) , trained for automatic speech recognition (ASR), speech translation, and language identification, is employed for semantic perception. For acoustic perception, we introduce Emotion2Vec-S, a refined extension of Emotion2Vec  (Ma et al., 2024b) , designed to enhance the extraction of emotion-related information from audio. Emotion2Vec-S incorporates semisupervised contrastive loss at the category level, improving emotional discrimination by combining self-supervised and semi-supervised learning.\n\nRecognizing that speech emotions are influenced by both speech content and speaking styles, such as aggressive speech characterized by a loud volume potentially indicating anger, C 2 SER employs a  CoT (Guo et al., 2025)  training approach to incentivize reasoning capability. This approach decomposes the SER task into sequential steps: first perceiving speech content and speaking style, followed by emotion inference, with the assistance of prior context. This structured method imitates human thinking and reduces the possibility of hallucinations. To further enhance stability and prevent error propagation, especially in longer thought chains, C 2 SER introduces self-distillation, transferring knowledge from explicit to implicit CoT. This process helps minimize error accumulation, improving the model's overall performance.\n\nWe validate the effectiveness of C 2 SER through extensive experiments using multiple speech corpora and compare C 2 SER with state-of-the-art models, including SECap  (Xu et al., 2024)  and Qwen2-Audio  (Chu et al., 2024) . To better simulate real-world context, we introduce a new SER test set, Emo-Emilia. Emo-Emilia is created through an automated labeling approach on the in-the-wild Emilia corpus and manually verified to ensure quality and diversity across various scenarios. Our experimental results demonstrate that C 2 SER significantly outperforms existing models on public test sets and Emo-Emilia in terms of weighted accuracy (WA), unweighted accuracy (UA), and Macro F1 score, while notably reducing hallucinationrelated errors. These findings highlight the potential of C 2 SER to provide stable and reliable emotion recognition in diverse contexts.\n\nThe key contributions of our work are summarized as follows:\n\n• We propose C 2 SER, a novel ALM that integrates contextual perception and chain of thought to mitigate hallucinations in SER.\n\n• We introduce Emotion2Vec-S, working as the acoustic perception module in C 2 SER, which enhances the original Emotion2Vec model by incorporating semi-supervised contrastive loss at the category level, greatly improving emotional discrimination. • We release the code, checkpoints, and test sets to promote further research in the field of SER.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Language Model",
      "text": "Large language models (LLMs) have made significant strides in natural language processing (NLP), showcasing remarkable capabilities across a variety of tasks  (Zhao et al., 2023) . As audio is a critical medium of communication in human interactions and human-computer engagement, recent research has extended LLMs to integrate the audio modality, leading to the development of audio-language models (ALMs)  (Wang et al., 2024) . ALMs tackle tasks such as audio event detection, audio captioning, and speech recognition, serving as a cornerstone for comprehensive audio understanding  (Chu et al., 2023) .\n\nWith the rapid advancements in both LLMs and the audio domain, ALMs have gained significant attention for their powerful general audio comprehension abilities. A typical ALM architecture consists of three core components: an audio encoder for modality-specific feature extraction, an LLM for text generation, and a projection layer to bridge the gap between the audio and text modalities. In addition to these foundational components, several studies have focused on refining ALM performance through innovative model architectures. For example, SALMONN  (Tang et al., 2024)  utilizes dual encoders to separately process speech and non-speech audio signals, effectively mitigating potential conflicts between different types of audio input. Other approaches have explored training strategies to enhance ALMs' capabilities, with Qwen2-Audio  (Chu et al., 2024)  being a notable example. This model employs a comprehensive training pipeline that includes pretraining, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF).\n\nWhile significant progress has been made in improving the generalization and intelligence of ALMs, their performance in speech emotion recognition (SER) remains unsatisfactory, primarily due to hallucinations. SER is particularly challenging because speech emotions are inherently complex and context-dependent  (Yang et al., 2024) , making it difficult for ALMs to interpret emotional states accurately. In many cases, the model may be misled by the content of the speech, leading to incorrect classifications or irrelevant responses.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Speech emotion is a key form of paralinguistic information that effectively conveys the speaker's intent. Speech Emotion Recognition (SER) aims to automatically identify the emotional state of a speaker from their speech. The typical SER pipeline consists of three stages: speech preprocessing, feature extraction, and emotion classification  (Khare et al., 2024) . Early studies relied on manually engineered feature sets, such as MFCC, and simple neural network architectures like CNN and RNN, achieving basic performance on laboratory datasets (e.g., CREMA-D  (Cao et al., 2014) , IEMOCAP  (Busso et al., 2008) ).\n\nTo address the challenge of recognizing diverse emotional expressions in real-world environments, recent research has shifted towards self-supervised learning (SSL) models  (Naini et al., 2024; Zhu and Sato, 2023) , known for their powerful generalization capabilities. SSL models are trained on large-scale unlabeled speech data in an unsupervised manner, allowing them to extract rich, generalizable representations directly from raw speech waveforms. Popular SSL models, such as Hu-BERT  (Hsu et al., 2021)  and wav2vec 2.0  (Baevski et al., 2020) , have demonstrated significant effec-tiveness in extracting emotional features, serving as robust encoders for SER tasks. Additionally, researchers have explored emotion-specific SSL models designed to capture emotion-relevant features. A popular approach involves fine-tuning SSL models on emotionally labeled data for specific emotional tasks. A prominent example is Emo-tion2Vec  (Ma et al., 2024b) , which is pre-trained on emotional data through self-supervised online distillation. Emotion2Vec uses both utterance-level and frame-level loss as supervision, demonstrating remarkable improvements in emotion recognition across different languages.\n\nDespite the advancements made in SER, mainstream emotional SSL models typically employ single-level constraints, such as utterance-level or category-level constraints. While Emotion2Vec combines utterance-level and frame-level losses that are actually constraints at the utterance level, it still struggles to distinguish similar emotional expressions, such as fear and sadness, potentially leading to confusion in emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Framework Overview",
      "text": "C 2 SER is designed to mitigate hallucinations in speech emotion recognition (SER) and to deliver stable emotion recognition. As illustrated in Figure  2 , the C 2 SER architecture consists of two primary components: a contextual perception module and a text-based large language model (LLM). The contextual perception module extracts detailed information regarding both the semantic and acoustic aspects, which the text LLM subsequently leverages via a chain-of-thought process to make final predictions.\n\nMore specifically, the contextual perception module comprises the following elements: a Whisper  (Radford et al., 2023)  encoder for semantic perception, Emotion2Vec-S for acoustic perception, and a connection model designed to align the feature dimensions with those required by the text LLM. Formally, given a speech waveform X, the Whisper encoder extract semantic representations S = s 1 , s 2 , ..., s N and the Emotion2Vec-S extracts acoustic representations A = a 1 , a 2 , ..., a M from X. Let Y = y 1 , y 2 , ..., y T be the text descriptions and P = p 1 , p 2 , ..., p L be the text prompts. The text LLM, parameterized by θ, takes S and A as input and predicts Y in an autoregressive manner. The overall process can be formulated as a condi-Please consider the speaking style and content, and directly describe emotion of this speech.\n\nSpoken by a young woman with a slow pace, normal pitch, and low energy, the phrase 'Dear! dear!' conveys a happy and sincere tone.\n\nPlease describe the speaking style and content, and infer emotion of this speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Large Language Model",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Text Tokenizer Whisper Encoder",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Stage 1: Explicit Cot",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Large Language Model",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Text Tokenizer",
      "text": "This speech conveys a happy and sincere tone. tional probability:\n\nP (y t |s 1 , . . . , s N , a 1 , . . . , a M , p 1 , . . . , p L , y 1 , . . . , y t-1 ; θ).\n\n(1)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contextual Perception",
      "text": "Our contextual perception module is designed to extract both semantic and acoustic representations from speech, and it comprises a Whisper encoder and Emotion2Vec-S. Specifically, C 2 SER employs the Whisper-Medium model as its speech encoder. This model features two one-dimensional convolutional layers with a 2× downsampling factor, followed by 24 Transformer layers. Since Whisper is a supervised model trained for speech recognition and translation, its encoded representations S capture rich semantic information. Emotion2Vec-S is built upon the universal speech emotion representation model, Emo-tion2Vec. Emotion2Vec follows the architecture of data2vec  (Baevski et al., 2022) , consisting of several standard Transformer blocks, and is pretrained on open-source, unlabeled emotion data using self-supervised online distillation. It combines an utterance-level loss to capture global emotional features with a frame-level loss to capture contextual emotion nuances. Emotion2Vec has demon-strated superior performance compared to previous state-of-the-art pre-trained universal models and emotion specialist models, which often require only the training of linear layers for the SER task on emotional speech corpora. However, because Emotion2Vec focuses solely on the emotional expressions within individual utterances, it potentially struggles to distinguish similar utterances that express different emotions, such as fear and sadness.\n\nWith the above observation, Emotion2Vec-S introduces a coarse-level supervision to Emotion2Vec. Vanilla Emotion2Vec expands data2vec2.0  (Baevski et al., 2023)  with a fixed number of utterance tokens and is trained with L U tt to learn the global emotion and L F rm to learn the context emotion. Inspired by CLIP  (Radford et al., 2021) , Emotion2Vec-S extends Emotion2Vec with a category-level contrastive loss L Cate . Specifically, let G be the global embedding of Emo-tion2Vec after average pooling. Emotion2Vec-S applies a contrastive loss on G by treating embeddings from utterances of the same emotion category as positive pairs and those from different categories as negative pairs. The model calculates cosine similarities between these embeddings, maximizing the similarity of positive pairs while minimizing that of negative pairs. The overall loss function for Emotion2Vec-S is formulated as follows:\n\nwhere λ utt and λ cate are hyperparameters that balance the contributions of the utterance-level and category-level losses, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Explicit Chain-Of-Thought",
      "text": "Explicit CoT reasoning enhances the ability of LLMs to handle specific tasks by detailing intermediate steps, thereby guiding the model through its reasoning process  (Yue et al., 2024; Yu et al., 2024) . In C 2  SER, explicit CoT is employed to sequentially address the SER task. After the contextual perception module extracts detailed information regarding speech content and speaking styles, C 2 SER first generates speech transcripts and descriptive captions of speaking styles and then infers the final speech emotion based on the aggregated context.\n\nExplicit CoT data. We leverage signal processing tools and text LLMs to construct training data for explicit CoT. Given an emotional speech corpus that includes text transcripts and emotion labels, we extract key attributes-such as speaking rate, pitch, and energy-and map these to discrete levels (e.g., low, medium, high). Next, we employ the GLM-4-9B-Chat model  (Zeng et al., 2024)  to generate a detailed reasoning path that combines text transcripts, extracted speech attributes, and corresponding emotion labels. This reasoning path serves as the explicit CoT training data.\n\nExplicit CoT training. In the explicit CoT training stage, the text LLM is integrated with the contextual perception module, and the entire system is fine-tuned using the explicit CoT data. To further improve the reasoning capabilities of the model, structured text prompts are used, as illustrated in Figure  2  (stage 1), to guide the model through each intermediate reasoning step. As a result, after this stage of training, C 2 SER is able to recognize speech content and speaking styles, and subsequently infer the final emotion categories based on the complete speech context.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Implicit Chain-Of-Thought",
      "text": "Although the explicit CoT approach enables C 2 SER to address SER step by step using detailed intermediate representations of speech content and speaking styles, it also introduces inefficiencies during inference and increases the risk of error accumulation  (Yuen et al., 2024; Deng et al., 2024) . To overcome these limitations, we propose a selfdistillation strategy that transitions C 2 SER from explicit CoT to implicit CoT.\n\nImplicit CoT Data. At this stage, we continue to use the same speech dataset as used in the explicit CoT training; however, the processing of the reasoning path is simplified. Rather than generating detailed intermediate descriptions, the GLM-4-9B-Chat model directly produces descriptions only in terms of emotion labels for each speech segment. Implicit CoT Training. During this phase, we fine-tune the model on a combination of explicit and implicit CoT data. To ensure that the model maintains its ability to infer emotion categories from rich speech context during the self-distillation process, we gradually transition the training data from explicit to implicit CoT data using a linear schedule. Furthermore, we employ customized text prompts, illustrated in Figure  2  (stage 2), to guide the model reasoning process under the implicit framework. This approach enables C 2 SER to efficiently generate accurate emotion predictions while addressing the inefficiencies and error propagation associated with explicit CoT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment Setup",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "The statistics of the training corpora are summarized in Table  1 , which covers seven emotions: anger, happiness, neutral, sadness, surprise, disgust and fear. We utilize six open-source corpora that contain both emotion and text labels, including IEMOCAP  (Busso et al., 2008)    (Zhou et al., 2021) , and MER2024  (Lian et al., 2024) , alongside an internal corpus containing text labels for model training. To obtain emotion labels for the internal corpus, we apply using an efficient automated labeling method using Emotion2Vec 2 for annotating speech emotions and GLM-4-9B-Chat 3    (Zeng et al., 2024)  for annotating text emotions. We then take the intersection of the two annotations to ensure consistency and reliability. Additionally, we incorporate an internal speech corpus containing approximately 2400 hours of unlabeled data during the training of Emotion2Vec. The distributions of each emotion and the label construction for both explicit and implicit CoT are provided in Appendix A and Appendix B.\n\nTo comprehensively evaluate the model's performance in downstream tasks, we follow the   , 2008) . Furthermore, considering the complexity of speech emotions in real-world scenarios, we introduce a diverse speech emotion test set. Specifically, we apply the automated labeling approach to annotate Emilia  (He et al., 2024) , a large-scale multilingual and diverse speech generation resource with over 100,000 hours of speech data that captures a wide range of emotional contexts. We then manually verify the accuracy of the emotion labels. Each utterance is checked by at least two experts to ensure both accuracy and reliability. The final proposed test set, Emo-Emilia, consists of 1400 test samples, with 100 samples per emotion category across seven types in both Chinese and English (700 samples per language).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Implement Details",
      "text": "The architecture of Emotion2Vec-S is based on the original Emotion2Vec 4  model, with the addition of a classifier consisting of three fully connected layers. λ utt and λ cate are set to 0.1 and 100, respectively. We employ the Whisper-medium 5  encoder for semantic feature extraction. The connection module is composed of a 4-layer Transformer followed by a linear layer, with intermediate feature dimensions set to 2,560 in the feed-forward module.\n\nFor the text LLM component, we utilize the Qwen2-7B-Instruct 6  model  (Chu et al., 2024)  and fine-tune it using Low-Rank Adaptation (LoRA)  (Hu et al., 2022) . The LoRA rank is set to 8, the scaling factor is 32, and the dropout rate for LoRA matrices is 0.1.\n\nTo train Emotion2Vec-S, we employ 8 Nvidia 4090 GPUs, with a gradient accumulation step set to 2. The optimizer used is Adam, with a learning rate of 7.5 × 10 -5 and a weight decay of 1 × 10 -2 . The learning rate scheduler follows a cosine annealing strategy with a warm-up ratio of 5%. The remaining hyperparameters are consistent with those used in the vanilla Emotion2Vec model. To train the entire C 2 SER model, we utilize 2 Nvidia A6000 GPUs and employ the AdamW optimizer with the following parameters: β 1 = 0.9 and β 2 = 0.99. The initial learning rate is 5.0×10 -5 , with a weight decay coefficient of 0.01. The learning rate scheduler uses WarmupLR, with the warm-up steps set to 15% of the total training steps. During C 2 SER training, Emotion2Vec-S is frozen to retain its ability to effectively extract emotional features from speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Comparison Systems And Evaluation Metrics",
      "text": "We conduct comparative experiments with several advanced models, including self-supervised learning (SSL) models (e.g., WavLM  (Chen et al., 2022) , Data2Vec  (Baevski et al., 2022) , Data2Vec 2.0  (Baevski et al., 2023) ), and audiolanguage models (e.g., SECap  (Xu et al., 2024) , Qwen2-Audio  (Chu et al., 2024) , and SenseVoice-Small  (An et al., 2024) ). For SSL models, we follow the methodology of EmoBox  (Ma et al., 2024a) . First, features are extracted from the last Transformer layer of the pre-trained models and undergo uniform layer normalization to accelerate convergence. Then, a downstream network is applied to perform the SER task, which consists of a simple linear hidden layer, a ReLU activation function, a pooling layer, and a classification head.\n\nFor ALMs, we train SECap on our speech corpora while directly using official checkpoints of Qwen2-Audio and SenseVoice-Small. We evaluate the models using three key metrics: weighted average accuracy (WA), unweighted average accuracy (UA), and the Macro F1 score. WA represents the overall accuracy of the model, UA corresponds to the average class-wise accuracy, and the Macro F1 score provides a balanced evaluation, particularly useful in cases of class imbalance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation Of Emotion2Vec-S",
      "text": "The results are presented in Table  2 , where we compare Emotion2Vec-S with various SSL pre-trained models of similar model size and training corpora. Notably, Emotion2Vec-F refers to the Emo-tion2Vec model trained directly on the same corpora as Emotion2Vec-S, allowing us to investigate the impact of different datasets on model performance. The results demonstrate that Emotion2Vec-S consistently outperforms other models across most datasets. Interestingly, the performance gap between Emotion2Vec and Emotion2Vec-S indicates that the training corpus does influence the results, but it does not always lead to significant improvements. Nonetheless, Emotion2Vec-S consistently shows steady improvement compared to both Emotion2Vec-F and Emotion2Vec, validating the effectiveness of semi-supervised contrastive learning.\n\nFurthermore, we observe that the models exhibit varying performance across different test sets and languages. Specifically, Emotion2Vec-S outperforms comparison models by a significant margin on Chinese test sets while achieving competitive results on English datasets. Additionally, Emotion2Vec-S excels in multilingual test sets, particularly in ESD and Emo-Emilia. When extended to other languages, Emotion2Vec-S achieves the best results on the Mexican test set and ranks second on the Italian test set. Although Emotion2Vec-S is trained primarily on Chinese and English speech corpora, these results highlight its impressive generalization capabilities across different languages. Overall, these findings suggest that Emotion2Vec-S offers superior emotion discrimination compared to the original Emotion2Vec model, establishing it as a robust foundation model for extracting speech emotion representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation Of C 2 Ser",
      "text": "We compare C 2 SER with several leading audiolanguage models (ALMs) across various test sets. As shown in Table  3 , C 2 SER outperforms most comparison models on the majority of datasets. Notably, even with only 3,500 randomly preserved utterances for evaluation in the ESD dataset, C 2 SER still achieves exceptional performance. We speculate that this can be attributed to the shared origin of the training and test sets, a factor that may also explain the Qwen2-Audio results on EmoV-DB. However, when compared to SECap, which is trained on the same speech corpora, C 2 SER consistently outperforms SECap across all test datasets except MESD, where the performance of both models is similar. These results validate that incorporating speech context through the chain-of-thought approach effectively improves SER. Additionally, we observe significant improvements when comparing C 2 SER with explicit CoT to C 2 SER with implicit CoT. This performance gain highlights the success of self-distillation, which helps preserve reasoning capabilities while significantly reducing potential error accumulation in longer thought chains.\n\nALMs, including C 2 SER, also exhibit notable domain shifts across different languages. Specifically, C 2 SER outperforms comparison models on the M3ED and CASIA datasets, while achieving comparable results on MELD. Moreover, C 2 SER excels across all multilingual test sets. When extended to other languages, such as Italian, C 2 SER continues to lead. These findings demonstrate the strong language generalization capabilities of C 2 SER, indirectly suggesting that both contextual perception and chain-of-thought contribute to improving emotion recognition across diverse speech data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct an ablation study to evaluate the contribution of each component in C 2 SER. The experimental results are presented in Table  4 . Firstly, removing the Whisper encoder leads to a significant degradation in performance, with the model failing to converge during explicit CoT training due to the lack of semantic perception. Secondly, the model incorporating Emotion2Vec-S outperforms the version without it, demonstrating that acoustic perception is crucial for capturing emotional expressions effectively. Finally, excluding CoT causes a substantial drop in performance. This result suggests the reasoning capability of C 2 SER is improved after CoT training, which leads to a more accurate and stable emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Limitation",
      "text": "Although C 2 SER effectively addresses the hallucination problem in ALM-based SER, we identified the need for further improvements in the performance across different languages. This is mainly due to the imbalance in the training corpora of C 2 SER. We believe that scaling up data size and leveraging balanced corpora to train C 2 SER will effectively improve performance across different languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A Data Distribution",
      "text": "The emotion and language distributions of the training corpora are shown in Figures  3  and 4 . As observed, neutral emotions account for nearly half of the dataset, while fear and disgust constitute less than 2%. The scarcity of fear and disgust data arises from challenges such as subjective annotation (e.g., difficulty in accurate identification), limited natural occurrences in contextual expressions, and technical barriers in detecting these emotions from speech or text. Additionally, the proportion of Chinese speech is approximately double that of English speech. We plan to develop more balanced training corpora to address these disparities.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B Explicit Cot Construction",
      "text": "To construct training data for both explicit and implicit CoT, we first use signal processing tools to collect relevant speech attributes, followed by the generation of reasoning paths via a text-based LLM. Specifically, we compute pitch contours from speech using the PENN library 7  , and calculate the utterance-level mean. We extract energy from the speech using the pyloudnorm library 8  , which returns a value representing the loudness level. The speaking rate is determined by dividing the number of phonemes in the transcript by the utterance length. Silences at both the beginning and end of the speech waveforms are trimmed for accuracy. After collecting statistics for all utterances, we calculate the mean µ and standard deviation σ, and divide the values into three levels based on the Central Limit Theorem: 'Low' corresponds value below µ -σ, 'Medium' corresponds values between µ -σ and µ + σ, and 'High' corresponds values above µ + σ.\n\nAfter gathering speech attributes, we use GLM-4-9B-Chat to generate a CoT path conditioned on the speech attributes, original text transcripts, and emotion labels. A simple prompt is designed to guide GLM-4-9B-Chat to first produce speaking styles and speech content, followed by the final emotion. The template for this prompt is shown in Table  5 .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C Evaluation Details",
      "text": "For all test sets, we retain utterances within the seven emotion categories: anger, happiness, neutral, sadness, surprise, disgust, and fear. Similar emotion labels, such as Amused, Joy, and Happy, are merged into the happiness category. During evaluation, SSL models are tested using leave-onesession-out five-fold cross-validation, with 20% of the training set used as the validation set for the Emo-Emilia test set. Evaluation on other test sets follows the EmoBox. Additionally, since the output of ALMs may include more than just emotion categories, we use Qwen2.5-14B-Chat 9  to directly extract emotion labels from the descriptions generated by ALMs. The evaluation script is available in our repository.\n\nTo assess the performance of C 2 SER, we compare with the following systems.\n\nQwen2-Audio: A multimodal framework for comprehensive audio understanding and generation. Qwen2-Audio employs Whisper-large-V3 as Prompt: Based on the provided speech features-including a speaking rate of <speaking rate label>, a volume level of <energy label>, and a pitch of <pitch label>-along with the text content '<text label>' and the emotion <emotion label>, generate a natural and logical emotional description. Here is an example: 'The speaker spoke at a <speaking rate label> pace, with a <pitch label> tone and <energy label> level: \"<text label>\". Based on the analysis of speech characteristics, the emotion was inferred to be <emotion label>.' Ensure including all speech features and logic of the description.\" Generated Example: The speaker spoke at a moderate pace, with a low-pitched tone and a soft volume: \"together you sort of get this whole narrative of feedback where students are constantly feeling like you're watching and you're paying attention and you're giving them suggestions I often think about like a basketball.\" Based on the speech characteristics, the emotion was inferred to be disgust, revealing a sense of resentment or aversion towards the described situation.\n\nthe audio encoder to capture subtle acoustic features and integrates the Qwen-7B LLM as the foundational component, enabling efficient alignment and generation between audio and text.\n\nSenseVoice-Small: An encoder-only speech foundation model designed for rapid voice understanding. It employs a memory-equipped selfattention network (SAN-M) to enable fast and efficient inference.\n\nSECap: A framework that generates highquality style captions. It uses HuBERT to extract speech features, Q-Former as the Bridge-Net, and LLaMA as the text decoder to produce coherent style captions. We train SECap on the same data as C 2 SER.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "D Category Accuracy Of Emotion2Vec-S",
      "text": "We evaluate the category accuracy of Emotion2Vec-S using five-fold cross-validation, with 20% of the training set used as the validation set. The average results across each fold are shown in Figure 5. Emotion2Vec-S outperforms Emotion2Vec in recognition accuracy for all emotion categories. Disgust achieves the highest recognition accuracy, while happiness has relatively lower recognition accuracy. In all cases, the accuracy of Emotion2Vec-S is higher than that of Emotion2Vec. Overall, the performance is relatively balanced across the emotions.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "E Category Accuracy Of C 2 Ser",
      "text": "We evaluate the category accuracy of C 2 SER through direct inference on the Emo-Emilia test set. The results are displayed in Figure  6 . C 2 SER achieves higher accuracy than Qwen2-audio across",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of C2SER. C2SER overcomes the",
      "page": 1
    },
    {
      "caption": "Figure 2: Detailed structure of C2SER. C2SER leverages speech content and speaking styles through chain-of-",
      "page": 4
    },
    {
      "caption": "Figure 2: (stage 1), to guide the model",
      "page": 5
    },
    {
      "caption": "Figure 2: (stage 2), to",
      "page": 5
    },
    {
      "caption": "Figure 3: Distribution of each emotions.",
      "page": 12
    },
    {
      "caption": "Figure 4: Distribution of different language.",
      "page": 12
    },
    {
      "caption": "Figure 5: Category Accuracy (%) of Emotion2Vec-S on",
      "page": 13
    },
    {
      "caption": "Figure 6: Category Accuracy (%) of C2SER on the",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 3: , C2SER outperforms most",
      "data": [
        {
          "Model": "",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "ESD (Mixlingual)"
        },
        {
          "Model": "WavLM-base\ndata2vec base\ndata2vec2.0 base\nEmotion2Vec\nEmotion2Vec-F\nEmotion2Vec-S",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "72.90\n72.90\n72.55\n65.05\n65.05\n64.55\n73.40\n73.40\n73.10\n70.22\n70.22\n70.06\n64.18\n64.18\n63.96\n79.84\n79.84\n79.72"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "ASVP-ESD (Mixlingual)"
        },
        {
          "Model": "WavLM-base\ndata2vec base\ndata2vec2.0 base\nEmotion2Vec\nEmotion2Vec-F\nEmotion2Vec-S",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "46.38\n58.05\n47.35\n37.66\n50.79\n38.26\n46.0\n57.57\n46.62\n48.60\n49.60\n58.30\n47.05\n57.77\n48.25\n58.88\n48.20\n45.63"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "Emo-Emilia (Mixlingual)"
        },
        {
          "Model": "WavLM-base\ndata2vec base\ndata2vec2.0 base\nEmotion2Vec\nEmotion2Vec-F\nEmotion2Vec-S",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "67.26\n67.26\n67.28\n63.80\n63.80\n63.72\n64.60\n64.60\n64.46\n68.02\n68.02\n68.00\n59.24\n59.24\n59.00\n80.66\n80.66\n80.58"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Firstly, distillation,maintainingreasoningcapabilitywhile",
      "data": [
        {
          "Model": "",
          "UA(%) ↑ WA(%) ↑\nF1(%) ↑": "ESD (Mixlingual)"
        },
        {
          "Model": "Qwen2-Audio\nSenseVoice-S\nSECap\nC2SER(Explicit CoT)\nC2SER(Implicit CoT)",
          "UA(%) ↑ WA(%) ↑\nF1(%) ↑": "56.26\n56.26\n33.06\n52.23\n52.23\n42.20\n42.51\n42.51\n25.55\n93.81\n93.86\n68.19\n96.33\n96.34\n81.62"
        },
        {
          "Model": "Model",
          "UA(%) ↑ WA(%) ↑\nF1(%) ↑": "ASVP-ESD (Mixlingual)"
        },
        {
          "Model": "Qwen2-Audio\nSenseVoice-S\nSECap\nC2SER(Explicit CoT)\nC2SER(Implicit CoT)",
          "UA(%) ↑ WA(%) ↑\nF1(%) ↑": "36.53\n43.44\n48.01\n16.55\n16.19\n21.57\n25.07\n27.95\n19.42\n41.62\n47.34\n32.58\n43.86\n48.54\n34.06"
        },
        {
          "Model": "Model",
          "UA(%) ↑ WA(%) ↑\nF1(%) ↑": "Emo-Emilia (Mixlingual)"
        },
        {
          "Model": "Qwen2-Audio\nSenseVoice-S\nSECap\nC2SER(Explicit CoT)\nC2SER(Implicit CoT)",
          "UA(%) ↑ WA(%) ↑\nF1(%) ↑": "39.07\n39.07\n31.91\n63.31\n63.31\n56.84\n32.50\n32.50\n23.62\n68.29\n68.29\n61.28\n69.00\n69.00\n61.61"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Table 2: Emotion2Vec-S performance on datasets of Chinese, English, Italian, Mexican and mixed languages. The best and the second best result is shown in bold and by underlined",
      "venue": "Table 2: Emotion2Vec-S performance on datasets of Chinese, English, Italian, Mexican and mixed languages. The best and the second best result is shown in bold and by underlined"
    },
    {
      "citation_id": "2",
      "title": "Models are tested in a zero-shot fashion. The best and the second best result is shown in bold and by underlined",
      "venue": "Models are tested in a zero-shot fashion. The best and the second best result is shown in bold and by underlined"
    },
    {
      "citation_id": "3",
      "title": "* Qwen2-Audio's results on EmoV-DB may indicate data leakage",
      "venue": "* Qwen2-Audio's results on EmoV-DB may indicate data leakage"
    },
    {
      "citation_id": "4",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "Adaeze Adigwe",
        "Noé Tits",
        "Kevin Haddad",
        "Sarah Ostadabbas",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems"
    },
    {
      "citation_id": "5",
      "title": "Improving audio explanations using audio language models",
      "authors": [
        "Alican Akman",
        "Qiyang Sun",
        "Björn Schuller"
      ],
      "year": "2025",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2025.3532218"
    },
    {
      "citation_id": "6",
      "title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "authors": [
        "Qian Keyu An",
        "Chong Chen",
        "Zhihao Deng",
        "Changfeng Du",
        "Zhifu Gao",
        "Yue Gao",
        "Ting Gu",
        "Hangrui He",
        "Kai Hu",
        "Shengpeng Hu",
        "Yabin Ji",
        "Zerui Li",
        "Heng Li",
        "Haoneng Lu",
        "Xiang Luo",
        "Bin Lv",
        "Ziyang Ma",
        "Chongjia Ma",
        "Changhe Ni",
        "Jiaqi Song",
        "Xian Shi",
        "Hao Shi",
        "Wen Wang",
        "Yuxuan Wang",
        "Zhangyu Wang",
        "Zhijie Xiao",
        "Yexin Yan",
        "Bin Yang",
        "Qinglin Zhang",
        "Shiliang Zhang",
        "Nan Zhang",
        "Siqi Zhao",
        "Zheng"
      ],
      "year": "2024",
      "venue": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms"
    },
    {
      "citation_id": "7",
      "title": "Efficient self-supervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "Alexei Baevski",
        "Arun Babu",
        "Wei-Ning Hsu",
        "Michael Auli"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning, ICML 2023"
    },
    {
      "citation_id": "8",
      "title": "2022. data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Multimodal audio-language model for speech emotion recognition",
      "authors": [
        "Jaime Bellver-Soler",
        "Iván Martín-Fernández",
        "Jose Bravo-Pacheco",
        "Sergio Romero",
        "Fernando Fernández Martínez",
        "Luis Fernando",
        "D' Haro"
      ],
      "year": "2024",
      "venue": "Odyssey 2024: The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "11",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "12",
      "title": "CREMA-D: crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "13",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Xiangzhan Zeng",
        "Furu Yu",
        "Wei"
      ],
      "year": "2022",
      "venue": "IEEE J. Sel. Top. Signal Process"
    },
    {
      "citation_id": "14",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report"
    },
    {
      "citation_id": "15",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models"
    },
    {
      "citation_id": "16",
      "title": "EMOVO corpus: an italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014"
    },
    {
      "citation_id": "17",
      "title": "From explicit cot to implicit cot: Learning to internalize cot step by step",
      "authors": [
        "Yuntian Deng",
        "Yejin Choi",
        "Stuart Shieber"
      ],
      "year": "2024",
      "venue": "From explicit cot to implicit cot: Learning to internalize cot step by step"
    },
    {
      "citation_id": "18",
      "title": "The mexican emotional speech database (MESD): elaboration and assessment based on machine learning *",
      "authors": [
        "Mathilde Marie Duville",
        "Luz María Alonso-Valerdi",
        "David Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society, EMBC 2021"
    },
    {
      "citation_id": "19",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Dejian Daya Guo",
        "Haowei Yang",
        "Junxiao Zhang",
        "Ruoyu Song",
        "Runxin Zhang",
        "Qihao Xu",
        "Shirong Zhu",
        "Peiyi Ma",
        "Xiao Wang",
        "Bi"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "20",
      "title": "Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "21",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "22",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "Edward Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "K Smith",
        "Victoria Khare",
        "Esmaeil Blanes-Vidal",
        "U Nadimi",
        "Acharya"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "24",
      "title": "Asvp-esd: A dataset and its benchmark for emotion recognition using both speech and non-speech utterances",
      "authors": [
        "Dejoli Landry",
        "Qianhua He",
        "Haikang Yan",
        "Yanxiong Li"
      ],
      "year": "2020",
      "venue": "Global Scientific Journals"
    },
    {
      "citation_id": "25",
      "title": "MER 2024: Semisupervised learning, noise robustness, and openvocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen",
        "Jiangyan Yi",
        "Rui Liu",
        "Kele Xu",
        "Bin Liu",
        "Erik Cambria",
        "Guoying Zhao",
        "Björn Schuller",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "2024a. Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Ziyang Ma",
        "Mingjie Chen",
        "Hezhao Zhang",
        "Zhisheng Zheng",
        "Wenxi Chen",
        "Xiquan Li",
        "Jiaxin Ye",
        "Xie Chen",
        "Thomas Hain"
      ],
      "venue": "Interspeech 2024",
      "doi": "10.21437/Interspeech.2024-788"
    },
    {
      "citation_id": "27",
      "title": "2024b. emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting"
    },
    {
      "citation_id": "28",
      "title": "The msp-conversation corpus",
      "authors": [
        "Luz Martinez-Lucas",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "21st Annual Conference of the International Speech Communication Association, Interspeech 2020, Virtual Event"
    },
    {
      "citation_id": "29",
      "title": "Generalization of self-supervised learning-based representations for cross-domain speech emotion recognition",
      "authors": [
        "Abinay Reddy Naini",
        "Mary Kohler",
        "Elizabeth Richerson",
        "Donita Robinson",
        "Carlos Busso"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "31",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021"
    },
    {
      "citation_id": "32",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Pmlr",
        "Jong Radford",
        "Tao Kim",
        "Greg Xu",
        "Christine Brockman",
        "Ilya Mcleavey",
        "Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning, ICML 2023"
    },
    {
      "citation_id": "33",
      "title": "SALMONN: towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "G Shreya",
        "Woan-Shiuan Upadhyay",
        "Bo-Hao Chien",
        "Lucas Su",
        "Ya-Tse Goncalves",
        "Ali Wu",
        "Carlos Salman",
        "Chi-Chun Busso",
        "Lee"
      ],
      "year": "2023",
      "venue": "11th International Conference on Affective Computing and Intelligent Interaction, ACII 2023"
    },
    {
      "citation_id": "35",
      "title": "Audiobench: A universal benchmark for audio large language models",
      "authors": [
        "Bin Wang",
        "Xunlong Zou",
        "Geyu Lin",
        "Shuo Sun",
        "Zhuohan Liu",
        "Wenyu Zhang",
        "Zhengyuan Liu",
        "Aiti Aw",
        "Nancy Chen"
      ],
      "year": "2024",
      "venue": "Audiobench: A universal benchmark for audio large language models"
    },
    {
      "citation_id": "36",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Yaoxun Xu",
        "Hangting Chen",
        "Jianwei Yu",
        "Qiaochu Huang",
        "Zhiyong Wu",
        "Shi-Xiong Zhang",
        "Guangzhi Li",
        "Yi Luo",
        "Rongzhi Gu"
      ],
      "year": "2024",
      "venue": "Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Air-bench: Benchmarking large audio-language models via generative comprehension",
      "authors": [
        "Qian Yang",
        "Jin Xu",
        "Wenrui Liu",
        "Yunfei Chu",
        "Ziyue Jiang",
        "Xiaohuan Zhou",
        "Yichong Leng",
        "Yuanjun Lv",
        "Zhou Zhao",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Metamath: Bootstrap your own mathematical questions for large language models",
      "authors": [
        "Longhui Yu",
        "Weisen Jiang",
        "Han Shi",
        "Jincheng Yu",
        "Zhengying Liu",
        "Yu Zhang",
        "James Kwok",
        "Zhenguo Li",
        "Adrian Weller",
        "Weiyang Liu"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations, ICLR 2024"
    },
    {
      "citation_id": "39",
      "title": "Mammoth: Building math generalist models through hybrid instruction tuning",
      "authors": [
        "Xiang Yue",
        "Xingwei Qu",
        "Ge Zhang",
        "Yao Fu",
        "Wenhao Huang",
        "Huan Sun",
        "Yu Su",
        "Wenhu Chen"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "40",
      "title": "Internalizing ASR with implicit chain of thought for efficient speech-to-speech conversational LLM",
      "authors": [
        "Robin Shing-Hei Yuen",
        "Timothy Tin-Long Tse",
        "Jian Zhu"
      ],
      "year": "2024",
      "venue": "Internalizing ASR with implicit chain of thought for efficient speech-to-speech conversational LLM"
    },
    {
      "citation_id": "41",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "Aohan Zeng",
        "Bin Xu",
        "Bowen Wang",
        "Chenhui Zhang",
        "Da Yin",
        "Diego Rojas",
        "Guanyu Feng",
        "Hanlin Zhao",
        "Hanyu Lai",
        "Hao Yu",
        "Hongning Wang",
        "Jiadai Sun",
        "Jiajie Zhang",
        "Jiale Cheng",
        "Jiayi Gui",
        "Jie Tang",
        "Jing Zhang",
        "Juanzi Li",
        "Lei Zhao",
        "Lindong Wu",
        "Lucen Zhong",
        "Mingdao Liu",
        "Minlie Huang",
        "Peng Zhang",
        "Qinkai Zheng",
        "Rui Lu",
        "Shuaiqi Duan",
        "Shudan Zhang",
        "Shulin Cao",
        "Shuxun Yang",
        "Weng Lam Tam",
        "Wenyi Zhao",
        "Xiao Liu",
        "Xiao Xia",
        "Xiaohan Zhang",
        "Xiaotao Gu",
        "Xin Lv",
        "Xinghan Liu",
        "Xinyi Liu",
        "Xinyue Yang",
        "Xixuan Song",
        "Xunkai Zhang",
        "Yifan An",
        "Yifan Xu",
        "Yilin Niu",
        "Yuantao Yang",
        "Yueyan Li",
        "Yushi Bai",
        "Yuxiao Dong",
        "Zehan Qi",
        "Zhaoyu Wang",
        "Zhen Yang",
        "Zhengxiao Du",
        "Zhenyu Hou",
        "Zihan Wang"
      ],
      "year": "2008",
      "venue": "Chatglm: A family of large language models from GLM-130B to GLM-4 all tools"
    },
    {
      "citation_id": "42",
      "title": "M3ED: multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "43",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Yifan Dong",
        "Chen Du",
        "Yushuo Yang",
        "Zhipeng Chen",
        "Jinhao Chen",
        "Ruiyang Jiang",
        "Yifan Ren",
        "Xinyu Li",
        "Zikang Tang",
        "Peiyu Liu",
        "Jian-Yun Liu",
        "Ji-Rong Nie",
        "Wen"
      ],
      "year": "2023",
      "venue": "A survey of large language models"
    },
    {
      "citation_id": "44",
      "title": "Seen and unseen emotional style transfer for voice conversion with A new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Deep investigation of intermediate representations in self-supervised learning models for speech emotion recognition",
      "authors": [
        "Zhi Zhu",
        "Yoshinao Sato"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2023 -Workshops"
    }
  ]
}