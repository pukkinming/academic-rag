{
  "paper_id": "2112.12380v1",
  "title": "> Replace This Line With Your Paper Identification Number (Double-Click Here To Edit",
  "published": "2021-12-23T06:35:07Z",
  "authors": [
    "Ensieh Khazaei",
    "Hoda Mohammadzade"
  ],
  "keywords": [
    "Emotion recognition",
    "Functional brain connectivity",
    "EEG signals"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG signals in emotion recognition absorb special attention owing to their high temporal resolution and their information about what happens in the brain. Different regions of brain work together to process information and meanwhile the activity of brain changes during the time. Therefore, the investigation of the connection between different brain areas and their temporal patterns plays an important role in neuroscience. In this study, we investigate the emotion classification performance using functional connectivity features in different frequency bands and compare them with the classification performance using differential entropy feature, which has been previously used for this task. Moreover, we investigate the effect of using different time periods on the classification performance. Our results on publicly available SEED dataset show that as time goes on, emotions become more stable and the classification accuracy increases. Among different time periods, we achieve the highest classification accuracy using the time period of 140s-end. In this time period, the accuracy is improved by 4 to 6% compared to using the entire signal. The mean accuracy of about 88% is obtained using any of the Pearson correlation coefficient, coherence and phase locking value features and SVM. Therefore, functional connectivity features lead to better classification accuracy than DE features (with the mean accuracy of 84.89%) using the proposed framework. Finally, in a relatively fair comparison, we show that using the best time interval and SVM, we achieve better accuracy than using Recurrent Neural Networks which need large amount of data and have high computational cost.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "MOTIONS play an important role in human decisions. Emotion recognition has numerous applications in the enhancement of human life and human ability  [1] . This interdisciplinary field plays a critical role in the development of Ensieh Khazaei and Hoda Mohammadzade are with Department of Electrical Engineering, Sharif University of Technology, Azadi Avenue, Tehran 11155-4363, Iran.\n\n(e-mail: ensieh.khazaei@ee.sharif.edu, hoda@sharif.edu). psychology, neuroscience, cognitive science, and computer science  [2] ,  [3] . In the field of computer science, automatic emotion recognition is used to improve human-computer interface  [4] ,  [5] . Other applications of emotion recognition include lie detection, behavior prediction and health monitoring. Among these various applications, humancomputer interface is particularly important  [3] ,  [6] . In this application, machines get the ability to understand the emotional states of people and so can interact better with them.\n\nThere are different approaches in emotion recognition which can be divided into two categories. The first category is based on non-physiological signals such as facial expressions, body movements and intonation  [3] ,  [4] ,  [5] ,  [6] ,  [7] . The second category is based on physiological signals such as electroencephalogram, electrocardiography, heart rate and respiration signals  [3] ,  [4] . Physiological signals provide more comprehensive and complex information and their results are more accurate  [3] ,  [4] . Among physiological signals, EEG signals receive more attention because they can better express brain states  [6] .\n\nThere are different models for expressing human emotions which can be generally divided into two categories: discrete basic emotions and continuous emotions. In the discrete model, emotions are classified into a set of discrete labels including six basic emotions: happiness, sadness, surprise, fear, anger, and disgust  [8] . In the continuous model, emotions are expressed using two dimensions of valence and arousal  [9] . Valence indicates how much an emotion is positive or negative, and arousal indicates how much a person is excited or indifferent  [6] ,  [10] .\n\nUp to now, a wide range of research has been done in the field of EEG-based emotion recognition. However, most recent research in the field of emotion recognition has focused on finding the strong connections, the important frequency bands and the important electrodes. Unfortunately, little research has been done to examine the significant time periods and the role of functional brain connectivity features in emotion classification which are the focus of this study. In this paper, we look for the optimal time period for emotion classification. Moreover, we examine the performance of recurrent neural networks to find temporal patterns in emotion classification.\n\nThe structure of this article is as follows. Section II gives a brief overview of related research on emotion recognition using EEG signals. Section III describes the dataset, preprocessing methods, feature extraction, and classification procedure. The results of recurrent neural networks and the optimal time period are given in Section IV. In Section V, we discuss our results and compare them against previous work. In Section VI, the conclusion is presented.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In recent years, due to the development of dry electrode techniques  [11] ,  [12] ,  [13] , EEG signals, possessing high temporal resolution, are frequently used for emotion recognition. In this section, we review some of the previous research in the field of emotion recognition using EEG signals.\n\nIn  [2] , differential entropy (DE), power spectral density (PSD), differential asymmetry (DASM), rational asymmetry (RASM) and differential causality (DCAU) in the five frequency bands, which are delta (1-3 Hz), theta (4-7 Hz), alpha (8-13 Hz), beta (14-30 Hz) and gamma  (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) , are extracted from the public SEED database [14] and then emotion classification is performed using deep neural networks. The results of this paper show that beta and gamma perform better than other frequency bands or in other words, there exist specific neural patterns in high frequency bands for positive, neutral, and negative emotions through time-frequency analysis. This paper also concludes that utilizing the electrodes that are located in the temporal area (FT7, FT8, T7, T8, C5, C6, TP7, TP8, CP5, CP6, P7, and P8) can increase classification accuracy by 2.66% compared to using all electrodes. In  [3] , by examining the DE and energy spectrum features extracted from their collected dataset, the authors infer that there is a close relationship between the emotional state of individuals and information of gamma band. In  [5] , a number of functional brain connectivity features like PLV, Pearson correlation coefficient and phase lag index (PLI)  [15]  are extracted and then the resulting connectivity matrices are fed to neural networks using different methods for spatial arrangement of electrodes. The purpose of this paper is to investigate the effect of spatial information in emotion classification. In  [5] , the arrangement of electrodes in the connectivity matrix is investigated and it is discovered to be effective in classification accuracy. In  [16] , the graph of PLV is obtained and then four features are extracted from the graph.\n\nThe authors then examine the simultaneous use of four graph features and local features such as DE and PSD.\n\nThey observed that adding graph features to local features leads to an increase in classification accuracy. Furthermore, this paper analyzes different frequency bands from which the PLV graph features are extracted, showing that utilizing the gamma and beta bands results in better performance compared to other frequency bands. In  [17] , the effect of familiar stimuli on emotion classification is investigated. The stimulus was a selected collection of music, and participants had education in music. Each participant listened to 8 familiar music and 8 unfamiliar music. Finally, results show that the classification accuracy is higher for unfamiliar music. In  [18] , the stable patterns of EEG signal over time in emotion recognition are studied. The results show that a) the lateral temporal areas are more active in positive emotions than negative ones in beta and gamma bands. b) Brain activity in a neutral state has a greater alpha response in parietal and occipital regions. c) Negative emotions are more active in delta band in the parietal and occipital regions. d) Activity of gamma band in the prefrontal region is more in negative emotions. Although a lot of works have been done on emotion recognition using EEG signals, a small number of them studied functional brain connectivity and the importance of different time periods during the stimuli which are the focus of this paper. The framework of our proposed approach is depicted in Fig.  1 . First of all, we processed raw EEG signals and extracted some features from the preprocessed signal. Then a feature selection method was employed to decrease the number of features and select the most informative features. Finally, we did temporal analysis and recognized the emotion using the obtained information in temporal analysis step. All of these steps will be explained in details in Section III.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Acquisition",
      "text": "We use the public SEED database [14] in this study. This database was recorded from 15 participants (7 males and 8 females, age range: 19-28 years). The stimuli in this database are 15 Chinese clips that create three emotions of positive, negative and neutral in participants; there are five clips for each emotion. The criteria for video selection were as follows: 1) the Fig.  1 . The framework of our proposed approach clips should not be too long to tire the participants, 2) the meaning of the videos can be understood without any explanation, 3) all the moments of the video convey only one emotion to the subject. The videos are approximately 4 minutes. The EEG signal of the participants was recorded using 62 electrodes placed on the head according to the 10-20 system  [18] . EOG signal was recorded to remove the eye movement artifacts  [2] . The location of the electrodes is shown in Fig.  2 . Fig.  2 . Location of electrodes in SEED database  [2]  The sampling rate of EEG recording was 1000 Hz. The signals were recorded in three sessions, where the time interval between two consecutive sessions was at least 7 days. The stimuli were the same in all three sessions; three sessions were recorded from each subject, and in each session, the participant watched all 15 video clips.  The participants were alerted 5s before watching the video. After watching each video, an assessment form was given to the participant to express their emotions in 45s. 15s after the assessment was intended for rest  [18] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Preprocessing",
      "text": "The raw EEG signals are not appropriate for feature extraction and a series of preprocessing steps are needed beforehand. The raw EEG signals are down-sampled to 200 Hz. The EEG signals are visually checked in order to remove the parts of the signal that are heavily contaminated with EMG and EOG  [2] . The EOG signals recorded during the experiment are also used to detect blink artifacts  [19] . EEG signal are processed with a bandpass filter with frequency band between 0.3 Hz and 50 Hz  [2] . Finally, the signals are divided into delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-31 Hz) and gamma (31-50 Hz) frequency bands and then in the time domain, the signal of each band is divided into time windows of 2s without overlapping. Time windowing is helpful to use the stationarity assumption in each time window because when the length of a time series is shorter, this assumption is more valid. Finally, features will be extracted from each of the 2s windows.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Feature Extraction",
      "text": "In previous studies, various features such as DE, PSD, DASM, RASM, etc. have been used in emotion recognition. These features represent the local activity of electrodes and do not take into account the relationship between the electrodes. Among the local features, DE has the best performance in emotion classification  [2] ,  [6] ,  [18] . Functional brain connectivity features show the relationship between different brain regions' activities which are recorded by different electrodes. Pearson correlation coefficient, coherence and PLV which are three important connectivity features are extracted in this study. These three features are non-directed and as a result their connectivity matrix is symmetric  [20] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1) Pearson Correlation Coefficient",
      "text": "Pearson correlation coefficient is a simple criterion of linear correlation between two time series. This criterion calculates the linear relationship between the two signals x and y as follows:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Coh F C F =",
      "text": "The cross-spectral density function of the signals x and y is normalized using the spectral density function of signal x and signal y . The value of coherence is between zero and +1. The value of +1 means the highest correlation at that frequency. Coherence with the value of zero means independence at that frequency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3) Phase Locking Value",
      "text": "The most common feature for measuring phase synchronization between different areas of the brain is Phase Locking Value (PLV). Suppose two signals 1\n\nx and 2\n\nx are filtered with a bandpass filter. An analytical signal i z can be defined as\n\nwhere H is the Hilbert transform operator. The phase difference is calculated using\n\nPLV is then defined as  [16] :\n\nPLV is between zero and one. The value of one indicates a phase lock, and the value of zero indicates a random phase distribution over time. The amount of PLV is independent of the signal amplitude and depends only on the phase difference between two signals  [22] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Classification",
      "text": "Most of the works that have used the SEED database have considered the first 9 trials of each session as training data and the final 6 trials of each session as test data. Due to the small number of trials in this database, it is better to use Leave-One-Out cross validation strategy. Leave-One-Out cross validation strategy decreases the probability of overfitting and has a higher degree of validity. In this strategy, we consider one trial as test and the other 14 trials as training. We repeat this procedure until every trial is considered once as test. Finally, we average the classification accuracies over all folds. We use SVM with linear kernel for classification and also the emotion classification is subject dependent. The number of samples is equal to number of time windows * number of trials * number of sessions. The number of time windows depends on the length of time windows. In this study, the length of time window is 2s, and time windows are non-overlapping. Given that our final purpose is to label the trials while samples are time windows, we use a simple voting scheme to determine the label of a trial using the label of its time windows ; for instance, if there are n time windows in a trial, we will have a vector of length n of labels and then, we vote among all the labels in the vector to find the label of the trial.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Dimension Reduction",
      "text": "It is known that an imbalance between the length of the feature vector and the number of samples can cause overfitting. The length of feature vector for local features such as DE is equal to the number of channels which is equal to 62 in SEED database. Coherence, PLV, and Pearson correlation coefficient are non-directed, and their connectivity matrix is symmetric. Therefore, the upper or lower half of connectivity matrix is sufficient for classification. As a result, the length of the vector of connectivity features is equal to (62*61)/2=1891, which is large relative to the number of samples which is approximately 4700. Thus, we should use dimension reduction techniques to prevent overfitting. In this study, we use Fisher score to reduce the dimension. The Fisher score assigns a score to each feature where higher score for a feature shows that the feature is more discriminative for classification. The score of a feature is obtained as follows  [23] :",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F. Recurrent Neural Networks",
      "text": "A recurrent neural network (RNN) is a type of neural networks in which connections between nodes form a graph along a sequence  [25] . RNNs  [26]  are widely used in text analysis  [27] ,  [28] , text generation  [29] ,  [30] , speech recognition  [31] , time series prediction  [32]  and processing of time signals such as electroencephalogram  [33] . The main idea of RNNs is the use of hidden states which are used as the network memory and are responsible to store the information from past inputs. In RNNs, the output  These networks need a large number of samples for training. The number of our samples in SEED database is small. Therefore, we select Gated Recurrent Units (GRUs)  [34]  which are a cell type of RNNs and their few parameters make them a suitable choice for small datasets  [35] . In addition, we increase the number of samples by adding Gaussian noise with standard deviations of 0.001, 0.004, 0.008 and 0.012 to the samples according to  [4] . The utilized network consists of a GRU layer that has 16 units and a fully connected layer for emotion classification. The filtered signal is then divided into time sliding windows of 180s with step length of 2 seconds as the input of network. We choose 180s as the length of our inputs because the signals in SEED database have different length but their minimum length of signals is 180s. We also opt step length of 2s to cover the end part of signals and also increase the number of inputs of the network. Leave-One-Out cross validation strategy is chosen for calculating accuracy in this section.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Investigation Of Frequency Bands",
      "text": "Previous research in the field of emotion recognition has shown that higher frequency bands have more information about emotional states and gamma band has the best performance in the emotion classification  [2] ,  [3] ,  [6] ,  [16] ,  [18] ,  [24] . However, most of them do not investigate the performance of different frequency bands for functional brain connectivity features. In this research, in addition to DE features, we investigate the performance of different frequency bands for coherence, PLV and Pearson correlation coefficient. Table  I  shows the classification accuracy for our features in different frequency bands before dimension reduction. As seen in Table  I , the gamma and beta bands have better classification accuracy rather than other bands. Specifically, DE achieves its maximum accuracy in beta band while functional brain connectivity features perform best in gamma band. The classification accuracy of DE feature in gamma band is 0.88% less than beta band while in functional connectivity features, the accuracy of gamma band is at least 0.89% more than beta band. Moreover, our focus in this research is on functional brain connectivity features, so we select gamma band for further analysis. We also do dimension reduction for all features only on the gamma band.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Dimension Reduction Results",
      "text": "We apply the Fisher score for DE, Pearson correlation coefficient, coherence and PLV features in gamma band because in Section IV, we investigate the performance of different frequency bands and we observe that the gamma band has better performance in emotion recognition. We calculate Fscore for all features and then sort them in order to find the first 100, 200, 300, 400, 500, 600, 700, 800, 1200 selected feature for coherence, PLV and Pearson correlation coefficient. We also calculate F-score for DE features and then sort them in order to find the first 10, 20, 30, 40, 50 selected features for DE. The optimal number of selected features for DE, Pearson correlation coefficient, coherence and PLV are 40, 700, 200 and 400, respectively. Classification accuracy versus the number of functional connectivity features and DE are shown in Fig.  4  and Fig.  5 , respectively. Fig.  4  and Fig.  5  show the importance of feature selection; If the number of selected features is little, then the features will not be discriminative and the classification accuracy will be low. On the other hand, if the number of features is too large, the accuracy of classification is reduced because overfitting happens. Table  II  shows the classification accuracy before and after performing Fisher on our features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Emotion Classification Using Fusion Of De And Functional Brain Connectivity",
      "text": "According to our results in the previous part and a wide range of research in the field of emotion recognition, we carry out our experiments only in gamma band. It is worth mentioning that from this part to the end of this study, we utilize all features in gamma band after dimension reduction for our analysis. In this part, we investigate the effect of the fusion of functional connectivity and DE features on the classification accuracy in emotion recognition. Table  III  shows classification accuracy using decision-level fusion of functional connectivity and DE features. The classification accuracy using the DE feature in gamma band is equal to 80.90% with 16.87% std. Table  III  shows that the fusion of functional connectivity and DE features improves the classification accuracy. However, because of our focus on temporal pattern of functional connectivity features, we do not fuse the functional connectivity and DE features in the rest of this article to purely observe the temporal pattern of functional brain connectivity features. It is also worth mentioning that we investigated the fusion of different connectivity features together but no improvement was observed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Analysis Of Temporal Pattern",
      "text": "Recent works mostly investigated location and frequency of brain activity during emotional stimuli. To the best of the authors' knowledge, brain activity during an emotional stimulation interval has not been yet studied in the literature. The length of stimulus in SEED database is approximately 4 minutes. In this section, we want to find temporal pattern over different time periods during each trial. To this end, we consider one minute sliding window with step length of 20 seconds and calculate classification accuracy in each sliding window. Table  IV  shows the classification accuracy in different time periods of signals.\n\nAs seen in Table  IV , there is a temporal pattern in the classification accuracy in which, as time goes on, the mean accuracy increases. The best time period is 140s-end in which the mean accuracy is maximum and standard deviation is minimum. Comparing the results of Table  III  and Table  IV  shows that the mean accuracy in 140s-end interval is increased compared to the entire signal by 3.99%, 4%, 6.22% and 5.77% for DE, coherence, PLV and Pearson correlation coefficient, respectively. Standard deviation in 140s-end interval is also decreased compared to the entire signal.\n\nIt is noteworthy that the length of stimuli in many datasets for EEG-based emotion recognition such as DEAP  [36]  are less than two minutes while the temporal analysis of SEED shows the importance of brain activities that occur after two minutes from the onset of stimuli. Therefore, we conclude that the appropriate length of stimuli in emotion recognition is about 3 to 4 minutes so that not only the subjects do not get tired but also there are the best periods for emotion recognition. In this study, we watched the videos and confirmed that all part of the videos express the same emotion as the label of the video. Although we can understand the label of the video from the beginning part of it, over time the emotion becomes more stable in our mind, and consequently the final intervals have better performance in emotion classification. We can also observe this phenomenon in our ordinary life, for instance, when a person tells a funny sentence, you feel happy and if that person tells a sequence of funny sentences, you feel happier and even a few minutes after telling those funny sentences, you still feel happy. Therefore, it seems that the length of stimuli in emotion recognition has an optimal interval at which the subject reaches the peak of his emotions. If the length of stimuli is less than the optimal value then, the subject's emotions are not well expressed, and if the length of stimuli is greater than the optimal length, the subjects get tired.  The results shown in Table  IV  are the averaged classification accuracies over different subjects. Another approach is to obtain the best interval for each subject separately. By using the second approach, we found that the best interval is not the same for all subjects, however, the best interval is 140s-end for 13, 11, 10 and 11 subjects (out of 15 subjects) in DE, coherence, PLV and Pearson correlation coefficient features, respectively. The best interval for other subjects occurs 120s, 100s, 80s and 100s after the onset of stimulation for DE, coherence, PLV and Pearson correlation coefficient features, respectively. To visualize the results of the temporal analysis, the accuracy using each time interval is shown in Fig.  6 , separately for each feature. In this figure, squares with higher intensity are indicators of better classification accuracy. Table  V  shows the comparison between results of these two approaches: 1) averaging between subjects and then finding the best interval (first row of Table  V ), 2) finding the best interval for each subject and then averaging between the maximum accuracy of each subject (second row of Table  V ). The results of Table  V  show that the average accuracy in the second approach compared to that in the first approach increases by 1.33% to 4%. Therefore, regarding to different character and reaction of different subjects, it is a wise idea to find the best time interval for each subject independently and then predict the subject's emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Classification Using Recurrent Neural Network",
      "text": "In the previous section, we found the best time interval using SVM. In this section, we investigate the performance of recurrent neural networks  [25]  in finding the temporal pattern of brain activity during emotional stimulation. In order to provide a fair comparison, we augment the training set by adding Gaussian noise to the training samples and then adding them to the training set. We also employ GRUs with few parameters for compatibility with the size of the dataset. Data augmentation and reduction of the number of the parameters of the network prevent overfitting of the classifier to train samples. Table VI compares the results of the recurrent neural network with the results of the previous section obtained using SVM. The results of Table  VI  show that the performance of SVM is better than recurrent neural networks for the utilized dataset in this study. In addition, neural networks require large amounts of data and their computational cost is high. Therefore, the proposed time-interval selection method provides both advantages of relatively high accuracy and low computational cost for the task of emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Discussion",
      "text": "In this study, our aim was to examine the functional connectivity features in more detail as well as the temporal analysis of the brain's response to emotional stimuli, which have been rarely studied in the field of emotion recognition. In this section, we compare our results with previous works. However, it is difficult to compare the classification accuracy of our method with previous works because they have used different datasets with different number of subjects and various methods of representing emotions. Moreover, even in the works that use SEED dataset for their experiments, the first 9 trials are considered as training and the final 6 trials as test data. As a result, due to the small size of this dataset, the accuracy reported by those methods may not be much reliable. In order to obtain more reliable results, we used the Leave-One-Out strategy on the trials to calculate the classification accuracy for each subject. Consequently, our classification accuracy might be lower than some of those reported in the literature due to our different strategy for constructing train and test sets but it is more valid. Therefore, in this comparison, it is important to note that firstly, we used a more reliable method to measure the classification accuracy, and secondly, our goal was to find optimal time intervals and analyze temporal patterns, but not to propose a method to increase the classification accuracy in emotion recognition.\n\nWe summarize the accuracy rates of emotion classification in previous works in Table  VII . In  [2] ,  [3] ,  [6] , and  [18] , frequency analysis for some features like DE and PSD have been investigated, but the functional connectivity features have not been investigated except in  [16] . In  [16] , it has been shown that using the gamma and beta bands results in better performance compared to using the other frequency bands for emotion classification. As discussed in Section IV.A, the results of our frequency analysis on the functional connectivity features using SEED database confirm the results of previous articles. It can be concluded that these two frequency bands contain more discriminative information related to the emotional states of individuals' brain.\n\nRegarding temporal analysis, unfortunately, none of the previous works has studied the temporal patterns during stimuli, but we examined this for the first time. We observed that each subject has an optimal time interval in which the classification performance is higher than the accuracy of using the whole signal. It is noteworthy that, as discussed in Section IV.D, the optimal time interval for all subjects in this database occurs 80 seconds after the onset of stimuli. Therefore, the different subjects behave relatively similar to each other.\n\nUsing DE features extracted from the gamma band we obtained the mean accuracy of 84.89%. More details regarding the comparison between our result and the previous works are as follow. In  [2] , the accuracy of 79.19% using DE features extracted from the gamma band and Deep Belief Networks (DBN) as the classifier has been achieved. In  [18] , the mean accuracy of 91.07% using the concatenation of DE from all frequency bands and Graph regularized Learning Machine (GELM) has been obtained. Despite the good accuracy obtained in  [18] , the computational complexity of GELM is higher than SVM which is utilized in our study. In  [3] , the accuracy of 84.25% using DE features from the gamma band has been achieved. In  [3] , the authors collected their own dataset which contains only two positive and negative emotional states, while in our research, the number of classes is three. In  [6] , the average accuracy of 83.36% using DE features from the gamma band and Graph Convolutional Neural Networks (GCNNs) has been obtained on SEED dataset. Despite of the good performance of Graph Neural Networks, their computational cost is higher than SVM which is used in our work. In  [4] , the maximum accuracy of 75% has been reached using DE features and ResNet  [37]  on SEED dataset. Due to the small size of SEED dataset, in  [4]  first data augmentation methods are used to provide sufficient data for training of the network.\n\nUsing functional connectivity features, we have achieved the mean accuracy of 88%, 88.44%, and 88.44% for coherence, PLV, and Pearson correlation coefficients, respectively. More details regarding the comparison between our results and the previous works are as follow. In  [5] , the accuracy rates of 93.80% and 96.62% using a two-layer Convolutional Neural Network (CNN) as the classifier and Pearson correlation coefficients and PLV matrices as features have been achieved, respectively. In  [5] , only the results of high and low valence classification are reported, which are on DEAP dataset. They have not reported their accuracy No results on the arousal classification have been reported. In  [16] , 33% accuracy on SEED dataset using SVM classifier and ENP features extracted from the gamma band using PLV graph has been achieved. In  [38] , first of all, the Pearson correlation coefficients and coherence were extracted then three topological features of strength, clustering coefficient, and eigenvector centrality were calculated for each connectivity indices. By using the featurelevel fusion method, these three topological features were concatenated. Finally, they obtained the accuracy of 79.16% and 78.15% on Pearson correlation coefficients and coherence indices.\n\nIt can be seen that although there are a few works with better accuracy rates on SEED dataset, the accuracy that we have obtained using the optimal time intervals and a simple SVM classifier is competitive with the results of previous works while it has less computational cost.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we examined the applicability of functional brain connectivity for the emotion recognition task. Our results indicate that using functional connectivity features better emotion classification accuracy can be obtained compared to using DE features. In order to study the temporal variation of different features in terms of classification accuracy, we used a one-minute sliding window on the signals. Classification accuracy increased with sliding window progress leading us to the conclusion that using the 140s-end interval results in the best performance compared to using other intervals as well as compared to using the entire signal in SEED dataset. The mean accuracy and standard deviation using this interval (140s-end) for DE, PLV, coherence and Pearson correlation coefficient are 84.89%/13.20%, 88.44%/10.53%, 88%/13.38% and 88.44%/12.20%, respectively, showing 4-6% improvement compared to using the entire signal. Although the temporal behavior of each subject is different, the selected interval (140send) is the best period for at least two thirds of the subjects. The results show that by using the best time interval, we can achieve high accuracy by relatively low computational cost and limited number of training samples.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of our proposed approach",
      "page": 2
    },
    {
      "caption": "Figure 2: Fig.  2.  Location of electrodes in SEED database [2]",
      "page": 3
    },
    {
      "caption": "Figure 3: Protocol of EEG recording in each session",
      "page": 3
    },
    {
      "caption": "Figure 5: , respectively. Fig. 4 and Fig. 5 show the importance of",
      "page": 5
    },
    {
      "caption": "Figure 4: Average accuracy of classification over different subjects versus the",
      "page": 5
    },
    {
      "caption": "Figure 5: Average accuracy of classification over different subjects versus the",
      "page": 6
    },
    {
      "caption": "Figure 6: , separately for each feature.",
      "page": 7
    },
    {
      "caption": "Figure 6: Visualization of the emotion classificationâ€™s accuracy for each subject",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DE": "80.44/16.03",
          "Coh": "79.56/17.54",
          "PLV": "76.45/17.97",
          "Pearson": "78.22/18.25"
        },
        {
          "DE": "80.90/16.87",
          "Coh": "84.00/15.49",
          "PLV": "82.22/16.65",
          "Pearson": "82.67/14.21"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Delta": "64.44/18.97",
          "Theta": "58.67/15.97",
          "Alpha": "65.33/18.72",
          "Beta": "81.32/16.17",
          "Gamma": "80.44/16.03"
        },
        {
          "Delta": "40.44/17.36",
          "Theta": "51.11/21.03",
          "Alpha": "66.67/19.52",
          "Beta": "77.78/18.97",
          "Gamma": "79.56/17.54"
        },
        {
          "Delta": "26.67/17.81",
          "Theta": "40.89/21.21",
          "Alpha": "60.44/22.17",
          "Beta": "75.56/18.11",
          "Gamma": "76.45/17.97"
        },
        {
          "Delta": "35.11/10.83",
          "Theta": "49.78/19.00",
          "Alpha": "65.78/19.97",
          "Beta": "75.56/17.02",
          "Gamma": "78.22/18.25"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 0: 10 20 30 40 50 60 70",
      "data": [
        {
          "Coh": "84.00 / 15.49",
          "PLV": "82.22 / 16.65",
          "Pearson": "82.67 / 14.21"
        },
        {
          "Coh": "84.00 / 15.28",
          "PLV": "83.11 / 13.58",
          "Pearson": "84.44 / 13.72"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 0: 10 20 30 40 50 60 70",
      "data": [
        {
          "0-60s": "65.33 / 20.65",
          "20-80s": "64.89 / 18.07",
          "40-100s": "69.78 / 17.43",
          "60-120s": "72.89 / 16.61",
          "80-140s": "77.33 / 16.09",
          "100-160s": "81.33 / 14.29",
          "120-180s": "81.78 / 11.39",
          "140s-end": "84.89 / 13.20"
        },
        {
          "0-60s": "61.33 / 22.70",
          "20-80s": "69.33 / 18.82",
          "40-100s": "74.22 / 19.00",
          "60-120s": "77.78 / 15.46",
          "80-140s": "82.67 / 13.75",
          "100-160s": "86.67 / 15.11",
          "120-180s": "86.22 / 15.82",
          "140s-end": "88.00 / 13.38"
        },
        {
          "0-60s": "59.56 / 22.17",
          "20-80s": "68.44 / 19.75",
          "40-100s": "72.00 / 19.38",
          "60-120s": "76.00 / 16.67",
          "80-140s": "81.33 / 16.36",
          "100-160s": "85.33 / 16.56",
          "120-180s": "85.33 / 13.84",
          "140s-end": "88.44 / 10.53"
        },
        {
          "0-60s": "60.44 / 20.84",
          "20-80s": "69.78 / 19.00",
          "40-100s": "72.89 / 18.59",
          "60-120s": "77.33 / 15.69",
          "80-140s": "81.78 / 166.22",
          "100-160s": "86.22 / 15.82",
          "120-180s": "87.11 / 13.20",
          "140s-end": "88.44 / 12.20"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Study": "Zheng et al [2]",
          "Year": "2015",
          "Data": "SEED",
          "Features": "DE",
          "Frequency band": "Gamma band",
          "Classifier": "DBN",
          "Number of \nclasses": "3",
          "Description": "",
          "Accuracy": "79.19%"
        },
        {
          "Study": "Duan et al [3]",
          "Year": "2013",
          "Data": "Collected",
          "Features": "DE",
          "Frequency band": "Gamma band",
          "Classifier": "SVM",
          "Number of \nclasses": "2: positive and \nnegative \nemotions",
          "Description": "",
          "Accuracy": "84.25%"
        },
        {
          "Study": "Wang et al [4]",
          "Year": "2018",
          "Data": "SEED",
          "Features": "DE",
          "Frequency band": "Concatenation of all \nfrequency bands",
          "Classifier": "ResNet",
          "Number of \nclasses": "3",
          "Description": "-",
          "Accuracy": "75%"
        },
        {
          "Study": "Zheng et al [18]",
          "Year": "2019",
          "Data": "SEED",
          "Features": "DE",
          "Frequency band": "Concatenation of all \nfrequency bands",
          "Classifier": "GELM",
          "Number of \nclasses": "3",
          "Description": "-",
          "Accuracy": "91.07%"
        },
        {
          "Study": "Song et al [6]",
          "Year": "2020",
          "Data": "SEED",
          "Features": "DE",
          "Frequency band": "Gamma band",
          "Classifier": "GCNN",
          "Number of \nclasses": "3",
          "Description": "-",
          "Accuracy": "83.36%"
        },
        {
          "Study": "Our method",
          "Year": "-",
          "Data": "SEED",
          "Features": "DE",
          "Frequency band": "Gamma band",
          "Classifier": "SVM",
          "Number of \nclasses": "3",
          "Description": "-",
          "Accuracy": "84.89 %"
        },
        {
          "Study": "Moon et al [5]",
          "Year": "2018",
          "Data": "DEAP",
          "Features": "PLV, PCC",
          "Frequency band": "i\nConcatenation of all \nfrequency bands",
          "Classifier": "CNN (two-\nlayer)",
          "Number of \nclasses": "2: low and high \nvalence",
          "Description": "",
          "Accuracy": "96.62%, \n93.80%"
        },
        {
          "Study": "Li et al [16]",
          "Year": "2019",
          "Data": "SEED",
          "Features": "ENP (extracted \nfrom PLV graph)",
          "Frequency band": "Gamma band",
          "Classifier": "SVM",
          "Number of \nclasses": "3",
          "Description": "-",
          "Accuracy": "33% *"
        },
        {
          "Study": "Wu et al [38]",
          "Year": "2019",
          "Data": "SEED",
          "Features": "Three topological \nfeatures (for PCC \nand Coh indices)",
          "Frequency band": "Concatenation of all \nfrequency bands",
          "Classifier": "SVM",
          "Number of \nclasses": "3",
          "Description": "Feature-level fusion \nmethod was used to \nfuse topological \nfeatures",
          "Accuracy": "79.16%, \n78.15%"
        },
        {
          "Study": "Our method",
          "Year": "-",
          "Data": "SEED",
          "Features": "Coh, PLV, \nPearson",
          "Frequency band": "Gamma band",
          "Classifier": "SVM",
          "Number of \nclasses": "3",
          "Description": "-",
          "Accuracy": "88.00%, \n88.44%, \n88.44%"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recognition of emotions using multichannel EEG data and DBN-GC-based ensemble deep learning framework",
      "authors": [
        "H Chao",
        "H Zhi",
        "L Dong",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "2",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Ment. Dev"
    },
    {
      "citation_id": "3",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "4",
      "title": "Data augmentation for EEG-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "F Wang",
        "S.-H Zhong",
        "J Peng",
        "J Jiang",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "MultiMedia Modeling",
      "doi": "10.1007/978-3-319-73600-6_8"
    },
    {
      "citation_id": "5",
      "title": "Convolutional neural network approach for EEG-based emotion recognition using brain connectivity and its spatial information",
      "authors": [
        "S.-E Moon",
        "S Jang",
        "J.-S Lee"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "EEG-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "8",
      "title": "Ubiquitous emotion-aware computing",
      "authors": [
        "E Van Den Broek"
      ],
      "year": "2013",
      "venue": "Pers. Ubiquitous Comput",
      "doi": "10.1007/s00779-011-0479-9"
    },
    {
      "citation_id": "9",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Curr. Psychol",
      "doi": "10.1007/BF02686918"
    },
    {
      "citation_id": "10",
      "title": "The dynamic emotion recognition system based on functional connectivity of brain regions",
      "authors": [
        "R Khosrowabadi",
        "M Heijnen",
        "A Wahab",
        "H Quek"
      ],
      "year": "2010",
      "venue": "2010 IEEE Intelligent Vehicles Symposium"
    },
    {
      "citation_id": "11",
      "title": "Dry and noncontact EEG sensors for mobile braincomputer interfaces",
      "authors": [
        "Y Chi",
        "Y.-T Wang",
        "Y Wang",
        "C Maier",
        "T.-P Jung",
        "G Cauwenberghs"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng"
    },
    {
      "citation_id": "12",
      "title": "PDMS-based low cost flexible dry electrode for long-term EEG measurement",
      "authors": [
        "L.-F Wang",
        "J.-Q Liu",
        "B Yang",
        "C.-S Yang"
      ],
      "year": "2012",
      "venue": "IEEE Sens. J"
    },
    {
      "citation_id": "13",
      "title": "Novel active comb-shaped dry electrode for EEG measurement in hairy site",
      "authors": [
        "Y.-J Huang",
        "C.-Y Wu",
        "-K Wong",
        "B.-S Lin"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "14",
      "title": "Phase lag index: assessment of functional connectivity from multi channel EEG and MEG with diminished bias from common sources",
      "authors": [
        "C Stam",
        "G Nolte",
        "A Daffertshofer"
      ],
      "year": "2007",
      "venue": "Hum. Brain Mapp"
    },
    {
      "citation_id": "15",
      "title": "EEG based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "16",
      "title": "Familiarity effects in EEG-based emotion recognition",
      "authors": [
        "N Thammasan",
        "K Moriyama",
        "K.-I Fukui",
        "M Numao"
      ],
      "year": "2017",
      "venue": "Brain Inform"
    },
    {
      "citation_id": "17",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "18",
      "title": "A removal of eye movement and blink artifacts from EEG data using Morphological Component Analysis",
      "authors": [
        "B Singh",
        "H Wagatsuma"
      ],
      "year": "2017",
      "venue": "Comput. Math. Methods Med"
    },
    {
      "citation_id": "19",
      "title": "A tutorial review of functional connectivity analysis methods and their interpretational pitfalls",
      "authors": [
        "A Bastos",
        "J.-M Schoffelen"
      ],
      "year": "2015",
      "venue": "Front. Syst. Neurosci",
      "doi": "10.3389/fnsys.2015.00175/full"
    },
    {
      "citation_id": "20",
      "title": "Review of advanced techniques for the estimation of brain connectivity measured with EEG/MEG",
      "authors": [
        "V Sakkalis"
      ],
      "year": "2011",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "21",
      "title": "Measuring phase synchrony in brain signals",
      "authors": [
        "J Lachaux",
        "E Rodriguez",
        "J Martinerie",
        "F Varela"
      ],
      "year": "1999",
      "venue": "Hum. Brain Mapp"
    },
    {
      "citation_id": "22",
      "title": "Combining SVMs with various feature selection strategies",
      "authors": [
        "Y.-W Chen",
        "C.-J Lin"
      ],
      "year": "2008",
      "venue": "Feature Extraction",
      "doi": "10.1007/978-3-540-35488-8_13"
    },
    {
      "citation_id": "23",
      "title": "Exploring EEG features in cross-subject emotion recognition",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "Y Zhang",
        "Y Hou",
        "B Hu"
      ],
      "year": "2018",
      "venue": "Front. Neurosci",
      "doi": "10.3389/fnins.2018.00162/full"
    },
    {
      "citation_id": "24",
      "title": "A critical review of recurrent neural networks for sequence learning",
      "authors": [
        "Z Lipton",
        "J Berkowitz",
        "C Elkan"
      ],
      "year": "2015",
      "venue": "A critical review of recurrent neural networks for sequence learning"
    },
    {
      "citation_id": "25",
      "title": "Recurrent Neural Networks: Design and Applications, 1st ed",
      "authors": [
        "L Jain",
        "L Medsker"
      ],
      "year": "1999",
      "venue": "Recurrent Neural Networks: Design and Applications, 1st ed"
    },
    {
      "citation_id": "26",
      "title": "Entity recognition from clinical texts via recurrent neural network",
      "authors": [
        "Z Liu"
      ],
      "year": "2017",
      "venue": "BMC Med. Inform. Decis. Mak"
    },
    {
      "citation_id": "27",
      "title": "Handwritten Chinese text recognition using separable multi-dimensional recurrent neural network",
      "authors": [
        "Y.-C Wu",
        "F Yin",
        "Z Chen",
        "C.-L Liu"
      ],
      "year": "2017",
      "venue": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
    },
    {
      "citation_id": "28",
      "title": "Generating text with recurrent neural network",
      "authors": [
        "I Sutskever",
        "J Martens",
        "G Hinton"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning (ICML'11)"
    },
    {
      "citation_id": "29",
      "title": "RNN-stega: Linguistic steganography based on recurrent neural networks",
      "authors": [
        "Z.-L Yang",
        "X.-Q Guo",
        "Z.-M Chen",
        "Y.-F Huang",
        "Y.-J Zhang"
      ],
      "year": "2019",
      "venue": "IEEE trans. inf. forensics secur"
    },
    {
      "citation_id": "30",
      "title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition",
      "authors": [
        "L Lu",
        "X Zhang",
        "S Renais"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Robust online time series prediction with recurrent neural networks",
      "authors": [
        "T Guo",
        "Z Xu",
        "X Yao",
        "H Chen",
        "K Aberer",
        "K Funaya"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)"
    },
    {
      "citation_id": "32",
      "title": "Deep recurrent neural network for seizure detection",
      "authors": [
        "L Vidyaratne",
        "A Glandon",
        "M Alam",
        "K Iftekharuddin"
      ],
      "year": "2016",
      "venue": "2016 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "33",
      "title": "Gate-variants of Gated Recurrent Unit (GRU) neural networks",
      "authors": [
        "R Dey",
        "F Salem"
      ],
      "year": "2017",
      "venue": "2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)"
    },
    {
      "citation_id": "34",
      "title": "Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?",
      "authors": [
        "N Gruber",
        "A Jockisch"
      ],
      "year": "2020",
      "venue": "Front Artif Intell",
      "doi": "10.3389/frai.2020.00040/full#:~:text=85%20vs.%20.,learn%20better%20high%20prevalent%20content."
    },
    {
      "citation_id": "35",
      "title": "DEAP: A dataset for emotion analysis using physiological and audiovisual signals",
      "authors": [
        "S Koelstra"
      ],
      "venue": "Qmul.ac.uk"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "Accessed"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "37",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "38",
      "title": "Identifying functional brain connectivity patterns for EEG-based emotion recognition",
      "authors": [
        "X Wu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Engineering"
    }
  ]
}