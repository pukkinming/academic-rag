{
  "paper_id": "2001.00191v1",
  "title": "Ensemble Emotion Recognizing With Multiple Modal Physiological Signals",
  "published": "2020-01-01T11:44:43Z",
  "authors": [
    "Jing Zhang",
    "Yong Zhang",
    "Suhua Zhan",
    "Cheng Cheng"
  ],
  "keywords": [
    "Emotion recognition",
    "physiological signal processing",
    "ensemble classifier"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Physiological signals that provide the objective repression of human affective states are attracted increasing attention in the emotion recognition field. However, the single signal is difficult to obtain completely and accurately description for emotion. Multiple physiological signals fusing models, building the uniform classification model by means of consistent and complementary information from different emotions to improve recognition performance. Original fusing models usually choose the particular classification method to recognition, which is ignoring different distribution of multiple signals. Aiming above problems, in this work, we propose an emotion classification model through multiple modal physiological signals for different emotions. Features are extracted from EEG, EMG, EOG signals for characterizing emotional state on valence and arousal levels. For characterization, four bands filtering theta, beta, alpha, gamma for signal preprocessing are adopted and three Hjorth parameters are computing as features. To improve classification performance, an ensemble classifier is built. Experiments are conducted on the benchmark DEAP datasets. For the two-class task, the best result on arousal is 94.42%, the best result on valence is 94.02%, respectively. For the four-class task, the highest average classification accuracy is 90.74, and it shows good stability. The influence of different peripheral physiological signals for results is also analyzed in this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is a very important part to each of us, affecting our daily life such as work efficiency, learning status, social contact and decision making. Recognizing emotions through speech  [1] , facial expressions  [2]  and gestures  [3]  are very significant components of research works in this field. However, all of these can be artificially changed or controlled, sometimes the results from them dont really reflect persons emotion status. Hence, more and more researchers pay attention to conduct emotion recognition by processing physiological signals, which are more objective. Due to the recent advancement in building wireless and wearable sensors  [4] , physiological signals are relatively easy to acquire without damage to body. Human body is a combination of various organs, for example heart, brain, muscles and eyes. Some organs produce electrical signals, such as the electroencephalogram (EEG) produced by the brain, electrooculography (EOG) produced by eyes, electromyogram (EMG) produced by muscles, and blood volume pressure (BVP). Most of these signals have been applied to many fields, such as building a wireless monitoring system to help patients and elders to monitor their own physiological conditions in real time  [5] , to detect the stress state of the subjects  [6] , to monitor the sleep quality  [7] , and so on. Among these signals, EEG has received considerable attentions from researchers in the field of emotion recognition. Alarcao and Fonseca  [8]  provided a comprehensive overview of the work published between 2009 and 2016 on emotion recognition using EEG signals, and gave some suggestions to achieve reproducible, replicable, well-validated and high-quality results. When classifying EEG signals, existing studies have indicated that peripheral physiological signals can also change with emotion  [9] . Jerritta et al.  [10]  extracted high-order statistical (HOS) features from facial EMG signals and mapped these features into corresponding emotions using k-nearest neighbor classifier. Nabian et al.  [11]  presented the development of a biosignal-specific processing toolbox (Bio-SP tool) for preprocessing and feature extraction of physiological signals, such as EOG, EMG, electrocardiogram (ECG), electrodermal activity, continuous blood pressure. However, some studies have found that it is very difficult to accurately reflect emotion status using a single physiological signal  [12] , and more and more studies attempt to achieve better classification performance by co-processing multiple features  [13] [14] [15] . Wen et al.  [16]  used galvanic skin response (GSR), fingertip oxygen saturation and heart rate as input signals to classify five emotions through random forests. Das et al.  [17]  combined ECG and GSR signals and calculated their power spectral density (PSD) as features for the classification of three emotions: happy, sad, and neutral. The emotion classification problems consist of processes such as preprocessing, feature extraction, classification and analysis  [18] . In the survey  [8] , approximately 84% of the works they collected used some band pass filters as the preprocessing method, reflecting the importance of different bands, such as delta, theta, alpha, beta and gamma. The feature extraction process can be handled using various methods, among which the Fourier transform, PSD and entropy are widely used  [8] . Fourier transform includes the short-time Fourier transform (STFT) and discrete Fourier transform (DFT). Entropy includes approximate entropy (AE), sample entropy (SE), differential entropy (DE), and wavelet entropy (WE). In addition, there are other methods also applied to extract features, like, wavelet transform  [19] , empirical mode decomposition (EMD)  [20] , auto-regressive (AR)  [21] , and so on. In addition to data preprocessing and feature extraction, classification phase is an important part of emotion recognition model. There are plenty of classifiers for automatic emotion identification  [22] , such as support vector machine (SVM), K-nearest neighbor (KNN), linear discriminant analysis (LDA), and naive Bayes  [8] . Most of the above methods use a single classifier to recognize emotions. Although a single classifier can achieve good recognition results, recent research shows that deep recognition models  [23] [24] [25] [26] , and combination of multiple classifiers, i.e., ensemble learning, can get better results. Many ensemble strategies have been proposed  [27] , such as bagging, boosting, and stacking. In this paper, we propose an emotion classification ensemble model for emotion classification problem. Our analysis mainly focused on the combination of different peripheral physiological signals with EEG, and the impact of integration of multiple classifiers on results. The performance of proposed method is investigated on DEAP emotion database  [28] . The experimental details and results will be shown and compared in the experimental sections. The rest of paper is organized as follows. Section 2 briefly introduces an emotion analysis database used in this paper. In Section 3, an emotion recognition model through multimodal physiological signals for different emotions is proposed. And the experimental results and analysis are given in Section 4. Section 5 concludes this paper.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Multiple Modal Physiological Signals",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Physiological Signals Description",
      "text": "The pre-processed data set from database for emotion analysis using physiological signals DEAP  [28]  is used in our research. The database contains 32 subjects physiological signals which were got from 40 channels, 32 channel EEG data were recorded using a Biosemi ActiveTwo system and 8 channel peripheral physiological signals were recorded around the body using sensors, including hEOG, vEOG, zEMG, tEMG, GSR, respiration belt, plethysmograph and temperature. Afterwards, the data was down-sampled to 128Hz from 512Hz, and eye blink artifact removal via independent component analysis. During collection, each subject was presented with forty, one-minute long music videos with varying emotional content. Then she/he was asked to fill a self-assessment for her/his valence, arousal, liking and dominance from 1 to 9. A standard for evaluating and comparing accuracies of emotion recognition  This section mainly focuses on the method of our emotional recognition model for each phase of preprocessing, feature extraction and classification. The specific procedure of our proposed method is showed in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Processing",
      "text": "Considering different signal bands contain characteristic of emotion, it is very necessary to filter different bands from physiological signals to extract more targeted features for improving the final classification performance. The frequency of signals contained in DEAP is from 4Hz to 45Hz. In our paper, theta (4-8Hz), alpha(8-13Hz), beta(13-30Hz) and gamma(30-43Hz) bands are filtered from different physiological signals respectively by adopting Butterworth filters  [29] . Butterworth filters have a magnitude response that is maximally flat in the pass band and monotonic overall. This smoothness comes at the price of decreased roll off steepness. Its low pass filter squared amplitude response can be represented as equation  (1) .\n\nwhere N is the order of filter. In our experiment, the value is set to 8. ω c is the cut-off frequency at which |H a (jω\n\nIn general, ω c is also known as the -3dB cut-off frequency. To simulate band pass filter to get the specific band mentioned above, we first consider the transfer function H a (s) in the s-domain as shown in equation (  2 ), which is the same as H a (jω) when s = jω.\n\nwhere s k (k = 0, 1, . . . , N -1) is a pole in the s-plane. Since the amplitude-frequency characteristics of the filters are different, all frequencies are normalized in order to unify the design. Let λ = ω ωc , where λ is normalized frequency, and p = j, λ = jω ωc , where p is normalized complex variable. Now the normalized Butterworth transfer function can be written as equation  (3) .\n\nwhere p k (k = 0, 1, . . . , N -1) is a normalized pole.\n\nTo change low pass filter to band pass filter, the index conversion is done. Let ω u be pass band upper limit frequency, ω l be pass band lower limit frequency, ω 0 be pass band center frequency. And let:\n\n. Through the index conversion we can get the band pass filter transfer function as shown in equation  (4) . By passing the specific four bands parameters to it, we get each of the four bands to extract features.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Features Extraction",
      "text": "Hjorth proposed the Hjorth parameter in 1970  [30] , which provides a fast method for calculating three important features of signals in time domain, including Activity, Mobility, and Complexity. It has been widely used in physiological signal processing area. After filtering out of different brain rhythms, features are extracted by calculating the parameters from each of them. The activity parameter represents the signal power, which can indicate the surface of power spectrum in the frequency domain. It can be calculated by equation  (5) .\n\nwhere y(t) represents the signal. The mobility parameter represents the mean frequency or the proportion of standard deviation of the power spectrum. This is defined as the square root of variance of the first derivative of the signal y(t) divided by variance of the signal y(t). It is denoted as equation  (6) .\n\nThe Complexity parameter represents the change in frequency. The parameter compares the signal's similarity to a pure sine wave, where the value converges to 1 if the signal is more similar. It is represented by the following equation  (7) .\n\nThe Hjorth parameters based on variance have faster calculation speed than other methods. We calculate the three parameters from EEG, EOG, EMG, and combine them to form feature sets for classification. In the ensemble classifier, one test sample can get three test labels from three base classifiers. We adopt the method of majority vote to determine the class it ultimately belongs to.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results And Discussion",
      "text": "(1) Performance evaluation parameters\n\nThe accuracy rate is one of the most commonly used evaluation parameters in classification. The accuracy pacc is defined as:\n\nwhere n T N is the number of correctly predicted high-level instances, n T P is the number of correctly predicted low-level instances, n F N and n F P are the number of wrongly classified instances for high-level and low-level respectively.\n\nConsidering the class imbalance, for two-class task F-score is also calculated as an evaluation parameter:\n\nF -score = 2P pre P sen P pre + P sen where P p re is defined as P pre = nT P nT P +nF P , P s en is defined as P sen = nT P nT P +nF N .   2  and Fig.  3 , we can find that the ensemble classifier has a very good classification accuracy for each class. CART performs good as well, whereas, by calculating standard deviation, the result is more fluctuating than ensemble classifier in Table  3 . RF and KNN do not show good classification performance for this problem during our experiments. For different combination of physiological signals, EEG with EOG and EMG have best results, the join of peripheral physiological signals makes the result more stable than EEG only. Whereas, from Fig.  4  and Fig.  5 , the influence of different classifiers on the classification effect is much greater than the combinations of the different signals. Table  4  shows some existing researches for two-class task. Zoubi et al.  [32]  identified the human emotional state through the LSM model with the accuracy of 88.54 on arousal and 84.63 on valence. Piho and Tjahjadi  [33]      result is more fluctuating than ensemble classifier in Table  3 . RF and KNN do not show good classification performance for this problem during our experiments.\n\nFor different combination of physiological signals, EEG with EOG and EMG have best results, the join of peripheral physiological signals makes the result more stable than EEG only. Whereas, from Fig.  4  and Fig.  5 , the influence of different classifiers on the arousal and 84.63% on valence. Piho and Tjahjadi  [25]  conducted emotion recognition by shortening the signal to find the strongest part of the mutual information and obtained the result of 89.84% on arousal and 89.61% on valence. Therefore, the methods proposed in this paper get higher results than existing methods. We can see that our model has the best performance. combination of signal and classifier performs most stable. From Fig.  2  and Fig.  3 , we can find that the ensemble classifier has a very good classification accuracy for each class. CART performs good as well, whereas, by calculating standard deviation, the result is more fluctuating than ensemble classifier in Table  3 . RF and KNN do not show good classification performance for this problem during our experiments.\n\nFor different combination of physiological signals, EEG with EOG and EMG have best results, the join of peripheral physiological signals makes the result more stable than EEG only. Whereas, from Fig.  4  and Fig.  5 , the influence of different classifiers on the classification effect is much greater than the combinations of the different signals. Table  4  shows some existing researches for two class task. Zoubi et al  [24 identified  the human emotional state through the LSM model with the accuracy of 88.54% on arousal and 84.63% on valence. Piho and Tjahjadi 25 onducted emotion recognition by shortenin he signal to find the strongest part of the mutual information and obtained the result of 89.84% on arousal and 89.61% on valence. Therefore, the methods proposed in this paper get higher results than existing methods. We can see that our model has the best performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Four Class Task",
      "text": "The emotion recognition classification results for four class task are displayed in Table  5 . From Table  5 , we can know that our model performs good on four class task. It can be seen that when the feature set is the combined feature set of EEG, EOG and EMG, the highest classification accuracy is obtained by using the ensemble classifier, which is 90.74%. It can be clearly seen from Figure  6  that when the same classification model is selected and different feature sets are combined, the classification results have little effect; but when the same feature set combination and different classification models are selected, the classification results are more obviously different. Therefore, we can conclude that when four class task is performed on arousal and valence, the selection of feature sets has less influence on the results, and the selection of classification models has a greater impact on the results. And while the feature set combined by EEG, EOG and EMG, the ensemble classifier is adopted, the model is the most stable.\n\nIt can be also seen from Table  5  and Fig.  6  that for four types of emotional states, decision trees, random forests, and ensemble classifier models have the best recognition ability on HAHV, while KNN performs strongest part of the mutual information and obtained the result of 89.84 on arousal and 89.61 on valence. Therefore, the methods proposed in this paper get higher results than existing methods. We can see that our model has the best performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Four-Class Task",
      "text": "The emotion recognition classification results for four-class task are displayed in Table  5 . From Table  5 , we can know that our model performs good on fourclass task. It can be seen that when the feature set is the combined feature set of EEG, EOG and EMG, the highest classification accuracy is obtained by using the ensemble classifier, which is 90.74. It can be clearly seen from Figure  6  that when the same classification model is selected and different feature sets are combined, the classification results have little effect; but when the same feature set combination and different classification models are selected, the classification results are more obviously different. Therefore, we can conclude that when four-class task is performed on arousal and valence, the selection of feature sets has less influence on the results, and the selection of classification models has a greater impact on the results. And while the feature set combined by EEG, EOG and EMG, the ensemble classifier is adopted, the model is the most stable.\n\nIt can be also seen from",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The ﬂow diagram of proposed method",
      "page": 5
    },
    {
      "caption": "Figure 1: 3.1 Pre-processing",
      "page": 5
    },
    {
      "caption": "Figure 2: The results of diﬀerent classiﬁers on arousal dimension. (a) EEG sig-",
      "page": 10
    },
    {
      "caption": "Figure 3: The results of diﬀerent classiﬁers on valence dimension. (a) EEG sig-",
      "page": 10
    },
    {
      "caption": "Figure 4: The results of diﬀerent physiological signals on classiﬁers for arousal",
      "page": 11
    },
    {
      "caption": "Figure 5: The results of diﬀerent physiological signals on classiﬁers for valence",
      "page": 11
    },
    {
      "caption": "Figure 6: that when the same classiﬁcation model is selected and diﬀerent feature sets",
      "page": 12
    },
    {
      "caption": "Figure 6: that for four types of emotional",
      "page": 12
    },
    {
      "caption": "Figure 6: Results of experiments for four-category task.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Received: date / Accepted: date": "Abstract Physiological signals that provide the objective repression of human"
        },
        {
          "Received: date / Accepted: date": "aﬀective states are attracted increasing attention in the emotion recognition"
        },
        {
          "Received: date / Accepted: date": "ﬁeld. However, the single signal is diﬃcult to obtain completely and accurately"
        },
        {
          "Received: date / Accepted: date": "description for emotion. Multiple physiological\nsignals\nfusing models, build-"
        },
        {
          "Received: date / Accepted: date": "ing the uniform classiﬁcation model by means of consistent and complemen-"
        },
        {
          "Received: date / Accepted: date": "tary information from diﬀerent emotions to improve recognition performance."
        },
        {
          "Received: date / Accepted: date": "Original\nfusing models usually choose the particular classiﬁcation method to"
        },
        {
          "Received: date / Accepted: date": "recognition, which is ignoring diﬀerent distribution of multiple signals. Aim-"
        },
        {
          "Received: date / Accepted: date": "ing above problems,\nin this work, we propose an emotion classiﬁcation model"
        },
        {
          "Received: date / Accepted: date": "through multiple modal physiological signals for diﬀerent emotions. Features"
        },
        {
          "Received: date / Accepted: date": "are extracted from EEG, EMG, EOG signals for characterizing emotional state"
        },
        {
          "Received: date / Accepted: date": "on valence and arousal\nlevels. For characterization, four bands ﬁltering theta,"
        },
        {
          "Received: date / Accepted: date": "beta, alpha, gamma for\nsignal preprocessing are adopted and three Hjorth"
        },
        {
          "Received: date / Accepted: date": "parameters are computing as features. To improve classiﬁcation performance,"
        },
        {
          "Received: date / Accepted: date": "an ensemble classiﬁer is built. Experiments are conducted on the benchmark"
        },
        {
          "Received: date / Accepted: date": "DEAP datasets. For the two-class task, the best result on arousal\nis 94.42%,"
        },
        {
          "Received: date / Accepted: date": "the best result on valence is 94.02%, respectively. For the four-class task, the"
        },
        {
          "Received: date / Accepted: date": "highest average classiﬁcation accuracy is 90.74, and it\nshows good stability."
        },
        {
          "Received: date / Accepted: date": "The\ninﬂuence of diﬀerent peripheral physiological\nsignals\nfor\nresults\nis also"
        },
        {
          "Received: date / Accepted: date": "analyzed in this paper."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nJing Zhang et al.": "Keywords Emotion recognition\nphysiological signal processing\nensemble"
        },
        {
          "2\nJing Zhang et al.": "·\n·"
        },
        {
          "2\nJing Zhang et al.": "classiﬁer"
        },
        {
          "2\nJing Zhang et al.": "1 Introduction"
        },
        {
          "2\nJing Zhang et al.": "Emotion is a very important part to each of us, aﬀecting our daily life such"
        },
        {
          "2\nJing Zhang et al.": "as work eﬃciency,\nlearning status, social contact and decision making. Recog-"
        },
        {
          "2\nJing Zhang et al.": "nizing emotions through speech [1],\nfacial expressions [2] and gestures [3] are"
        },
        {
          "2\nJing Zhang et al.": "very signiﬁcant components of research works in this ﬁeld. However, all of these"
        },
        {
          "2\nJing Zhang et al.": "can be artiﬁcially changed or controlled, sometimes the results from them dont"
        },
        {
          "2\nJing Zhang et al.": "really reﬂect persons emotion status. Hence, more and more researchers pay"
        },
        {
          "2\nJing Zhang et al.": "attention to conduct emotion recognition by processing physiological signals,"
        },
        {
          "2\nJing Zhang et al.": "which are more objective. Due to the recent advancement in building wireless"
        },
        {
          "2\nJing Zhang et al.": "and wearable sensors\n[4], physiological\nsignals are relatively easy to acquire"
        },
        {
          "2\nJing Zhang et al.": "without damage to body. Human body is a combination of various organs,"
        },
        {
          "2\nJing Zhang et al.": "for example heart, brain, muscles and eyes. Some organs produce electrical"
        },
        {
          "2\nJing Zhang et al.": "signals, such as the electroencephalogram (EEG) produced by the brain, elec-"
        },
        {
          "2\nJing Zhang et al.": "trooculography (EOG) produced by eyes, electromyogram (EMG) produced"
        },
        {
          "2\nJing Zhang et al.": "by muscles, and blood volume pressure (BVP). Most of these signals have been"
        },
        {
          "2\nJing Zhang et al.": "applied to many ﬁelds, such as building a wireless monitoring system to help"
        },
        {
          "2\nJing Zhang et al.": "patients and elders to monitor their own physiological conditions in real time"
        },
        {
          "2\nJing Zhang et al.": "[5], to detect the stress state of the subjects [6], to monitor the sleep quality"
        },
        {
          "2\nJing Zhang et al.": "[7], and so on. Among these signals, EEG has received considerable attentions"
        },
        {
          "2\nJing Zhang et al.": "from researchers in the ﬁeld of emotion recognition. Alarcao and Fonseca [8]"
        },
        {
          "2\nJing Zhang et al.": "provided a comprehensive overview of the work published between 2009 and"
        },
        {
          "2\nJing Zhang et al.": "2016 on emotion recognition using EEG signals, and gave some suggestions to"
        },
        {
          "2\nJing Zhang et al.": "achieve reproducible, replicable, well-validated and high-quality results. When"
        },
        {
          "2\nJing Zhang et al.": "classifying EEG signals, existing studies have indicated that peripheral physi-"
        },
        {
          "2\nJing Zhang et al.": "ological signals can also change with emotion [9]. Jerritta et al.\n[10] extracted"
        },
        {
          "2\nJing Zhang et al.": "high-order\nstatistical\n(HOS)\nfeatures\nfrom facial EMG signals and mapped"
        },
        {
          "2\nJing Zhang et al.": "these features into corresponding emotions using k-nearest neighbor classiﬁer."
        },
        {
          "2\nJing Zhang et al.": "Nabian et al.\n[11] presented the development of a biosignal-speciﬁc processing"
        },
        {
          "2\nJing Zhang et al.": "toolbox (Bio-SP tool) for preprocessing and feature extraction of physiological"
        },
        {
          "2\nJing Zhang et al.": "signals, such as EOG, EMG, electrocardiogram (ECG), electrodermal activ-"
        },
        {
          "2\nJing Zhang et al.": "ity, continuous blood pressure. However,\nsome studies have found that\nit\nis"
        },
        {
          "2\nJing Zhang et al.": "very diﬃcult to accurately reﬂect emotion status using a single physiological"
        },
        {
          "2\nJing Zhang et al.": "signal\n[12], and more and more studies attempt to achieve better classiﬁcation"
        },
        {
          "2\nJing Zhang et al.": "performance by co-processing multiple features\n[13-15]. Wen et al.\n[16] used"
        },
        {
          "2\nJing Zhang et al.": "galvanic skin response (GSR), ﬁngertip oxygen saturation and heart rate as"
        },
        {
          "2\nJing Zhang et al.": "input signals to classify ﬁve emotions through random forests. Das et al.\n[17]"
        },
        {
          "2\nJing Zhang et al.": "combined ECG and GSR signals and calculated their power spectral density"
        },
        {
          "2\nJing Zhang et al.": "(PSD) as\nfeatures\nfor\nthe\nclassiﬁcation of\nthree\nemotions: happy,\nsad, and"
        },
        {
          "2\nJing Zhang et al.": "neutral. The emotion classiﬁcation problems consist of processes such as pre-"
        },
        {
          "2\nJing Zhang et al.": "processing,\nfeature extraction, classiﬁcation and analysis\n[18].\nIn the survey"
        },
        {
          "2\nJing Zhang et al.": "[8], approximately 84% of the works they collected used some band pass ﬁlters"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "as the preprocessing method, reﬂecting the importance of diﬀerent bands, such"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "as delta, theta, alpha, beta and gamma. The feature extraction process can be"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "handled using various methods, among which the Fourier transform, PSD and"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "entropy are widely used [8]. Fourier transform includes the short-time Fourier"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "transform (STFT) and discrete Fourier\ntransform (DFT). Entropy includes"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "approximate entropy (AE),\nsample entropy (SE), diﬀerential entropy (DE),"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "and wavelet entropy (WE). In addition, there are other methods also applied"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "to extract\nfeatures,\nlike, wavelet\ntransform [19], empirical mode decomposi-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "tion (EMD)\n[20], auto-regressive (AR)\n[21], and so on.\nIn addition to data"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "preprocessing and feature extraction, classiﬁcation phase is an important part"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "of\nemotion recognition model. There are plenty of\nclassiﬁers\nfor automatic"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "emotion identiﬁcation [22], such as support vector machine (SVM), K-nearest"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "neighbor (KNN), linear discriminant analysis (LDA), and naive Bayes [8]. Most"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "of the above methods use a single classiﬁer to recognize emotions. Although"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "a single classiﬁer can achieve good recognition results, recent research shows"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "that deep recognition models [23-26], and combination of multiple classiﬁers,"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "i.e., ensemble learning, can get better results. Many ensemble strategies have"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "been proposed [27], such as bagging, boosting, and stacking. In this paper, we"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "propose an emotion classiﬁcation ensemble model\nfor\nemotion classiﬁcation"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "problem. Our analysis mainly focused on the combination of diﬀerent periph-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "eral physiological signals with EEG, and the impact of\nintegration of multiple"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "classiﬁers on results. The performance of proposed method is investigated on"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "DEAP emotion database [28]. The\nexperimental details and results will be"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "shown and compared in the experimental sections. The rest of paper is orga-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "nized as follows. Section 2 brieﬂy introduces an emotion analysis database used"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "in this paper. In Section 3, an emotion recognition model through multimodal"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "physiological signals for diﬀerent emotions is proposed. And the experimental"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "results and analysis are given in Section 4. Section 5 concludes this paper."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "2 Multiple modal physiological signals"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "2.1 Physiological signals description"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "The pre-processed data set from database for emotion analysis using physio-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "logical signals DEAP [28]\nis used in our\nresearch. The database contains 32"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "subjects physiological\nsignals which were got\nfrom 40 channels, 32 channel"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "EEG data were recorded using a Biosemi ActiveTwo system and 8 channel"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "peripheral physiological signals were recorded around the body using sensors,"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "including hEOG, vEOG,\nzEMG,\ntEMG, GSR,\nrespiration belt, plethysmo-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "graph and temperature. Afterwards,\nthe data was down-sampled to 128Hz"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "from 512Hz, and eye blink artifact removal via independent component analy-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "sis. During collection, each subject was presented with forty, one-minute long"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "music videos with varying emotional content. Then she/he was asked to ﬁll a"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "self-assessment for her/his valence, arousal,\nliking and dominance from 1 to"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n3": "9. A standard for evaluating and comparing accuracies of emotion recognition"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: and TABLE 2 shows the number of samples for each class",
      "data": [
        {
          "High Arousal": "methods is not established now. The selection of EEG electrode channels and",
          "296\n458": ""
        },
        {
          "High Arousal": "time segments are always a controversial problem. In this research, to avoid the",
          "296\n458": ""
        },
        {
          "High Arousal": "loss of",
          "296\n458": "information, we used the whole data from EEG channels. For periph-"
        },
        {
          "High Arousal": "eral physiological signals, hEOG and vEOG are merged called EOG, zEMG",
          "296\n458": ""
        },
        {
          "High Arousal": "and tEMG are merged called EMG. They are combined with EEG data to",
          "296\n458": ""
        },
        {
          "High Arousal": "classify emotion state. The accuracies from them separately are compared and",
          "296\n458": ""
        },
        {
          "High Arousal": "discussion in section IV.",
          "296\n458": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "Fig. 1: The ﬂow diagram of proposed method"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "3 The proposed model"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "This section mainly focuses on the method of our emotional recognition model"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "for each phase of preprocessing, feature extraction and classiﬁcation. The spe-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "ciﬁc procedure of our proposed method is showed in Fig. 1."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "3.1 Pre-processing"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "Considering diﬀerent signal bands contain characteristic of emotion,\nit is very"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "necessary to ﬁlter diﬀerent bands from physiological signals to extract more"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "targeted features for\nimproving the ﬁnal classiﬁcation performance. The fre-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "quency of\nsignals\ncontained in DEAP is\nfrom 4Hz\nto 45Hz.\nIn our paper,"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "theta (4-8Hz), alpha(8-13Hz), beta(13-30Hz) and gamma(30-43Hz) bands are"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "ﬁltered from diﬀerent physiological\nsignals\nrespectively by adopting Butter-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "worth ﬁlters [29]. Butterworth ﬁlters have a magnitude response that is max-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "imally ﬂat in the pass band and monotonic overall. This smoothness comes at"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n5": "the price of decreased roll oﬀ steepness. Its low pass ﬁlter squared amplitude"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nJing Zhang et al.": "response can be represented as equation (1)."
        },
        {
          "6\nJing Zhang et al.": "1"
        },
        {
          "6\nJing Zhang et al.": "(1)\n2 =\nHa (jω)"
        },
        {
          "6\nJing Zhang et al.": "k\nk"
        },
        {
          "6\nJing Zhang et al.": "1 + ( ω\nωc )2N"
        },
        {
          "6\nJing Zhang et al.": "where N is the order of ﬁlter.\nis\nIn our experiment, the value is set to 8. ωc"
        },
        {
          "6\nJing Zhang et al.": "is also known\nthe cut-oﬀ frequency at which\n= 1/√2. In general, ωc\nHa (jω)"
        },
        {
          "6\nJing Zhang et al.": "|\n|"
        },
        {
          "6\nJing Zhang et al.": "as the -3dB cut-oﬀ frequency."
        },
        {
          "6\nJing Zhang et al.": "To simulate band pass ﬁlter to get the speciﬁc band mentioned above, we ﬁrst"
        },
        {
          "6\nJing Zhang et al.": "consider the transfer function Ha (s) in the s-domain as shown in equation (2),"
        },
        {
          "6\nJing Zhang et al.": "which is the same as Ha (jω) when s = jω."
        },
        {
          "6\nJing Zhang et al.": "ωN\nc"
        },
        {
          "6\nJing Zhang et al.": "(2)\nHa(s) ="
        },
        {
          "6\nJing Zhang et al.": "N −1"
        },
        {
          "6\nJing Zhang et al.": "sk)\nk=0 (s"
        },
        {
          "6\nJing Zhang et al.": "−"
        },
        {
          "6\nJing Zhang et al.": "(k = 0, 1, . . . , N\n1) is a pole in the s-plane.\nwhere sk"
        },
        {
          "6\nJing Zhang et al.": "−"
        },
        {
          "6\nJing Zhang et al.": "Since\nthe amplitude-frequency characteristics of\nthe ﬁlters are diﬀerent, all"
        },
        {
          "6\nJing Zhang et al.": "frequencies are normalized in order to unify the design. Let λ = ω"
        },
        {
          "6\nJing Zhang et al.": "ωc , where λ"
        },
        {
          "6\nJing Zhang et al.": "is normalized frequency, and p = j, λ = jω"
        },
        {
          "6\nJing Zhang et al.": "ωc , where p is normalized complex"
        },
        {
          "6\nJing Zhang et al.": "variable. Now the normalized Butterworth transfer function can be written as"
        },
        {
          "6\nJing Zhang et al.": "equation (3)."
        },
        {
          "6\nJing Zhang et al.": "1"
        },
        {
          "6\nJing Zhang et al.": "(3)\nHa (p) ="
        },
        {
          "6\nJing Zhang et al.": "N −1"
        },
        {
          "6\nJing Zhang et al.": "(p\npk)"
        },
        {
          "6\nJing Zhang et al.": "k=0"
        },
        {
          "6\nJing Zhang et al.": "−"
        },
        {
          "6\nJing Zhang et al.": "(k = 0, 1, . . . , N\n1) is a normalized pole.\nwhere pk"
        },
        {
          "6\nJing Zhang et al.": "Q"
        },
        {
          "6\nJing Zhang et al.": "−"
        },
        {
          "6\nJing Zhang et al.": "To change low pass ﬁlter to band pass ﬁlter, the index conversion is done. Let"
        },
        {
          "6\nJing Zhang et al.": "ωu be pass band upper limit frequency, ωl be pass band lower limit frequency,"
        },
        {
          "6\nJing Zhang et al.": "frequency. And let: B = ωu\nωl, ηl = ωl\nω0 be pass band center\nB , ηu = ωu\nB ,"
        },
        {
          "6\nJing Zhang et al.": "−"
        },
        {
          "6\nJing Zhang et al.": "η2\n.\n0 = ηlηu. So λ = η2−η2"
        },
        {
          "6\nJing Zhang et al.": "Through the index conversion we can get the band pass ﬁlter transfer function"
        },
        {
          "6\nJing Zhang et al.": "as shown in equation (4). By passing the speciﬁc four bands parameters to it,"
        },
        {
          "6\nJing Zhang et al.": "we get each of the four bands to extract features."
        },
        {
          "6\nJing Zhang et al.": "(4)\nHa (s) = Ha (p)\ns2 +ωlωu"
        },
        {
          "6\nJing Zhang et al.": "|p=\ns(ωu−ωl)"
        },
        {
          "6\nJing Zhang et al.": "3.2 Features extraction"
        },
        {
          "6\nJing Zhang et al.": "Hjorth proposed the Hjorth parameter\nin 1970 [30], which provides a fast"
        },
        {
          "6\nJing Zhang et al.": "method for\ncalculating three\nimportant\nfeatures of\nsignals\nin time domain,"
        },
        {
          "6\nJing Zhang et al.": "including Activity, Mobility, and Complexity. It has been widely used in phys-"
        },
        {
          "6\nJing Zhang et al.": "iological signal processing area. After ﬁltering out of diﬀerent brain rhythms,"
        },
        {
          "6\nJing Zhang et al.": "features are extracted by calculating the parameters from each of them."
        },
        {
          "6\nJing Zhang et al.": "The activity parameter\nrepresents\nthe signal power, which can indicate the"
        },
        {
          "6\nJing Zhang et al.": "surface of power spectrum in the frequency domain.\nIt can be calculated by"
        },
        {
          "6\nJing Zhang et al.": "equation (5)."
        },
        {
          "6\nJing Zhang et al.": "Activity = var(y(t))\n(5)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ensemble emotion recognizing with multiple modal physiological signals": "where y(t) represents the signal.",
          "7": ""
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals": "The mobility parameter\nrepresents the mean frequency or",
          "7": "the proportion of"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals": "",
          "7": "standard deviation of the power spectrum. This is deﬁned as the square root"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals": "of variance of the ﬁrst derivative of the signal y(t) divided by variance of the",
          "7": ""
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals": "signal y(t). It is denoted as equation (6).",
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nJing Zhang et al.": "4 Experiments and analysis"
        },
        {
          "8\nJing Zhang et al.": "4.1 Experiments setup"
        },
        {
          "8\nJing Zhang et al.": "The experiments are divided into two parts: two-class task and four-class task"
        },
        {
          "8\nJing Zhang et al.": "on the valence-arousal scale model. To validate the eﬀectiveness of our model"
        },
        {
          "8\nJing Zhang et al.": "and analyze the impact of peripheral physiological signals,\nfour combinations"
        },
        {
          "8\nJing Zhang et al.": "of physiological\nsignals are used, namely, EEG only, EEG with EOG, EEG"
        },
        {
          "8\nJing Zhang et al.": "with EMG, EEG with EOG and EMG."
        },
        {
          "8\nJing Zhang et al.": "There are 7680 (128*60) data points for each samples per channel. EEG col-"
        },
        {
          "8\nJing Zhang et al.": "lected 32 channels, while both EOM and EMG collected two channels, respec-"
        },
        {
          "8\nJing Zhang et al.": "tively. After ﬁltering each row data to four bands, namely theta, alpha, beta"
        },
        {
          "8\nJing Zhang et al.": "and gamma, 30720 (7680*4) data points are obtained for one channel. Ob-"
        },
        {
          "8\nJing Zhang et al.": "viously,\nit\nis diﬃcult\nto classify directly due to massive data. To reduce the"
        },
        {
          "8\nJing Zhang et al.": "feature dimensions and get eﬀective features, we compute Hjorth parameters"
        },
        {
          "8\nJing Zhang et al.": "in time domain after processing signals in frequency domain."
        },
        {
          "8\nJing Zhang et al.": "For each sample, each channel has 60 seconds data to deal with. During the cal-"
        },
        {
          "8\nJing Zhang et al.": "culation of the features, the ﬁxed time window of each segment is 10 seconds."
        },
        {
          "8\nJing Zhang et al.": "That is to say, every ten seconds of data can be calculated into three param-"
        },
        {
          "8\nJing Zhang et al.": "eters. So, we can get 2304 features\nfrom each band of each channel\nfor one"
        },
        {
          "8\nJing Zhang et al.": "sample to classify. EEG features and the combination of EEG with EOG fea-"
        },
        {
          "8\nJing Zhang et al.": "tures, EMG features are input into the ensemble classiﬁer respectively. Besides"
        },
        {
          "8\nJing Zhang et al.": "ensemble classiﬁer, each base classiﬁer, KNN, RF, CART, is also experimented"
        },
        {
          "8\nJing Zhang et al.": "to compare with each other. The results are shown and analysized in the fol-"
        },
        {
          "8\nJing Zhang et al.": "lowing section."
        },
        {
          "8\nJing Zhang et al.": "In the ensemble classiﬁer, one test sample can get three test labels from three"
        },
        {
          "8\nJing Zhang et al.": "base classiﬁers. We adopt the method of majority vote to determine the class"
        },
        {
          "8\nJing Zhang et al.": "it ultimately belongs to."
        },
        {
          "8\nJing Zhang et al.": "4.2 Experimental results and discussion"
        },
        {
          "8\nJing Zhang et al.": "(1) Performance evaluation parameters"
        },
        {
          "8\nJing Zhang et al.": "The accuracy rate is one of the most commonly used evaluation parameters"
        },
        {
          "8\nJing Zhang et al.": "in classiﬁcation. The accuracy pacc is deﬁned as:"
        },
        {
          "8\nJing Zhang et al.": "pacc = (nT N + nT P )/(nT N + nF N + nT P + nF P )"
        },
        {
          "8\nJing Zhang et al.": "is\nwhere nT N is the number of correctly predicted high-level\ninstances, nT P"
        },
        {
          "8\nJing Zhang et al.": "the number of correctly predicted low-level\nare the\ninstances, nF N and nF P"
        },
        {
          "8\nJing Zhang et al.": "number of wrongly classiﬁed instances for high-level and low-level respectively."
        },
        {
          "8\nJing Zhang et al.": "Considering the class imbalance,\nfor two-class task F-score is also calculated"
        },
        {
          "8\nJing Zhang et al.": "as an evaluation parameter:"
        },
        {
          "8\nJing Zhang et al.": "Psen"
        },
        {
          "8\nJing Zhang et al.": "F\nscore = 2Ppre"
        },
        {
          "8\nJing Zhang et al.": "−\nPpre + Psen"
        },
        {
          "8\nJing Zhang et al.": "nT P\nnT P"
        },
        {
          "8\nJing Zhang et al.": "where Ppre is deﬁned as Ppre ="
        },
        {
          "8\nJing Zhang et al.": "nT P +nF P , Psen is deﬁned as Psen =\nnT P +nF N ."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: From Table 3, we can obtain the following observations: First,",
      "data": [
        {
          "Table 3: Results of Experiments for Two-class Task": ""
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "Methods"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": ""
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+KNN"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+CART"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+RF"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+ENS"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+KNN"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+CART"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+RF"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+ENS"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EMG+KNN"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EMG+CART"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EMG+RF"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EMG+ENS"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+EMG+KNN"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+EMG+CART"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+EMG+RF"
        },
        {
          "Table 3: Results of Experiments for Two-class Task": "EEG+EOG+EMG+ENS"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: From Table 3, we can obtain the following observations: First,",
      "data": [
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "(2) Two-class task"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "The emotion recognition classiﬁcation results for two-class task are displayed"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "in Table 3. From Table 3, we\ncan obtain the\nfollowing observations: First,"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "compared with single base classiﬁer, the ensemble classiﬁer performs best, and"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "the corresponding result outperforms those mentioned above. Second, the ad-"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "dition of peripheral physiological signals can enhance the classiﬁcation results"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "of EEG signals, but not obviously."
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "The\nfocus of\nthis\nresearch was\nto do emotion classiﬁcation using EEG,"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "EOG, EMG signals on arousal and valence dimension, and comparing results"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "by diﬀerent combinations with diﬀerent classiﬁer. The best accuracies are 94.42"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "and 94.02, obtained respectively by our ensemble classiﬁer via EEG enhanced"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "by EOG and EMG on arousal and valence dimensions. And this combination"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "of\nsignal and classiﬁer performs most\nstable. From Fig.2 and Fig.3, we can"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "ﬁnd that\nthe\nensemble classiﬁer has a very good classiﬁcation accuracy for"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "each class. CART performs good as well, whereas, by calculating standard"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "deviation,\nthe result\nis more ﬂuctuating than ensemble classiﬁer\nin Table 3."
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "RF and KNN do not\nshow good classiﬁcation performance for\nthis problem"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "during our experiments."
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "For diﬀerent combination of physiological signals, EEG with EOG and EMG"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "have best results, the join of peripheral physiological signals makes the result"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "more stable than EEG only. Whereas,\nfrom Fig.4 and Fig.5, the inﬂuence of"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "diﬀerent classiﬁers on the classiﬁcation eﬀect\nis much greater than the com-"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "binations of the diﬀerent signals. Table 4 shows some existing researches for"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "two-class task."
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "Zoubi et al.\n[32]\nidentiﬁed the human emotional state through the LSM model"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "with the accuracy of 88.54 on arousal and 84.63 on valence. Piho and Tjah-"
        },
        {
          "EEG+EOG+EMG+ENS\n94.42% ± 1.96%\n0.9310\n94.02% ± 2.15%\n0.9308": "jadi\n[33] conducted emotion recognition by shortening the signal to ﬁnd the"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG+EOG+EMG+RF \n0.9": "0.85",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.85",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "EEG+EOG+EMG+ENS",
          "87.49%": "94.42%",
          "3.67%": "1.96%",
          "0.8475 \n0.9": "0.9310",
          "87.77%": "94.02%",
          "3.48%": "2.15%"
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.8",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.8",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.75",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.75",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.7",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.7",
          "87.77%": "",
          "3.48%": "(b)"
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "",
          "87.49%": "6",
          "3.67%": "9",
          "0.8475 \n0.9": "",
          "87.77%": "3",
          "3.48%": "5"
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.95",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.95",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.9",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.9",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.85",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.85",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.8",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.8",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.75",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.75",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "",
          "87.77%": "",
          "3.48%": "(d)"
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.7",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "0.7",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "1",
          "87.49%": "",
          "3.67%": "",
          "0.8475 \n0.9": "1",
          "87.77%": "",
          "3.48%": ""
        },
        {
          "EEG+EOG+EMG+RF \n0.9": "0.95",
          "87.49%": "6",
          "3.67%": "9",
          "0.8475 \n0.9": "0.95",
          "87.77%": "3",
          "3.48%": "5"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "10\nJing Zhang et al."
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+EMG+ENS \n94.42%\n1.96% \n0.9310 \n94.02%\n2.15% \n0.9308"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "(a)\n(b)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n1"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48% \n0.8596 \n0.9\n0.9"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.85\n0.85"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+EMG+ENS \n94.42%\n1.96% \n0.9310 \n94.02%\n2.15% \n0.9308"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.7\n0.7\n(a)\n(b)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+CART\nEEG+EOG+ENS\nEEG+CART\nEEG+ENS"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.9\n0.9"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+KNN\nEEG+EOG+RF\nEEG+KNN\nEEG+RF\n0.85\n0.85"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "(d)\n(c)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.7\n0.7"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n1"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.9\n0.9\nEEG+EOG+CART\nEEG+EOG+ENS\nEEG+CART\nEEG+ENS"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.85\n0.85\nEEG+EOG+KNN\nEEG+EOG+RF\nEEG+KNN\nEEG+RF"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.7\n0.7\n(d)\n(c)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EMG+CART\nEEG+EMG+ENS\nEEG+EOG+EMG+CART\nEEG+EOG+EMG+ENS"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.9\n0.9"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EMG+KNN\nEEG+EMG+RF\nEEG+EOG+EMG+KNN\nEEG+EOG+EMG+RF\n0.85\n0.85"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "Fig. 2. The results of different classifiers on arousal dimension. (a) EEG signal on different classifiers; (b) the combination of EEG and EOG"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "Fig. 2: The results of diﬀerent classiﬁers on arousal dimension. (a) EEG sig-"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "signals on different classifiers; (c) the combination of EEG and EMG signals on different classifiers; (d) the combination of EEG, EOG and"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EMG+ENS\nnal on diﬀerent classiﬁers; (b)\nthe combination of EEG and EOG signals on"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EMG+KNN\nEEG+EMG+RF\nEEG+EOG+EMG+KNN\nEEG+EOG+EMG+RF"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "diﬀerent classiﬁers; (c) the combination of EEG and EMG signals on diﬀerent"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "(a)\n(b)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "classiﬁers; (d)\nthe combination of EEG, EOG and EMG signals on diﬀerent"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "Fig. 2. The results of different classifiers on arousal dimension. (a) EEG signal on different classifiers; (b) the combination of EEG and EOG \n0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "signals on different classifiers; (c) the combination of EEG and EMG signals on different classifiers; (d) the combination of EEG, EOG and \nclassiﬁers."
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.85\nEMG signals on different classifiers."
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.7\n0.7\n(a)\n(b)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+CART\nEEG+EOG+ENS\nEEG+CART\nEEG+ENS"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.9\n0.9"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+KNN\nEEG+EOG+RF\nEEG+KNN\nEEG+RF\n0.85\n0.85"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "(d)\n(c)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.7\n0.7"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.9\n0.9\nEEG+EOG+CART\nEEG+EOG+ENS\nEEG+CART\nEEG+ENS"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.85\n0.85"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EOG+KNN\nEEG+EOG+RF\nEEG+KNN\nEEG+RF"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.7\n0.7\n(d)\n(c)"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "1\n1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.95\n0.95"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EMG+CART\nEEG+EMG+ENS\nEEG+EOG+EMG+CART\nEEG+EOG+EMG+ENS"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.9\n0.9"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EEG+EMG+KNN\nEEG+EMG+RF\nEEG+EOG+EMG+KNN\nEEG+EOG+EMG+RF"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.85\n0.85"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.8\n0.8"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "0.75\n0.75"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "Fig. 3. The results of different classifiers on valence dimension. (a) EEG signal on different classifiers; (b) the combination of EEG and EOG"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "signals on different classifiers; (c) the combination of EEG and EMG signals on different classifiers; (d) the combination of EEG, EOG and"
        },
        {
          "EEG+EOG+EMG+RF \n87.49%\n3.67% \n0.8475 \n87.77%\n3.48%": "EMG signals on different classifiers."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.75\n0.75": "Fig. 3. The results of different classifiers on valence dimension. (a) EEG signal on different classifiers; (b) the combination of EEG and EOG"
        },
        {
          "0.75\n0.75": "signals on different classifiers; (c) the combination of EEG and EMG signals on different classifiers; (d) the combination of EEG, EOG and"
        },
        {
          "0.75\n0.75": "EMG signals on different classifiers. \nEEG+EMG+CART\nEEG+EMG+ENS\nEEG+EOG+EMG+CART\nEEG+EOG+EMG+ENS"
        },
        {
          "0.75\n0.75": "EEG+EMG+KNN\nEEG+EMG+RF\nEEG+EOG+EMG+KNN\nEEG+EOG+EMG+RF"
        },
        {
          "0.75\n0.75": "classifier.  The \nbest \naccuracies \nare \n94.42% \nand \nThe  focus  of \nthis  research  was \nto  do  emotion"
        },
        {
          "0.75\n0.75": "94.02%, \nobtained \nrespectively \nby \nour \nensemble \nclassification  using  EEG,  EOG,  EMG  signals  on"
        },
        {
          "0.75\n0.75": "Fig. 3. The results of different classifiers on valence dimension. (a) EEG signal on different classifiers; (b) the combination of EEG and EOG"
        },
        {
          "0.75\n0.75": "arousal \nFig. 3: The results of diﬀerent classiﬁers on valence dimension. (a) EEG sig-"
        },
        {
          "0.75\n0.75": "signals on different classifiers; (c) the combination of EEG and EMG signals on different classifiers; (d) the combination of EEG, EOG and"
        },
        {
          "0.75\n0.75": "arousal \nand \nvalence \ndimensions. \nAnd \nthis \nresults \nby \ndifferent \ncombinations  with \ndifferent"
        },
        {
          "0.75\n0.75": "nal on diﬀerent classiﬁers; (b)\nthe combination of EEG and EOG signals on"
        },
        {
          "0.75\n0.75": "diﬀerent classiﬁers; (c) the combination of EEG and EMG signals on diﬀerent\nThe  focus  of \nand"
        },
        {
          "0.75\n0.75": "classification  using  EEG,  EOG,  EMG  signals  on \nensemble \nclassiﬁers; (d)\nthe combination of EEG, EOG and EMG signals on diﬀerent"
        },
        {
          "0.75\n0.75": "arousal \nand \nvalence \ndimension, \nand \ncomparing \nclassifier  via  EEG  enhanced  by  EOG  and  EMG  on"
        },
        {
          "0.75\n0.75": "classiﬁers."
        },
        {
          "0.75\n0.75": "results \nby \ndifferent \ncombinations  with \ndifferent \narousal \nand \nvalence \ndimensions. \nAnd \nthis"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: RF and KNN do not show good [25] conducted emotion recognition by shortening",
      "data": [
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "accuracy  for  each  class.  CART  performs  good  as \nal. [24] identified the human emotional state through"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "well,  whereas, by calculating  standard deviation,  the \nthe  LSM  model  with \nthe  accuracy  of  88.54%  on"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "result  is  more fluctuating than  ensemble  classifier  in \narousal  and  84.63%  on  valence.  Piho  and  Tjahjadi"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "Table \n3. \nRF \nand  KNN \ndo \nnot \nshow \ngood \n[25]\nconducted  emotion  recognition  by  shortening"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "classification  performance  for \nthis  problem  during \nthe  signal  to  find  the  strongest  part  of  the  mutual"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "our experiments. \ninformation  and  obtained  the  result  of  89.84%  on"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "For different combination of physiological signals, \narousal \nand  89.61%  on  valence.  Therefore, \nthe"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "EEG with EOG and EMG have best results, the join \nmethods  proposed \nin \nthis  paper  get  higher  results"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "of  peripheral  physiological  signals  makes  the  result \nthan existing methods. We can see that our model has"
        },
        {
          "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et": "Ensemble emotion recognizing with multiple modal physiological signals\n11"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: RF and KNN do not show good [25] conducted emotion recognition by shortening",
      "data": [
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "combinations of the different  signals. Table 4 shows \nstable.  From  Fig.2  and  Fig.3,  we  can  find  that  the"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two-class task. Zoubi et"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "accuracy  for  each  class.  CART  performs  good  as \nal. [24] identified the human emotional state through"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "well,  whereas, by calculating  standard deviation,  the \nthe  LSM  model  with \nthe  accuracy  of  88.54%  on"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "result  is  more fluctuating than  ensemble  classifier  in \narousal  and  84.63%  on  valence.  Piho  and  Tjahjadi"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "Table \n3. \nRF \nand  KNN \ndo \nnot \nshow \ngood \n[25]\nconducted  emotion  recognition  by  shortening"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "classification  performance  for \nthis  problem  during \nthe  signal  to  find  the  strongest  part  of  the  mutual"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "our experiments. \ninformation  and  obtained  the  result  of  89.84%  on"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "For different combination of physiological signals, \narousal \nand  89.61%  on  valence.  Therefore, \nthe"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "EEG with EOG and EMG have best results, the join \nmethods  proposed \nin \nthis  paper  get  higher  results"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "of  peripheral  physiological  signals  makes  the  result \nthan existing methods. We can see that our model has"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "Ensemble emotion recognizing with multiple modal physiological signals\n11"
        },
        {
          "classification \neffect \nis  much \ngreater \nthan \nthe \ncombination  of  signal  and  classifier  performs  most": "Fig.5, \nthe \ninfluence  of  different  classifiers  on \nthe"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: RF and KNN do not show good [25] conducted emotion recognition by shortening",
      "data": [
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "Fig.5, \nthe \ninfluence  of  different  classifiers  on \nthe"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "(a)\n(b)"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.96\n0.96"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.95\n0.95"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.94\n0.94"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.93\n0.93"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.92\n0.92"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.91\n0.91"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.9\n0.9"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "EEG+CART\nEEG+EOG+CART\nEEG+ENS\nEEG+EOG+ENS"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "EEG+EMG+CART\nEEG+EOG+EMG+CART\nEEG+EMG+ENS\nEEG+EOG+EMG+ENS"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "(c)\n(d)"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.84\n0.91"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.83\n0.9"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.82\n0.89"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "combination  of  signal  and  classifier  performs  most \nclassification \neffect \nis  much \ngreater \nthan \nthe"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.81\n0.88"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "stable.  From  Fig.2  and  Fig.3,  we  can  find  that  the \ncombinations of the different signals. Table 4 shows"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "ensemble  classifier  has  a  very  good  classification \nsome existing researches for two class task. Zoubi et"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "0.78"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "accuracy  for  each  class.  CART  performs  good  as \nal  [24\nidentified the human emotional state through"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "well,  whereas, by calculating  standard deviation, the \nthe  LSM  model  with \nthe  accuracy  of  88.54%  on"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "EEG+KNN\nEEG+EOG+KNN\nEEG+RF\nEEG+EOG+RF"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "result is  more fluctuating than ensemble classifier in \narousal  and  84.63%  on  valence.  Piho  and  Tjahjadi"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n11": "Table \n3.  RF \nand  KNN \ndo \nnot \nshow \ngood \n25\nonducted  emotion  recognition  by  shortenin"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: RF and KNN do not show good [25] conducted emotion recognition by shortening",
      "data": [
        {
          "0.78": "accuracy  for  each  class.  CART  performs  good  as"
        },
        {
          "0.78": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
        },
        {
          "0.78": "well,  whereas, by calculating  standard deviation, the"
        },
        {
          "0.78": "EEG+KNN\nEEG+EOG+KNN"
        },
        {
          "0.78": "result is  more fluctuating than ensemble classifier in"
        },
        {
          "0.78": "Table \n3.  RF \nand  KNN \ndo \nnot \nshow \ngood"
        },
        {
          "0.78": "Fig.  4.  The  results  of  different  physiological  signals  on  classifiers  for  arousal  dimension.  (a)  different  physiological  signals  on  CART;  (b)"
        },
        {
          "0.78": "Fig. 4: The results of diﬀerent physiological signals on classiﬁers for arousal"
        },
        {
          "0.78": "our experiments."
        },
        {
          "0.78": "dimension. (a) diﬀerent physiological signals on CART;"
        },
        {
          "0.78": "(a)"
        },
        {
          "0.78": ""
        },
        {
          "0.78": "logical signals on ENS; (c) diﬀerent physiological signals on KNN; (d) diﬀerent"
        },
        {
          "0.78": "of  peripheral  physiological  signals  makes  the  result"
        },
        {
          "0.78": "physiological signals on RF."
        },
        {
          "0.78": "more stable than EEG only. Whereas, from Fig.4 and"
        },
        {
          "0.78": "0.93"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: RF and KNN do not show good [25] conducted emotion recognition by shortening",
      "data": [
        {
          "1\n2\n3\n4": "EEG+CART",
          "5": "",
          "6": "",
          "7\n8\n9\n10": "EEG+EOG+CART"
        },
        {
          "1\n2\n3\n4": "EEG+EMG+CART",
          "5": "",
          "6": "",
          "7\n8\n9\n10": "EEG+EOG+EMG+CART"
        },
        {
          "1\n2\n3\n4": "",
          "5": "(c)",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "",
          "5": "",
          "6": "",
          "7\n8\n9\n10": ""
        },
        {
          "1\n2\n3\n4": "1\n2\n3\n4",
          "5": "5",
          "6": "6",
          "7\n8\n9\n10": "7\n8\n9\n10"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: and Fig. 6 that for four types of emotional",
      "data": [
        {
          "4.3 Four-class task": "The emotion recognition classiﬁcation results for four-class task are displayed"
        },
        {
          "4.3 Four-class task": "in Table 5. From Table 5, we can know that our model performs good on four-"
        },
        {
          "4.3 Four-class task": "class task.\nIt can be seen that when the feature set\nis\nthe combined feature"
        },
        {
          "4.3 Four-class task": "set of EEG, EOG and EMG, the highest classiﬁcation accuracy is obtained by"
        },
        {
          "4.3 Four-class task": "using the ensemble classiﬁer, which is 90.74. It can be clearly seen from Figure"
        },
        {
          "4.3 Four-class task": "6 that when the same classiﬁcation model\nis selected and diﬀerent feature sets"
        },
        {
          "4.3 Four-class task": "are combined, the classiﬁcation results have little eﬀect; but when the same"
        },
        {
          "4.3 Four-class task": "feature\nset combination and diﬀerent\nclassiﬁcation models are selected,\nthe"
        },
        {
          "4.3 Four-class task": "classiﬁcation results are more obviously diﬀerent. Therefore, we can conclude"
        },
        {
          "4.3 Four-class task": "that when four-class task is performed on arousal and valence, the selection of"
        },
        {
          "4.3 Four-class task": "feature sets has less inﬂuence on the results, and the selection of classiﬁcation"
        },
        {
          "4.3 Four-class task": "models has a greater impact on the results. And while the feature set combined"
        },
        {
          "4.3 Four-class task": "by EEG, EOG and EMG, the ensemble classiﬁer is adopted, the model\nis the"
        },
        {
          "4.3 Four-class task": "most stable."
        },
        {
          "4.3 Four-class task": "It can be also seen from Table 5 and Fig. 6 that for four types of emotional"
        },
        {
          "4.3 Four-class task": "states, decision trees, random forests, and ensemble classiﬁer models have the"
        },
        {
          "4.3 Four-class task": "best recognition ability on HAHV, while KNN performs best on LAHV. For"
        },
        {
          "4.3 Four-class task": "the recognition of four emotional states, the combination of these three physi-"
        },
        {
          "4.3 Four-class task": "ological signals and the ensemble classiﬁer model obtain the best classiﬁcation"
        },
        {
          "4.3 Four-class task": "accuracy, namely, 88.81 on LALV, 91.58 on LAHV, 90.96 on HALV, 91.22 on"
        },
        {
          "4.3 Four-class task": "HAHV, respectively."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Experimental results for four-class task": "ﬁrst"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+ENS"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+KNN 80.66%"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+CART"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+RF"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+ENS"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+KNN"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+CART"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+RF"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EMG+ENS"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EMG+KNN"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EMG+CART"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EMG+RF"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+EMG+ENS"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+EMG+KNN"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+EMG+CART"
        },
        {
          "Table 5: Experimental results for four-class task": "EEG+EOG+EMG+RF"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nJing Zhang et al.": "for Liaoning Innovative Talents\nin University (No. LR2017044), and Dalian"
        },
        {
          "14\nJing Zhang et al.": "Science and Technology Innovation Fund (No.2018J12GX047)."
        },
        {
          "14\nJing Zhang et al.": "7 References"
        },
        {
          "14\nJing Zhang et al.": "References"
        },
        {
          "14\nJing Zhang et al.": "1.\nJ. Deng, S. Frhholz, Z. Zhang, and B. Schuller, Recognizing Emotions From Whispered"
        },
        {
          "14\nJing Zhang et al.": "Speech Based on Acoustic Feature Transfer Learning, IEEE Access, vol. 5, pp. 5235-5246,"
        },
        {
          "14\nJing Zhang et al.": "Mar. 2017, doi: 10.1109/ACCESS.2017.2672722."
        },
        {
          "14\nJing Zhang et al.": "2.\nS. Shojaeilangari, W.Y. Yau, K. Nandakumar, J. Li, and E.K. Teoh, Robust Repre-"
        },
        {
          "14\nJing Zhang et al.": "sentation and Recognition of Facial Emotions Using Extreme Sparse Learning,\nIEEE"
        },
        {
          "14\nJing Zhang et al.": "Transactions\non\nImage Processing,\nvol.\n24,\nno.\n7,\npp.\n2140-2152, Mar.\n2015,\ndoi:"
        },
        {
          "14\nJing Zhang et al.": "10.1109/TIP.2015.2416634."
        },
        {
          "14\nJing Zhang et al.": "3. Z. Yang and S.S. Narayanan, Modeling Dynamics of Expressive Body Gestures In Dyadic"
        },
        {
          "14\nJing Zhang et al.": "Interactions, IEEE Transactions on Aﬀective Computing, vol. 8, no. 3, pp. 369-381, 2017,"
        },
        {
          "14\nJing Zhang et al.": "doi: 10.1109/TAFFC.2016.2542812."
        },
        {
          "14\nJing Zhang et al.": "4. A.J. Casson, D.C. Yates, S.J. Smith, J.S. Duncan, and E. Rodriguez-Villegas, Wearable"
        },
        {
          "14\nJing Zhang et al.": "electroencephalography,\nIEEE Eng Med Biol Mag, vol. 29, no. 3, pp. 44-56, 2010."
        },
        {
          "14\nJing Zhang et al.": "5.\nJ.P. Rajan and S.E. Rajan, An internet of things based physiological signal monitoring"
        },
        {
          "14\nJing Zhang et al.": "and receiving system for virtual enhanced health care network, Technology and Health"
        },
        {
          "14\nJing Zhang et al.": "Care, vol. 26, no. 2, pp. 379-385, Jan. 2018."
        },
        {
          "14\nJing Zhang et al.": "6. C. Maaoui and A. Pruski, Unsupervised stress detection from remote physiological signal,"
        },
        {
          "14\nJing Zhang et al.": "Proc.\nIEEE International Conference on Industrial Technology (ICIT), Feb. 2018, doi:"
        },
        {
          "14\nJing Zhang et al.": "10.1109/ICIT.2018.8352409."
        },
        {
          "14\nJing Zhang et al.": "7.\nJ. He and B. Han, Non-contact sleep staging algorithm based on physiological signal mon-"
        },
        {
          "14\nJing Zhang et al.": "itoring, Proc. 4th World Conference on Control, Electronics and Computer Engineering"
        },
        {
          "14\nJing Zhang et al.": "(WCCECE 2018), pp. 308-314, Feb. 2018, doi: 10.25236/wccece.2018.63."
        },
        {
          "14\nJing Zhang et al.": "8.\nS.M.\nAlarcao\nand M.J.\nFonseca,\nEmotions\nrecognition\nusing\nEEG\nsignals:\nA"
        },
        {
          "14\nJing Zhang et al.": "Survey,\nIEEE\nTransactions\non Aﬀective\nComputing,\npreprint,\n12\nJun.\n2017,\ndoi:"
        },
        {
          "14\nJing Zhang et al.": "10.1109/TAFFC.2017.2714671. (PrePrint)"
        },
        {
          "14\nJing Zhang et al.": "9. Y. Zhong, M. Zhao, and Y. Wang, Recognition of emotions using multimodal physiolog-"
        },
        {
          "14\nJing Zhang et al.": "ical\nsignals and an ensemble deep learning model, Computer Methods and Programs in"
        },
        {
          "14\nJing Zhang et al.": "Biomedicine, vol. 140, pp. 93-110, Mar. 2017."
        },
        {
          "14\nJing Zhang et al.": "10.\nS. Jerritta, M. Murugappan, K. Wan, and S. Yaacob, Emotion recognition from facial"
        },
        {
          "14\nJing Zhang et al.": "EMG signals using higher order\nstatistics and principal component analysis, Journal of"
        },
        {
          "14\nJing Zhang et al.": "the Chinese Institute of Engineers, vol. 37, no. 3, pp. 385-394, 2014."
        },
        {
          "14\nJing Zhang et al.": "11. M. Nabian, Y. Yin, J. Wormwood, K.S. Quigley, L.F. Barrett, and S. Ostadabbas, An"
        },
        {
          "14\nJing Zhang et al.": "Open-Source Feature Extraction Tool\nfor the Analysis of Peripheral Physiological Data,."
        },
        {
          "14\nJing Zhang et al.": "IEEE Journal of Translational Engineering in Health and Medicine, vol. 6, Oct. 2018, doi:"
        },
        {
          "14\nJing Zhang et al.": "10.1109/JTEHM.2018.2878000."
        },
        {
          "14\nJing Zhang et al.": "12.\nS. Lin, J.Y. Xie, M.Y. Yang, and Z.Y. Li, A review of emotion recognition using physi-"
        },
        {
          "14\nJing Zhang et al.": "ological signals, Sensors, vol. 18, no. 7, pp. 2074-2114, 2018."
        },
        {
          "14\nJing Zhang et al.": "13. Y. Wang, W. Zhang, L Wu, X Lin, X Zhao, Unsupervised metric fusion over multiview"
        },
        {
          "14\nJing Zhang et al.": "data by graph random walk-based cross-view diﬀusion,\nIEEE Transactions\non Neural"
        },
        {
          "14\nJing Zhang et al.": "Networks and Learning Systems 28 (1), 57-70, 2017."
        },
        {
          "14\nJing Zhang et al.": "14. Y. Wang et al.,\nIterative Views Agreement: An Iterative Low-Rank based Structured"
        },
        {
          "14\nJing Zhang et al.": "Optimization Method to Multi-View Spectral Clustering, IJCAI 2016."
        },
        {
          "14\nJing Zhang et al.": "15. Y. Wang et al., Multiview Spectral Clustering via Structured Low-Rank Matrix Factor-"
        },
        {
          "14\nJing Zhang et al.": "ization. IEEE Trans. Neural Networks and Learning Systems, 2018."
        },
        {
          "14\nJing Zhang et al.": "16. W. Wen, G. Liu, N. Cheng, J. Wei, P. Shangguan, and W. Huang, Emotion recognition"
        },
        {
          "14\nJing Zhang et al.": "based on multi-variant correlation of physiological signals, IEEE Transactions on Aﬀective"
        },
        {
          "14\nJing Zhang et al.": "Computing, vol. 5, no. 2, pp. 126-140, 2014."
        },
        {
          "14\nJing Zhang et al.": "17. P. Das, A. Khasnobish, and D.N. Tibarewala, Emotion recognition employing ECG and"
        },
        {
          "14\nJing Zhang et al.": "GSR signals as markers of ANS, Proc. Conference on the Advances in Signal Processing,"
        },
        {
          "14\nJing Zhang et al.": "Pune, India, pp. 37-42, 2016."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "18. W.Y. Hsu, EEG-based motor\nimagery classiﬁcation using neuro-fuzzy prediction and"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "wavelet\nfractal\nfeatures, Journal of Neuroscience Methods, vol. 189, no. 2, pp. 295-302,"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "June 2010."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "19. Z. Mohammadi, J. Frounchi, and M. Amiri, Wavelet-based emotion recognition system"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "using EEG signal, Neural Computing and Applications, vol. 28, no. 8, pp. 1985-1990, Aug."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "2017."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "20. Y. Zhang, S.H. Zhang, and X.M. Ji, EEG-based classiﬁcation of emotions using empirical"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "mode decomposition and autoregressive model, Multimedia Tools and Applications, vol."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "77, no. 20, pp. 26697-26710, Oct. 2018."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "21. Y. Zhang, X.M. Ji, B. Liu, D. Huang, F.D. Xie, and Y.T. Zhang, Combined feature"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "extraction method for classiﬁcation of EEG signals, Neural Computing and Applications,"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "vol. 28, no. 11, pp. 3153-3161, 2017."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "22. Y. Wang et al., Clustering via geometric median shift over Riemannian manifolds. In-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "formation Sciences 220, 292-305, 2013."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "23. L. Wu, R. Hong, Y. Wang, M Wang. Cross-Entropy Adversarial View Adaptation for"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Person Re-identiﬁcation.\nIEEE Transactions on Circuits and Systems for Video Technol-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "ogy, 2019."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "24. L. Wu, Y. Wang, H. Yin et al., Few-Shot Deep Adversarial Learning for Video-based"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Person Re-identiﬁcation, IEEE Transactions on Image Processing, 29 (1), 1233-1245, 2020."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "25. L. Wu, Y. Wang, X. Li, J. Gao. Deep Attention-based Spatially Recursive Networks for"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Fine-Grained Visual Recognition.\nIEEE Transactions on Cybernetics 49 (5), 1791-1802,"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "2019."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "26. L. Wu, Y. Wang, L. Shao. Cycle-Consistent Deep Generative Hashing for Cross-Modal"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Retrieval. IEEE Transactions on Image Processing, 28 (4), 1602-1612, 2019."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "27. Z.H. Zhou, Ensemble Learning,\nin: S.Z. Li, A.K. Jain (eds) Encyclopedia of Biometrics."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Springer, Boston, MA, 2015."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "28.\nS. Koelstra, C. Muhl, and M. Soleymani, DEAP: A Database\nfor Emotion Analysis"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Using Physiological Signals, IEEE Transactions on Aﬀective Computing, vol. 3, no. 1, pp."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "18-31, Dec. 2011, doi: 10.1109/T-AFFC.2011.15."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "29. N.A. Pashtoon,\nIIR Digital Filters,\nin: D.F. Elliott\n(eds) Handbook of Digital Signal"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Processing. Academic Press, pp. 289-357, 1987."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "30. B. Hjorth, ”EEG analysis based on time domain properties,” Electroencephalography"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "and Clinical Neurophysiology, vol. 29, no. 3, pp. 306-310, Sep. 1970, doi: 10.1016/0013-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "4694(70)90143-4."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "31. A. Chatchinarat, K.W. Wong, and C.C. Fung, A comparison study on the relationship"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "between the\nselection of EEG electrode\nchannels and frequency bands used in classiﬁ-"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "cation for emotion recognition, Proc.\nInternational Conference on Machine Learning &"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Cybernetics (ICMLC), pp. 251-256, February 2017, doi: 10.1109/ICMLC.2016.7860909."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "32. O.A. Zoubi, M. Awad, and N.K. Kasabov, Anytime multipurpose emotion recognition"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "from EEG data using a Liquid State Machine based framework, Artiﬁcial Intelligence in"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "Medicine, vol. 86, pp. 1-8, Jan. 2018, doi: 10.1016/j.artmed.2018.01.001."
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "33. L. Piho and T. Tjahjadi, A mutual information based adaptive windowing of informative"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "EEG for emotion recognition, IEEE Transactions on Aﬀective Computing, May 2018, doi:"
        },
        {
          "Ensemble emotion recognizing with multiple modal physiological signals\n15": "10.1109/TAFFC.2018.2840973. (PrePrint)"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recognizing Emotions From Whispered Speech Based on Acoustic Feature Transfer Learning",
      "authors": [
        "J Deng",
        "S Frhholz",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2017.2672722"
    },
    {
      "citation_id": "2",
      "title": "Robust Representation and Recognition of Facial Emotions Using Extreme Sparse Learning",
      "authors": [
        "S Shojaeilangari",
        "W Yau",
        "K Nandakumar",
        "J Li",
        "E Teoh"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2015.2416634"
    },
    {
      "citation_id": "3",
      "title": "Modeling Dynamics of Expressive Body Gestures In Dyadic Interactions",
      "authors": [
        "Z Yang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2542812"
    },
    {
      "citation_id": "4",
      "title": "IEEE Eng Med Biol Mag",
      "authors": [
        "A Casson",
        "D Yates",
        "S Smith",
        "J Duncan",
        "E Rodriguez-Villegas",
        "Wearable Electroencephalography"
      ],
      "year": "2010",
      "venue": "IEEE Eng Med Biol Mag"
    },
    {
      "citation_id": "5",
      "title": "An internet of things based physiological signal monitoring and receiving system for virtual enhanced health care network",
      "authors": [
        "J Rajan",
        "S Rajan"
      ],
      "year": "2018",
      "venue": "Technology and Health Care"
    },
    {
      "citation_id": "6",
      "title": "Unsupervised stress detection from remote physiological signal",
      "authors": [
        "C Maaoui",
        "A Pruski"
      ],
      "year": "2018",
      "venue": "Proc. IEEE International Conference on Industrial Technology (ICIT)",
      "doi": "10.1109/ICIT.2018.8352409"
    },
    {
      "citation_id": "7",
      "title": "Non-contact sleep staging algorithm based on physiological signal monitoring",
      "authors": [
        "J He",
        "B Han"
      ],
      "year": "2018",
      "venue": "Proc. 4th World Conference on Control, Electronics and Computer Engineering (WCCECE 2018)",
      "doi": "10.25236/wccece.2018.63"
    },
    {
      "citation_id": "8",
      "title": "Emotions recognition using EEG signals: A Survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing, preprint",
      "doi": "10.1109/TAFFC.2017.2714671"
    },
    {
      "citation_id": "9",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Y Zhong",
        "M Zhao",
        "Y Wang"
      ],
      "year": "2017",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from facial EMG signals using higher order statistics and principal component analysis",
      "authors": [
        "S Jerritta",
        "M Murugappan",
        "K Wan",
        "S Yaacob"
      ],
      "year": "2014",
      "venue": "Journal of the Chinese Institute of Engineers"
    },
    {
      "citation_id": "11",
      "title": "An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data",
      "authors": [
        "M Nabian",
        "Y Yin",
        "J Wormwood",
        "K Quigley",
        "L Barrett",
        "S Ostadabbas"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Translational Engineering in Health and Medicine",
      "doi": "10.1109/JTEHM.2018.2878000"
    },
    {
      "citation_id": "12",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "S Lin",
        "J Xie",
        "M Yang",
        "Z Li"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised metric fusion over multiview data by graph random walk-based cross-view diffusion",
      "authors": [
        "Y Wang",
        "W Zhang",
        "X Wu",
        "X Lin",
        "Zhao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "14",
      "title": "Iterative Views Agreement: An Iterative Low-Rank based Structured Optimization Method to Multi-View Spectral Clustering",
      "authors": [
        "Y Wang"
      ],
      "year": "2016",
      "venue": "Iterative Views Agreement: An Iterative Low-Rank based Structured Optimization Method to Multi-View Spectral Clustering"
    },
    {
      "citation_id": "15",
      "title": "Multiview Spectral Clustering via Structured Low-Rank Matrix Factorization",
      "authors": [
        "Y Wang"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Neural Networks and Learning Systems"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition based on multi-variant correlation of physiological signals",
      "authors": [
        "W Wen",
        "G Liu",
        "N Cheng",
        "J Wei",
        "P Shangguan",
        "W Huang"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition employing ECG and GSR signals as markers of ANS",
      "authors": [
        "P Das",
        "A Khasnobish",
        "D Tibarewala"
      ],
      "year": "2016",
      "venue": "Proc. Conference on the Advances in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "EEG-based motor imagery classification using neuro-fuzzy prediction and wavelet fractal features",
      "authors": [
        "W Hsu"
      ],
      "year": "2010",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "19",
      "title": "Wavelet-based emotion recognition system using EEG signal",
      "authors": [
        "Z Mohammadi",
        "J Frounchi",
        "M Amiri"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "20",
      "title": "EEG-based classification of emotions using empirical mode decomposition and autoregressive model",
      "authors": [
        "Y Zhang",
        "S Zhang",
        "X Ji"
      ],
      "year": "2018",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "21",
      "title": "Combined feature extraction method for classification of EEG signals",
      "authors": [
        "Y Zhang",
        "X Ji",
        "B Liu",
        "D Huang",
        "F Xie",
        "Y Zhang"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "22",
      "title": "Clustering via geometric median shift over Riemannian manifolds",
      "authors": [
        "Y Wang"
      ],
      "year": "2013",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "23",
      "title": "Cross-Entropy Adversarial View Adaptation for Person Re-identification",
      "authors": [
        "L Wu",
        "R Hong",
        "Y Wang",
        "M Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "24",
      "title": "Few-Shot Deep Adversarial Learning for Video-based Person Re-identification",
      "authors": [
        "L Wu",
        "Y Wang",
        "H Yin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "25",
      "title": "Deep Attention-based Spatially Recursive Networks for Fine-Grained Visual Recognition",
      "authors": [
        "L Wu",
        "Y Wang",
        "X Li",
        "J Gao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "26",
      "title": "Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval",
      "authors": [
        "L Wu",
        "Y Wang",
        "L Shao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "27",
      "title": "Ensemble Learning",
      "authors": [
        "Z Zhou"
      ],
      "year": "2015",
      "venue": "Encyclopedia of Biometrics"
    },
    {
      "citation_id": "28",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "29",
      "title": "Handbook of Digital Signal Processing",
      "authors": [
        "N Pashtoon"
      ],
      "year": "1987",
      "venue": "Handbook of Digital Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "EEG analysis based on time domain properties",
      "authors": [
        "B Hjorth"
      ],
      "year": "1970",
      "venue": "Electroencephalography and Clinical Neurophysiology",
      "doi": "10.1016/0013-4694"
    },
    {
      "citation_id": "31",
      "title": "A comparison study on the relationship between the selection of EEG electrode channels and frequency bands used in classification for emotion recognition",
      "authors": [
        "A Chatchinarat",
        "K Wong",
        "C Fung"
      ],
      "year": "2017",
      "venue": "Proc. International Conference on Machine Learning & Cybernetics (ICMLC)",
      "doi": "10.1109/ICMLC.2016.7860909"
    },
    {
      "citation_id": "32",
      "title": "Anytime multipurpose emotion recognition from EEG data using a Liquid State Machine based framework",
      "authors": [
        "O Zoubi",
        "M Awad",
        "N Kasabov"
      ],
      "year": "2018",
      "venue": "Artificial Intelligence in Medicine",
      "doi": "10.1016/j.artmed.2018.01.001"
    },
    {
      "citation_id": "33",
      "title": "A mutual information based adaptive windowing of informative EEG for emotion recognition",
      "authors": [
        "L Piho",
        "T Tjahjadi"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2840973"
    }
  ]
}