{
  "paper_id": "2310.16676v3",
  "title": "Sslcl: An Efficient Model-Agnostic Supervised Contrastive Learning Framework For Emotion Recognition In Conversations",
  "published": "2023-10-25T14:41:14Z",
  "authors": [
    "Tao Shi",
    "Xiao Liang",
    "Yaoyuan Liang",
    "Xinyi Tong",
    "Shao-Lun Huang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing community, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and modelagnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label representations by projecting discrete labels into dense embeddings through a shallow multilayer perceptron, and formulate the training objective to maximize the similarity between sample features and their corresponding ground-truth label embeddings, while minimizing the similarity between sample features and label embeddings of disparate classes. Moreover, we innovatively adopt the Soft-HGR maximal correlation as a measure of similarity between sample features and label embeddings, leading to significant performance improvements over conventional similarity measures. Additionally, multimodal cues of utterances are effectively leveraged by SSLCL as data augmentations to boost model performances. Extensive experiments on two ERC benchmark datasets, IEMO-CAP and MELD, demonstrate the compatibility and superiority of our proposed SSLCL framework compared to existing state-of-the-art SCL methods. Our code is available at https://github.com/TaoShi1998/SSLCL.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversations (ERC) aims to accurately identify the emotion conveyed in each utterance during a conversation. The widespread potential applications in areas such as opinion mining  (Majumder et al. 2019) , empathetic chatbots (Zhang, Chen, and Chen 2023), and medical diagnosis  (Hu et al. 2021 ) make ERC an essential task within the field of natural language processing. * These authors contributed equally. † Corresponding author. To address the problem of ERC, an increasing number of studies  (Tu et al. 2022; Arumugam, Bhattacharjee, and Yuan 2022; Li, Yan, and Qiu 2022; Song et al. 2022; Shi and Huang 2023; Yang et al. 2023a,b; Tu et al. 2023 ) have leveraged supervised contrastive learning (SCL)  (Khosla et al. 2020)  to learn robust and generalized feature representations that are better suited for emotion classifications. Supervised contrastive learning is a representation learning technique that aims to enhance the robustness of learned representations through pulling samples belonging to the same category (positive pairs) closer, while simultaneously pushing apart samples from disparate classes (negative pairs). Currently, SCL-based approaches in ERC can be broadly classified into two categories: (1) Vanilla supervised contrastive loss (SupCon loss)  (Tu et al. 2022; Arumugam, Bhattacharjee, and Yuan 2022; Yang et al. 2023a ); (2) Variants of the SupCon loss that integrate SCL with other learning strategies, including prototypical networks and curriculum learning  (Song et al. 2022) , multiview representation learning  (Li, Yan, and Qiu 2022) , hard example mining  (Shi and Huang 2023) , self-supervised contrastive learning  (Tu et al. 2023) , and emotion prototypes  (Yang et al. 2023b) .\n\nDespite the surging popularity of applying SCL to ERC, several challenges still remain unresolved: (1) ERC models using the vanilla SupCon loss typically require a large batch size to achieve satisfactory performances, which is computationally expensive. Existing widely-used ERC benchmark datasets, such as IEMOCAP  (Busso et al. 2008)  and MELD  (Poria et al. 2019) , are plagued by the class im-balance problem. As illustrated in Figure  1 , Both IEMOCAP and MELD are class-imbalanced, with MELD being particularly skewed towards certain classes. Thus, when utilizing SupCon on these datasets, a large batch size is often needed to guarantee that each training sample from minority classes has at least one positive pair within the batch, otherwise the loss can not be properly calculated, leading to a significant degradation in model performance  (Shi and Huang 2023) .\n\n(2) Variants of the SupCon loss tend to have difficulty integrating with most existing ERC approaches. While combining SCL with other learning techniques allows many SupCon variants to overcome the constraint of a large batch size, these SupCon variants tend to be incompatible with the majority of existing ERC frameworks, since they depend on specific modeling assumptions  (Song et al. 2022; Tu et al. 2023; Yang et al. 2023b ) that can not be easily extended to other models. Moreover, SupCon variants often suffer from an excessive level of complexity. (3) The inherent multimodal nature of utterances in ERC has not been leveraged by existing SCL-based algorithms. A distinctive characteristic of the ERC task is that each utterance is accompanied by its corresponding textual, audio and visual modalities. However, the potential of leveraging multimodal cues has remained unexplored by existing SCL literature.\n\nTo tackle the aforementioned challenges, in this paper, we propose a novel SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which is an efficient and model-agnostic approach that eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Additionally, multimodal information is effectively leveraged by SSLCL as data augmentation to achieve more promising performances.\n\nSpecifically, the key insight of our work is a fundamental shift in how labels are utilized: in contrast to the prevalent use of one-hot vector label representations in existing SCL-based approaches, we embed each discrete emotion category into a dense label embedding through a shallow multilayer perceptron, and utilize the learned label embedding of a specific class as the \"ground-truth representation\" for samples belonging to that class. Then, we formulate the training objective to maximize the similarity between sample features and their corresponding ground-truth label embeddings (positive sample-label pairs), while minimizing the similarity between sample features and label embeddings of different classes (negative sample-label pairs). Furthermore, inspired by Cutout (DeVries and Taylor 2017), a commonlyused image augmentation technique in the computer vision community, we utilize multimodal views of the same sample as its data augmentations. Consequently, each training sample is guaranteed to have multiple positive sample-label pairs within the batch, regardless of the batch size. (Lee and Lee 2022)\n\nAnother notable contribution of SSLCL that distinguishes it from existing SCL methods is the adaptation of the soft-Hirschfeld-Gebelein-Rényi (Soft-HGR) maximal correlation  (Wang et al. 2019 ) as a measure of similarity between sample features and label embeddings. Soft-HGR aims to extract maximally correlated representations across multiple random variables. In our SSLCL setting, we utilize Soft-HGR as a similarity measure to capture the complex correlations between sample-label pairs, leading to significant performance improvements over conventional similarity measures (refer to the Results and Analysis section).\n\nFinally, to demonstrate the compatibility and superiority of SSLCL, we conduct extensive experiments on two ERC benchmark datasets: IEMOCAP and MELD. Experimental results provide compelling evidence that SSLCL exhibits three key advantages against existing SCL literature: (1) SSLCL can be easily integrated with existing ERC models using any methodologies, without introducing any modelspecific assumptions; (2) ERC methods using SSLCL can achieve excellent performances without the need for a large batch size; (3) The utilization of SSLCL yields significant improvements in model performances, surpassing existing SCL-based approaches and achieving new state-of-the-art results on both IEMOCAP and MELD.\n\nTo summarize, the main contributions of this work are four-fold: (1) Through a novel utilization of label representations, we effectively address the constraints posed by large batch sizes and incompatibility with most existing ERC architectures encountered in current SCL-based methods; (2) To the best of our knowledge, we are the first in the SCL community to leverage Soft-HGR as a measure of similarity, which yields considerable performance improvements over traditional similarity measures; (3) We innovatively leverage multimodal information as data augmentation to enhance model performances; (4) Extensive experiments on two ERC benchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and superiority of SSLCL compared with existing state-of-the-art SCL approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Supervised Contrastive Learning",
      "text": "Supervised contrastive learning  (Khosla et al. 2020 ) is a representation learning technique that extends self-supervised contrastive learning (self-supervised CL)  (Wu et al. 2018 ) by leveraging label information, in which samples from the same class are pulled closer while samples belonging to different classes are pushed apart within the batch.\n\nRecently, an increasing number of ERC models have utilized the vanilla SupCon loss to enhance model performances. For example, Sentic GAT  (Tu et al. 2022)  leverages SupCon to discriminate context-free and contextsensitive representations.  Arumugam, Bhattacharjee, and Yuan (2022)  employs SupCon to enhance cross-modal consistency. SCMM  (Yang et al. 2023a ) utilizes the SupCon loss to improve the discriminability of learned multimodal features. However, vanilla SupCon in ERC typically requires a large batch size to achieve satisfactory performances, rendering it computationally expensive.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversations",
      "text": "Recurrence-based Methods BC-LSTM  (Poria et al. 2017)  captures conversational contexts based on long shortterm memory (LSTM) networks.  DialogueRNN (Majumder et al. 2019 ) employs three gated recurrent units (GRUs) to model the context and keep track of speaker states. Inspired by the cognitive theory of emotion, DialogueCRN  (Hu, Wei, and Huai 2021)  introduces a cognitive perspective to comprehend contextual information based on LSTMs.\n\nTransformer-based Methods HiTrans  (Li et al. 2020 ) utilizes two hierarchical transformers to capture local-and global-level utterance representations. EmoBERTa  (Kim and Vossen 2021)  learns inter-and intra-speaker states and conversational contexts through fine-tuning a pre-trained RoBERTa. CoG-BART is proposed by  Li, Yan, and Qiu (2022) , which leverages a pretrained encoder-decoder model BART to capture contextual information. CoMPM (Lee and Lee 2022) utilizes a transformer-encoder based context model and a speaker-aware pretrained memory module to capture conversational contexts.\n\nGraph-based Methods DialogueGCN  (Ghosal et al. 2019 ) models conversations based on a graph convolutional network (GCN). DAG-ERC  (Shen et al. 2021 ) encodes utterances through a directed acyclic graph (DAG) to better understand the inherent structure within a conversation. M3Net (Chen et al. 2023) leverages graph neural networks (GNNs) to capture multivariate relationships among multiple modalities and conversational contexts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Scl-Based Methods",
      "text": "The increasing popularity of SCL has led to numerous attempts to combine SCL with other learning strategies. To illustrate, SPCL  (Song et al. 2022)  integrates SCL with prototypical networks and curriculum learning to alleviate the imbalanced class problem without requiring a large batch size. However, SPCL is incompatible with most existing ERC models because its difficulty measure function requires the emotion representations learned by the ERC model to be distance-aware. CoG-BART  (Li, Yan, and Qiu 2022)  introduces multiview-SupCon to addresses the batch size constraint associated with SupCon, in which each sample is paired with a gradient-detached copy. Nevertheless, a significant drawback of multiview-SupCon is that it calculates the similarity between identical samples, which can lead to suboptimal representations and potentially hinder the model's ability to generalize effectively. Multi-EMO (Shi and Huang 2023) introduces a sample-weighted focal contrastive (SWFC) loss, which combines SCL with hard sample mining to alleviate the difficulty of recognizing minority and semantically similar emotions. However, similar to vanilla SCL, the SWFC loss also relies on a significantly large batch size to achieve desirable results. CKCL  (Tu et al. 2023)  integrates SCL with self-supervised CL to differentiate context-and knowledge-independent utterances. Nonetheless, a major limitation of CKCL is it can not be applied to most ERC models that do not rely on external knowledge. SCCL  (Yang et al. 2023b ) combines SCL with valence-arousal-dominance (VAD) emotion Prototypes to boost model performances. However, SCCL only considers context-independent word-level VAD, whereas the majority of existing ERC approaches are context-dependent.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Preliminary Soft-Hgr Maximum Correlation",
      "text": "Soft-Hirschfeld-Gebelein-Rényi (Soft-HGR) maximum correlation  (Wang et al. 2019 ) is an extension of the HGR maximum correlation  (Hirschfeld 1935; Gebelein 1941; Rényi 1959) , which is a measure of dependence between random variables. Soft-HGR preserves the same feature geometry as in the HGR maximal correlation, while simultaneously avoiding strict whitening constraints imposed by HGR.\n\nSoft-HGR aims to extract maximally correlated feature representations from multiple random modalities. Formally, given two random variables represented by X and Y , the objective of Soft-HGR is to learn nonlinear feature mappings f(X) and g(Y ) that are maximally correlated. The Soft-HGR objective is defined as follows:\n\n(1)\n\nAs illustrated in equation 1, Soft-HGR consists of two inner products: one between feature mappings and another between feature covariances. The first inner product is consistent with the objective of the HGR maximal correlation, while the second one serves as a soft regularizer that replaces the hard whitening constraints in HGR.\n\nFrom an information theory perspective, the optimal nonlinear feature transformations f(X) and g(Y ) learned through Soft-HGR capture the maximum amount of mutual information shared by random variables X and Y , rendering them highly informative and well-suited for downstream discriminative tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology Problem Definition",
      "text": "Given a conversation which consists of n utterances u 1 , u 2 , . . . , u n expressed by speakers S u1 , S u2 , . . . , S un , the objective of ERC is to assign an emotion label from a predefined K-class emotion category set Y to each utterance in this conversation. Each utterance is accompanied by its corresponding textual (t), audio (a) and visual (v) modalities, which can be represented as follows:\n\nwhere",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overview Of The Proposed Sslcl Framework",
      "text": "The overall framework of SSLCL is illustrated in Figure  2 , which is made up of three key components: sample feature extraction, label learning, and sample-label contrastive learning. We discuss each component in detail as follows.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sample Feature Extraction",
      "text": "A major advantage of SSLCL over established SCL literature is its ability to seamlessly integrate with existing ERC approaches without introducing any model-specific assumptions. As shown in Figure  2 , this is achieved through a simple yet effective approach: for any ERC model, we extract This is my wedding.\n\nOh, do you need a hug? You don't have to bring me anything! . . .  the sample features from its second-last hidden layer and contrast them with label embeddings learned through a label embedding network, which is jointly optimized with the ERC model during training. There are no architectural modifications or modeling assumptions required in this process.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Label Learning",
      "text": "In contrast to the commonly adopted one-hot vector label representations in existing SCL-based approaches, our proposed SSLCL framework innovatively projects each discrete emotion category into a dense embedding through a simple yet powerful two-layer multilayer perceptron (MLP), which consists of a embedding layer, a fully-connected layer and a rectified linear unit (ReLU). Although a deeper and more complex label embedding network  (Sun et al. 2017 ) could be adopted, experimental results demonstrate that a twolayer shallow MLP is sufficient to achieve satisfactory performance (refer to Appendix D). Specifically, given a K-class discrete emotion category set Y = {1, 2, . . . , K}, the label embedding g i ∈ R d for the i-th emotion label is obtained as follows:\n\n(3)\n\n1 Our SSLCL framework can be applied to ERC models with any modalities. We use the multimodal case in Figure  2  in order to illustrate how data augmentation works.\n\nwhere e i ∈ R de is the hidden output, W g ∈ R d×de is the weight matrix, and b g ∈ R d is the bias parameter.\n\nAfter projecting emotion labels into label embeddings, we utilize the label embedding g i as the \"ground-truth representation\" for samples belonging to class i, and formulate the training objective to maximize the similarity between sample features and their corresponding ground-truth label embeddings, while simultaneously minimizing the similarity between sample features and label embeddings of different classes. This novel formulation guarantees that each training sample is accompanied by at least one positive feature-label pair within each batch, regardless of the batch size.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sample-Label Contrastive Learning",
      "text": "Soft-HGR as a Measure of Similarity Soft-HGR aims to learn maximally correlated feature representations from different random variables. Motivated by the success of Soft-HGR in transfer learning  (Bao et al. 2019 ) and multimodal learning  (Ma, Huang, and Zhang 2021; Shi and Huang 2023) , we innovatively apply Soft-HGR to SCL as a similarity measure to effectively capture the underlying correlations between sample features and label embeddings. Specifically, by regarding input samples and their corresponding emotion labels as two distinct random variables, we treat sample features and label embeddings as their respective feature mappings, which allows us to leverage Soft-HGR to learn sample features and label embeddings that are maximally cor-\n\nBased on the empirical Soft-HGR objective provided by  Wang et al. (2019) , the Soft-HGR similarity between zero-meaned 2 F and G z can be calculated as follows:\n\n(5) Based on Equation  5 , the Soft-HGR similarity between a 2 This corresponds to the zero-mean constraint in Soft-HGR. single sample-label pair f i and g zi can be written as follows:\n\nPositive Sample-Label Pairs A pair (f i , g zi ) is referred to as a positive sample-label pair if z i is the ground-truth label of f i . SSLCL aims to maximize the correlation between positive sample-label pairs. However, only one positive sample-label pair is available for each training sample, which could potentially hinder the generalizability and robustness of learned features. To address this limitation, inspired by Cutout (DeVries and Taylor 2017), a widely-used image augmentation technique in the computer vision community which masks out partial regions of input images during training, we leverage the multimodal nature of ERC utterances to generate additional positive sample-label pairs. Specifically, we consider utterances with one or two missing modalities as partially masked examples of the original input utterances, and utilize their corresponding learned sample features as augmentations of the original sample feature.  (3) f i (t + v): the audio modality is missing. Importantly, our data augmentation technique does not alter the multimodal fusion mechanism employed by the ERC model. Although the majority of existing ERC approaches make use of multimodal information, it is worth noting that several ERC methods are either bimodal-based  (Luo, Phan, and Reiss 2023)  or solely text-based  (Li et al. 2020; Kim and Vossen 2021; Li, Yan, and Qiu 2022) . In the case of bimodal ERC approaches, data augmentation is limited to the textual modality. Conversely, for text-based ERC models, data augmentation is not performed.\n\nThe augmented positive sample-label loss for the i-th sample can be calculated as follows:\n\nwhere α is a positive focusing parameter that forces the model to focus on hard positive examples,\n\naug is the set of augmented sample features for f i . In the multimodal case,\n\nThe text-based unimodal scenario has an empty augmentation set.\n\nNegative Sample-Label Pairs A pair (f i , g zi ) is referred to as a negative sample-label pair if z i is not the ground-truth label of f i . Unlike existing SCL approaches that do not directly compute the loss from negative pairs, SSLCL explicitly minimizes the similarity between negative sample-label pairs, which leads to improved model performances (refer to the Results and Analysis section). The negative sample-label loss for the i-th sample can be calculated as follows:\n\nwhere β is a negative focusing parameter that assigns more focus to hard negative samples, G\n\nLabel-Label Discrimination In addition to computing the loss of sample-label pairs, we introduce an auxiliary loss to minimize the similarity between all pairs of non-identical label embeddings. Experiments demonstrate that explicitly encouraging distinct representations for different label embeddings leads to considerable improvements in model performances (refer to the Results and Analysis section). The auxiliary loss is defined as follows:\n\nwhere the • symbol represents dot product, G i = {g j |j ̸ = i} is the set of label embeddings other than g i . As shown in Equation  12 , when calculating the similarity probability between g i and g j ∈ G i , we explicitly set the similarity between g i and itself to 0 to prevent the value from becoming disproportionately large compared to dot products with other label embeddings, thus avoiding the softmax function from encountering regions with extremely small gradients.\n\nSSLCL Loss In summary, the SSLCL loss for a batch of N samples can be calculated as follows:\n\nwhere n yi is the count of label y i within the batch, γ is a sample-weight parameter that assigns higher weights to minority emotions, λ is a trade-off hyperparameter between the sample-label loss and the label-label loss. Table  4 : Ablation study of SSLCL on IEMOCAP and MELD to evaluate the impact of different label embedding (LE) models on model performances. The three-layer MLP comprises an embedding layer followed by two fully-connected layers, while the Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer MLP and Bi-GRU are tuned on the validation set.\n\nCross-Entropy Loss Apart from the SSLCL loss, we also adopt a cross-entropy (CE) loss to minimize the difference between predicted probabilities and ground-truth labels. Let p i denote the probability distribution of emotion classes for f i , the CE loss can be defined as follows:\n\nOverall Training Objective The overall training objective is a linear combination of the SSLCL loss and the CE loss, which is defined as follows:\n\nwhere η is a trade-off hyperparameter between the SSLCL loss and the CE loss.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings",
      "text": "Datasets IEMOCAP The IEMOCAP dataset  (Busso et al. 2008 ) consists of around 12 hours of dyadic conversation videos, which are divided into 151 dialogues with a total of 7433 individual utterances. Each utterance in IEMOCAP is annotated with one of six emotion labels: happy, sad, neutral, angry, excited, and frustrated.\n\nMELD The MELD dataset  (Poria et al. 2019 ) is a collection of multi-party conversations extracted from the TV series Friends, which consists of 1433 dialogues and a total of 13708 individual utterances. Each utterance in MELD is annotated with one of seven emotion categories: angry, disgust, fear, joy, neutral, sad, and surprise.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Methods",
      "text": "To demonstrate the compatibility and superiority of SSLCL compared to existing SCL methods, we conduct a comprehensive comparison between SSLCL and existing SCL literature in ERC, which can be categorized into two groups:\n\n(1) Compatible SCL methods: SCL approaches that can be integrated with the majority of existing ERC models, including vanilla SupCon loss  (Khosla et al. 2020) , multiview-SupCon (mv-SupCon) loss  (Li, Yan, and Qiu 2022) , and SWFC loss  (Shi and Huang 2023) ; (2) Incompatible SCL methods: SCL frameworks that are incompatible with most existing ERC models, including SPCL  (Song et al. 2022) , CKCL  (Tu et al. 2023) , and SCCL  (Yang et al. 2023b ). Specifically, the effectiveness of SSLCL and compatible SCL approaches are compared on the following categories As for incompatible SCL methods, due to the difficulty of applying them to most existing ERC models, we simply compare SSLCL with their original implementations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "Evaluation Metrics Following  Shi and Huang (2023) , we use the weighted-average F1 (w-F1) score to evaluate model performances on both IEMOCAP and MELD. All reported results are averaged over 5 random seeds.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Modality Settings",
      "text": "In order to demonstrate the effectiveness of SSLCL across different modality settings, we leverage textual, audio and visual modalities for DialogueRNN, DialogueCRN and M3Net. Conversely, for text-based ERC models, specifically HiTrans, COG-BART and DAG-ERC, we solely utilize the textual modality.\n\nHyperparameter Settings (1) SSLCL Hyperparameters: SSLCL requires minimal hyperparameter tuning across different ERC models. Specifically, the positive and negative focusing parameters α and β are empirically set to 2.0 and 0.5 respectively, the sample-weight parameter γ is chosen among {0.5, 1.0, 1.5}, the label loss trade-off hyperparameter γ is selected among {0.5, 1.0, 2.0}, the CE tradeoff hyperparameter η is set to 1.0, the label embedding network is optimized using Adam (Kingma and Ba 2014) with a learning rate of 10 -5 , and its hidden dimension is designed to be twice the sample feature dimension learned by the ERC model. (2) Hyperparameter Settings of Compared SCL Baselines: To ensure a fair comparison, we implement both compatible and incompatible SCL baselines using the optimal hyperparameter values specified in their original papers. (3) Hyperparameter Settings of ERC models: In order to achieve a fair comparison, when comparing SSLCL with compatible SCL baselines on a ERC model, the hyperparameter settings of the ERC model are identical for all methods being evaluated. Specifically, to demonstrate that SSLCL can achieve superior performances without the need for a large batch size, we set the batch size to 4 on IEMO-CAP and 8 on MELD for recurrence-based and graph-based ERC models, while for transformer-based ERC models, the batch size is set to 2 on IEMOCAP and 4 on MELD.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis",
      "text": "Comparison with Baseline Methods Impact of Negative Sample-Label Loss As shown in Table 2, when SSLCL does not directly calculate the loss of negative sample-label pairs, there is a significant decrease in performance across all emotion categories on IEMOCAP. This highlights the effectiveness of explicitly minimizing the similarity between negative sample-label pairs.\n\nImpact of Label-Label Discrimination Based on Table  2 , it is evident that removing the label-label loss from SSLCL leads to a considerable performance degradation across all emotion categories on IEMOCAP, particularly in non-neutral emotions. The results emphasize the significance of explicitly learning distinct representations for different label embeddings.\n\nImpact of Soft-HGR Similarity Measure Table  3  demonstrates that Soft-HGR consistently outperforms dot product similarity and cosine similarity on both IEMOCAP and MELD, which highlights the superiority of Soft-HGR in effectively capturing the underlying correlations between sample features and label embeddings.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Impact Of Label Embedding Network",
      "text": "To study the impact of label embedding network on model performances, we implement SSLCL-based M3Net with different label embedding (LE) models, including a single embedding (Emb) layer, a two-layer MLP, a three-layer MLP (an embedding layer followed by two fully-connected layers) and Bi-GRU (an embedding layer followed by two bidirectional GRUs), on both IEMOCAP and MELD. As illustrated in Table  4 , the performance of the single embedding layer is considerably poorer compared to other LE models, while the twolayer MLP achieves a comparable performance to the threelayer MLP. Notably, Bi-GRU outperforms the other compared methods on both IEMOCAP and MELD. Nevertheless, it is important to note that the improvement of Bi-GRU over the two-layer MLP is not substantial, especially considering the significantly higher computational cost associated with Bi-GRU compared to the two-layer MLP. Therefore, in order to trade off between model performance and efficiency, we select the two-layer MLP as the label embedding network in SSLCL.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Batch Size Stability Of Sslcl",
      "text": "To investigate the stability of SSLCL across different batch sizes, we conduct experiments on both MELD and IEMO-CAP using the M3Net. As shown in Table  5 , SSLCL consistently outperforms existing compatible SCL approaches by a significant margin across all batch size settings, with a more pronounced improvement in small batch sizes. The results clearly demonstrate the superiority of SSLCL over existing SCL methods in achieving remarkable model performances without being constrained by the batch size.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose an efficient and model-agnostic SCL framework named SSLCL for the ERC task, in which a novel utilization of label representation is designed to address the challenges with large batch sizes and model incompatibility encountered in existing SCL approaches. Additionally, we innovatively leverage Soft-HGR as a measure of similarity, which significantly outperforms traditional similarity measures. Furthermore, multimodal information is innovatively utilized by SSLCL as data augmentation to boost model performances. Experimental results on IEMOCAP and MELD demonstrate the compatibility and superiority of SSLCL over existing state-of-the-art SCL approaches.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of imbalanced class distributions in",
      "page": 1
    },
    {
      "caption": "Figure 1: , Both IEMOCAP",
      "page": 2
    },
    {
      "caption": "Figure 2: , which is made up of three key components: sample fea-",
      "page": 3
    },
    {
      "caption": "Figure 2: , this is achieved through a sim-",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the overall framework of SSLCL1, which consists of three key components: sample feature extraction,",
      "page": 4
    },
    {
      "caption": "Figure 2: in order to",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "https://github.com/TaoShi1998/SSLCL.": "Introduction"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "Emotion recognition in conversations (ERC) aims to accu-"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "rately identify the emotion conveyed in each utterance dur-"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "ing a conversation. The widespread potential applications in"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "areas such as opinion mining (Majumder et al. 2019), empa-"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "thetic chatbots (Zhang, Chen, and Chen 2023), and medical"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "diagnosis (Hu et al. 2021) make ERC an essential task within"
        },
        {
          "https://github.com/TaoShi1998/SSLCL.": "the field of natural language processing."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "������������������������������������������������": "�������"
        },
        {
          "������������������������������������������������": "�������"
        },
        {
          "������������������������������������������������": "����������"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "���"
        },
        {
          "������������������������������������������������": "���\n���"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "��\n����"
        },
        {
          "������������������������������������������������": "��\n��"
        },
        {
          "������������������������������������������������": "���\n���\n�������\n�����"
        },
        {
          "������������������������������������������������": "��\n�����\n���"
        },
        {
          "������������������������������������������������": "���"
        },
        {
          "������������������������������������������������": "���\n���\n���\n���"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "�������\n�����\n��������\n���"
        },
        {
          "������������������������������������������������": "���������������\n������������"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "Figure 1:\nIllustration of\nimbalanced class distributions\nin"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "IEMOCAP and MELD."
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "To address\nthe problem of ERC, an increasing number"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "of studies (Tu et al. 2022; Arumugam, Bhattacharjee, and"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "Yuan 2022; Li, Yan, and Qiu 2022; Song et al. 2022; Shi and"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "Huang 2023; Yang et al. 2023a,b; Tu et al. 2023) have lever-"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "aged supervised contrastive learning (SCL)\n(Khosla et al."
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "2020) to learn robust and generalized feature representations"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "that are better suited for emotion classifications. Supervised"
        },
        {
          "������������������������������������������������": "contrastive learning is a representation learning technique"
        },
        {
          "������������������������������������������������": "that aims to enhance the robustness of\nlearned representa-"
        },
        {
          "������������������������������������������������": "tions through pulling samples belonging to the same cate-"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "gory (positive pairs) closer, while simultaneously pushing"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "apart samples from disparate classes (negative pairs). Cur-"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "rently, SCL-based approaches in ERC can be broadly clas-"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "sified into two categories: (1) Vanilla supervised contrastive"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "loss (SupCon loss) (Tu et al. 2022; Arumugam, Bhattachar-"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "jee, and Yuan 2022; Yang et al. 2023a); (2) Variants of the"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "SupCon loss that\nintegrate SCL with other\nlearning strate-"
        },
        {
          "������������������������������������������������": "gies, including prototypical networks and curriculum learn-"
        },
        {
          "������������������������������������������������": "ing (Song et al. 2022), multiview representation learning (Li,"
        },
        {
          "������������������������������������������������": "Yan, and Qiu 2022), hard example mining (Shi and Huang"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "2023), self-supervised contrastive learning (Tu et al. 2023),"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "and emotion prototypes (Yang et al. 2023b)."
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "Despite the surging popularity of applying SCL to ERC,"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "several challenges still remain unresolved: (1) ERC models"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "using the vanilla SupCon loss typically require a large"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "batch size to achieve satisfactory performances, which"
        },
        {
          "������������������������������������������������": ""
        },
        {
          "������������������������������������������������": "is computationally expensive. Existing widely-used ERC"
        },
        {
          "������������������������������������������������": "benchmark datasets, such as IEMOCAP (Busso et al. 2008)"
        },
        {
          "������������������������������������������������": "and MELD (Poria et al. 2019), are plagued by the class im-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "and MELD are class-imbalanced, with MELD being partic-",
          "extract maximally correlated representations across multi-": "ple random variables. In our SSLCL setting, we utilize Soft-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "ularly skewed towards certain classes. Thus, when utilizing",
          "extract maximally correlated representations across multi-": "HGR as a similarity measure to capture the complex correla-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "SupCon on these datasets, a large batch size is often needed",
          "extract maximally correlated representations across multi-": "tions between sample-label pairs, leading to significant per-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "to guarantee that each training sample from minority classes",
          "extract maximally correlated representations across multi-": "formance improvements over conventional similarity mea-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "has at least one positive pair within the batch, otherwise the",
          "extract maximally correlated representations across multi-": "sures (refer to the Results and Analysis section)."
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "loss can not be properly calculated,\nleading to a significant",
          "extract maximally correlated representations across multi-": "Finally,\nto demonstrate the compatibility and superiority"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "degradation in model performance (Shi and Huang 2023).",
          "extract maximally correlated representations across multi-": "of SSLCL, we conduct extensive experiments on two ERC"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "the SupCon loss tend to have difficulty\n(2) Variants of",
          "extract maximally correlated representations across multi-": "benchmark datasets: IEMOCAP and MELD. Experimental"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "integrating with most existing ERC approaches. While",
          "extract maximally correlated representations across multi-": "results provide compelling evidence that SSLCL exhibits"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "combining SCL with other learning techniques allows many",
          "extract maximally correlated representations across multi-": "three key advantages\nagainst\nexisting SCL literature:\n(1)"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "SupCon variants to overcome the constraint of a large batch",
          "extract maximally correlated representations across multi-": "SSLCL can be easily integrated with existing ERC models"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "size,\nthese SupCon variants\ntend to be incompatible with",
          "extract maximally correlated representations across multi-": "using any methodologies, without\nintroducing any model-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "the majority of existing ERC frameworks,\nsince they de-",
          "extract maximally correlated representations across multi-": "specific assumptions;\n(2) ERC methods using SSLCL can"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "pend on specific modeling assumptions (Song et al. 2022;",
          "extract maximally correlated representations across multi-": "achieve excellent performances without the need for a large"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "Tu et al. 2023; Yang et al. 2023b) that can not be easily ex-",
          "extract maximally correlated representations across multi-": "batch size;\n(3) The utilization of SSLCL yields significant"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "tended to other models. Moreover, SupCon variants often",
          "extract maximally correlated representations across multi-": "improvements in model performances, surpassing existing"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "suffer from an excessive level of complexity. (3) The inher-",
          "extract maximally correlated representations across multi-": "SCL-based approaches and achieving new state-of-the-art"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "ent multimodal nature of utterances in ERC has not been",
          "extract maximally correlated representations across multi-": "results on both IEMOCAP and MELD."
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "leveraged by existing SCL-based algorithms. A distinc-",
          "extract maximally correlated representations across multi-": "To summarize,\nthe main contributions of\nthis work are"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "tive characteristic of the ERC task is that each utterance is",
          "extract maximally correlated representations across multi-": "four-fold: (1) Through a novel utilization of label represen-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "accompanied by its corresponding textual, audio and visual",
          "extract maximally correlated representations across multi-": "tations, we effectively address the constraints posed by large"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "modalities. However, the potential of leveraging multimodal",
          "extract maximally correlated representations across multi-": "batch sizes and incompatibility with most existing ERC ar-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "cues has remained unexplored by existing SCL literature.",
          "extract maximally correlated representations across multi-": "chitectures encountered in current SCL-based methods; (2)"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "To\ntackle\nthe\naforementioned\nchallenges,\nin\nthis\npa-",
          "extract maximally correlated representations across multi-": "To the best of our knowledge, we are the first\nin the SCL"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "per, we propose\na novel SCL framework named Super-",
          "extract maximally correlated representations across multi-": "community to leverage Soft-HGR as a measure of similarity,"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "vised\nSample-Label Contrastive Learning with\nSoft-",
          "extract maximally correlated representations across multi-": "which yields considerable performance improvements over"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "HGR Maximal Correlation (SSLCL), which is an efficient",
          "extract maximally correlated representations across multi-": "traditional similarity measures; (3) We innovatively leverage"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "and model-agnostic approach that eliminates the need for a",
          "extract maximally correlated representations across multi-": "multimodal\ninformation as data augmentation to enhance"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "large batch size and can be seamlessly integrated with ex-",
          "extract maximally correlated representations across multi-": "model performances; (4) Extensive experiments on two ERC"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "isting ERC models without\nintroducing any model-specific",
          "extract maximally correlated representations across multi-": "benchmark datasets,\nIEMOCAP and MELD, demonstrate"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "assumptions. Additionally, multimodal information is effec-",
          "extract maximally correlated representations across multi-": "the compatibility and superiority of SSLCL compared with"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "tively leveraged by SSLCL as data augmentation to achieve",
          "extract maximally correlated representations across multi-": "existing state-of-the-art SCL approaches."
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "more promising performances.",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "Specifically,\nthe key insight of our work is a fundamen-",
          "extract maximally correlated representations across multi-": "Related Work"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "tal shift\nin how labels are utilized:\nin contrast\nto the preva-",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "Supervised Contrastive Learning"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "lent use of one-hot vector\nlabel\nrepresentations in existing",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "Supervised contrastive learning (Khosla et al. 2020) is a rep-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "SCL-based approaches, we\nembed each discrete\nemotion",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "resentation learning technique that extends self-supervised"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "category into a dense label embedding through a shallow",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "contrastive learning (self-supervised CL)\n(Wu et al. 2018)"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "multilayer perceptron, and utilize the learned label embed-",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "by leveraging label\ninformation,\nin which samples from the"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "ding of a specific class as the “ground-truth representation”",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "same class are pulled closer while samples belonging to dif-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "for samples belonging to that class. Then, we formulate the",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "ferent classes are pushed apart within the batch."
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "training objective to maximize the similarity between sam-",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "Recently,\nan\nincreasing\nnumber\nof ERC models\nhave"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "ple features and their corresponding ground-truth label em-",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "utilized\nthe\nvanilla SupCon\nloss\nto\nenhance model\nper-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "beddings (positive sample-label pairs), while minimizing the",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "formances.\nFor\nexample,\nSentic GAT (Tu\net\nal.\n2022)"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "similarity between sample features and label embeddings of",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "leverages SupCon to discriminate context-free and context-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "different classes (negative sample-label pairs). Furthermore,",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "sensitive\nrepresentations. Arumugam, Bhattacharjee,\nand"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "inspired by Cutout (DeVries and Taylor 2017), a commonly-",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "Yuan (2022) employs SupCon to enhance cross-modal con-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "used image augmentation technique in the computer vision",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "sistency. SCMM (Yang et al. 2023a) utilizes the SupCon loss"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "community, we utilize multimodal views of the same sam-",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "to improve the discriminability of\nlearned multimodal\nfea-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "ple as its data augmentations. Consequently, each training",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "tures. However, vanilla SupCon in ERC typically requires a"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "sample is guaranteed to have multiple positive sample-label",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "large batch size to achieve satisfactory performances,\nren-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "pairs within the batch, regardless of the batch size. (Lee and",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "dering it computationally expensive."
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "Lee 2022)",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "Another notable contribution of SSLCL that distinguishes",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "",
          "extract maximally correlated representations across multi-": "Emotion Recognition in Conversations"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "it\nfrom existing SCL methods\nis\nthe\nadaptation\nof\nthe",
          "extract maximally correlated representations across multi-": ""
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "soft-Hirschfeld-Gebelein-R´enyi (Soft-HGR) maximal corre-",
          "extract maximally correlated representations across multi-": "Recurrence-based Methods\nBC-LSTM (Poria\net\nal."
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "lation (Wang et al. 2019) as a measure of similarity between",
          "extract maximally correlated representations across multi-": "2017) captures conversational contexts based on long short-"
        },
        {
          "balance problem. As illustrated in Figure 1, Both IEMOCAP": "sample features and label embeddings. Soft-HGR aims to",
          "extract maximally correlated representations across multi-": "term memory (LSTM) networks. DialogueRNN (Majumder"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "model the context and keep track of speaker states. Inspired",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Soft-HGR Maximum Correlation"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "by the cognitive theory of emotion, DialogueCRN (Hu, Wei,",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Soft-Hirschfeld-Gebelein-R´enyi (Soft-HGR) maximum cor-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "and Huai 2021) introduces a cognitive perspective to com-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "relation (Wang et al. 2019) is an extension of the HGR max-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "prehend contextual information based on LSTMs.",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "imum correlation (Hirschfeld 1935; Gebelein 1941; R´enyi"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "1959), which is a measure of dependence between random"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "Transformer-based Methods\nHiTrans (Li et al. 2020) uti-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "variables. Soft-HGR preserves\nthe same feature geometry"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "lizes\ntwo hierarchical\ntransformers\nto capture\nlocal-\nand",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "as in the HGR maximal correlation, while simultaneously"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "global-level\nutterance\nrepresentations. EmoBERTa\n(Kim",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "avoiding strict whitening constraints imposed by HGR."
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "and Vossen 2021) learns inter- and intra-speaker states and",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Soft-HGR aims\nto extract maximally correlated feature"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "conversational\ncontexts\nthrough fine-tuning\na\npre-trained",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "representations from multiple random modalities. Formally,"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "RoBERTa. CoG-BART is proposed by Li, Yan,\nand Qiu",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "given two random variables represented by X and Y ,\nthe"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "(2022), which leverages a pretrained encoder-decoder model",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "objective of Soft-HGR is\nto learn nonlinear\nfeature map-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "BART to\ncapture\ncontextual\ninformation. CoMPM (Lee",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "pings\nf(X) and g(Y )\nthat are maximally correlated. The"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "and Lee 2022) utilizes a transformer-encoder based context",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Soft-HGR objective is defined as follows:"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "model and a speaker-aware pretrained memory module to",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "capture conversational contexts.",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "1 2\nmax\nE[fT(X)g(Y )] −\ntr(cov(f(X))cov(g(Y ))),"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "f,g\n(1)"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "E[f(X)] = E[g(Y )] = 0.\ns.t."
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "Graph-based Methods\nDialogueGCN\n(Ghosal\net\nal.",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "2019) models conversations based on a graph convolutional",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "As\nillustrated in equation 1, Soft-HGR consists of\ntwo"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "network (GCN). DAG-ERC (Shen et al. 2021) encodes utter-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "inner products: one between feature mappings and another"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "ances through a directed acyclic graph (DAG) to better un-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "between feature covariances. The first\ninner product\nis con-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "derstand the inherent structure within a conversation. M3Net",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "sistent with the objective of the HGR maximal correlation,"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "(Chen et al. 2023) leverages graph neural networks (GNNs)",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "while the second one serves as a soft regularizer that replaces"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "to capture multivariate relationships among multiple modal-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "the hard whitening constraints in HGR."
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "ities and conversational contexts.",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "From an\ninformation\ntheory\nperspective,\nthe\noptimal"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "nonlinear\nfeature transformations\nf(X) and g(Y )\nlearned"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "SCL-based Methods",
          "Preliminary": "through Soft-HGR capture the maximum amount of mutual"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "The increasing popularity of SCL",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "information shared by random variables X and Y ,\nrender-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "has led to numerous attempts to combine SCL with other",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "ing them highly informative and well-suited for downstream"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "learning strategies. To illustrate, SPCL (Song et al. 2022)",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "discriminative tasks."
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "integrates SCL with prototypical networks and curriculum",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "learning to alleviate the imbalanced class problem without",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "requiring a large batch size. However, SPCL is incompatible",
          "Preliminary": "Methodology"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "with most existing ERC models because its difficulty mea-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Problem Definition"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "sure function requires the emotion representations learned",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "n\nGiven\na\nconversation which\nconsists\nof\nutterances"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "by the ERC model\nto be distance-aware. CoG-BART (Li,",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "u1, u2, . . . , un expressed by speakers Su1, Su2, . . . , Sun , the"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "Yan, and Qiu 2022)\nintroduces multiview-SupCon to ad-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "objective of ERC is to assign an emotion label from a pre-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "dresses the batch size constraint associated with SupCon, in",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "defined K-class emotion category set Y to each utterance in"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "which each sample is paired with a gradient-detached copy.",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "this conversation. Each utterance is accompanied by its cor-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "Nevertheless, a significant drawback of multiview-SupCon",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "responding textual\n(t), audio (a) and visual (v) modalities,"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "is that it calculates the similarity between identical samples,",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "which can be represented as follows:"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "which can lead to suboptimal representations and potentially",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "hinder\nthe model’s ability to generalize effectively. Multi-",
          "Preliminary": "(2)\nui = {ut\ni, ua\ni , uv\ni }, i ∈ {1, . . . , n},"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "EMO (Shi and Huang 2023) introduces a sample-weighted",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "where ut\ni ∈ Rdt, ua\ni ∈ Rda , uv\ni ∈ Rdv ."
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "focal contrastive (SWFC)\nloss, which combines SCL with",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "hard sample mining to alleviate the difficulty of recognizing",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Overview of the Proposed SSLCL Framework"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "minority and semantically similar emotions. However, sim-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "ilar to vanilla SCL,\nthe SWFC loss also relies on a signifi-",
          "Preliminary": "The overall\nframework of SSLCL is\nillustrated in Figure"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "cantly large batch size to achieve desirable results. CKCL",
          "Preliminary": "2, which is made up of three key components: sample fea-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "(Tu et\nal. 2023)\nintegrates SCL with self-supervised CL",
          "Preliminary": "ture extraction,\nlabel\nlearning, and sample-label contrastive"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "to differentiate context- and knowledge-independent utter-",
          "Preliminary": "learning. We discuss each component in detail as follows."
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "ances. Nonetheless, a major limitation of CKCL is it can not",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "",
          "Preliminary": "Sample Feature Extraction"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "be applied to most ERC models that do not\nrely on exter-",
          "Preliminary": ""
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "nal knowledge. SCCL (Yang et al. 2023b) combines SCL",
          "Preliminary": "A major advantage of SSLCL over established SCL litera-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "with valence-arousal-dominance (VAD) emotion Prototypes",
          "Preliminary": "ture is its ability to seamlessly integrate with existing ERC"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "to boost model performances. However, SCCL only consid-",
          "Preliminary": "approaches without introducing any model-specific assump-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "ers context-independent word-level VAD, whereas the ma-",
          "Preliminary": "tions. As shown in Figure 2, this is achieved through a sim-"
        },
        {
          "et al. 2019) employs three gated recurrent units (GRUs) to": "jority of existing ERC approaches are context-dependent.",
          "Preliminary": "ple yet effective approach: for any ERC model, we extract"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "layer\nlayer"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "Label \nK-class discrete \nLabel embedding"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "embeddings \nemotion categories\nnetwork"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "Figure 2: Illustration of the overall framework of SSLCL1, which consists of three key components: sample feature extraction,"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "label learning, and sample-label contrastive learning. For the sake of simplicity, sample-label contrastive learning is illustrated"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "using the N -th utterance in the batch. In this Figure, t, a and v represent the textual, audio and visual modalities of utterances"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "respectively, fN (t), fN (t + a), fN (t + v) are data augmentations of the N -th sample feature with different missing modalities."
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "the sample features\nfrom its\nsecond-last hidden layer and"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "contrast\nthem with label embeddings learned through a la-"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "bel embedding network, which is jointly optimized with the"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "ERC model during training. There are no architectural mod-"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "ifications or modeling assumptions required in this process."
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "Label Learning"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "In contrast\nto the commonly adopted one-hot vector\nlabel"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "representations in existing SCL-based approaches, our pro-"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "posed SSLCL framework innovatively projects each discrete"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "emotion category into a dense embedding through a simple"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "yet powerful two-layer multilayer perceptron (MLP), which"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "consists of a embedding layer, a fully-connected layer and"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "a rectified linear unit (ReLU). Although a deeper and more"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "complex label embedding network (Sun et al. 2017) could"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "be\nadopted,\nexperimental\nresults demonstrate\nthat\na\ntwo-"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "layer shallow MLP is sufficient\nto achieve satisfactory per-"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "formance (refer to Appendix D)."
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "Specifically, given a K-class discrete emotion category"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "set Y = {1, 2, . . . , K}, the label embedding gi ∈ Rd for the"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "i-th emotion label is obtained as follows:"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "(3)\nei = max(0, EmbeddingLayer(i)),"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "(4)\ngi = Wgei + bg,"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": ""
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "1Our SSLCL framework can be applied to ERC models with"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "any modalities. We use the multimodal case in Figure 2 in order to"
        },
        {
          "Embedding  \n    Fully-connected \nJoy\ngJoy": "illustrate how data augmentation works."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "Angry",
          "MELD": "Sad"
        },
        {
          "IEMOCAP": "61.40",
          "MELD": "18.24"
        },
        {
          "IEMOCAP": "62.31",
          "MELD": "20.29"
        },
        {
          "IEMOCAP": "59.57",
          "MELD": "18.24"
        },
        {
          "IEMOCAP": "57.56",
          "MELD": "16.72"
        },
        {
          "IEMOCAP": "61.59",
          "MELD": "22.60"
        },
        {
          "IEMOCAP": "63.72",
          "MELD": "24.32"
        },
        {
          "IEMOCAP": "60.65",
          "MELD": "23.00"
        },
        {
          "IEMOCAP": "64.63",
          "MELD": "25.44"
        },
        {
          "IEMOCAP": "64.78",
          "MELD": "27.95"
        },
        {
          "IEMOCAP": "65.68",
          "MELD": "28.97"
        },
        {
          "IEMOCAP": "55.61",
          "MELD": "37.33"
        },
        {
          "IEMOCAP": "52.94",
          "MELD": "34.22"
        },
        {
          "IEMOCAP": "55.41",
          "MELD": "37.91"
        },
        {
          "IEMOCAP": "53.92",
          "MELD": "37.15"
        },
        {
          "IEMOCAP": "59.33",
          "MELD": "39.09"
        },
        {
          "IEMOCAP": "60.82",
          "MELD": "38.20"
        },
        {
          "IEMOCAP": "61.63",
          "MELD": "40.11"
        },
        {
          "IEMOCAP": "60.33",
          "MELD": "41.23"
        },
        {
          "IEMOCAP": "62.46",
          "MELD": "41.55"
        },
        {
          "IEMOCAP": "70.52",
          "MELD": "35.77"
        },
        {
          "IEMOCAP": "69.86",
          "MELD": "36.55"
        },
        {
          "IEMOCAP": "71.39",
          "MELD": "36.31"
        },
        {
          "IEMOCAP": "69.10",
          "MELD": "38.69"
        },
        {
          "IEMOCAP": "68.51",
          "MELD": "37.78"
        },
        {
          "IEMOCAP": "69.30",
          "MELD": "40.46"
        },
        {
          "IEMOCAP": "67.68",
          "MELD": "39.56"
        },
        {
          "IEMOCAP": "69.01",
          "MELD": "42.05"
        },
        {
          "IEMOCAP": "67.42",
          "MELD": "39.48"
        },
        {
          "IEMOCAP": "71.44",
          "MELD": "42.46"
        },
        {
          "IEMOCAP": "68.33",
          "MELD": "41.18"
        },
        {
          "IEMOCAP": "-",
          "MELD": "-"
        },
        {
          "IEMOCAP": "67.30",
          "MELD": "43.94"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "-\n-\n-\n-\n-\n-\nCKCL†",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "67.16\n-\n-\n-\n-\n-\n-\n-\n66.21"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "SCCL\n48.87\n79.57\n66.59\n67.30\n71.55\n62.99",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "67.12\n76.97\n58.97\n19.75\n43.94\n63.30\n24.72\n53.54\n64.74"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "Table 1: Experimental results on IEMOCAP and MELD. The best results in each emotion category and the overall w-F1 score",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "are highlighted in bold. The ∗ symbol denotes that the original COG-BART implementation adopts mv-SupCon. The † symbol",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "means that the source code of CKCL has not been released, and per-class w-F1 results are unavailable from the original paper.",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "related. Since a higher correlation indicates a greater simi-",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "single sample-label pair fi and gzi can be written as follows:"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "larity between feature mappings captured by Soft-HGR, we",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "extend Soft-HGR as a measure of similarity between sample",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "1"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "1 2\n−\n) =\n),\nfT"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "N(cid:88) l\ni gzi\nSim(fi, gzi\ncov(fi, fl)cov(gzi\n, gzl"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "features and label embeddings.",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "N − 1"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "=1"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "Formally, give\na batch of N labeled training samples",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "(6)"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "{(ut\ni, ua\ni , uv\ni , yi)}N\ni=1, we denote the sample features learned",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "∈ Gz, 1 ≤ i ≤ N .\nwhere fi ∈ F, gzi"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "through the ERC model as F = [f1, f2, . . . , fN ]T ∈ RN ×d,",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "and label embeddings learned through the label embedding",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "Positive Sample-Label Pairs\nA pair (fi, gzi"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "to as a positive sample-label pair\nis the ground-truth\nif zi"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "network as G = [g1, g2, . . . , gK]T ∈ RK×d. For an arbi-",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "label of\nto maximize the correlation be-\nfi. SSLCL aims"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "trary N discrete emotion labels z = [z1, z2, . . . , zN ]T ∈",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "tween positive sample-label pairs. However, only one pos-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "RN , where zi ∈ Y, 1 ≤ i ≤ N , we denote their corre-",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "itive sample-label pair is available for each training sample,"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "]T ∈",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "sponding feature mappings as Gz = [gz1\n, gz2\n, . . . , gzN",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "which could potentially hinder\nthe generalizability and ro-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "RN ×d. Based on the empirical Soft-HGR objective provided",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "bustness of learned features. To address this limitation,\nin-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "by Wang et al.\n(2019),\nthe Soft-HGR similarity between",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "spired by Cutout (DeVries and Taylor 2017), a widely-used"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "zero-meaned 2 F and Gz can be calculated as follows:",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "image augmentation technique in the computer vision com-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "munity which masks out partial regions of input images dur-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "1",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "ing training, we leverage the multimodal nature of ERC ut-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "1 2\n−\nfT\ntr(cov(F)cov(Gz)).\nSim(F, Gz) =",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "N(cid:88) i\ni gzi",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "N − 1",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "terances to generate additional positive sample-label pairs."
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "=1",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "Specifically, we consider utterances with one or two missing"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "(5)",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "modalities as partially masked examples of the original\nin-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "Based on Equation 5, the Soft-HGR similarity between a",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": ""
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "put utterances, and utilize their corresponding learned sam-"
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "ple features as augmentations of the original sample feature."
        },
        {
          "SPCL\n40.89\n78.48\n65.27\n68.33\n71.13\n64.09": "2This corresponds to the zero-mean constraint in Soft-HGR.",
          "66.22\n79.95\n57.63\n12.99\n41.18\n62.71\n30.77\n50.73\n65.85": "Considering the crucial role of the textual modality in emo-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Ablation study of SSLCL on IEMOCAP and",
      "data": [
        {
          "IEMOCAP": ""
        },
        {
          "IEMOCAP": "Angry"
        },
        {
          "IEMOCAP": "71.44"
        },
        {
          "IEMOCAP": "66.67"
        },
        {
          "IEMOCAP": "68.79"
        },
        {
          "IEMOCAP": "67.44"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Ablation study of SSLCL on IEMOCAP and",
      "data": [
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": "loss, and label-label discrimination on model performances."
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": "Method"
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": "SSLCL-based M3Net"
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        },
        {
          "Table 2: Ablation study of SSLCL on IEMOCAP to evaluate the impacts of positive pairs augmentation, negative sample-label": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "Angry",
          "MELD": "Sad"
        },
        {
          "IEMOCAP": "68.79",
          "MELD": "40.74"
        },
        {
          "IEMOCAP": "71.44",
          "MELD": "42.46"
        },
        {
          "IEMOCAP": "",
          "MELD": ""
        },
        {
          "IEMOCAP": "69.34",
          "MELD": "41.48"
        },
        {
          "IEMOCAP": "68.66",
          "MELD": "41.64"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "MLP and Bi-GRU are tuned on the validation set."
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "Cross-Entropy Loss\nApart from the SSLCL loss, we also"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "adopt a cross-entropy (CE) loss to minimize the difference"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "between predicted probabilities and ground-truth labels. Let"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "pi denote the probability distribution of emotion classes for"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "fi, the CE loss can be defined as follows:"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "N(cid:88) i\n(15)\nLCE = −\nlog pi[yi],"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "=1"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "Overall Training Objective\nThe overall\ntraining objec-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "tive is a linear combination of the SSLCL loss and the CE"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "loss, which is defined as follows:"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "(16)\nLTrain = LSSLCL + ηLCE,"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "where η is a trade-off hyperparameter between the SSLCL"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "loss and the CE loss."
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "Experimental Settings"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "Datasets"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "IEMOCAP\nThe IEMOCAP dataset\n(Busso et al. 2008)"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "consists of around 12 hours of dyadic conversation videos,"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "which are divided into 151 dialogues with a total of 7433"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "individual utterances. Each utterance in IEMOCAP is an-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "notated with one of six emotion labels: happy, sad, neutral,"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "angry, excited, and frustrated."
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "MELD\nThe MELD dataset (Poria et al. 2019) is a collec-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "tion of multi-party conversations extracted from the TV se-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "ries Friends, which consists of 1433 dialogues and a total of"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "13708 individual utterances. Each utterance in MELD is an-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "notated with one of seven emotion categories: angry, disgust,"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "fear, joy, neutral, sad, and surprise."
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "Baseline Methods"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": ""
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "To demonstrate the compatibility and superiority of SSLCL"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "compared to existing SCL methods, we conduct a compre-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "hensive comparison between SSLCL and existing SCL lit-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "erature in ERC, which can be categorized into two groups:"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "(1) Compatible SCL methods: SCL approaches\nthat can"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "be integrated with the majority of existing ERC models, in-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "cluding vanilla SupCon loss (Khosla et al. 2020), multiview-"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "SupCon (mv-SupCon)\nloss\n(Li, Yan, and Qiu 2022), and"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "SWFC loss (Shi and Huang 2023); (2) Incompatible SCL"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "methods: SCL frameworks that are incompatible with most"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "existing ERC models,\nincluding SPCL (Song et al. 2022),"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "CKCL (Tu et al. 2023), and SCCL (Yang et al. 2023b)."
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "Specifically,\nthe effectiveness of SSLCL and compatible"
        },
        {
          "Bi-GRU is composed of an embedding layer and two bidirectional GRUs. The hidden layer dimensions for both the three-layer": "SCL approaches are compared on the following categories"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Impact of Label Embedding Network To study the im-",
      "data": [
        {
          "IEMOCAP (w-F1)": "",
          "MELD (w-F1)": ""
        },
        {
          "IEMOCAP (w-F1)": "BS = 8",
          "MELD (w-F1)": "BS = 8"
        },
        {
          "IEMOCAP (w-F1)": "70.02",
          "MELD (w-F1)": "65.40"
        },
        {
          "IEMOCAP (w-F1)": "68.99",
          "MELD (w-F1)": "65.34"
        },
        {
          "IEMOCAP (w-F1)": "",
          "MELD (w-F1)": ""
        },
        {
          "IEMOCAP (w-F1)": "70.83",
          "MELD (w-F1)": "65.42"
        },
        {
          "IEMOCAP (w-F1)": "72.25",
          "MELD (w-F1)": "66.92"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: Impact of Label Embedding Network To study the im-",
      "data": [
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "performances across different batch sizes. BS represents batch size."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Results and Analysis"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Comparison with Baseline Methods"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "The\ncomparisons between SSLCL and existing SCL ap-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "proaches on IEMOCAP and MELD are shown in Table 1."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Experimental results conclusively demonstrate that SSLCL"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "consistently brings noteworthy improvements to the perfor-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "mances of ERC models across different categories, achiev-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "ing new state-of-the-art results not only in the overall w-F1"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "score but also across all emotion categories. Specifically,"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "the average relative improvement SSLCL brought\nto ERC"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "models\nare 3.32% on IEMOCAP and 2.48% on MELD,"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "with SSLCL-based M3Net surpassing previous state-of-the-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "art approaches on both datasets. In contrast, under the con-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "straint of a small batch size, existing compatible SCL meth-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "ods only yield marginal or even detrimental effects across"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "different ERC approaches. Furthermore, SSLCL-integrated"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "M3Net outperforms incompatible SCL frameworks by a sig-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "nificant margin on both IEMOCAP and MELD."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Quantitative Analysis of SSLCL"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Impact of Positive Pairs Augmentation\nAs illustrated in"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Table 2, experimental\nresults clearly demonstrate that\nre-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "moving positive pairs augmentation from SSLCL leads to"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "a considerable decline in model performances. Particularly,"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "there is a more noticeable decrease observed in minority"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "emotions compared to majority emotions,\nsuch as Happy"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "and Angry. The inferior performances of SSLCL without"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "data augmentation verifies the effectiveness of positive pairs"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "augmentation in enhancing model performances, especially"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "in underrepresented emotion categories."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Impact of Negative Sample-Label Loss\nAs shown in Ta-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "ble 2, when SSLCL does not directly calculate the loss of"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "negative sample-label pairs,\nthere is a significant decrease"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "in performance across all emotion categories on IEMOCAP."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "This highlights the effectiveness of explicitly minimizing the"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "similarity between negative sample-label pairs."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Impact of Label-Label Discrimination\nBased on Table"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "2,\nit\nis\nevident\nthat\nremoving\nthe\nlabel-label\nloss\nfrom"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "SSLCL leads\nto a\nconsiderable performance degradation"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "across\nall\nemotion categories on IEMOCAP, particularly"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "in non-neutral emotions. The results emphasize the signif-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "icance of explicitly learning distinct representations for dif-"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "ferent label embeddings."
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": ""
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "Impact\nof\nSoft-HGR\nSimilarity Measure\nTable\n3"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "demonstrates\nthat Soft-HGR consistently outperforms dot"
        },
        {
          "Table 5: Ablation study of SSLCL on IEMOCAP and MELD to demonstrate the capability of SSLCL in achieving stable model": "product similarity and cosine similarity on both IEMOCAP"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "P.; Maschinot, A.; Liu, C.; and Krishnan, D. 2020.\nSuper-"
        },
        {
          "References": "Arumugam, B.; Bhattacharjee, S. D.; and Yuan,\nJ. 2022.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "information\nvised contrastive learning. Advances in neural"
        },
        {
          "References": "Multimodal Attentive Learning for Real-time Explainable",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "processing systems, 33: 18661–18673."
        },
        {
          "References": "Emotion Recognition in Conversations.\nIn 2022 IEEE In-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "ternational Symposium on Circuits and Systems\n(ISCAS),",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Kim, T.; and Vossen, P. 2021. EmoBERTa: Speaker-Aware"
        },
        {
          "References": "1210–1214.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Emotion\nRecognition\nin\nConversation with\nRoBERTa."
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "arXiv:2108.12009."
        },
        {
          "References": "Bao, Y.; Li, Y.; Huang, S.-L.; Zhang, L.; Zheng, L.; Zamir,",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "A.; and Guibas, L. 2019.\nAn Information-Theoretic Ap-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Kingma, D. P.;\nand Ba,\nJ. 2014.\nAdam: A method for"
        },
        {
          "References": "proach to Transferability in Task Transfer Learning. In 2019",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "stochastic optimization. arXiv preprint arXiv:1412.6980."
        },
        {
          "References": "IEEE International Conference on Image Processing (ICIP),",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Lee,\nJ.; and Lee, W. 2022.\nCoMPM: Context Modeling"
        },
        {
          "References": "2309–2313.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "with Speaker’s Pre-trained Memory Tracking for Emotion"
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "the 2022\nRecognition in Conversation.\nIn Proceedings of"
        },
        {
          "References": "Busso, C.; Bulut, M.; Lee, C.-C.; Kazemzadeh, A.; Mower,",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Conference of\nthe North American Chapter of\nthe Associa-"
        },
        {
          "References": "E.; Kim, S.; Chang,\nJ. N.; Lee, S.; and Narayanan, S. S.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "tion for Computational Linguistics: Human Language Tech-"
        },
        {
          "References": "2008. IEMOCAP: Interactive emotional dyadic motion cap-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "nologies, 5669–5679. Seattle, United States: Association for"
        },
        {
          "References": "ture database. Language resources and evaluation, 42: 335–",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Computational Linguistics."
        },
        {
          "References": "359.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Li, J.; Ji, D.; Li, F.; Zhang, M.; and Liu, Y. 2020. HiTrans: A"
        },
        {
          "References": "Chen, F.; Shao, J.; Zhu, S.; and Shen, H. T. 2023. Multivari-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Transformer-Based Context- and Speaker-Sensitive Model"
        },
        {
          "References": "ate, Multi-Frequency and Multimodal: Rethinking Graph",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "for Emotion Detection in Conversations.\nIn Proceedings of"
        },
        {
          "References": "Neural Networks for Emotion Recognition in Conversation.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "the 28th International Conference on Computational Lin-"
        },
        {
          "References": "the IEEE/CVF Conference on Computer\nIn Proceedings of",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "guistics, 4190–4200. Barcelona, Spain (Online):\nInterna-"
        },
        {
          "References": "Vision and Pattern Recognition, 10761–10770.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "tional Committee on Computational Linguistics."
        },
        {
          "References": "DeVries, T.; and Taylor, G. W. 2017.\nImproved regulariza-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Li, S.; Yan, H.; and Qiu, X. 2022. Contrast and generation"
        },
        {
          "References": "arXiv\ntion of convolutional neural networks with cutout.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "make bart a good dialogue emotion recognizer.\nIn Proceed-"
        },
        {
          "References": "preprint arXiv:1708.04552.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "ings of\nthe AAAI conference on artificial\nintelligence, vol-"
        },
        {
          "References": "Gebelein, H. 1941. Das statistische Problem der Korrela-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "ume 36, 11002–11010."
        },
        {
          "References": "tion als Variations-und Eigenwertproblem und sein Zusam-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Luo, J.; Phan, H.; and Reiss, J. 2023. Cross-Modal Fusion"
        },
        {
          "References": "ZAMM-Journal of\nmenhang mit der Ausgleichsrechnung.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Techniques for Utterance-Level Emotion Recognition from"
        },
        {
          "References": "Applied Mathematics and Mechanics/Zeitschrift\nf¨ur Ange-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Text and Speech.\nIn ICASSP 2023 - 2023 IEEE Interna-"
        },
        {
          "References": "wandte Mathematik und Mechanik, 21(6): 364–379.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "tional Conference on Acoustics, Speech and Signal Process-"
        },
        {
          "References": "Ghosal, D.; Majumder, N.; Poria, S.; Chhaya, N.; and Gel-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "ing (ICASSP), 1–5."
        },
        {
          "References": "bukh, A. 2019. DialogueGCN: A Graph Convolutional Neu-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Ma, F.; Huang, S.-L.; and Zhang, L. 2021. An efficient ap-"
        },
        {
          "References": "ral Network for Emotion Recognition in Conversation.\nIn",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "proach for audio-visual emotion recognition with missing"
        },
        {
          "References": "Proceedings of\nthe 2019 Conference on Empirical Meth-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "labels and missing modalities.\nIn 2021 IEEE international"
        },
        {
          "References": "ods in Natural Language Processing and the 9th Interna-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "conference on multimedia and Expo (ICME), 1–6. IEEE."
        },
        {
          "References": "tional Joint Conference on Natural Language Processing",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "(EMNLP-IJCNLP), 154–164. Hong Kong, China: Associa-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Majumder, N.; Poria, S.; Hazarika, D.; Mihalcea, R.; Gel-"
        },
        {
          "References": "tion for Computational Linguistics.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "bukh, A.; and Cambria, E. 2019. Dialoguernn: An attentive"
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "rnn for emotion detection in conversations.\nIn Proceedings"
        },
        {
          "References": "Hirschfeld, H. O. 1935. A connection between correlation",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "of the AAAI conference on artificial intelligence, volume 33,"
        },
        {
          "References": "and contingency.\nIn Mathematical Proceedings of the Cam-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "6818–6825."
        },
        {
          "References": "bridge Philosophical Society, volume 31, 520–524. Cam-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "bridge University Press.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Poria, S.; Cambria, E.; Hazarika, D.; Majumder, N.; Zadeh,"
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "A.; and Morency, L.-P. 2017. Context-Dependent Sentiment"
        },
        {
          "References": "Hu, D.; Wei, L.; and Huai, X. 2021. DialogueCRN: Contex-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "the\nAnalysis in User-Generated Videos.\nIn Proceedings of"
        },
        {
          "References": "tual Reasoning Networks for Emotion Recognition in Con-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "55th Annual Meeting of\nthe Association for Computational"
        },
        {
          "References": "the 59th Annual Meeting of\nversations.\nIn Proceedings of",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Linguistics (Volume 1: Long Papers), 873–883. Vancouver,"
        },
        {
          "References": "the Association for Computational Linguistics and the 11th",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Canada: Association for Computational Linguistics."
        },
        {
          "References": "International Joint Conference on Natural Language Pro-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Poria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria,"
        },
        {
          "References": "cessing (Volume 1: Long Papers), 7042–7052. Online: As-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "E.; and Mihalcea, R. 2019. MELD: A Multimodal Multi-"
        },
        {
          "References": "sociation for Computational Linguistics.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Party Dataset\nfor Emotion Recognition in Conversations."
        },
        {
          "References": "Hu, J.; Liu, Y.; Zhao, J.; and Jin, Q. 2021. MMGCN: Mul-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "the 57th Annual Meeting of\nthe Asso-\nIn Proceedings of"
        },
        {
          "References": "timodal Fusion via Deep Graph Convolution Network for",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "ciation for Computational Linguistics, 527–536. Florence,"
        },
        {
          "References": "Emotion Recognition in Conversation.\nIn Proceedings of",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Italy: Association for Computational Linguistics."
        },
        {
          "References": "the 59th Annual Meeting of\nthe Association for Computa-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "R´enyi, A. 1959. On measures of dependence. Acta mathe-"
        },
        {
          "References": "tional Linguistics and the 11th International Joint Confer-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "matica hungarica, 10(3-4): 441–451."
        },
        {
          "References": "ence on Natural Language Processing (Volume 1: Long Pa-",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": ""
        },
        {
          "References": "pers), 5666–5675. Online: Association for Computational",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Shen, W.; Wu, S.; Yang, Y.; and Quan, X. 2021. Directed"
        },
        {
          "References": "Linguistics.",
          "Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,": "Acyclic Graph Network for Conversational Emotion Recog-"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "sociation for Computational Linguistics and the 11th Inter-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "national Joint Conference on Natural Language Processing"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "(Volume 1: Long Papers), 1551–1560. Online: Association"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "for Computational Linguistics."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Shi, T.; and Huang, S.-L. 2023. MultiEMO: An Attention-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Based Correlation-Aware Multimodal Fusion Framework"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "for Emotion Recognition in Conversations.\nIn Proceedings"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "of the 61st Annual Meeting of the Association for Computa-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "tional Linguistics (Volume 1: Long Papers), 14752–14766."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Toronto, Canada: Association for Computational Linguis-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "tics."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Song, X.; Huang, L.; Xue, H.; and Hu, S. 2022. Supervised"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Prototypical Contrastive Learning for Emotion Recognition"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "in Conversation.\nIn Proceedings of the 2022 Conference on"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Empirical Methods in Natural Language Processing, 5197–"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "5206. Abu Dhabi, United Arab Emirates: Association for"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Computational Linguistics."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Sun, X.; Wei, B.; Ren, X.; and Ma, S. 2017.\nLabel Em-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "bedding Network: Learning Label Representation for Soft"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Training of Deep Networks. arXiv:1710.10393."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Tu, G.; Liang, B.; Mao, R.; Yang, M.; and Xu, R. 2023. Con-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "text or Knowledge is Not Always Necessary: A Contrastive"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Learning Framework for Emotion Recognition in Conver-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "the Association for Computational\nsations.\nIn Findings of"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Linguistics: ACL 2023, 14054–14067. Toronto, Canada: As-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "sociation for Computational Linguistics."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Tu, G.; Wen,\nJ.; Liu, C.;\nJiang, D.;\nand Cambria, E."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "2022. Context- and Sentiment-Aware Networks for Emotion"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "IEEE Transactions on Artifi-\nRecognition in Conversation."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "cial Intelligence, 3(5): 699–708."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Wang, L.; Wu, J.; Huang, S.-L.; Zheng, L.; Xu, X.; Zhang,"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "L.; and Huang, J. 2019. An efficient approach to informative"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "feature extraction from multimodal data.\nIn Proceedings of"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "the AAAI Conference on Artificial Intelligence, volume 33,"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "5281–5288."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Wu, Z.; Xiong, Y.; Yu, S. X.; and Lin, D. 2018. Unsuper-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "vised feature learning via non-parametric instance discrimi-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "nation.\nIn Proceedings of the IEEE conference on computer"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "vision and pattern recognition, 3733–3742."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Yang, H.; Gao, X.; Wu, J.; Gan, T.; Ding, N.; Jiang, F.; and"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Nie, L. 2023a. Self-adaptive Context and Modal-interaction"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Modeling For Multimodal Emotion Recognition.\nIn Find-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "ings of the Association for Computational Linguistics: ACL"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "2023, 6267–6281. Toronto, Canada: Association for Com-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "putational Linguistics."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Yang, K.; Zhang, T.; Alhuzali, H.; and Ananiadou, S. 2023b."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Cluster-level contrastive learning for emotion recognition in"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "conversations.\nIEEE Transactions on Affective Computing."
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Zhang, D.; Chen, F.; and Chen, X. 2023. DualGATs: Dual"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Graph Attention Networks for Emotion Recognition in Con-"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "the 61st Annual Meeting of\nversations.\nIn Proceedings of"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "the Association for Computational Linguistics\n(Volume 1:"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Long Papers), 7395–7408. Toronto, Canada: Association for"
        },
        {
          "nition. In Proceedings of the 59th Annual Meeting of the As-": "Computational Linguistics."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal Attentive Learning for Real-time Explainable Emotion Recognition in Conversations",
      "authors": [
        "B Arumugam",
        "S Bhattacharjee",
        "J Yuan"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
      "citation_id": "2",
      "title": "An Information-Theoretic Approach to Transferability in Task Transfer Learning",
      "authors": [
        "Y Bao",
        "Y Li",
        "S.-L Huang",
        "L Zhang",
        "L Zheng",
        "A Zamir",
        "L Guibas"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "3",
      "title": "Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan",
        "F Chen",
        "J Shao",
        "S Zhu",
        "H Shen"
      ],
      "year": "2008",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Improved regularization of convolutional neural networks with cutout",
      "authors": [
        "T Devries",
        "G Taylor"
      ],
      "year": "2017",
      "venue": "Improved regularization of convolutional neural networks with cutout",
      "arxiv": "arXiv:1708.04552"
    },
    {
      "citation_id": "5",
      "title": "Das statistische Problem der Korrelation als Variations-und Eigenwertproblem und sein Zusammenhang mit der Ausgleichsrechnung",
      "authors": [
        "H Gebelein"
      ],
      "year": "1941",
      "venue": "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik"
    },
    {
      "citation_id": "6",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "7",
      "title": "A connection between correlation and contingency",
      "authors": [
        "H Hirschfeld"
      ],
      "year": "1935",
      "venue": "Mathematical Proceedings of the Cambridge Philosophical Society"
    },
    {
      "citation_id": "8",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "9",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "12",
      "title": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
      "authors": [
        "D Kingma",
        "J Ba",
        "J Lee",
        "W Lee"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "13",
      "title": "HiTrans: A Transformer-Based Context-and Speaker-Sensitive Model for Emotion Detection in Conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "15",
      "title": "Cross-Modal Fusion Techniques for Utterance-Level Emotion Recognition from Text and Speech",
      "authors": [
        "J Luo",
        "H Phan",
        "J Reiss"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "16",
      "title": "An efficient approach for audio-visual emotion recognition with missing labels and missing modalities",
      "authors": [
        "F Ma",
        "S.-L Huang",
        "L Zhang",
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "2021 IEEE international conference on multimedia and Expo (ICME)"
    },
    {
      "citation_id": "17",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "On measures of dependence",
      "authors": [
        "A Rényi"
      ],
      "year": "1959",
      "venue": "Acta mathematica hungarica"
    },
    {
      "citation_id": "20",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recog-nition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations",
      "authors": [
        "T Shi",
        "S.-L Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "",
      "authors": [
        "Canada Toronto"
      ],
      "venue": ""
    },
    {
      "citation_id": "23",
      "title": "Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation",
      "authors": [
        "X Song",
        "L Huang",
        "H Xue",
        "S Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "",
      "authors": [
        "Abu Dhabi"
      ],
      "venue": ""
    },
    {
      "citation_id": "25",
      "title": "Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks",
      "authors": [
        "X Sun",
        "B Wei",
        "X Ren",
        "S Ma"
      ],
      "year": "2017",
      "venue": "Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks",
      "arxiv": "arXiv:1710.10393"
    },
    {
      "citation_id": "26",
      "title": "Context or Knowledge is Not Always Necessary: A Contrastive Learning Framework for Emotion Recognition in Conversations",
      "authors": [
        "G Tu",
        "B Liang",
        "R Mao",
        "M Yang",
        "R Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "27",
      "title": "Context-and Sentiment-Aware Networks for Emotion Recognition in Conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu",
        "D Jiang",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "An efficient approach to informative feature extraction from multimodal data",
      "authors": [
        "L Wang",
        "J Wu",
        "S.-L Huang",
        "L Zheng",
        "X Xu",
        "L Zhang",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Unsupervised feature learning via non-parametric instance discrimination",
      "authors": [
        "Z Wu",
        "Y Xiong",
        "S Yu",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "2023a. Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition",
      "authors": [
        "H Yang",
        "X Gao",
        "J Wu",
        "T Gan",
        "N Ding",
        "F Jiang",
        "L Nie"
      ],
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "31",
      "title": "2023b. Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "H Alhuzali",
        "S Ananiadou",
        "D Zhang",
        "F Chen",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}