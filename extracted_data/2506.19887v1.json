{
  "paper_id": "2506.19887v1",
  "title": "Mater: Multi-Level Acoustic And Textual Emotion Representation For Interpretable Speech Emotion Recognition",
  "published": "2025-06-24T05:35:53Z",
  "authors": [
    "Hyo Jin Jon",
    "Longbin Jin",
    "Hyuntaek Jung",
    "Hyunseo Kim",
    "Donghun Min",
    "Eun Yi Kim"
  ],
  "keywords": [
    "speech emotion recognition",
    "multi-level feature extraction",
    "multimodal",
    "ensemble Word-Level Utterance-Level Embedding-Level MATER Prosody Loudness",
    "Jitter",
    "Shimmer",
    "etc Loudness",
    "HNR",
    "Pause",
    "Jitter",
    "Shimmer",
    "etc"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents our contributions to the Speech Emotion Recognition in Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion recognition and emotional attribute prediction. To handle the complexities of natural speech, including intra-and inter-subject variability, we propose Multi-level Acoustic-Textual Emotion Representation (MATER)-a novel hierarchical framework that integrates acoustic and textual features at the word, utterance, and embedding levels. By fusing low-level lexical and acoustic cues with high-level contextualized representations, MATER effectively captures both fine-grained prosodic variations and semantic nuances. Additionally, we introduce an uncertainty-aware ensemble strategy to mitigate annotator inconsistencies, improving robustness in ambiguous emotional expressions. MATER ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of 0.5928, securing second place in valence prediction with an impressive CCC of 0.6941.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are fundamental to human experience, influencing decision-making, communication, and mental well-being  [1] . Speech, as the most common form of communication, conveys a rich blend of emotional cues, including speaker identity, affect, and linguistic emphasis  [2] . Consequently, speech emotion recognition (SER) has gained significant attention over the past two decades for its ability to recognize emotions across categorical labels  [3]  or continuous emotional dimensions  [4] .\n\nDespite extensive research, most SER datasets fail to fully capture real-world emotional expressions, as they are often composed of acted  [5, 6, 7]  or elicited  [8, 9]  recordings. Even datasets featuring natural speech  [10, 11, 12]  are often constrained by a limited number of participants, reducing their generalizability to diverse real-world scenarios. To address these issues, the MSP-Podcast corpus  [13] , provided in the Speech Emotion Recognition in Naturalistic Conditions (SERNC) Challenge  [14] , offers natural speech from 2,826 speakers, supporting two tasks: Task 1 (Categorical Emotion Recognition) and Task 2 (Emotional Attribute Prediction).\n\nOne of the primary challenges in naturalistic SER is developing representations that are robust to intra-and inter-speaker variability while effectively capturing the complexity of emotional expression. To address this, we introduce Multi-level Acoustic and Textual Emotion Representation (MATER)-a novel hierarchical framework that integrates acoustic and tex-",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Model",
      "text": "WavLM, HuBERT",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Syntax",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Part-Of-Speech Tags",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sentiment",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Seance",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Language Model",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Bert, T5",
      "text": "So yeah.\n\nWhat do you think?\n\nFigure  1 : The MATER framework that integrates word-, utterance-, and embedding-level features to construct robust and context-aware emotion representations.\n\ntual features across multiple levels for a comprehensive understanding of emotions. As illustrated in Figure  1 , MATER systematically models emotions at three distinct levels: 1) Wordlevel: Extracting syntax-aware prosodic cues to capture the finegrained interplay between linguistic structure and intonation.\n\n2) Utterance-level: Analyzing sentiment and rhythmic patterns over the entire speech segment to capture both lexical meaning and overall emotional nuances. 3) Embedding-level: Leveraging pretrained deep models (e.g., WavLM, HuBERT) to derive context-sensitive and speaker-invariant representations for better generalization. This multi-level framework enables MATER to model emotions from low-level syntactic and prosodic cues to high-level contextualized representations, effectively integrating both speech and text for robust emotion modeling.\n\nA further challenge in SER with natural speech arises from overlapping, ambiguous, and highly variable emotional expressions, which often hinder annotator consensus. This inconsistency introduces confidence disparities and biases across emotion categories. To tackle this, we propose a novel uncertaintyaware ensemble that improves robustness by selecting emotion predictions with the least uncertainty, thereby mitigating annotation biases and enhancing reliability.\n\nMATER achieves high-ranking performance in both tasks of the SERNC Challenge, with a Macro-F1 of 41.01% and an average CCC of 0.5928. The results underscore its effectiveness in capturing fine-grained emotional nuances in natural speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "This study utilizes the MSP-Podcast corpus  [13] , provided by the organizers of the SERNC Challenge. This dataset contains short naturalistic speech segments extracted from podcasts. The dataset is annotated for two tasks: 1. Categorical Emotion Recognition-Speech segments are labeled into eight emotion categories; Angry (A), Contempt (C), Disgust (D), Fear (F), Happy (H), Neutral (N), Sad (S), and Surprise (U). Samples labeled as \"Other\" or those lacking annotator agreement are excluded. 2. Emotional Attribute Prediction-Speech is rated on a sevenpoint Likert scale along three emotional dimensions: arousal (calm to active), dominance (weak to strong), and valence (negative to positive). The train and development sets contain 84,260 samples from 2,112 speakers and 31,961 samples from 714 speakers, respectively. The test set comprises 3,200 balanced samples across eight emotion categories. However, unlike the train and development sets, it does not provide transcripts or additional metadata. To ensure consistency, we generate transcripts and forced alignments for all sets using Whisper-large-v3  [15] .\n\nGiven its real-world nature, the dataset presents challenges such as class imbalance and inconsistent annotation consensus across emotion categories, as illustrated in Figure  2 . To address the class imbalance, we evaluate models using the average performance across five in-house test sets, each randomly sampled from the development set with 326 samples per emotion. Additionally, to manage annotation uncertainties, we introduce a novel ensemble strategy that employs a non-parametric approach to enhance prediction reliability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mater",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction",
      "text": "To model emotional expressions in natural speech, we extract acoustic and textual features across three different levels. Each level captures complementary information, enabling the model to learn both fine-grained speech patterns and high-level contextual cues. The following sections detail each feature level. Word-level Word-level features encode both syntactic and prosodic aspects of speech. A BERTweet-based syntactic parser  [16]  extracts linguistic patterns, including grammatical person information for pronouns, forming a 20-dimensional syntactic feature vector per word. To capture prosodic variations, a 22dimensional feature vector is extracted using the openSMILE library  [17] , incorporating attributes like loudness, jitter, shimmer, alpha ratio, and voiced/unvoiced segment statistics. By concatenating these features, MATER creates a syntax-aware prosodic representation, allowing the model to learn the interplay between linguistic structure and intonation, which plays a crucial role in emotion expression.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Utterance-Level Features Embedding-Level Features Word-Level Features",
      "text": "Utterance-level Utterance-level features provide a broader perspective on the emotional characteristics of speech by modeling sentiment and rhythmic patterns. Sentiment features are derived from the SEANCE feature set  [18] , producing a 517dimensional representation that encapsulates affective tendencies across the entire transcript. Rhythmic features, on the other hand, are extracted to analyze the flow, intensity, and nuances of speech. These include loudness, jitter, shimmer, Harmonicto-Noise Ratio (HNR), pauses, and voiced/unvoiced segment statistics, resulting in a 34-dimensional feature vector. By incorporating both sentiment and rhythm, the model captures not only the meaning of words but also how they are delivered, which is critical for distinguishing subtle emotional variations.\n\nEmbedding-level Embedding-level features leverage pretrained audio and text encoders to derive context-sensitive and speaker-invariant representations. The audio encoders, WavLM  [19]  and HuBERT  [20] , capture rich phonetic and prosodic information, while the text encoders, BERT  [21]  and T5  [22] , provide semantically informed representations of spoken content.\n\nTo enhance domain adaptation, we post-pretrain these encoders on the MSP-Podcast corpus, following the baseline's strategy of fine-tuning WavLM with attentive statistics pooling  [23] . Empirical results confirm that models trained with domain-adapted embeddings improve SER performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Aggregation",
      "text": "MATER integrates multi-level features through dedicated processing architectures. Figure  3  illustrates the overall aggregation process. At the word-level, features are fed into a two-layer LSTM  [24] , where the final hidden state serves as the word- At the utterance-level, features are first processed through a piecewise linear embedding (PLE) layer  [25] , followed by a linear layer to produce a fixed-dimensional representation. At the embedding-level, when multiple embedding sources are used, a Perceiver architecture  [26]  fuses them into a single representation. Otherwise, the pooled features are used directly without additional processing. Finally, the concatenated multi-level embeddings are fed into linear layers to predict emotion categories or continuous emotional attributes, providing a comprehensive and context-aware emotion representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Uncertainty-Aware Ensemble",
      "text": "SER faces inherent challenges due to intra-and inter-speaker variability, as well as overlapping and ambiguous emotional ex-pressions. These factors contribute to annotation inconsistencies, leading to biased predictions and reduced model reliability.\n\nCertain emotions, such as contempt and disgust, exhibit lower annotator consensus, resulting in unstable confidence scores and high misclassification errors. Previous attempts, such as Shamsi et al.  [27] , sought to mitigate these biases by optimizing decision thresholds, but such approaches often lead to overfitting and fail to generalize across diverse evaluation sets.\n\nTo address this issue, we introduce a novel uncertaintyaware ensemble strategy designed for balanced evaluation sets. Instead of tuning the decision thresholds, we estimate epistemic uncertainty by ranking predicted probabilities within each emotion category. The final ensemble prediction is obtained by averaging these rank-based uncertainties from the top-performing models across feature combinations, prioritizing high-confidence predictions. This approach reduces ambiguity and ensures robust predictions across emotion categories, even with biased label consensus. Additionally, unlike parametric methods, our rank-based method requires no additional optimization, making it inherently adaptable and generalizable across datasets. These advantages provide a scalable and reliable solution for improving SER in real-world settings. The detailed ensemble process is outlined in Algorithm 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We evaluate MATER and its individual feature levels. For individual levels, word-level features are processed using a twolayer LSTM (128 hidden units), while utterance-level features are classified via SVM with an RBF kernel. Embedding-level features are fine-tuned using large models with attentive statistical pooling, following the baseline. In MATER, word-and utterance-level features are projected into 128-dimensional vectors, while the Perceiver produces a 768-dimensional output with a 64 √ó 768 latent array. Each embedding-level feature passes through the Perceiver block twice before integration.\n\nLoss functions are task-specific: weighted cross-entropy for Task 1 and concordance correlation coefficient (CCC) loss for",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance On Emotional Attribute Prediction",
      "text": "MATER effectively captures emotional polarity through its multi-level integration of acoustic and textual features. As shown in Table  2 , valence prediction improves significantly, whereas gains in arousal and dominance remain limited. This aligns with the intuition that valence is more text-dependent, while arousal and dominance are driven by acoustic cues. Since MATER integrates both modalities, its text-heavy fusion may not fully exploit arousal-and dominance-related acoustic variations. Despite this, MATER remains competitive across all emotional dimensions, achieving a strong overall ranking.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Post-Challenge Analysis On Mater",
      "text": "Due to time constraints during the challenge, a detailed feature impact analysis was not fully conducted. Post-challenge investigations reveal key correlations between feature sets and task performance, offering interpretable insights for further refinement.\n\nAs shown in Figure  4 , acoustic features consistently outperform textual ones across both tasks, highlighting the importance of prosodic and spectral variations in SER. At the embeddinglevel, optimal encoders differ across tasks, emphasizing the need for task-specific encoder selection. Additionally, multimodal fusion in MATER enhances performance at the word and utterance levels, confirming its effectiveness in integrating  acoustic and textual cues. These findings suggest that adaptive feature weighting or task-driven feature selection could further optimize MATER's performance. Given that arousal and dominance rely more on acoustic cues, a dynamic fusion strategy balancing audio-text integration may yield additional gains.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce MATER, a hierarchical framework that integrates acoustic and textual features across word, utterance, and embedding levels, capturing both low-level cues and high-level contextual representations. Our uncertainty-aware ensemble enhances robustness against annotation inconsistencies, improving reliability in ambiguous emotional expressions. MATER ranked fourth in both tasks of the SERNC Challenge, achieving a Macro-F1 of 41.01% and an average CCC of 0.5928, notably securing the second place in valence prediction. Despite its effectiveness, challenges remain in arousal and dominance prediction. Future work will focus on emotion-specific feature selection and extending MATER to diverse SER datasets, advancing toward a more robust real-world SER system.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The MATER framework that integrates word-,",
      "page": 1
    },
    {
      "caption": "Figure 1: , MATER sys-",
      "page": 1
    },
    {
      "caption": "Figure 2: Emotional category distribution and annotator agree-",
      "page": 2
    },
    {
      "caption": "Figure 3: Feature aggregation of MATER. Embedding-level fea-",
      "page": 2
    },
    {
      "caption": "Figure 3: illustrates the overall aggrega-",
      "page": 2
    },
    {
      "caption": "Figure 4: , acoustic features consistently outper-",
      "page": 4
    },
    {
      "caption": "Figure 4: Performance comparison across feature levels for",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "So yeah. Whatdoyou think?\nWord-Level Utterance-Level Embedding-Level\nProsody Rhythm Audio Model\nLoudness, Jitter, Loudness, HNR, WavLM, HuBERT\nShimmer, etc. Pause, Jitter,\nShimmer, etc.\nSyntax Sentiment Language Model\nPart-Of-Speech SEANCE BERT, T5\nTags": "MATER"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‚Äù ‚Äúwhat‚Äù ‚ÄúSo yeah. What\nEmbedding-Level\n‚Äúthink?‚Äù do you think?‚Äù\nFeatures\nLatent\nArray\nLevel Utterance-Level KV Q\nures Features\nCross Attn.\nSelf Attn.\nM PLE Perceiver √óùüê\nM Linear Average Pool": "Concat\nMATER\near Classifier Linear Regressor"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: , valence prediction improves significantly,",
      "data": [
        {
          "31.70%": ".6839"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "35.81%": ".6664"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "28.10%": ".4123"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "29.13%": ".3275"
        },
        {
          "29.13%": "10.39%"
        },
        {
          "29.13%": ".3425"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "7.83%": ".2633"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "6.70%": ".1409"
        },
        {
          "6.70%": "14.72%"
        },
        {
          "6.70%": ".5205"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "12.98%": ".4986"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: , word-level features contribute more signif-",
      "data": [
        {
          "9.35%": ".2392"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotional well-being, mental health awareness, and prevention of suicide: Covid-19 pandemic and digital psychiatry",
      "authors": [
        "A Kakunje",
        "R Mithur",
        "M Kishor"
      ],
      "year": "2020",
      "venue": "Archives of Medicine and Health Sciences"
    },
    {
      "citation_id": "3",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "4",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "5",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "6",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "The vera am mittag german audio-visual emotional speech database",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "2008 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "8",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Cheavd: a chinese natural emotional audio-visual database",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao",
        "W Bao",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "13",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I √úlgen",
        "T Thebaud",
        "L Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions"
    },
    {
      "citation_id": "16",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "17",
      "title": "Annotating the tweebank corpus on named entity recognition and building nlp models for social media analysis",
      "authors": [
        "H Jiang",
        "Y Hua",
        "D Beeferman",
        "D Roy"
      ],
      "year": "2022",
      "venue": "Annotating the tweebank corpus on named entity recognition and building nlp models for social media analysis",
      "arxiv": "arXiv:2201.07281"
    },
    {
      "citation_id": "18",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M W√∂llmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Sentiment analysis and social cognition engine (seance): An automatic tool for sentiment, social cognition, and social-order analysis",
      "authors": [
        "S Crossley",
        "K Kyle",
        "D Mcnamara"
      ],
      "year": "2017",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "20",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "22",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "23",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "24",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Attentive statistics pooling for deep speaker embedding",
      "arxiv": "arXiv:1803.10963"
    },
    {
      "citation_id": "25",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter"
      ],
      "year": "1997",
      "venue": "Neural Computation MIT"
    },
    {
      "citation_id": "26",
      "title": "On embeddings for numerical features in tabular deep learning",
      "authors": [
        "Y Gorishniy",
        "I Rubachev",
        "A Babenko"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Perceiver: General perception with iterative attention",
      "authors": [
        "A Jaegle",
        "F Gimeno",
        "A Brock",
        "O Vinyals",
        "A Zisserman",
        "J Carreira"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "28",
      "title": "The conilium proposition for odyssey emotion challenge leveraging major class with complex annotations",
      "authors": [
        "M Shamsi",
        "L Gauder",
        "M Tahon"
      ],
      "venue": "The Speaker and Language Recognition Workshop"
    }
  ]
}