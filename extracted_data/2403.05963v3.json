{
  "paper_id": "2403.05963v3",
  "title": "Robust Emotion Recognition In Context Debiasing",
  "published": "2024-03-09T17:05:43Z",
  "authors": [
    "Dingkang Yang",
    "Kun Yang",
    "Mingcheng Li",
    "Shunli Wang",
    "Shuaibing Wang",
    "Lihua Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference, we eliminate the direct context effect from the total causal effect by comparing factual and counterfactual outcomes, resulting in bias mitigation and robust prediction. As a modelagnostic framework, CLEF can be readily integrated into existing methods, bringing consistent performance gains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "\"Context is the key to understanding, but it can also be the key to misunderstanding.\"",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "-Jonathan Lockwood Huie",
      "text": "As the spiritual grammar of human life, emotions play an essential role in social communication and intelligent automation  [21] . Accurately recognizing subjects' emotional states from resource-efficient visual content has been ex- § Corresponding author. tensively explored in various fields, including online education  [15] , driving monitoring  [59] , and human-computer interaction  [1] . Conventional works have focused on extracting emotion-related information from subject attributes, such as facial expressions  [9] , body postures  [3] , acoustic behaviors  [26] , or multimodal combinations  [22, 31, 54, 55, 57, 60] . Despite considerable advances in subject-oriented efforts, their performance suffers from severe bottlenecks in uncontrolled environments. As shown in Figure  1a , physical representations of subjects in wild-collected images are usually indistinguishable (e.g., ambiguous faces) due to natural occlusions that fail to provide usable emotional signals.\n\nInspired by psychological research  [2] , context-aware emotion recognition (CAER)  [18]  has been proposed to seek additional affective semantics from situational contexts. The contexts  [19]  are typically considered to include out-of-subject factors, such as background objects, place attributes, scene elements, and dynamic interactions of surrounding agents. These rich contextual stimuli promisingly provide complementary emotion clues for accurate recogni- tion. Most existing methods perform emotion inference by extracting ensemble representations from subjects and contexts using sophisticated structures  [24, 32, 33, 53, 56, 65] or customized mechanisms  [5, 7, 11, 14, 25, 42] . Nevertheless, a recent study  [58]  found that CAER models tend to rely on spurious correlations caused by a context bias rather than beneficial ensemble representations. An intuitive illustration is displayed in Figure  1 . We first randomly choose some training samples on the EMOTIC dataset  [19]  and perform unsupervised clustering. From Figure  1a , samples containing seaside-related contexts form compact feature clusters, confirming the semantic similarity in the feature space. These samples have positive emotion categories, while negative emotions are nonexistent in similar contexts.\n\nIn this case, the model  [32]  is easily misled to capture spurious dependencies between context-specific semantics and emotion labels. In the testing phase from Figure  1b , oriented to the sample with similar contexts but negative emotion categories, the model is confounded by the harmful context bias to infer completely wrong emotional states.\n\nA straightforward solution is to conduct a randomized controlled trial by collecting images with all emotion annotations in all contexts. This manner is viewed as an approximate intervention for biased training. However, the current CAER debiasing effort  [58]  is sub-optimal since the predefined intervention fails to decouple good and bad context semantics. We argue that context semantics consists of the good prior and the bad bias. The toy experiments are performed to verify this insight. Specifically, we train on the EMOTIC dataset separately using the subject branch, the ensemble branches, and the context branch of a CAER baseline  [18]  in Figure  2a . Recognized subjects in samples during context training are masked to capture the direct context effect. Observing the testing results in Figure  2b , the context prior in ensemble learning as the valuable indirect effect helps the model filter out unnecessary candidates (i.e., removing the \"Disapproval\" and \"Esteem\" categories) compared to the subject branch. Conversely, the harmful bias as the direct context effect in the context branch builds a misleading mapping between dim contexts and negative emotions during training, causing biased predictions.\n\nTo disentangle the two effects in context semantics and achieve more appropriate context debiasing, we propose a unified counterfactual emotion inference (CLEF) framework from a causality perspective. CLEF focuses on assisting existing CAER methods to mitigate the context bias and breakthrough performance bottlenecks in a model-agnostic manner rather than beating them. Specifically, we first formulate a generalized causal graph to investigate causal relationships among variables in the CAER task. Along the causal graph, CLEF estimates the direct context effect caused by the harmful bias through a non-intrusive context branch during the training phase. Meanwhile, the valuable indirect effect of the context prior in ensemble representations of subjects and contexts is calculated following the vanilla CAER model. In the inference phase, we subtract the direct context effect from the total causal effect by depicting a counterfactual scenario to exclude bias interference. This scenario is described as follows:\n\nCounterfactual CAER: What would the prediction be, if the model only sees the confounded context and does not perform inference via vanilla ensemble representations? Intuitively, ensemble representations in the counterfactual outcome are blocked in the no-treatment condition. As such, the model performs biased emotion estimation relying only on spurious correlations caused by the pure context bias, which results similarly to the predictions of the context branch in Figure  2b . By comparing factual and counterfactual outcomes, CLEF empowers the model to make unbiased predictions using the debiased causal effect. The main contributions are summarized as follows:\n\n• We are the first to embrace counterfactual thinking to investigate causal effects in the CAER task and reveal that the context bias as the adverse direct causal effect misleads the models to produce spurious prediction shortcuts.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Context-Aware Emotion Recognition. Benefiting from advances in deep learning algorithms  [6, 27-29, 46-48, 50-52, 61-63] , traditional emotion recognition typically infers emotional states from subject-oriented attributes, such as facial expressions  [9, 23] , body postures  [3, 59] , and acoustic behaviours  [26, 31] . However, these efforts are potentially vulnerable in practical applications since subject characteristics in uncontrolled environments are usually indistinguishable, leading to severe performance deterioration.\n\nRecently, a pioneering work  [18]  inspired by psychological research  [2]  has advocated extracting complementary emotional clues from rich contexts, called context-aware emotion recognition (CAER). Kosti et al.  [19]  begin by utilizing a two-stream convolutional neural network (CNN) to capture effective semantics from subject-related regions and global contexts of complete images. The implementation is similar to the ensemble branch training in Figure  2a . After that, most CAER methods  [5, 7, 11, 14, 20, 24, 25, 32, 33, 53, 56, 64, 65]  follow an ensemble learning pattern: i) extracting unimodal/multimodal features from subject attributes; ii) learning emotionally relevant features from created contexts based on different definitions; and iii) producing ensemble representations for emotion predictions via fusion mechanisms. For instance, Yang et al.  [56]  discretize the context into scenes, agent dynamics, and agent-object interactions, using customized components to learn complementary contextual information. Despite achievements, they invariably suffer from performance bottlenecks due to spurious correlations caused by the context bias. Causal Inference. Causal inference  [12]  is first extensively used in economics  [45]  and psychology  [10]  as a scientific theory that seeks causal relationships among variables. The investigation of event causality generally follows two directions: intervention and counterfactuals. Intervention  [36]  aims to actively manipulate the probability distributions of variables to obtain unbiased estimations or discover confounder effects. Counterfactuals [37] typically utilize distinct treatment conditions to imagine outcomes that are contrary to factual determinations, empowering systems to reason and think like humans. In recent years, several learningbased approaches have attempted to introduce causal inference in diverse fields to pursue desired model effects and exclude the toxicity of spurious shortcuts, including scene graph generation  [44] , visual dialogue  [34, 40] , image recognition  [4, 30, 49] , and adversarial learning  [16, 17] .\n\nThe CAER debiasing effort  [58]  most relevant to our work utilizes a predefined dictionary to approximate interventions and adopts memory-query operations to mitigate the bias dilemma. Nevertheless, the predefined-level intervention fails to capture pure bias effects in the context semantics, causing a sub-optimal solution. Inspired by  [34] , we remove the adverse context effect by empowering models with the debiasing ability of twice-thinking through counterfactual causality, which is fundamentally different in design philosophy and methodology.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminaries",
      "text": "Before starting, we first introduce the concepts and notations related to causal inference to facilitate a better understanding of our framework and philosophy. Causal graph is a highly generalized analytical tool to reveal causal dependencies among variables. It usually follows the structured causal mode  [39]  defined as a directed acyclic graph G = {V, E}, where V stands for a set of variables and E implies the corresponding causal effects. A causal graph example with three variables is intuitively displayed in Figure  3a . Here, we represent a random variable as a capital letter (e.g., P ), and denote its observed value as a lowercase letter (e.g., p). The causality from cause P to effect Q is reflected in two parts: the direct effect follows the causal link P → Q, and the indirect effect follows the link P → M → Q through the mediator variable M . Counterfactual inference endows the models with the ability to depict counterfactual outcomes in factual observations through different treatment conditions  [37] . In the factual outcome, the value of Q would be formalized under the conditions that P is set to p and M is set to m:\n\nCounterfactual outcomes can be obtained by exerting distinct treatments on the value of P . As shown in Figure  3b , when P is set to p * , and the descendant M is changed, we have  According to the causal theory  [38] , The Total Effect (TE) of treatment P = p on Q by comparing the two hypothetical outcomes is formulated as:\n\nTE can be disentangled into the Natural Direct Effect (NDE) and the Total Indirect Effect (TIE)  [12] . NDE reflects the effect of P = p on Q following the direct link P → Q, and excluding the indirect effect along link P → M → Q due to M is set to the value when P had been p * . It reveals the response of Q when P converts from p to p * :\n\nIn this case, TIE is calculated by directly subtracting NDE from TE, which is employed to measure the unbiased prediction results in our framework:\n\n4. The proposed CLEF Framework The mediator E is obtained depending on the feature integration mechanisms of different vanilla methods, such as feature concatenation  [18]  or attention fusion  [20] . In particular, C provides the valuable context prior along the good causal link C → E → Y , which gives favorable estimations of potential emotional states when the subjects' characteristics are indistinguishable.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Counterfactual Inference",
      "text": "Our design philosophy is to mitigate the interference of the harmful context bias on model predictions by excluding the biased direct effect along the link X → C → Y . Following the notations on causal effects in Section 3, the causality in the factual scenarios is formulated as follows:\n\nY c,e (X) reflects confounded emotion predictions because it suffers from the detrimental direct effect of C, i.e., the pure context bias. To disentangle distinct causal effects in the context semantics, we calculate the Total Effect (TE) of C = c and S = s, which is expressed as follows:\n\nHere, c * and e * represent the non-treatment conditions for observed values of C and E, where c and s leading to e are not given. Immediately, we estimate the Natural Direct Effect (NDE) for the harmful bias in context semantics:\n\nY c,e * (X) describes a counterfactual outcome where C is set to c and E would be imagined to be e * when C had been c * and S had been s * . The causal notation is expressed as:\n\nSince the indirect causal effect of ensemble representations E on the link X → C/S → E → Y is blocked, the model can only perform biased predictions by relying on the direct context effect in the link X → C → Y that causes spurious correlations. To exclude the explicitly captured context bias in NDE, we subtract NDE from TE to estimate Total Indirect Effect (TIE):\n\nWe employ the reliable TIE as the unbiased prediction in the inference phase.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Instantiation",
      "text": "Framework Structure. From Figure  4 , CLEF's predictions consist of two parts: the prediction Y c (X) = N C (c|x) of the additional context branch (i.e., X → C → Y ) and Y e (X) = N C,S (c, s|x) of the vanilla CAER model (i.e., X → C/S → E → Y ). The context branch is instantiated as a simple neural network N C (•) (e.g., ResNet  [13] ) to receive context images with masked recognized subjects. The masking operation forces the network to focus on pure context semantics for estimating the direct effect. For a given input x, its corresponding context image I x is expressed as:\n\nwhere bbox subject means the bounding box of the subject. N C,S (•) denotes any CAER model based on their specific mechanisms to learn ensemble representations e from c and s for prediction. Subsequently, a pragmatic fusion strategy ϕ(•) is introduced to obtain the final score Y c,e (X):\n\nwhere σ is the sigmoid activation. Training Procedure. As a universal framework, we take the multi-class classification task in Figure  4  as an example to adopt the cross-entropy loss CE(•) as the optimization objective. The task-specific losses for Y c,e (X) and Y c,e * (X) are as follows:\n\nwhere y means the ground truth. Since neural models cannot handle no-treatment conditions where the inputs are void, we devise a trainable parameter initialized by the uniform distribution in practice to represent the imagined Y e * (X), which is shared by all samples. The design intuition is that the uniform distribution ensures a safe estimation for NDE, which is justified in subsequent ablation studies. To avoid inappropriate Y e * (X) that potentially causes TIE to be dominated by TE or NDE, we employ the Kullback-Leibler divergence KL(•) to regularize the difference between Y c,e * (X) and Y c,e (X) to estimate Y e * (X):\n\nThe final loss is expressed as:\n\nInference Procedure. According to Eq. (  9 ), the debiased prediction is performed as follows:\n\n5. Experiments",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "Experiments are conducted on two large-scale image-based CAER datasets, including EMOTIC  [19]  and CAER-S  [20] .\n\nEMOTIC is the first benchmark to support emotion recognition in real-world contexts, which has from video clips. These images record 7 emotional states of different subjects in various context scenarios from 79 TV shows. The data samples are randomly divided into training, validation, and testing sets in the ratio of 7:1:2. We utilize the standard mean Average Precision (mAP) and classification accuracy to evaluate the results on the EMOTIC and CAER-S datasets, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Zoo",
      "text": "We evaluate the effectiveness of the proposed CLEF using five representative methods, which have completely different network structures and contextual modelling paradigms. Concretely, EMOT-Net  [18]  is a two-stream classical CNN model where one stream extracts human features from body regions, and the other captures global context semantics. CAER-Net  [20]  extracts subject attributes from faces and uses the images after hiding faces as background contexts. GNN-CNN  [65]  utilizes the graph neural network (GNN) to integrate emotion-related objects in contexts and distills subject information with a VGG-16  [43] . CD-Net  [53]  designs a tube-transformer to perform fine-grained interactions from facial, bodily, and contextual features. Emoti-Con  [32]  employs attention and depth maps to model context representations. Subject-relevant features are extracted from facial expressions and body postures.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We use a ResNet-152  [13]  pre-trained on the Places365  [66]  dataset to parameterize the non-invasive context branch in CLEF. The output of the last linear layer is replaced to produce task-specific numbers of neurons for predictions. Rich scene attributes in Places365 provide proper semantics for distilling the context bias. In addition to the annotated EMOTIC, we employ the Faster R-CNN  [41]  to detect bounding boxes of recognized subjects in CAER-S.\n\nImmediately, the context images I x are obtained by masking the target subjects in samples based on the corresponding bounding boxes. For a fair comparison, the five selected CAER methods are reproduced via the PyTorch toolbox  [35]  following their reported training settings, including the optimizer, loss function, learning rate strategy, etc. All models are implemented on NVIDIA Tesla V100 GPUs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "We compare the five CLEF-based methods with existing SOTA models, including HLCR  [7] , TEKG  [5] , RRLA  [24] , VRD [14], SIB-Net  [25] , MCA  [56] , and GRERN  [11] .\n\nQuantitative Results on the EMOTIC. Table  1  shows the Average Precision (AP) of the vanilla methods and their counterparts in the CLEF framework for each emotion category. We have the following critical observations. i) CLEF significantly improves the performance of all models in most categories. For instance, CLEF yields average gains of 8.33% and 6.52% on the AP scores for \"Affection\" and \"Sadness\", reflecting positivity and negativity, respectively. ii) Our framework favorably improves several categories heavily confounded by the harmful context bias due to uneven distributions of emotional states across distinct  contexts. For example, the CLEF-based models improve the AP scores for \"Engagement\" and \"Happiness\" categories to 90.46%∼97.39% and 72.37%∼87.06%, outperforming the results in the vanilla baselines by large margins. Table  2  presents the comparison results with existing models regarding the mean AP (mAP) scores. i) Thanks to CLEF's bias exclusion, the mAP scores of EMOT-Net, CAER-Net, GNN-CNN, CD-Net, and EmotiCon are consistently increased by 3.74%, 3.59%, 4.02%, 3.64%, and 2.77%, respectively. Among them, the most noticeable improvement in GNN-CNN is because the vanilla model more easily captures spurious context-emotion correlations based on fine-grained context element exploration  [65] , leading to the better debiasing effect with CLEF. ii) Compared to SIB-Net and MCA with complex module stacking  [56]  and massive parameters  [25] , the CLEF-based EmotiCon achieves the best performance with the mAP score of 38.05% through efficient counterfactual inference. Quantitative Results on the CAER-S. Table  3  provides the evaluation results on the CAER-S dataset. i) Evidently, CLEF consistently improves different baselines by decoupling and excluding the prediction bias of emotional states in the TV show contexts. Concretely, the overall accuracies of EMOT-Net, CAER-Net, GNN-CNN, CD-Net, and EmotiCon are improved by 2.52%, 2.39%, 2.32%, 3.08%, Table  3 . Quantitative results of different models and CLEF-based methods on the CAER-S dataset.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "Accuracy (%)\n\nFine-tuned VGGNet  [43]  64.85 Fine-tuned ResNet  [13]  68.46 SIB-Net  [25]  74.56 MCA  [56]  79.57 GRERN  [11]  81.31 RRLA  [24]  84 and 1.97%, respectively. ii) The gains of our framework on the CAER-S are slightly weaker than those on the EMOTIC. A reasonable explanation is that the EMOTIC contains richer context semantics than the CAER-S, such as scene elements and agent dynamics  [19] . As a result, CLEF more accurately estimates the adverse context effect and favorably removes its interference. iii) Also, we find in Figure  5  that the classification accuracies of most emotion categories across the five methods are improved appropriately.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "In Table  4 , we select the SOTA CD-Net and EmotiCon to perform thorough ablation studies on both datasets. Necessity of Framework Structure. i) When removing CAER models from CLEF, the significant performance deterioration suggests that the indirect causal effect in ensemble representations provides valuable emotion semantics. ii) When the additional context branch (ACB) is excluded, CLEF degrades to a debiased pattern that is not context-conditional, treated as TE. TE's gains are inferior to TIE's since it reduces the general bias over the whole dataset rather than the specific context bias. iii) Also, we find that the KL( images of ACB is essential for ensuring reliable capture of the context-oriented adverse direct effect. ii) When the ResNet-152 pre-trained on Places365  [66]  is replaced with the one pre-trained on ImageNet  [8]  in ACB, the gain drops prove that scene-level semantics are more expressive than object-level semantics in reflecting the context bias. This makes sense since scene attributes usually contain diverse object concepts. iii) Moreover, the improvements from CLEF gradually increase as more advanced pre-training backbones are used, which shows that our framework does not rely on a specific selection of instantiated networks. Effectiveness of No-treatment Assumption. We provide two alternatives regarding the no-treatment condition assumption, where random and average feature embeddings are obtained by the random initialization and the prior distribution of the training set, respectively. The worse-thanbaseline results imply that our uniform distribution assumption ensures a safe estimation of the biased context effect. Debiasing Ability Comparison. A gain comparison between our CLEF and the previous CAER debiasing effort CCIM on both datasets is presented in Table  5 . Intuitively, our framework consistently outperforms CCIM  [58]  in both methods. The reasonable reason is that CCIM fails to capture the pure context bias due to over-reliance on the predefined context confounders, causing sub-optimal solutions. In contrast, CLEF decouples the good context prior and the bad context effect, enabling robust debiased predictions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Qualitative Evaluation",
      "text": "Figure  6  shows the performance of vanilla CD-Net before and after counterfactual debiasing via CLEF. Intuitively, our framework effectively corrects the misjudgments of the vanilla method for emotional states in diverse contexts. Taking Figure  6a  as an example, CLEF eliminates spurious correlations between vegetation-related contexts and positive emotions, giving negative categories aligned with ground truths. Moreover, the CLEF-based CD-Net in Figure  6e  excludes misleading clues about negative emotions provided by dim contexts and achieves an unbiased prediction.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes CLEF, a causal debiasing framework based on counterfactual inference to address the context bias interference in CAER. CLEF reveals that the harmful bias confounds model performance along the direct causal effect via the tailored causal graph, and accomplishes bias mitigation by subtracting the direct context effect from the total causal effect. Extensive experiments prove that CLEF brings favorable improvements to existing models.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the context bias in the CAER task. GT",
      "page": 1
    },
    {
      "caption": "Figure 2: We conduct toy experiments to show the effects of con-",
      "page": 2
    },
    {
      "caption": "Figure 1: We first randomly",
      "page": 2
    },
    {
      "caption": "Figure 2: a. Recognized subjects in sam-",
      "page": 2
    },
    {
      "caption": "Figure 2: b. By comparing factual and counterfac-",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) Examples of a causal graph where nodes represent",
      "page": 3
    },
    {
      "caption": "Figure 3: a. Here, we represent a random variable",
      "page": 3
    },
    {
      "caption": "Figure 4: High-level overview of the proposed CLEF framework implementation. In addition to the vanilla CAER model, we introduce an",
      "page": 4
    },
    {
      "caption": "Figure 3: c, there are five variables in the pro-",
      "page": 4
    },
    {
      "caption": "Figure 2: b as an example, the context branch learns",
      "page": 4
    },
    {
      "caption": "Figure 4: , CLEF’s predic-",
      "page": 5
    },
    {
      "caption": "Figure 4: as an example",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotion classification accuracy (%) for each category of different CLEF-based methods on the CAER-S dataset.",
      "page": 7
    },
    {
      "caption": "Figure 5: that the classification accuracies of most emotion categories",
      "page": 7
    },
    {
      "caption": "Figure 6: Qualitative results of the vanilla and CLEF-based CD-Net [53] on the EMOTIC and CAER-S datasets. Three testing sample",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the performance of vanilla CD-Net before",
      "page": 8
    },
    {
      "caption": "Figure 6: a as an example, CLEF eliminates spurious cor-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Ablation study results on the EMOTIC and CAER-S Table5. DebiasingcomparisonresultsofCCIM[58]andthepro-",
      "data": [
        {
          "Anger": "Happy",
          "Neutral": "Sad"
        },
        {
          "Anger": "Disgust",
          "Neutral": "Happy"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Ablation study results on the EMOTIC and CAER-S Table5. DebiasingcomparisonresultsofCCIM[58]andthepro-",
      "data": [
        {
          "Disconnection\nDisquietment\nDoubt/Confusion\nEngagement": "Anticipation\nDoubt/Confusion\nEngagement\nSuffering\nConfidence\nExcitement\nSensitivity\nYearning",
          "Affection\nHappiness\nPeace\nSympathy": "Anticipation\nConfidence\nDisapproval\nDisconnection\nEmbarrassment\nPain\nSuffering\nSensitivity"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Humancomputer interaction with detection of speaker emotions using convolution neural networks",
      "authors": [
        "Abeer Ali Alnuaim",
        "Mohammed Zakariah",
        "Aseel Alhadlaq",
        "Chitra Shashidhar",
        "Atef Wesam",
        "Hussam Hatamleh",
        "Prashant Tarazi",
        "Rajnish Kumar Shukla",
        "Ratna"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Context in emotion perception. Current Directions in Psychological",
      "authors": [
        "Lisa Feldman",
        "Batja Mesquita",
        "Maria Gendron"
      ],
      "year": "2011",
      "venue": "Science"
    },
    {
      "citation_id": "3",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya",
        "Trisha Mittal",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "4",
      "title": "Visual causal feature learning",
      "authors": [
        "Krzysztof Chalupka",
        "Pietro Perona",
        "Frederick Eberhardt"
      ],
      "year": "2014",
      "venue": "Visual causal feature learning",
      "arxiv": "arXiv:1412.2309"
    },
    {
      "citation_id": "5",
      "title": "Incorporating structured emotion commonsense knowledge and interpersonal relation into context-aware emotion recognition",
      "authors": [
        "Jing Chen",
        "Tao Yang",
        "Ziqiang Huang",
        "Kejun Wang",
        "Meichen Liu",
        "Chunyan Lyu"
      ],
      "year": "2007",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Miss: A generative pretraining and finetuning approach for med-vqa",
      "authors": [
        "Jiawei Chen",
        "Dingkang Yang",
        "Yue Jiang",
        "Yuxuan Lei",
        "Lihua Zhang"
      ],
      "year": "2024",
      "venue": "Miss: A generative pretraining and finetuning approach for med-vqa",
      "arxiv": "arXiv:2401.05163"
    },
    {
      "citation_id": "7",
      "title": "High-level context representation for emotion recognition in images",
      "authors": [
        "Willams De",
        "Lima Costa",
        "Estefania Talavera",
        "Lucas Silva Figueiredo",
        "Veronica Teichrieb"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)"
    },
    {
      "citation_id": "8",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "Learning associative representation for facial expression recognition",
      "authors": [
        "Yangtao Du",
        "Dingkang Yang",
        "Peng Zhai",
        "Mingchen Li",
        "Lihua Zhang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "10",
      "title": "Causal inference and developmental psychology",
      "authors": [
        "Foster Michael"
      ],
      "year": "2010",
      "venue": "Developmental Psychology"
    },
    {
      "citation_id": "11",
      "title": "Graph reasoning-based emotion recognition network",
      "authors": [
        "Qinquan Gao",
        "Hanxin Zeng",
        "Gen Li",
        "Tong Tong"
      ],
      "year": "2007",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Causal inference in statistics: A primer",
      "authors": [
        "Madelyn Glymour",
        "Judea Pearl",
        "Nicholas Jewell"
      ],
      "year": "2016",
      "venue": "Causal inference in statistics: A primer"
    },
    {
      "citation_id": "13",
      "title": "Context-aware emotion recognition based on visual relationship detection",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun",
        "; Manh-Hung Hoang",
        "Soo-Hyung Kim",
        "Hyung-Jeong Yang",
        "Guee-Sang Lee"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "14",
      "title": "Emotion regulation in education: Conceptual foundations, current applications, and future directions. International Handbook of Emotions in Education",
      "authors": [
        "E Scott",
        "James Jacobs",
        "Gross"
      ],
      "year": "2014",
      "venue": "Emotion regulation in education: Conceptual foundations, current applications, and future directions. International Handbook of Emotions in Education"
    },
    {
      "citation_id": "15",
      "title": "Structural agnostic model, causal discovery and penalized adversarial learning",
      "authors": [
        "Diviyan Kalainathan",
        "Olivier Goudet",
        "Isabelle Guyon",
        "David Lopez-Paz",
        "Michèle Sebag",
        "Sam"
      ],
      "year": "2018",
      "venue": "Structural agnostic model, causal discovery and penalized adversarial learning"
    },
    {
      "citation_id": "16",
      "title": "Causalgan: Learning causal implicit generative models with adversarial training",
      "authors": [
        "Murat Kocaoglu",
        "Christopher Snyder",
        "Alexandros Dimakis",
        "Sriram Vishwanath"
      ],
      "year": "2017",
      "venue": "Causalgan: Learning causal implicit generative models with adversarial training",
      "arxiv": "arXiv:1709.02023"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2008",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "20",
      "title": "Text-oriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences",
      "authors": [
        "Yuxuan Lei",
        "Dingkang Yang",
        "Mingcheng Li",
        "Shunli Wang",
        "Jiawei Chen",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "Text-oriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences",
      "arxiv": "arXiv:2307.13205"
    },
    {
      "citation_id": "21",
      "title": "Towards robust multimodal sentiment analysis under uncertain signal missing",
      "authors": [
        "Mingcheng Li",
        "Dingkang Yang",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "22",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "Weixin Li",
        "Xuan Dong",
        "Yunhong Wang"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Sequential interactive biased network for context-aware emotion recognition",
      "authors": [
        "Xinpeng Li",
        "Xiaojiang Peng",
        "Changxing Ding"
      ],
      "year": "2007",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "25",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Learning appearance-motion normality for video anomaly detection",
      "authors": [
        "Yang Liu",
        "Jing Liu",
        "Mengyang Zhao",
        "Dingkang Yang",
        "Xiaoguang Zhu",
        "Liang Song"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "27",
      "title": "Ampnet: Appearance-motion prototype network assisted automatic video anomaly detection system",
      "authors": [
        "Yang Liu",
        "Jing Liu",
        "Kun Yang",
        "Bobo Ju",
        "Siao Liu",
        "Yuzheng Wang",
        "Dingkang Yang",
        "Peng Sun",
        "Liang Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "28",
      "title": "Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models",
      "authors": [
        "Yang Liu",
        "Dingkang Yang",
        "Yan Wang",
        "Jing Liu",
        "Liang Song"
      ],
      "year": "2023",
      "venue": "Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models",
      "arxiv": "arXiv:2302.05087"
    },
    {
      "citation_id": "29",
      "title": "Discovering causal signals in images",
      "authors": [
        "David Lopez-Paz",
        "Robert Nishihara",
        "Soumith Chintala",
        "Bernhard Scholkopf",
        "Léon Bottou"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "31",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "32",
      "title": "Multimodal and context-aware emotion perception model with multiplicative fusion",
      "authors": [
        "Trisha Mittal",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "33",
      "title": "Counterfactual vqa: A causeeffect look at language bias",
      "authors": [
        "Yulei Niu",
        "Kaihua Tang",
        "Hanwang Zhang",
        "Zhiwu Lu",
        "Xian-Sheng Hua",
        "Ji-Rong Wen"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Soumith Chintala",
        "Gregory Chanan",
        "Edward Yang",
        "Zachary Devito",
        "Zeming Lin",
        "Alban Desmaison",
        "Luca Antiga",
        "Adam Lerer"
      ],
      "year": "2017",
      "venue": "Automatic differentiation in pytorch"
    },
    {
      "citation_id": "35",
      "title": "Causal inference in statistics: An overview",
      "authors": [
        "Judea Pearl"
      ],
      "year": "2009",
      "venue": "Statistics Surveys"
    },
    {
      "citation_id": "36",
      "title": "Interpretation and identification of causal mediation",
      "authors": [
        "Judea Pearl"
      ],
      "year": "2014",
      "venue": "Psychological Methods"
    },
    {
      "citation_id": "37",
      "title": "Models, reasoning and inference",
      "authors": [
        "Judea Pearl"
      ],
      "year": "2000",
      "venue": "Models, reasoning and inference"
    },
    {
      "citation_id": "38",
      "title": "Two causal principles for improving visual dialog",
      "authors": [
        "Jiaxin Qi",
        "Yulei Niu",
        "Jianqiang Huang",
        "Hanwang Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "39",
      "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "authors": [
        "Kaiming Shaoqing Ren",
        "Ross He",
        "Jian Girshick",
        "Sun"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "40",
      "title": "Context-aware generation-based net for multi-label visual emotion recognition",
      "authors": [
        "Kun Shulan Ruan",
        "Yijun Zhang",
        "Hanqing Wang",
        "Weidong Tao",
        "Guangyi He",
        "Enhong Lv",
        "Chen"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "41",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "42",
      "title": "Unbiased scene graph generation from biased training",
      "authors": [
        "Kaihua Tang",
        "Yulei Niu",
        "Jianqiang Huang",
        "Jiaxin Shi",
        "Hanwang Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "43",
      "title": "Causal inference in economics and marketing",
      "authors": [
        "Hal R Varian"
      ],
      "year": "2016",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "44",
      "title": "Tsa-net: Tube self-attention network for action quality assessment",
      "authors": [
        "Shunli Wang",
        "Dingkang Yang",
        "Peng Zhai",
        "Chixiao Chen",
        "Lihua Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "45",
      "title": "Caspacenet: Counterfactual analysis for 6d pose estimation in space",
      "authors": [
        "Shunli Wang",
        "Shuaibing Wang",
        "Bo Jiao",
        "Dingkang Yang",
        "Liuzhen Su",
        "Peng Zhai",
        "Chixiao Chen",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "46",
      "title": "Cpr-clip: Multimodal pre-training for composite error recognition in cpr training",
      "authors": [
        "Shunli Wang",
        "Dingkang Yang",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "47",
      "title": "Visual commonsense r-cnn",
      "authors": [
        "Tan Wang",
        "Jianqiang Huang",
        "Hanwang Zhang",
        "Qianru Sun"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "48",
      "title": "Model robustness meets data privacy: Adversarial robustness distillation without original data",
      "authors": [
        "Yuzheng Wang",
        "Zhaoyu Chen",
        "Dingkang Yang",
        "Pinxue Guo",
        "Kaixun Jiang",
        "Wenqiang Zhang",
        "Lizhe Qi"
      ],
      "year": "2023",
      "venue": "Model robustness meets data privacy: Adversarial robustness distillation without original data",
      "arxiv": "arXiv:2303.11611"
    },
    {
      "citation_id": "49",
      "title": "Adversarial contrastive distillation with adaptive denoising",
      "authors": [
        "Yuzheng Wang",
        "Zhaoyu Chen",
        "Dingkang Yang",
        "Yang Liu",
        "Siao Liu",
        "Wenqiang Zhang",
        "Lizhe Qi"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "50",
      "title": "Sampling to distill: Knowledge transfer from open-world data",
      "authors": [
        "Yuzheng Wang",
        "Zhaoyu Chen",
        "Jie Zhang",
        "Dingkang Yang",
        "Zuhao Ge",
        "Yang Liu",
        "Siao Liu",
        "Yunquan Sun",
        "Wenqiang Zhang",
        "Lizhe Qi"
      ],
      "year": "2023",
      "venue": "Sampling to distill: Knowledge transfer from open-world data",
      "arxiv": "arXiv:2307.16601"
    },
    {
      "citation_id": "51",
      "title": "Context-dependent emotion recognition",
      "authors": [
        "Zili Wang",
        "Lingjie Lao",
        "Xiaoya Zhang",
        "Yong Li",
        "Tong Zhang",
        "Zhen Cui"
      ],
      "year": "2008",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "52",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "53",
      "title": "Contextual and cross-modal interaction for multi-modal speech emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Yang Liu",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "54",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Shunli Wang",
        "Yang Liu",
        "Peng Zhai",
        "Liuzhen Su",
        "Mingcheng Li",
        "Lihua Zhang"
      ],
      "year": "2007",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "55",
      "title": "Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences",
      "authors": [
        "Dingkang Yang",
        "Haopeng Kuang",
        "Shuai Huang",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "56",
      "title": "Context deconfounded emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Yuzheng Wang",
        "Shunli Wang",
        "Mingcheng Li",
        "Siao Liu",
        "Xiao Zhao",
        "Shuai Huang",
        "Zhiyan Dong",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "57",
      "title": "Aide: A vision-driven multi-view, multimodal, multi-tasking dataset for assistive driving perception",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Zhi Xu",
        "Zhenpeng Li",
        "Shunli Wang",
        "Mingcheng Li",
        "Yuzheng Wang",
        "Yang Liu",
        "Kun Yang",
        "Zhaoyu Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "58",
      "title": "Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences",
      "authors": [
        "Dingkang Yang",
        "Yang Liu",
        "Can Huang",
        "Mingcheng Li",
        "Xiao Zhao",
        "Yuzheng Wang",
        "Kun Yang",
        "Yan Wang",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "59",
      "title": "How2comm: Communication-efficient and collaboration-pragmatic multiagent perception",
      "authors": [
        "Dingkang Yang",
        "Kun Yang",
        "Yuzheng Wang",
        "Jing Liu",
        "Zhi Xu",
        "Rongbin Yin",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "60",
      "title": "Spatio-temporal domain awareness for multi-agent collaborative perception",
      "authors": [
        "Kun Yang",
        "Dingkang Yang",
        "Jingyu Zhang",
        "Mingcheng Li",
        "Yang Liu",
        "Jing Liu",
        "Hanqi Wang",
        "Peng Sun",
        "Liang Song"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "61",
      "title": "What2comm: Towards communication-efficient collaborative perception via feature decoupling",
      "authors": [
        "Kun Yang",
        "Dingkang Yang",
        "Jingyu Zhang",
        "Hanqi Wang",
        "Peng Sun",
        "Liang Song"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31th ACM International Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "62",
      "title": "A graph convolutional network for emotion recognition in context",
      "authors": [
        "Hanxin Zeng",
        "Gen Li",
        "Tong Tong",
        "Qinquan Gao"
      ],
      "year": "2020",
      "venue": "2020 Cross Strait Radio Science & Wireless Technology Conference (CSRSWTC)"
    },
    {
      "citation_id": "63",
      "title": "Contextaware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2007",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "64",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}