{
  "paper_id": "2007.15048v2",
  "title": "The Biraffe2 Experiment. Study In Bio-Reactions And Faces For Emotion-Based Personalization For Ai Systems",
  "published": "2020-07-29T18:35:34Z",
  "authors": [
    "Krzysztof Kutt",
    "Dominika Drążyk",
    "Maciej Szelążek",
    "Szymon Bobek",
    "Grzegorz J. Nalepa"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The paper describes BIRAFFE2 data set, which is a result of an affective computing experiment conducted between 2019 and 2020, that aimed to develop computer models for classification and recognition of emotion. Such work is important to develop new methods of natural Human-AI interaction. As we believe that models of emotion should be personalized by design, we present an unified paradigm allowing to capture emotional responses of different persons, taking individual personality differences into account. We combine classical psychological paradigms of emotional response collection with the newer approach, based on the observation of the computer game player. By capturing ones psycho-physiological reactions (ECG, EDA signal recording), mimic expressions (facial emotion recognition), subjective valence-arousal balance ratings (widget ratings) and gameplay progression (accelerometer and screencast recording), we provide a framework that can be easily used and developed for the purpose of the machine learning methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing (AfC)  [16]  -an interdisciplinary field of study regarding human emotions -is to large extent built upon the assumption that we are able to precisely detect, label and manipulate emotional responses of agents. Therefore the proper understanding and modeling of this complex phenomena  [1, 3] , as well as maintaining ingenious experimental setup to do so, is a crucial determinant of success in this field. The setup should require conditions where human subjects are exposed to specific emotion evoking stimuli, and furthermore their reactions are somehow measured.\n\nIn our work we assume that the measurement of bodily reactions to stimuli can serve as proper foundation for emotion recognition (James-Lange approach  [7] ). We also use a representation of affective data which is common in many experiments in psychology and human-computer interaction, i.e. the two dimensional Valence/Arousal space. We aim to continue a longer effort in building enhanced AfC models, previously presented on the AfCAI workshop, as well as later on in  [13] . To make a measurement context as ecological as possible, we use wearable devices as well as other sensors which are possibly non-intrusive to the subjects. We also assume that the data processing should be possible offhand -with the use of devices that users have with them, e.g. mobile phones. Our recent results in this regards are summarized in  [14] .\n\nThe original contribution of the paper is the report on a an affective computing experiment conducted between 2019 and 2020, that aimed to develop computer models for classification and recognition of emotion. The experiment resulted in the creation of publically developed dataset called BIRAFFE2. We believe that this work might be an important step to develop new methods of natural Human-AI interaction. The rest of the paper is organized as follows. In Section 2 we inroduce the motivation for our work and the context of the previous experiment in this area. Then in Section 3 we discuss our experimental methodology. The structure of the resulting dataset is described in Section 4. The paper ends with a summary and plans for future work in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Motivation",
      "text": "In order to fully exploit advantages of user's emotion detection modules, whether in games or in apps, the recognition and classification of emotional states must be personalised. By \"personalised\" we mean adjusted to ones personality traits and customs. To implement that kind of tailoring, the appropriate variety of information about the user must be collected. In order to verify these assumptions and to gather data for further development of the framework (for summary of the history of our approach see:  [13, 14] ) we constructed a combined experimental paradigm. First, in the \"classical approach\" part, we presented stimuli to the subjects and collected their answers: both with questionnaires and physiological signals measurement. Secondly, within the \"ecological approach\", we embedded the reaction measurement in specific context of simple computer games. When the player is loosing in the game, and the system detect the increased intensity of the her/his reaction, it is crucial for the model to interpret the context of such change -only by the contextual information coming from the game progression, we are able to tell whether the intensification was a collateral of anger or joy. Knowing its contextual origin, we can also easily prevent these specific changes from happening again, or on the contrary -elicit detected and labeled emotions once again. As such, games offer a perfect opportunity for putting the human in the feedback loop with the computer system (also called affective loop)  [5] .\n\nThe paradigm presented in this paper is the continuation of our previous work on the BIRAFFE1 experiment presented in  [10] . In the current work, a number of improvements were introduced, drawing on the conclusions from the BIRAFFE1 study (details are presented in Section 3):\n\n1. We improved the affect assessment widget. 2. We have enhanced the stimulus selection: it currently covers a wider area in the Valence-Arousal space, is more randomized and contains no erotic stimuli. 3. We have developed new custom computer games designed to arouse players' emotions -unlike BIRAFFE1, here the games focus on a limited number of mechanics to evoke a limited set of emotions, which should make it easier to analyze game logs and draw conclusions. 4. We used the GEQ questionnaire to assess the involvement in the game, as well as asked about previous experiences with games to allow more accurate analysis of the emotions in games. 5. We extended the range of the measured responses by the introduction of accelerometer from game pad. 6. EDA and ECG electrodes placement was changed to overcome issues identified in previous experiment. 7. Several small improvements were also made, e.g., face photos are now taken with higher frequency, the screencast is recorded during game sessions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Questionnaires",
      "text": "First, the paper-and-pen Polish adaptation  [19]  of the NEO Five Factor Inventory  [4]  consisting of 60 self-descriptive statements evaluated on 1-5 scale (1 -strongly disagree; 5 -strongly agree) was used to measure the Big Five personality traits. Second, our own paper-and-pen Polish translation of The Game Experience Questionnaire (GEQ) Core Module  [6]  was used to measure players' feelings during the game session. The module consists of 33 items, ranked on 0-4 scale (0 -not at all; 4 -extremely). Items were arranged in seven components in original version: Competence, Sensory and Imaginative Immersion, Flow, Tension/Annoyance, Challenge, Negative affect and Positive affect. The questionnaire has been used in many game studies  [8, 12] . However, the 7-factor structure has not been confirmed by anyone. In  [8]  revised version (GEQ-R) was proposed. Tension/Annoyance, Challenge and Negative affect were merged into Negativity, leading to 5-components solution.\n\nFinally, our own simple questionnaire was used to measure gaming experience. It consists of two questions: (1) \"Over the past year I have played computer / mobile / video games:\" (2) \"In the period of my most intense interest in computer / mobile / video games, I played:\". Both were answered by selecting one of the five possible answers: (a) daily or almost daily, (b) several times a week, (c) several times a month, (d) several times a year, (e) not at all. There was also a space for leaving comments on the experiment.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Stimuli Selection",
      "text": "Standardized emotionally-evocative images and sounds from IAPS  [11]  and IADS  [2]  datasets were used as stimuli, each charac-terized by its coordinates in the Valence-Arousal space. The analysis of IADS and IAPS scores revealed the following trend: arousal score increases as the valence score strives for it's positive or negative extreme (Figure  1 ). For the purpose of the experiment, we divided the stimuli into three groups according to their arousal and valence index: + (positive valence and high arousal), 0 (neutral valence and medium arousal), -(negative valence and high arousal). Erotic stimuli were excluded from the blind selection, due to the risk of creating weird or disgusting combinations (e.g. picture of kid or snake is paired with the erotic sound), not intended by the aim of this study.\n\nThe stimuli set for each participant was generated by random sampling without replacement and formed nine conditions, each consisting of 13 stimuli:\n\n• three consisting only of pictures: p+, p0, p-, • three consisting only of sounds: s+, s0, s-, • three, where pictures were paired with sounds: + picture with + sound (p+s+), 0 picture with 0 sound (p0s0), -picture withsound (p-s-).\n\nConditions were mixed during the presentation, which was divided into two sessions (17.5 min each) and separated by the game session.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Evaluation Widget",
      "text": "Emotional assessment was carried out using the Valence-arousal faces widget controlled by a left joystick on a gamepad. Widget was adapted from our previous experiment  [10] , with the following improvements:\n\n• Emoticons placed as hints were moved outside the selection area. Also, the border of the selection area was introduced. In the previous version the subjects often chose the location of the smiley as their answer. Now, there is no possibility to put selection marker on them. • The selection marker changes color to indicate that there is only half a second left for the rating. In the previous version there was no information about the remaining time. • The returned valence and arousal scores are now within a range of  [1, 9] , so they are within the same range as the assessments in IAPS and IADS. In the previous version, they were in the [-1, 1] range, which required conversion of values before the analysis started.\n\nTo compare current and previous version of Valence-arousal faces widget see Figure  2 . Both are presented in Polish, as in studies. X axis has labels \"negative\", \"neutral\", \"positive\", while Y axis has labels: \"high arousal\" and \"low arousal\".",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Games",
      "text": "Three affective games developed by our team  [20]  were used during the study. All of them were controlled by a gamepad and produced game log CSV files. They have been designed with the emphasis on differentiating the levels of difficulty:\n\n• Room of the Ghosts: The goal: pass through a series of rooms and defeat the arriving ghosts (see Figure  3 ). Difficulty: very easy, achieved by the following implementations: collider for protagonist is smaller than his real model -it removes the feeling of being hit before the projectile hits the player; the protagonist's weapon that can shoot more often and faster than the opponents' weapons. • Jump!: The goal: reach the end of the path by jumping on the platforms and avoiding obstacles (see Figure  4 ). Difficulty: hard and frustrating, achieved by the following implementations: colliders are too big -player can get hit by trap before s/he touches it with the model; movement is clunky, and there are several traps, i.e. invisible blocks, which increases the confusion and irritation in player; each time the protagonist dies, the background music is getting less pleasant (the pitch and distort levels of music playing in background increases by 0.07). • Labyrinth: The goal: walk the protagonist through the labyrinth (see Figure  5 ). Difficulty: optimal, achieved by the following implementations: the colliders have been adjusted to not hit the walls too often and to make the movement smooth; the protagonist control is natural and predictable.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hardware",
      "text": "Experimental setup consists of (see Figure  6 ):\n\n• Full HD 23\" LCD screen,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Software",
      "text": "The procedure was running under the Python 3.8, written with Psy-choPy 3.2.4 library  [15] . Python code controlled the execution of the whole protocol, i.e. stimuli presentation, screencast recording (using OBS Studio 5  software), photos taking, games' start and end management.\n\nPhysiological signals were gathered using BITalino (r)evolution kit, as it is the most promising of cheap mobile hardware platforms (for comparison see  [9] ). Electrocardiogram recording was implemented using the classical 3 leads montage with electrodes placed below the collarbones (V-and reference) and below the last rib on the left side of the body (V+). EDA signal was gathered by 2 leads placed on the forehead (placement as good as classical palmar location  [17] , with no side effects related to gamepad held by the subjects). EDA electrodes were hidden under the body-coloured band, to not interfere with the facial emotion recognition process conducted by the API in the later experimental stages, and to provide tight and stable contact of the EDA electrode and skin. Both signals were probed with 1000 Hz sampling rate.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Procedure",
      "text": "Two research stands were prepared in the same room, located next to the opposite walls, so that the subjects sat back to back. Each participant was seated in front of a monitor and provided with the consent and short information about the whole experiment. Throughout the whole procedure, the investigator remained at his desk, rear-facing to the subjects, to reduce the Hawthrone effect.\n\nSubjects filled out the NEO-FFI inventory and were connected to measuring devices, with the headphones set up, and a gamepad given to hand. Computer protocol consisted of five phases:\n\n1. Baseline signals recording (1 minute), 2. Instructions and training (approx. 5 minutes), 3. First part of stimuli presentation and rating (17.5 minutes): each presentation lasted 6 seconds 6  , followed by 6 seconds for affective rating and 6 second ISI  7  . Participants were instructed to navigate the procedure via gamepad. 4. Games session (up to 15 minutes in total): each game had a 5 min time limit, after which it turned itself off. After the completion of one game, another automatically switched on. 5. Second part of stimuli presentation and rating (17.5 minutes).\n\nAfter the computer protocol, subjects filled out three GEQ questionnaires (one for each game) and gaming experience questionnaire.\n\nECG and GSR signal, as well as gamepad accelerometer and gyroscope readings, were collected continuously during the whole experiment. Facial photos were taken every 250 milliseconds. A screencast was recorded during the game session, in case for the need to fill in the missing information in game logs after the experiment. The whole protocol lasted up to 75 minutes. BIRAFFE2 dataset consists of data gathered from 102 out of 103 participants. Unfortunately, during the protocol for subject 723 there were problems with the hard drive and all data was lost, except the paper-and-pen NEO-FFI and GEQ questionnaires. Also, some smaller issues occurred for a subset of participants, e.g. game crashed, Bluetooth signal was lost, electrode contact was poor. We have published also the incomplete records (see Tab. 1), as in many analysis only selected of the subsets will be used and it will not be the problem. Missing values in all files are represented by NaN.  where 9  :",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "The Biraffe2 Dataset",
      "text": "• ID -a randomly assigned subject ID from range {100,999}. It is used to identify all subject-related files as filenames. Filenames are created according to the format SUBxxx-yyyy, where xxx is the ID, and yyyy is the data type identifier (e.g. BioSigs, Gamepad), • NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME-   [6] , while 2018 is the revised version  [8] ). The values are ranging from 0 (not at all) to 4 (extremely) for each factor.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Biraffe2-Biosigs.Zip",
      "text": "Each SUBxxx-BioSigs.csv file represents one subject and consists of one line per each sensor recording. Values were recorded with 1 kHz frequency. The fields contained in each line are:\n\nTIMESTAMP;ECG;GSR where:\n\n• ECG -signal (units: mV) gathered by BITalino, after units transformation recommended by BITalino Sensor Datasheet 10 , lowpass filtering in 35 Hz and baseline removal performed using Python library for biosignal processing  [18] . • GSR -signal (units: µS) gathered by BITalino, after units transformation recommended by BITalino Sensor Datasheet 11 and lowpass filtering (in range between 0.5 to 50 Hz depending on the noise level in the individual file) performed using Python library for biosignal processing  [18] .\n\nThe signals were recorded using the Python library for BITalino 12 .\n\nDue to the instability of the bluetooth connection, and in the absence of handling such a situation in the library, the timestamps after series 13 of several connection errors cannot be considered as fully reliable. This will be further investigated by our team.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Biraffe2-Gamepad.Zip",
      "text": "Each SUBxxx-gamepad.csv file represents one subject and consists of one line per each gamepad recording. The values were recorded as quickly as they were transmitted through the USB interface, with an average frequency of 250 Hz. The fields contained in each line are:\n\nTIMESTAMP;GYR-X;GYR-Y;GYR-Z;ACC-X;ACC-Y;ACC-Z where GYR-are gyroscope readings, while ACC-are accelerometer values. Note that the gyroscope values represent the position of the gamepad, not the angular rate 14 :\n\n• GYR-X -right side of the gamepad upward,\n\n• GYR-Y -buttons and joysticks panel upward,\n\n• GYR-Z -audio port upward / light bar downward,\n\n• ACC-X -yaw counter-clockwise,\n\n• ACC-Y -pitch upward,\n\n• ACC-Z -roll left side of gamepad down.  See: https://bitalino.com/datasheets/EDA_Sensor_ Datasheet.pdf.\n\n12 See: http://bitalino.com/pyAPI/. 13 Errors are considered to occur in the series if the interval between consecutive error entries in the Procedure log (see Sect. 4.7) was less than 0.05 s. 14 As in the DS4Windows library, which code was used as the base for our Python data acquisition code. See: https://github.com/ Jays2Kings/DS4Windows.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Biraffe2-Games.Zip",
      "text": "Five JSON log files are created for each subject. Note that the file was created when the level has started. If the game has crashed in a given level (which sometimes happened), the subsequent levels were not started and the subject was returned to the stimulus session. This means that in some cases files for may be missing, e.g. files for level 3 if the game crashed at the second level.\n\nSUBxxx-Level01_Log.json contains a log from the Room of the Ghosts composed of information collected at multiple points in time about the current position and status of the user. It is a repeated pattern of the following structure:\n\nSUBxxx-Level01_MapLog.json contains information about the dynamic environment in the Room of the Ghosts. It is composed of series of three subsequent lists -each describing current position of existing ghosts, money bags and health pickups: SUBxxx-Level02_Log.json is a log from the Jump!, as the log from the first level, it is composed of a repeated pattern collected at multiple points in time: There are also three files containing static map of each level (same for each subject): Level01_StaticMap.json, Level02_StaticMap.json, Level03_StaticMap.json. They are located in the root directory of the subset. Each of them consists of a list of position of all (squared) blocks building the maps:\n\n1 [ 2 { \"x\": Float, \"y\": Float, \"z\": Float }, # 1st 3 { \"x\": Float, \"y\": Float, \"z\": Float }, # 2nd",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Biraffe2-Photo.Zip",
      "text": "Each SUBxxx-Face.csv file represents one subject and consists of one line per each photo taken. Raw photos are not available due to privacy reasons. File consists of values calculated by MS Face API with recognition_02 model. Photos were taken with 4 Hz frequency during games and during stimuli presentation (every 15 frames at 60 fps). Photos were not taken while the subject was responding on the widget. When no face was recognized or two faces were found (the second was the experimenter face) NaN value was used.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Biraffe2-Screencast.Zip",
      "text": "Each SUBxxx-Screencast.mkv file contains a screen recording (1920x1080 resolution, 60 fps, h.264 codec) of game session for one subject. The entire subset weights 12 GB and, due to its large size, it is available on request.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Summary And Future Work",
      "text": "This paper describes the BIRAFFE2 data set collected in 2020 during the affective computing experiment, aiming to provide a framework for the enhanced models of emotion classification an recognition. The enhancement, founded by the variety of collected data types and behavioural features, is grounded in the belief, that the up-to-date emotion recognition models should be personalized by design. The construction of such models, as well as the experimental design used for data collection, should take the individual personality differences into account and provide a solution for its operationalization. We believe, that the data set described in this paper presents an important contribution that supports the development and replication of experiments in affective computing and Human-AI interaction.\n\nBased on the data acquired in the reported experiment we now plan a series of analyses aimed at the development of personalized models of emotion. We are planning to analyze the data from the first phase of the experiment to develop certain calibration and personalization methods to fine tune the interpretation of emotion of in several tasks. While we are currently experimenting with video games, we are also considering the incorporation of the developed models into decision support and recommendation systems.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Trends in the IADS stimuli ratings.",
      "page": 2
    },
    {
      "caption": "Figure 2: Current and previous [10] versions of the Valence-arousal faces widget (pictures presented with a negative ﬁlter).",
      "page": 3
    },
    {
      "caption": "Figure 2: Both are presented in Polish, as in studies. X axis",
      "page": 3
    },
    {
      "caption": "Figure 3: ). Difﬁculty: very easy,",
      "page": 3
    },
    {
      "caption": "Figure 4: ). Difﬁculty: hard and",
      "page": 3
    },
    {
      "caption": "Figure 5: ). Difﬁculty: optimal, achieved by the following im-",
      "page": 3
    },
    {
      "caption": "Figure 3: Room of the Ghosts gameplay.",
      "page": 3
    },
    {
      "caption": "Figure 4: Jump! gameplay.",
      "page": 3
    },
    {
      "caption": "Figure 5: Labyrinth gameplay.",
      "page": 3
    },
    {
      "caption": "Figure 6: Research setup: 23” LCD screen, headphones, external web cam-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2;GAME-3;PHOTOS;PROCEDURE;SCREENCAST;OPENNESS;"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "CONSCIENTIOUSNESS;NEUROTICISM;AGREEABLENESS;"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "EXTRAVERSION;GAME-EXO-PAST-YEAR;GAME-EXP-MOST-"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "INTENSE;GEQ-1-COMPETENCE-2013;GEQ-1-IMMERSION-2013;"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "GEQ-1-FLOW-2013;GEQ-1-TENSION-2013;GEQ-1-CHALLENGE"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2013;GEQ-1-NEGATIVE-AFFECT-2013;GEQ-1-POSITIVE-"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "AFFECT-2013;GEQ-1-POSITIVE-AFFECT-2018;GEQ-1-"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "NEGATIVITY-2018;GEQ-1-COMPETENCE-2018;GEQ-1-FLOW"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2018;GEQ-1-IMMERSION-2018;GEQ-2-COMPETENCE-2013;"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "GEQ-2-IMMERSION-2013;GEQ-2-FLOW-2013;GEQ-2-TENSION"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2013;GEQ-2-CHALLENGE-2013;GEQ-2-NEGATIVE-AFFECT"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2013;GEQ-2-POSITIVE-AFFECT-2013;GEQ-2-POSITIVE-"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "AFFECT-2018;GEQ-2-NEGATIVITY-2018;GEQ-2-COMPETENCE"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2018;GEQ-2-FLOW-2018;GEQ-2-IMMERSION-2018;GEQ-3-"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "COMPETENCE-2013;GEQ-3-IMMERSION-2013;GEQ-3-FLOW"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-2013;GEQ-3-TENSION-2013;GEQ-3-CHALLENGE-2013;GEQ"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "-3-NEGATIVE-AFFECT-2013;GEQ-3-POSITIVE-AFFECT-2013;"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "GEQ-3-POSITIVE-AFFECT-2018;GEQ-3-NEGATIVITY-2018;"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "GEQ-3-COMPETENCE-2018;GEQ-3-FLOW-2018;GEQ-3-"
        },
        {
          "ID;AGE;SEX;NEO-FFI;GEQ;BIOSIGS;GAMEPAD;GAME-1;GAME": "IMMERSION-2018"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[": "{"
        },
        {
          "[": "\"x\": Float, # current position"
        },
        {
          "[": "\"y\": Float,"
        },
        {
          "[": "\"deathCount\": Int,"
        },
        {
          "[": "\"distortionLevel\": Float, # current parameters"
        },
        {
          "[": "\"pitchLevel\": Float, # of the sound"
        },
        {
          "[": "\"timestamp\": Int,"
        },
        {
          "[": "\"idOfSound\": String, # as in the first level"
        },
        {
          "[": "\"timestampOfSound\": String,"
        },
        {
          "[": "\"xMin\": Float, # visible area of the map"
        },
        {
          "[": "\"yMin\": Float,"
        },
        {
          "[": "\"xMax\": Float,"
        },
        {
          "[": "\"yMax\": Float"
        },
        {
          "[": "},"
        },
        {
          "[": "..."
        },
        {
          "[": "]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[": "{"
        },
        {
          "[": "\"x\": Float, # current position on the map"
        },
        {
          "[": "\"y\": Float,"
        },
        {
          "[": "\"deathCount\": Int,"
        },
        {
          "[": "\"shootsCounter\": Int, # both may be used to"
        },
        {
          "[": "\"hitCounter\": Int, # calculate the accuracy"
        },
        {
          "[": "# of the player"
        },
        {
          "[": "\"money\": Int, # current score (death: -5)"
        },
        {
          "[": "\"collectedMoney\": Int, # how many pickups"
        },
        {
          "[": "\"collectedHealth\": Int, # were collected"
        },
        {
          "[": "\"health\": Int,"
        },
        {
          "[": "\"timestamp\": Int, # current Unix timestamp"
        },
        {
          "[": "\"idOfSound\": String, # ID of IADS sound played"
        },
        {
          "[": "# in the background"
        },
        {
          "[": "\"timestampOfSound\": String, # time when sound"
        },
        {
          "[": "# was started"
        },
        {
          "[": "\"xMin\": Float, # visible area of the map"
        },
        {
          "[": "\"yMin\": Float,"
        },
        {
          "[": "\"xMax\": Float,"
        },
        {
          "[": "\"yMax\": Float"
        },
        {
          "[": "},"
        },
        {
          "[": "..."
        },
        {
          "[": "]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[": "{"
        },
        {
          "[": "\"Invisible blocks\": ["
        },
        {
          "[": "{"
        },
        {
          "[": "\"x\": Float,"
        },
        {
          "[": "\"y\": Float,"
        },
        {
          "[": "\"timestamp\": Int"
        },
        {
          "[": "},"
        },
        {
          "[": "... # each block has a separate entry"
        },
        {
          "[": "},"
        },
        {
          "[": "{"
        },
        {
          "[": "\"Falling blocks\": ["
        },
        {
          "[": "... # the list is the same as above"
        },
        {
          "[": "]"
        },
        {
          "[": "},"
        },
        {
          "[": "..."
        },
        {
          "[": "]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[": "{"
        },
        {
          "[": "\"x\": Float, # current position"
        },
        {
          "[": "\"y\": Float,"
        },
        {
          "[": "\"deadEnds\": Int, # counts how many times"
        },
        {
          "[": "# the player was off the"
        },
        {
          "[": "# correct path"
        },
        {
          "[": "\"wasOnCorrectPath\": Bool,"
        },
        {
          "[": "\"timestamp\": Int,"
        },
        {
          "[": "\"idOfSound\": String,"
        },
        {
          "[": "\"timestampOfSound\": String,"
        },
        {
          "[": "\"xMin\": Float,"
        },
        {
          "[": "\"yMin\": Float,"
        },
        {
          "[": "\"xMax\": Float,"
        },
        {
          "[": "\"yMax\": Float"
        },
        {
          "[": "},"
        },
        {
          "[": "..."
        },
        {
          "[": "]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[": "{"
        },
        {
          "[": "\"Ghosts\": ["
        },
        {
          "[": "{"
        },
        {
          "[": "\"x\": Float, # current position"
        },
        {
          "[": "\"y\": Float,"
        },
        {
          "[": "\"timestamp\": Int # Unix timestamp when the"
        },
        {
          "[": "# position was recorded"
        },
        {
          "[": "},"
        },
        {
          "[": "... # each ghost has a separate entry"
        },
        {
          "[": "},"
        },
        {
          "[": "{"
        },
        {
          "[": "\"Money bags\": ["
        },
        {
          "[": "... # the list is the same as for ghosts"
        },
        {
          "[": "]"
        },
        {
          "[": "},"
        },
        {
          "[": "{"
        },
        {
          "[": "\"Health pickups\": ["
        },
        {
          "[": "... # the list is the same as for ghosts"
        },
        {
          "[": "]"
        },
        {
          "[": "},"
        },
        {
          "[": "..."
        },
        {
          "[": "]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[": "{ \"x\": Float, \"y\": Float, \"z\": Float }, # 1st"
        },
        {
          "[": "{ \"x\": Float, \"y\": Float, \"z\": Float }, # 2nd"
        },
        {
          "[": "..."
        },
        {
          "[": "]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "EYELEFTINNER-X;FACELANDMARKS-EYELEFTINNER-Y;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-EYELEFTOUTER-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "EYELEFTOUTER-Y;FACELANDMARKS-EYELEFTTOP-X;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-EYELEFTTOP-Y;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "EYERIGHTBOTTOM-X;FACELANDMARKS-EYERIGHTBOTTOM-Y;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-EYERIGHTINNER-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "EYERIGHTINNER-Y;FACELANDMARKS-EYERIGHTOUTER-X;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-EYERIGHTOUTER-Y;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "EYERIGHTTOP-X;FACELANDMARKS-EYERIGHTTOP-Y;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-MOUTHLEFT-X;FACELANDMARKS-MOUTHLEFT-Y"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": ";FACELANDMARKS-MOUTHRIGHT-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "MOUTHRIGHT-Y;FACELANDMARKS-NOSELEFTALAROUTTIP-X;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-NOSELEFTALAROUTTIP-Y;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "NOSELEFTALARTOP-X;FACELANDMARKS-NOSELEFTALARTOP-Y;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-NOSERIGHTALAROUTTIP-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "NOSERIGHTALAROUTTIP-Y;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "NOSERIGHTALARTOP-X;FACELANDMARKS-NOSERIGHTALARTOP-Y"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": ";FACELANDMARKS-NOSEROOTLEFT-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "NOSEROOTLEFT-Y;FACELANDMARKS-NOSEROOTRIGHT-X;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-NOSEROOTRIGHT-Y;FACELANDMARKS-NOSETIP"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "-X;FACELANDMARKS-NOSETIP-Y;FACELANDMARKS-PUPILLEFT-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "X;FACELANDMARKS-PUPILLEFT-Y;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "PUPILRIGHT-X;FACELANDMARKS-PUPILRIGHT-Y;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-UNDERLIPBOTTOM-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "UNDERLIPBOTTOM-Y;FACELANDMARKS-UNDERLIPTOP-X;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-UNDERLIPTOP-Y;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "UPPERLIPBOTTOM-X;FACELANDMARKS-UPPERLIPBOTTOM-Y;"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "FACELANDMARKS-UPPERLIPTOP-X;FACELANDMARKS-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "UPPERLIPTOP-Y;FACERECTANGLE-HEIGHT;FACERECTANGLE-"
        },
        {
          "FACELANDMARKS-EYELEFTBOTTOM-Y;FACELANDMARKS-": "LEFT;FACERECTANGLE-TOP;FACERECTANGLE-WIDTH"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GAME-TIMESTAMP;FRAME-NUMBER;IADS-ID;IAPS-ID;ANGER;": "CONTEMPT;DISGUST;FEAR;HAPPINESS;NEUTRAL;SADNESS;"
        },
        {
          "GAME-TIMESTAMP;FRAME-NUMBER;IADS-ID;IAPS-ID;ANGER;": "SURPRISE"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TIMESTAMP;ID;COND;IADS-ID;IAPS-ID;ANS-VALENCE;ANS-": "AROUSAL;ANS-TIME;EVENT"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-BLUR-BLURLEVEL;FACEATTRIBUTES-BLUR-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "VALUE;FACEATTRIBUTES-EXPOSURE-EXPOSURELEVEL;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-EXPOSURE-VALUE;FACEATTRIBUTES-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACIALHAIR-BEARD;FACEATTRIBUTES-FACIALHAIR-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "MOUSTACHE;FACEATTRIBUTES-FACIALHAIR-SIDEBURNS;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-GENDER;FACEATTRIBUTES-GLASSES;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-HAIR-BALD;FACEATTRIBUTES-HAIR-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "HAIRCOLOR-BLACK;FACEATTRIBUTES-HAIR-HAIRCOLOR-BLOND"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": ";FACEATTRIBUTES-HAIR-HAIRCOLOR-BROWN;FACEATTRIBUTES"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "-HAIR-HAIRCOLOR-GRAY;FACEATTRIBUTES-HAIR-HAIRCOLOR-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "OTHER;FACEATTRIBUTES-HAIR-HAIRCOLOR-RED;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-HAIR-INVISIBLE;FACEATTRIBUTES-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "HEADPOSE-PITCH;FACEATTRIBUTES-HEADPOSE-ROLL;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-HEADPOSE-YAW;FACEATTRIBUTES-MAKEUP-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "EYEMAKEUP;FACEATTRIBUTES-MAKEUP-LIPMAKEUP;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACEATTRIBUTES-NOISE-NOISELEVEL;FACEATTRIBUTES-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "NOISE-VALUE;FACEATTRIBUTES-SMILE;FACEID;"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "FACELANDMARKS-EYEBROWLEFTINNER-X;FACELANDMARKS-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "EYEBROWLEFTINNER-Y;FACELANDMARKS-EYEBROWLEFTOUTER-X"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": ";FACELANDMARKS-EYEBROWLEFTOUTER-Y;FACELANDMARKS-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "EYEBROWRIGHTINNER-X;FACELANDMARKS-EYEBROWRIGHTINNER"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "-Y;FACELANDMARKS-EYEBROWRIGHTOUTER-X;FACELANDMARKS-"
        },
        {
          "FACEATTRIBUTES-ACCESSORIES;FACEATTRIBUTES-AGE;": "EYEBROWRIGHTOUTER-Y;FACELANDMARKS-EYELEFTBOTTOM-X;"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Lisa Feldman",
        "Michael Lewis",
        "Jeannette Haviland-Jones"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Affective ratings of sounds and instruction manual. technical report B-3",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "2007",
      "venue": "Affective ratings of sounds and instruction manual. technical report B-3"
    },
    {
      "citation_id": "3",
      "title": "The Oxford Handbook of Affective Computing",
      "authors": [
        "Rafael Calvo",
        "Sidney D'mello",
        "Jonathan Gratch",
        "Arvid Kappas"
      ],
      "year": "2015",
      "venue": "The Oxford Handbook of Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Revised NEO Personality Inventory (NEO-PI-R) and NEO Five Factor Inventory (NEO-FFI). Professional manual",
      "authors": [
        "P Costa",
        "R Mccrae"
      ],
      "year": "1992",
      "venue": "Psychological Assessment Resources"
    },
    {
      "citation_id": "5",
      "title": "Affective loop experiences -what are they?",
      "authors": [
        "Kristina Höök",
        "; Per",
        "F Hasle",
        "Marja Harjumaa",
        "Katarina Segerståhl",
        "Peter Øhrstrøm"
      ],
      "year": "2008",
      "venue": "Persuasive Technology, Third International Conference"
    },
    {
      "citation_id": "6",
      "title": "The Game Experience Questionnaire",
      "authors": [
        "W Ijsselsteijn",
        "Y De Kort",
        "K Poels"
      ],
      "year": "2013",
      "venue": "The Game Experience Questionnaire"
    },
    {
      "citation_id": "7",
      "title": "What is an emotion?",
      "authors": [
        "William James"
      ],
      "year": "1884",
      "venue": "Mind"
    },
    {
      "citation_id": "8",
      "title": "Validation of two game experience scales: The player experience of need satisfaction (PENS) and game experience questionnaire (GEQ)",
      "authors": [
        "M Daniel",
        "M Johnson",
        "Ryan Gardner",
        "Perry"
      ],
      "year": "2018",
      "venue": "Int. J. Hum. Comput. Stud"
    },
    {
      "citation_id": "9",
      "title": "Towards the development of sensor platform for processing physiological data from wearable sensors",
      "authors": [
        "Krzysztof Kutt",
        "Wojciech Binek",
        "Piotr Misiak",
        "Grzegorz Nalepa",
        "Szymon Bobek"
      ],
      "year": "2018",
      "venue": "Artificial Intelligence and Soft Computing -17th International Conference"
    },
    {
      "citation_id": "10",
      "title": "BIRAFFE: Bio-reactions and faces for emotion-based personalization",
      "authors": [
        "Krzysztof Kutt",
        "Dominika Dr Ążyk",
        "Paweł Jemioło",
        "Szymon Bobek",
        "Barbara Giżycka",
        "Rodríguez Víctor",
        "Grzegorz Fernández",
        "Nalepa"
      ],
      "year": "2020",
      "venue": "AfCAI 2019: Workshop on Affective Computing and Context Awareness in Ambient Intelligence"
    },
    {
      "citation_id": "11",
      "title": "International affective picture system (iaps): Affective ratings of pictures and instruction manual. technical report B-3",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2008",
      "venue": "International affective picture system (iaps): Affective ratings of pictures and instruction manual. technical report B-3"
    },
    {
      "citation_id": "12",
      "title": "Systematic review and validation of the game experience questionnaire (geq)implications for citation and reporting practice",
      "authors": [
        "Effie L.-C Law",
        "Florian Brühlmann",
        "Elisa Mekler"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play, CHI PLAY '18"
    },
    {
      "citation_id": "13",
      "title": "Mobile platform for affective context-aware systems",
      "authors": [
        "J Grzegorz",
        "Krzysztof Nalepa",
        "Szymon Kutt",
        "Bobek"
      ],
      "year": "2019",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "14",
      "title": "Analysis and use of the emotional context with wearable devices for games and intelligent assistants",
      "authors": [
        "J Grzegorz",
        "Krzysztof Nalepa",
        "Barbara Kutt",
        "Paweł Giżycka",
        "Szymon Jemioło",
        "Bobek"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "Psychopy2: Experiments in behavior made easy",
      "authors": [
        "Jonathan Peirce",
        "Jeremy Gray",
        "Sol Simpson",
        "Michael Macaskill",
        "Richard Höchenberger",
        "Hiroyuki Sogo",
        "Erik Kastman",
        "Jonas Kristoffer"
      ],
      "year": "2019",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "16",
      "title": "Affective Computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Emotional sweating across the body: Comparing 16 different skin conductance measurement locations",
      "authors": [
        "J Marieke Van Dooren",
        "Joris De Vries",
        "Janssen"
      ],
      "year": "2012",
      "venue": "Physiology & Behavior"
    },
    {
      "citation_id": "18",
      "title": "HeartPy -Python Heart Rate analysis toolkit",
      "authors": [
        "Jonathan Paul Van Gent",
        "Kris De Bruin",
        "Glenn Fernandes"
      ],
      "venue": "HeartPy -Python Heart Rate analysis toolkit",
      "doi": "10.5281/zenodo.1324310"
    },
    {
      "citation_id": "19",
      "title": "Inwentarz osobowości NEO-FFI Costy i McCrae. Polska adaptacja",
      "authors": [
        "B Zawadzki",
        "J Strelau",
        "P Szczepaniak",
        "M Śliwińska"
      ],
      "year": "1998",
      "venue": "Pracowania Testów Psychologicznych PTP"
    },
    {
      "citation_id": "20",
      "title": "Game Design with Unity for Affective Games",
      "authors": [
        "Laura Żuchowska"
      ],
      "year": "2020",
      "venue": "Game Design with Unity for Affective Games"
    }
  ]
}