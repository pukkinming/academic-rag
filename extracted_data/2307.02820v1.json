{
  "paper_id": "2307.02820v1",
  "title": "Evaluating Raw Waveforms With Deep Learning Frameworks For Speech Emotion Recognition",
  "published": "2023-07-06T07:27:59Z",
  "authors": [
    "Zeynep Hilal Kilimci",
    "Ulku Bayraktar",
    "Ayhan Kucukmanisa"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a challenging task in speech processing field. For this reason, feature extraction process has a crucial importance to demonstrate and process the speech signals. In this work, we represent a model, which feeds raw audio files directly into the deep neural networks without any feature extraction stage for the recognition of emotions utilizing six different data sets, namely, The Berlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), Toronto Emotional Speech Database (TESS), Crowd-sourced Emotional Multimodal Actors (CREMA), Surrey Audio-Visual Expressed Emotion (SAVEE), and TESS+RAVDESS. To demonstrate the contribution of proposed model, the performance of traditional feature extraction techniques namely, mel-scale spectogram, mel-frequency cepstral coefficients, are blended with machine learning algorithms, ensemble learning methods, deep and hybrid deep learning techniques. Support vector machine, decision tree, naive Bayes, random forests models are evaluated as machine learning algorithms while majority voting and stacking methods are assessed as ensemble learning techniques. Moreover, convolutional neural networks, long short-term memory networks, and hybrid CNN-LSTM model are evaluated as deep learning techniques and compared with machine learning and ensemble learning methods. To demonstrate the effectiveness of proposed model, the comparison with state-of-the-art studies are carried out. Based on the experiment results, CNN model excels existent approaches with 95.86% of accuracy for TESS+RAVDESS data set using raw audio files, thence determining the new state-of-the-art. The proposed model performs 90.34% of accuracy for EMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of accuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model, 85.76% of accuracy for SAVEE with CNN model in speaker-independent audio categorization problems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are complex psychophysiological changes resulting from the interaction of individual moods with biochemical and environmental influences. This change can be expressed in different ways such as speech, facial expression, body motions, and brain signs by emphasizing emotions like anger, sadness, happiness, fear, excitement, surprise, etc. Speech is a complicated sign that includes significant information related to the content of message and features of speaker  (gender, emotion, language, accent, etc.) . That is why it has been explored by many disciplines and art forms.\n\nSpeech emotion recognition (SER) is a common research field in the last decades. SER is utilized in different application areas such as human-machine interaction, education, management of multimedia contents, entertainment, automobile industry, text to speech conversion, medical diagnosis  (Ingale and Chaudhari, 2012) .\n\nEmotion recognition from speech signal is reasonably hard since the styles of speaking (i.e. pronunciation), speech rates of the speakers is totally diverse from individual to individual and it modifies from location to location (i.e. distinct for native speakers and non-native speakers)  (Ingale and Chaudhari, 2012) . Thence, it is more significant to pick up specific attributes of speech which are not influenced by the territory, culture, speaking genre of the talker. Various characteristics such as spectral, prosodic, and acoustic are employed by extracting features from speech signal for emotion recognition task in computer science  (Selvaraj et al., 2016) . Then, the procedure is proceeded by classifiers to determine the emotion of speech.\n\nDeep learning-based models are commonly employed by the researchers because of providing better predictions and results compared with traditional machine learning algorithms in different domains such as face recognition, voice recognition, image recognition  Mittal et al. (2018) ,  Bae et al. (2016) ,  He et al. (2016) . The usage of deep learning architectures facilitates automatic feature selection process unlike gathering hand-crafted features. Lately, diverse deep learning-based models are also utilized for speech emotion recognition tasks in the state-of-the-artstudies. The literature works generally focus on to discover important attributes of speech signal using deep learning models  Trigeorgis et al. (2016)  or demonstrate the performance of deep learning models on a specific feature extraction technique  Han et al. (2014) . In this work, we obtain deep features from raw sound data and feeding them into different deep learning algorithms for speech emotion recognition task instead of employing conventional feature extraction techniques such as MFCC, MEL.\n\nIn this work, evaluating raw waveforms is proposed without applying any feature extraction stage using traditional machine learning algorithms, ensemble learning approaches, and deep learning architectures for speech emotion recognition task. For this purpose, convolutional neural networks, long short-term memory networks, and hybrid CNN-LSTM models are evaluated deep learning techniques while support vector machine, decision tree, naive Bayes, random forests, majority voting, stacking models are evaluated as machine learning and ensemble algorithms. To demonstrate the contribution of the proposed framework, the performance of traditional feature extraction techniques and the results of state-of-the-art studies are compared with the proposed model on six different dataset. The utilization of raw audio files instead of employing feature extraction process for speech emotion recognition task shows remarkable results when compared to the literature studies.\n\nThe remaining of the paper is organized as follows: Section 2 presents literature review. Materials and methods used in the study are given in Section 3. Data acquisition and proposed framework are demonstrated in Section 4. In Section 5, experiment results and conclusions are represented.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Literature Review",
      "text": "This section gives a brief summary of literature works for speech emotion recognition.  (Issa et al., 2020)  introduce convolutional neural network (CNN) architecture is introduced for speech emotion recognition task. Mel-frequency Cepstral Coefficients (MFCCs), Mel-scaled spectrogram, Chromagram, Spectral contrast feature, and Tonnetz representation are evaluated at the stage of feature extraction. After that, extracted features from sound files are fed into CNN to show the effectiveness of feature extraction models in RAVDESS, EMO-DB, and IEMOCAP data sets. The proposed model exhibits the best classification performance in EMO-DB data set with 86.1% of accuracy.  (Sajjad et al., 2020)  propose a new clustering based approach with the help of radial-based function network for SER. Determined key sequences are fed into Bidirectional long short-term memory network to obtain final state of the emotion. The proposed approach is assessed over IEMOCAP, EMO-DB, and RAVDESS data sets. Experiment results show that the proposed approach represents remarkable results in terms of classification accuracy when compared to the state-of-the-art studies.  (Kwon, 2019)  presents a CNN-based framework for speech emotion recognition. To show the effectiveness of the model, IEMOCAP and RAVDESS data sets are evaluated. The authors report that the proposed model enhance the classification accuracy by 7.85% for IEMOCAP and 4.5% for RAVDESS.  Zhao et al. (2019)  evaluate 1D and 2D CNN-LSTM networks to recognize speech emotion. The performance of hybrid deep learning model is compared with deep belief network and CNN architecture over two data sets namely, IEMOCAP and EMO-DB. They report that the performance of hybrid deep learning model for SER is rather competitive when compared to the conventional techniques.  (Koduru et al., 2020)  investigate the impact of feature extraction techniques to enhance the performance of speech emotion rate. For this purpose, Mel frequency cepstral coefficients, Discrete Wavelet Transform (DWT), pitch, energy and Zero crossing rate (ZCR) models are employed at the stage of feature extraction. To show the efficieny of feature extraction techniques, support vector machine, decision tree (DT) and LDA models are evaluated. The utilization of DT performs the best classification accuracy with nearly 85%.  (Kwon et al., 2021)  propose a multi-learning strategy by providing end-to-end real time model for SER. The proposed dilated CNN (DCNN) model is evaluated on two benchmark data sets namely, IEMOCAP and EMO-DB. Authors report that the usage DCNN model exhibits significant accuracy results with 73% for IEMOCAP and 90 % for EMO-DB.  (Zehra et al., 2021)  propose an ensemble-based framework for cross corpus multi-lingual recognition of speech emotion. The performance of an ensemble model, majority voting, is compared with conventional machine learning techniques. The utilization of ensemble model ensures enhancement in classification accuracy nearly 13% for Urdu data set, roughly 8% for German data set, 11% for Italian data set, and 5% for English data set.  (Anvarjon and Kwon, 2020)  concentrate on a lightweight CNN approach for speech emotion recognition task. To show the efficiency of proposed model, experiments are carried on IEMOCAP, and EMO-DB data sets. Experiment results indicate that a lightweight CNN model is capable to recognize emotion of speech with 77.01 % of accuracy for IEMOCAP data set, and 92.02% of accuracy for EMO-DB data set.  (√ñzseven, 2019)  presents a novel statistical feature selection technique by taking into consideration average of each featue in the features set for SER. Recognition performance of the model is compared on EMO-DB, eNTERFACE05, EMOVO and SAVEE data sets by using SVM, MLP, and k-NN classifiers. Except eNTERFACE05 data set, SVM model outperforms other machine learning algorithms in terms of classification accuracy in all data sets.  (Sun et al., 2019)  propose DNN-decision tree SVM model by calculating the confusion degree of emotion with decision tree SVM and training with DNN architecture. To demonstrate the effectiveness of the proposed model, experiments are carried on Chinese Academy of Sciences Emotional data set. The proposed model achieves remarkable experiment results when compared to conventional SVM and DNN-SVM technique by ensuring nearly 6% and roughly 3% enhancement in the success of recognition rate, respectively.  Cai et al. (2021)  focus on the multitask learning approach for speech emotion recognition by ensuring speech-to-text recognition and emotion categorization, simultaneously. The efficiency of the model is demonstrated on the IEMOCAP data set by achieving nearly 78% of accuracy.  (Chen et al., 2020)  present two-layer fuzzy multiple ensemble framework using fuzzy C-means algorithm and random forest model for SER. The proposed framework is capable to recognize emotions on CASIA and EMO-DB data sets by improving recognition accuracy when compared to back propagation and random forest models.  (Farooq et al., 2020)  concentrate on the effect of feature selection model utilizing deep convolutional neural network (DCNN) for SER. After extracting features from pretrained models, the most discriminatory features are determined by a correlation-based feature selection technique. At the classification stage, support vector machines, random forests, the k-nearest neighbors algorithm, and neural network classifier are employed on EMO-DB, SAVEE, IEMOCAP, and RAVDESS data sets. The model achieves 95.10% for Emo-DB, 82.10% for SAVEE, 83.80% for IEMOCAP, and 81.30% for RAVDESS in terms of classification accuracies.  (Zhang et al., 2021)  propose a novel deep multimodal model for spontaneous SER. The proposed model is based on the combination of three different audio inputs by feeding them into multi-CNN fusion model. The combination strategy performs promising classification results when compared with the sate-of-the-art results.  (Tuncer et al., 2021)  focus on SER by employing twine shuffle pattern and iterative neighborhood component analysis methods. The proposed model is based on feature generation and selection stages utilizing shuffle box and iterative neighborhood component analysis methodologies, respectively. To demonstrate the efficiency of the model, the experiments are carried out on EMO-DB, SAVEE, RAVDESS, EMOVO data sets. Proposed model achieves 87.43% for RAVDESS, 90.09% for EMO-DB, 84.79% for SAVEE, and 79.08% for EMOVO in terms of classification accuracy.  (Lu et al., 2022)  present domain invariant feature learning (DIFL) models to address speaker-independent speech emotion recognition. The experiments are performed on EMO-DB, eNTERFACE, and CASIA data set to demonstrate the contribution of the proposed model. Experiment results indicate that the utilization of proposed model exhibits remarkable results compared to the literature studies.\n\nTo the best of our knowledge, our study is the first attempt to process raw audio files by blending them with machine learning and deep learning methods for the task of speech emotion recognition and differs from the aforementioned literature studies in this aspect.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets And Methodology",
      "text": "In this section, datasets used in the study, and proposed methodology are presented. Six different publicly available and widely applied datasets, The Berlin Database of Emotional Speech (EMO-DB), The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), Toronto Emotional Speech Database (TESS), Crowdsourced Emotional Multimodal Actors dataset (CREMA), Surrey Audio-Visual Expressed Emotion (SAVEE), and TESS+RAVDESS are employed in the experiments. After that, methodology is introduced with feature extraction stage, models, and the proposed framework. The proposed speech emotion recognition methodology is demonstrated in Figure  1 . TESS Pichora-Fuller and Dupuis (2020) comprises of audio records of 2 female speakers pronouncing English sentences. TESS contains 2,800 utterances of with anger, disgust, neutral, fear, happiness, sadness, bored, surprise emotional categories. CREMA  (Cao et al., 2014)  includes 7,442 original clips generated by 43 female and 48 male actors from various ethnicities such as Hispanic, Asian, African, American. Specified 12 sentence are vocalized capturing six different emotions. These are sad, happy, disgust, neutral, anger, fear. SAVEE dataset  Haq et al. (2008)  covers 1,680 utterances vocalized by 14 male actors in seven different emotions. These are surprise, anger, happiness, disgust, neutral, fear, sadness. The sentences recorded by actors are picked up from the TIMIT Acoustic-Phonetic Continuous Speech corpus. The combination of TESS and RAVDESS data sets is called TESS+RAVDESS in this study. Because the emotional categories are common, there is no problem to consolidate them. The dataset consists of 4,240 utterances voiced by 14 female and 12 male actors. The distribution of the datasets used in the study is given in Table  1 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction And Pre-Processing",
      "text": "Feature extraction stage plays an importance role at specifying the performance of any learning methodology. Eligible selection of feature could enable to a better trained method, while inconvenient features would crucially disrupt the training procedure  Trigeorgis et al. (2016) . In this work, we mainly focus on to detect the speech emotion from raw audio files without using hand-crafted features. The feature extraction stage is automatically carried out in the deep learning architectures by processing raw audio files, directly. To show the effectiveness of proposed model, the performance of the system is compared with the conventional feature extraction techniques, namely, Mel-scale Spectogram and Mel-Frequency Cepstral coefficients (MFCC).\n\nMel-scaled spectrogram is widely applied in sound classification and speech emotion recognition tasks  (Stevens et al., 1937) . The features obtained with Mel-scaled spectrogram makes possible to imitate the sound frequency of human in a specific rank. It is known that Mel-scaled spectrogram performs well recognition and, pursing timbre The MFCC  Dave (2013)  feature extraction technique is considered to be the closest feature extraction technique to the human auditory system. Initially, in this procedure, the original signal is translated from the time domain to the frequency domain using the discrete Fourier transform (DFT). For this conversion, the power spectrum is employed. For the purpose of reducing frequency distortion brought on by segmentation prior to DFT, hamming window is utilized. The frequency is then wrapped from the hertz scale to the mel scale using a filter bank. In conclusion, the logarithm of the Mel scale power spectrum's feature vectors are extracted using discrete cosine transformation (DCT)  Sunitha and Chandra (2015) . At the pre-processing stage, as same as with Mel-spectogram noise reduction, windowing, framing are carried out to the speech signal. 40 features are acquired as a result of MFCC processes. The size of the MFCC Training acoustic models straight from the raw wave-form data is challenging task in speech recognition field. Traditional deep neural network-based acoustic methods is based on processing hand-crafted input features. In this work, we propose deep neural network-based acoustic method which fed with raw multichannel waveforms as input by inspiring studies  Dai et al. (2017) ,  Hoshen et al. (2015) ,  (Palaz et al., 2013) ,  T√ºske et al. (2014) ,  Jaitly and Hinton (2011)  without performing feature extraction stage by constructing high-level representative features. As a pre-processing step, raw sounds are first normalized to mean 0 and variance 1. If the length of the audio data is more than the specified upper limit size (6 sn), it is clipped. Otherwise if length is lower than threshold, input array is padded with \"0\" values.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Machine Learning Based Methods",
      "text": "Machine learning which is a subfield of artificial intelligence, is a way of teaching computers how to do things that people can do naturally, like learning to recognize patterns in data. There are different machine learning algorithms, each of which is good at solving different types of problems. In this work, some of popular machine learning algorithms and their ensemble versions are used to solve speech emotion recognition problem.\n\nSVM  (Cristianini and Shawe-Taylor, 2000)  is a machine learning algorithm used for classification (determining whether objects belong to a certain category) and regression (in predicting future values). It is particularly useful for classification problems in which the objects in the data set can be neatly divided into groups, or classes. The SVM algorithm helps us to divide a space into categories so that we can easily put new data points into the right category in the future. This is done by creating a decision boundary that separates the different categories. The Support Vector Machine (SVM) selects the points along the axes that are most helpful in creating a hyperplane. These points are called \"support vectors,\" and this is the reason algorithm is called as a Support Vector Machine.\n\nk-NN  (Mucherino et al., 2009)  algorithm assumes that things that are similar tend to be near each other. It is the simplest machine learning algorithm. KNN is basically based on the premise that the class values of nearest samples will be similar. In this evaluation two values are used. Distance: The distance of the k nearest sample of the sample whose class value is to be calculated. K (neighbor count): It is the number of nearest neighbors which this calculation will be made.\n\nDecision tree  (Rokach and Maimon, 2015)  is a algorithm that can help model situations and make decisions based on dividing the data set into smaller subgroups that are within the framework of certain rules (decision rules). It has a tree-like structure, with branches representing different possible outcomes. Trees are also useful for modeling resource costs and possible outcomes for decision making. The tree structure contains some units. Internal nodes representing the tests or attributes of each stage. Each branch indicate an attribute result. At final, the path from leaf to root represents rules of classification.\n\nNa√Øve Bayes  (Webb et al., 2010)  classification is a method used to estimate the probability that a particular set of features belongs to a particular class. It uses the Bayes theorem to calculate the probability of each class, and then selects the class with the highest probability. This method is much faster than more complex methods, and is often used to quickly determine which class a particular set of features belongs to.\n\nEnsemble methods are a way to combine the predictions of several different estimators to improve the accuracy and reliability of predictions. One of the most known and used ensemble methods is majority voting. In this approach, different models perform predictions and the prediction with the most votes is determined as the final decision.\n\nAnother popular ensemble approach is stacking  (D≈æeroski, 2004) . This approach includes a two-level learning process, Base (Level 0) and Meta (Level 1). The base classifiers run in parallel and their estimations are combined into a metadata. Then these estimations become input into the meta classifier. Basically, stacking approach tries to figure out the best way to combine the input predictions to get a more accurate output prediction.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Deep Learning Based Methods",
      "text": "In this work, 3 different deep learning approaches are used: conventional CNN, LSTM and CNN-LSTM. Deep learning is considered as the cutting edge of artificial intelligence today. It is basically inspired by the human learning system. With its hierarchical connections and its multi-layered structure, it enables the learning from the lowest level features to the highest level features. The values of the weights of the connections in the layers are the most important point of the learning function. Convolutional Neural Networks (CNN) is developed based on the specialization of neurons in the visual and perception system of humans. While 2-dimensional filters are used in 2-dimensional images, learning function is performed with 1-dimensional filters in 1-dimensional data such as sound or time series.\n\nRecurrent Neural Networks (RNNs) are a type of Deep Learning structures used to predict the next step. RNNs use the output of the previous step as the input of the current step, whereas classical deep learning networks work independently of each other. As a result, the RNN ensures that each output it generates follows the previous step. As a result, it tries to store the results of the previous steps in its memory. However, they are successful in predicting short-term dependencies, they are not successful enough in long-term dependencies. Because of these fundamental RNN problems, later variants of Long Short-Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  networks have been proposed.\n\nLSTM is proposed as a solution to the short-term memory problem. It solves this problem using Cell State and various gates. Cell State is a line that carries meaningful information across cells, and the gates use the sigmoid activation function to squash the data between 0 to 1. If the values that the sigmoid activation function can have are taken into consideration, 0 means that the information will be forgotten, and 1 means that it will continue to be used as it is. LSTM has 3 gates as forget gate, input gate, and output gate. The Forget Gate determines amount of information in a memory will be forgotten or kept. The Input Gate updates the Cell State based on the results of the sigmoid process. The Output Gate decides what the next cell's input will be.\n\nThe proposed CNN model which is depicted in Figure  3  consists of one-dimensional convolutional layers consolidated with activation, batch normalization and dropout layers. The first layer is consisted of 256 filters with the kernel size of 1 √ó 5 and stride 1. After that, the output is activated by using rectified linear unit (ReLU) and dropout layer with the ratio of 0.25 is performed. The second layer and the third layer is also constructed with 256 filters, and the same stride and kernel size are processed similarly preceding layer. In these 2 layers after convolution, batch normalization is carried out sending its output to the dropout layer with the ratio of 0.25. After that, convolution layers with 128 filters of size 1 √ó 5 is applied in fourth and fifth layers and before fifth convolutional layer ReLU activation and dropout layers is used. Then, 2 Fully Connected Layers (FCN) with 2432 and 8 are pursued after the last convolutional layer. Finally, output layer has 8 output for emotions and it uses Softmax function.\n\nThe proposed LSTM network used in this work shown in Figure  4 . First, LSTM block with 512 node is performed on input data. Then batch normalization, dropout layer with ratio of 0.25 and FCN with 256 size is applied, respectively. Next, batch normalization, dropout layer with ratio of 0.25 and FCN with 128 size is constructed. After that dropout and FCN with 64 is carried out. Then, batch normalization is carried out sending its output to the dropout layer with the ratio of 0.25. Afterwards, it is continued 2 Fully Connected Layers (FCN) with 2000 and 8 after the last LSTM with 50 nodes. Finally, output layer has 8 output for emotions and it uses Softmax function.\n\nThe CNN-LSTM method shown in Figure  5  has been applied to combine the feature extraction of CNN networks and the long term dependencies of LSTM. The first layer is consisted of 256 filters and second layer is consisted of 128 filters with the kernel size of 1 √ó 5 and stride 1. After that, batch normalization is carried out sending its output to the dropout layer with the ratio of 0.25. Then 2 set of convolutional layer, batch normalization and dropout is carried out. Filter sizes of this convloutional layer is 128 and 64 respectively. Next, LSTM block with 512 node is performed. It is continued 2 Fully Connected Layers (FCN) with 2000 and 8. Finally, output layer has 8 output for emotions as same as the other 2 architectures.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "In this study, 6 different dataset is used to demonstrate performance and generalization capability of proposed method. Details of these datasets are given in Section 3.1. All datasets are divided into 80% training and 20% test. The performance of the proposed methods is calculated using the accuracy formula given in (1). In (1), every class can be named as ùê∂ ùë•  (happy, neutral etc.) . TP (True Positive) denotes audio belonging to the ùê∂ ùë• is correctly classified as  Evaluation results of machine learning based methods are given in Table  2  and Table  3 . Table  2  shows the results where MFCC is used as the feeature extraction in machine learning-based methods, and Table  3  shows the results where Mel-Spectrogram is the feature extraction. In Table  2 , SVM is can be considered to be more successful as it gives the best result in 4 of 6 datasets. According to In Table  3 , Random Forest has the best results in all datasets. When Table  2  and Table  3  are evaluated together, usage of Mel-Spectrogram and Random forest jointly gives more successful results.\n\nConsidering that machine learning-based methods did not show sufficient performance, MFCC and Mel feature extraction methods are used together with the deep learning architectures given in Section 3.4. The MFCC and Melspectrogram features of the raw audio signals are extracted and these features are given as an input to the deep networks. Table  4  and Table  5  show results using MFCC features and Mel-spectrogram features as inputs, respectively. It can be understood that, using MFCC and CNN together gives best results in all datasets. In addition, it is clearly seen that this approach is superior to machine learning-based methods.\n\nInstead of extracting distinctive features of the audio signal using feature extraction methods such as MFCC or Mel-Spectrogram, the use of raw audio as input for deep learning models is analyzed. Here, the most important aim is to eliminate human intervention and to determine the most characteristic features automatically with the deep learning approach. Table  6  shows end-to-end deep learning result of models given in Section 3.4. The training parameters of the deep architectures are given in Table  7 . It can be seen in Table  7 , the CNN-based deep learning approach give the The bold values show the best performance best results in 5 out of 6 datasets. Intriguingly, these results show that long-term dependencies features seem to play a minor role at Speech Emotion Recognition.\n\nTable  8  shows the comparison of the recent and proposed methods for sound emotion recognition in the literature. The comparison is carried out on the EMO-DB, RAVDESS, SAVEE, TESS, CREMA, TESS+RAVDESS datasets. The literature results used in comparison in Table  8  are provided on their papers. In this table, proposed method refers CNN method with raw audio. As seen in Table  8 , proposed method has superior performance results in 5 out of 6 datasets and has second best result in the other.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Kilimci et al.: Preprint submitted to Elsevier",
      "page": 3
    },
    {
      "caption": "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology",
      "page": 4
    },
    {
      "caption": "Figure 2: RAVDESS dataset (Livingstone and Russo, 2018) contains English sentences voiced by 12 male and 12",
      "page": 4
    },
    {
      "caption": "Figure 2: The wave forms of seven emotions from EMO-DB dataset (a) Neutral (b) Bored (c) Angry (d) Disgust (e) Fear",
      "page": 5
    },
    {
      "caption": "Figure 3: consists of one-dimensional convolutional layers",
      "page": 7
    },
    {
      "caption": "Figure 4: First, LSTM block with 512 node is performed",
      "page": 7
    },
    {
      "caption": "Figure 5: has been applied to combine the feature extraction of CNN networks",
      "page": 7
    },
    {
      "caption": "Figure 3: CNN architecture",
      "page": 8
    },
    {
      "caption": "Figure 4: LSTM architecture",
      "page": 8
    },
    {
      "caption": "Figure 5: CNN-LSTM architecture",
      "page": 9
    },
    {
      "caption": "Figure 6: The confusion matrix obtained using Proposed method on various datasets (a) EMO-DB (b) RAVDESS (c)",
      "page": 11
    },
    {
      "caption": "Figure 6: The confusion matrices show how many times an observation‚Äôs predicted class (the column in the table) matches its",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for": "speech emotion recognition"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "Zeynep Hilal Kilimcia, Ulk√º Bayraktarb and Ayhan K√º√ß√ºkmanisab,‚àó"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "aDepartment of Information Systems Engineering, Kocaeli University, 41001, Kocaeli, Turkey"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "bDepartment of Electronics and Communication Engineering, Kocaeli University, 41001, Kocaeli, Turkey"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "A R T I C L E I N F O"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "Keywords:"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "Speech emotion recognition"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "Raw audio files"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "Deep learning"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "LSTM"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "CNN"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": "CNN-LSTM"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Deep learning-based models are commonly employed by the researchers because of providing better predictions"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and results compared with traditional machine learning algorithms in different domains such as face recognition,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "voice recognition,\nimage recognition Mittal et al.\n(2018), Bae et al.\n(2016), He et al.\n(2016). The usage of deep"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "learning architectures facilitates automatic feature selection process unlike gathering hand-crafted features. Lately,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "diverse deep learning-based models are also utilized for\nspeech emotion recognition tasks\nin the state-of-the-art-"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "studies. The literature works generally focus on to discover important attributes of speech signal using deep learning"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "models Trigeorgis et al. (2016) or demonstrate the performance of deep learning models on a specific feature extraction"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "technique Han et al. (2014). In this work, we obtain deep features from raw sound data and feeding them into different"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "deep learning algorithms for speech emotion recognition task instead of employing conventional\nfeature extraction"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "techniques such as MFCC, MEL."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "In this work, evaluating raw waveforms is proposed without applying any feature extraction stage using traditional"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "machine learning algorithms, ensemble learning approaches, and deep learning architectures\nfor\nspeech emotion"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "recognition task. For this purpose, convolutional neural networks, long short-term memory networks, and hybrid CNN-"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "LSTM models are evaluated deep learning techniques while support vector machine, decision tree, naive Bayes, random"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "forests, majority voting, stacking models are evaluated as machine learning and ensemble algorithms. To demonstrate"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the contribution of the proposed framework, the performance of traditional feature extraction techniques and the results"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "of state-of-the-art studies are compared with the proposed model on six different dataset. The utilization of raw audio"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "files instead of employing feature extraction process for speech emotion recognition task shows remarkable results"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "when compared to the literature studies."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "The remaining of the paper is organized as follows: Section 2 presents literature review. Materials and methods"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "used in the study are given in Section 3. Data acquisition and proposed framework are demonstrated in Section 4. In"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Section 5, experiment results and conclusions are represented."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "2. Literature Review"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "This section gives a brief summary of literature works for speech emotion recognition."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(Issa et al., 2020)\nintroduce convolutional neural network (CNN) architecture is introduced for speech emotion"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "recognition task. Mel-frequency Cepstral Coefficients\n(MFCCs), Mel-scaled spectrogram, Chromagram, Spectral"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "contrast feature, and Tonnetz representation are evaluated at the stage of feature extraction. After that, extracted features"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "from sound files are fed into CNN to show the effectiveness of feature extraction models in RAVDESS, EMO-DB,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and IEMOCAP data sets. The proposed model exhibits the best classification performance in EMO-DB data set with"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "86.1% of accuracy. (Sajjad et al., 2020) propose a new clustering based approach with the help of radial-based function"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "network for SER. Determined key sequences are fed into Bidirectional\nlong short-term memory network to obtain"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "final state of the emotion. The proposed approach is assessed over IEMOCAP, EMO-DB, and RAVDESS data sets."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Experiment results show that the proposed approach represents remarkable results in terms of classification accuracy"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "when compared to the state-of-the-art studies."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(Kwon, 2019) presents a CNN-based framework for speech emotion recognition. To show the effectiveness of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the model, IEMOCAP and RAVDESS data sets are evaluated. The authors report\nthat\nthe proposed model enhance"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the classification accuracy by 7.85% for IEMOCAP and 4.5% for RAVDESS. Zhao et al. (2019) evaluate 1D and 2D"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "CNN-LSTM networks to recognize speech emotion. The performance of hybrid deep learning model\nis compared"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "with deep belief network and CNN architecture over\ntwo data sets namely,\nIEMOCAP and EMO-DB. They report"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "that the performance of hybrid deep learning model for SER is rather competitive when compared to the conventional"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "techniques. (Koduru et al., 2020) investigate the impact of feature extraction techniques to enhance the performance of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "speech emotion rate. For this purpose, Mel frequency cepstral coefficients, Discrete Wavelet Transform (DWT), pitch,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "energy and Zero crossing rate (ZCR) models are employed at\nthe stage of feature extraction. To show the efficieny"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "of\nfeature extraction techniques, support vector machine, decision tree (DT) and LDA models are evaluated. The"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "utilization of DT performs the best classification accuracy with nearly 85%."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(Kwon et al., 2021) propose a multi-learning strategy by providing end-to-end real\ntime model\nfor SER. The"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "proposed dilated CNN (DCNN) model\nis evaluated on two benchmark data sets namely, IEMOCAP and EMO-DB."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Authors report\nthat\nthe usage DCNN model exhibits significant accuracy results with 73% for IEMOCAP and 90 %"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "for EMO-DB. (Zehra et al., 2021) propose an ensemble-based framework for cross corpus multi-lingual recognition"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "of speech emotion. The performance of an ensemble model, majority voting, is compared with conventional machine"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "learning techniques. The utilization of ensemble model ensures enhancement in classification accuracy nearly 13% for"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Page 2 of 14\nKilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Urdu data set, roughly 8% for German data set, 11% for Italian data set, and 5% for English data set. (Anvarjon and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Kwon, 2020) concentrate on a lightweight CNN approach for speech emotion recognition task. To show the efficiency"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "of proposed model, experiments are carried on IEMOCAP, and EMO-DB data sets. Experiment results indicate that a"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "lightweight CNN model is capable to recognize emotion of speech with 77.01 % of accuracy for IEMOCAP data set,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and 92.02% of accuracy for EMO-DB data set. (√ñzseven, 2019) presents a novel statistical feature selection technique"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "by taking into consideration average of each featue in the features set for SER. Recognition performance of the model"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "is compared on EMO-DB, eNTERFACE05, EMOVO and SAVEE data sets by using SVM, MLP, and k-NN classifiers."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Except eNTERFACE05 data set, SVM model outperforms other machine learning algorithms in terms of classification"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "accuracy in all data sets."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(Sun et al., 2019) propose DNN-decision tree SVM model by calculating the confusion degree of emotion with"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "decision tree SVM and training with DNN architecture. To demonstrate the effectiveness of\nthe proposed model,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "experiments are carried on Chinese Academy of Sciences Emotional data set. The proposed model achieves remarkable"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "experiment\nresults when compared to conventional SVM and DNN-SVM technique by ensuring nearly 6% and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "roughly 3% enhancement\nin the success of\nrecognition rate,\nrespectively.Cai et al.\n(2021)\nfocus on the multitask"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "learning approach for speech emotion recognition by ensuring speech-to-text recognition and emotion categorization,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "simultaneously. The efficiency of the model\nis demonstrated on the IEMOCAP data set by achieving nearly 78% of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "accuracy. (Chen et al., 2020) present\ntwo-layer fuzzy multiple ensemble framework using fuzzy C-means algorithm"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and random forest model for SER. The proposed framework is capable to recognize emotions on CASIA and EMO-"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "DB data sets by improving recognition accuracy when compared to back propagation and random forest models."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(Farooq et al., 2020) concentrate on the effect of feature selection model utilizing deep convolutional neural network"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(DCNN) for SER. After extracting features from pretrained models, the most discriminatory features are determined"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "by a correlation-based feature selection technique. At the classification stage, support vector machines, random forests,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the k-nearest neighbors algorithm, and neural network classifier are employed on EMO-DB, SAVEE, IEMOCAP, and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "RAVDESS data sets. The model achieves 95.10% for Emo-DB, 82.10% for SAVEE, 83.80% for IEMOCAP, and 81.30%"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "for RAVDESS in terms of classification accuracies."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(Zhang et al., 2021) propose a novel deep multimodal model for spontaneous SER. The proposed model is based on"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the combination of three different audio inputs by feeding them into multi-CNN fusion model. The combination strategy"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "performs promising classification results when compared with the sate-of-the-art results. (Tuncer et al., 2021) focus"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "on SER by employing twine shuffle pattern and iterative neighborhood component analysis methods. The proposed"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "model is based on feature generation and selection stages utilizing shuffle box and iterative neighborhood component"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "analysis methodologies, respectively. To demonstrate the efficiency of the model,\nthe experiments are carried out on"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "EMO-DB, SAVEE, RAVDESS, EMOVO data sets. Proposed model achieves 87.43% for RAVDESS, 90.09% for EMO-"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "DB, 84.79% for SAVEE, and 79.08% for EMOVO in terms of classification accuracy. (Lu et al., 2022) present domain"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "invariant feature learning (DIFL) models to address speaker-independent speech emotion recognition. The experiments"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "are performed on EMO-DB, eNTERFACE, and CASIA data set to demonstrate the contribution of the proposed model."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Experiment results indicate that the utilization of proposed model exhibits remarkable results compared to the literature"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "studies."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "To the best of our knowledge, our study is the first attempt to process raw audio files by blending them with machine"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "learning and deep learning methods for the task of speech emotion recognition and differs from the aforementioned"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "literature studies in this aspect."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "3. Datasets and methodology"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "In this\nsection, datasets used in the\nstudy,\nand proposed methodology are presented. Six different publicly"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "available and widely applied datasets, The Berlin Database of Emotional Speech (EMO-DB), The Ryerson Audio-"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Visual Database of Emotional Speech and Song (RAVDESS), Toronto Emotional Speech Database (TESS), Crowd-"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "sourced Emotional Multimodal Actors dataset\n(CREMA), Surrey Audio-Visual Expressed Emotion (SAVEE), and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "TESS+RAVDESS are employed in the experiments. After\nthat, methodology is introduced with feature extraction"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "stage, models, and the proposed framework. The proposed speech emotion recognition methodology is demonstrated"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "in Figure 1."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "3.1. Datasets"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Six different data sets (EMO-DB, RAVDESS, TESS, CREMA, SAVEE, TESS+RAVDESS) are utilized to enhance"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "the generalization capacity of\nthe consequences acquired in the proposed work. EMO-DB dataset Burkhardt et al."
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "(2005)\nis composed of definition of different\nfeelings\n(neutral, happiness, anger, disgust,\nsadness, boredom, and"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "fear/anxiety) by ten different actors whom of 5 is female. The dataset\nincludes 535 words in German and each of"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "audio file has 16 kHz sampling frequency with 16 bit quantization. The waveform of each emotion is represented in"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Figure 2. RAVDESS dataset\n(Livingstone and Russo, 2018) contains English sentences voiced by 12 male and 12"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "female actors. RAVDESS is composed of 1,440 utterances with eight different emotion categories, namely, surprised,"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "angry, happy, bored, sad, fearful, disgust, and neutral."
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "TESS Pichora-Fuller and Dupuis (2020) comprises of audio records of 2 female speakers pronouncing English"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "sentences. TESS contains 2,800 utterances of with anger, disgust, neutral, fear, happiness, sadness, bored, surprise"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "emotional categories. CREMA (Cao et al., 2014) includes 7,442 original clips generated by 43 female and 48 male"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "actors\nfrom various ethnicities\nsuch as Hispanic, Asian, African, American. Specified 12 sentence are vocalized"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "capturing six different emotions. These are sad, happy, disgust, neutral, anger, fear. SAVEE dataset Haq et al. (2008)"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "covers 1,680 utterances vocalized by 14 male actors in seven different emotions. These are surprise, anger, happiness,"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "disgust, neutral,\nfear, sadness. The sentences recorded by actors are picked up from the TIMIT Acoustic-Phonetic"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Continuous Speech corpus. The combination of TESS and RAVDESS data sets is called TESS+RAVDESS in this"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "study. Because the emotional categories are common, there is no problem to consolidate them. The dataset consists of"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "4,240 utterances voiced by 14 female and 12 male actors. The distribution of the datasets used in the study is given in"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Table 1."
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "3.2. Feature extraction and pre-processing"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Feature extraction stage plays an importance role at specifying the performance of any learning methodology."
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Eligible selection of\nfeature could enable to a better\ntrained method, while inconvenient\nfeatures would crucially"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "disrupt the training procedure Trigeorgis et al. (2016). In this work, we mainly focus on to detect the speech emotion"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "from raw audio files without using hand-crafted features. The feature extraction stage is automatically carried out\nin"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "the deep learning architectures by processing raw audio files, directly. To show the effectiveness of proposed model,"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "the performance of\nthe system is compared with the conventional\nfeature extraction techniques, namely, Mel-scale"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Spectogram and Mel-Frequency Cepstral coefficients (MFCC)."
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Mel-scaled spectrogram is widely applied in sound classification and speech emotion recognition tasks (Stevens"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "et al., 1937). The features obtained with Mel-scaled spectrogram makes possible to imitate the sound frequency of"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "human in a specific rank.\nIt\nis known that Mel-scaled spectrogram performs well\nrecognition and, pursing timbre"
        },
        {
          "Figure 1: The block diagram of proposed methodology (a) ML based methodology (b) DL based methodology": "Page 4 of 14\nKilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(g)": "Figure 2: The wave forms of seven emotions from EMO-DB dataset (a) Neutral (b) Bored (c) Angry (d) Disgust (e) Fear"
        },
        {
          "(g)": "(f) Happy (g) Sad."
        },
        {
          "(g)": "fluctuations\nin an audio file. On the other hand,\nit\ninclines\nto be weak when a distinguishable representation of"
        },
        {
          "(g)": "pitch classes and harmony are considered Beigi\n(2011). Log-mel spectrogram feature set comprises in Mel-scaled"
        },
        {
          "(g)": "spectrograms demonstrating emotion states. At\nthe pre-processing stage, noise reduction, windowing,\nframing are"
        },
        {
          "(g)": "carried out to the speech signal. 1024 size of short time Fourier transform and 0.025 s and 0.010 s overlapped window"
        },
        {
          "(g)": "size with hamming window are employed in the experiments.\nIn addition, 128 equal-width log-energies are used."
        },
        {
          "(g)": "Finally, 168 features are acquired as a result of Mel-spectogram processes."
        },
        {
          "(g)": "The MFCC Dave (2013) feature extraction technique is considered to be the closest feature extraction technique"
        },
        {
          "(g)": "to the human auditory system. Initially, in this procedure, the original signal is translated from the time domain to the"
        },
        {
          "(g)": "frequency domain using the discrete Fourier transform (DFT). For this conversion, the power spectrum is employed. For"
        },
        {
          "(g)": "the purpose of reducing frequency distortion brought on by segmentation prior to DFT, hamming window is utilized."
        },
        {
          "(g)": "The frequency is then wrapped from the hertz scale to the mel scale using a filter bank. In conclusion, the logarithm of"
        },
        {
          "(g)": "the Mel scale power spectrum‚Äôs feature vectors are extracted using discrete cosine transformation (DCT) Sunitha and"
        },
        {
          "(g)": "Chandra (2015). At\nthe pre-processing stage, as same as with Mel-spectogram noise reduction, windowing, framing"
        },
        {
          "(g)": "are carried out\nto the speech signal. 40 features are acquired as a result of MFCC processes. The size of the MFCC"
        },
        {
          "(g)": "Page 5 of 14\nKilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Distribution of emotion categories for each dataset",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "NR"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "79"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "288"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "400"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "1271"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "120"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "688"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Ensemble methods are a way to combine the predictions of several different estimators to improve the accuracy"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and reliability of predictions. One of the most known and used ensemble methods is majority voting. In this approach,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "different models perform predictions and the prediction with the most votes is determined as the final decision."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Another popular ensemble approach is stacking (D≈æeroski, 2004). This approach includes a two-level"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "process, Base (Level 0) and Meta (Level 1). The base classifiers run in parallel and their estimations are combined into"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "a metadata. Then these estimations become input\ninto the meta classifier. Basically, stacking approach tries to figure"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "out the best way to combine the input predictions to get a more accurate output prediction."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "3.4. Deep learning based methods"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "In this work, 3 different deep learning approaches are used: conventional CNN, LSTM and CNN-LSTM. Deep"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "learning is considered as the cutting edge of artificial intelligence today. It is basically inspired by the human learning"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "system. With its hierarchical connections and its multi-layered structure, it enables the learning from the lowest level"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "features to the highest level features. The values of the weights of the connections in the layers are the most important"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "point of\nthe learning function. Convolutional Neural Networks (CNN)\nis developed based on the specialization of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "neurons in the visual and perception system of humans. While 2-dimensional filters are used in 2-dimensional images,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "learning function is performed with 1-dimensional filters in 1-dimensional data such as sound or time series."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Recurrent Neural Networks (RNNs) are a type of Deep Learning structures used to predict\nthe next step. RNNs"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "use the output of the previous step as the input of the current step, whereas classical deep learning networks work"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "independently of each other. As a result, the RNN ensures that each output it generates follows the previous step. As"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "a result,\nit\ntries to store the results of\nthe previous steps in its memory. However,\nthey are successful\nin predicting"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "short-term dependencies,\nthey are not successful enough in long-term dependencies. Because of these fundamental"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "RNN problems,\nlater variants of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "have been proposed."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "LSTM is proposed as a solution to the short-term memory problem. It solves this problem using Cell State and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "various gates. Cell State is a line that carries meaningful\ninformation across cells, and the gates use the sigmoid"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "activation function to squash the data between 0 to 1.\nIf\nthe values that\nthe sigmoid activation function can have"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "are taken into consideration, 0 means that\nthe information will be forgotten, and 1 means that\nit will continue to be"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "used as it\nis. LSTM has 3 gates as forget gate,\ninput gate, and output gate. The Forget Gate determines amount of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "information in a memory will be forgotten or kept. The Input Gate updates the Cell State based on the results of the"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "sigmoid process. The Output Gate decides what the next cell‚Äôs input will be."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "The proposed CNN model which is depicted in Figure 3 consists of one-dimensional\nconvolutional\nlayers"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "consolidated with activation, batch normalization and dropout\nlayers. The first\nlayer is consisted of 256 filters with"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the kernel size of 1 √ó 5 and stride 1. After that, the output is activated by using rectified linear unit (ReLU) and dropout"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "layer with the ratio of 0.25 is performed. The second layer and the third layer\nis also constructed with 256 filters,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and the same stride and kernel size are processed similarly preceding layer. In these 2 layers after convolution, batch"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "normalization is carried out sending its output to the dropout layer with the ratio of 0.25. After that, convolution layers"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "with 128 filters of size 1 √ó 5 is applied in fourth and fifth layers and before fifth convolutional layer ReLU activation and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "dropout layers is used. Then, 2 Fully Connected Layers (FCN) with 2432 and 8 are pursued after the last convolutional"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "layer. Finally, output layer has 8 output for emotions and it uses Softmax function."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "The proposed LSTM network used in this work shown in Figure 4. First, LSTM block with 512 node is performed"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "on input data. Then batch normalization, dropout layer with ratio of 0.25 and FCN with 256 size is applied, respectively."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Next, batch normalization, dropout layer with ratio of 0.25 and FCN with 128 size is constructed. After that dropout"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and FCN with 64 is carried out. Then, batch normalization is carried out sending its output to the dropout layer with"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "the ratio of 0.25. Afterwards,\nit\nis continued 2 Fully Connected Layers (FCN) with 2000 and 8 after the last LSTM"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "with 50 nodes. Finally, output layer has 8 output for emotions and it uses Softmax function."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "The CNN-LSTM method shown in Figure 5 has been applied to combine the feature extraction of CNN networks"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and the long term dependencies of LSTM. The first layer is consisted of 256 filters and second layer is consisted of 128"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "filters with the kernel size of 1 √ó 5 and stride 1. After that, batch normalization is carried out sending its output to the"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "dropout layer with the ratio of 0.25. Then 2 set of convolutional layer, batch normalization and dropout is carried out."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Filter sizes of this convloutional layer is 128 and 64 respectively. Next, LSTM block with 512 node is performed. It is"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "continued 2 Fully Connected Layers (FCN) with 2000 and 8. Finally, output layer has 8 output for emotions as same"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "as the other 2 architectures."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Page 7 of 14\nKilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: LSTM architecture": "Using convolutional neural network architecture, deep features are fed into the three different deep and hybrid"
        },
        {
          "Figure 4: LSTM architecture": "deep neural networks, namely convolutional neural network (CNN), long short-term memory network (LSTM), CNN-"
        },
        {
          "Figure 4: LSTM architecture": "LSTM. Thence, speech emotion recognition procedure is implemented using deep features."
        },
        {
          "Figure 4: LSTM architecture": "4. Experimental results"
        },
        {
          "Figure 4: LSTM architecture": "In this study, 6 different dataset\nis used to demonstrate performance and generalization capability of proposed"
        },
        {
          "Figure 4: LSTM architecture": "method. Details of these datasets are given in Section 3.1. All datasets are divided into 80% training and 20% test. The"
        },
        {
          "Figure 4: LSTM architecture": "performance of the proposed methods is calculated using the accuracy formula given in (1). In (1), every class can"
        },
        {
          "Figure 4: LSTM architecture": "be named as ùê∂ùë•\n(happy, neutral etc.). TP (True Positive) denotes audio belonging to the ùê∂ùë•"
        },
        {
          "Figure 4: LSTM architecture": "Page 8 of 14\nKilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Evaluation results of machine learning methods with MFCC features",
      "data": [
        {
          "Evaluation results of machine learning methods with MFCC features": "DATASET"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "EMO-DB"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "RAVDESS"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "TESS"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "CREMA"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "SAVEE"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "TESS+RAVDESS"
        },
        {
          "Evaluation results of machine learning methods with MFCC features": "The bold values show the best performance"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Evaluation results of machine learning methods with Mel-Spectrogram",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "SVM"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "76.01"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "27.08"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "84.61"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "33.78"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "63.05"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "57.11"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Evaluation results of machine learning methods with Mel-Spectrogram",
      "data": [
        {
          "Evaluation results of deep learning methods with MFCC features": "DATASET"
        },
        {
          "Evaluation results of deep learning methods with MFCC features": "EMO-DB"
        },
        {
          "Evaluation results of deep learning methods with MFCC features": "RAVDESS"
        },
        {
          "Evaluation results of deep learning methods with MFCC features": "TESS"
        },
        {
          "Evaluation results of deep learning methods with MFCC features": "CREMA"
        },
        {
          "Evaluation results of deep learning methods with MFCC features": "SAVEE"
        },
        {
          "Evaluation results of deep learning methods with MFCC features": "TESS+RAVDESS"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(a)"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(c)"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(e)\n(f)"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Figure 6: The confusion matrix obtained using Proposed method on various datasets"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "TESS (d) CREMA (e) SAVEE (f) TESS+RAVDESS."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Kilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 7: Training parameters",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Parameter"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": ""
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Learning rate"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Epoch"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Batch"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Optimization algorithm"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 7: Training parameters",
      "data": [
        {
          "Table 8": "Comparison of proposed method and literature"
        },
        {
          "Table 8": "Method"
        },
        {
          "Table 8": "(√ñzseven, 2019)"
        },
        {
          "Table 8": "(Bhavan et al., 2019)"
        },
        {
          "Table 8": "(Suganya and Charles, 2019)"
        },
        {
          "Table 8": "(Assun√ß√£o et al., 2020)"
        },
        {
          "Table 8": "(Issa et al., 2020)"
        },
        {
          "Table 8": "(Nguyen et al., 2020)"
        },
        {
          "Table 8": "(U A and V K, 2021)"
        },
        {
          "Table 8": "(Ristea and Ionescu, 2021)"
        },
        {
          "Table 8": "(Tuncer et al., 2021)"
        },
        {
          "Table 8": "(Krishnan et al., 2021)"
        },
        {
          "Table 8": "(Blumentals and Salimbajevs, 2022)"
        },
        {
          "Table 8": "(Singh and Goel, 2023)"
        },
        {
          "Table 8": "Proposed Method"
        },
        {
          "Table 8": "The bold values show the best performance,"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "References"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Anvarjon, T., Kwon, S., 2020. Deep-net: A lightweight cnn-based speech emotion recognition system using deep frequency features. Sensors 20,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "5212."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Assun√ß√£o, G., Menezes, P., Perdig√£o, F., 2020. Speaker awareness for speech emotion recognition.\nInternational Journal of Online and Biomedical"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Engineering (iJOE) 16, 15."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Bae, H.S., Lee, H.J., Lee, S.G., 2016. Voice recognition based on adaptive mfcc and deep learning, in: 2016 IEEE 11th Conference on Industrial"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Electronics and Applications (ICIEA), IEEE. pp. 1542‚Äì1546."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Beigi, H., 2011. Fundamentals of speaker recognition. Springer Science & Business Media."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Bhavan, A., Chauhan, P., Hitkul, Shah, R.R., 2019.\nBagged support vector machines for emotion recognition from speech. Knowledge-Based"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Systems 184, 104886."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Blumentals, E., Salimbajevs, A., 2022. Emotion recognition in real-world support call center data for latvian language,\nin: ACM IUI Workshops"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "2022, Springer, Helsinki, Finland."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F., Weiss, B., et al., 2005. A database of german emotional speech.,\nin: Interspeech, pp."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "1517‚Äì1520."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Cai, X., Yuan, J., Zheng, R., Huang, L., Church, K., 2021. Speech emotion recognition with multi-task learning., in: Interspeech, pp. 4508‚Äì4512."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A., Verma, R., 2014. Crema-d: Crowd-sourced emotional multimodal actors dataset."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "IEEE transactions on affective computing 5, 377‚Äì390."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Chen, L., Su, W., Feng, Y., Wu, M., She, J., Hirota, K., 2020. Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "interaction.\nInformation Sciences 509, 150‚Äì163."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Cristianini, N., Shawe-Taylor, J., 2000. An introduction to support vector machines. Cambridge University Press, Cambridge, UK."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Dai, W., Dai, C., Qu, S., Li, J., Das, S., 2017. Very deep convolutional neural networks for raw waveforms, in: 2017 IEEE international conference"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "on acoustics, speech and signal processing (ICASSP), IEEE. pp. 421‚Äì425."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Dave, N., 2013. Feature extraction methods lpc, plp and mfcc in speech recognition.\nInternational Journal for Advance Research in Engineering"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and Technology 1, 1‚Äì4."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "D≈æeroski, S., Z.B., 2004.\nIs combining classifiers with stacking better than selecting the best one? Machine Learning 54, 255‚Äì273."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Farooq, M., Hussain, F., Baloch, N.K., Raja, F.R., Yu, H., Zikria, Y.B., 2020.\nImpact of feature selection algorithm on speech emotion recognition"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "using deep convolutional neural network. Sensors 20, 6008."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Han, K., Yu, D., Tashev, I., 2014. Speech emotion recognition using deep neural network and extreme learning machine, in: Interspeech 2014."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Haq, S., Jackson, P.J., Edge, J., 2008. Audio-visual feature selection and reduction for emotion classification, in: Proc. Int. Conf. on Auditory-Visual"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Speech Processing (AVSP‚Äô08), Tangalooma, Australia."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "and pattern recognition, pp. 770‚Äì778."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Computation 9, 1735‚Äì1780."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Hoshen, Y., Weiss, R.J., Wilson, K.W., 2015. Speech acoustic modeling from raw multichannel waveforms, in: 2015 IEEE international conference"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "on acoustics, speech and signal processing (ICASSP), IEEE. pp. 4624‚Äì4628."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Ingale, A.B., Chaudhari, D., 2012. Speech emotion recognition.\nInternational Journal of Soft Computing and Engineering (IJSCE) 2, 235‚Äì238."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Issa, D., Demirci, M.F., Yazici, A., 2020. Speech emotion recognition with deep convolutional neural networks. Biomedical Signal Processing and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Control 59, 101894."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Jaitly, N., Hinton, G., 2011. Learning a better representation of speech soundwaves using restricted boltzmann machines, in: 2011 IEEE International"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 5884‚Äì5887."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Koduru, A., Valiveti, H.B., Budati, A.K., 2020. Feature extraction algorithms to improve the speech emotion recognition rate. International Journal"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "of Speech Technology 23, 45‚Äì55."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Krishnan, P.T., Joseph Raj, A.N., Rajangam, V., 2021.\nEmotion classification from speech signal based on empirical mode decomposition and"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "non-linear features. Complex & Intelligent Systems 7, 1919‚Äì1934."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Kwon, S., 2019. A cnn-assisted enhanced audio signal processing for speech emotion recognition. Sensors 20, 183."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Kwon, S., et al., 2021. Mlt-dnet: Speech emotion recognition using 1d dilated cnn based on multi-learning trick approach. Expert Systems with"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Applications 167, 114177."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Livingstone, S.R., Russo, F.A., 2018. The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "facial and vocal expressions in north american english. PloS one 13, e0196391."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Lu, C., Zong, Y., Zheng, W., Li, Y., Tang, C., Schuller, B.W., 2022. Domain invariant feature learning for speaker-independent speech emotion"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "recognition.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing 30, 2217‚Äì2230."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Mittal, S., Agarwal, S., Nigam, M.J., 2018. Real time multiple face recognition: A deep learning approach, in: Proceedings of the 2018 International"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Conference on Digital Medicine and Image Processing, pp. 70‚Äì76."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Mucherino, A., Papajorgji, P.J., PM., P., 2009. K-nearest neighbor classification. Data Mining in Agriculture 34, 83‚Äì106."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Nguyen, D., Sridharan, S., Nguyen, D.T., Denman, S., Tran, S.N., Zeng, R., Fookes, C., 2020. Joint deep cross-domain transfer learning for emotion"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "recognition. arXiv:arXiv:2003.11136."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "√ñzseven, T., 2019. A novel feature selection method for speech emotion recognition. Applied Acoustics 146, 320‚Äì326."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Palaz, D., Collobert, R., Doss, M.M., 2013. Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "networks. arXiv preprint arXiv:1304.1018 ."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Pichora-Fuller, M.K., Dupuis, K., 2020. Toronto emotional speech set (TESS). URL: https://doi.org/10.5683/SP2/E8H2MF, doi:10.5683/"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "SP2/E8H2MF."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Ristea, N.C., Ionescu, R.T., 2021. Self-paced ensemble learning for speech and audio classification. arXiv:arXiv:2103.11988."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Rokach, L., Maimon, O.Z., 2015. Data mining with decision trees: Theory and applications. World Scientific, New Jersey, U.S.A."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Page 13 of 14\nKilimci et al.: Preprint submitted to Elsevier"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Sajjad, M., Kwon, S., et al., 2020. Clustering-based speech emotion recognition by incorporating learned features and deep bilstm. IEEE Access 8,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "79861‚Äì79875."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Selvaraj, M., Bhuvana, R., Padmaja, S., 2016. Human speech emotion recognition. International Journal of Engineering & Technology 8, 311‚Äì323."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Singh, Y.B., Goel, S., 2023.\nA lightweight 2d cnn based approach for speaker-independent emotion recognition from speech with new indian"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "emotional speech corpora. Multimedia Tools and Applications ."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Stevens, S.S., Volkmann, J., Newman, E.B., 1937. A scale for the measurement of the psychological magnitude pitch. The journal of the acoustical"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "society of america 8, 185‚Äì190."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Suganya, S., Charles, E.Y., 2019.\nSpeech emotion recognition using deep learning on audio recordings.\n2019 19th International Conference on"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Advances in ICT for Emerging Regions (ICTer) ."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Sun, L., Zou, B., Fu, S., Chen, J., Wang, F., 2019. Speech emotion recognition based on dnn-decision tree svm model. Speech Communication 115,"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "29‚Äì37."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Sunitha, C., Chandra, E.H., 2015. Speaker recognition using mfcc and improved weighted vector quantization algorithm.\nInternational Journal of"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Engineering and Technology 5, 1685‚Äì1692."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Trigeorgis, G., Ringeval, F., Brueckner, R., Marchi, E., Nicolaou, M.A., Schuller, B., Zafeiriou, S., 2016. Adieu features? end-to-end speech emotion"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "recognition using a deep convolutional recurrent network,\nin: 2016 IEEE international conference on acoustics, speech and signal processing"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "(ICASSP), IEEE. pp. 5200‚Äì5204."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Tuncer, T., Dogan, S., Acharya, U.R., 2021.\nAutomated accurate speech emotion recognition system using twine shuffle pattern and iterative"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "neighborhood component analysis techniques. Knowledge-Based Systems 211, 106547."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "T√ºske, Z., Golik, P., Schl√ºter, R., Ney, H., 2014. Acoustic modeling with deep neural networks using raw time signal for lvcsr, in: Fifteenth annual"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "conference of the international speech communication association."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "U A, A., V K, K., 2021. Speech emotion recognition-a deep learning approach. doi:10.1109/i-smac52330.2021.9640995."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Webb, G.I., Keogh, E., R., M., 2010. Naive bayes. Encyclopedia of Machine Learning 15, 713‚Äì714."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Zehra, W., Javed, A.R., Jalil, Z., Khan, H.U., Gadekallu, T.R., 2021. Cross corpus multi-lingual speech emotion recognition using ensemble learning."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Complex & Intelligent Systems 7, 1845‚Äì1854."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Zhang, S., Tao, X., Chuang, Y., Zhao, X., 2021. Learning deep multimodal affective features for spontaneous speech emotion recognition. Speech"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Communication 127, 73‚Äì81."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "Zhao, J., Mao, X., Chen, L., 2019. Speech emotion recognition using deep 1d & 2d cnn lstm networks. Biomedical signal processing and control"
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "47, 312‚Äì323."
        },
        {
          "Evaluating raw waveforms with deep learning frameworks for speech emotion recognition": "√ñzseven, T., 2019. A novel feature selection method for speech emotion recognition. Applied Acoustics 146, 320‚Äì326."
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep-net: A lightweight cnn-based speech emotion recognition system using deep frequency features",
      "authors": [
        "T Anvarjon",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "Speaker awareness for speech emotion recognition",
      "authors": [
        "G Assun√ß√£o",
        "P Menezes",
        "F Perdig√£o"
      ],
      "year": "2020",
      "venue": "International Journal of Online and Biomedical Engineering (iJOE)"
    },
    {
      "citation_id": "3",
      "title": "Voice recognition based on adaptive mfcc and deep learning",
      "authors": [
        "H Bae",
        "H Lee",
        "S Lee"
      ],
      "year": "2016",
      "venue": "IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)"
    },
    {
      "citation_id": "4",
      "title": "Fundamentals of speaker recognition",
      "authors": [
        "H Beigi"
      ],
      "year": "2011",
      "venue": "Fundamentals of speaker recognition"
    },
    {
      "citation_id": "5",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "Hitkul",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in real-world support call center data for latvian language",
      "authors": [
        "E Blumentals",
        "A Salimbajevs"
      ],
      "year": "2022",
      "venue": "ACM IUI Workshops 2022"
    },
    {
      "citation_id": "7",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition with multi-task learning"
    },
    {
      "citation_id": "9",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "10",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "11",
      "title": "An introduction to support vector machines",
      "authors": [
        "N Cristianini",
        "J Shawe-Taylor"
      ],
      "year": "2000",
      "venue": "An introduction to support vector machines"
    },
    {
      "citation_id": "12",
      "title": "Very deep convolutional neural networks for raw waveforms",
      "authors": [
        "W Dai",
        "C Dai",
        "S Qu",
        "J Li",
        "S Das"
      ],
      "year": "2017",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "13",
      "title": "Feature extraction methods lpc, plp and mfcc in speech recognition",
      "authors": [
        "N Dave"
      ],
      "year": "2013",
      "venue": "International Journal for Advance Research in Engineering and Technology"
    },
    {
      "citation_id": "14",
      "title": "Is combining classifiers with stacking better than selecting the best one?",
      "authors": [
        "S D≈æeroski"
      ],
      "year": "2004",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "17",
      "title": "Audio-visual feature selection and reduction for emotion classification",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2008",
      "venue": "Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP'08)"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "20",
      "title": "IEEE international conference on acoustics, speech and signal processing",
      "authors": [
        "Y Hoshen",
        "R Weiss",
        "K Wilson"
      ],
      "year": "2015",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition",
      "authors": [
        "A Ingale",
        "D Chaudhari"
      ],
      "year": "2012",
      "venue": "International Journal of Soft Computing and Engineering (IJSCE)"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "23",
      "title": "Learning a better representation of speech soundwaves using restricted boltzmann machines",
      "authors": [
        "N Jaitly",
        "G Hinton"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "25",
      "title": "Emotion classification from speech signal based on empirical mode decomposition and non-linear features",
      "authors": [
        "P Krishnan",
        "A Joseph Raj",
        "V Rajangam"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "26",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Kwon"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "27",
      "title": "Mlt-dnet: Speech emotion recognition using 1d dilated cnn based on multi-learning trick approach",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Real time multiple face recognition: A deep learning approach",
      "authors": [
        "S Mittal",
        "S Agarwal",
        "M Nigam"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 International Conference on Digital Medicine and Image Processing"
    },
    {
      "citation_id": "31",
      "title": "K-nearest neighbor classification",
      "authors": [
        "A Mucherino",
        "P Papajorgji",
        "P Pm"
      ],
      "year": "2009",
      "venue": "Data Mining in Agriculture"
    },
    {
      "citation_id": "32",
      "title": "Joint deep cross-domain transfer learning for emotion recognition",
      "authors": [
        "D Nguyen",
        "S Sridharan",
        "D Nguyen",
        "S Denman",
        "S Tran",
        "R Zeng",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "Joint deep cross-domain transfer learning for emotion recognition",
      "arxiv": "arXiv:arXiv:2003.11136"
    },
    {
      "citation_id": "33",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T √ñzseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "34",
      "title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks",
      "authors": [
        "D Palaz",
        "R Collobert",
        "M Doss"
      ],
      "year": "2013",
      "venue": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks",
      "arxiv": "arXiv:1304.1018"
    },
    {
      "citation_id": "35",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "36",
      "title": "Self-paced ensemble learning for speech and audio classification",
      "authors": [
        "N Ristea",
        "R Ionescu"
      ],
      "year": "2021",
      "venue": "Self-paced ensemble learning for speech and audio classification",
      "arxiv": "arXiv:arXiv:2103.11988"
    },
    {
      "citation_id": "37",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "L Rokach",
        "O Maimon",
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2015",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "38",
      "title": "Human speech emotion recognition",
      "authors": [
        "M Selvaraj",
        "R Bhuvana",
        "S Padmaja"
      ],
      "year": "2016",
      "venue": "International Journal of Engineering & Technology"
    },
    {
      "citation_id": "39",
      "title": "A lightweight 2d cnn based approach for speaker-independent emotion recognition from speech with new indian emotional speech corpora",
      "authors": [
        "Y Singh",
        "S Goel"
      ],
      "year": "2023",
      "venue": "A lightweight 2d cnn based approach for speaker-independent emotion recognition from speech with new indian emotional speech corpora"
    },
    {
      "citation_id": "40",
      "title": "A scale for the measurement of the psychological magnitude pitch",
      "authors": [
        "S Stevens",
        "J Volkmann",
        "E Newman"
      ],
      "year": "1937",
      "venue": "The journal of the acoustical society of america"
    },
    {
      "citation_id": "41",
      "title": "Speech emotion recognition using deep learning on audio recordings",
      "authors": [
        "S Suganya",
        "E Charles"
      ],
      "year": "2019",
      "venue": "19th International Conference on Advances in ICT for Emerging Regions"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition based on dnn-decision tree svm model",
      "authors": [
        "L Sun",
        "B Zou",
        "S Fu",
        "J Chen",
        "F Wang"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "43",
      "title": "Speaker recognition using mfcc and improved weighted vector quantization algorithm",
      "authors": [
        "C Sunitha",
        "E Chandra"
      ],
      "year": "2015",
      "venue": "International Journal of Engineering and Technology"
    },
    {
      "citation_id": "44",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "45",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "46",
      "title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr",
      "authors": [
        "Z T√ºske",
        "P Golik",
        "R Schl√ºter",
        "H Ney"
      ],
      "year": "2014",
      "venue": "Acoustic modeling with deep neural networks using raw time signal for lvcsr"
    },
    {
      "citation_id": "47",
      "title": "Speech emotion recognition-a deep learning approach",
      "authors": [
        "A V K"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition-a deep learning approach",
      "doi": "10.1109/i-smac52330.2021.9640995"
    },
    {
      "citation_id": "48",
      "title": "Naive bayes. Encyclopedia of Machine Learning",
      "authors": [
        "G Webb",
        "E Keogh"
      ],
      "year": "2010",
      "venue": "Naive bayes. Encyclopedia of Machine Learning"
    },
    {
      "citation_id": "49",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "50",
      "title": "Learning deep multimodal affective features for spontaneous speech emotion recognition",
      "authors": [
        "S Zhang",
        "X Tao",
        "Y Chuang",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "51",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "52",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T √ñzseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    }
  ]
}