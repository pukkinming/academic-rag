{
  "paper_id": "2508.19193v2",
  "title": "Emotions As Ambiguity-Aware Ordinal Representations",
  "published": "2025-08-26T16:55:11Z",
  "authors": [
    "Jingyao Wu",
    "Matthew Barthet",
    "David Melhart",
    "Georgios N. Yannakakis"
  ],
  "keywords": [
    "continuous emotion recognition",
    "affect modeling",
    "rater ambiguity",
    "ordinal emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions are inherently ambiguous and dynamic phenomena, yet existing continuous emotion recognition approaches either ignore their ambiguity or treat ambiguity as an independent and static variable over time. Motivated by this gap in the literature, in this paper we introduce ambiguityaware ordinal emotion representations, a novel framework that captures both the ambiguity present in emotion annotation and the inherent temporal dynamics of emotional traces. Specifically, we propose approaches that model emotion ambiguity through its rate of change. We evaluate our framework on two affective corpora-RECOLA and GameVibe-testing our proposed approaches on both bounded (arousal, valence) and unbounded (engagement) continuous traces. Our results demonstrate that ordinal representations outperform conventional ambiguity-aware models on unbounded labels, achieving the highest Concordance Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores, highlighting their effectiveness in modeling the traces' dynamics. For bounded traces, ordinal representations excel in SDA, revealing their superior ability to capture relative changes of annotated emotion traces.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Modeling Emotion Ambiguity",
      "text": "Ambiguity arises from the subjectivity of affective experiences  [18]  and their diverse manifestations across individuals 1 Although strictly speaking ordinal refers to non-measurable quantities with an underlying order (e.g., increase or decrease), in this paper we follow a second-order approach  [18] ,  [19]  and use ordinal to reflect the rate of change of the trace. and contexts. In time-continuous affect labeling, ambiguity is often reflected in divergent annotator traces. Recent work has emphasized the need to model ambiguity explicitly, recognizing that disagreement among annotators is informative and should be integrated into affect recognition models to better capture the complexity of emotional understanding  [11] -  [17] . To achieve this, increasing efforts have been made to represent ambiguity using probability distributions. Various distribution types have proven effective, including parametric approaches (e.g., Gaussian distribution  [11] , Gaussian Mixture Model  [12] , and Beta distribution  [13] ) and non-parametric methods  [14] . However, most existing studies view ambiguity as a function of emotion magnitude, overlooking the temporal dependencies in emotion and, thus, limiting the ability to fully capture its dynamic nature.\n\nAmong the few studies addressing temporal dependencies in emotion distributions, Dang et al.,  [12]  modeled emotion labels using Gaussian mixture models and incorporated Kalman filters to capture the temporal evolution of the distributions' parameters, assuming that temporal dynamics follow a linear dynamical system. Several works have also adopted Long Short-Term Memory (LSTM)-based systems to learn temporal dependencies of distribution parameters  [11] ,  [15] ,  [16] . Wu et al.  [14]  explored the prediction of time-varying emotion distributions using a nonlinear dynamical system with a sequential Monte Carlo approach. More recently, a system with Dual-Constrained Dynamical Neural Ordinary Differential Equations (CD-NODE γ ) has been proposed to explicitly model changes of distribution parameters using ODE functions with additional constraints, ensuring smooth transitions and the validity of the distributions  [17] .\n\nDespite the aforementioned efforts to model temporal dynamics of emotion, existing research primarily assumes that ambiguity across raters is independent with respect to time, neglecting the ordinal nature of emotion annotations-specifically, the consistency among raters in how emotions are annotated over time.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Ordinal Affect Modeling",
      "text": "Most current practices in soliciting time-continuous labels in affective computing predominantly rely on absolute ratings, while temporal dynamics and relative changes in ratings-key aspects of the relative ordinal nature of emotions-remain underexplored  [23] . Ordinal affect modeling explicitly acknowledges the relative nature of subjective emotional judgments, addressing the critical limitations of absolute ratingbased approaches. Notably, treating ratings as absolute values often amplifies the inherent perception ambiguity of annotation signals. This is because people struggle to consistently map their internal emotional experiences onto a fixed scale. This, in turn, does not match how our internal value assessment system works, leading to personal perceptual ambiguity. Human affective assessments are inherently relative and subject to contextual factors and different anchoring and recency effects  [18] ,  [24] ,  [25] . Therefore, ordinal models offer a more robust representation, aligning better with our own cognitive processes when we annotate affect. Representing subjective labels in an ordinal fashion yields increased label consistency and inter-rater reliability across different disciplines including market research  [26] , behavioral economics  [27]  and AC  [28] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Ambiguity-Aware Emotion Representation",
      "text": "As discussed earlier, the existing dominant approach treats traces (that are inherently time-continuous) as interval data without accounting for any temporal change. We name this approach Interval Representation and present it in Section III-A. In contrast, we detail a new paradigm for modeling ambiguity-aware affect, capturing both the inherent ambiguity and the ordinal nature of annotated traces. Our proposed types of Ordinal Representation are detailed in Section III-B. Since all representations considered are ambiguity-aware, for simplicity purposes, we omit the term in the remainder of the paper. For a conceptual visualization of the two representations refer to Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Interval Representation",
      "text": "An increasing effort has been made in modeling ambiguity using probability distribution representations  [12] -  [17] ,  [29] . Under this formulation, given a set of ground truth annotations y n = {y 1 n , y 2 n , . . . , y m n } from m annotators at a single time instance t n , for n ∈ [1, N ], the ambiguity-aware emotional state is assumed to be represented as the following probability distribution:\n\nwhere f (y; θ n ) denotes a probabilistic model of the emotion state parameterized by θ n ; P (y|y n ) represents the empirical distribution of ratings given the set of ground truth annotations y n from multiple raters at time t n ; θ n = {µ n , σ n } denotes the distribution parameters, where the central tendency µ n of the distribution corresponds to the dominant emotional state and the standard deviation σ n reflects its associated ambiguity at t n .\n\nIn scenarios where emotion annotations are time-continuous traces, annotations from neighboring frames are often concatenated to incorporate the temporal information to enhance the fitting of the distribution. The concatenated annotations, including those from the neighboring frames, are denoted as:\n\nwhere F represents the number of neighboring frames considered around the current time instance t n . Consequently, the final interval representation with the incorporation of neighboring information is given by:\n\nwhere f (y; θn ) denotes the probability distribution fitted with the set of concatenated ratings ỹn ; P (y|ỹ n ) represents the empirical distribution of ratings given ỹn at time t n . Note that we adopt the approach of concatenating neighboring frames in all experiments reported in this paper. Thus, for the sake of simplicity we disregard the tilde from all math notations; e.g., we use θ n instead of θn in the remainder of the paper. The distribution parameters can be obtained using maximum likelihood estimation.\n\nThe distributions used to fit the data can take various forms (e.g., Gaussian  [11] , Beta  [15] , non-parametric distribution  [14]  etc.); however, they are commonly represented using central tendency (µ) and standard deviation (σ). Thus, in this paper, we denote the Interval Representation as I = {θ n }, ∀n ∈ [1, N ], where θ n = {µ n , σ n }. Fig.  2a  depicts an example segment of arousal traces collected from the RECOLA dataset, with the associated ambiguity shown in the shaded area.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Ordinal Representation",
      "text": "The proposed ordinal representation accounts for both genuine disagreement at each moment and consistent trends in how emotions evolve across multiple annotators' perceptions. Specifically, we propose two types of ordinal representation: Individual Representation-denoted as O I -and Group Representation-denoted as O G -both of which are detailed in the remainder of this section.\n\n1) Individual Representation: While the I representation captures aspects of ambiguity, it overlooks any temporal information about the trace signal. To address this, O I aims to capture how an individual's perception of emotion evolves over time, as well as the variability across different individual annotators. It does so by first computing the rate of change (i.e., the trace's gradient) of each individual's trace, thereby capturing the unique temporal dynamics of each annotator. Then, it models the distribution of these gradients, enabling a population-level characterization of how different individuals annotate emotions.\n\nTo derive O I , we first compute the gradient of each annotator's (m) trace with respect to time, g m n , as follows:\n\nwhere y m n denotes the trace of annotator m at time t n and N is the total number of time steps. These gradients are computed across past and future samples to ensure temporal smoothness, as defined in Eq.  (5) .\n\nwhere x is a time variable (e.g., annotator's traces y m ). Then, we estimate a probability distribution f (g m n ; ϕ n ) over these gradients according to Eq.  (6) .\n\nwhere ϕ n denotes the distribution parameters; µ I n and σ I n represent the central tendency and spread of the gradient distribution at time t n , respectively. Finally, the individual representation is given as\n\nFigure  2b  shows the trace gradients of multiple annotators illustrating the µ I and σ I components of O I as derived from these gradients. The figure demonstrates how O I captures agreements (or disagreements) on arousal changes across multiple annotators. For instance, at around 24 seconds, the original traces appear to be flattened (see Fig.  2a ); evidently, gradients at that time window are close to zero for all annotators (see Fig.  2b ), thereby yielding a very narrow distribution and centered around zero for O I (i.e., both µ I and σ I are close to zero). In contrast, when individual gradients deviate substantially (e.g., between 5 and 10 seconds) the distribution to be modeled is wider.\n\n2) Group Representation: The alternative group ordinal representation aims to capture how the annotation distribution of the entire group of annotators changes over time. Specifically, we compute the gradients of the central tendency (µ) and the ambiguity (σ) over time. This approach allows us to capture changes in the examined emotional state and its associated ambiguity. These changes provide a smooth representation of the temporal dynamics of the trace, accounting for subtle fluctuations. Consequently, the group representation is given as\n\nwhere g G n are the temporal gradients of the distribution's parameters (e.g., mean and standard deviation) at time t n :\n\nwhere {µ n , σ n } are the distribution parameters estimated according to Eq. (  3 ); dµn dt and dσn dt denote, respectively, the rate of change of the distribution parameters µ n and σ n at time t n ; these gradients are computed via Eq. (  5 ).\n\nFigure  2c  illustrates the dµ dt and dσ dt components of O G . Comparing these traces to I reveals how this representation amplifies relative changes. Observing the signals of Fig.  2a  between 17 and 24 seconds for instance we note a rise of µ accompanied by a lower level of σ-followed by a flattening of µ and a definite increase in σ. In contrast in Fig.  2c (O G  ), the dµ dt trace rapidly rises and then falls close to zero as µ tapers off. Similarly dσ dt amplifies the temporal dynamics of the σ trace observed in Fig.  2a . Instead of a minor dip in values, with dσ dt we can observe a sharp decline, followed by a sharp increase encoding the rapidly changing levels of ambiguity before converging to values around zero (as the σ trace settles around a constant value).\n\nIV. DATASETS Among the corpora that contain time-continuous emotion labels, the RECOLA and GameVibe datasets are selected for the experimental analysis in this paper. The RECOLA dataset has been widely used in continuous emotion recognition tasks and has been particularly employed in many recent studies on ambiguity modeling  [12] ,  [14] -  [17] . The GameVibe dataset is a newly introduced corpus of particular interest due to the ordinal nature of its emotion annotations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Recola Corpus",
      "text": "The RECOLA dataset consists of 9.5 hours of spontaneous dyadic conversation recordings in French  [5] . The dataset is annotated by six human annotators with continuous arousal . The original ratings are sampled at a period of 40 ms, which are further aggregated into a window size of 3 seconds. A 4 second time offset is applied to each utterance for both arousal and valence to compensate for the annotation delay  [30] . The training and development sets each contain nine five-minute utterances, consistent with the data partition in the AVEC challenge 2015  [9] . Since the challenge test sets are not publicly available, the system is trained, and hyperparameters are optimized using the training set and tested on the development set, as per standard practice  [12] ,  [14] ,  [15] ,  [17] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Gamevibe Corpus",
      "text": "GameVibe  [4]  is a novel multimodal corpus consisting of 2 hours of gameplay video footage from 30 different firstperson shooter games, annotated for viewer engagement by 20 annotators. The dataset is separated into 4 sessions containing 30 unique one minute clips-a clip from each game. Each session is annotated by the same set of 5 annotators; clips within each session are presented in a random order. The video clips are sampled at 30 Hz with a resolution of 1280 × 720 for modern games and 541 × 650 for older games.\n\nAnnotations are collected using the RankTrace annotation tool  [31]  via the PAGAN  [32]  platform. Annotators watched the game clips and provided engagement labels in real time by scrolling up or down on a mouse wheel to indicate increases or decreases in their engagement level respectively. No recordings of the participants' faces or the annotation interface form part of the final corpus. The engagement ratings are unbounded and originally sampled at a period of 250 ms and further aggregated into 3-second time windows following previous studies  [33] ,  [34] . Some videos are slightly shorter than one minute. To maintain consistency, only the first 19 time windows are used, discarding any extras. One 10-window video is also excluded as an outlier due to its short length. Finally, the corpus is separated into train-validation splits using 10-fold cross-validation, resulting in a leave-3-gamesout protocol. This means that the training sets for each fold consist of 108 videos from 27 games, and the validation sets consist 12 videos from 3 games.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experimental Settings",
      "text": "In this section we integrate the proposed emotion representation into conventional continuous emotion recognition systems. We aim to investigate the impact of ordinal modeling on ambiguity-aware emotion recognition and assess whether representing emotions through their rate of change, rather than static values, enhances the system's ability to align more closely with human perception-capturing both ambiguity and temporal dynamics.\n\nA graphical representation of the framework is shown in Fig.  3 . The three different emotion representations detailed in Section III are tested as part of a CER system. This yields three types of model outputs as depicted in each box at the bottom of Fig.  3 : I, O G and O I . Within each CER system, emotional stimuli (speech or video) are first fed into a feature extractor, which in turn feeds an LSTM-based architecture for CER. This follows the structure of the backbone model adopted in several recent studies demonstrating effectiveness in affect prediction  [15] ,  [16] ,  [29] . As each emotion representation consists of two separate parameters (i.e., µ and σ), we train two parameterindependent CER systems to optimize learning for both (see the two outputs illustrated in the magenta boxes of Fig.  3 . The feature extraction methods and distributions selected, the model implementation settings employed, and the evaluation measures used are detailed in the remainder of this section.\n\nA. Feature Extractor 1) Audio Features: For emotion utterances from the RECOLA dataset, we adopt the Bag-of-Audio-Words (BoAW) features following state-of-the-art studies in ambiguity modeling in this dataset  [15] ,  [16] . The 20 extracted MFCCs and their deltas are first computed from the input speech utterances. Then, the audio words were determined as clusters in this space  [35] . The BoAW features employed in our experiments were generated using 100 clusters, yielding a 100-dimensional BoAW representation. Feature extraction was implemented using OpenXbow  [36] , a detailed explanation of which can be found in  [35] .\n\n2) Video Embeddings: Visual features are extracted from GameVibe's videos using the VideoMAEv2  [37] , a state-ofthe-art pre-trained video masked autoencoder which has been used successfully on this corpus in earlier work  [33] . Each video is processed as 3-second non-overlapping clips aligned with the time windows given for the annotation signals. We selected 16 frames from each of these clips by uniformly sampling across the three seconds to create a gif for each time window. We then used the base VideoMAEv2 model to extract embeddings for each gif, resulting in a latent vector of 768 values representing visual and temporal information across the time window. This resulted in a final training corpus of 2261 × 768 (time windows × embeddings).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Distribution Selection",
      "text": "We adopt Beta and Gaussian distributions to fit the distributions (I representation) for the annotations in RECOLA and GameVibe datasets, respectively. For the arousal and valence traces of RECOLA-which are originally bounded within the [-1, 1] range-we employ the Beta distribution given its suitability, compared to other commonly used distributions, in this dataset  [5] . The original annotations are first mapped to [0, 1] via a linear transformation in order to meet the requirement of Beta fit, following the approach described in  [13] . The annotations of neighboring frames F = 1 are concatenated for the estimation, according to Eq  (2) . For the engagement traces of GameVibe, we adopt the Gaussian distribution following the prior works  [38] ,  [39] . The distribution parameter values are set using the maximum likelihood estimation as described in Section III.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Emotion Model Settings",
      "text": "The LSTM module is a two-layer, unidirectional network that processes sequential input representations, followed by a linear transformation with a tanh activation function, applied at each time step, to generate the final outputs. For RECOLA, the training utterances are split into 100 frames in each batch to improve training efficiency and tested with the entire utterance to match the practical scenarios. For GameVibe, the videos are split into 19 time windows in each batch to match the corpus' video length. We use the Adam optimizer to train our LSTM models with an initial learning rate of 1e-3 and a weight decay of 1e-4. For RECOLA, the maximum number of iterations is set to 100, with early stopping based on the best loss performance on the test set. For GameVibe, the maximum number of iterations is set to 2,500 using the same early stopping method. Training is guided by the Concordance Correlation Coefficient (CCC) loss given as L ρ = 1-ρ, where ρ is the CCC value  [40] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Evaluation Measures",
      "text": "Emotion models are evaluated using both the CCC  [40]  and the Signed Differential Agreement (SDA)  [41]   is a conventional metric commonly used in continuous emotion recognition tasks. We selected SDA as a complementary metric to CCC because it is a pairwise measure of reliability between two signals, emphasizing the alignment of their directional trends rather than their magnitude. This property makes SDA particularly well-suited for assessing ordinal labels.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Results",
      "text": "In this section, we evaluate the impact of different emotion representations used in a traditional CER system by testing the proposed representations on the RECOLA and GameVibe datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Recola",
      "text": "The results presented in Table  I  show that the interval representation I yields the highest performance in terms of CCC values for both µ and σ predictions. However, the proposed O G and O I representations perform better when evaluated via the SDA metric. For instance, the µ predictions of arousal show that O G outperforms I with a relative increase of 17.67%, and O I outperforms it by 12.09%. A similar pattern is observed for the µ prediction of valence, where O I (0.221) and O G (0.219) achieves higher performances than I (0.130). This suggests that while I is more effective for capturing absolute emotion states, ordinal representations better reflect relative changes in emotion perception, as SDA specifically measures agreement in directional changes rather than magnitude. We do not observe consistent results for the σ predictions, likely due to the common challenges associated with predicting σ, as noted in earlier research  [11] ,  [17] .\n\nInterestingly, we also note that O G outperforms O I for arousal prediction (both µ and σ) in terms of both CCC and",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Gamevibe",
      "text": "Table  II  shows the results of our experiments on the GameVibe unbounded ordinal dataset. Because GameVibe data is unbounded and ordinal, engagement traces can virtually  range infinitely from any negative to any positive value. Looking at µ predictions, we can observe that O G outperforms the other types of emotion representations both in terms of CCC (0.250) and SDA (0.297). These findings confirm that the rate of change across multiple annotators is a more robust representation of emotion, offering greater reliability. Aggregating trends across multiple annotators reduces noise and enhances predictive accuracy-at least when it comes to the central tendency of the signal (µ). These results also benchmark the performances of performing regression tasks on this dataset -which have never done before.\n\nThe picture is not immediately clear, however, when we look at predictions of ambiguity (σ). While O G still offers the best representation in terms of CCC (0.173), the obtained SDA values reveal the opposite finding. Particularly, O G yields the lowest SDA value (0.112), while I demonstrates a staggering SDA value of 0.586, which indicates considerable agreement with the deviation existent in the ground truth. This occurs because, there is a tendency of unbounded human annotation traces to progressively diverge over time (see an example on Fig.  4 ). As a result, signal variations increase significantly, making it easier to predict the values of σ but leading to a loss of sensitivity in tracking changes in σ.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vii. Discussion & Conclusions",
      "text": "This paper introduced a novel paradigm for emotion representation we name Ambiguity-aware Ordinal Emotional Representation. This paradigm is demonstrated to reliably capture both the ambiguity and the temporal dynamics of emotion traces. We tested the proposed representations on two wellestablished datasets-RECOLA and GameVibe-using both bounded interval (arousal and valence) and unbounded ordinal (engagement ) continuous labels. Our results show that even though using the absolute values of time-continuous labels are beneficial when modeling bounded annotations-an ordinal representation is beneficial at modeling unbounded traces and estimating the temporal dynamics of the data both in terms of central tendency and ambiguity. Furthermore, our findings indicate that our proposed Group representation outperforms Individual representation, underscoring the importance of relying on global trends in modeling emotional ambiguity. By integrating these representations into conventional affect modeling systems, we demonstrate that detecting emotions through their relative temporal changes-rather than through their static magnitudes-enhances the model's ability to better align with human perception.\n\nWhile initial findings suggest that capturing ambiguity via ordinal representations is promising, there are a number of limitations related to our study. Most notably, we approach the modeling of the central tendency and the ambiguity of the annotation traces as a regression downstream task. Although continuous affect labels and emotional intensity are often modeled in such a way  [45] ,  [46] , this approach does not adhere to natural cognitive processes-which are inherently ordinal  [18] . Future work should focus on approaching the problem through alternative learning paradigms such as classification  [47]  and preference learning  [48] . Our results on GameVibe highlight the sensitivity of the models to global trends in the data. As unbounded labels tend to dissociate over time, future work should investigate the effect of data normalization on our results. Beyond the different normalization strategies we could employ, another way to increase the ecological validity of our results is to reproduce them across new datasets and modalities. While some limited experiments on building general affect models have already been reported in the literature  [48] ,  [49] , our results on Ambiguity-aware Ordinal Group Representation open new avenues for future research in this area. Further work in the future should also integrate and test ambiguity-aware ordinal emotion representation with alternative models beyond LSMTs such as transformer-based architectures.\n\nAdditionally, while ordinal representations are effective for modeling relative changes in emotion perception, they may be less suited for applications that require capturing absolute emotional intensity (e.g., identifying spikes of \"high arousal\" states). Furthermore, although we evaluated interval and ordinal representations separately, combining them as complementary inputs may offer a more holistic view--particularly by capturing both absolute levels and relative changes in emotions. Future work could explore hybrid architectures that jointly leverage the strengths of both representations to enhance continuous affect modeling, especially under ambiguous or subjective annotation conditions.\n\nIn summary, our study highlights the significance of temporal dynamics and inter-rater ambiguity in emotion modeling, and introduces a novel approach to emotion representation. Through a series of experiments, we showcased different methods of affect-aware emotion representations and demonstrated the strengths of ordinal approaches when it comes to estimating central trends and the changing ambiguity of annotations-paving the way for more robust and humanaligned affective computing systems.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Viii. Ethical Impact Statement",
      "text": "This work introduces a novel ambiguity-aware ordinal representation to model both the ambiguous and ordinal nature of emotions, emphasizing the role of temporal changes in affective computing. Unlike conventional methods that treat emotional states as static or independent variables, our approach leverages ordinal structures to better capture the progression and relative changes in emotion over time. By accounting for variations in emotion annotation and tracing how emotions evolve, our framework provides a more structured and interpretable representation of human emotions.\n\nThe ethical implications of modeling emotions-and the inherent challenges in capturing their ambiguous and ordinal nature-are significant, especially in applications related to mental health, human-computer interaction, and affective AI. Misrepresenting or oversimplifying emotional states can lead to misleading conclusions and unintended consequences. Our approach mitigates such risks by emphasizing relational and progression-based representations, which better reflect the way emotions are perceived and experienced.\n\nDespite these advantages, there are potential risks associated with deploying ordinal emotion models in real-world applications. We acknowledge that even with the proposed representation, our system is trained on given datasets collected from two specific group of annotators. This highlights the importance of ensuring that such systems are trained and evaluated across diverse populations to prevent reinforcing existing disparities or introducing unintended limitations. Therefore, it is essential to for improving the generalizability and fairness of affective computing models.\n\nAnother critical ethical consideration involves privacy concerns. Affective computing systems, including the methods proposed in this work, require emotion data collected from human participants, which may pose privacy and surveillance risks. To safeguard individuals' rights, it is essential to implement stringent privacy protections, including secure data collection, storage, and processing practices. Clear regulatory frameworks should be established to ensure ethical data usage, and users must be fully informed about the nature of the data being collected while providing explicit consent before participation.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: High-level conceptual visualization of ambiguity-aware interval",
      "page": 1
    },
    {
      "caption": "Figure 1: exemplifies the notion of",
      "page": 1
    },
    {
      "caption": "Figure 1: , although the traces",
      "page": 1
    },
    {
      "caption": "Figure 1: ) that merely consider relative changes—–i.e.,",
      "page": 2
    },
    {
      "caption": "Figure 1: ) may not capture effectively the underlying",
      "page": 2
    },
    {
      "caption": "Figure 1: A. Interval Representation",
      "page": 3
    },
    {
      "caption": "Figure 2: b shows the trace gradients of multiple annotators",
      "page": 3
    },
    {
      "caption": "Figure 2: a); evidently,",
      "page": 4
    },
    {
      "caption": "Figure 2: b), thereby yielding a very narrow distribution",
      "page": 4
    },
    {
      "caption": "Figure 2: c illustrates the",
      "page": 4
    },
    {
      "caption": "Figure 2: a. Instead of a minor dip in values,",
      "page": 4
    },
    {
      "caption": "Figure 2: a. The solid line and the",
      "page": 4
    },
    {
      "caption": "Figure 2: An example of the different emotion representations as employed",
      "page": 4
    },
    {
      "caption": "Figure 2: a) to increase or decrease. For the",
      "page": 4
    },
    {
      "caption": "Figure 3: The three different emotion representations detailed in",
      "page": 5
    },
    {
      "caption": "Figure 3: I, OG and OI. Within each CER system, emotional",
      "page": 5
    },
    {
      "caption": "Figure 3: The feature extraction methods and distributions selected, the",
      "page": 5
    },
    {
      "caption": "Figure 3: A visual representation of the methodology followed. Traces from the RECOLA [5] (arousal, valence) and GameVibe [4] (engagement) corpora",
      "page": 6
    },
    {
      "caption": "Figure 4: An example of signal dissociation over time between annotators in",
      "page": 7
    },
    {
      "caption": "Figure 4: ). As a result, signal variations increase significantly,",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000+\u0000L\u0000J\u0000K\u0000\u0003\u0000$\u0000P\u0000E\u0000L\u0000J\u0000X\u0000L\u0000W\u0000\\\n\u0000/\u0000R\u0000Z\u0000\u0003\u0000$\u0000P\u0000E\u0000L\u0000J\u0000X\u0000L\u0000W\u0000\\": "\u0000/\u0000R\u0000Z\u0000\u0003\u0000$\u0000P\u0000E\u0000L\u0000J\u0000X\u0000L\u0000W\u0000\\\n\u0000+\u0000L\u0000J\u0000K\u0000\u0003\u0000$\u0000P\u0000E\u0000L\u0000J\u0000X\u0000L\u0000W\u0000\\"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CER System": "Feature Embedding\nμ LSTM\nσ LSTM\nLSTM Layer\nLSTM Layer\nLSTM Layer\nLSTM Layer\nFC Layer\nFC Layer\nCentral\nAmbiguity\nσ\nμ\nTendency"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RECOLA": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "GameVibe"
        },
        {
          "Datasets": "Engagement"
        },
        {
          "Datasets": ""
        },
        {
          "Datasets": "Arousal / Valence"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "Basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "3",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "4",
      "title": "GameVibe: a multimodal affective game corpus",
      "authors": [
        "M Barthet",
        "M Kaselimi",
        "K Pinitas",
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2024",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "5",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "6",
      "title": "The ambiguous world of emotion representation",
      "authors": [
        "V Sethu",
        "E Provost",
        "J Epps",
        "C Busso",
        "N Cummins",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "The ambiguous world of emotion representation",
      "arxiv": "arXiv:1909.00360"
    },
    {
      "citation_id": "7",
      "title": "Automatic, dimensional and continuous emotion recognition",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "International Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "8",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "9",
      "title": "AVEC 2015: The 5th international audio/visual emotion challenge and workshop",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Dynamic multi-rater gaussian mixture regression incorporating temporal dependencies of emotion uncertainty using kalman filters",
      "authors": [
        "T Dang",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Parametric distributions to model numerical emotion labels",
      "authors": [
        "D Bose",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "14",
      "title": "A novel sequential Monte Carlo framework for predicting ambiguous emotion states",
      "authors": [
        "J Wu",
        "T Dang",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Continuous emotion ambiguity prediction: Modeling with beta distributions",
      "authors": [
        "D Bose",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Can modelling inter-rater ambiguity lead to noise-robust continuous emotion predictions?",
      "authors": [
        "Y.-T Wu",
        "J Wu",
        "V Sethu",
        "C.-C Lee"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech 2024"
    },
    {
      "citation_id": "17",
      "title": "Dual-constrained dynamical neural odes for ambiguity-aware continuous emotion prediction",
      "authors": [
        "J Wu",
        "T Dang",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2024",
      "venue": "Dual-constrained dynamical neural odes for ambiguity-aware continuous emotion prediction",
      "arxiv": "arXiv:2407.21344"
    },
    {
      "citation_id": "18",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing (Early Access)"
    },
    {
      "citation_id": "19",
      "title": "The ordinal nature of emotions",
      "year": "2017",
      "venue": "Proceedings of the Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "20",
      "title": "Indirect scaling methods for testing quantitative emotion theories",
      "authors": [
        "M Junge",
        "R Reisenzein"
      ],
      "year": "2013",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "21",
      "title": "The relativity of 'absolute'judgements",
      "authors": [
        "D Laming"
      ],
      "year": "1984",
      "venue": "British Journal of Mathematical and Statistical Psychology"
    },
    {
      "citation_id": "22",
      "title": "From interval to ordinal: A HMM based approach for emotion label conversion",
      "authors": [
        "J Wu",
        "T Dang",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "The impact of rewards and trait reward responsiveness on player motivation",
      "authors": [
        "C Phillips",
        "D Johnson",
        "M Klarkowski",
        "M White",
        "L Hides"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Symposium on Computer-Human Interaction in Play"
    },
    {
      "citation_id": "24",
      "title": "Descartes' error: Emotion, rationality and the human brain",
      "authors": [
        "A Damasio"
      ],
      "year": "1994",
      "venue": "Descartes' error: Emotion, rationality and the human brain"
    },
    {
      "citation_id": "25",
      "title": "Anchors, scales and the relative coding of value in the brain",
      "authors": [
        "B Seymour",
        "S Mcclure"
      ],
      "year": "2008",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "26",
      "title": "The relation between culture and response styles: Evidence from 19 countries",
      "authors": [
        "T Johnson",
        "P Kulesa",
        "Y Cho",
        "S Shavitt"
      ],
      "year": "2005",
      "venue": "Journal of Cross-cultural psychology"
    },
    {
      "citation_id": "27",
      "title": "The framing of decisions and the psychology of choice",
      "authors": [
        "A Tversky",
        "D Kahneman"
      ],
      "year": "1981",
      "venue": "Science"
    },
    {
      "citation_id": "28",
      "title": "A study on affect model validity: Nominal vs ordinal labels",
      "authors": [
        "D Melhart",
        "K Sfikas",
        "G Giannakakis",
        "G Liapis"
      ],
      "year": "2020",
      "venue": "Workshop on Artificial Intelligence in Affective Computing. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "29",
      "title": "Dynamic difficulty awareness training for continuous emotion prediction",
      "authors": [
        "Z Zhang",
        "J Han",
        "E Coutinho",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "An investigation of annotation delay compensation and outputassociative fusion for multimodal continuous emotion prediction",
      "authors": [
        "Z Huang",
        "T Dang",
        "N Cummins",
        "B Stasak",
        "P Le",
        "V Sethu",
        "J Epps"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "31",
      "title": "Ranktrace: Relative and unbounded affect annotation",
      "authors": [
        "P Lopes",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proceedings of the Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "32",
      "title": "PAGAN: Video affect annotation made easy",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "33",
      "title": "Varying the context to advance affect modelling: A study on game engagement prediction",
      "authors": [
        "K Pinitas",
        "N Rasajski",
        "M Barthet",
        "M Kaselimi",
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "34",
      "title": "Acrossgame engagement modelling via few-shot learning",
      "authors": [
        "K Pinitas",
        "K Makantasis",
        "G Yannakakis"
      ],
      "year": "2024",
      "venue": "Acrossgame engagement modelling via few-shot learning",
      "arxiv": "arXiv:2409.13002"
    },
    {
      "citation_id": "35",
      "title": "At the border of acoustics and linguistics: Bag-of-audio-words for the recognition of emotions in speech",
      "authors": [
        "M Schmitt",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "36",
      "title": "openxbow-introducing the passau opensource crossmodal bag-of-words toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "37",
      "title": "Videomae v2: Scaling video masked autoencoders with dual masking",
      "authors": [
        "L Wang",
        "B Huang",
        "Z Zhao",
        "Z Tong",
        "Y He",
        "Y Wang",
        "Y Wang",
        "Y Qiao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "38",
      "title": "Predicting the distribution of emotion perception: capturing inter-rater variability",
      "authors": [
        "B Zhang",
        "G Essl",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "39",
      "title": "Using gaussian processes with lstm neural networks to predict continuous-time, dimensional emotion in ambiguous speech",
      "authors": [
        "M Atcheson",
        "V Sethu",
        "J Epps"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "41",
      "title": "Fifty shades of green: Towards a robust measure of inter-annotator agreement for continuous signals",
      "authors": [
        "B Booth",
        "S Narayanan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 international conference on multimodal interaction"
    },
    {
      "citation_id": "42",
      "title": "Multimodal affect models: An investigation of relative salience of audio and visual cues for emotion prediction",
      "authors": [
        "J Wu",
        "T Dang",
        "V Sethu",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "43",
      "title": "Real-world automatic continuous affect recognition from audiovisual signals",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Multimodal behavior analysis in the wild"
    },
    {
      "citation_id": "44",
      "title": "Vocal expression and perception of emotion",
      "authors": [
        "J.-A Bachorowski"
      ],
      "year": "1999",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "45",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Moment-to-moment engagement prediction through the eyes of the observer: Pubg streaming on twitch",
      "authors": [
        "D Melhart",
        "D Gravina",
        "G Yannakakis"
      ],
      "year": "2020",
      "venue": "Proceedings of the 15th International Conference on the Foundations of Digital Games"
    },
    {
      "citation_id": "48",
      "title": "Towards general models of player experience: A study within genres",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "49",
      "title": "Towards general models of player affect",
      "authors": [
        "E Camilleri",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proceedings of the Intl. Conference on Affective Computing and Intelligent Interaction"
    }
  ]
}