{
  "paper_id": "2007.14071v1",
  "title": "Emotion Correlation Mining Through Deep Learning Models On Natural Language Text",
  "published": "2020-07-28T08:59:16Z",
  "authors": [
    "Xinzhi Wang",
    "Luyao Kou",
    "Vijayan Sugumaran",
    "Xiangfeng Luo",
    "Hui Zhang"
  ],
  "keywords": [
    "Affective computing",
    "deep neural networks",
    "emotion correlation mining",
    "emotion recognition",
    "natural language processing (NLP)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion analysis has been attracting researchers' attention. Most previous works in the artificial-intelligence field focus on recognizing emotion rather than mining the reason why emotions are not or wrongly recognized. The correlation among emotions contributes to the failure of emotion recognition. In this article, we try to fill the gap between emotion recognition and emotion correlation mining through natural language text from Web news. The correlation among emotions, expressed as the confusion and evolution of emotion, is primarily caused by human emotion cognitive bias. To mine emotion correlation from emotion recognition through text, three kinds of features and two deep neural-network models are presented. The emotion confusion law is extracted through an orthogonal basis. The emotion evolution law is evaluated from three perspectives: one-step shift, limited-step shifts, and shortest path transfer. The method is validated using three datasets: 1) the titles; 2) the bodies; and 3) the comments of news articles, covering both objective and subjective texts in varying lengths (long and short). The experimental results show that in subjective comments, emotions are easily mistaken as anger. Comments tend to arouse emotion circulations of love-anger and sadness-anger. In objective news, it is easy to recognize text emotion as love and cause fear-joy circulation. These findings could provide insights for applications regarding affective interaction, such as network public sentiment, social media communication, and human-computer interaction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION is complex, individualized, subjective, and sensitive to context. Emotion guides decision, prepares the body for action, and shapes the ongoing behavior  [1] . Philosophers tend to conclude that emotion is a subjective response to the objective world, which means emotion stems from the interaction between society and individuals. Individual emotion is complex in at least the following three aspects.\n\n1) Steady individual value is formed through long-term experience. Emotion response among individuals differs even in the same context. For instance, the Napoleonic War is disputed with two opposite views. The supporters hold the view that the war attacks the French feudal force and prompts historical progress, while the opponents think the war is unjustified due to its aggressive purpose. 2) Misunderstanding occurs when individuals communicate. The understanding of the context varies as individual prior backgrounds differ. The opinion of an individual becomes more profound when obtaining more knowledge on target events. The misunderstanding of initial emotion happens when there is a prior knowledge gap between the information sender and the receiver. 3) Individual emotion turbulence exists. The turbulence is affected by external instant negative or positive mood. Emotion changes along with instant conditions for the same event. For most individuals, it is a common phenomenon in daily life that external conditions influence internal emotions. For example, a sweet-sounding tweet can also be disturbing when one's work performance is judged negatively. On the one hand, the emotion of individuals is complex due to individualized long-term social experiences, interpersonal misunderstandings, and external instant mood influence. On the other hand, public emotion concerning the social event is complex because of the following reasons.\n\n1) The social event, with a set of aspects, is complex. The information about social events is released online, including all kinds of topics. Individualized netizens tend to pay attention to different aspects. The emotion of the readers may be diverse, giving different aspects of the same social event. If the reader is concerned about more than one aspect, then his/her words may carry more than one kind of emotion. 2) Public emotion is compound and diverse. There is still controversy on the classification of emotions in the research area of social psychology. One of the wellacknowledged categories is dividing human emotion into six categories, that is, love, joy, anger, sadness, fear, and surprise. Shaver et al.  [2]  employed a tree structure to describe these basic emotions. Ekman's theory, which was similar to Shaver's theory, included the controversial emotion surprise  [3] . This article classifies emotions into these six categories. The proposed method is also applicable when given other kinds of emotion categories. Emotions are correlated rather than independent, which contributes to the complexity of individual and public emotions. Emotion correlation mining can help analyze the individual and public emotions at least in the following applications:\n\n1) Public Sentiment Analysis: As Zhao et al.  [4]  pointed out, emotion variation contributes a lot to netizens' behavior comprehension and abnormal event detection in social media. 2) Social Media Communication: It is beneficial to generate low ambiguous messages that are empathetic to the information receiver for both news compilation and interpersonal communication. Emotion correlation mining can provide clues for the expression of the intended emotion. 3) Human-Computer Interaction (HCI): Emotions contribute to improve HCI, for example, social companion robots. Emotion is intuitive in providing robots clues to understand and predict behavior for humanistic reaction. The potential applications of emotion analysis have been attracting a lot of attention from researchers. However, most efforts have focused on emotion recognition while neglecting emotion correlation mining. This article tries to fill the gap between emotion recognition and emotion correlation mining. We propose an approach for emotion correlation mining, which includes extracting emotion confusion and evolution. The confusion of emotion refers to the distance among emotions, obtained through projecting the emotions to a space with appropriate dimensions. The evolution of emotion is analyzed to shed light on the developing direction of the network public sentiment. The mining process is based on the emotion recognition results of the deep learning classification models using text.\n\nThis article is organized as follows. Related works are reviewed in Section II. Section III presents the term definition, the potential error causes, and directions to mine emotion correlation. Sections IV and V provide a detailed description of the calculation method for emotion confusion and evolution, respectively. In Section VI, the experimental evaluation of the proposed approach is described. Finally, conclusions are drawn in Section VII.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion analysis has been drawing researchers' attention in recent years. Text emotion distribution learning  [5] ,  [6] , considered as one of the most important research areas, contributes to many applications. The research on emotion recognition  [7] ,  [8]  is opening up numerous opportunities pertaining to social media in terms of understanding users' preferences, habits, and their contents. The application of the subjective and emotional data from social media includes but is not limited to sentiment analysis  [9] ,  [10] ; sarcasm detection  [11] ; event dissemination  [12] ; user clustering  [13] ,  [14] ; and user behavior analysis  [15] . Some tasks of the applications are combined together, such as multitask assignment on sentiment classification and sarcasm detection, which employs the deep neural network in natural language processing (NLP) tasks  [16] .\n\nEmotion analysis from text is one of the hot topics in modern natural language understanding. Embedding and attention mechanisms help a lot with emotion recognition in deep learning methods. Continuous word representations, including word2vec  [17] , weighted word embedding  [18] , and the derivatives  [19]  denoted words with dense embeddings and provided new ideas for automatic feature mining. Later, different kinds of attention mechanisms and pretrained models were proposed. Wang et al.  [20]  proposed an embedded recursive neural network for improving emotion recognition. Barros et al.  [21]  introduced a personalized affective memory. In 2017, Vaswani et al.  [22]  proposed a new network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. With the capability of modeling bidirectional contexts, denoising autoencoding-based pretraining, such as BERT  [23] , and an autoregressive pretraining method, such as XLNet  [24] , achieved better performance than many other pretrained models on GLUE tasks. Some researchers solved the emotion recognition task through graph modeling  [25] , such as a capsule network. Popular natural language understanding models, such as long short-term memory (LSTM)  [26] , convolutional neural network (CNN)  [27] , recursive autoencoders  [28] , adversarial learning  [29] , and attention mechanism  [30] , have been applied for emotion analysis and classification tasks. Electroencephalography signals and facial expression sequences were also used for emotion recognition with deep learning models  [31] ,  [32] . More complex and classificationoriented deep learning models made it harder to understand the correlation among emotions even with remarkable recognition accuracy.\n\nEmotion analysis, as an important traditional branch of knowledge mining, is categorized into three levels, namely: word level, sentence level, and document level. In word level, emotion words were extracted mainly through three ways: 1) manual approach  [33] ; 2) dictionary-based approach  [34] ; and 3) corpus-based approach  [35] .  Strapparava and Valitutti [36]  developed WordNet affect through tagging a subset of synets with affective meanings in English WordNet (EWN). Staiano and Guerini  [37]  presented DepecheMood, an emotion lexicon produced automatically by harvesting social media data annotated with emotion scores. Then, Badaro et al.  [38]  provided EmoWordNet by expanding DepecheMood with the synonymy semantic relation from EWN. Emotion lexicons for different languages were developed. In SemEval 2018 Task 1: Affect in Tweets  [39] , labeled data from English, Arabic, and Spanish tweets are created for each task. Badaro et al.  [40]  achieved the best result in the SemEval 2018 emotion classification subtask for the Arabic language. Features that they used were word embeddings from AraVec, and emotion features extracted from ArSEL  [41]  and NRC emotion lexicon. In the sentence-level analysis, intrasentential and intersentential emotion consistency were explored  [42] . Qiu et al.  [43]  employed dependency grammar to describe relations for double propagation between features and opinions. Ganapathibhotla and Liu  [44]  adopted dependency grammar for the emotion analysis of comparative sentences. The conditional random fields (CRFs) method  [45]  was used as the sequence learning technique for extraction. A multitask multilabel (MTML) classification model was proposed to classify sentiment and topics concurrently  [46] . By doing this, the closely related tasks, that is, sentiment and topic classification have been improved. Machine-learning methods were widely used in both the sentence and document levels. Naive Bayesian  [47] , maximum entropy classification  [48] , graphical model  [49] , and pattern recognition methods  [50]  were employed frequently. Zhao et al.  [51]  explored the correlations among different microblogs for social event detection. Hu and Flaxman  [52]  provided multimodel sentiment analysis by combining visual analysis and NLP to predict various emotional states of the user in social media. Most of the previous works focused on recognizing emotions from text rather than why emotion was wrongly recognized.\n\nStudies on emotion or sentiment propagation provided clues on building correlation emotion. The common phenomena of spread of sentiment  [53]  have been found, including positive  [54]  and negative sentiments  [55] . Stieglitz and Dang-Xuan  [56]  explored the association between emotion and the user's information-sharing behavior. They found that emotionally charged Twitter messages tend to be retweeted with both higher quantity and speed in the social media setting. Some findings showed that negative sentiment may contribute to content diffusion more than positive sentiment in the news domains, such as the findings of Hansen et al.  [57] . Fan et al.  [58]  divided the sentiment into four categories differing from the previously oversimplified sentiment classification (e.g., polarity detection  [59] ) and revealed that anger was more likely to spread than joy, disgust, and sadness especially toward social problems via Weibo in China. In other non-news domains, the opposite conclusion may even hold due to the complexity of emotions. Ferrara and Yang  [53]  explored the dynamics of emotional contagion with a random sample of Twitter users. They pointed out that users were more likely to adopt positive emotions than negative emotions on Twitter. The propagation prediction of emotion or sentiment was built on the precise emotion or sentiment recognition, which still neglected mining the correlation of emotions.\n\nAll the above works improved the performance of emotion recognition. However, just as Wilson et al.  [60]  pointed out, a single text may contain multiple opinions. Parrott  [61]  demonstrated that human emotions were prototyped and complex. Most of the recent works just focus on recognizing the emotion expressed in text and emotion diffusion in social media. Little attention is paid to associate emotion calculation  [62]   based on the quantitative and engineering approach with interemotion correlation typically studied in psychology. Analysis of correlation among emotions caused by the complexity of emotion is few focused and few covered in the literature in computer science.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Model Framework",
      "text": "This article aims to mine the potential and meaningful correlation among emotions from Web news. The emotion classification models in Section III-C focus on discriminating the emotion orientation of input texts. Three features of datasets and two deep neural-network models are proposed to do that. The calculation result supports emotion correlation mining in Sections IV and V. The total framework is depicted in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Term Definitions",
      "text": "1) The terms \"human emotion,\" \"public emotion,\" and \"social event\" are often used in affective computing studies. While definitions of these terms may appear self-explanatory. In this article, they are defined as follows. a) Emotion is defined as a strong feeling deriving from one's circumstances, mood, or relationships with others in Oxford Dictionary  [63] . Emotion responses to significant internal and external event  [64] . This article divides human emotion into six categories as presented by Shaver et al.  [2] .\n\nb) Public emotion refers to the sum of the individuals' emotional states  [65] . The emotions of audiences may differ when they pay attention to different aspects of the same event. These various emotions constitute public emotion. c) Social event indicates all kinds of events that are published online. Common topics of a social event include health, government, education, business, entertainment, unusual events, etc. 2) Emotion correlation is described as emotion confusion and evolution. Several laws are concluded.\n\na) The confusion of emotion refers to \"distance\" among emotions. Absolute confusion of an emotion denotes the average probability measured by the distance that the emotion is confused with all other emotions. Relative confusion of the emotion is based on the relative distance. For example, if the distance between fear and surprise is shorter than that between fear and anger, it reveals that in terms of fear, the relative confusion degree of surprise is higher than that of anger. Fear and surprise are more likely to be confused. Basically, there is only one absolute confusion value for a given emotion, but multiple relative confusion values for that given emotion, one with each other emotion. b) The evolution of emotion refers to the emotion changes during the process of event propagation.\n\nMisjudgment of emotions is one of the important factors in the evolution of emotion. For example, a reader recognizes a text which contains love as joy, and then another netizen mistakes the text which contains joy as surprise. The above misjudgments contribute to the evolution of love-joy-surprise. c) The laws mentioned in this article stem from correlations among emotions. The interemotion correlation is summarized as several laws. The confusion and the evolution of emotion correspond to the emotion confusion law discussed in Section IV and the emotion evolution law in Section V, respectively. The emotion evolution law includes a misjudgment law of emotion and a circulation law of emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Error Analysis",
      "text": "As mentioned in the introduction, both emotions and social events are complex. Consequently, the essence of public emotions may cause calculation errors. Dataset and emotion classification models also contribute to the overall errors.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Errors Caused By Emotion Complexity:",
      "text": "The complexity is composed of two parts, that is, the diversity of emotions and the complexity of social events. The complexity of the problem will inevitably lead to errors without considering other factors. Generally, the emotion of individuals is complex and is impacted by individualized long-term social experiences, misunderstandings caused by prior background knowledge, and external instant mood influences. Furthermore, public emotion is compound and diverse. Social events are complex, which are characterized by a set of aspects. The complexity of an event has a positive correlation with the number of aspects. Hence, if a reviewer pays attention to more than one aspect, then his/her words may carry more than one kind of emotion.\n\n2) Errors Caused by Dataset: Text features, which represent text, are abstract basic language units. Characters (letters) form a single word, and words make up phrases, then phrases build sentences, paragraphs, and chapters. Character and word (explicit) are two features used frequently. In this article, implicit expression of words is employed as the third feature to reduce the sparsity caused by using words. These three kinds of features capture text information from different levels.\n\n1) Character: Letter in language. The basic feature of the text, whose number is limited and thus compact in the corpus. 2) Implicit Expression: Synonym tag of words from the synonymous dictionary. If several words are synonyms, they share the same synonym tag, extracted from the HIT synonymous dictionary [HIT IR-Lab Tongyici Cilin (Extended)]. 3) Explicit Expression: Word in language. The number of features in explicit expression is not less than that in implicit expression, as synonyms share the same tag in implicit expression. In a sufficiently large corpus, there are more words than synonyms and more synonyms than characters. Taking \"love\" and \"like\" for example, the character features are six letters \"l,\" \"o,\" \"v,\" \"e,\" \"i,\" and \"k.\" In implicit expression, the feature is the synonym tag for the reason that the two words are synonymous. It means that the synonym tag extracted from the HIT dictionary is employed to substitute the original two words. In explicit expression, the features are love and like, the semantics of which are dispersed compared to the implicit expression. Character, implicit, and explicit features carry distinguishable representative capability. Accordingly, the three features, which differ in efficiency and precision, are used to reduce the errors caused by the dataset.\n\n3) Errors Caused by Models: Misunderstandings often occur when humans try to figure out the emotion from the text. Similarly, the emotion classification results of models may differ, given the same input text as models' inner calculation logic varies. Considering the models as intelligent agents, models will make mistakes. To reduce the error caused by models, this article presents more than one model. Through the complementary advantages of the models, the results can be of higher authenticity and reliability. Besides, the decisions of the majority make sense even when the majority makes wrong decisions sometimes. We try to find the mistakes that most models make and try to conclude the common mistake laws. Two deep learning models are established to minimize the errors caused by a single model.\n\n4) Error Influence: The above three errors account for the deviation from the emotion correlation. Let the errors caused by emotion complexity, datasets, and models be denoted as ε 1 , ε 2 , and ε 3 , respectively. Suppose Y is the ground truth of the emotion, which should be obtained from text writers and is therefore unavailable. Y is marked by the public and is used to substitute Y. The details are shown in Section VI-A.\n\nAuthorized licensed use limited to: Tsinghua University. Downloaded on May 14,2020 at 02:28:47 UTC from IEEE Xplore. Restrictions apply. ε 1 stems from the human emotion cognitive bias and the complexity of social events, which is irreducible. Errors caused by datasets (ε 2 ) and models (ε 3 ) are considered to be minimized, as they are reducible. To do that, the voting mechanism is presented by using more than one model and by importing three kinds of input features, namely, character features, implicit expression, and explicit expression. The details are shown in Fig.  2 . The above measures are taken to estimate the confusion and evolution of target public emotion. By combining three features and two emotion classification models, this article tries to minimize the errors caused by the dataset (ε 2 ) and the model (ε 3 ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Emotion Classification Model",
      "text": "Here, two deep neural-network models, CNN-LSTM2 (M1) and CNN-LSTM2-STACK (M2), are employed for emotion recognition. In both models, the length of an input text can be either short or long. The output of the models is one of the six kinds of emotions, that is, love, joy, anger, sadness, fear, and surprise. The calculation process can be divided into three parts. M1 is constructed with Part I and Part II. M2 is constructed by adding an additional Part III to M1. The brief details of the three parts are described as follows. More details can be found in Appendix A.\n\n1) Part I (Feature Processing): Part I focuses on feature processing which transforms the original features into dense vector information. There are four operations: vector lookup; window sliding; convolutional calculation; and rectified linear units (ReLUs) activation.\n\n2) Part II (Emotion Calculation): Part II focuses on the emotion calculation. There are five operations, that is, LSTM calculation, dropout operation, average calculation, fully connected layer, and softmax. Recurrent neural networks (RNNs) are powerful for sequence processing tasks, such as text classification. As a variant of RNNs, an LSTM network is capable of classifying and predicting sequence when there are very long time lags of unknown size between important time steps. IV. CONFUSION OF EMOTION Let x 0 (s i |s j , F α , M β ) represent the number of the texts in a dataset with an input label (Y = s j ) and an output label ( Y = s i ), given a feature F α and a model M β . Let x (k) 0 ∈ R 6×6 be a matrix whose element is x 0 (s i |s j , F α , M β ). k is an integer ranging from 1 to 6, with α being an integer ranging from 1 to 3, and β being an integer ranging from 1 to 2.\n\nThen, the emotion correlation matrix x\n\n1 is obtained by applying normalization to each row of x (k) 0 . This matrix x (k) 1 ∈ R 6×6 comprises the proportion that emotions are recognized correctly and mistaken as other five emotions\n\n(\n\nThere are m 1 (six) kinds of feature-model combinations of choices in total. That means, for each dataset, there are m 1 corresponding emotion correlation matrices\n\nwhere X 1 is the set of matrices for the dataset X.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Emotion Principal Component Analysis",
      "text": "An emotion feature extraction method is applied on the emotion correlation matrix x (k)\n\n1,i ∈ R D is a column vector with dimensionality D. This method focuses on finding out an orthonormal basis with dimensionality D 1 (D 1 < D), onto which the variance of the projected data is maximized. The orthonormal basis, which is the most distinct direction of the space with dimensionality D 1 , stands for the emotions features.\n\nThe orthonormal basis is denoted as\n\n1 denotes the mean of the projected data, where x(k) 1 is the column vector mean. The variance of the projected data is calculated by the following equation:\n\nwhere is the covariance matrix corresponding to x (k) 1 . In order to obtain the maximal variance, that is, maximize v T  1 v 1 with respect to v 1 , the Lagrange multiplier is introduced and denoted as λ\n\nby setting the first derivative with respect to v 1 as 0, the condition of the maximal variance is given by\n\nwhich indicates that (v 1 , λ 1 ) is an eigenpair of . Left multiply by v T 1 and consider v T 1 v 1 = 1 which is the property of an orthonormal basis. The variance can be formulated as\n\nThe variance of the data projected onto v 1 is the maximum when v 1 is an eigenvector of corresponding to the maximal eigenvalue λ 1 . λ 1 is known as the first principal component in the principal component analysis (PCA)  [66] . The additional directions that maximize the projected variance are obtained by choosing the new directions that are orthonormal to those already available. The eigenvectors of the covariance matrix, [v 1 , . . . , v D 1 ], are the most distinct directions of the matrix, which can be demonstrated by induction.\n\nThe above discussion is the derivation process based on the maximum variance theory. Moreover, the orthonormal basis also conforms to the minimum error theory that minimizes the average projected cost, which can be developed likewise.\n\nA set of principal component vectors and corresponding weights are acquired through the emotion feature extraction method. The principal component vectors refer to a standard orthonormal basis, the calculation of which observes two principles: 1) project the primary data points onto the vectors such that the projected data variance is maximized and 2) the average distance between the primary data points and their projections is minimized. Principal component vectors represent the most distinct directions of the emotion correlation matrix. Emotion PCA is mainly used to obtain these vectors to represent the emotional features. Then, distances (confusion degree) between emotions can be calculated.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Emotion Confusion Feature Extraction",
      "text": "Each emotion correlation matrix stands for a perspective to estimate emotion correlation. This section adds the matrix x (m 1 +1) 1 equaling the mean of x (1)  1 , . . . , x (m 1 ) 1 to the matrix X 1 in (2). Accordingly, there are m 1 + 1 points of view presented to explore the confusion of emotion.\n\nEmotion is divided into m 0 (six) categories. For an emotion correlation matrix x\n\nIn practice, the first m 2 vectors are adopted. The decrease of the dataset dimensionality leads to information loss. The cumulative variance contribution of the first m 2 vectors represents the proportion of retained information in the original dataset. Then, the weighted average of the first m 2 vectors is given by\n\nThe feature of the dataset with m 1 kinds of emotion correlation matrix is denoted as X 2 = [v (1) , v (2) , . . . , v (m 1 +1) ] T that can also be represented by the column vectors\n\nwhere e g is the gth column vector, and g ranges from 0 to m 0 -1.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Confusion Degree",
      "text": "Let e g indicate the feature of emotion s g . The confusion degree refers to distance among emotions, which is obtained from subtraction of the corresponding columns. The closer distance represents a higher level of confusion. Subtract e 0 , . . . , e m 0 -1 from e g . Then, the emotion s g centered distance matrix are obtained and represented as\n\nwhere\n\ng 3 is a column vector. The elements of d g i represent the distance between emotion s i and emotion s g , from m 1 + 1 points of view. d g g equals 0. 1) Absolute Confusion Degree: Absolute confusion degree represents the likelihood that an emotion is confused with other emotions. An increase in the confusion degree of emotion s g with any other emotions will enhance the absolute confusion degree of s g\n\nwhere the larger A g means a farther distance between s g and all other emotions, that is, lower absolute confusion degree of s g .\n\n2) Relative Confusion Degree: For a matrix X g 3 , the six elements in each row are sorted in ascending order, that is,\n\nwhere the element of the first column (se g 0 ) indicates emotion s g . The second column's (se g 1 ) element denotes the emotion that has the maximal confusion degree with emotion s g , the first column' elements. Conversely, the last column's (se g m 0 -1 ) element, indicates the emotion that has the minimal confusion degree with emotion s g , the first column' elements.\n\nFor the sequence matrix X g 4 , if all the elements in a column are identical, it means that the m 1 + 1 perspectives are the same. A reliable result is obtained. That is, the more the identical number of the elements in a column, the more reliable the result will be. The introduction of information entropy eliminates untrustworthy results\n\nwhere p i represents the proportion of a emotion in a column. Entropy indicates the degree of chaos. The lower information entropy refers to a higher consistency, that is, a more reliable result. Consequently, the columns of X g 4 , whose information entropy is below the average, are extracted and analyzed.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Evolution Of Emotion",
      "text": "Emotion confusion refers to the distance among emotions resulting from mutual misjudgment between two emotions. The confusion of emotion is bidirectional and, therefore, nondirectional. The evolution of emotion is directional, focusing on emotion shift during social events propagation. Emotion evolution can be evaluated from the following aspects.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "X (K)",
      "text": "1 indicates the emotion correlation matrix mentioned in Section IV. Let prob n (s i , s j ) represent the probability that s i is recognized as s j after n shift steps. The corresponding path is denoted as trace n (s i , s j ). Every shift step corresponds to a misjudgment between two emotions. prob n (s i , s j ) is the multiplication of misjudgment probability of every shift step.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. One-Step Shift",
      "text": "The top misunderstanding emotion pair is figured out given an emotion s i . According to the matrix x (k) 1 , compare prob 1 (s i , s j ) in terms of each emotion s j . Then, the top pair is the two emotions with the maximal prob 1 (s i , s j ). The trace is denoted as trace 1 (s i ,",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Limited-Step Shifts",
      "text": "Under three conditions, that is, given initial emotion, given ultimate emotion, and given initial and ultimate emotion, the emotion evolution within limited steps is observed.\n\nTo extract the most possible shift path, the probability product prob n (s i , s j ) is calculated and compared. However, the product may close to zero after n shift steps. The logarithm of the matrix x (k) 1 is therefore employed. Specifically, the logarithm of the misjudgment probability for each shift step is summed and used, which is positively correlated with prob n (s i , s j ).\n\nLet the initial and ultimate emotions be denoted as s ini and s ult , respectively. When s ini or s ult is given, there are n m 0 -1 kinds of potential shift paths in n shift steps with the existence of misjudgments. When s ini and s ult are both given, there are n m 0 -2 kinds of potential shift paths. Under the above three conditions, the shift path with the maximal probability is extracted. In other words, at every shift step, the most possible emotion that is misjudged with the former emotion are selected for the emotion flow. The above process is expressed as follows: max log prob n s ini , s j |j = 0, . . . , m 0 -1  (14)  max log prob n (s i , s ult )|i = 0, . . . , m 0 -1 (15) max log prob n (s ini , s ult ) .\n\n(16)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Shortest Path Transfer",
      "text": "Given the initial and ultimate emotion, the shortest path between two emotions is observed. The shortest path from emotion s ini to s ult is either one or n steps through other emotions. Specifically, the trace with the maximal probability of misjudgments among one to n shift steps is the shortest one max max log prob n (s ini , s ult ) |n ∈ N * .\n\n(\n\nVI. EXPERIMENTS",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets",
      "text": "The datasets are collected from one of the most popular social network platforms, news channel (http://news.sina.com.cn/society/moodrank/). Each news item is split into three parts: 1) the news comment; 2) the Authorized licensed use limited to: Tsinghua University. Downloaded on May 14,2020 at 02:28:47 UTC from IEEE Xplore. Restrictions apply. Fig.  3 . Absolute confusion degree of emotion. The darker/lighter the block, the longer/shorter distance from the targeting emotion to other emotions, the lower/higher the absolute confusion value (A g ). Anger in text is the most unlikely emotion to be confused with others in comment, love in text is the most unlikely emotion to be confused with others in news body, and fear in text is the most unlikely emotion to be confused with others in news title. Fig.  4 . The sequence matrix of comment. 0: love, 1: fear, 2: joy, 3: sadness, 4: surprise and 5: anger. x axes means the sequence from one to six. y axes denote seven different points of views. The confusion degree decreases along with the x-coordinate from two to six. The second and the sixth column refer to the emotion of maximal and minimal confusion degree with the emotion in the first column, respectively. Taking (b) comment_fear for example. The first row of this matrix shows that 4 surprise is most likely to be confused (column 2) with 1 fear from the first point of view. Conversely, 5 anger is least likely (column 6) to be confused with 1 fear from the first point of view.\n\nnews body; and 3) the news title, where the comment is treated as subjective text, and the news body and title are regarded as objective text. The emotion labels of the three datasets are generated through the vote of the public, strong rules, and manual selection. The public who read the news can vote for the news information on six emotions. First, only the data with total votes more than 200 are counted. Second, we label the data with emotion whose vote ratio count for more than 50% and discard data that do not meet the requirement. Take data with vote 50(love), 50(joy), 50(anger), 50(sadness), 50(fear), and 260(surprise) for instance, the data are labeled as surprise. If the vote of surprise is 240, the data are then discarded. Third, we check the data to see if the data are abnormal. The distribution of emotions in the three datasets is shown in Appendix B Table  I . The comments contain more than 150 000 sample texts. The news bodies and news titles both contain more than 24 000 sample texts. The performance of the two models with the three features  on the testing data, are illustrated in Appendix B Table  II . The details of the models are analyzed by Wang et al.  [62] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Confusion Of Emotion Result",
      "text": "Considering the three kinds of features (explicit expression, implicit expression, and character) and the two models, there are six (three times two) analytic logic for each dataset. The average of the six-emotion correlation matrix is also considered. That means one dataset is evaluated from seven perspectives. The six categories of emotions are adopted, that is, love, fear, joy, sadness, surprise, and anger. Specifically, m 0 and m 1 both equal six. C, B, and T indicate the comment, news body, and title datasets, which means X 1 = (C 1 , B 1 , T 1 ), and x\n\nThe first four principal component vectors are adopted, as the variance contribution of which is greater than 85% and approaches 90%. Then, (  7 )-(  9 ) are applied for the emotional features and absolute differences of the three datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "1) Absolute Confusion Degree:",
      "text": "The absolute confusion degree is obtained through  (10) . The result is shown in Fig.  3 . In the comments dataset, the absolute confusion degree of Authorized licensed use limited to: Tsinghua University. Downloaded on May 14,2020 at 02:28:47 UTC from IEEE Xplore. Restrictions apply.  anger is the lowest among the six kinds of emotions while that of love is the highest. It follows that anger is unlikely to be confused with other emotions. Absolute confusion degree Fig.  10 . Circulation law of emotion under three conditions. 0: love, 1: fear, 2: joy, 3: sadness, 4: surprise, and 5: anger. of emotion love is proportional to two elements, for example, the probability that love is mistaken as other emotions and other emotions are misjudged as love. The increase of either element would enhance the absolute confusion degree of emotion love. In the news body dataset, the absolute confusion degree of love is the lowest while that of joy is the highest. In the news title dataset, fear has the lowest absolute confusion degree while surprise has the highest one. 2) Relative Confusion Degree: According to  (11) , the sequence matrix of the three datasets is shown in Figs.  4 5 6 . In Fig.  4 , the sixth column elements of fear-centered matrix are consistent, which means anger is least likely to be confused with fear in the comments from seven different perspectives. On the contrary, in Fig.  5 , the sixth column of the fear-centered matrix is highly unordered, the reliability of which is low.\n\nFig.  7  shows the information entropy of the second and the last column which correspond to the emotion of the maximal and the minimal confusion degree, respectively. The mean of the information entropy decreases in the order: news body, title, and comment, stemming from the complexity of the long text in the news body and the difference in objective and subjective texts. The columns of the sequence matrix with information entropy below the average value exhibit more reliable results, which are further analyzed. For example, in the comments, the emotions with the maximal confusion degree with fear, sadness, and anger are extracted. The emotions of the minimal confusion with fear, joy, and surprise attract our attention. The confusion law is shown in Fig.  8 .\n\nThe relative confusion degree differs for comment, news body, and title, due to different traits of datasets. In the comments, anger is the emotion of the minimal confusion degree with fear, joy, and surprise. Conversely, surprise, love, and sadness have the maximal confusion degree with fear, sadness, and anger respectively. In the news bodies, the emotion confusion law is more dispersed. Love and fear are most unlikely to be confused with each other as the two emotions are on the diagonal symmetry. Sadness and anger are easy to be confused mutually. The long text of news body, which carries multiple information, contributes to the complexity of the emotional confusion. In the news titles, the models are unlikely to confuse joy and love while they tend to confuse anger and love. Joy has the minimal confusion degree with love, sadness, and surprise. Surprise is the emotion that is most likely to be confused with fear.\n\nFor the three datasets, anger and love are closely correlated in either a direct way (i.e., in the news titles) or an indirect way (i.e., in the comments and news bodies). In the news titles, the two emotions tend to be confused mutually. In the comments and news bodies, love is the emotion that has the maximal confusion degree with sadness, and sadness is the emotion that has the maximal confusion degree with anger. Anger and love are therefore associated indirectly. For example, the news title \"more than 100 people were killed in the terrorist attack,\" which may arouse the mixture public emotions of anger and love.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Evolution Of Emotion Result",
      "text": "1) Misjudgment of Emotion: According to  (13)  in Section V-A, the top misunderstanding emotion pairs approved by two or more emotion correlation matrices are illustrated in Fig.  9 . For example, in the comments, one-third of the matrices are likely to misinterpret fear as anger, the average probability of which is 0.095. One-third of the matrices recognize fear as joy with the same probability.\n\nOverall, comments can be easily misjudged as anger, especially the texts that involve sadness and love. Conversely, anger is unlikely to be mistaken in the comments. In news body and title, the models are likely to be confused between fear and joy. The texts that cause sadness, surprise, and anger are easy to be recognized as love incorrectly.\n\n2) Circulation of Emotion: The three conditions mentioned in Section V-B are considered for the most-likely traces in the evolution of emotion. Eight emotion shift steps are employed for the three conditions considering the validity and efficiency of computation. The most-likely traces include the phenomenon of emotion circulation under the above three conditions. Moreover, the emotion circulations that are extracted from an emotion correlation matrix x (k) 1 are the same, even though the original condition differs. The circulation law of emotion under three conditions, by (  14 )-(  16 ), can therefore be concluded as Fig.  10 .\n\nIn the comments, half the traces of the maximal probability contain the circulation of love and anger, sadness, and anger. Take love and anger as an example, in computational terms, the circulations represent that prob 1 (0, 5) * prob 1  (5, 0)  are larger than other probability products. In emotional terms, comments that cause love and anger are easy to be confused mutually.\n\nAuthorized licensed use limited to: Tsinghua University. Downloaded on May 14,2020 at 02:28:47 UTC from IEEE Xplore. Restrictions apply. The circulations result from the higher probability that love and sadness are misinterpreted as anger in Section VI-C1. Two cases are shown in Fig.  11 . In the objective texts, especially in the news titles, there are the circulations of fear and joy, which satisfies the analysis in Section VI-C1.\n\n3) Shortest Path of Emotion Transfer: Equation (  17 ) in Section V-C is applied for the shortest path between two emotions. The results reveal that most shortest paths are one step. The shortest path can be two steps when prob 1 (s ini , s ult ) is equal or near to zero shown in Fig.  12 .\n\nLet Imp_M 2 represent the emotion correlation matrix obtained through the analysis of implicit expression by the second model. Take the news title as an example. As prob 1  (1, 4)  is equal to zero, the shortest path between fear and surprise in Imp_M 2 is established with joy, formulated as trace 2 (1, 4) = 1 → 2 → 4. Besides, as prob 1 (4, 1) is close to zero in Char_M 1 , the shortest path is trace 2 (4, 1) = 4 → 2 → 1. The horizontal lines represent that it is possible to misinterpret surprise as fear, that is, prob 1 (4, 1) is close to zero rather than equaling to zero. Most matrices support that the shortest path is one step. There exists the shortest path of two-shift steps between two emotions under some circumstances.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "Mining the emotion correlation is significant for tracking emotional development. The complexity of emotion and event make emotion recognition hard for both subjective and objective information. The contributions of this article are concluded as follows.\n\n1) This article mines the correlation of emotions based on the emotion recognition result of state-of-the-art deep learning models.\n\n2) The errors caused by the dataset and models are cut down by designing three kinds of features and two deep neural-network models.\n\n3) The emotion correlation is mined through an emotion confusion law, which is undirected, and an emotion evolution law, which is directed. Experiments on subjective-objective and long-short Web news texts are conducted. There are several promising discoveries of emotion correlations.\n\n1) Emotion correlation differs for different types of datasets. 2) In subjective comments, the absolute confusion degree of anger is low for the reason that anger is unlikely to be mistaken, even though all other emotions are easy to be recognized as anger. In objective texts, some emotions tend to be misinterpreted as love. 3) Comments arouse the emotion circulations of love and anger, and sadness and anger. In news body and title, fear and joy are the repeated emotions. 4) For specific datasets, some emotions are hard to be confused through one shift, but with higher probability to be confused through two shifts. This phenomenon conforms to the diversity of public emotion. The emotion correlation (confusion and evolution law) is potentially applicable to a wide variety of situations and events, such as public sentiment analysis, social media communication, and HCI. This article employs Sina News in Chinese as a case study. Note that diverse culture leads to different results. However, the methods presented for exploring emotion correlation are applicable in various languages and domains.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Appendix A Emotion Classification Model",
      "text": "Here, two deep neural-network models, CNN-LSTM2 (M1) and CNN-LSTM2-STACK (M2), are employed for emotion recognition. In both models, the length of an input text can be either short or long. The output of the models is one of the six kinds of emotions, that is, love, joy, anger, sadness, fear, and surprise. The calculation process can be divided into three parts. M1 is constructed with Part I and Part II. M2 is constructed by adding an additional Part III to M1.",
      "page_start": 5,
      "page_end": 11
    },
    {
      "section_name": "A. Part I (Feature Processing)",
      "text": "Part I focuses on feature processing which transforms the original features into dense vector information. There are four operations in this part: 1) vector lookup; 2) window sliding; 3) convolutional calculation; and 4) ReLUs activation.\n\nLet the input be denoted as w (j) i , which means the ith feature of the jth sample text. The jth sample text is indicated as [w\n\nwhere the text is padded by \"none\" to length of N. Here, none is the reserved symbol in the vocabulary. For instance, if the first sample text is \"I like small cat\" and N = 5, then w\n\nThe second operation, window sliding, packages a target input feature and its context together after none padding. Specifically, the window size is set to 5, and w   (19)  where h (j) i,1 is the first hidden layer. The introduction of hidden layers contributes to no-linear behavior exhibited by the network. [•] means embedding concatenation. In the last operation, a ReLU activation layer is added. ReLU is one of the most commonly used activation functions, which has strengths for vanishing gradient problem, higher computation, and convergence speed h (j) i,2 = ReLU h (j) i,1 ≈ log h (j) i,1  (20)  where h (j) i,2 is the second hidden layer of the ith feature of the jth sample. h (j) i,2 acts as the input for Part II.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. Part Ii (Emotion Calculation)",
      "text": "Part II focuses on the emotion calculation after feature processing in Part I. There are five operations in this part, that is, LSTM calculation, dropout operation, average calculation, fully connected layer, and softmax. RNNs are powerful for sequence processing tasks, such as text classification. First, the output of Part I (h  2 ) is fed into a two-layer LSTM component, the outputs of which are represented as h    (23)  where i is the index of the text sequence, which is an integer ranging from 1 to N.\n\nIn practice, even though the texts have been padded to the same length of N, their actual lengths still vary. To settle this problem, ms (j) i ∈ {0, 1} is defined as the mask. The sequence data are combined to a fixed-length vector h (j)\n\nIf the ith feature of the jth sample text is valid (i.e., not none), then ms\n\nwhere W and b are the weight matrix and bias, respectively. l is the emotion index, which refers to one of the six emotions. ŷ(j) means the predicted probability distribution of six categories of emotions for the jth input data. Corresponding to the modelpredicted label ( Y), ŷ(j) l denotes the predicted probability of emotion l for the jth input data. Corresponding to the model input label (Y), y (29)\n\nThen, the layer h .  (30)  The other operations in Part II remain the same. Part III aims to emphasize the impact of the input feature embedding on the emotion calculation result. In other words, by stacking Part III, the network pays more attention to the original feature information.\n\nThe two models CNN-LSTM2 and CNN-LSTM2-STACK use deep neural-network methods to calculate text emotion. The detailed description are demonstrated by Wang et al.  [62] .\n\nIn our model design, both long and short texts can act as inputs. Three parts are introduced, including feature processing, emotion calculation, and original feature attention. The details are shown in Fig.  2 . The emotion calculation results of the two models with three text features support the inter emotion correlation analysis.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of emotion correlation mining. 0: love, 1: fear, 2: joy, 3:",
      "page": 3
    },
    {
      "caption": "Figure 1: A. Term Deﬁnitions",
      "page": 3
    },
    {
      "caption": "Figure 2: Measures taken to minimize the errors of emotion classiﬁcation and interemotion correlation.",
      "page": 5
    },
    {
      "caption": "Figure 1: On the one hand, emotion complexity ε1 leads to",
      "page": 5
    },
    {
      "caption": "Figure 2: The above measures are taken to estimate the",
      "page": 5
    },
    {
      "caption": "Figure 2: The emotion calculation results of the two models",
      "page": 5
    },
    {
      "caption": "Figure 3: Absolute confusion degree of emotion. The darker/lighter the block,",
      "page": 8
    },
    {
      "caption": "Figure 4: The sequence matrix of comment. 0: love, 1: fear, 2: joy, 3: sadness,",
      "page": 8
    },
    {
      "caption": "Figure 5: The sequence matrix of news body. Refer to the title of Fig. 4.",
      "page": 8
    },
    {
      "caption": "Figure 6: The sequence matrix of news title. Refer to the title of Fig. 4.",
      "page": 8
    },
    {
      "caption": "Figure 3: In the comments dataset, the absolute confusion degree of",
      "page": 8
    },
    {
      "caption": "Figure 7: Information entropy of the second and last column in the sequence matrices Xg",
      "page": 9
    },
    {
      "caption": "Figure 8: Relative confusion degree of emotion. The white blocks represent the maximal confusion degree while the black blocks indicate the minimal degree.",
      "page": 9
    },
    {
      "caption": "Figure 9: Misjudgment law of emotion. 0: love, 1: fear, 2: joy, 3: sadness, 4:",
      "page": 9
    },
    {
      "caption": "Figure 10: Circulation law of emotion under three conditions. 0: love, 1: fear,",
      "page": 9
    },
    {
      "caption": "Figure 11: Comment circulations of social news. 0: love, 3: sadness, and 5: anger. (a) Sadness–anger circulation in the comments. (b) Love–anger circulation",
      "page": 10
    },
    {
      "caption": "Figure 4: , the sixth column elements of fear-centered matrix are",
      "page": 10
    },
    {
      "caption": "Figure 5: , the sixth column of the fear-centered",
      "page": 10
    },
    {
      "caption": "Figure 7: shows the information entropy of the second and",
      "page": 10
    },
    {
      "caption": "Figure 8: The relative confusion degree differs for comment, news",
      "page": 10
    },
    {
      "caption": "Figure 9: For example, in the comments, one-third of the matrices",
      "page": 10
    },
    {
      "caption": "Figure 10: In the comments, half the traces of the maximal probability",
      "page": 10
    },
    {
      "caption": "Figure 12: The shortest path of two-steps shift rather than one-step. 0: love,",
      "page": 11
    },
    {
      "caption": "Figure 11: In the objective texts, especially",
      "page": 11
    },
    {
      "caption": "Figure 12: Let Imp_M2 represent the emotion correlation matrix",
      "page": 11
    },
    {
      "caption": "Figure 2: The emotion calculation results",
      "page": 13
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "The duality of everyday life: Dual-process and dual system models in social psychology",
      "authors": [
        "F Strack",
        "R Deutsch"
      ],
      "year": "2015",
      "venue": "APA Handbook of Personality and Social Psychology"
    },
    {
      "citation_id": "2",
      "title": "Emotion knowledge: Further exploration of a prototype approach",
      "authors": [
        "P Shaver",
        "J Schwartz",
        "D Kirson",
        "C O'connor"
      ],
      "year": "2001",
      "venue": "Emotions Soc. Psychol. Essential Read"
    },
    {
      "citation_id": "3",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cogn. Emotion"
    },
    {
      "citation_id": "4",
      "title": "MoodLens: An emoticon-based sentiment analysis system for Chinese tweets",
      "authors": [
        "J Zhao",
        "L Dong",
        "J Wu",
        "K Xu"
      ],
      "year": "2012",
      "venue": "Proc. 18th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min"
    },
    {
      "citation_id": "5",
      "title": "Learning unsupervised semantic document representation for fine-grained aspect-based sentiment analysis",
      "authors": [
        "H.-M Fu",
        "P.-J Cheng"
      ],
      "year": "2019",
      "venue": "Proc. ACM 42nd Int. SIGIR Conf. Res. Develop. Inf. Retrieval"
    },
    {
      "citation_id": "6",
      "title": "Text emotion distribution learning from small sample: A meta-learning approach",
      "authors": [
        "Z Zhao",
        "X Ma"
      ],
      "year": "2019",
      "venue": "Proc. Conf. Empirical Methods Nat. Lang. Process. 9th Int. Joint Conf. Nat. Lang. Process. (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "7",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "8",
      "title": "New avenues in opinion mining and sentiment analysis",
      "authors": [
        "E Cambria",
        "B Schuller",
        "Y Xia",
        "C Havasi"
      ],
      "year": "2013",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "9",
      "title": "A neural network model for social-aware recommendation",
      "authors": [
        "X Lin",
        "M Zhang",
        "Y Liu",
        "S Ma"
      ],
      "year": "2017",
      "venue": "Proc. Asia Inf. Retrieval Symp"
    },
    {
      "citation_id": "10",
      "title": "Sentiment processing of social media information from both wireless and wired network",
      "authors": [
        "X Wang",
        "Z Hui",
        "S Yuan",
        "J Wang",
        "Z Yang"
      ],
      "year": "2016",
      "venue": "EURASIP J. Wireless Commun. Netw",
      "doi": "10.1186/s13638-016-0661-x"
    },
    {
      "citation_id": "11",
      "title": "Sarcasm SIGN: Interpreting sarcasm with sentiment based monolingual machine translation",
      "authors": [
        "L Peled",
        "R Reichart"
      ],
      "year": "2017",
      "venue": "Sarcasm SIGN: Interpreting sarcasm with sentiment based monolingual machine translation",
      "arxiv": "arXiv:1704.06836"
    },
    {
      "citation_id": "12",
      "title": "Understanding the patterns of health information dissemination on social media during the ZIKA outbreak",
      "authors": [
        "X Gui"
      ],
      "year": "2017",
      "venue": "Proc. AMIA Symp"
    },
    {
      "citation_id": "13",
      "title": "Collaborative user clustering for short text streams",
      "authors": [
        "S Liang",
        "Z Ren",
        "E Yilmaz",
        "E Kanoulas"
      ],
      "year": "2017",
      "venue": "Proc. 31st AAAI Conf"
    },
    {
      "citation_id": "14",
      "title": "Explainable user clustering in short text streams",
      "authors": [
        "Y Zhao",
        "S Liang",
        "Z Ren",
        "J Ma",
        "E Yilmaz",
        "M De Rijke"
      ],
      "year": "2016",
      "venue": "Proc. 39th Int"
    },
    {
      "citation_id": "15",
      "title": "Understanding and predicting usefulness judgment in Web search",
      "authors": [
        "J Mao"
      ],
      "year": "2017",
      "venue": "Proc. Int. ACM SIGIR Conf"
    },
    {
      "citation_id": "16",
      "title": "Sentiment and sarcasm classification with multitask learning",
      "authors": [
        "N Majumder",
        "S Poria",
        "H Peng",
        "N Chhaya",
        "E Cambria",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "17",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "C Kai",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Proc"
    },
    {
      "citation_id": "18",
      "title": "Learning word embeddings with CHI-square weights for healthcare tweet classification",
      "authors": [
        "S Kuang",
        "B Davison"
      ],
      "year": "2017",
      "venue": "Appl. Sci"
    },
    {
      "citation_id": "19",
      "title": "Sentence vector model based on implicit word vector expression",
      "authors": [
        "X Wang",
        "H Zhang",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Encoding syntactic dependency and topical information for social emotion classification",
      "authors": [
        "C Wang",
        "B Wang",
        "W Xiang",
        "M Xu"
      ],
      "year": "2019",
      "venue": "Proc. 42nd Int"
    },
    {
      "citation_id": "21",
      "title": "A personalized affective memory model for improving emotion recognition",
      "authors": [
        "P Barros",
        "G Parisi",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "22",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Proc"
    },
    {
      "citation_id": "23",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. Conf. North Amer"
    },
    {
      "citation_id": "24",
      "title": "XLNET: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "XLNET: Generalized autoregressive pretraining for language understanding",
      "arxiv": "arXiv:1906.08237"
    },
    {
      "citation_id": "25",
      "title": "Graph convolutional networks for text classification",
      "authors": [
        "L Yao",
        "C Mao",
        "Y Luo"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "26",
      "title": "Linguistically regularized LSTMs for sentiment classification",
      "authors": [
        "Q Qian",
        "M Huang",
        "J Lei",
        "X Zhu"
      ],
      "year": "2016",
      "venue": "Linguistically regularized LSTMs for sentiment classification",
      "arxiv": "arXiv:1611.03949"
    },
    {
      "citation_id": "27",
      "title": "A convolutional neural network for modelling sentences",
      "authors": [
        "N Kalchbrenner",
        "E Grefenstette",
        "P Blunsom"
      ],
      "year": "2014",
      "venue": "A convolutional neural network for modelling sentences",
      "arxiv": "arXiv:1404.2188"
    },
    {
      "citation_id": "28",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "R Socher"
      ],
      "year": "2013",
      "venue": "Proc. Conf. Empirical Methods Nat"
    },
    {
      "citation_id": "29",
      "title": "Adversarial multi-task learning for text classification",
      "authors": [
        "P Liu",
        "X Qiu",
        "X Huang"
      ],
      "year": "2017",
      "venue": "Adversarial multi-task learning for text classification",
      "arxiv": "arXiv:1704.05742"
    },
    {
      "citation_id": "30",
      "title": "Recurrent attention network on memory for aspect sentiment analysis",
      "authors": [
        "P Chen",
        "Z Sun",
        "L Bing",
        "W Yang"
      ],
      "year": "2017",
      "venue": "Proc. Conf. Empirical Meth. Natural Lang. Process"
    },
    {
      "citation_id": "31",
      "title": "EmotionMeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cybern"
    },
    {
      "citation_id": "32",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cybern"
    },
    {
      "citation_id": "33",
      "title": "Yahoo! for Amazon: Sentiment extraction from small talk on the Web",
      "authors": [
        "S Das",
        "M Chen"
      ],
      "year": "2007",
      "venue": "Manag. Sci"
    },
    {
      "citation_id": "34",
      "title": "Sentiment lexicon construction with representation learning based on hierarchical sentiment supervision",
      "authors": [
        "L Wang",
        "R Xia"
      ],
      "year": "2017",
      "venue": "Proc. Conf. Empirical Meth. Language Lang. Process"
    },
    {
      "citation_id": "35",
      "title": "Social sentiment detection of event via microblog",
      "authors": [
        "X Wang",
        "X Luo",
        "J Chen"
      ],
      "year": "2013",
      "venue": "Proc. IEEE Int"
    },
    {
      "citation_id": "36",
      "title": "WordNet affect: An affective extension of WordNet",
      "authors": [
        "C Strapparava",
        "A Valitutti"
      ],
      "year": "2004",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "37",
      "title": "DepecheMood: A lexicon for emotion analysis from crowd-annotated news",
      "authors": [
        "J Staiano",
        "M Guerini"
      ],
      "year": "2014",
      "venue": "DepecheMood: A lexicon for emotion analysis from crowd-annotated news",
      "arxiv": "arXiv:1405.1605"
    },
    {
      "citation_id": "38",
      "title": "EmowordNet: Automatic expansion of emotion lexicon using English WordNet",
      "authors": [
        "G Badaro",
        "H Jundi",
        "H Hajj",
        "W El-Hajj"
      ],
      "year": "2018",
      "venue": "Proc. 7th Joint Conf. Lexical Comput. Semantics"
    },
    {
      "citation_id": "39",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proc. 12th Int. Workshop Semantic Eval"
    },
    {
      "citation_id": "40",
      "title": "EMA at SemEval-2018 task 1: Emotion mining for Arabic",
      "authors": [
        "G Badaro"
      ],
      "year": "2018",
      "venue": "Proc. 12th Int. Workshop Semantic Eval"
    },
    {
      "citation_id": "41",
      "title": "ARSEL: A large scale arabic sentiment and emotion lexicon",
      "authors": [
        "G Badaro",
        "H Jundi",
        "H Hajj",
        "W El-Hajj",
        "N Habash"
      ],
      "year": "2018",
      "venue": "Proc. 3rd Workshop Open Source Arabic Corpora Process. Tools (OSACT)"
    },
    {
      "citation_id": "42",
      "title": "Automatic construction of domainspecific sentiment lexicon based on constrained label propagation",
      "authors": [
        "H Sheng",
        "Z Niu",
        "C Shi"
      ],
      "year": "2014",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "43",
      "title": "Expanding domain sentiment lexicon through double propagation",
      "authors": [
        "G Qiu",
        "L Bing",
        "J Bu",
        "C Chen"
      ],
      "year": "2009",
      "venue": "Proc. Int. Joint Conf"
    },
    {
      "citation_id": "44",
      "title": "Identifying preferred entities in comparative sentences",
      "authors": [
        "G Ganapathibhotla",
        "B Liu"
      ],
      "year": "2008",
      "venue": "Proc. Int. Conf. Comput. Linguist. (COLING)"
    },
    {
      "citation_id": "45",
      "title": "Identify sentiment-objects from Chinese sentences based on cascaded conditional random fields",
      "authors": [
        "M Zheng",
        "Z Lei",
        "X Liao",
        "G Chen"
      ],
      "venue": "J. Chin. Inf. Process"
    },
    {
      "citation_id": "46",
      "title": "Sentiment and topic analysis on social media: A multi-task multi-label classification approach",
      "authors": [
        "H Shu",
        "P Wei",
        "J Li",
        "D Lee"
      ],
      "year": "2013",
      "venue": "Proc. ACM Web Sci. Conf"
    },
    {
      "citation_id": "47",
      "title": "An integrated method for micro-blog subjective sentence identification based on three-way decisions and Naive Bayes",
      "authors": [
        "Y Zhu",
        "H Tian",
        "J Ma",
        "J Liu",
        "T Liang"
      ],
      "year": "2014",
      "venue": "Proc. Int. Conf. Rough Sets Knowl"
    },
    {
      "citation_id": "48",
      "title": "Opinion mining and sentiment analysis",
      "authors": [
        "B Pang",
        "L Lee"
      ],
      "year": "2008",
      "venue": "Found. Trends Inf. Retrieval"
    },
    {
      "citation_id": "49",
      "title": "Fast supervised topic models for short text emotion detection",
      "authors": [
        "J Pang"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cybern., early access",
      "doi": "10.1109/TCYB.2019.2940520"
    },
    {
      "citation_id": "50",
      "title": "Lexical-syntactical patterns for subjectivity analysis of social issues",
      "authors": [
        "M Karamibekr",
        "A Ghorbani"
      ],
      "year": "2013",
      "venue": "Proc. Int. Conf. Active Media Technol"
    },
    {
      "citation_id": "51",
      "title": "Real-time multimedia social event detection in microblog",
      "authors": [
        "S Zhao",
        "Y Gao",
        "G Ding",
        "T.-S Chua"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Cybern"
    },
    {
      "citation_id": "52",
      "title": "Multimodal sentiment analysis to explore the structure of emotions",
      "authors": [
        "A Hu",
        "S Flaxman"
      ],
      "year": "2018",
      "venue": "Proc. 24th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min"
    },
    {
      "citation_id": "53",
      "title": "Measuring emotional contagion in social media",
      "authors": [
        "E Ferrara",
        "Z Yang"
      ],
      "year": "2015",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "54",
      "title": "Service with a smile: Emotional contagion in the service encounter",
      "authors": [
        "S Pugh"
      ],
      "year": "2001",
      "venue": "Acad. Manag. J"
    },
    {
      "citation_id": "55",
      "title": "Modeling and inferring mobile phone users' negative emotion spreading in social networks",
      "authors": [
        "Z Du",
        "Y Yang",
        "Q Cai",
        "C Zhang",
        "B Yuan"
      ],
      "year": "2018",
      "venue": "Future Gen. Comput. Syst"
    },
    {
      "citation_id": "56",
      "title": "Emotions and information diffusion in social media-Sentiment of microblogs and sharing behavior",
      "authors": [
        "S Stieglitz",
        "L Dang-Xuan"
      ],
      "year": "2013",
      "venue": "J. Manag. Inf. Syst"
    },
    {
      "citation_id": "57",
      "title": "Good friends, bad news-affect and virality in Twitter",
      "authors": [
        "L Hansen",
        "A Arvidsson",
        "F Nielsen",
        "E Colleoni",
        "M Etter"
      ],
      "year": "2011",
      "venue": "Future Information Technology"
    },
    {
      "citation_id": "58",
      "title": "Anger is more influential than joy: Sentiment correlation in WEIBO",
      "authors": [
        "R Fan",
        "J Zhao",
        "Y Chen",
        "K Xu"
      ],
      "year": "2013",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "59",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Just how mad are you? Finding strong and weak opinion clauses",
      "authors": [
        "T Wilson",
        "J Wiebe",
        "R Hwa"
      ],
      "year": "2004",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "61",
      "title": "Emotions in Social Psychology: Essential Readings",
      "authors": [
        "W Parrott"
      ],
      "year": "2001",
      "venue": "Emotions in Social Psychology: Essential Readings"
    },
    {
      "citation_id": "62",
      "title": "Estimation of inter-sentiment correlations employing deep neural network models",
      "authors": [
        "X Wang",
        "S Yuan",
        "H Zhang",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "Estimation of inter-sentiment correlations employing deep neural network models",
      "arxiv": "arXiv:1811.09755"
    },
    {
      "citation_id": "63",
      "title": "",
      "authors": [
        "O Dictionary",
        "Oxford English Dictionary",
        "J Simpson",
        "E Weiner"
      ],
      "year": "1989",
      "venue": ""
    },
    {
      "citation_id": "64",
      "title": "Psychology: European Edition",
      "authors": [
        "D Schacter",
        "D Gilbert",
        "D Wegner",
        "B Hood"
      ],
      "year": "2011",
      "venue": "Psychology: European Edition"
    },
    {
      "citation_id": "65",
      "title": "Group Emotion: A View From Top and Bottom",
      "authors": [
        "S Barsade",
        "D Gibson"
      ],
      "year": "1998",
      "venue": "Group Emotion: A View From Top and Bottom"
    },
    {
      "citation_id": "66",
      "title": "Pattern Recognition and Machine Learning",
      "authors": [
        "C Bishop"
      ],
      "year": "2006",
      "venue": "Pattern Recognition and Machine Learning"
    }
  ]
}