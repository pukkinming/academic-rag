{
  "paper_id": "2403.18397v2",
  "title": "Colour And Brush Stroke Pattern Recognition In Abstract Art Using Modified Deep Convolutional Generative Adversarial Networks",
  "published": "2024-03-27T09:35:56Z",
  "authors": [
    "Srinitish Srinivasan",
    "Varenya Pathak",
    "Abirami S"
  ],
  "keywords": [
    "Generative Adversarial Network",
    "Art",
    "Brush Strokes",
    "Colours",
    "Deep Learning",
    "Pattern Recognition",
    "Latent Space",
    "Random Walk"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Pattern recognition is based on the generalisation of objects to classify real-world items by identifying certain shared common features. Pattern recognition, a machine learning algorithm, considers useful features while eliminating redundant ones  [1] . This technique is invaluable in various fields such as text pattern recognition  [2] , fingerprint scanning  [3] , seismic activity analysis  [4] , audio recognition  [5]  and healthcare  [6] .\n\nIn our study, pattern recognition was implemented through Generative Modelling  [7]  using Generative Adversarial Networks (GANs)  [8] . Generative Models have the ability to learn and explain the distribution large amount of data in various forms such as audio, images, words, etc. Once the model is trained, it can generate new data by extracting samples from the derived data or from random noise. These models have to ability to learn and model underlying categories, dimensions, and other aspects without specific programming. There are different generative models tailored for specific requirements, including GANs for style transfer in images  [9] , Hidden Markov Models (HMMs) for speech recognition  [10] , Variational Autoencoders (VAEs) for image generation  [11] , and Auto-encoders for anomaly detection  [12] .\n\nGenerative Adversarial Networks(GANs) consist of a generator and a discriminator. The generator produces images after learning the distribution while the discriminator, a Deep Neural Network  [13] ,determines whether the generated image is real or fake. GANs are chosen for their ability to produce high-quality outputs from grainy inputs and for their flexibility and fine-tuning capabilities, crucial aspects of any model.\n\nThis study explores and derives the mathematical features of colour patterns through latent space exploration by random walks  [14] . A 1D random walk involves an object moving left, right, or staying in place, with probabilities determining its future movements. In a 2D random walk, the movement extends to the XY, YZ, or XZ axis-up, down, left, or right-with equal probabilities, akin to the movement of chess pieces on a board. In a 3D random walk, spatial positions are considered, predicting the object's probability in a Monte Carlo  [15]  randomised algorithm.\n\nRandom walks are implemented in this study to support style transfer and fusion, where a random walk in a latent space mixes various styles smoothly. This approach starts from a known point in the latent space  [16]  and explores random directions, generating unique art styles while mapping artistic parameters provided by the user, such as textures and brush stroke styles. These mapped parameters serve as inputs for the model, guiding it through random directions in the latent space. This method also generates a sequence of latent vectors, enabling a gradual transformation and evolution of the generated images. The transitions between styles in our outputs are gradual and exploratory providing state of the art results enabling us to successfully study abstract art patterns. Further this paper includes the study of distorted patterns after a certain point in training and describes the statistical analysis and tests performed in order to compare the distorted colour space with the original space.\n\nThe following sections of the paper are divided as follows: Section II describes the related work with respect to pattern recognition and Generative Adversarial Networks(GAN). Section III describes the proposed workflow and architecture of our study. Section IV discusses the results of the brush stroke colour patterns qualitatively, latent space exploration by random walks and a statistical analysis of distorted outputs at a certain period of training. Section V concludes our study and describes the future scope of this study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we discuss related literature with respect to pattern recognition using Machine learning techniques, and the use of GANs of in several pattern recognition, generative and style transfer tasks.  [17]  studied the spatial and temporal variations of water quality of the Suquia River Basin. The researchers used factor analysis, cluster analysis, principal component analysis and Discriminant Analysis to cluster and obtain patterns. They concluded that Cluster Analysis gives good results as an initial exploratory method to evaluate spatial and temporal differences and reduces the number of parameters by 77% to differentiate between four spatial areas and Discriminant Analysis provides the best results for temporal and spatial analysis. It performs 73% reduction in parameters to differentiate between wet and dry season upto a precision of 87%.  [18]  studied different pattern recognition techniques for apple sorting. They made use of K-Nearest Neighbours algorithm using 1 and 2 nearest neighbours , decision trees and Artificial neural networks. They noted that with respect to classification performance, 1 and 2 nearest neighbours methods using five input features yielded the second best results while the Neural Network was able to detect non linear relationships in apple sorting patterns.\n\nIn the field of pattern recognition and classification with respect to art works,  [19]  studied various pattern extraction techniques using Generative Adversarial Neural Networks and Deep Convolutional Neural Networks to classify art periods, emotions from art works and construct a social network of artists.  [20]  used a Radial Basis Function neural network classifier to model western paintings for classification. These groups of networks are very powerful and have been used in function approximation, pattern classification and data compression. For the feature extraction process, the researchers made use of Gabor wavelets, a popular wavelet transform in image processing  [21] .  [22]  performed a 3-step hierarchal classification of paintings using face and brush stroke models. Their 3-step approach is inclusive of colour classification by grouping portrait miniatures by computing the mean RGB value, followed by shape classification on a region by region basis by reducing the search space to a specific Region of Interest followed by stroke detection and classification.  [23]  studied fast texture synthesis using tree-structured vector quantisation. They implemented a Gaussian Pyramid and Markov Random Field like architecture and used tree-structured vector quantisation for acceleration, a common method for data compression. The approach has the ability to replicate an image on given texture as input.  [24]  studied style and abstraction in portrait sketching. The researchers replicate the sketch stroke of artists by performing edge detection using the canny edge detection operator in addition to stroke matching and curve detection.  [25]  analyses various algorithms and methods for stroke-based rendering. The optimisation method includes Voronoi Algorithms that use the property of SBR to perform efficient global update steps, trial and error algorithms performs heuristically chosen tests to reduce randomness. The researchers studied Greedy algorithms were studied as well but it was concluded that they are too slow for any interactive application.  [26]  proposed features derived from colours, edges and grey scale-texture of images that discriminate paintings from photographs. They proposed a neural-network classification methodology with 6 sigmoidal units in a unique hidden layer to perform painting-photograph discrimination.\n\nGenerative Adversarial Neural networks, since its introduction by  [8]  has become popular in the field of art being used for generation and style transfer purposes.  [27]  compared various popular GAN architectures.\n\nThey concluded that Pix2Pix could be relevant for contemporary simple-styled style transfer tasks for Orthoimages but not suitable for old map styles which are more different and visually complex in content and styles, while Cycle-GAN could be more revenant for such images.  [28]  uses a 5-layer CNN to perform style transfer on images. They noted that the speed of image synthesis is hindered by the image and resolution of the fearer space, in addition to this they mentioned that denoising images is a challenge with this architecture.  [29]  implemented UnityGAN to learn the style changes between camera, producing shapestable style unity images for each camera. They made use of skip-connections between multi-depth layers which enabled the retention of more structural information therefore accoutring for the stability of the generated image.  [30]  proposed APDrawingGAN++ to transform the photo of a face to a high quality APDrawing. It made use of auto encoders to improve facial feature drawings, lip and hair classifiers were introduced to guide the local generator and auto encoder to a desired style. Moreover the researchers made use of DT loss to penalise large misalignments.  [31]  compared and analysed various kinds of Neural networks for art-based applications such as GANS, Image stylisation, DeepDream and Perception Engines which includes image Fourier models.  [32]  implements BigGAN-deep model on the ImageNET dataset with hierarchical latent spaces. BigGAN deep differs from the BigGAN model as it contains 2 extra 1X1 convolutions to provide the required number of output filters for the images. On increasing the depth by two, the researchers noted performances were negatively affected to an extent.  [33]  used mGANprior that employs multiple latent codes for reconstructing real images with a pre trained-GAN model. It enables the use of GAN's as a powerful prior for pre-processing tasks such as colorisation, in painting, inverting images.  [34]  implements DCGAN and finds closest latent features in order to update the latent vector gradually and smoothly to generate the desired image. The architecture was used to make desired edits to images based on users requirements.  [35]  proposed an InterfaceGAN to interpret the semantics encoded in the latent space of GANs. Provides a rigorous analysis of the semantic attributes emerging in the latent space of well-trained GAN models, and then constructs a manipulation pipeline of for leveraging the semantics in the latent code for facial attribute editing. The architecture is tested against encoder-decoder generative models and StyleGANs.  [36]  implements a progressive GAN growth experiment for improved quality in GAN outputs.\n\nThe researchers start from a low resolution, and progressively add new layers. The researchers tested the performance of the model on CIFAR10 with an inception score of 8.80.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Work 1. Pre-Processing",
      "text": "The images used for the study have different sizes such as 1024X2048, 512X512, 2048X2048 etc. They were resized to a standard size of 256X256, the average dimensions of the images. In the resized images, standard noise filtering techniques such as Gaussian and Median filters are applied in order to filter out additive gaussian noise. The standard deviation used for the gaussian filter was 0.001, to ensure the process does not lose features through blurring. Further, normalisation is performed by calculating a z-score for all 3 channels. Equation 3.1 describes the expression used for normalisation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Architecture",
      "text": "In this section, we discuss about the architecture of mDCGAN proposed for art image generation. The proposed work involves modifications to both the generator and discriminator components of a DC-GAN architecture, aimed at achieving stable and enhanced art image generation. A GAN consists of two deep neural network models, a generator and discriminator. The generator tries to overcome the discriminator by trying to make the generator predict all its outputs as real whereas the discriminator tries to distinguish between real and fake images, setting up an adversarial scenario as per game theory  [37] . In this section, we The architectural modifications of mDCGAN draw inspiration from  [38] , with adaptations tailored to accommodate 256x256 image dimensions. These architectural modifications of mDCGAN aim to enhance the stability and diversity of art image generation, accommodating the unique demands of generating art while mitigating common challenges associated with GANs such as modal collapse, and noisy outputs. A 4X4 kernel for the final convolution layer, a final reshape layer and, additional dropouts are added at the discriminator to ensure the discriminator does not overfit the generator. As suggested in DC-GAN, mDCGAN uses leaky ReLU activated functions with negative slope of 0.2 for the discriminator. The final layer activation of the generator was mapped using tanh(z) activation function, where z represents the output of a convolution mapping. In mDCGAN, square kernels of length 4 is used for every layer of the generator, with padding set to 1. A convolution layer in the generator consists of (i)\n\nTranspose convolution layers of stride 2 and padding 1, (ii) A Batch normalisation layer (iii) ReLU  [39]  activation function for all layers except the last layer which uses tanh. The number of kernels decrease by a factor of 2 for every layer to construct an image with 3 channels. The first 2 layers in the generator are linear and reshape[citation] layers respectively, aimed to transform a 100-dimensional vector to a vector with the help of a linear transformation. The 16384-feature vector is reshaped to a block of size 1024X4X4, where 1024 represents the number of kernel filters or channels and 4X4 represents the height and width dimensions of the embedding. After reshaping the embeddings are transpose convolved through 6 layers to construct an image of size 3X256X256 where 3 represents the RGB channels. For the discriminator, mDCGAN uses square kernels of size 3 for every layer with padding set to 1. A convolution layer in the discriminator consists of (i)Convolution mapping of stride 2 and padding 1, (ii) Batch normalisation layer (iii)Leaky ReLU activated functions with negative slope set as 0.2. For every layer, the number of kernels increase by a factor of 2 which enables the model to learn low level features at a smaller spatial dimension. The final convolution layer of the discriminator uses a square kernel of size 4, this plays a role in reducing the dimensions to 1X1X1 with fewer convolution layers, ensuring the discriminator does not overfit the generator due to a deeper network. The final layer of the discriminator is a reshape layer that transforms an image of size 1X1X1 into a one-dimensional vector, so that it can be passed into a probability mapping function such as sigmoid. Table  3 .1 and 3.2 describe the layers of the discriminator and generator respectively.",
      "page_start": 5,
      "page_end": 9
    },
    {
      "section_name": "Training Workflow",
      "text": "During training, labels are assigned for real and fake images. The images from the dataset are labelled as real and the images generated from noise is labelled as fake. The role of the discriminator is to accurately classify all real images as real and fake images as fake. First the output of the discriminator model is computed on an equal distribution of real and fake images. Then the loss is estimated to update the discriminator weights.\n\nThen a sample of 100 data points, represented as a 100-dimensional vector is obtained from a uniform random distribution and passed into the generator. The generated samples, output of the generator, are passed into the discriminator which maps them to real labels (real/fake). That is, the generator tries to convince the discriminator that its generated image is real. In this process, if the generator successfully does so, the generator is rewarded, else it's penalised. The loss value is computed and the weights for the generator are updated. This sets up an adversarial training loop.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Objective Function",
      "text": "The mDCGAN is trained using the Binary cross entropy loss function. The ground true labels are real or fake, and the input is a probability obtained from the sigmoid function. Equation (  2 ) describes the expression used to compute the loss value,\n\nIn equation (  2 ), represents the ground truth label for real or fake, ùëÅ is the number of samples, and ùëßùëõ is the output of the sigmoid activation. The loss function mentioned in equation (  2 ) is the negative log likelihood function. Both the generator and discriminator are trained using this loss function. In order to set up an",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Layer Output Size Parameters",
      "text": "adversarial training loop, the loss of the generator and discriminator are modelled as per the min max algorithm  [8] . Equation (  3 ) and (  4 ) describe the generator and discriminator loss as described in  [40] , where , represent loss of the discriminator and generator respectively.\n\n(3) (4)\n\nWhile training the discriminator, we tried concatenating the real and fake images with their respective labels before passing it into the discriminator. However, this approach resulted in unstable training where the discriminator loss experienced a steep drop to a value around 0 within the first 5 to 10 epochs as shown in figure 3. To tackle this issue, the real and fake images were passed separately to compute their loss and their arithmetic mean was computed. This approach provided us with a much more stable training loop which we describe in section 4. Equation (  5 ) describes our approach for training the generator.\n\n(\n\nwhere, , represents the discriminator loss on passing real and fake samples respectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Optimisation Algorithm",
      "text": "L oss DR L oss DF Both the generator and discriminator were trained using the Adam optimiser  [41] . Adams optimiser uses Momentum and the exponential moving average of gradients from RMSProp in order to attain convergence quickly. The following parameters of Adam's optimiser were used for our study: i. ùõΩ1, ùõΩ2 which are the exponential decay rates of the first and second moment of the gradients respectively were set to 0.5 and 0.999.\n\nii. The learning rate parameter was set to 0.002\n\niii. ùúñ set to its default value of",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Iv. Experiment And Results",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiment Setup",
      "text": "The experiments were performed on a 2022 model Macbook Pro, with Apple Silicon M2 chip. The configurations of the system are inclusive of an 8GB unified memory with 256GB Hard disk storage. The system has 8 Core CPU, which has a split-up of 4 performance cores and 4 efficiency cores, and a 10 core GPU. In addition to this, the system has a 16-core neural engine.\n\nThe models used for the experiment were trained on PyTorch 2. In order to prevent the MPS backend from running out of memory and memory leaks, the following steps were put to practice in the experiment process:\n\n‚Ä¢ Batch size restricted to 32\n\n‚Ä¢ Usage of del  [42]  to delete tensors after a train and test step\n\n‚Ä¢ Clear cache blocks of the MPS backend using mps.empty_cache()  [42]",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Dataset",
      "text": "The dataset chosen for experimentation is the Abstract Art Gallery obtained from Kaggle  [43] . It is a diverse dataset comprising of 2782 images of different paintings, each with a distinct colour scheme and pattern scraped from various web sources. From initial visualisation most images have used strokes of green, red, black, and orange. For the purpose of training, randomly sampled 2000 images were used from the dataset while maintaining the overall distribution of the images. The images have varied sizes and resolutions such as 1024X2048, 1024X1024, 512X512 etc, for the purpose of uniformity we resized all images to 1024X1024. All images were smoothened using standard filters such as Gaussian and Median, with no data augmentation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Environment",
      "text": "Tuning of hyper-parameters especially for training Generative Adversarial Networks play a crucial role in recognising patterns and generating stable images. GANs are highly susceptible to unstable training, modal collapse and noise, therefore it is important to fine tune hyper-parameters optimally  [44]  Table  2  describes the parameters used to obtain stable training of mDCGAN. The Adams Beta 1 and Learning Rate parameter were tuned according to the parameters mentioned in  [38] . Batch size was set to 32 taking memory constraints of the system environment into account.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Training Analysis",
      "text": "The model was trained according to the training parameters mentioned in section 2.3. The work involved training the two modules of mDCGAN together: the generator and the discriminator. No upper or lower bounds on loss were observed due to the unstable training conditions of GANs. It is to note that the loss function is not a metric for training but only used for plot visualisation purposes. After epoch 500 we get noisy outputs due to overfitting which is further discussed in the next section. The outputs obtained after generating images are as shown in Figure  5 (a) to Figure  5 (e). As noted in section III, the input is a random noise vector generated from a Uniform distribution of 100 dimensions. It can be observed that the model generates multiple different brush stroke patterns using the colours present in the original dataset. We observe the dominance of black strokes, which is a prevalent feature in modern day abstract art  [45] . Other patterns include brush strokes of lighter and darker shades of blue, and tin shades of green and red. These colours usually dominate in abstract art paintings symbolising various emotions and depictions of artists. The generated brush strokes have unique features with respect to gradients, edges and directional patterns. It can be observed that when red is the dominant colour, blue is usually used as a complementary colour along with strokes of purple and black. However in certain paintings, dark blue is observed as the dominant colour which is usually complemented by a black shade to make the painting a blue-purple combination.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Analysis Of Generated Brush Stroke Patterns",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Space Exploration Through Random Walks",
      "text": "This section describes the experiment of exploring the latent space. The distribution of the patterns lies in the latent space, which is a multi-dimensional abstract space that encodes the information of the outside world.\n\nThe latent space of the brush stroke and colour patterns generated by mDCGAN is explored by performing algebraic vector operations to discover new patterns and colours.   6 ), (  7 ) and  (8)  describe the operations performed.\n\nOn performing the above set of operations, the rise of new colour shades and patterns, and a mathematical relation between different shades of colours is observed. The development of colour shades such ad blue, yellow and lighter shades of green is seen through this process. Hence it is possible to develop vector relationships between colour contrasts, analogous to logical relationships between words captured by word embeddings  [46] . From figure  6 (a) and 6(b), we can derive the following colour-based relationships described by equations (  9 ) and (  10 )\n\nIt is to note that the above mentioned equations are derived from a pure qualitative basis. The Experiment",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Unstable Gan Outputs With Time",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "7.I Qualitative And Numerical Analysis",
      "text": "The model was trained upto 500 epochs and began result visualisation after the epoch. It was noted that after epoch 250, the patterns were distorted with highly pixelated outputs resulting in heavy noise and feature loss. This is probably due to the weights of the generator trying to overcorrect resulting in large updates when the discriminator begins to outperform the generator. Figure  7 (a) and 7(b) highlight this observation further.\n\nWe notice that the black brush strokes are still clear while the other colours such as blue, green, yellow etc try to overfit the canvas resulting in high distortion A quantitative analysis was performed on the noisy image distribution by comparing it with the stable distribution.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Metric Equation Value",
      "text": "Distance Signal to Noise ratio greater than 0 represents a strong signal power. The SNR proves the argument that the images after epoch 250 are highly distorted as noted qualitatively. In addition, and distances were computed between the two distributions. The values mentioned in Table  3  for the above two metrics show that the two distributions are dissimilar thereby proving the qualitative argument. It is to note that these are informal metrics to measure image quality and GAN performance.",
      "page_start": 14,
      "page_end": 16
    },
    {
      "section_name": "7.Ii Hypothesis Test Of Difference In Variance Using F-Test",
      "text": "To formalise the argument that there is a significant difference between the art space distribution before and after epoch 250, an F-Test is conducted to compare the variance of the two distributions. The test statistic for the samples i.e sample variance is computed for around 101 samples.  The Hypothesis is defined as follows let be the standard deviations of the two distributions, and represent the null and alternative hypothesis respectively. We define the test as follows:\n\nThe Null Hypothesis states that there is no significant difference between the standard deviations of the two samples, while the Alternative hypothesis states otherwise. The test statistic is computed using  (15) .  (15)  represent the sample standard deviation for samples 1 and 2 respectively. On inserting the values listed in Table  4  in equation  (15) , the value of test statistics is 1.629. Confidence intervals of 95% and 99% are considered, therefore the level of significance is 0.05 and 0.01 respectively. The obtained results on computing the test statistic along with the critical value of the confidence intervals are listed in Table  5 .\n\nFor a two tailed test, the rejection region is defined as where is the critical value. For both the intervals, therefore null hypothesis is rejected. Hence the qualitative argument that there is a significant difference between the variances of the two distributions is proved. In our study, we generate colour and brush stroke patterns in abstract art by using a modified version of a DCGAN to fit our needs. We see that darker colours such as black, dark red, purple and dark blue are very popular among abstract artists. We observe the use of dark brush strokes complemented by lighter colours in the palette. Later we performed a random walk to explore the latent space comprising of multicoloured strokes and were, qualitatively, able to extract vector relationships between colours through multiple random walk experiments performed at the end of training and at its early stages. Furthermore we analyse unstable distorted GAN outputs after epoch 250 in training by a statistical analysis using Signal to Noise Ratio and distance between distributions metrics and hypothesis testing of difference in variance to show that there is a significant difference between sample distributions before and after epoch 250. Further work can be carried out by employing larger GANs such as StyleGANs for higher resolution outputs while techniques such as edge detection, gradient-based study of brush strokes etc can explored apart from latent space exploration.",
      "page_start": 16,
      "page_end": 19
    },
    {
      "section_name": "V. Conclusion And Further Work",
      "text": "",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Workflow of the proposed model",
      "page": 5
    },
    {
      "caption": "Figure 2: (a): Generator Architecture",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) and 2(b).",
      "page": 6
    },
    {
      "caption": "Figure 2: (b): Discriminator Architecture",
      "page": 6
    },
    {
      "caption": "Figure 3: To tackle this issue, the real and fake images were passed separately to compute their loss and their",
      "page": 9
    },
    {
      "caption": "Figure 3: Unstable Training using combined distribution of Real and Fake Samples",
      "page": 9
    },
    {
      "caption": "Figure 3: describes the",
      "page": 11
    },
    {
      "caption": "Figure 3: , it can be seen that a close to ideal situation of GAN training is achieved in",
      "page": 11
    },
    {
      "caption": "Figure 5: (a) to Figure 5(e). As noted in section",
      "page": 12
    },
    {
      "caption": "Figure 4: 3 depicts the argument mentioned",
      "page": 12
    },
    {
      "caption": "Figure 4: Generator and Discriminator Loss",
      "page": 12
    },
    {
      "caption": "Figure 6: (a) and 6(b), we can derive the following colour-based relationships",
      "page": 13
    },
    {
      "caption": "Figure 6: (b) was performed mid-training i.e after 175 epochs. The results of experiment 6(b)",
      "page": 13
    },
    {
      "caption": "Figure 6: (c) show the",
      "page": 13
    },
    {
      "caption": "Figure 7: (a) and 7(b) highlight this",
      "page": 13
    },
    {
      "caption": "Figure 6: (a): Random Walk Experiment",
      "page": 16
    },
    {
      "caption": "Figure 6: (b): Random Walk Experiment: Mid Training",
      "page": 17
    },
    {
      "caption": "Figure 6: (c): Random Walk Experiment",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table 3: 1 and 3.2 describe the layers of the discriminator and generator respectively.",
      "data": [
        {
          "Layer\nOutput Size\nParameters": "Conv2d-1"
        },
        {
          "Layer\nOutput Size\nParameters": "BatchNorm2d-2"
        },
        {
          "Layer\nOutput Size\nParameters": "Dropout2d-3"
        },
        {
          "Layer\nOutput Size\nParameters": "LeakyReLU-4"
        },
        {
          "Layer\nOutput Size\nParameters": "Conv2d-5"
        },
        {
          "Layer\nOutput Size\nParameters": "Dropout2d-6"
        },
        {
          "Layer\nOutput Size\nParameters": "BatchNorm2d-7"
        },
        {
          "Layer\nOutput Size\nParameters": "LeakyReLU-8"
        },
        {
          "Layer\nOutput Size\nParameters": "Conv2d-9"
        },
        {
          "Layer\nOutput Size\nParameters": "Dropout2d-10"
        },
        {
          "Layer\nOutput Size\nParameters": "BatchNorm2d-11"
        },
        {
          "Layer\nOutput Size\nParameters": "LeakyReLU-12"
        },
        {
          "Layer\nOutput Size\nParameters": "Conv2d-13"
        },
        {
          "Layer\nOutput Size\nParameters": "Dropout2d-14"
        },
        {
          "Layer\nOutput Size\nParameters": "BatchNorm2d-15"
        },
        {
          "Layer\nOutput Size\nParameters": "LeakyReLU-16"
        },
        {
          "Layer\nOutput Size\nParameters": "Conv2d-17"
        },
        {
          "Layer\nOutput Size\nParameters": "Dropout2d-18"
        },
        {
          "Layer\nOutput Size\nParameters": "BatchNorm2d-19"
        },
        {
          "Layer\nOutput Size\nParameters": "LeakyReLU-20"
        },
        {
          "Layer\nOutput Size\nParameters": "Conv2d-21"
        },
        {
          "Layer\nOutput Size\nParameters": "Dropout2d-22"
        },
        {
          "Layer\nOutput Size\nParameters": "BatchNorm2d-23"
        },
        {
          "Layer\nOutput Size\nParameters": "LeakyReLU-24"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: (b): Generator Layers",
      "data": [
        {
          "Layer": "Linear-1",
          "Output Size": "[8, 16384]",
          "Parameters": "16,54,784"
        },
        {
          "Layer": "ConvTranspose2d-2",
          "Output Size": "[8, 512, 8, 8]",
          "Parameters": "83,89,120"
        },
        {
          "Layer": "BatchNorm2d-3",
          "Output Size": "[8, 512, 8, 8]",
          "Parameters": "1,024"
        },
        {
          "Layer": "ReLU-4",
          "Output Size": "[8, 512, 8, 8]",
          "Parameters": "0"
        },
        {
          "Layer": "ConvTranspose2d-5",
          "Output Size": "[8, 256, 16, 16]",
          "Parameters": "20,97,408"
        },
        {
          "Layer": "BatchNorm2d-6",
          "Output Size": "[8, 256, 16, 16]",
          "Parameters": "512"
        },
        {
          "Layer": "ReLU-7",
          "Output Size": "[8, 256, 16, 16]",
          "Parameters": "0"
        },
        {
          "Layer": "ConvTranspose2d-8",
          "Output Size": "[8, 128, 32, 32]",
          "Parameters": "5,24,416"
        },
        {
          "Layer": "BatchNorm2d-9",
          "Output Size": "[8, 128, 32, 32]",
          "Parameters": "256"
        },
        {
          "Layer": "ReLU-10",
          "Output Size": "[8, 128, 32, 32]",
          "Parameters": "0"
        },
        {
          "Layer": "ConvTranspose2d-11",
          "Output Size": "[8, 64, 64, 64]",
          "Parameters": "1,31,136"
        },
        {
          "Layer": "BatchNorm2d-12",
          "Output Size": "[8, 64, 64, 64]",
          "Parameters": "128"
        },
        {
          "Layer": "ReLU-13",
          "Output Size": "[8, 64, 64, 64]",
          "Parameters": "0"
        },
        {
          "Layer": "ConvTranspose2d-14",
          "Output Size": "[8, 32, 128, 128]",
          "Parameters": "32,800"
        },
        {
          "Layer": "BatchNorm2d-15",
          "Output Size": "[8, 32, 128, 128]",
          "Parameters": "64"
        },
        {
          "Layer": "ReLU-16",
          "Output Size": "[8, 32, 128, 128]",
          "Parameters": "0"
        },
        {
          "Layer": "ConvTranspose2d-17",
          "Output Size": "[8, 3, 256, 256]",
          "Parameters": "1,539"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Summary of Training Environment Parameters",
      "data": [
        {
          "Sr. No\nHyperparameter \nValue": "1"
        },
        {
          "Sr. No\nHyperparameter \nValue": "2"
        },
        {
          "Sr. No\nHyperparameter \nValue": "3"
        },
        {
          "Sr. No\nHyperparameter \nValue": "4"
        },
        {
          "Sr. No\nHyperparameter \nValue": "5"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Comparison between Stable and Erroneous Brush Stroke Colour Distribution",
      "data": [
        {
          "Metric\nEquation\nValue": "Signal to Noise Ratio"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric\nEquation\nValue": "Distance \nL2"
        },
        {
          "Metric\nEquation\nValue": "Distance\nL1"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 4: Sample mean and standard deviation",
      "data": [
        {
          "Generated Samples after Epoch \nGenerated Samples at Epoch \nn1 = n2 = 101\n250(Sample 1)\n250(Sample 2)": "Sample Mean\n123.68409\n125.07779"
        },
        {
          "Generated Samples after Epoch \nGenerated Samples at Epoch \nn1 = n2 = 101\n250(Sample 1)\n250(Sample 2)": "Sample Standard Deviation\n40.81453\n31.979033"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 4: Sample mean and standard deviation",
      "data": [
        {
          "degrees of \nLevel of Significance  Œ±\nCritical Value  c\nTest Statistic  F\nfreedom( \n)=100\nn ‚àí 1": "95% Confidence\n0.05\n1.392\n1.629"
        },
        {
          "degrees of \nLevel of Significance  Œ±\nCritical Value  c\nTest Statistic  F\nfreedom( \n)=100\nn ‚àí 1": "99% Confidence\n0.01\n1.598\n1.629"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "LeakyReLU-4",
      "venue": "LeakyReLU-4"
    },
    {
      "citation_id": "3",
      "title": "Conv",
      "venue": "Conv"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "Conv",
      "venue": "Conv"
    },
    {
      "citation_id": "6",
      "title": "Conv2d-13",
      "venue": "Conv2d-13"
    },
    {
      "citation_id": "7",
      "title": "LeakyReLU-16",
      "venue": "LeakyReLU-16"
    },
    {
      "citation_id": "8",
      "title": "Conv",
      "year": "2008",
      "venue": "Conv"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "Conv",
      "year": "1995",
      "venue": "Conv"
    },
    {
      "citation_id": "11",
      "title": "",
      "authors": [
        "Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "12",
      "title": "LeakyReLU-24",
      "year": "2004",
      "venue": "LeakyReLU-24"
    },
    {
      "citation_id": "13",
      "title": "ConvTranspose",
      "venue": "ConvTranspose"
    },
    {
      "citation_id": "14",
      "title": "",
      "authors": [
        "Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "15",
      "title": "ReLU-4",
      "year": "2008",
      "venue": "ReLU-4"
    },
    {
      "citation_id": "16",
      "title": "ConvTranspose",
      "venue": "ConvTranspose"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "Batchnorm"
      ],
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "ConvTranspose",
      "venue": "ConvTranspose"
    },
    {
      "citation_id": "19",
      "title": "BatchNorm",
      "venue": "BatchNorm"
    },
    {
      "citation_id": "20",
      "title": "ConvTranspose",
      "venue": "ConvTranspose"
    },
    {
      "citation_id": "21",
      "title": "Irrelevant Features in Pattern Recognition",
      "year": "1978",
      "venue": "Irrelevant Features in Pattern Recognition",
      "doi": "10.1109/tc.1978.1675182"
    },
    {
      "citation_id": "22",
      "title": "Text Recognition in the Wild",
      "authors": [
        "X Chen",
        "L Jin",
        "Y Zhu",
        "C Luo",
        "T Wang"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/3440756"
    },
    {
      "citation_id": "23",
      "title": "Automated fingerprint recognition using structural matching",
      "authors": [
        "A Hrechak",
        "J Mchugh"
      ],
      "year": "1990",
      "venue": "Pattern Recognition",
      "doi": "10.1016/0031-3203(90)90134-7"
    },
    {
      "citation_id": "24",
      "title": "Pattern recognition to forecast seismic time series",
      "authors": [
        "A Morales-Esteban",
        "F Mart√≠nez-√Ålvarez",
        "A Troncoso",
        "J Justo",
        "C Rubio-Escudero"
      ],
      "year": "2010",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2010.05.050"
    },
    {
      "citation_id": "25",
      "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/taslp.2020.3030497"
    },
    {
      "citation_id": "26",
      "title": "Predictive analytics in health care using machine learning tools and techniques",
      "authors": [
        "B Nithya",
        "V Ilango"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Intelligent Computing and Control Systems (ICICCS)",
      "doi": "10.1109/iccons.2017.8250771"
    },
    {
      "citation_id": "27",
      "title": "Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models",
      "authors": [
        "S Bond-Taylor",
        "A Leach",
        "Y Long",
        "C Willcocks"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2021.3116668"
    },
    {
      "citation_id": "28",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2020",
      "venue": "Communications of the ACM",
      "doi": "10.1145/3422622"
    },
    {
      "citation_id": "29",
      "title": "Synthesizing Depth Hand Images with GANs and Style Transfer for Hand Pose Estimation",
      "authors": [
        "W He",
        "Z Xie",
        "Y Li",
        "X Wang",
        "W Cai"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "30",
      "title": "Integrating natural language constraints into HMM-based speech recognition",
      "authors": [
        "Hy Murveit",
        "R Moore"
      ],
      "year": "2002",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/icassp.1990.115777"
    },
    {
      "citation_id": "31",
      "title": "Attribute2Image: Conditional Image Generation from Visual Attributes",
      "authors": [
        "X Yan",
        "J Yang",
        "K Sohn",
        "H Lee"
      ],
      "year": "2016",
      "venue": "Attribute2Image: Conditional Image Generation from Visual Attributes",
      "doi": "10.1007/978-3-319-46493-0_47"
    },
    {
      "citation_id": "32",
      "title": "Autoencoder-based network anomaly detection",
      "authors": [
        "Z Chen",
        "C Yeo",
        "B Lee",
        "C Lau"
      ],
      "year": "2018",
      "venue": "IEEE Xplore"
    },
    {
      "citation_id": "33",
      "title": "Understanding of a Convolutional Neural Network",
      "authors": [
        "S Albawi",
        "T Mohammed",
        "S Al-Zawi"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Engineering and Technology (ICET)",
      "doi": "10.1109/icengtechnol.2017.8308186"
    },
    {
      "citation_id": "34",
      "title": "Random walks",
      "authors": [
        "S Karlin",
        "J Mcgregor"
      ],
      "year": "1959",
      "venue": "Illinois Journal of Mathematics",
      "doi": "10.1215/ijm/1255454999"
    },
    {
      "citation_id": "35",
      "title": "Introduction To Monte Carlo Simulation",
      "authors": [
        "R Harrison"
      ],
      "venue": "AIP conference proceedings"
    },
    {
      "citation_id": "37",
      "title": "ClusterGAN: Latent Space Clustering in Generative Adversarial Networks",
      "authors": [
        "S Mukherjee",
        "H Asnani",
        "E Lin",
        "S Kannan"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33014610"
    },
    {
      "citation_id": "38",
      "title": "Pattern Recognition Techniques for the Evaluation of Spatial and Temporal Variations in Water Quality. A Case Study",
      "authors": [
        "W Alberto",
        "Marƒ± √Å Del Pilard",
        "P Marƒ± √Å Valeriaa",
        "H Fabiana",
        "Marƒ± Cecilia",
        "De Los √Ångelesb"
      ],
      "year": "2001",
      "venue": "Water Research"
    },
    {
      "citation_id": "39",
      "title": "Evaluation of different pattern recognition techniques for apple sorting",
      "authors": [
        "ƒ∞ Kavdƒ±r",
        "D Guyer"
      ],
      "year": "2008",
      "venue": "Biosystems Engineering",
      "doi": "10.1016/j.biosystemseng.2007.09.019"
    },
    {
      "citation_id": "40",
      "title": "Deep learning approaches to pattern extraction and recognition in paintings and drawings: an overview",
      "authors": [
        "G Castellano",
        "G Vessio"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-021-05893-z"
    },
    {
      "citation_id": "41",
      "title": "Stochastic modeling western paintings for effective classification",
      "authors": [
        "J Shen"
      ],
      "year": "2023",
      "venue": "?referer=&httpsredir=1&article=1757&context=sis_research"
    },
    {
      "citation_id": "42",
      "title": "Gabor Wavelets in Image Processing",
      "authors": [
        "D Barina"
      ],
      "year": "2016",
      "venue": "Gabor Wavelets in Image Processing"
    },
    {
      "citation_id": "43",
      "title": "Hierarchical classification of paintings using face-and brush stroke models",
      "authors": [
        "R Sablatnig",
        "P Kammerer",
        "E Zolda"
      ],
      "year": "2002",
      "venue": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)",
      "doi": "10.1109/icpr.1998.711107"
    },
    {
      "citation_id": "44",
      "title": "Fast texture synthesis using tree-structured vector quantization",
      "authors": [
        "L.-Y Wei",
        "M Levoy"
      ],
      "year": "2000",
      "venue": "International Conference on Computer Graphics and Interactive Techniques",
      "doi": "10.1145/344779.345009"
    },
    {
      "citation_id": "45",
      "title": "Style and abstraction in portrait sketching",
      "authors": [
        "I Berger",
        "A Shamir",
        "M Mahler",
        "E Carter",
        "J Hodgins"
      ],
      "year": "2013",
      "venue": "ACM Transactions on Graphics",
      "doi": "10.1145/2461912.2461964"
    },
    {
      "citation_id": "46",
      "title": "A survey of stroke-based rendering",
      "authors": [
        "A Hertzmann"
      ],
      "year": "2003",
      "venue": "IEEE Computer Graphics and Applications",
      "doi": "10.1109/mcg.2003.1210867"
    },
    {
      "citation_id": "47",
      "title": "Estimating the photorealism of images: distinguishing paintings from photographs",
      "authors": [
        "F Cutzu",
        "R Hammoud",
        "A Leykin"
      ],
      "year": "2003",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/cvpr.2003.1211484"
    },
    {
      "citation_id": "48",
      "title": "Neural map style transfer exploration with GANs",
      "authors": [
        "S Christophe",
        "S Mermet",
        "M Laurent",
        "Guillaume Touya"
      ],
      "year": "2022",
      "venue": "International journal of cartography",
      "doi": "10.1080/23729333.2022.2031554"
    },
    {
      "citation_id": "49",
      "title": "Image Style Transfer Using Convolutional Neural Networks",
      "authors": [
        "L Gatys",
        "A Ecker",
        "M Bethge"
      ],
      "year": "2016",
      "venue": "IEEE Xplore"
    },
    {
      "citation_id": "50",
      "title": "Random style transfer for person re-identification with one example",
      "authors": [
        "Y Li",
        "T Wang",
        "L Liu"
      ],
      "year": "2021",
      "venue": "AIMS Mathematics",
      "doi": "10.3934/math.2021277"
    },
    {
      "citation_id": "51",
      "title": "Line Drawings for Face Portraits From Photos Using Global and Local Structure Based GANs",
      "authors": [
        "R Yi",
        "M Xia",
        "Y.-J Liu",
        "Y.-K Lai",
        "P Rosin"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/tpami.2020.2987931"
    },
    {
      "citation_id": "52",
      "title": "Aesthetics of Neural Network Art",
      "authors": [
        "A Hertzmann"
      ],
      "year": "2019",
      "venue": "Aesthetics of Neural Network Art"
    },
    {
      "citation_id": "53",
      "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
      "authors": [
        "A Brock",
        "J Donahue",
        "K Deepmind",
        "Simonyan"
      ],
      "year": "2019",
      "venue": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS"
    },
    {
      "citation_id": "54",
      "title": "Image Processing Using Multi-Code GAN Prior",
      "authors": [
        "J Gu",
        "Y Shen",
        "B Zhou"
      ],
      "year": "2019",
      "venue": "Image Processing Using Multi-Code GAN Prior"
    },
    {
      "citation_id": "55",
      "title": "Generative Visual Manipulation on the Natural Image Manifold",
      "authors": [
        "J.-Y Zhu",
        "P Kr√§henb√ºhl",
        "E Shechtman",
        "A Efros"
      ],
      "year": "2018",
      "venue": "Generative Visual Manipulation on the Natural Image Manifold"
    },
    {
      "citation_id": "56",
      "title": "Interpreting the Latent Space of GANs for Semantic Face Editing Original Pose Age Gender Eyeglasses",
      "authors": [
        "Y Shen",
        "J Gu",
        "X Tang",
        "B Zhou"
      ],
      "year": "2019",
      "venue": "Interpreting the Latent Space of GANs for Semantic Face Editing Original Pose Age Gender Eyeglasses"
    },
    {
      "citation_id": "57",
      "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
      "authors": [
        "T Karras",
        "T Aila",
        "S Laine",
        "J Lehtinen"
      ],
      "year": "2018",
      "venue": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION"
    },
    {
      "citation_id": "58",
      "title": "Game of GANs: Game-Theoretical Models for Generative Adversarial Networks",
      "authors": [
        "M Moghadam"
      ],
      "year": "2022",
      "venue": "Game of GANs: Game-Theoretical Models for Generative Adversarial Networks"
    },
    {
      "citation_id": "59",
      "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "arXiv.org"
    },
    {
      "citation_id": "60",
      "title": "Deep Learning using Rectified Linear Units (ReLU),\" arXiv.org, 2018",
      "authors": [
        "Abien Agarap",
        "Fred"
      ],
      "venue": "Deep Learning using Rectified Linear Units (ReLU),\" arXiv.org, 2018"
    },
    {
      "citation_id": "61",
      "title": "Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution",
      "authors": [
        "A Lucas",
        "S Lopez-Tapia",
        "R Molina",
        "A Katsaggelos"
      ],
      "year": "2018",
      "venue": "Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution"
    },
    {
      "citation_id": "62",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A Method for Stochastic Optimization"
    },
    {
      "citation_id": "63",
      "title": "PyTorch",
      "authors": [
        "Pytorch"
      ],
      "venue": "PyTorch"
    },
    {
      "citation_id": "64",
      "title": "Abstract Art Gallery",
      "year": "2021",
      "venue": "Abstract Art Gallery"
    },
    {
      "citation_id": "65",
      "title": "Improved Techniques for Training GANs",
      "authors": [
        "T Salimans",
        "I Goodfellow",
        "W Zaremba",
        "V Cheung",
        "A Radford",
        "X Chen"
      ],
      "year": "2016",
      "venue": "arXiv.org"
    },
    {
      "citation_id": "66",
      "title": "The influence of color on prices of abstract paintings",
      "authors": [
        "M Borisov",
        "V Kolycheva",
        "A Semenov",
        "D Grigoriev"
      ],
      "year": "2022",
      "venue": "The influence of color on prices of abstract paintings"
    },
    {
      "citation_id": "67",
      "title": "Word Embeddings: A Survey",
      "authors": [
        "F Almeida",
        "G Xex√©o"
      ],
      "year": "2023",
      "venue": "Word Embeddings: A Survey"
    }
  ]
}