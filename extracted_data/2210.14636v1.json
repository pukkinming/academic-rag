{
  "paper_id": "2210.14636v1",
  "title": "Fast Yet Effective Speech Emotion Recognition With Self-Distillation",
  "published": "2022-10-26T11:28:29Z",
  "authors": [
    "Zhao Ren",
    "Thanh Tam Nguyen",
    "Yi Chang",
    "Björn W. Schuller"
  ],
  "keywords": [
    "self-distillation",
    "speech emotion recognition",
    "adaptive inference",
    "efficient deep learning",
    "efficient edge analytics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is the task of recognising human's emotional states from speech. SER is extremely prevalent in helping dialogue systems to truly understand our emotions and become a trustworthy human conversational partner. Due to the lengthy nature of speech, SER also suffers from the lack of abundant labelled data for powerful models like deep neural networks. Pretrained complex models on large-scale speech datasets have been successfully applied to SER via transfer learning. However, finetuning complex models still requires large memory space and results in low inference efficiency. In this paper, we argue achieving a fast yet effective SER is possible with self-distillation, a method of simultaneously fine-tuning a pretrained model and training shallower versions of itself. The benefits of our self-distillation framework are threefold: (1) the adoption of self-distillation method upon the acoustic modality breaks through the limited ground-truth of speech data, and outperforms the existing models' performance on an SER dataset; (2) executing powerful models at different depth can achieve adaptive accuracy-efficiency trade-offs on resource-limited edge devices; (3) a new fine-tuning process rather than training from scratch for self-distillation leads to faster learning time and the state-of-theart accuracy on data with small quantities of label information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) nowadays is an idiosyncratic task in many dialogue systems, such as Siri, Cortana, and Alexa  [1] . Through classifying human speech signals into various emotional states (e. g., happiness, surprise, anger, disgust, fear, sadness, neutral, etc.), SER helps human-computer systems become more personalised and trustworthy as well as adjust the contexts accordingly in car-driving, heath-diagnosis, call-center, aircraft-cockpit, and web/mobile applications  [2, 3] .\n\nExisting techniques for SER are limited by the inherent lack of labelled data due to the expensive efforts of annotation (e.g. thousands of hours of speech over nearly 7,000 spoken languages  [4] ). They often rely on large deep neural networks that are pre-trained by unsupervised learning, contrastive learning, or self-supervised learning, such as wav2vec  [5] , wav2vec 2.0  [4] , and vq-wav2vec  [6] . However, fine-tuning large models has a high demand of memory space and inference time  [7] .\n\nIn machine learning, self-distillation has emerged as a paradigm to develop a student model with a more lightweight architecture that can even outperform the teacher  [8] . This has been particularly successfully applied to computer vision  [8, 9] . However, in contrast to the visual modality, the acoustic modality is significantly more challenging due to limited ground-truth. Self-distillation methods cannot be applied directly to SER since they often require large labelled data to simultaneously train a teacher model from scratch with shallower student versions of itself  [8] .\n\nIn this paper, we present a framework of self-distillation for fast, yet effective speech emotion recognition. While our framework is demonstrated on wav2vec 2.0  [4]  (one of the state-of-the-art (SOTA) pre-trained models for speech representations), it can be applied to other models and datasets with limited ground-truth information. In our framework (see Figure  1 ), the pre-trained wav2vec 2.0 (i. e., the teacher model) was fine-tuned together with shallower model parameters from itself (i. e., the student models), when the teacher and all students are predicting emotional states from speech samples.\n\nTo the best of our knowledge, this is the first attempt to develop a self-distillation framework for SER. The contributions of our selfdistillation framework include: (1) the application of self-distillation on speech data overcomes the difficulty caused by limited annotations, and outperforms the existing models' performance on an SER dataset;  (2)  executing powerful models at different depths increases the possibility to achieve adaptive accuracy-efficiency trade-offs on resource-limited edge devices; (3) a new fine-tuning process rather than training from scratch for self-distillation leads to faster learning time and SOTA accuracy on data with limited ground-truth. Related Works. Spectrum features  [10, 11]  have been often used as the input of deep neural networks for SER  [12] , while selecting the appropriate spectrum features is a time-consuming work. Moreover, the performance of SER is limited to expensive human annotations; lacking of labelled data for deep learning. More recently, selfsupervised learning on speech data has shown promising to learn effective representations, and the pre-trained models have been successfully fine-tuned for SER tasks  [13] [14] [15] . Therefore, we apply an end-to-end self-supervised learning model, wav2vec 2.0, to SER.\n\nKnowledge distillation is one of the popular methods to achieve high efficiency by transferring knowledge from a teacher model to a smaller student model  [7] . Similar to other model compression approaches such as pruning and quantisation, they sacrifice information loss (thus accuracy) and could not overcome the accuracy-efficiency trade-offs. Our self-distillation approach can achieve the best of both worlds by reusing the architecture and allowing inference at different depths of the teacher model itself.\n\nDifferent types of self-distillation methods have been developed recently, including iteration-based  [16, 17] , aggregate-based  [18] , and branch-based approaches  [8, 9] . Iteration-based methods perform knowledge distillation from a teacher model to a student model with the same architecture and this procedure is repeated a few times  [16, 17] . However, the training and inference costs are not reduced, as the teacher and the student are the same. Aggregatedbased methods use data augmentation to produce more versions of the teacher model on different augmentations and then combine the outputs  [18] . However, existing data augmentations are domainspecific and are not applicable to acoustic modality. Our work relates closely to the branch-based approaches, which add branches at different depths of the teacher model using bottlenecks/attentionblocks and shallow classifiers  [8, 9] . However, these layers are not applicable for wave2vec 2.0, as it already contains transformer layers. Our work is also different from the layer-wise knowledge distillation, which fine-tunes a student model from the teacher model itself for predicting deep layers of the teacher  [7] . The layer-wise knowledge distillation can produce a general student model, while it requires fine-tuning efforts for a specific task. The fixed number of model parameters of the student is limited for performance improvement with a deeper structure and lacks of flexibility.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminaries",
      "text": "Self-supervised Learning with wav2vec 2.0. Self-supervised learning has shown its superiority compared to supervised learning on many audio tasks, such as speaker recognition  [19]  and SER  [20] . Wav2vec 2.0  [4]  was trained on the large-scale Librispeech corpus  [21]  in a self-supervised learning framework. Wav2vec 2.0 has been widely used to extract effective representations for SER tasks  [13, 22] . A Wav2vec 2.0 model consist of multi-layer convolutional neural networks (CNNs) (i. e., encoder) and multiple transformer layers (i. e., context network). The latent representations learnt from the encoder are discretised into a set of quatisation representations, which are then processed with the output of the context network in a contrastive task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Distillation Framework:",
      "text": "The Case of Wav2vec 2.0",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "Teacher Model. We assume the input data of wav2vec 2.0 is represented as (X, y), where X is the raw speech signals and y is the emotional states. The final (i. e., N -th) transformer layer TN of wav2vec 2.0 is followed by two linear layers (L 1 and L 2 ) with output dimensions of D1 and D2, where D2 is the number of emotional classes (see Figure  1 ). Regarding the intermediate features, the output of each transformer layer has a dimension of (B, F, R), where B is the sample number in a batch, F represents the number of time steps, and R denotes the dimension of representations at each time step. With the goal of classification, the N -th transformer layer's output is pooled into HN with a dimension of (B, R) before being fed into the two linear layers. Student Model. To reduce the model parameters of wav2vec 2.0 with self-distillation, additional layers are added after the intermediate transformer layers of wav2vec 2.0 (see Figure  1 ). In a student ). Mai is a neural network, and L 2 ai is trained for predicting emotional classes. Herein, as Mai is expected to learn representations similar to those from TN , the output of Tai is directly fed into the block model without pooling. Therefore, the output of Mai has a dimension of (B, F, R), and is pooled into HM ai with a dimension of (B, R). HM ai is then fed into L 1 ai for further process. Apart from the above single distillation model, multiple distillation models could be learnt together in self-distillation to improve the flexibility for different depths of models. For instance, two distillation models in Figure  1  are developed after transformer layers Tai and Taj to build SER models with different numbers of model parameters.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Loss Function",
      "text": "As the pre-trained wav2vec 2.0 has already strong capability of learning representations from speech, we initialise the teacher model's parameters with the pre-trained wav2vec 2.0. We assume Na distillation models are built after transformer layers indexed by A = {a1, a2, ...}. The representations HL and HL ai are learnt from L 1 and L 1 ai , respectively. The outputs of the second linear layers L 2 and L 2 ai are represented as O and Oai. The model parameters are optimised with the loss function in self-distillation:\n\nwhere Lc is the cross-entropy loss, L k is the Kullback-Leibler (KL) divergence loss, Ls is the similarity loss, and α and β are constant values.\n\nCross-entropy Loss. To train wav2vec 2.0 and distillation models for performing SER, the cross-entropy loss contains i) the cross entropy loss on the teacher model (i. e., L 1 , L 2 , and wav2vec 2.0), and ii) the cross entropy loss on the student models (i. e., distillation models and partial model parameters of wav2vec 2.0):\n\nwhere Lce is the typical cross entropy loss for training a model in supervised learning, and γ is a constant value.\n\nKL Loss. The outputs of the distillation models are expected to be similar to that of the linear layer L 2 , which is the final layer of the teacher model. With this target, the Kullback-Leibler (KL) loss aims to regularise the outputs O and Oai:\n\nSimilarity Loss. Apart from the loss functions computed on the model outputs, loss functions on the interval features learnt from the intermediate layers can further help train strong student models.\n\nIn this work, we compare three loss functions, including L1, L2, and cosine similarity. Their combinations are also compared with single functions. Furthermore, these loss functions could be either on the outputs of the N -th transformer layer and the blocks (HN and HM ai ), or on the output of the first linear layers (HL and HL ai ):\n\nwhere L sim is a loss function of L1, L2 or negative cosine similarity.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Inference",
      "text": "Although the deep layers of wav2vec 2.0 can often learn higherlevel reresentations than shallow ones, self-distillation can provide dynamic inference models  [9]  via training both shallow and deep student models with good performance. Shallow distillation models require less parameters than deep ones, and deep ones may perform better than shallow ones. The flexibility of self-distillation enables SER applications to be applied to various hardwares, from wearable devices to work stations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "The database of elicited mood in speech (DEMoS)  [23]  is used to verify the self-distillation for SER. DEMoS with 9 365 emotional and 332 neutral Italian speech samples was collected from 68 speakers (f: 23, m: 45)  [23] . Each speech sample was annotated with one of the eight classes: anger, disgust, fear, guilt, happiness, sadness, surprise, and neutral. To implement experiments that can be compared to other studies  [24, 25]  on DEMoS, the minor neutral class is not used, and the speaker-independent training/development/test sets are the same to our prior study  [24] . The detail of the data distribution on the seven emotional classes can be found in  [24] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings",
      "text": "Evaluations Metrics. In this study, the unweighted average recall (UAR) is employed to evaluate the performance of SER models. A UAR is computed as the average of all class-wise recalls.\n\nImplementation Details. With the goal of classifying emotion states, the wav2vec 2.0 model is followed by two linear layers with the numbers of output neurons {256, 7}, respectively. In each distillation model, the block could be one of the three layers: CNN (number of output channels: 1, kernel size: (1, 1)), Long Short-Time Memory recurrent neural networks (LSTM-RNN) (number of output   All training procedures of self-distillation are optimised by an Adam optimiser with a learning rate of 3e -5, and stopped at the 20-th epoch, when the batch size is 16. Reproducibility Environments. To improve the reproducibility, the code of this work is released at: https://github.com/ leibniz-future-lab/SelfDistill-SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sensitivity Analysis",
      "text": "Figure  2  shows the performance of self-distillation with single CNNbased distillation model. We can see that, on both development set (Figure  2 (a) ) and test set (Figure  2 (b) ), the performance is increasing when the distillation layer is going deeper. This indicates that deeper model layers of wav2vec 2.0 can learn more abstract representations than shallower ones. The teacher models perform better than shallow student models, but are comparable with deep student models, especially after the 7-th distillation layer.\n\nAs the distillation layers at the embedding level perform slightly better than those at the linear level, the embedding level is selected in the next experiments for self-distillation. To provide different model sizes with self-distillation, we group the distillation layers into three groups and select layers which have the best performance on the development set. Therefore, we use layers {3, 8, 10} for multi-layer self-distillation in the following experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "With the distillation layers {3, 8, 10}, we compare the three block models (i. e., CNN, LSTM-RNN, and GRU-RNN) with various similarity loss functions (i. e., L sim ). From Table  2 , we can find that all similarity loss functions perform comparably for each block model.\n\nParticularly, the single similarity loss functions (i. e., L1, L2, and cosine similarity) perform better than the combinations of them. This might be related to the setting of hyperparameters in the loss functions. Furthermore, the LSTM-RNN and the GRU-RNN models outperform the CNN one when comparing the three models blocks. This may be because RNNs can better learn sequential information than CNNs. Regarding the self-distillation, the performance of the student models at layers 8 and 10 is comparable with the deepest model. Layer 3 performs slightly worse than layers 8 and 10, as the corresponding student model of layer 3 is shallower than those of layers 8 and 10. Finally, the fusion results of the three distillation models are comparable with the results of layer 10.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Sota",
      "text": "We compared the results of self distillation with the other SOTA models, including the following three groups of models. (1) CNN-4, VGG-16, ResNet-50, and VGG-16 with adversarial training are trained from scratch  [24] . (2) The models of wav2vec 2.0 with finetuning are trained for 20 epochs based on the pre-trained wav2vec 2.0, when the transformer layers after finetuned layers are frozen.\n\n(3) The layer-wise models are trained via the layer-wise knowledge distillation in  [7] . As wav2vec 2.0 has 12 transformer layers, which is the same as the number of encoder layers in HuBERT in  [7] , the layer-wise models are mostly developed according to the settings in  [7] . The teacher model is the pre-trained wav2vec 2.0, and the student model is part of the pre-trained wav2vec 2.0 itself (from the first layer to the second transformer layer). Notably, all parameters of the teacher model are frozen. The layer-wise models are trained at two stages: i) training the student model to predict layers {4, 8, 12}) of the teacher model, and ii) fine-tuning the student model on DEMoS for SER. To train a strong student model, the first stage is trained with 20 epochs. To implement fair experimental comparisons, the second stage also consists of 20 epochs. As wav2vec 2.0 was pre-trained on the large-scale speech database, The models based on wav2vec 2.0 are mostly better than models trained from scratch (CNN-4, VGG-16 (+ adversarial training), and ResNet-50). When comparing fine-tuned wav2vec 2.0 models and self-distillation, the student model at layer 3 outperforms the corresponding fine-tuned one. The fine-tuned models are comparable with self-distillation at layers 8 and 10, while self-distillation trains student models at different layers in single training procedure. Our self-distillation outperforms layer-wise distillation at all three layers. This may be caused by the shallower student models (encoder and two transformer layers) in layer-wise distillation. For a specific task, self-distillation requires less training epochs (20 in our work) than layer-wise distillation (40 in our study), increasing the training efficiency.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This work aimed to reduce model parameters via self-distillation for fast and effective speech emotion recognition. The experiments were implemented on the Database of Elicited Mood in Speech (DE-MoS)  [23]  with the pre-trained wav2vec 2.0. The experimental results demonstrated that the student model at a shallow layer (layer 3) outperformed the corresponding fine-tuned wav2vec 2.0, and selfdistillation achieved comparable performance with that of fine-tuned wav2vec 2.0 at deep layers. Moreover, self-distillation performed better than layer-wise knowledge distillation. In future work, the self-distillation approach will be verified on multiple databases. We will also investigate to further reduce the wav2vec 2.0 model by the state-of-the-art model compression approaches  [26, 27] .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), the pre-trained wav2vec 2.0 (i. e., the",
      "page": 1
    },
    {
      "caption": "Figure 1: ). Regarding the intermediate features,",
      "page": 2
    },
    {
      "caption": "Figure 1: ). In a student",
      "page": 2
    },
    {
      "caption": "Figure 1: The framework of self-distillation on wav2vec 2.0.",
      "page": 2
    },
    {
      "caption": "Figure 1: are developed after transformer layers",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison of the performance (UAR [%]) of the teacher",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the performance of self-distillation with single CNN-",
      "page": 3
    },
    {
      "caption": "Figure 2: (a)) and test set (Figure 2 (b)), the performance is increas-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Comparison of the performance (UAR[%]) between our",
      "data": [
        {
          "NN": "",
          "Loss": "",
          "Deepest": "Devel",
          "Layer 3": "Devel",
          "Layer 8": "Devel",
          "Layer 10": "Devel",
          "Fusion": "Devel"
        },
        {
          "NN": "Self-distillation w/ CNN",
          "Loss": "L1\nL2\nCosine sim.\nL1 + Cosine sim.\nL2 + Cosine sim.",
          "Deepest": "87.3\n88.3\n89.6\n88.4\n88.1",
          "Layer 3": "75.2\n70.2\n76.0\n77.4\n77.5",
          "Layer 8": "86.9\n88.6\n89.5\n88.4\n88.8",
          "Layer 10": "87.1\n88.2\n89.7\n88.4\n88.4",
          "Fusion": "85.7\n86.8\n88.7\n87.5\n87.4"
        },
        {
          "NN": "Self-distillation w/ LSTM",
          "Loss": "L1\nL2\nCosine sim.\nL1 + Cosine sim.\nL2 + Cosine sim.",
          "Deepest": "91.4\n90.7\n90.8\n90.3\n91.8",
          "Layer 3": "83.3\n78.5\n79.2\n77.7\n83.3",
          "Layer 8": "91.7\n91.0\n91.1\n90.1\n91.1",
          "Layer 10": "91.8\n90.8\n90.8\n90.3\n91.7",
          "Fusion": "90.5\n89.6\n90.1\n88.8\n90.1"
        },
        {
          "NN": "Self-distillation w/ GRU",
          "Loss": "L1\nL2\nCosine sim.\nL1 + Cosine sim.\nL2 + Cosine sim.",
          "Deepest": "86.3\n90.9\n90.2\n91.2\n88.1",
          "Layer 3": "80.2\n82.0\n74.8\n79.5\n72.2",
          "Layer 8": "84.7\n91.2\n90.3\n91.4\n87.5",
          "Layer 10": "84.5\n91.1\n89.9\n91.0\n86.5",
          "Fusion": "86.1\n90.0\n88.7\n89.5\n86.8"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Comparison of the performance (UAR[%]) between our",
      "data": [
        {
          "NN": "CNN-4 [24]\nVGG-16 [24]\nResNet-50 [24]\nVGG-16 + adversarial training [24]\nWav2vec2 (layer 3) + ﬁne-tuning\nWav2vec2 (layer 8) + ﬁne-tuning\nWav2vec2 (layer 10) + ﬁne-tuning\nWav2vec2 (deepest) + ﬁne-tuning\nLayer-wise distillation w/ CNN\nLayer-wise distillation w/ LSTM\nLayer-wise distillation w/ GRU",
          "Devel": "82.6\n79.8\n71.9\n87.5\n77.1\n90.1\n91.7\n91.1\n54.3\n70.8\n73.1",
          "Test": "83.6\n83.6\n81.3\n86.7\n83.4\n90.9\n89.2\n90.6\n72.5\n79.0\n77.4",
          "#Param": "4.3 M\n14.7 M\n23.5 M\n14.7 M\n31.5 M\n66.9 M\n81.1 M\n95.2 M\n24.4 M\n38.5 M\n35.0 M"
        },
        {
          "NN": "Self-distillation (layer 3)\nSelf-distillation (layer 8)\nSelf-distillation (layer 10)\nSelf-distillation (teacher)",
          "Devel": "83.3\n91.7\n91.8\n91.8",
          "Test": "85.7\n90.7\n90.5\n91.4",
          "#Param": "36.2 M\n71.6 M\n85.8 M\n100.0 M"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS, Vancouver"
    },
    {
      "citation_id": "6",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "7",
      "title": "vqwav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "ICLR, Virtual"
    },
    {
      "citation_id": "8",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "Heng-Jui Chang",
        "Shu-Wen Yang",
        "Hung-Yi Lee"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Be your own teacher: the performance of convolutional neural networks via self distillation",
      "authors": [
        "Linfeng Zhang",
        "Jiebo Song",
        "Anni Gao",
        "Jingwei Chen",
        "Chenglong Bao",
        "Kaisheng Ma"
      ],
      "year": "2019",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "10",
      "title": "Selfdistillation: Towards efficient and compact neural networks",
      "authors": [
        "Linfeng Zhang",
        "Chenglong Bao",
        "Kaisheng Ma"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Ssast: Self-supervised audio spectrogram transformer",
      "authors": [
        "Yuan Gong",
        "-I Cheng",
        "Yu-An Lai",
        "James Chung",
        "Glass"
      ],
      "venue": "Proc. AAAI, Virtual, 2022"
    },
    {
      "citation_id": "12",
      "title": "A multimodal hierarchical approach to speech emotion recognition from audio and text",
      "authors": [
        "Prabhav Singh",
        "Ridam Srivastava",
        "Rana Kps",
        "Vineet Kumar"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "13",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "Taiba Majid Wani",
        "Teddy Surya Gunawan",
        "Syed Asif",
        "Ahmad Qadri",
        "Mira Kartiwi",
        "Eliathamby Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "16",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "arxiv": "arXiv:2203.07378"
    },
    {
      "citation_id": "17",
      "title": "Self-distillation amplifies regularization in hilbert space",
      "authors": [
        "Hossein Mobahi",
        "Mehrdad Farajtabar",
        "Peter Bartlett"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "18",
      "title": "Revisiting self-distillation",
      "authors": [
        "Minh Pham",
        "Minsu Cho",
        "Ameya Joshi",
        "Chinmay Hegde"
      ],
      "year": "2022",
      "venue": "Revisiting self-distillation",
      "arxiv": "arXiv:2206.08491"
    },
    {
      "citation_id": "19",
      "title": "Selfsupervised label augmentation via input transformations",
      "authors": [
        "Hankook Lee",
        "Sung Hwang",
        "Jinwoo Shin"
      ],
      "year": "2020",
      "venue": "Proc. ICML, Virtual"
    },
    {
      "citation_id": "20",
      "title": "Fine-tuning wav2vec2 for speaker recognition",
      "authors": [
        "Nik Vaessen",
        "David Van Leeuwen"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP, Singapore"
    },
    {
      "citation_id": "21",
      "title": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "authors": [
        "Omar Mohamed",
        "A Salah",
        "Aly"
      ],
      "year": "2021",
      "venue": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "arxiv": "arXiv:2110.04425"
    },
    {
      "citation_id": "22",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Real-time end-to-end speech emotion recognition with cross-domain adaptation",
      "authors": [
        "Konlakorn Wongpatikaseree",
        "Sattaya Singkul",
        "Narit Hnoohom",
        "Sumeth Yuenyong"
      ],
      "year": "2022",
      "venue": "Big Data and Cognitive Computing"
    },
    {
      "citation_id": "24",
      "title": "DEMoS: An Italian emotional speech corpus",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Giovanni Costantini",
        "Anton Batliner",
        "Maximilian Schmitt",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "DEMoS: An Italian emotional speech corpus"
    },
    {
      "citation_id": "25",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Alice Zhao Ren",
        "Jing Baird",
        "Zixing Han",
        "Björn Zhang",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "26",
      "title": "Enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
      "authors": [
        "Jing Zhao Ren",
        "Nicholas Han",
        "Björn Cummins",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "27",
      "title": "Deep model compression and architecture optimization for embedded systems: A survey",
      "authors": [
        "Anthony Berthelier",
        "Thierry Chateau",
        "Stefan Duffner",
        "Christophe Garcia",
        "Christophe Blanc"
      ],
      "year": "2021",
      "venue": "Journal of Signal Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Towards model compression for deep learning based speech enhancement",
      "authors": [
        "Ke Tan",
        "Deliang Wang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    }
  ]
}