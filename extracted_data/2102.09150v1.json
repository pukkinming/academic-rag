{
  "paper_id": "2102.09150v1",
  "title": "An Enhanced Adversarial Network With Combined Latent Features For Spatio-Temporal Facial Affect Estimation In The Wild",
  "published": "2021-02-18T04:10:12Z",
  "authors": [
    "Decky Aspandi",
    "Federico Sukno",
    "Björn Schuller",
    "Xavier Binefa"
  ],
  "keywords": [
    "Affective Computing",
    "Temporal Modelling",
    "Adversarial Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective Computing has recently attracted the attention of the research community, due to its numerous applications in diverse areas. In this context, the emergence of video-based data allows to enrich the widely used spatial features with the inclusion of temporal information. However, such spatio-temporal modelling often results in very high-dimensional feature spaces and large volumes of data, making training difficult and time consuming. This paper addresses these shortcomings by proposing a novel model that efficiently extracts both spatial and temporal features of the data by means of its enhanced temporal modelling based on latent features. Our proposed model consists of three major networks, coined Generator, Discriminator, and Combiner, which are trained in an adversarial setting combined with curriculum learning to enable our adaptive attention modules. In our experiments, we show the effectiveness of our approach by reporting our competitive results on both the AFEW-VA and SEWA datasets, suggesting that temporal modelling improves the affect estimates both in qualitative and quantitative terms. Furthermore, we find that the inclusion of attention mechanisms leads to the highest accuracy improvements, as its weights seem to correlate well with the appearance of facial movements, both in terms of temporal localisation and intensity. Finally, we observe the sequence length of around 160 ms to be the optimum one for temporal modelling, which is consistent with other relevant findings utilising similar lengths. a",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing has recently attracted the attention of the research community, due to its numerous applications in diverse areas which include education  (Duo and Song, 2010)  or healthcare  (Liu et al., 2008) , among others. The growing availability of affect-related datasets, such as AFEW-VA  (Kossaifi et al., 2017)  and the recently introduced SEWA (Kossaifi et al., 2019) database enable the rapid development of deep learning-based techniques, which currently hold the state of the art.\n\nFurther, the emergence of video-based data allows to enrich the widely used spatial features with the inclusion of temporal information. To this end, several authors have explored the use of long-short term memory (LSTM) recurrent neural networks (RNNs)  (Tellamekala and Valstar, 2019; Ma et al., 2019) , endowed also with attention mechanisms  (Luong et al., 2015; Li et al., 2020; Xiaohua et al., 2019) . However, such spatio-temporal modelling often results in very high-dimensional feature spaces and large volumes of data, making training difficult and time consuming. Moreover, it has been shown that the sequence length considered during training can be a decisive factor for successful temporal modelling  (Kossaifi et al., 2017; Xia et al., 2020; Farhadi and Fox, 2018; Aspandi et al., 2019b) , and yet a detailed study of this aspect is lacking in the field. This paper addresses both the lack of incorporation and analysis of temporal modelling on affective analysis. We propose a novel model which can be efficiently used to extract both spatial and temporal features of the data by means of its enhanced temporal modelling based on latent features. We do so by incorporating three major networks, coined Generator, Discriminator, and Combiner, which are trained in an adversarial setting to estimate the affect domains of Valence (V) and Arousal (A). Furthermore, we capitalise on these latent features to enable temporal modelling using LSTM RNNs, which we train progressively using curriculum learning enhanced with adaptive attention. Specifically, the contributions of this paper are as follows:\n\n(a) We upgrade the standard adversarial setting, consisting of a Generator and a Discriminator, with a third network that combines the features from these networks, which are modified accordingly. This yields features that combine the latent space from the autoencoder-based Generator and a V-A Quadrant estimate produced by the modified Discriminator, resulting in a compact but meaningful representation that helps reduce the training complexity.\n\n(b) We propose the use of curriculum learning to enable analysis and optimisation of the temporal modelling length.\n\n(c) We incorporate dynamic attention to further enhance our model estimates and show its effectiveness by reporting state of the art accuracy on both the AFEW-VA and SEWA datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Affective Computing started by exploiting the use of classical machine learning techniques to enable automatic affect estimation. Examples of early approaches include partial least squares regression  (Povolny et al., 2016) , and support vector machines  (Nicolaou et al., 2011) . Subsequently, to further progress the investigations in this field, the development of larger and bigger datasets was addressed. Several datasets have been introduced so far, starting with SEMAINE  (McKeown et al., 2010) , AFEW-VA  (Kossaifi et al., 2017) , RECOLA  (Ringeval et al., 2013) , OMG  (Barros et al., 2018) , AffectNet  (Mollahosseini et al., 2015) , and more recently SEWA  (Kossaifi et al., 2019) , aff-wild  (Kollias et al., 2019; Zafeiriou et al., 2017) , and aff-wild2  (Kollias and Zafeiriou, 2019; Kollias et al., 2020) . Furthermore, the V-A labels have become the standard emotional dimensions over time, as opposed to hard emotion labels, given their continuous nature  (Kossaifi et al., 2017; Kossaifi et al., 2019) . Throughout the last few years, models based on Deep Learning have emerged and currently hold the state of the art in the context of affective analysis, given their ability to learn from large scale data. A recent example along this line is the work from Mitenkova et al.  (Mitenkova et al., 2019) , who introduce tensor modelling for affect estimations by using spatial features. In their work, they use tucker tensor regression optimised by means of deep gradient methods, thus allowing to preserve the structure of the data and reduce the number of parameters. Other works, such as  (Handrich et al., 2020) , adopt the multi-task approach to simultaneously address face detection and affective states prediction. Specifically, they use YOLO-based CNN models  (Huang et al., 2018)  to estimate the facial locations alongside V-A values through their proposed combined losses. As such, their models are able to incorporate the characteristics of facial attributes and estimate their relevance to affect inferences.\n\nThe recent growth of video-based datasets has encouraged the inclusion of temporal modelling, which has shown to improve models' training  (Xie et al., 2016; Cootes et al., 1998) . Relevant examples in Affective Computing include the works of Tellamekala et al.  (Tellamekala and Valstar, 2019)  and  Ma et al. (Ma et al., 2019) . In their work, Tellamekala et al.  (Tellamekala and Valstar, 2019)  enforce temporal coherency and smoothness aspects on their feature representation by constraining the differences between adjacent frames, while Ma et al. resort to the utilisation of LSTM RNNs with residual connections applied to multi-modal data. Furthermore, the use of attention has also been recently explored by Xiaohua et al.  (Xiaohua et al., 2019)  and  Li et al. (Li et al., 2020) . Xiahoua et al. adopt multi-stage attention, which involves both spatial and temporal attention, on their facial based affect estimations. Meanwhile, using spectrogram data as input,  Li et al.  propose a deep network that utilises an attention mechanism  (Luong et al., 2015)  on top of their LSTM networks to predict the affective states.\n\nUnfortunately, to our knowledge, all previous works involving temporal modelling on affective computing miss one important aspect of the analysis: the involved sequence length in their training. While the specified length of temporal modelling has been shown to affect the final results on other related facial analysis tasks  (Kossaifi et al., 2017; Xia et al., 2020; Farhadi and Fox, 2018; Aspandi et al., 2019b) , the computational cost required to train large spatio-temporal models hampers one to address such analysis. However, these problems could be mitigated by: 1) the use of progressive sequence learning to permit stepwise observations of various sequence lengths; this approach has been shown in the recent work of (Aspandi et al., 2019b) on facial landmark estimations, which uses curriculum learning enabling more robust training analysis and tuning of the temporal length; 2) the use of reduced feature sizes, enabling more effi- cient training process  (Comas et al., 2020) ; this has been explored in the affective computing field by the recent works such as  (Aspandi et al., 2020) , which uses generative modelling to extract a latent space of representative features. These two aspects have inspired us to propose the combined models presented in this work, as explained in the next section.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Figure  1  shows the overview of our proposed models, which consist of three networks: a Latent Feature Extractor (acting as Generator, G), a Quadrant Estimator (or Discriminator, D), and a Valence/Arousal Estimator (or Combiner, C). Given input image I which contains the facial area, both G and D will be responsible to learn low dimensional features that the combiner will use to estimate the associated Valence (V) and Arousal (A) state θ. The architecture of both the G and D networks follows the recent work from  (Aspandi et al., 2020) , and we propose to use LSTM enhanced with attention to create our C network. We proposed two main architecture variants: the ANCLaF network (left part of Figure  1 ), which uses single images as input and estimates V and A values independently for each frame, and ANCLaF-S and ANCLaF-SA (right part of Figure  1 ) that uses sequences of latent features extracted from n frames as input, and utilises LSTM RNNs for the inference (-S), optionally combined with internal attention layers (-SA).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Adversarial Network With",
      "text": "Combined Latent Features (ANCLaF)\n\nThe pipeline of our base model ANCLaF starts with the G network. It receives either the original input image I, or a distorted version of it, Ĩ, as detailed in  (Aspandi et al., 2019c; Aspandi et al., 2019a) . It simultaneously produces the cleaned reconstruction of the input image Î and a 2D latent representation that will be used as features (Z):\n\nwhere Φ are the parameters of the respective networks, enc and dec are the encoder and decoder, respectively. Subsequently, the D network receives Î and tries to estimate whether it was obtained from a true or fake example (namely, original or distorted input image), as well as a rough estimate of the affective state. In contrast with the formulation in  (Aspandi et al., 2020) , in which D targets directly the intensity of V and A, we propose to base the estimated affect on the circumplex quadrant (Q)  (Russell, 1980)  which discretises emotions along the valence and arousal dimensions (four quadrants). This, in turn, reduces the training complexity. Thus, letting FC stand for fully connected layer:\n\nThen, Q is used to condition the extracted latent features Z through layer-wise concatenation, which we call ZQ  (Dai et al., 2017; Ye et al., 2018) . Given these conditioned latent features, the C network performs the final stage of affect estimation, producing refined predictions of both V and A  (Lv et al., 2017; Triantafyllidou and Tefas, 2016; Aspandi et al., 2019b) . Thus, if θ denote the estimated V and A:\n\n(3)",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Attention Enhanced Sequence Latent Affect Networks",
      "text": "We propose two sequence-based variants of our models: ANCLaF-S and -SA. Both of them use the combined features extracted by the G and D networks ZQ, which are fed to LSTM networks to allow for temporal modelling  (Hochreiter and Schmidhuber, 1997)  and complemented with an FC layer to produce the final estimates. These networks are trained using curriculum learning  (Bengio et al., 2009; Farhadi and Fox, 2018; Aspandi et al., 2019b) , in which the number of frames is progressively increased, allowing more throughout analysis of the training progress. Moreover, the training outcome for a given length facilitates the subsequent training of larger sequences  (Farhadi and Fox, 2018) . In this work, we considered a series of 2, 4, 8, 16, and 32 successive frames (N = {2, 4, 8, 6, 32}) for both training and inference stages. Depending on the number of frames to take into account (n), we use ANCLaF-S-n and ANCLaF-SA-n to name the respective variants of both ANCLaF-S and ANCLaF-SA networks. Lastly, the main difference between the two sequence models is that ANCLaF-SA also includes internal attentional modelling using the current and previous internal states from the LSTM layers. Thus, V-A predictions of ANCLaF-S-n are:\n\n)), (4) where LSTM is the Long Short Term Memory network  (Hochreiter and Schmidhuber, 1997) , and h n are LSTM states (h) after n successive frames. Built upon ANCLaF-SA, we further use attention modelling  (Luong et al., 2015)  to enable adaptive weights on model inferences by calculating the context vectors (C) that summarise the importance of each previous state h. Differently from the original method, however, here, we also propose to include both the LSTM inner state (c) and outgoing states (h)  (Kim et al., 2018)  to provide the full previous information, and also to adapt these techniques to only consider n previous states following our curriculum learning approach. Hence, given the combined LSTM states at frame t, denoted (S t = [c t , h t ]), and n previous states ( S), the alignment score is calculated as:\n\n.\n\nThen, the location-based function computes the align-ment scores from the previous states ( S):\n\nGiven the alignment vector, it is used to compute the context vector C t as the weighted average over the considered n previous hidden states:\n\nFinally, the context vector is concatenated with the current ZQ to be used as input to the C network pipeline:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Losses",
      "text": "We\n\nWe use similar L a f c losses as in  (Aspandi et al., 2020) , which incorporates multiple affect metrics: Rooted Mean Square Error (RMSE) (Eq. 11), Correlation(COR) (Eq. 12), Concordance Correlation Coefficients (CCC) (Eq. 13), and  (Kossaifi et al., 2017)  with the addition of Intra-class Correlation Coefficient (ICC)  (Kossaifi et al., 2019) . Thus, with { θ,θ} as the predicted and the ground truth V-A values, the L a f c is defined as follows:\n\nwhere f i is the total number of instances of discrete V-A classes i, and F is a normalisation factor  (Aspandi et al., 2019a)  for the total V-A classes (discretised by a value of 10). This normalisation factor is crucial in cases of large imbalance in the number of instances per class, like in the AFEW-VA dataset (see Section 4.1).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Training",
      "text": "We use both the AFEW  (Kossaifi et al., 2017)  and SEWA  (Kossaifi et al., 2019)  datasets to train all our model variants, by following their original subjectindependent protocol (5-fold cross validation). We conducted two training stages for each of our proposed models. Firstly, we trained the G, D, and C networks simultaneously using adversarial loss as indicated in Equation  9 . This training stage produced our baseline results without any sequential modelling, and conditional latent features ZQ to be used for the next stages of ANCLaF-S(A) Training.\n\nIn the second stage, The training of both ANCLaF-S and ANCLaF-SA was performed using the combined latent and quadrant features, under the previously defined curriculum learning scheme. We progressively train our ANCLaF-S models from 2, 4, 8, 16 to 32 steps of temporal modelling with multi-stage transfer learning  (Christodoulidis et al., 2016) . Subsequently, we add our proposed attention mechanism to the pre-trained ANCLaF-S models, thus obtaining our ANCLaF-SA models. In both cases, we optimise the affect loss defined in Equation 10 with the same experimental settings used to train ANCLaF.\n\nWe need to note that our combined training setup translates to more than 100 experiments in total. Hence, the use of latent features (known as a good choice to achieve reduced dimensionality representations) is critical to speed up our training process and make our experiments feasible. We observed a saving up to 1 : 4 of the original times during training each of our models by using the extracted latent features, with respect to using the original image (around 12 hours versus 2 days) on a single NVIDIA Titan X GPU. Full definitions of our models can be found in the respective online source code 1 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets And Experiment Settings",
      "text": "To quantify the impact of our temporal modelling, we opted to use two of the most popular and ac-1 https://github.com/deckyal/Seq-Att-Affect cessible video datasets available: Acted Facial Expressions in the Wild (AFEW-VA)  (Kossaifi et al., 2017)  and Automatic Sentiment Analysis in the Wild (SEWA)  (Kossaifi et al., 2019) . On the one hand, AFEW-VA has more individual examples (600 versus 538) than SEWA, however, the latter has more frame examples, more contextual information (such as subject, id of the associated culture) and is more balanced in terms of V-A labels  (Mitenkova et al., 2019) . Furthermore, both datasets contain in the wild situations, enabling real time model evaluations. Finally, the labels provided are in the form of continuous V-A values, together with additional facial landmark locations that we refined further using other external models  (Aspandi et al., 2019b)  to obtain more stable detection of the facial area.\n\nIn each experiment, we provide the results from all variants of our models to highlight the contribution of each module: first, we evaluate the ANCLaF model, which operates by exclusively using the latent features extracted on each frame (ZQ) without any temporal modelling. Then, we provide results from both ANCLaF-S and ANCLaF-SA, which incorporate temporal modelling (and attention in the case of -SA). We report both RMSE and COR results, on both datasets, adding also ICC and CCC metrics for the AFEW-VA and SEWA datasets, respectively, to facilitate quantitative analysis to other results reported in the literature. Finally, for fair comparisons, we compare our models against external results which followed similar experimental protocols, i. e., using exclusively this dataset in their training stage.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparative Results",
      "text": "Table  1  and table 2 provide the full comparisons of our proposed models against other reported results for both the AFEW-VA and SEWA datasets, respectively. We can identify several findings based on these results: Firstly, that our base ANCLaF model, relying on a single image at a time, can produce quite competitive accuracy compared to other results from the literature. Furthermore, its accuracy is also higher than the results from the original AEG-CD-SZ models in which it is based upon  (Aspandi et al., 2020) , as shown by its higher accuracy on the SEWA datasets, especially for Valence. This may indicate its better processing capabilities of the visual features, considering that AEG-CD-SZ also incorporates audio features, which in a way also explains its higher accuracy on the prediction of Arousal.\n\nSecondly, we notice a slight accuracy improvement when our models incorporate sequence modelling (ANCLaF-S), especially in terms of correlations,  namely, concordeance corelation coefficient (CCC), and ICC. This finding demonstrates the benefit of the temporal modelling, yielding more stable results than those achieved by ANCLaF (cf. Section 4.3). However, even though the overall accuracy of ANCLaF-S is better than that of ANCLaF (and quite comparable to other state of the art models), the improvement can be considered modest, especially if we compare it with the improvement achieved when we include attention in our models. Indeed, we can see that our ANCLaF-SA outperforms almost all compared models across the different affect metrics. These findings suggest that the plain utilisation of LSTMs may not be enough to attain a considerable and substantial increase of ac-  curacy  (Schmitt et al., 2019) , justifying the inclusion of the attention mechanism in our approach.\n\nThirdly, we further observe a noticeable trend of steady increase in accuracy from the predictions of both ANCLaF-S and ANCLaF-SA as the number of considered frames grows from 2 to 8, and then it plateaus (or even worsens a bit) as n continues to increase. This trend suggests that generally, a medium sequence length (between 4 to 16 frames) is optimal to produce more accurate predictions and that too short and too long sequences degrade temporal modelling. This finding is quite consistent with those from  (Aspandi et al., 2019b) , indicating the importance of progressive learning, which allows us to analyse and choose the optimal sequence length during training. Lastly, this sequence length selection may also impact the context vector along with its weights learnt in our attentional module, which explains why a similar trend was observed in the results from these models (see Section 4.4 for more details).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Analysis Of The Impact Of Sequence Modelling",
      "text": "Figure  2  shows an example of V-A predictions for ANCLaF and ANCLaF-S-n, together with the groundtruth annotations. Specifically, in the top part, we can see the predicted affect states from our models that, in general, are quite related to the ground truth values. However, we notice that the results of our sequence based models are more accurate than their non-sequential counterparts. We can also see that the the predicted values from ANCLaF are quite sparse, thus, quite unstable compared to ANCLAF-S, which explains its lower COR, CCC, and ICC values. Our sequence modelling, on the other hand, is able to create smooth predictions with higher overall accuracy.\n\nOn the bottom part of the figure, showing a mag-nified portion of the same example, we further notice that the results for all ANCLaF-S-n are quite similar, with those from ANCLaF-S-8 showing the highest resemblance to the ground-truth. Thus, inclusion of too short or too long sequences yields sub-optimal results due to the complexity of the facial movements included between frames (see the next section for further details).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Analysis Of The Role Of The Learnt Attentions Weights",
      "text": "To analyse the impact of the attention mechanism on our sequence modelling, we first show in Figure  3  a comparison of our baseline sequence modelling (ANCLaF-S) against ANCLaF-SA with attention activated. In the top part, we can see the predictions from the best performing models with and without attention (ANCLaF-S-8 and ANCLaF-SA-8). Comparing the predictions from both models, we find that the results are quite similar, though in some cases ANCLaF-SA seems to be more accurate and closer to the ground truth. The quantitative accuracy results indicated on the respective legends confirm this observation. The attention weights learnt by ANCLaF-SA, in- volving the previous eight frames, are also displayed at the bottom of the prediction plots. We can see that the weights calculated with respect to the associated frames seem to be higher in the presence of changes. Indeed, we observe that the attention weights are usually activated prior to subsequent facial movements. Interestingly, the intensity of the activations also appears to highlight the level of these facial movements, or the changes between frames. For instance, from frames 280-287, we can see that the different level of the weight intensity seems to be small, which also correlates to the subtle changes observed in those frames (e. g., closing of the eyes). In contrast, in frames 643-650, we see high levels of activation on the first few frames that correspond to the more discernible facial movements on the respective frames, such as the changes observed in the mouth area. These correlations illustrate how our models are capable of learning temporal changes.\n\nFigure  4  provides further details on the attention mechanism for different temporal modelling lengths. We can see that all the displayed models show quite smooth results, thanks to the temporal modelling, but not all of them achieve the same accuracy on the pre-dictions. The bottom part of the figure, highlighting the input sequence from frames 622 to 653, can help to provide an intuition about the optimal temporal modelling length, which was found to be about 8 frames. To this end, let us start by looking at the whole set of 32 frames: we can see that such a sequence of frames comprises multiple facial changes, and considering all of them together makes the training task harder to optimise. On the other hand, if we consider groups of very few frames (e. g., 2 or 4 frames), the system is likely to capture only part of a given facial action, which may impede it to properly interpret it. Therefore, we see that the optimal sequence length is the one that contains enough frames to interpret facial changes without extending too much the temporal context, which may unnecessarily increase training complexity and reduce accuracy.\n\nFinally, it is important to emphasise that the optimal sequence length needs to take into account the frame rate and the specific facial movements that are present in each dataset. In the considered dataset, with an overall frame rates of 50 fps, this length corresponds to 160 ms.\n\nIn this work, we have successfully built a sequenceattention based neural network for affect estimations in the wild. We did so by incorporating three major sub-networks: the Generator, which is responsible to extract latent features on each frame; the Discriminator, which is used to supply the first step of affect estimates of emotional quadrant, and the Combiner, which merges latent features and quadrant information to produce the final refined affect estimates of Valence and Arousal on a frame by frame basis. We then added an LSTM layer to allow temporal modelling, which we further enhanced by using step-wise attention modelling. We trained these three major sub-networks in an adversarial setting, and used curriculum learning on the sequential training stages.\n\nWe showed the effectiveness of our approach by reporting top state of the art results on two of the most widely used video datasets for affect analysis, namely AFEW-VA and SEWA. Specifically, our baseline models, which operate without any sequence modelling, yield quite competitive results with other models reported in the literature. On the other hand, our more advanced models, which are sequence-based, clearly helped to improve the affect estimates both in qualitative and quantitative terms. Qualitatively, the temporal modelling helped to produce more stable results, with visibly smoother transitions between affect predictions. Quantitatively, our models produced the overall best accuracy results reported so far on both tested datasets.\n\nWithin sequence-based models, we observed the highest accuracy improvements when the attention mechanism was included. Detailed analysis of the attention weights highlighted their correlation with the appearance of facial movements, both in terms of (temporal) localisation and intensity. Finally, we found a sequence length of around 160 ms to be the optimum one for temporal modelling, which is consistent with other relevant findings utilising similar lengths.\n\nFuture work will need to explore further optimisation of the considered adversarial topologies and attention mechanisms as well as their transferability across databases, cultures, and domains.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic representation of our Full ANCLaF Networks. Left is our base model, which consists of three networks",
      "page": 3
    },
    {
      "caption": "Figure 1: shows the overview of our proposed models,",
      "page": 3
    },
    {
      "caption": "Figure 1: ), which uses single images as",
      "page": 3
    },
    {
      "caption": "Figure 1: ) that uses sequences of latent features",
      "page": 3
    },
    {
      "caption": "Figure 2: Analysis of prediction results from a single frame (ANCLaF) and from multiple frames with temporal modelling",
      "page": 6
    },
    {
      "caption": "Figure 3: Analysis of the attention impact on the prediction results of our sequence modelling (results from ANCLaF-S-8 and",
      "page": 7
    },
    {
      "caption": "Figure 2: shows an example of V-A predictions for",
      "page": 7
    },
    {
      "caption": "Figure 3: a comparison of our baseline sequence modelling",
      "page": 7
    },
    {
      "caption": "Figure 4: Analysis of the relationship between the selection of sequence length (n) and the learnt weights of our attentional",
      "page": 8
    },
    {
      "caption": "Figure 4: provides further details on the attention",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: and table 2 provide the full comparisons of",
      "page": 5
    },
    {
      "caption": "Table 1: Quantitative comparisons on the AFEW-VA dataset.",
      "page": 6
    },
    {
      "caption": "Table 2: Quantitative comparisons on the SEWA dataset.",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Latent-based adversarial neural networks for facial affect estimations",
      "authors": [
        "D Aspandi",
        "A Mallol-Ragolta",
        "B Schuller",
        "X Binefa"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE FG"
    },
    {
      "citation_id": "2",
      "title": "Heatmapguided balanced deep convolution networks for family classification in the wild",
      "authors": [
        "D Aspandi",
        "O Martinez",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE FG 2019"
    },
    {
      "citation_id": "3",
      "title": "Fully end-to-end composite recurrent convolution network for deformable facial tracking in the wild",
      "authors": [
        "D Aspandi",
        "O Martinez",
        "F Sukno",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "14th IEEE FG"
    },
    {
      "citation_id": "4",
      "title": "Robust facial alignment with internal denoising autoencoder",
      "authors": [
        "D Aspandi",
        "O Martinez",
        "F Sukno",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "2019 16th Conference on Computer and Robot Vision (CRV)"
    },
    {
      "citation_id": "5",
      "title": "The omgemotion behavior dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "6",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09"
    },
    {
      "citation_id": "7",
      "title": "Multisource transfer learning with convolutional neural networks for lung pattern analysis",
      "authors": [
        "S Christodoulidis",
        "M Anthimopoulos",
        "L Ebner",
        "A Christe",
        "S Mougiakakou"
      ],
      "year": "2016",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "8",
      "title": "End-to-end facial and physiological model for affective computing and applications",
      "authors": [
        "J Comas",
        "D Aspandi",
        "X Binefa"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020) (FG)"
    },
    {
      "citation_id": "9",
      "title": "Active appearance models",
      "authors": [
        "T Cootes",
        "G Edwards",
        "C Taylor"
      ],
      "year": "1998",
      "venue": "Computer Vision -ECCV'98"
    },
    {
      "citation_id": "10",
      "title": "Towards diverse and natural image descriptions via a conditional gan",
      "authors": [
        "B Dai",
        "S Fidler",
        "R Urtasun",
        "D Lin"
      ],
      "year": "2017",
      "venue": "The IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "11",
      "title": "An e-learning system based on affective computing",
      "authors": [
        "S Duo",
        "L Song"
      ],
      "year": "2010",
      "venue": "Physics Procedia"
    },
    {
      "citation_id": "12",
      "title": "Re 3: Real-time recurrent regression networks for visual tracking of generic objects",
      "authors": [
        "D Farhadi",
        "D Fox"
      ],
      "year": "2018",
      "venue": "IEEE Robot. Autom. Lett"
    },
    {
      "citation_id": "13",
      "title": "Simultaneous prediction of valence/arousal and emotions on affectnet, aff-wild and afew-va",
      "authors": [
        "S Handrich",
        "L Dinges",
        "A Al-Hamadi",
        "P Werner",
        "Al Aghbari"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "15",
      "title": "Yolo-lite: a real-time object detection algorithm optimized for non-gpu computers",
      "authors": [
        "R Huang",
        "J Pedoeem",
        "C Chen"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "16",
      "title": "Multi-object tracking with neural gating using bilinear lstm",
      "authors": [
        "C Kim",
        "F Li",
        "J Rehg"
      ],
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "17",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw 2020 competition",
      "arxiv": "arXiv:2001.11409"
    },
    {
      "citation_id": "18",
      "title": "Deep affect prediction in-the-wild: Affwild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "20",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "21",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "B Schuller",
        "K Star"
      ],
      "year": "2019",
      "venue": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "arxiv": "arXiv:1901.02839"
    },
    {
      "citation_id": "22",
      "title": "Exploring temporal representations by leveraging attentionbased bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "23",
      "title": "Online affect detection and robot behavior adaptation for intervention of children with autism",
      "authors": [
        "C Liu",
        "K Conn",
        "N Sarkar",
        "W Stone"
      ],
      "year": "2008",
      "venue": "IEEE T Robot"
    },
    {
      "citation_id": "24",
      "title": "Effective approaches to attention-based neural machine translation",
      "authors": [
        "M.-T Luong",
        "H Pham",
        "C Manning"
      ],
      "year": "2015",
      "venue": "Effective approaches to attention-based neural machine translation",
      "arxiv": "arXiv:1508.04025"
    },
    {
      "citation_id": "25",
      "title": "A deep regression architecture with two-stage re-initialization for high performance facial landmark detection",
      "authors": [
        "J.-J Lv",
        "X Shao",
        "J Xing",
        "C Cheng",
        "X Zhou"
      ],
      "year": "2017",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "The semaine corpus of emotionally coloured character interactions",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "IEEE Int Con Multi"
    },
    {
      "citation_id": "28",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "A Mitenkova",
        "J Kossaifi",
        "Y Panagakis",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "Valence and arousal estimation in-the-wild with tensor methods"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression. Valence, and Arousal Computing in the Wild Department of",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2015",
      "venue": "Affectnet: A database for facial expression. Valence, and Arousal Computing in the Wild Department of"
    },
    {
      "citation_id": "30",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE T Affect Comput"
    },
    {
      "citation_id": "31",
      "title": "Multimodal emotion recognition for avec 2016 challenge",
      "authors": [
        "F Povolny",
        "P Matejka",
        "M Hradis",
        "A Popková",
        "L Otrusina",
        "P Smrz",
        "I Wood",
        "C Robin",
        "L Lamel"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC '16"
    },
    {
      "citation_id": "32",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE FG"
    },
    {
      "citation_id": "33",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "34",
      "title": "Continuous emotion recognition in speech-do we need recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Training"
    },
    {
      "citation_id": "35",
      "title": "Temporally coherent visual representations for dimensional affect recognition",
      "authors": [
        "M Tellamekala",
        "M Valstar"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "36",
      "title": "Face detection based on deep convolutional neural networks exploiting incremental facial part learning",
      "authors": [
        "D Triantafyllidou",
        "A Tefas"
      ],
      "year": "2016",
      "venue": "2016 23rd International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "37",
      "title": "Weighted speech distortion losses for neural-network-based real-time speech enhancement",
      "authors": [
        "Y Xia",
        "S Braun",
        "C Reddy",
        "H Dubey",
        "R Cutler",
        "I Tashev"
      ],
      "year": "2020",
      "venue": "Weighted speech distortion losses for neural-network-based real-time speech enhancement"
    },
    {
      "citation_id": "38",
      "title": "Two-level attention with twostage multi-task learning for facial emotion recognition",
      "authors": [
        "W Xiaohua",
        "P Muzi",
        "P Lijuan",
        "H Min",
        "J Chunhua",
        "R Fuji"
      ],
      "year": "2019",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "39",
      "title": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks",
      "authors": [
        "J Xie",
        "R Girshick",
        "A Farhadi"
      ],
      "year": "2016",
      "venue": "ECCV 2016"
    },
    {
      "citation_id": "40",
      "title": "Channel agnostic end-to-end learning based communication systems with conditional gan",
      "authors": [
        "H Ye",
        "G Li",
        "B.-H Juang",
        "K Sivanesan"
      ],
      "year": "2018",
      "venue": "IEEE Globecom Workshops (GC Wkshps)"
    },
    {
      "citation_id": "41",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "IEEE CVPRW"
    }
  ]
}