{
  "paper_id": "2002.07590v1",
  "title": "Speech Emotion Recognition Using Support Vector Machine",
  "published": "2020-02-03T09:56:56Z",
  "authors": [
    "Manas Jain",
    "Shruthi Narayan",
    "Pratibha Balaji",
    "Bharath K P",
    "Abhijit Bhowmick",
    "Karthik R",
    "Rajesh Kumar Muthu"
  ],
  "keywords": [
    "Emotion",
    "datasets",
    "SVM",
    "MFCC",
    "framing",
    "autocorrelation I"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this project, we aim to classify the speech taken as one of the four emotions namely, sadness, anger, fear and happiness. The samples that have been taken to complete this project are taken from Linguistic Data Consortium (LDC) and UGA database. The important characteristics determined from the samples are energy, pitch, MFCC coefficients, LPCC coefficients and speaker rate. The classifier used to classify these emotional states is Support Vector Machine (SVM) and this is done using two classification strategies: One against All (OAA) and Gender Dependent Classification. Furthermore, a comparative analysis has been conducted between the two and LPCC and MFCC algorithms as well.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human computer intelligence is an upcoming field of research which aims to make computers learn from experiences and decide how to respond to a particular situation. This has resulted in improved interaction between users and the computer. With the help of certain algorithms and procedures, the computer can be made fit to detect the various characteristics present in the voice sample and deduce the emotion underlying  [2] . This emotion detection can be done in two methods, one being speech and the other being image. Since speech is a very important part of communication, it is essential to be able to detect the emotion from it. There are various methods and classifications that are available like K-Nearest Neighbor, Artificial Neural Networks, Hidden Markov Model, Support Vector Machines, and others that have been developed to classify human emotions based on training datasets  [2] [3] [4] [5] .\n\nFeature extraction is the first step that needs to be carried out while detecting emotions. In this project, we have used MFCC and LPC to extract features and then used SVM to train the data sets to identify the emotion or sentiment  [5] . SVM is a supervised machine learning technique that is used for classification as well as regression. It attempts to categorize data by finding suitable hyperplanes that can separate data by the highest margin. Based on the training sets, the new values are segregated and analyzed. In the paper written by Feng Yu etal, they have used support vector machines method to classify basic four types of emotions sadness, happiness, anger and neutral using 721 utterances  [2] . In the paper written by Sapra etal, they have used K-nearest Neighbors to classify emotions and used only one technique of MFCC to extract features  [4] . In the paper by Utane etal, they have used MFCC to extract features in the speech signal and made a comparative analysis of various other classifiers like Markov model, Gaussian model and support vector machines  [5] . El Ayadi etal in his paper has conducted a survey on three important aspects present in speech emotion recognition namely how to design emotional speech corpora, what are the impacts of speech features on performance and classification systems that are present in speech emotion recognition  [6] . Kim Samuel etal in their paper has built an application for emotion detection system which operates in real time and uses multi-modal fusion of various features of the speech.  [7] . Farouk etal, in his paper have conducted the emotion detection using wavelet analysis and have concluded that wavelet analysis has helped improving the basic features of speech signal as well  [8] . Schuller etal, in their paper have used techniques of Hidden markov model to classify emotions. While doing so they have incorporated two varieties in it one that takes global statistics and is classified using Gaussian model and the other where temporal complexity was introduced using several low level instantaneous features  [9] .Kwon etal, in their paper have used MFCC for feature extraction and have used quadratic discriminant analysis to carry out the speech recognition resulting in decently good results  [10] .Schuller etal ,in their paper have presented a model firstly by making use of",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition Using Support Vector Machine",
      "text": "acoustic features after which characteristics like pitch, energy and others are used with respective classifiers to classify human emotions  [12] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methodology",
      "text": "Speech Emotion Detection aims to identify the emotions of a person based on the input voice sample. This is done by analyzing the input voice signal and carrying out an in depth procedure. First, the speech signal is procured after which features are extracting using MFCC, LPCC which contain the emotional information, voice pattern and coefficients which will be further given as the input to the classification system for further analysis. Our project contains four modules: input speech signal, Feature extraction using MFCC and LPC, Classification based on SVM and the output  [3] .   Energy calculation is one of the most important features of speech analysis  [12] . To obtain the complete statistics of all energy features, we use a few short-term functions to extract the value of energy in every speech frame. Using the above information we can obtain the overall statistics of energy for the entire speech sample by calculating the energy such as mean value, maximum value, and variation range present in the voice sample.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "V. Speech Rate:",
      "text": "Speech rate shows how fast a human speaks. It has strong correlations with any emotion like happy, sad, fear and angry.\n\nWe studied the waveform of the speech signal in time domain; and the speech is calculated using MATLAB function.\n\nIII.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Svm Algorithm",
      "text": "SVM is a very simple and efficient classifying algorithm which is used for classification and pattern recognition. Support Vector Machines algorithm was introduced by Vladimir Vapnik in 1995.The main aim of this algorithm is to obtain a function that constructs hyper planes or boundaries. These hyper planes are used to separate different categories of input data points. SVM uses binary classification  [2] . SVM are systems that use hyper planes in feature space of high dimensions to differentiate values based on a particular specification. Hyper planes are trained with specific algorithms to use statistical learning. SVM method of classification is similar to supervised learning that involves the feature extraction and generates desirable outputs. The advantage of SVM is that it is very easy to train. It can scale high dimensional data better than neural networks  [3] . There are typically two types of SVM classifiers, Linear and Nonlinear.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Simulation Outputs And Results",
      "text": "Waveform and speaking rate of an utterance is shown in Fig.  1  and the features Energy, Pitch and MFCC coefficients and LPCC coefficient are shown in Fig.  3     The proposed GUI prototype is shown in Fig.  8  and the final emotion classification result and output for an angry emotion is shown in Fig.  9 . Accuracy of both classifiers (OAA and gender dependent) is shown in table 1 for different emotions whereas table  2  shows the accuracy of both the datasets (LDC and UGA). The overall accuracy using MFCC and LPCC algorithm is also computed and is shown in the table 3. LDC datasets shows overall accuracy of 90.08% much higher than the accuracy of UGA dataset (65.95 %) as LDC datasets are performed by trained actors and in noiseless environment whereas UGA datasets are performed by students in noisy environment. The overall accuracy of Gender dependent classifier was found out to be 84.42% higher than the OAA (One against all) SVM classifier (72.785). Graph 1 shows the comparison of accuracies of two classifiers and through that we can conclude that gender dependent classifier shows better accuracy than OAA classifier algorithm. Graph 2 shows the comparison of accuracies of MFCC and LPCC algorithms and it clearly shows MFCC shows better results than LPCC. The overall accuracy of the system was found out to be 85.085 %.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion And Future Works",
      "text": "Today, the Speech Emotion Recognition has become one of the most important research areas. The type and the number of emotional classes, feature selection, classification algorithm are the important factors of this system. In this project we have used SVM for gaining higher classification accuracy. Two classification strategies (OAA and gender dependent classifier) has been performed and compared. MFCC and LPCC feature values are extracted from speech utterances and emotional states are classified using both the classifiers. On completing the analyses and recording the outputs, we can see that the features extraction using MFCC has garnered a higher degree of accuracy compared to that of LPCC. For training set with acted speakers we obtained recognition rate by 90.08% with LDC datasets and of 65.97% with UGA datasets. Since the LDC datasets are recorded in noise less environment and by professional actors, the accuracy of this dataset is very high. The overall accuracy of Gender dependent classifier was found out to be 84.42% higher than the OAA (One against all) SVM classifier (72.785%). We can conclude that gender dependent classifier shows better accuracy than OAA classifier algorithm. The accuracy using LPCC algorithm was found out to be 73.125 % which is less than the overall accuracy using MFCC algorithm which is 85.085 %.\n\nIn our future works, we tend to work on modifying the system by combining another feature values with MFCC and MEDC so as to create the accuracy of system higher. This emotion recognition system is enforced in voice mail application that queues up the calls supported emotions.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed Model",
      "page": 2
    },
    {
      "caption": "Figure 2: Block diagram of MFCC",
      "page": 2
    },
    {
      "caption": "Figure 3: Block diagram of LPCC",
      "page": 2
    },
    {
      "caption": "Figure 1: and the features Energy, Pitch and MFCC coefficients and",
      "page": 3
    },
    {
      "caption": "Figure 3: ,4,5,6 and 7 respectively.",
      "page": 3
    },
    {
      "caption": "Figure 4: Speaking rate and Waveform",
      "page": 3
    },
    {
      "caption": "Figure 7: MFCC coefficients",
      "page": 4
    },
    {
      "caption": "Figure 8: LPCC coefficients",
      "page": 4
    },
    {
      "caption": "Figure 8: and the final",
      "page": 4
    },
    {
      "caption": "Figure 9: Accuracy of both classifiers (OAA and gender dependent) is",
      "page": 4
    },
    {
      "caption": "Figure 9: Proposed GUI model",
      "page": 4
    },
    {
      "caption": "Figure 10: Emotion Classification",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions": "Happy",
          "OAA (%)": "94.62",
          "Gender dependent \n(%)": "98.53"
        },
        {
          "Emotions": "Sad",
          "OAA (%)": "72.72",
          "Gender dependent \n(%)": "76.1"
        },
        {
          "Emotions": "Angry",
          "OAA (%)": "66.66",
          "Gender dependent \n(%)": "91.66"
        },
        {
          "Emotions": "Fear",
          "OAA (%)": "57.14",
          "Gender dependent \n(%)": "71.42"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotio\nns": "",
          "LDC(%)": "OAA(\n%)",
          "UGA(%)": "OAA(\n%)"
        },
        {
          "Emotio\nns": "Happy",
          "LDC(%)": "98.52",
          "UGA(%)": "42.85"
        },
        {
          "Emotio\nns": "Sad",
          "LDC(%)": "63.63",
          "UGA(%)": "54.54"
        },
        {
          "Emotio\nns": "Angry",
          "LDC(%)": "71.42",
          "UGA(%)": "66.66"
        },
        {
          "Emotio\nns": "Fear",
          "LDC(%)": "83.33",
          "UGA(%)": "37.50"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions": "",
          "Overall accuracy (%)": "MFCC"
        },
        {
          "Emotions": "Happy",
          "Overall accuracy (%)": "99.64"
        },
        {
          "Emotions": "Sad",
          "Overall accuracy (%)": "83.33"
        },
        {
          "Emotions": "Angry",
          "Overall accuracy (%)": "91.66"
        },
        {
          "Emotions": "Fear",
          "Overall accuracy (%)": "65.71"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Rao",
        "Sreenivasa"
      ],
      "year": "2012",
      "venue": "International Journal of Computer Science and Information Technologies"
    },
    {
      "citation_id": "2",
      "title": "Emotion detection from speech to enrich multimedia content",
      "authors": [
        "Feng Yu"
      ],
      "year": "2001",
      "venue": "Emotion detection from speech to enrich multimedia content"
    },
    {
      "citation_id": "3",
      "title": "Emotion Detection from Speech",
      "authors": [
        "Tomas Pfister"
      ],
      "year": "2010",
      "venue": "Emotion Detection from Speech"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech",
      "authors": [
        "Ankur Sapra",
        "Nikhil Panwar",
        "Sohan Panwar"
      ],
      "year": "2013",
      "venue": "International journal of emerging technology and advanced engineering"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition through Speech",
      "authors": [
        "Akshay Utane",
        "S Nalbalwar"
      ],
      "year": "2013",
      "venue": "International Journal of Applied Information Syatems (IJAIS)"
    },
    {
      "citation_id": "6",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "El Ayadi",
        "Mohamed Moataz",
        "Fakhri Kamel",
        "Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Real-time emotion detection system using speech: Multi-modal fusion of different timescale features",
      "authors": [
        "Samuel Kim"
      ],
      "year": "2007",
      "venue": "Multimedia Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Emotion Recognition from Speech",
      "authors": [
        "Mohamed Farouk",
        "Hesham"
      ],
      "year": "2018",
      "venue": "Application of Wavelets in Speech Processing"
    },
    {
      "citation_id": "9",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2003",
      "venue": "Multimedia and Expo, 2003. ICME'03. Proceedings. 2003 International Conference on"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "Kwon",
        "Oh-Wook"
      ],
      "year": "2003",
      "venue": "Eighth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition from Speech",
      "authors": [
        "Andreas Wendemuth"
      ],
      "year": "2017",
      "venue": "Companion Technology"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2004",
      "venue": "Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "Tin Nwe",
        "Say Lay",
        "Liyanage Foo",
        "Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "14",
      "title": "Iterative feature normalization scheme for automatic emotion detection from speech",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2013",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "15",
      "title": "Speaker normalisation for speech-based emotion detection",
      "authors": [
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah",
        "Julien Epps"
      ],
      "year": "2007",
      "venue": "Digital Signal Processing"
    }
  ]
}