{
  "paper_id": "2101.11433v2",
  "title": "Analysis Of Basic Emotions In Texts Based On Bert Vector Representation",
  "published": "2021-01-21T07:11:21Z",
  "authors": [
    "A. Artemov",
    "A. Veselovskiy",
    "I. Khasenevich",
    "I. Bolokhov"
  ],
  "keywords": [
    "7-dimensional emotional model",
    "Pytorch",
    "GAN",
    "deep learning",
    "cosine similarity",
    "Ekman",
    "BERT",
    "NLP",
    "BRAIN2NLP",
    "emotions",
    "collisions",
    "Big Data"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the following paper the authors present a GAN-type model and the most important stages of its development for the task of emotion recognition in text. In particular, we propose an approach for generating a synthetic dataset of all possible emotions combinations based on manually labelled incomplete data. \n Dataset vectorization Initially, it is necessary to form a dataset for training the model. The dataset should contain data about seven basic emotions for each certain piece of text -a paragraph or a sentence. The authors couldn't find a publicly available dataset with this or similar data. Therefore, it was",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There are several approaches to classification of human emotions. Authors consider the methodology according to P.Ekman to be the most reasonable  [4] . In accordance with it, the following seven human emotions can be distinguished: fear, sadness, anger, disgust, calm, happiness, surprise. These emotions were classified according to the analysis of facial expressions of a person during events of different semantic meaning. The unambiguous perception of facial expressions with different people and nations was the proof of the sufficiency and validity of the seven basic emotions.\n\nAt the same time, although there is a number of solutions for emotional recognition, a standard for the number of basic emotions for text data has not yet been created. For example, in Rodrigo Masaru Ohashi's work  [7]  the model of bidirectional LSTM with a convolutional neural network layer was successfully trained to identify 4 emotions: joy, fear, sadness and anger. Chew-Yean Yam's article \"Emotion Detection and Recognition from Text Using Deep Learning  [2] \" describes the neural network with 3 hidden layers of 125, 25 and 5 neurons for determining 5 emotions: anger, sadness, fear, happiness, excitement. Jangwon Park  [8]  created a Huggingface Transformers based model that predicts 7 emotions, the F1-measure of the model is 59.81 and 61.48 for the train and test dataset, respectively. This model is based on the GoEmotions  [3]  dataset with 27 emotion classes (see Table  5 ).\n\nLet us assume, as a hypothesis, that the 7 basic emotions by Ekman are applicable to texts. That is, the labelling of texts for this set of emotion classes by a person is complete. Based on the possibility of obtaining such a dataset, the task is to train a model that can predict the vector of seven emotions for an arbitrary text. The result of this work is described in the article.\n\nA positive solution to this problem opens up prospects for combining different data channels (audio and video) to determine emotions when machines are communicating with humans. Moreover, of particular interest is the description of information spread through means of mass communication (social networks), taking into account the phenomenon of emotions social exchange. decided to form it automatically using data on the emotional reaction of people, expressed in texts. Such data were emoticons in texts from Twitter, YouTube and other social networks.\n\nThe original texts were divided into sentences, emoticons were extracted from each sentence. In order to vectorize emotions, a special dictionary was compiled containing emoticons divided into emotion types according to uniquely interpreted classes:\n\nThe classification of the texts' emotions was carried out using the created dictionary. By counting the number of encounters of each emotion in the text, vectors of 7 emotions were generated for each sentence.\n\nText vectorization was carried out using our own NLP framework -BRAIN2NLP  1  . The text vectorization was based on the BERT language model trained by Google. Thus, at the output, pairs of the following form were obtained: 512-dimensional vector BERT to sevendimensional vector of emotion. In total, more than 100,000 such pairs were constructed. In the process, it turned out that the resulting dataset contains a lot of conflicting data (collisions). Collisions are examples of data, where the same properties of an object lead to different classes or to a significant number of classes. The automation of the collision search process required the development of a small script. The algorithm of its work is given below.\n\nAlgorithm 1: Finding collisions in a dataset 1. Determine the allowed fraction of the k-number of classes to which objects in the same cluster can belong. 2. Classify dataset objects by the vector of their properties (features). 3. Assign the same ID to dataset objects belonging to the same cluster. 4. Add up the values by class for objects with the same ID. 5. Estimate for each ID the mathematical expectation of the vector of values by class. 6. Calculate the number of Z values of the class vector that is greater than or equal to the expected value. 7. Evaluate each cluster, and where Z > k mark it as a collision. 8. Assign a collision mark to all objects in the cluster.\n\nFor the purpose of \"human\" marking, 10,000 texts were selected, among which there were only 6913 examples without collisions with a length of no more than two sentences. The experts were asked to assign no more than 2 emotions to each of these texts. But the dataset formed in this way also contained minor examples of collisions, which were also eliminated  [1] . As a result, a golden dataset with 2813 examples was obtained.\n\nBERT-vectors are successfully used for classification tasks of texts  [9]  and their fragments  [5] . Considering this fact, we have chosen these vectors as a basic description of the space of text properties, which can be used to predict the emotion vector.\n\nDuring the search for the optimal solution for the classification of emotions, different variants of embedding length BERT (lengths 768 and 512) and different sizes of datasets were tested. Also, we have tested a certain number of negative samples, various dictionaries for the extraction of emoticons, normalization of the dataset, different model architectures, loss functions and activation functions. As a result of this work, the following hypothesis was formulated and confirmed in practice: It is possible to improve the quality of classification by abandoning the attempt to normalize the unbalanced initial data -the number of examples for a set of classes, replacing it with uniformly generated BERT vectors for each set of classes. Among all tested neural network architectures, Generative adversarial network (GAN)  [6]  showed the best resultsan architecture consisting of a generative network and a discriminative network configured to work against each other. Discriminative models learn the boundary between the classes; generative model generates new data instances. A collision free dataset (Dataset_1) was used for training the generative network and for final testing of the discriminative network. In the formulation of the study, the vector of emotions has 7 dimensions, we denote it as V1. Table  1  shows a list of combinations of vector V1 values found in Dataset_1.\n\nSince 37 combinations 2 do not make up a complete list of 128 combinations, the missing ones had to be generated.\n\nFor this purpose, based on Dataset_1, we have trained a generator model for known combinations of emotions. Then the combinations V1 were initiated, where the initial -has the values [0, 0, 0, 0, 0, 0, 0], and the final one is [1, 1, 1, 1, 1, 1, 1]. The total number of vectors V1 was 128. Using the generator, 128 BERT -V2 vectors were obtained corresponding to various V1 combinations.\n\nThus, a new dataset (Dataset_2) was formed, where each vector V1 corresponded to the generated vector V2 (Figure  1 ). Dataset_2 was used for fine-tuning the Generator model and Discriminator training, predicting the emotion vector from the BERT vector of the text. Figure  1  shows the process of sequential training of the Generator, and Figure  2  shows the Discriminator.\n\nFigure  3  shows graphs of the \"descent rate\" when training the Generator and the Discriminator. The large value of the generator error is explained by the fact that a simple product of vectors is used, and the error is considered by MSE.\n\nThe opposite situation is with the discriminator, where not only the output value is normalized, but also the error is calculated by the cosine measure.\n\nIn order to move to a non-negative real number, the output vector of the discriminator value was normalized as follows\n\nwhere V is a vector of forecast values.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results",
      "text": "Model training was performed on 70% of the data with testing on the remaining 30%. The evaluation of the quality of the model is illustrated in Figure  4 . Comparative results of the model training process are presented in Table  2 . Based on the input data, the model makes a forecast in the form of a 7-dimensional vector of emotions. To be able to compare it with gold dataset, it was converted into 2-dimensional vector by selecting emotion classes with maximum values. If at least one of the two emotions coincided with the gold labelling, then it was considered that the emotion was predicted correctly. The accuracy of the forecast for such an assessment varies depending on the class of emotion (see Table  3 ). The average accuracy of emotion predictions is 0.65.\n\nThe results of the model for different words with different emotional sentiment are shown in Table  4 .\n\nBased on the study of the data on model's performance, the following conclusions were made:\n\n-in all cases, the model correctly identified at least one of the dominant emotions, a little less oftentwo of the two.\n\n-the model does not define the calm emotion well, in most cases its value is the lowest.\n\n-Intends to show more emotions than necessary (see examples in the first five sentences of Table  4 ). That is, the model \"gives a chance\" to emotions that were not defined in the gold dataset. We consider this to be a special property of the model rather than a bug, since the model was trained on a larger variety (of generated data) than those observed by experts. This is a very important consequence that needs to be taken into account when reconstructing complete data based on expert assessments.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "Among all the architectures that were tested, GAN performed better than the others (see Table  5 ). When com-pared with the models of emotions of some other authors, our model showed the best result. The F1-measure value is 0.025 higher than the \"goemotions-pytorch\" model, and the accuracy value is 0.0053 higher than the \"Emotion Detection and Recognition from Text Using Deep Learning\" model, but 0.19 lower than the \"Bidirectional LSTM with a Convolutional Neural Network\"model. In the latter case, it is worth saying that in our model the number of defined emotions is seven against four in \"Bidirectional LSTM with a Convolutional Neural Network\". The hypothesis about the possibility of improving the quality of classification by refusing to normalize the class distribution of a real sample and replacing it with a sample obtained by generating all possible combinations of classes (BERT vectors) for a finite number of variables was fully justified. The authors have not encountered such an approach in other publications. At the same time, it opens up new perspectives for machine learning with minimizing human involvement by automatically generating \"big data\" for training neural networks on \"small\" incomplete data.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: shows the Discriminator.",
      "page": 3
    },
    {
      "caption": "Figure 1: The process of generator training with the formation of Dataset_2",
      "page": 3
    },
    {
      "caption": "Figure 3: shows graphs of the \"descent rate\" when training",
      "page": 4
    },
    {
      "caption": "Figure 4: Comparative",
      "page": 4
    },
    {
      "caption": "Figure 2: The process of joint training of the Generator and the Discriminator",
      "page": 5
    },
    {
      "caption": "Figure 3: Gradient descent of a) Generator model; b) Discriminator model",
      "page": 5
    },
    {
      "caption": "Figure 4: The matrix of the predictive distribution and the",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#": "0",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "1",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "1",
          "surprise": "0"
        },
        {
          "#": "2",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "1",
          "surprise": "1"
        },
        {
          "#": "3",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "4",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "5",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "1",
          "surprise": "0"
        },
        {
          "#": "6",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "1",
          "surprise": "1"
        },
        {
          "#": "7",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "8",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "9",
          "fear": "0",
          "sadness": "0",
          "anger": "0",
          "disgust": "1",
          "calm": "1",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "10",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "11",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "12",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "1",
          "surprise": "0"
        },
        {
          "#": "13",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "1",
          "surprise": "1"
        },
        {
          "#": "14",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "15",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "16",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "1",
          "happiness": "1",
          "surprise": "0"
        },
        {
          "#": "17",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "18",
          "fear": "0",
          "sadness": "0",
          "anger": "1",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "19",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "20",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "21",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "1",
          "surprise": "0"
        },
        {
          "#": "22",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "23",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "24",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "1",
          "surprise": "0"
        },
        {
          "#": "25",
          "fear": "0",
          "sadness": "1",
          "anger": "0",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "26",
          "fear": "0",
          "sadness": "1",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "27",
          "fear": "0",
          "sadness": "1",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "28",
          "fear": "0",
          "sadness": "1",
          "anger": "1",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "29",
          "fear": "1",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "30",
          "fear": "1",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "31",
          "fear": "1",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "1",
          "surprise": "1"
        },
        {
          "#": "32",
          "fear": "1",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "33",
          "fear": "1",
          "sadness": "0",
          "anger": "0",
          "disgust": "0",
          "calm": "1",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "34",
          "fear": "1",
          "sadness": "0",
          "anger": "0",
          "disgust": "1",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "35",
          "fear": "1",
          "sadness": "0",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "1"
        },
        {
          "#": "36",
          "fear": "1",
          "sadness": "1",
          "anger": "0",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        },
        {
          "#": "37",
          "fear": "1",
          "sadness": "1",
          "anger": "1",
          "disgust": "0",
          "calm": "0",
          "happiness": "0",
          "surprise": "0"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "fear",
          "Accuracy": "0.62"
        },
        {
          "Emotion": "sadness",
          "Accuracy": "0.70"
        },
        {
          "Emotion": "anger",
          "Accuracy": "0.63"
        },
        {
          "Emotion": "disgust",
          "Accuracy": "0.76"
        },
        {
          "Emotion": "calm",
          "Accuracy": "0.64"
        },
        {
          "Emotion": "happiness",
          "Accuracy": "0.68"
        },
        {
          "Emotion": "surprise",
          "Accuracy": "0.54"
        },
        {
          "Emotion": "MEAN",
          "Accuracy": "0.65"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Network": "",
          "Dataset": "Collisions",
          "Architecture": "1-stlayer",
          "Learn": "Weight\nInitialization"
        },
        {
          "Network": "Generator",
          "Dataset": "Yes",
          "Architecture": "7x128",
          "Learn": "Random"
        },
        {
          "Network": "Discriminator",
          "Dataset": "Yes",
          "Architecture": "512x7",
          "Learn": "Metod\nFM*"
        },
        {
          "Network": "Generator",
          "Dataset": "No",
          "Architecture": "7x128",
          "Learn": "Random"
        },
        {
          "Network": "Discriminator",
          "Dataset": "No",
          "Architecture": "512x7",
          "Learn": "Metod\nFM*"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-": "",
          "Example of sentence": "",
          "Expert’s\nEmotion": "",
          "Predicted emotion by the Model": "fear"
        },
        {
          "-": "1",
          "Example of sentence": "There is danger all around,\nit’s scary to go outside.",
          "Expert’s\nEmotion": "fear,\nsadness",
          "Predicted emotion by the Model": "0.49"
        },
        {
          "-": "2",
          "Example of sentence": "It is very sad that nothing\ncan be done. The city\nburned down and everyone\ndied",
          "Expert’s\nEmotion": "sadness,\nfear",
          "Predicted emotion by the Model": "0.18"
        },
        {
          "-": "3",
          "Example of sentence": "I am super angry,\nI’m in a fury!\nYou’re dead, bastards!",
          "Expert’s\nEmotion": "anger",
          "Predicted emotion by the Model": "0.21"
        },
        {
          "-": "4",
          "Example of sentence": "The soup was terrible,\nI’ve never tasted such\ndisgusting meal. I’ve been\nsick of it for an hour.",
          "Expert’s\nEmotion": "disgust,\nanger",
          "Predicted emotion by the Model": "0.17"
        },
        {
          "-": "5",
          "Example of sentence": "Relax and listen to nature, feel this\nlight summer wind. The\nsilence calms better than a\nthousand words.",
          "Expert’s\nEmotion": "calm,\nhappiness",
          "Predicted emotion by the Model": "0.03"
        },
        {
          "-": "6",
          "Example of sentence": "What could be better than\nwatching the sunset together\nwith close friends!\nJust wonderful!",
          "Expert’s\nEmotion": "happiness,\ncalm",
          "Predicted emotion by the Model": "0.01"
        },
        {
          "-": "7",
          "Example of sentence": "John won more than\n$1 million in the lottery,\nthat was a pleasant surprise\nfor him.",
          "Expert’s\nEmotion": "surprise,\nhappiness",
          "Predicted emotion by the Model": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "«Bidirectional LSTM with a Convolutional Neural\nNetwork» by Rodrigo Masaru Ohashi (2019)",
          "Emotions": "4",
          "F1 macro": "-",
          "Accuracy": "0.84"
        },
        {
          "Model": "«Emotion Detection and Recognition from Text Using\nDeep Learning» by Chew-Yean Yam (2015)",
          "Emotions": "5",
          "F1 macro": "-",
          "Accuracy": "0.64"
        },
        {
          "Model": "«GoEmotions-pytorch» by Jangwon Park (2020)",
          "Emotions": "7",
          "F1 macro": "0.61",
          "Accuracy": "-"
        },
        {
          "Model": "GAN BERT-EMO model (2020)",
          "Emotions": "7",
          "F1 macro": "0.64",
          "Accuracy": "0.65"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "CLASSIFICATION-OF-BASIC-EMOTIONS-USING-BERT",
      "authors": [
        "A Artemov",
        "& Veselovskiy"
      ],
      "year": "2021",
      "venue": "CLASSIFICATION-OF-BASIC-EMOTIONS-USING-BERT"
    },
    {
      "citation_id": "2",
      "title": "Emotion Detection and Recognition from Text Using Deep Learning",
      "authors": [
        "Chew-Yean"
      ],
      "year": "2015",
      "venue": "Emotion Detection and Recognition from Text Using Deep Learning"
    },
    {
      "citation_id": "3",
      "title": "A Dataset of Fine-Grained Emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "Jeongwoo Ko",
        "A Cowen",
        "G Nemade",
        "Sujith Ravi",
        "Goemotions"
      ],
      "year": "2020",
      "venue": "A Dataset of Fine-Grained Emotions"
    },
    {
      "citation_id": "4",
      "title": "The Nature of Emotion Fundamental Questions",
      "authors": [
        "P Ekman",
        "R Davidson"
      ],
      "year": "1994",
      "venue": "The Nature of Emotion Fundamental Questions"
    },
    {
      "citation_id": "5",
      "title": "A Sequence-to-Sequence Approach to Dialogue State Tracking",
      "authors": [
        "Y Feng",
        "Y Wang",
        "Hang Li"
      ],
      "year": "2020",
      "venue": "A Sequence-to-Sequence Approach to Dialogue State Tracking"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "From Sentiment Analysis to Emotion Recognition: A NLP story",
      "authors": [
        "R Ohashi"
      ],
      "year": "2019",
      "venue": "From Sentiment Analysis to Emotion Recognition: A NLP story"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Jangwon Park",
        "Pytorch"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "A BERT-based Dual Embedding Model for Chinese Idiom Prediction",
      "authors": [
        "M Tan",
        "J Jiang"
      ],
      "year": "2020",
      "venue": "A BERT-based Dual Embedding Model for Chinese Idiom Prediction"
    }
  ]
}