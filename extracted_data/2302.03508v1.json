{
  "paper_id": "2302.03508v1",
  "title": "Cluster-Level Contrastive Learning For Emotion Recognition In Conversations",
  "published": "2023-02-07T14:49:20Z",
  "authors": [
    "Kailai Yang",
    "Tianlin Zhang",
    "Hassan Alhuzali",
    "Sophia Ananiadou"
  ],
  "keywords": [
    "Emotion Recognition in Conversations",
    "Cluster-Level Contrastive Learning",
    "Valence-Arousal-Dominance",
    "Pre-trained Knowledge Adapters"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A key challenge for Emotion Recognition in Conversations (ERC) is to distinguish semantically similar emotions. Some works utilise Supervised Contrastive Learning (SCL) which uses categorical emotion labels as supervision signals and contrasts in high-dimensional semantic space. However, categorical labels fail to provide quantitative information between emotions. ERC is also not equally dependent on all embedded features in the semantic space, which makes the high-dimensional SCL inefficient. To address these issues, we propose a novel low-dimensional Supervised Cluster-level Contrastive Learning (SCCL) method, which first reduces the high-dimensional SCL space to a three-dimensional affect representation space Valence-Arousal-Dominance (VAD), then performs cluster-level contrastive learning to incorporate measurable emotion prototypes. To help modelling the dialogue and enriching the context, we leverage the pre-trained knowledge adapters to infuse linguistic and factual knowledge. Experiments show that our method achieves new state-of-the-art results with 69.81% on IEMOCAP, 65.7% on MELD, and 62.51% on DailyDialog datasets. The analysis also proves that the VAD space is not only suitable for ERC but also interpretable, with VAD prototypes enhancing its performance and stabilising the training of SCCL. In addition, the pre-trained knowledge adapters benefit the performance of the utterance encoder and SCCL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTION Recognition in Conversations (ERC) aims at identifying the emotion of each utterance within a dialogue from a pre-defined emotion category set  [1] . In recent years, ERC has attracted increasing research interest from the NLP community due to the growing availability of public datasets  [2] ,  [3] ,  [4]  and its wide applications. For example, ERC enables dialogue systems to generate emotionally coherent and empathetic responses  [5] . It has also been utilised for opinion mining from customer reviews  [6] ,  [7]  and emotion-related social media analysis  [8] ,  [9] , etc.\n\nContext modelling is a key challenge for ERC. The emotion of each utterance is influenced both by the previous utterances of the speaker and the responses of other participants  [10] . Current methods mainly utilise Pre-trained Language Models (PLMs)  [11]  to deal with this challenge. However, PLMs are found to poorly capture the semantic meaning of sentences without careful fine-tuning  [12] , which also raises difficulties for the identification of semantically similar emotions (e.g., excited and happy). Since previous works utilise unsupervised contrastive learning to alleviate this problem  [12] ,  [13]  and obtain promising results in several text classification tasks, Li et al.  [14]  manage to introduce Supervised Contrastive Learning (SCL) to ERC, where utterances with the same emotion label are considered as positive pairs, and the instance-level utterance representations are directly utilised for contrastive learning. SCL decouples the overlap between samples with similar emotions in the semantic space, and facilitates the learning of the decision boundary.\n\nHowever, SCL treats two samples as a negative pair as long as they are with different labels, regardless of the quantitative semantic similarity between emotions (e.g., happy is closer to excited than sad). This negligence is manifested by the fact that the representation similarities between the current sample and all negative samples are minimised at the same rate in standard SCL loss. Besides, the success of works with manual feature selection  [15]  shows that ERC is not equally dependent on all features embedded in the high-dimensional utterance representations. We expect a low-dimensional prototype for each emotion, which is defined as a representative embedding for a group of similar instances  [16] , to be more efficient in contrastive learning. High-dimensional SCL space also leads to other limitations: (a) The curse of dimensionality  [17] . (b) The results are hard to interpret and visualise. (c) Stable SCL requires large batch sizes  [18]  which leads to high computational cost.\n\nTo tackle the above challenges, we propose a novel lowdimensional Supervised Cluster-level Contrastive Learning (SCCL) method for ERC. With a PLM-based contextaware utterance encoder, we improve SCL as follows: (a) we reduce the high-dimensional contrastive learning space to a three-dimensional space called Valence-Arousal-Dominance (VAD), a widely explored affect representation model in psychology  [19] ,  [20] . (b) we introduce a humanlabelled prototype for each emotion in VAD space, which brings quantitative information between all emotion labels. We provide an example for some emotions in Figure  1 , arXiv:2302.03508v1 [cs.CL] 7 Feb 2023 where the emotions within the same sentiment polarity lie closer and their relative positions are reasonable. Regarding each emotion category as a cluster centre, SCCL predicts the cluster-level VAD for each emotion and transfers the instance-level emotion labels to cluster level with the emotion prototypes, then performs cluster-level contrastive learning. Meanwhile, Liu et al.  [21]  argue that current PLMs lack fine-grained linguistic knowledge, which is proved useful to help modelling the utterances in sentiment-related tasks  [22] . Factual knowledge is defined as the fact-related commonsense knowledge stored in text-based triplets or sentences  [23] , which is also widely leveraged in ERC  [24] ,  [25] ,  [26]  and proved effective in enriching the context and providing relevant knowledge for emotion reasoning. Therefore, we infuse linguistic and factual knowledge utilising the pre-trained knowledge adapter in a plug-in manner, which avoids modification of the PLM weights. Experimental results show that our method achieves state-of-the-art results on three widely used datasets: IEMOCAP, MELD, and DailyDialog. Further analysis shows the effectiveness of each proposed module.\n\nTo summarise, this work mainly makes the following contributions:\n\n• We reduce the high-dimensional SCL space to a three-dimensional space VAD, which improves model performance and facilitates interpretability.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "For the first time in ERC, we incorporate VAD prototypes to SCL by proposing a novel Supervised Cluster-level Contrastive Learning (SCCL). Analysis shows that SCCL remains stable with both large and small batch sizes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We infuse linguistic and factual knowledge into the utterance encoder by utilising the pre-trained knowledge adapters, and analyse their benefits via the ablation study and empirical comparisons.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition In Conversations",
      "text": "A key challenge of ERC is to leverage rich information in the dialogue context. Early works utilise Recurrent Neural Networks (RNN) to model the intra-speaker dependencies within the utterance sequence for each of the dialogue participants  [27] ,  [28] , and revise the output at each time step as memories. Considering the inter-speaker dependencies, DialogueRNN  [29]  proposes a global state RNN to model multi-party relations and emotional dynamics. Another branch of work leverages the strong context modelling ability of Transformer-based networks to model the dialogue as a whole  [10] ,  [30] ,  [31] . To introduce more interpretable structures, there are also many works  [32] ,  [33] ,  [34]  that construct a graph on the dialogue, and devise graph neural networks to model ERC as a node-classification task.\n\nConstrained by the size of available datasets, many works manage to infuse task-related information to aid emotion reasoning. Some methods  [24] ,  [26] ,  [35]  explicitly incorporate commonsense knowledge to enrich the semantic space. Hazarika et al.  [36]  and Chapuis et al.  [37]  design relevant pre-training task and transfer the pre-trained weights to ERC. Sentiment scores  [24] , topic information  [7] ,  [38]  and speaker-utterance relations  [30]  are also leveraged to enhance model performance. As an effective dimensional emotion representation model  [39] , VAD information is also incorporated to facilitate emotion recognition in multiple modalities, such as text  [40] ,  [41] ,  [42]  and acoustics  [43] ,  [44] , which considerably boosts the model performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Unsupervised Contrastive Learning (UCL) aims to construct training samples in an unsupervised manner. In Computer Vision, Chen et al.  [45]  utilise different data augmentation methods to create new samples, and regard samples obtained from the same picture as positive pairs. Based on that idea, Li et al.  [46]  propose contrastive clustering to produce clustering-favorite representations, which regards each classification class as a cluster, and obtains positive pairs from two different data augmentation methods. Then contrastive learning is performed on both instance and cluster levels. There are also many attempts to reduce the high-dimensional UCL space to incorporate prior knowledge  [16] ,  [47] , boost semi-supervised learning  [17]  and visualise the results  [48] . With a similar training framework in NLP, UCL is mainly devised to enforce the sentence representations of PLMs to distinguish similar semantics. For example, Yan et al.  [49]  develop data augmentation methods for texts, and Kim et al.  [50]  train a Siamese model to construct positive pairs.\n\nOn the other hand, SCL fully leverages the supervision signals to obtain separable sentence embeddings, which facilitates the model to find the decision boundaries. Existing approaches take samples with the same label as positive pairs to calculate contrastive loss  [18] ,  [51] ,  [52] . In emotion recognition, Li et al.  [14]  combine SCL in a multi-task learning setting, which aims to make similar utterances mutually exclusive. Besides, Alhuzali and Ananiadou  [53]  introduces a variant of triplet centre loss that combines both intraand inter-class variations into the emotion classification loss function.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Definition And Model Overview",
      "text": "We define ERC task as follows: a dialogue D in the dataset contains a series of utterances {D 1 , D 2 , ..., D n }, with the corresponding emotion labels {Y 1 , Y 2 , ..., Y n }, where Y i ∈ E is a discrete value indicating the emotion label, and E is the pre-defined categorical emotion set. Each utterance D i consists of n i tokens, denoted as D i = {D 1 i , D 2 i , ..., D ni i }. D i is uttered by P (D i ), where P (D i ) ∈ P denotes the speaker, and P is the set of dialogue participants' names. Given the above information, ERC aims to identify the emotions of each utterance, which can be formalized as Ŷi = f (D i , D, P (D i )). The overview of our model is presented in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Context-Aware Utterance Encoder",
      "text": "To introduce speaker information, we pre-pend the name of the speaker P (D j ) for each utterance D j as Dj . Then the current utterance Di is concatenated with both past and future contexts to get the context-aware input R i : R i = {[CLS]; Di-Wp ; ...; Di ; ...; Di+W f ; [EOS]}, where W p and W f denotes past and future context window size, [CLS] and [EOS] denote the start-of-sentence and end-of-sentence token in PLMs. Then we use R i to obtain the context-aware utterance embeddings:\n\nwhere Encoder denotes the RoBERTa  [54]  encoder, H L i ∈ R S×D h denotes the final output of the L-th layer, S denotes sequence length and D h is the hidden size of the encoder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Knowledge Infusion With Adapter",
      "text": "We incorporate external knowledge into the utterance encoder by injecting pre-trained knowledge adapters. The knowledge adapter is a multi-layer Transformer-based model separately initialised and pre-trained for each of As shown in Figure  2 (a), we follow the methodology of Wang et al.  [23]  and pre-train two knowledge adapters with commonsense knowledge from T-REx  [56]  (FacAdapter) and linguistic knowledge provided by Stanford Parser 1 (LinAdapter). T-REx is a large-scale factual knowledge graph built from over 11.1M alignments between statements and triples of Wikipedia, which provide relevant knowledge to enrich the context and aid emotion reasoning. For example, the statement \"Vincent van Gogh and other late 19th century painters used blue not just to depict nature, but to create bad moods and emotions\" is aligned with triples <Vincent van Gogh, occupation, painters> and <blue, represent, bad moods and emotions>. During the pre-training process, given the statements and entities as input, the FacAdapter predicts the relation type of the aligned triples. Linguistic knowledge is naturally embedded in language texts, which benefits sentence modelling. It can be obtained by running dependency parser to get semantic and syntactic information. Therefore, for pre-training on linguistic knowledge, the LinAdapter takes the texts as input and predicts the syntactic and semantic relations annotated by the parser.\n\nThe adapter is combined as follows: let Encoder l 1. https://nlp.stanford.edu/software/lex-parser.html denotes the l-th hidden layer of the utterance encoder. LinAdapter, denoted as Adapter, has n k Transformer-based layers, where n k ≤ L and Adapter j denotes j-th layer of the adapter. Adapter is also pre-defined an interactive layer set L = {l 1 , l 2 , ..., l n k }, where the hidden states of Encoder lj will be combined in Adapter j . Specifically, for i-th utterance and each l j ∈ L, this process can be formalised as:\n\nwhere H j a ∈ R D h denotes the j-th layer output of the knowledge adapter, H lj i is the l j -th layer output of the utterance encoder, ⊕ denotes element-wise addition, and H 1 a is initialised with an all-zero matrix. The final layer output H n k a of the adapter is combined with the PLM embeddings as the final utterance representations:\n\nwhere Ĥi ∈ R S×D h denotes the knowledge-enhanced utterance embeddings, T anh denotes the tanh activation function, and",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Supervised Cluster-Level Contrastive Learning",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Prototypes",
      "text": "Valence-Arousal-Dominance (VAD) maps emotion states to a three-dimensional continuous space, where Valence reflects the pleasantness of a stimulus, arousal reflects the intensity of emotion provoked by a stimulus, and dominance reflects the degree of control exerted by a stimulus  [57] . Instead of directly leveraging the one-hot categorical emotion labels for supervision, VAD allows each categorical emotion to be projected into the space with measurable distances. A few ERC resources  [2]  are human-labelled with a contextdependent VAD score for each utterance j: H-VAD j ∈ R 3 , which can be leveraged for accurately computing emotion prototypes.\n\nHowever, utterance-level VAD labels are expensive and unavailable in most cases. For application in more scenarios, we consider the context-independent word-level VAD information from sentiment lexicons. We utilise NRC-VAD  [58] , a VAD sentiment lexicon that contains reliable human-ratings of VAD for 20,000 English words. All the terms in NRC-VAD denote or connote emotions, and are selected from commonly used sentiment lexicons and tweets. Each of these terms is first strictly annotated via best-worst scaling with crowdsourcing annotators. Then an aggregation process calculates the VAD for each term ranging from 0 to 1. With the pre-defined categorical emotion set E, we extract the VAD for each of the emotion e ∈ E from NRC-VAD: NRC-VAD e ∈ R 3 . For example, the emotion happiness is assigned: [0.9600, 0.7320, 0.8500]. The VAD information from either of the above methods is utilised to obtain cluster-level emotion representations. We expect utterance-level H-VADs to outperform word-level NRC-VADs since they are contextdependent and bear more fine-grained VAD information.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Cluster-Level Contrastive Learning",
      "text": "Though VAD prototypes provide useful quantitative information, they are difficult to be infused into SCL. Therefore, we propose to perform SCL at cluster level instead of instance level with a novel SCCL method. Regarding each emotion category as a cluster centre, we perform SCCL with cluster-level representations separately obtained from emotion labels and model predictions, where both processes are introduced below.\n\nWe first compute for emotion labels using the emotion prototypes. For a batch B, as shown in Figure  2(b) , the emotion labels are projected to a one-hot label matrix M ∈ R |B|×|E| , where M i ∈ R |E| is the i-th row of M , denoting the one-hot emotion label of the i-th sample, and M j ∈ R |B| is the j-th column of M , denoting the samples with the label e j ∈ E. For the j-th cluster e j , we map M j to the VAD space as follows:\n\nwhere M jk denotes k-th element of M j , M j ∈ R 3 denotes the cluster-level representation of e j . When utterancelevel VAD information is available, VAD ej = H-VAD j . When NRC-VAD information is utilised, VAD ej = NRC-VAD ej and NRC-VAD ej is directly regarded as the cluster-level emotion representation for e j .\n\nThen we compute for the model predictions. One choice is to adopt a similar approach as the emotion labels, which utilises the normalised categorical predictions with Softmax, and maps them to the VAD space using Eqn.5. However, it may reduce SCCL to the vanilla case where the model only learns the one-hot label information and ignores the emotion prototypes. Therefore, we utilise a neural network to parameterise the dimension reduction process from the semantic space to the VAD space. Specifically, for Ĥi , we regard the embedding of the start-of-sentence token at position 0",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Ĥ[Cls]",
      "text": "i as its utterance-level embedding, and map Ĥ[CLS] i to the VAD space:\n\nwhere\n\nAs shown in Figure  2 (c), following the idea of labels as representations, for each batch, we calculate the SCCL loss as follows:\n\nwhere ĤV AD j ∈ R 3 denotes the cluster-level embedding for e j from model predictions, A(j) = {i|Y i = e j , i ∈ [1, |B|]} records the samples D i ∈ B labelled with the emotion e j , • denotes dot-product operation, τ ∈ R + is the temperature coefficient, and L SCCL denotes the SCCL loss.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Training",
      "text": "We combine SCCL with ERC in a multi-task learning manner. For the i-th utterance, we still utilise Ĥ[CLS] i as the utterance-level embedding, and compute the final classification probability as follows:\n\nwhere Ŷi ∈ R |E| , and W 3 ∈ R D h ×|E| , b 3 ∈ R |E| are learnable parameters. Then we compute the ERC loss using the standard cross-entropy loss:\n\nwhere Y j i and Ŷ j i are the j-th element of Y i and Ŷi . Finally, we combine the ERC loss and SCCL loss in the following manner:\n\nwhere α ∈ [0, 1] denotes the pre-defined weight coefficient of L SCCL .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our method on the following four benchmark datasets. The statistics of all datasets are presented in Table  1 . IEMOCAP  [2] : A two-party multi-modal conversation dataset derived from the scenarios in the scripts of the two actors. For all datatsets, we only utilise the text modality in our experiments. The pre-defined categorical emotions are neutral, sad, anger, happy, frustrated, excited.\n\nMELD  [3] : A multi-party multi-modal dataset enriched from EmotionLines dataset, collected from the scripts of American TV show Friends. The pre-defined emotions are neutral, sad, anger, disgust, fear, happy, surprise.\n\nEmoryNLP  [4] : Another dataset collected from TV show Friends, but annotated with different emotion label categories. The pre-defined emotions are neutral, sad, mad, scared, powerful, peaceful, joyful.\n\nDailyDialog  [59] : A dataset compiled from humanwritten daily conversations with only two parties involved and no speaker information. The pre-defined emotion labels are the Ekman's emotion types: neutral, happy, surprise, sad, anger, disgust, fear.\n\nAmong the above datasets, human-labelled utterancelevel VAD scores are only available in IEMOCAP, where the aggregation process calculates the VAD for each utterance ranging from 1 to 5. To cope with the SCCL method, we linearly transform all VAD scores to the range [0, 1] during inference.\n\nWhen NRC-VAD is utilised, the emotion prototypes of the labels for all datasets are listed in Table  2 . According to the assignments, most of the cluster centres reflect appropriate positions of the corresponding emotions in VAD space, where similar emotions are measurably closer to each other while maintaining a fine-grained difference to facilitate the model to distinguish them. For example, happy stays closer to excited than anger in IEMOCAP. In addition, for all four datasets, positive and negative emotions are mostly separated by neutral in the dimension Valence, while the emotions within each sentiment polarity mostly differs in Arousal and Dominance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "We select the following strong baseline models to compare with our model: BERT-Large  [60] : Initialised from pre-trained weights of BERT-Large. The [CLS] embedding is fine-tuned for ERC.\n\nDialogXL  [10] : This PLM-based work uses dialog-aware self-attention to model inter-and intra-speaker dependencies, and utilises utterance recurrence to model long-range contexts.\n\nRGAT  [61] : The model constructs a graph to introduce prior knowledge in context modelling, and combines a relation position encoding to introduce sequential information.\n\nCOSMIC  [26] : Based on RNN structure to model dependencies, this work introduces utterance-level commonsense knowledge to model the mental states of speakers. KI-Net  [24] : This work leverages token-level commonsense knowledge from knowledge graphs to enrich contexts, and implicitly introduces sentiment scores from sentiment lexicons to guide emotion reasoning.\n\nDAG-ERC  [33] : Based on RoBERTa-Large, this model builds a Directed Acyclic Graph (DAG) on the dialogue. Then DAGNN is used for graph embedding.\n\nSGED  [62] : This method proposes a speaker-guided encoder-decoder framework to exploit speaker information for ERC.\n\nSKAIG  [63] : This work builds a graph on the dialogue and utilises psychological commonsense knowledge to enrich edge representations.\n\nCoG-BART  [14] : Based on BART-Large  [64] , this work utilises SCL and a response generation auxiliary task to distinguish semantics of utterances with similar emotions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We conduct the experiments on IEMOCAP, MELD, and EmoryNLP using a single Nvidia Tesla V100 GPU with 16GB of memory, and set the batch size to 4. For large-scale dataset DailyDialog, we conduct the experiments using a single Nvidia Tesla A100 GPU with 80GB of memory, and set the batch size to 16. We initialise the pre-trained weights of PLMs and use the tokenization tools both provided by Huggingface 2 . The pre-trained knowledge adapter weights are from Wang et al.  [23] , and these weights are fused during training. We leverage AdamW optimiser  [65]  to train the model, with a linear warm-up learning rate scheduling  [66]  of warm-up ratio 20% and peak learning rate 1e-5. Due to the limitation of computation memory, we use mixed floating point precision  [67]  during training. Hyper-parameters are tuned on the validation set, where = 1.0 and α is tuned on the interval [0.5, 1.0] and set to 1.0 for MELD and 0.8 for all other datasets. S = 512, D h = 1024, L = 24 for RoBERTa-Large, and D h = 768, L = 12 for RoBERTa-Base. We set a dropout rate 0.1, a L 2 regularisation rate 0.01 to avoid over-fitting. We use the weighted-F1 measure as the evaluation metric for IEMOCAP, MELD and EmoryNLP. Since neutral occupies most of DailyDialog, we use micro-F1 for this dataset, and ignore the label neutral when calculating the results as in the previous works  [14] ,  [33] . All reported results are averages of five random runs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overall Performance",
      "text": "Table  3  presents the performance of our method, and compares it to the strong baseline models.\n\nAccording to the results, BERT-Large and DialogXL achieves competitive results on all datasets. These PLMbased methods are usually used as foundations for other works. KI-Net, COSMIC and SKAIG all explicitly incorporate factual knowledge at the token level or mental state knowledge at the utterance level, and achieve competitive performance especially on short-context datasets, such as over 57% on DailyDialog (7.9 utterances per dialogue). SGED also implicitly models speaker information via an encoder-decoder framework, which leads to a balanced improvement on all datasets and the best performance 40.24% on EmoryNLP. These results demonstrate that infusing task-related knowledge and information is beneficial for ERC task. Though RGAT and DAG-ERC both utilise graph structure to model the context, DAG-ERC significantly outperforms RGAT with over 3% gain on all datasets, showing the importance of more reasonable dialogue modelling structures. The competitive performance of CoG-BART also shows the effectiveness of other representation learning techniques such as supervised contrastive learning and response generation. We can only test HVAD-SCCL on IEMOCAP since all other datasets do not provide human-labelled utterancelevel VAD scores. According to the results, HVAD-SCCL achieves a new state-of-the-art result of 69.88%, but outperforms NRC-SCCL slightly on IEMOCAP, which does not correspond with our early hypothesis. We notice that NRC-VAD follows strict best-worst scaling annotation and aggregation processes with a minimum of 6 annotators per word. In comparison, the IEMOCAP VAD (HVAD) annotation process follows a rough scheme, which brings inaccuracy to the annotated labels and only provides coarsegrained VAD information within each emotion. According to the visualisation results in Figure  3 , the VAD shifts within each emotion are mostly discrete, which leads to a limited advantage over the fixed NRC-VAD prototypes. In addition, the VAD distributions of semantically similar emotions (e.g. Frustrated and Sad) appear to be more entangled, which increases confusion during the training process.\n\nOn the other hand, NRC-SCCL obtains competitive results on all datasets, and achieves new state-of-the-art re-   2 ), we find that many utterances labelled with these emotions do not yield positive sentiments. Therefore, unified VAD prototypes of the fuzzy emotions are misleading for many samples. We provide some cases in Table  4  to explain the above hypothesis. All examples are from the training set of EmoryNLP. In the samples of peaceful, utterance #1 expresses no apparent emotions with a moderate Valence score 0.460, utterance #2 conveys weak sadness and anger with lower Valence 0.317 and higher Dominance 0.470, and utterance #3 shows implicit happiness with higher Valence 0.752. In the samples of powerful, though all utterances express high Arousal (emotion intensity) which corresponds to the NRC-VAD emotion prototypes, these examples have different Valence and Dominance levels. For example, utterance #1 has high dominance 0.898 with a strong sense of control, but utterance #2 and #4 show relatively low dominance 0.519 and 0.372. On the other hand, utterance #3 conveys high Valence 0.722 with apparent pleasantness, while utterance #4 and #5 express sadness and fear with low Valence 0.325 and 0.322. Therefore, the model is unable to learn finegrained shifts in fuzzy emotions with the unified NRC-VAD emotion prototypes. One direction of our future work is leveraging more fine-grained supervision signals to handle the change of situations for fuzzy emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We investigate the performance of each proposed module via an ablation study on Lin-SCCL in Table  5 . According to the results, Lin-SCCL outperforms the context-aware utterance encoder by over 3% on IEMOCAP and DailyDialog, and over 2% on MELD and EmoryNLP. These improvements show the joint contribution of linguistic knowledge and SCCL. While the removal of either SCCL or LinAdapter decreases the model performance, removing SCCL leads to a more serious decrease in performance, with over a 1.5% drop for IEMOCAP, MELD and DailyDialog. According to the previous analysis in NRC-VAD emotion prototypes, SCCL is expected to be beneficial in distinguishing similar emotions, which is crucial for ERC and leads to more improvement than LinAdapter on these datasets. On EmoryNLP, LinAdapter benefits model performance more significantly than SCCL since the fuzzy emotions affect the contrastive learning process in VAD space, as analysed in Sec. 5.1. Lin-SCCL outperforms Lin-RL by over 1% on most datasets, showing SCCL as more appropriate for leveraging VAD information. A possible reason is that regression loss only introduces the current emotion's cluster-level representation, while SCCL also introduces and pushes apart all other emotion prototypes. SCCL further enables the model to be aware of the quantitative information between each pair of emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Empirical Comparison Of Knowledge Adapters",
      "text": "We analyse the effect of linguistic knowledge and factual knowledge on SCCL and the utterance encoder by comparing their performance in ERC, where the results are shown in Table  5 . According to the results, both LinAdapter and FacAdapter contribute to the performance positively, denoting the effectiveness of both knowledge types. Lin-SCCL outperforms Fac-SCCL on all four datasets, because linguistic knowledge provides utterance structure information to help discover the linguistic patterns for emotion expression, which benefits the contrastive learning process.\n\nOn the other hand, much factual knowledge is unrelated to affect and brings noise to the fine-grained emotion reasoning in SCCL. With the removal of SCCL, the utterance encoder achieves superior results on MELD and DailyDialog with FacAdapter, since the factual knowledge enriches the semantics of utterances, which benefits the dialogues with short contexts. This hypothesis is further indicated by the more significant improvement with LinAdapter on the other two rich-context datasets IEMOCAP and EmoryNLP. Overall, the empirical comparison of both knowledge adapters verifies the more benefits of linguistic knowledge on SCCL, and factual knowledge provides more information to the utterance encoder in short-context scenarios.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison Of Contrastive Learning Methods",
      "text": "We compare the results of different contrastive learning methods with RoBERTa encoder in Figure  4 .\n\nBoth large and base-size encoders are leveraged to compare the performance of encoders with different context modelling ability. \"VADCL\" denotes performing SCL directly on VAD space without introducing emotion prototypes, and \"Random-SCCL\" utilises the same structure as SCCL except randomly initialising the prototype for each emotion instead of utilising NRC-VAD.\n\nFor the results on RoBERTa-Large, SCCL outperforms the RoBERTa baseline with an improvement of 2.71% on IEMOCAP and 1.28% on MELD. VADCL achieves comparable performance with SCL on both datasets, proving the viability of performing contrastive learning on a lowdimensional space instead of the semantic space, which also provides useful information to facilitate the identification of emotions. SCCL also outperforms VADCL on both datasets, denoting that emotion prototypes guide samples of each emotion to cluster towards proper positions and maintain appropriate quantitative relations. To further analyse this hypothesis, we experiment on RoBERTa+Random-SCCL, and Random-SCCL yields worse outcomes than RoBERTa on both datasets. These results indicate that SCCL relies on emotion prototypes instead of merely clustering the same emotion as in SCL. The quantitative information embedded in the prototype of each emotion is eliminated as the consequence of the random initialisation, and these false relations mislead SCCL.\n\nWe also present the results with RoBERTa-Base encoder. As expected, RoBERTa-Large outperforms RoBERTa-Base with all contrastive learning methods. Similar conclusions about the comparisons of contrastive learning methods are drown from the results of RoBERTa-Base, showing that our above conclusions are robust with utterance representations of varying quality. In addition, the advantage of SCCL is more apparent on IEMOCAP, showing the consistent benefits of rich context on SCCL with different utterance encoders.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Batch Size Stability",
      "text": "With the change in batch size, we compare the training stability of SCCL, VADCL and SCL in Weighted-F1 scores on IEMOCAP. The results are shown in Figure  5 . Due to the limitation in computational resources, we utilise RoBERTa-Base as encoder and range the batch size from 2 0 = 1 to 2 4 = 16.\n\nAccording to the results, RoBERTa achieves the most stable outcomes as the batch size changes, with a Standard Fig.  6 . Key elements of the VAD visualisation results on all test sets. We only present the samples of representative emotions to provide a more intuitive view.\n\nDerivation (SD) of 0.32%. SCCL obtains 0.82% better results than RoBERTa on average, and performs stable as the batch size change, with a SD of 0.66%. This result shows that emotion prototypes obtained from NRC-VAD provide a fixed clustering direction for samples of each emotion. Therefore, the model does not need a large amount of observations at each training step for a stable convergence.\n\nFor SCL, while the results remain competitive and stable with large batch sizes, the performance drops fast below the RoBERTa baseline as the batch size decreases, leading to a high SD of 1.40%. In the extreme case where the batch size drops to 1, SCL fails to converge and brings noise to the training process, resulting in a severe 2.83% drop compared to RoBERTa. We also provide the variation scale at each batch size for SCCL and SCL. The results show that SCCL has relatively low variances compared to SCL, especially with small batch sizes. This result shows the benefit of NRC-VAD emotion prototypes and the low-dimensional contrastive space, which relieves the curse of dimensionality problem.\n\nVADCL suffers from the similar problems as SCL, with the highest SD of 1.67%. In addition, VADCL performs worse than SCL with small batch sizes. When observing only a few samples at each training step, the model fails to extract effective features in the three-dimensional space without emotion prototypes as the guidance.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Visualisation In Vad Space",
      "text": "With the three-dimensional VAD space, we are able to directly visualise the predictions instead of utilising dimension reduction techniques. Each VAD prediction also reflects the model's corresponding emotion reasoning process from the Valence-Arousal-Dominance perspective, which benefits interpretability. We present the key elements of the visualisation results on all four test sets in Figure  6 .\n\nFor IEMOCAP, we select and present semantically similar emotions (e.g., excited and happy) to gain clearer insights to the effect of SCCL, demonstarting their relationships to each other. For dissimilar emotions such as happy and sad, Valence alone separates them well enough. In addition, similar emotions are also well distinguished in VAD space by Arousal and Dominance, which corresponds with our early hypothesis. For example, frustrated and sad significantly vary in terms of Arousal, and happy and excited are jointly divided by Arousal and Dominance.\n\nIn section 5.1, we speculate that SCCL provides less improvement to EmoryNLP due to the fuzzy emotions where the VAD prototypes vary in different situations. In the visualisation on EmoryNLP, we present the two fuzzy emotions powerful, peaceful and two relatively invariant emotions joyful, sad to provide an intuitive comparison. According to the results, the model makes accurate and well-clustered VAD predictions for samples of joyful and sad, while the predictions of peaceful and powerful spread across the VAD space and fail to cluster.\n\nThe visualisation results of MELD and DailyDialog shows similar well-separated samples of emotions, such as joy/happy and surprise. However, the predictions of several emotions are inaccurate and not well clustered (e.g., anger and sad in MELD, fear in DailyDialog). We notice that the label distribution of both MELD and DailyDialog is highly imbalanced. Training samples of sad cover merely 6.8% in MELD. In DailyDialog, over 60% of utterances are labelled with neutral or happy, while the ratio of fear and sad are both below 5%. Therefore, another direction of future work is to handle the lack of training samples caused by label imbalance for SCCL. In addition, emotions such as anger and sad are often expressed implicitly, which is closely dependent on the context. Therefore, the lack of contextual information in MELD and DailyDialog brings more challenges to the prediction of these emotions. Overall, the above visualisation results correspond with other experimental outcomes.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a low-dimensional supervised cluster-level contrastive learning model for emotion recognition in conversations. We reduce the high-dimensional supervised contrastive learning space to a three-dimensional space Valence-Arousal-Dominance, and incorporate VAD prototypes from the emotion lexicon NRC-VAD by proposing the novel SCCL method. In addition, we infuse linguistic knowledge and factual knowledge into the context-aware utterance encoder by utilising the pre-trained knowledge adapters.\n\nExperimental results show that our method achieves new state-of-the-art results on three datasets IEMOCAP, MELD, and DailyDialog. Ablation study proves the effectiveness of each proposed module, and further analysis indicates that VAD space is an appropriate and interpretable space for SCCL. Emotion prototypes from NRC-VAD provide useful quantitative information to guide SCCL, which improves model performance and stabilises the training process. The knowledge infused by pre-trained knowledge adapters also enhances the performance of the utterance encoder and SCCL. In the future, we will leverage more fine-grained supervision signals to handle fuzzy emotions, and develop efficient methods to alleviate label imbalance and lack of context problems for SCCL.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of appropriate emotion prototypes in VAD space",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of our model architecture. In ERC and SCCL, each colour denotes an emotion category. (a). linguistic and factual knowledge",
      "page": 3
    },
    {
      "caption": "Figure 2: (a), we follow the methodology of",
      "page": 3
    },
    {
      "caption": "Figure 2: (c), following the idea of labels as",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualisation of HVAD annotations in IEMOCAP training set.",
      "page": 6
    },
    {
      "caption": "Figure 3: , the VAD shifts within",
      "page": 6
    },
    {
      "caption": "Figure 4: Performance of different contrastive learning methods with",
      "page": 8
    },
    {
      "caption": "Figure 4: Both large and base-size encoders are leveraged to com-",
      "page": 8
    },
    {
      "caption": "Figure 5: Change of F1 scores with different batch sizes on IEMOCAP,",
      "page": 8
    },
    {
      "caption": "Figure 5: Due to the",
      "page": 8
    },
    {
      "caption": "Figure 6: Key elements of the VAD visualisation results on all test sets. We only present the samples of representative emotions to provide a more",
      "page": 9
    },
    {
      "caption": "Figure 6: For IEMOCAP, we select and present semantically simi-",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Statistics of the datasets. Conv. and Utter. denotes the conversation",
      "page": 5
    },
    {
      "caption": "Table 2: The NRC-VAD assignments to all emotions in the four datasets.",
      "page": 5
    },
    {
      "caption": "Table 2: According",
      "page": 5
    },
    {
      "caption": "Table 3: presents the performance of our method, and com-",
      "page": 6
    },
    {
      "caption": "Table 3: The test results on IEMOCAP, MELD, EmoryNLP and DailyDialog",
      "page": 6
    },
    {
      "caption": "Table 4: Some samples of the fuzzy emotion peaceful and powerful that shift in Valence-Arousal-Dominance. We provide the NRC-VAD emotion",
      "page": 7
    },
    {
      "caption": "Table 2: ), we ﬁnd that many",
      "page": 7
    },
    {
      "caption": "Table 4: to explain the above",
      "page": 7
    },
    {
      "caption": "Table 5: According to",
      "page": 7
    },
    {
      "caption": "Table 5: Results of ablation study for two knowledge types. Lin-SCCL denotes",
      "page": 7
    },
    {
      "caption": "Table 5: According to the results, both LinAdapter",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "4",
      "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "5",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Y Ma",
        "K Nguyen",
        "F Xing",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Emotion detection of textual data: An interdisciplinary survey",
      "authors": [
        "S Zad",
        "M Heidari",
        "J Jones",
        "O Uzuner"
      ],
      "year": "2021",
      "venue": "AIIoT"
    },
    {
      "citation_id": "7",
      "title": "Sentiment classification in customer service dialogue with topic-aware multi-task learning",
      "authors": [
        "J Wang",
        "J Wang",
        "C Sun",
        "S Li",
        "X Liu",
        "L Si",
        "M Zhang",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "8",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "P Nandwani",
        "R Verma"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "9",
      "title": "Understanding emotions in text using deep learning and big data",
      "authors": [
        "A Chatterjee",
        "U Gupta",
        "M Chinnakotla",
        "R Srikanth",
        "M Galley"
      ],
      "year": "2019",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "10",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "11",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "X Qiu",
        "T Sun",
        "Y Xu",
        "Y Shao",
        "N Dai",
        "X Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences"
    },
    {
      "citation_id": "12",
      "title": "On the sentence embeddings from pre-trained language models",
      "authors": [
        "B Li",
        "H Zhou",
        "J He",
        "M Wang",
        "Y Yang",
        "L Li"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "13",
      "title": "Declutr: Deep contrastive learning for unsupervised textual representations",
      "authors": [
        "J Giorgi",
        "O Nitski",
        "B Wang",
        "G Bader"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "14",
      "title": "Contrast and generation make BART a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "AAAI"
    },
    {
      "citation_id": "15",
      "title": "A survey of state-of-theart approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "16",
      "title": "Prototypical contrastive learning of unsupervised representations",
      "authors": [
        "J Li",
        "P Zhou",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "ICLR. OpenReview.net"
    },
    {
      "citation_id": "17",
      "title": "Comatch: Semi-supervised learning with contrastive graph regularization",
      "authors": [
        "J Li",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "18",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "20",
      "title": "Framework for a comprehensive description and measurement of emotional states",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1995",
      "venue": "Framework for a comprehensive description and measurement of emotional states"
    },
    {
      "citation_id": "21",
      "title": "Linguistic knowledge and transferability of contextual representations",
      "authors": [
        "N Liu",
        "M Gardner",
        "Y Belinkov",
        "M Peters",
        "N Smith"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "22",
      "title": "Sentilare: Sentimentaware language representation learning with linguistic knowledge",
      "authors": [
        "P Ke",
        "H Ji",
        "S Liu",
        "X Zhu",
        "M Huang"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "23",
      "title": "K-adapter: Infusing knowledge into pretrained models with adapters",
      "authors": [
        "R Wang",
        "D Tang",
        "N Duan",
        "Z Wei",
        "X Huang",
        "J Ji",
        "G Cao",
        "D Jiang",
        "M Zhou"
      ],
      "year": "2021",
      "venue": "Findings of ACL"
    },
    {
      "citation_id": "24",
      "title": "Knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations",
      "authors": [
        "Y Xie",
        "K Yang",
        "C Sun",
        "B Liu",
        "Z Ji"
      ],
      "year": "2021",
      "venue": "Findings of EMNLP"
    },
    {
      "citation_id": "25",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "26",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of EMNLP. Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "ICON: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "28",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "29",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "30",
      "title": "Hitrans: A transformerbased context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "COLING. International Committee on Computational Linguistics"
    },
    {
      "citation_id": "31",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "32",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "EMNLP. Association for Computational Linguistics"
    },
    {
      "citation_id": "33",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "ACL. Association for Computational Linguistics"
    },
    {
      "citation_id": "34",
      "title": "S+ page: A speaker and position-aware graph neural network model for emotion recognition in conversation",
      "authors": [
        "C Liang",
        "C Yang",
        "J Xu",
        "J Huang",
        "Y Wang",
        "Y Dong"
      ],
      "year": "2021",
      "venue": "S+ page: A speaker and position-aware graph neural network model for emotion recognition in conversation",
      "arxiv": "arXiv:2112.12389"
    },
    {
      "citation_id": "35",
      "title": "Knowledge aware emotion recognition in textual conversations via multi-task incremental transformer",
      "authors": [
        "D Zhang",
        "X Chen",
        "S Xu",
        "B Xu"
      ],
      "year": "2020",
      "venue": "COLING. International Committee on Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "37",
      "title": "Hierarchical pre-training for sequence labelling in spoken dialog",
      "authors": [
        "E Chapuis",
        "P Colombo",
        "M Manica",
        "M Labeau",
        "C Clavel"
      ],
      "year": "2020",
      "venue": "Findings of EMNLP"
    },
    {
      "citation_id": "38",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "L Zhu",
        "G Pergola",
        "L Gui",
        "D Zhou",
        "Y He"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "39",
      "title": "Emotion analysis as a regression problem -dimensional models and their implications on emotion representation and metrical evaluation",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2016",
      "venue": "ECAI"
    },
    {
      "citation_id": "40",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "41",
      "title": "Dimensional emotion detection from categorical emotion",
      "authors": [
        "S Park",
        "J Kim",
        "S Ye",
        "J Jeon",
        "H Park",
        "A Oh"
      ],
      "year": "2021",
      "venue": "EMNLP. Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Understanding the role of affect dimensions in detecting emotions from tweets: A multi-task approach",
      "authors": [
        "R Mukherjee",
        "A Naik",
        "S Poddar",
        "S Dasgupta",
        "N Ganguly"
      ],
      "year": "2021",
      "venue": "SIGIR"
    },
    {
      "citation_id": "43",
      "title": "Primitivesbased evaluation and estimation of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "44",
      "title": "A multi-task learning framework for emotion recognition using 2d continuous space",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "45",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "46",
      "title": "Contrastive clustering",
      "authors": [
        "Y Li",
        "P Hu",
        "J Liu",
        "D Peng",
        "J Zhou",
        "X Peng"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "47",
      "title": "A low rank promoting prior for unsupervised contrastive learning",
      "authors": [
        "Y Wang",
        "J Lin",
        "Q Cai",
        "Y Pan",
        "T Yao",
        "H Chao",
        "T Mei"
      ],
      "year": "2021",
      "venue": "A low rank promoting prior for unsupervised contrastive learning",
      "arxiv": "arXiv:2108.02696"
    },
    {
      "citation_id": "48",
      "title": "Improving contrastive learning by visualizing feature transformation",
      "authors": [
        "R Zhu",
        "B Zhao",
        "J Liu",
        "Z Sun",
        "C Chen"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "49",
      "title": "Consert: A contrastive framework for self-supervised sentence representation transfer",
      "authors": [
        "Y Yan",
        "R Li",
        "S Wang",
        "F Zhang",
        "W Wu",
        "W Xu"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "50",
      "title": "Self-guided contrastive learning for BERT sentence representations",
      "authors": [
        "T Kim",
        "K Yoo",
        "S Lee"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "51",
      "title": "Simcse: Simple contrastive learning of sentence embeddings",
      "authors": [
        "T Gao",
        "X Yao",
        "D Chen"
      ],
      "year": "2021",
      "venue": "EMNLP. Association for Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "Supervised contrastive learning for pre-trained language model fine-tuning",
      "authors": [
        "B Gunel",
        "J Du",
        "A Conneau",
        "V Stoyanov"
      ],
      "year": "2021",
      "venue": "Supervised contrastive learning for pre-trained language model fine-tuning"
    },
    {
      "citation_id": "53",
      "title": "Improving textual emotion recognition based on intra-and inter-class variation",
      "authors": [
        "H Alhuzali",
        "S Ananiadou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "55",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "J Kirkpatrick",
        "R Pascanu",
        "N Rabinowitz",
        "J Veness",
        "G Desjardins",
        "A Rusu",
        "K Milan",
        "J Quan",
        "T Ramalho",
        "A Grabska-Barwinska"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "56",
      "title": "T-rex: A large scale alignment of natural language with knowledge base triples",
      "authors": [
        "H Elsahar",
        "P Vougiouklis",
        "A Remaci",
        "C Gravier",
        "J Hare",
        "F Laforest",
        "E Simperl"
      ],
      "year": "2018",
      "venue": "LREC"
    },
    {
      "citation_id": "57",
      "title": "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "58",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20, 000 english words",
      "authors": [
        "S Mohammad"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "59",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "IJCNLP. Asian Federation of Natural Language Processing"
    },
    {
      "citation_id": "60",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "61",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "62",
      "title": "Speaker-guided encoder-decoder framework for emotion recognition in conversation",
      "authors": [
        "Y Bao",
        "Q Ma",
        "L Wei",
        "W Zhou",
        "S Hu"
      ],
      "year": "2022",
      "venue": "IJCAI. ijcai.org"
    },
    {
      "citation_id": "63",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of EMNLP"
    },
    {
      "citation_id": "64",
      "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "65",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "ICLR. OpenReview.net"
    },
    {
      "citation_id": "66",
      "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "authors": [
        "P Goyal",
        "P Dollár",
        "R Girshick",
        "P Noordhuis",
        "L Wesolowski",
        "A Kyrola",
        "A Tulloch",
        "Y Jia",
        "K He"
      ],
      "year": "2017",
      "venue": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "arxiv": "arXiv:1706.02677"
    },
    {
      "citation_id": "67",
      "title": "",
      "authors": [
        "P Micikevicius",
        "S Narang",
        "J Alben",
        "G Diamos",
        "E Elsen",
        "D García",
        "B Ginsburg",
        "M Houston",
        "O Kuchaiev",
        "G Venkatesh",
        "H Wu"
      ],
      "year": "2018",
      "venue": ""
    }
  ]
}