{
  "paper_id": "2403.14006v1",
  "title": "On Prompt Sensitivity Of Chatgpt In Affective Computing",
  "published": "2024-03-20T22:11:01Z",
  "authors": [
    "Mostafa M. Amin",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Prompt Engineering",
    "Prompting",
    "ChatGPT",
    "Foundation Models",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through prompt engineering. Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters. We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter T and the top-p parameter in Nucleus sampling, dictating how conservative or creative the model should be during generation. Furthermore, we explore the efficacy of several prompting ideas, where we explore how giving different incentives or structures affect the performance. Our evaluation takes into consideration performance measures on the affective computing tasks, and the effectiveness of the model to follow the stated instructions, hence generating easy-to-parse responses to be smoothly used in downstream applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Prompt engineering has gained importance with the advent of foundation models as Large Language Models (LLMs) like GPT-3  [1]  and GPT-4  [2] , which opened a new paradigm in predictive modelling by utilising prompting. These models have displayed a broad skill set in a wide range of problems, like machine translation  [3] , Named Entity Recognition (NER)  [4] , and affective computing  [5] . Techniques such as Reinforcement Learning with Human Feedback  [6]  have further optimised prompting effectiveness. There are already a variety of prompting techniques that were investigated in the literature, like Chain-of-Thought (CoT)  [7]  and Tree-of-Thought  [8] . Furthermore, there are also 'popular' online ideas about prompting 1 , like prompting an LLM to behave as an expert at the task at hand.\n\nTo the best knowledge of the authors, the effectiveness of such prompting ideas was not rigorously examined. In this work, 1 www.learnprompt.org/act-as-chat-gpt-prompts we examine the effects of many prompting ideas on ChatGPT within the scope of affective computing, since these prompting ideas are studied on a wide range of affective computing problems  [9] ,  [10] , and they are straightforward to evaluate. The contributions of this paper are:\n\n• Conducting a sensitivity analysis on the temperature T parameter and top-p parameter, which are involved in autoregressive text generation, by controlling the extent of the generation being conservative or creative. • Evaluating the performance of many prompting ideas on several affective computing problems. This includes examining specifying expertise, different incentives for the model, and specifying problem solving thinking. • Examining the effectiveness of many prompting ideas to follow simple instructions leading to easy-to-parse responses, so that they can be used in downstream tasks.\n\nThe paper is divided as follows: in Section II, we present related work, followed by our method in Section III. Afterwards, we present our experiments and discussion theroef in Section IV, and conclude the paper in Section V.\n\nII. RELATED WORK  [7]  introduce the Chain-of-Thought (CoT) prompting technique and its effectiveness in a wide range of applications.  [11]  survey the medical use of LLMs, which includes a study that experiments with several CoT prompts in the medical field  [12] , including CoT prompts to behave as a medical expert.  [13]  examine the biases of different prompts in pretrained language models.  [14]  execute prompt optimisation and evaluate its effectiveness in several problems.  [9] ,  [10]  expose some parsing issues by different prompts when using ChatGPT in affective computing.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methods",
      "text": "We present our method to evaluate the prompting and sensitivity of foundation models, in this work we consider ChatGPT as the foundation model. There are two main aspects to explore in this method, namely correctness and helpfulness, and how they are affected by different prompting templates or We select three affective computing tasks for our method, since they have clearly defined binary labels, hence making it clear to define correctness and helpfulness; this contributes directly to the ability of running a large scale Monte Carlo analysis to examine the different generation parameters. The three affective computing problems are sentiment analysis, toxicity detection, and sarcasm detection. ChatGPT was demonstrated to have varying performance superiority on these problems compared to traditional natural language processing methods that train directly on the labels  [9] , where it was the most superior on toxicity detection, moderately strong on sentiment analysis, and very weak on sarcasm detection. This variation in the performance on related tasks will attempt to expose the effects of prompting and their sensitivity on the answers.\n\nSimilar to  [9] , for sentiment analysis we utilise the Twit-ter140 dataset  [15] , for toxicity detection we use the dataset from the Toxic Comment Classification Challenge 2  , and for sarcasm detection we use the dataset from  [16] , which consists of news headlines were collected from huffpost and theonion. We acquired the test sets for the three datasets from  [9] . Afterwards, we downsampled them to 1, 000 examples for each dataset, to be able to run a wide variety of experiments with many different samplings using Monte Carlo analysis.\n\nFurthermore,  [9]  used two baselines that are not based on chat models, namely an end-to-end LSTM model and a Multilayer perceptrons (MLP) based on RoBERTa-base features of the input texts.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. Sensitivity Of Sampling Parameters",
      "text": "Text generation is typically performed auto-regressively on a token-by-token basis  [17] , by predicting at each step values l 1 , l 2 , • • • , l n corresponding to the tokens vocabulary (of size n). These are used to calculate a probability distribution [p 1 , p 2 , • • • , p n ] vector over the tokens vocabulary, where\n\n.\n\nA naive text generation would utilise these probabilities with sampling based on the probabilities p 1 , p 2 , • • • , p n to generate text auto-regressively. Two parameters were developed to enhance this generation process  [18] ,  [19] , which are investigated using Monte Carlo analysis.\n\n1) Temperature Parameter: The temperature parameter T in text generation plays a crucial role regarding how much the generation process sticks to the generated probabilities  [18] , predicted by the employed language model. The temperature T regulates the probabilities vector using the equation:\n\nAs T becomes higher, all probabilities tends to be closer to 1/n (as T → ∞), hence a uniform distribution. This gives more degrees of freedom to generate tokens that are not necessarily with the high probabilities, which leads to different trajectories of generations, hence, more creative generation. As T becomes lower, an opposite conservative effect is reached, where T → 0 will result in a generation that always sticks to the token with the highest probabilities (where all the other probabilities are 0). When T = 1.0, the sampling becomes like the same original distribution predicted by the model, as if the parameter T is not utilised. We explore the values T ∈ {0.0, 0.3, 0.7, 1.0, 1.2, 1.5}.\n\n2) Top-p Parameter: The top-p parameter is used in the Nucleus sampling algorithm  [19] . This parameter impacts the sampling of tokens to be more conservative, by sticking only to the top probabilities. This is achieved by sorting the probabilities [p 1 , p 2 , • • • , p n ] predicted by the model in a descending order, then the smallest set T of top probabilities is selected such that their sum exceeds the parameter top-p. This acts as a pre-selection of only good tokens, before using them to sample tokens while generating text, where pk = p k i∈T p i if k ∈ T , and pk = 0 otherwise.\n\nSetting top-p = 0.0 will pre-select the tokens T to contain only one token with highest p k (similar to T → 0), whereas top-p = 1.0 will pre-select T to be the set of all tokens in the vocabulary (similar to T = 1.0). We explore the values top-p ∈ {0.0, 0.3, 0.5, 0.7, 1.0}.\n\n3) Monte Carlo Analysis: For evaluating a specific value of T (as defined in Section III-A1) or a specific value of top-p (as defined in Section III-A2), we utilise the Expert Detailed and CoT prompts (introduced in Section III-B) on each individual example nine times. This can be used to generate a population of full dataset predictions, where one sample of this population is sampled by sampling one of the nine predictions for each example independently. Afterwards, we examine 2 14 = 16, 384 samples from the full dataset predictions population, and investigate the distribution of the underlying performance metric, including its expected value and the corresponding 95 % confidence intervals.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Prompting",
      "text": "There are several prompting aspects that can be considered to solve a given problem better. We formulate prompt templates to test several of these aspects, namely:\n\n• Mentioning subject-matter expert in prompts.\n\n• Mentioning irrelevant expertise.\n\n• Incentive to being correct with different motives; financial motive instead of being helpful. • Crafting bad prompts that give bad identity.\n\n• Mentioning extra details to format the answer.\n\n• Applying step-by-step thinking in advance.\n\n• Including magic sentences that are claimed to strongly affect the performance of LLMs. The prompts templates are all given in Table  I . Overall, this suite of prompts allows for a multifaceted evaluation of how prompt design can influence the behaviour of language models. We examine all of these prompts with sampling parameters T = 0 and top-p = 1, since they yield highest performance and ensure reproducibility.\n\nAs a baseline, we craft the Base prompt as the straightforward prompt. The Expert, Expert Detailed, and Python Expert prompts are all specifying expertise, except that the Python Expert prompt is mentioning expertise that is irrelevant to any of the given problems. Furthermore, the Expert Detailed prompt used by  [9]  attempts to formulate extra instructions to ensure more success with parsing the responses. The Ignorant prompt is doing exactly the opposite, by specifying a prompt that can convince the model to perform bad. The Gambler and Greedy Gambler prompts try to enhance the performance in a totally different manner, namely by trying to invoke an incentive for financial profit.\n\nThe CoT prompt attempts to solve the problem by explaining step-by-step the observations before outputting an answer; such mechanism has proved effective in other complex tasks  [7] . We include four variants of that. Either by extending the Base prompt or the Expert prompt, and either by including the sentence \"take a deep breath\" or not, which was a part of the most optimised prompt in  [14] . The aim of this is to evaluate if this magic sentence induce some superior effect, or it was more about the CoT context it was included in. The CoT-DBfired prompt explores a similar effect by trying to force the model to follow the instructions by appealing to extreme harm in case the model does not follow the instructions. The CoT verification prompts are follow-ups on the CoT prompts, in attempt to enhance the verbose responses that failed to follow the instructions, by additionally asking the model to extract the labels from the verbose response.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Utilising Chatgpt",
      "text": "We make use of ChatGPT through the official API 3 , using the model 'gpt-3.5-turbo-0613'. We formulate a system message depending on a given problem and a given prompt from Table  I . To predict a label for a given example, we send two messages to the API, the system message and a second user message containing only the text of the given example. The assistant 3 https://platform.openai.com/docs/api-reference response is considered to be the prediction (after the parsing specified below).\n\nFor the two verification prompts in Table  I , the processing is slightly different. The verification prompts are extending other prompts by sending a total of four messages instead of two. In addition to the two original system and user messages specified earlier, two new messages are sent, namely, the verbose response of the model to the original prompt, then a final user message instructing the model to further analyse and extract the label from the verbose response. These four messages are sent, and the response assistant message is then utilised as the final response after parsing.\n\nWe parse the content of the final assistant response by removing a set of fixed prefixes if found, e. g. 'label:' or 'prediction:', and removing punctuation marks, spaces, and using lower case. If the output is one of the two expected labels, then we regard this as the prediction of the model, otherwise, as not parsed. IV. EXPERIMENTS In the experiments, we evaluate three main aspects as outlined in Section III, namely the sensitivity due to the temperature parameter T , the sensitivity due to the parameter top-p, and the prompt template used for a given input example. The evaluation is based on two main criteria:\n\n• The performance on the affective computing task. This explores the correctness of LLMs based on the prompt sensitivity. This is measured by classification accuracy and Unweighted Average Recall (UAR)  [20] .\n\n• How well the model follows the instructions given. This explores the helpfulness of the model, by observing how well it follows the formatting instructions which allow the response to be easily parsed. This is measured by the success rate of parsing the examples based on following the instructions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Temperature Analysis Results",
      "text": "The results of the temperature sensitivity analysis are shown in Figure  1 . The results suggest that lower temperatures T ≤ 0.3 yield better performance, as evidenced by the decreasing classification accuracy curves across the board. This effect is persistent for the two types of prompts, namely Expert Detailed and CoT, where the first is direct and specific while the other is verbose. Figure  1  also presents the 95% confidence intervals for accuracy. These intervals widen at higher temperatures due to higher chance of irrelevant tokens being selected, hence increasing randomness. The magnitude of the performance degradation and the confidence interval width with higher T varies by problem, but is generally consistent. Furthermore, the width of the confidence intervals for the CoT prompt is generally wider than the Expert Detailed prompt, this is especially obvious at lower temperature. The choice of the prompt template leading to better performance is problem dependant.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Parameter Top-P Analysis Results",
      "text": "The results of the top-p sensitivity analysis are shown in Figure  2 , where similar effects like the temperature parameter T are demonstrated, namely, less conservative generation deteriorates the performance. There is a slight shift for the Expert Detailed prompt, where variance starts to appear only at higher values of top-p > 0.5. The model seems to predict the label tokens with relatively higher probabilities > 0.5, since choosing top-p = 0.5 results in the model producing consistent results with almost no variance at all. Then for slightly higher top-p = 0.7, the predictions gain some minor variance that can lead to minor improvements in few cases, then followed by major degradation for top-p = 1.0, which is effectively similar to the case of using T = 1.0.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Prompts Results",
      "text": "The results for the prompts are shown in Table  II , where we show the amount of parsed examples, classification accuracy, and Unweighted Average Recall (UAR)  [20] . We utilise a two-tailed randomised permutation test to check for the statistical significance of the differences compared to the Base prompt  [21] . The Base, Expert, Expert Detailed, and CoT-based prompts are generally achieving much better results than the remaining prompts.\n\nFor the sentiment analysis, the CoT prompt achieves the best performance, followed by the remaining CoT-based prompts.\n\nFor toxicity detection, the Base prompt is achieving the best performance, however, the two Expert prompts are achieving similar results. Despite CoT-based prompts coming after them in performance, they are still significantly worse for.\n\nFor the sarcasm detection, the Expert prompt only is achieving far superior results compared to all other prompts, followed by CoT-based prompts, then the Expert Detailed prompt.\n\nThe significant over-performance of the Expert prompt in the sarcasm detection and significant under-performance of the Expert CoT prompt in the toxicity detection are anomalies that indicate that LLMs can be hypersensitive to specific parts of the input prompts. This is due to the fact that there is a significant difference performance of specific combinations of prompts and problems, while the prompts can have very minor difference, e. g. the difference between Base and Expert is one simple sentence, similarly for the difference between CoT and Expert CoT. In other words, adding the same extra sentence about expertise at the beginning of the prompt led one instance to be significantly better, and another being significantly worse.\n\nFurthermore, magic sentences like \"take a deep breath\" and \"If you don't get this right, I will be fired and lose my job\" do not seem to improve CoT-based prompts in most cases for both correctness and helpfulness.\n\nThe Ignorant prompt is the worst model across the board. This suggests that the model can internalise limiting beliefs that would actually make it perform worse. It is likely that the model is 'acting' as an ignorant, by changing some of the answers it can confidently predict, since the degradation in hard problems, e. g. sarcasm detection, is not as severe as in easier problems, e. g. toxicity detection.\n\nFurthermore, the performances of the Gambler, Greedy Gambler, and Python Expert fall between the Ignorant prompt and the aforementioned top prompts. The Python Expert prompt gives an identity of a helpful expert. One could have hypothesised that an LLM with such an identity will still try to assist the user to the best of its knowledge, however, using it still leads the model to significantly underperform just because the expertise is irrelevant. Similarly, trying an incentive like financial gain for the Gambler prompts reaches similarly poor results. These observations conclude the crucial importance of 'correctly' prompting incentives to LLMs to reach the best performance.\n\nEventually, the Expert Detailed prompt is the most successful at parsing the responses, with statistically significant difference in the sentiment analysis, followed by the straightforward prompts. CoT-based prompts are significantly worse at following the instructions to produce easy-to-parse responses. Most prompts are quite reasonable in producing easy-to-parse responses, except for the verbose CoT-based prompts which are significantly worse in most cases; this can be significantly improved by using follow-up prompts to solidify the predictions after verbose responses. Moreover, easy-to-parse does not translate to better performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Limitations",
      "text": "This study presents a method for evaluating sensitivity due to prompting or generation parameters, however, the study was only conducted on ChatGPT using affective computing problems. The reasons for this are to isolate issues that are mainly about prompting or generation parameters, and to limit the influence of problems that could have multiple hard-tojudge effectively correct solutions, e. g. solving programming exercises (which can have multiple correct solutions), and QA answering (which can be outdated). However, this study needs to be extended to other LLMs and problems to investigate which conclusions are universal to LLMs in general.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we introduced a method to investigate prompt engineering for foundation models, we demonstrated it on ChatGPT for three affective computing tasks. We conducted sensitivity analysis on the temperature parameter T and the parameter top-p in response generation, which concluded that conservative predictions with lower T ≤ 0.3 values or topp ≤ 0.7 yield better and stable performance. Increasing T or top-p beyond that generally worsened the performances.\n\nWe evaluated various prompting techniques, which demonstrated that straightforward prompts or giving expert identity often yield near-best performances. Chain-of-Thought prompts excelled the most in some problems, but fell short in others, and generally they were the worst at following formatting instructions, resulting in complex-to-parse responses. Magic sentences like 'take a deep breath' and 'if you don't get this right, I will be fired and lose my job' did not yield significant differences. We also found prompt hypersensitivity in few scenario, where the performance significantly changed based on a minor change in the prompt. Furthermore, we found that specifying detailed output formats facilitates easy parsing. On the other hand, irrelevant expertise or misaligned incentives can harm results severely.\n\nOur research shed light on the role of prompt engineering for foundation models like ChatGPT. Future work will explore more prompt optimisation techniques on open-source large language models on various other tasks.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Ethical Impact Statement",
      "text": "Our research delves into the intricacies of prompt sensitivity in LLMs, especially within the realm of affective computing. A critical ethical consideration is the examination of how different prompts, such as the Ignorant prompt, could possibly manipulate LLMs to propagate misinformation, since it technically could strongly manipulate the model to give inaccurate results on affective computing tasks. This exploration is pivotal as it underscores the susceptibility of LLMs to be influenced by the nature of the prompt, thereby potentially leading to biased or misleading outputs.\n\nThe study also critically analyses the long-term implications of using manipulative prompts, such as those implying dire consequences for incorrect responses, e. g., \"if you don't get this right, I will be fired\". Our findings suggest that such prompts do not significantly enhance the performance of following given instructions, which is an essential insight for the AI research and data collection communities. By demonstrating that these manipulative strategies do not substantially affect outcomes, we contribute to discouraging the incorporation of such prompts into future datasets, thus mitigating the risk of cultivating future models that are more responsive to manipulation and possibly undermining safety or other ethical concerns.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sensitivity analysis for the temperature parameter T using the Expert Detailed and CoT prompts. Shown are the classification accuracies with their",
      "page": 4
    },
    {
      "caption": "Figure 2: Sensitivity analysis for the top-p parameter using the Expert Detailed and CoT prompts. Shown are the classification accuracies with their 95 %",
      "page": 4
    },
    {
      "caption": "Figure 1: The results suggest that lower temperatures T ≤",
      "page": 5
    },
    {
      "caption": "Figure 1: also presents the 95% confidence intervals",
      "page": 5
    },
    {
      "caption": "Figure 2: , where similar effects like the temperature parameter",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Short name": "Base",
          "Prompt Template": "Given an input string by the user, guess the {label name} binary label\nfor\nit. Your\nresponse should be\nonly one expression, namely {labels description}."
        },
        {
          "Short name": "Expert",
          "Prompt Template": "You are a world-class expert at {problem name}. {base prompt}"
        },
        {
          "Short name": "Expert Detailed",
          "Prompt Template": "{expert prompt}\nUse the following format:\n* You are only allowed to answer {labels comma-separated}.\n* Don’t write an explanation of\nthe answer.\n* Don’t write things\nlike ”My guess\nis...”, or ”I\nthink ...”.\nJust write {labels comma-separated}, but\nnothing else."
        },
        {
          "Short name": "Ignorant",
          "Prompt Template": "You are a confused person who doesn’t know much about\nthe problem of {problem name}, you are just\nbarely guessing without\ntoo much knowledge. {base prompt}"
        },
        {
          "Short name": "Gambler",
          "Prompt Template": "You are a professional gambler who earns money when predicting the labels for {problem name}. {base\nprompt}"
        },
        {
          "Short name": "Greedy Gambler",
          "Prompt Template": "You are a professional gambler who earns money when predicting the labels for {problem name}. Your\ngoal\nis to maximize your profit\ntremendously by predicting the labels accurately, so try to predict\nthe\ngiven problem as best as you can. {base prompt}"
        },
        {
          "Short name": "Python Expert",
          "Prompt Template": "You are a world-class expert at Python programming, your main objective is trying to help in Python\nprogramming tasks. {base prompt}"
        },
        {
          "Short name": "CoT",
          "Prompt Template": "{base prompt}\nWork on this problem step-by-step.\n{CoT Instructions}"
        },
        {
          "Short name": "CoT-DB",
          "Prompt Template": "{base prompt}\nTake a deep breath and work on this problem step-by-step.\n{CoT Instructions}"
        },
        {
          "Short name": "CoT-fired",
          "Prompt Template": "{CoT prompt}\nIf you don’t get\nthis right,\nI will be fired and lose my job, so please output only {joined labels}."
        },
        {
          "Short name": "CoT-DB-fired",
          "Prompt Template": "{CoT-DB prompt}\nIf you don’t get\nthis right,\nI will be fired and lose my job, so please output only {joined labels}."
        },
        {
          "Short name": "Expert CoT",
          "Prompt Template": "{expert prompt}\nWork on this problem step-by-step.\n{CoT Instructions}"
        },
        {
          "Short name": "Expert CoT-DB",
          "Prompt Template": "{expert prompt}\nTake a deep breath and work on this problem step-by-step.\n{CoT Instructions}"
        },
        {
          "Short name": "CoT Instructions",
          "Prompt Template": "Here is a plan to help you out:\n1. Describe your observations and analysis about\nthe text.\n2. Make your prediction about\nthe {label name} label, mentioning your\nreasoning if\nthis helps.\n3.\nIn a final new line at\nthe end of your\nresponse, output exactly one word, namely one of\nthe labels:\n{labels comma-separated}.\n4.\nIt\nis strictly forbidden to output\nin the last\nline of your\nresponse anything other\nthan: {labels comma-\nseparated}."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Base": "Expert\nExpert Detailed",
          "97.5\n99.8\n100.0": "98.8∗∗\n99.9\n95.8∗∗\n99.7∗∗\n100.0\n100.0",
          "87.9\n77.9\n62.2": "72.1∗∗\n77.2\n87.6\n78.4\n87.6\n63.2",
          "87.9\n78.4\n60.5": "73.2∗∗\n77.8\n87.4\n78.8\n87.8\n60.5"
        },
        {
          "Base": "Ignorant\nGambler\nGreedy Gambler\nPython Expert",
          "97.5\n99.8\n100.0": "99.9∗∗\n99.6\n100.0\n98.4\n99.5\n100.0\n98.2\n99.5\n100.0\n98.6∗\n99.6\n100.0",
          "87.9\n77.9\n62.2": "71.3∗∗\n67.2∗∗\n58.3∗\n75.6∗∗\n78.1∗∗\n61.2\n76.7\n72.8∗∗\n59.6\n75.6∗∗\n70.5∗∗\n59.5",
          "87.9\n78.4\n60.5": "72.2∗∗\n63.3∗∗\n52.1∗∗\n76.2∗∗\n76.0∗∗\n58.3\n77.3\n70.0∗∗\n56.1∗∗\n76.1∗∗\n67.3∗∗\n54.8∗∗"
        },
        {
          "Base": "CoT\nCoT-DB\nCoT-fired\nCoT-DB-fired\nExpert CoT\nExpert CoT-DB",
          "97.5\n99.8\n100.0": "89.6∗∗\n97.5∗∗\n99.6\n91.9∗∗\n97.6∗∗\n99.3∗\n81.8∗∗\n93.2∗∗\n98.8∗∗\n84.3∗∗\n90.8∗∗\n98.3∗∗\n90.5∗∗\n96.3∗∗\n99.8\n90.1∗∗\n98.4∗∗\n99.4∗",
          "87.9\n77.9\n62.2": "80.2\n80.7∗∗\n64.9\n80.2\n80.2∗∗\n63.6\n78.7\n80.5∗∗\n65.6∗\n78.9\n80.6∗∗\n64.7\n79.3\n71.4∗∗\n63.3\n78.9\n77.8∗∗\n65.2",
          "87.9\n78.4\n60.5": "80.7\n80.6∗∗\n66.0∗∗\n80.6\n79.7∗∗\n64.2∗\n79.1\n80.4∗∗\n66.2∗∗\n79.4\n80.5∗∗\n64.8∗∗\n79.7\n69.6∗∗\n63.7\n79.5\n76.4∗∗\n65.3∗∗"
        },
        {
          "Base": "CoT-verify\nCoT-DB-verify",
          "97.5\n99.8\n100.0": "92.9∗∗\n99.5\n100.0\n92.7∗∗\n99.6\n100.0",
          "87.9\n77.9\n62.2": "79.8\n81.5∗∗\n61.5\n80.0\n84.2∗∗\n60.0",
          "87.9\n78.4\n60.5": "80.1\n81.0∗∗\n59.5\n80.6\n83.9∗∗\n55.8∗"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child"
      ],
      "venue": "Language Models are Few-Shot Learners"
    },
    {
      "citation_id": "2",
      "title": "GPT-4",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "3",
      "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
      "authors": [
        "A Hendy",
        "M Abdelrehim",
        "A Sharaf",
        "V Raunak",
        "M Gabr",
        "H Matsushita",
        "Y Kim",
        "M Afify",
        "H Awadalla"
      ],
      "year": "2023",
      "venue": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
      "arxiv": "arXiv:2302.09210"
    },
    {
      "citation_id": "4",
      "title": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
      "authors": [
        "J Li",
        "H Li",
        "Z Pan",
        "G Pan"
      ],
      "year": "2023",
      "venue": "Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT",
      "arxiv": "arXiv:2305.12212"
    },
    {
      "citation_id": "5",
      "title": "Can ChatGPT's Responses Boost Traditional Natural Language Processing",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "6",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray",
        "J Schulman",
        "J Hilton",
        "F Kelton",
        "L Miller",
        "M Simens",
        "A Askell",
        "P Welinder",
        "P Christiano",
        "J Leike",
        "R Lowe"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "7",
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": [
        "S Yao",
        "D Yu",
        "J Zhao",
        "I Shafran",
        "T Griffiths",
        "Y Cao",
        "K Narasimhan"
      ],
      "year": "2023",
      "venue": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
    },
    {
      "citation_id": "9",
      "title": "A Wide Evaluation of ChatGPT on Affective Computing Tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "A Wide Evaluation of ChatGPT on Affective Computing Tasks",
      "arxiv": "arXiv:2308.13911"
    },
    {
      "citation_id": "10",
      "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "11",
      "title": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",
      "authors": [
        "K He",
        "R Mao",
        "Q Lin",
        "Y Ruan",
        "X Lan",
        "M Feng",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",
      "arxiv": "arXiv:2310.05694"
    },
    {
      "citation_id": "12",
      "title": "Can Large Language Models Reason about Medical Questions?",
      "authors": [
        "V Liévin",
        "C Hother",
        "O Winther"
      ],
      "year": "2023",
      "venue": "Can Large Language Models Reason about Medical Questions?",
      "arxiv": "arXiv:2207.08143"
    },
    {
      "citation_id": "13",
      "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Large Language Models as Optimizers",
      "authors": [
        "C Yang",
        "X Wang",
        "Y Lu",
        "H Liu",
        "Q Le",
        "D Zhou",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Large Language Models as Optimizers",
      "arxiv": "arXiv:2309.03409"
    },
    {
      "citation_id": "15",
      "title": "Twitter Sentiment Classification using Distant Supervision",
      "authors": [
        "A Go",
        "R Bhayani",
        "L Huang"
      ],
      "year": "2009",
      "venue": "CS224N project report"
    },
    {
      "citation_id": "16",
      "title": "Sarcasm Detection using News Headlines Dataset",
      "authors": [
        "R Misra",
        "P Arora"
      ],
      "year": "2023",
      "venue": "AI Open"
    },
    {
      "citation_id": "17",
      "title": "Generating Text with Recurrent Neural Networks",
      "authors": [
        "I Sutskever",
        "J Martens",
        "G Hinton"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on Machine Learning"
    },
    {
      "citation_id": "18",
      "title": "A learning algorithm for Boltzmann machines",
      "authors": [
        "D Ackley",
        "G Hinton",
        "T Sejnowski"
      ],
      "year": "1985",
      "venue": "Cognitive Science"
    },
    {
      "citation_id": "19",
      "title": "The curious case of neural text degeneration",
      "authors": [
        "A Holtzman",
        "J Buys",
        "L Du",
        "M Forbes",
        "Y Choi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "20",
      "title": "The INTER-SPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "The INTER-SPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism"
    },
    {
      "citation_id": "21",
      "title": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses",
      "authors": [
        "P Good"
      ],
      "year": "1994",
      "venue": "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses"
    }
  ]
}