{
  "paper_id": "2502.13954v1",
  "title": "Latent Distribution Decoupling: A Probabilistic Framework For Uncertainty-Aware Multimodal Emotion Recognition",
  "published": "2025-02-19T18:53:23Z",
  "authors": [
    "Jingwang Huang",
    "Jiang Zhong",
    "Qin Lei",
    "Jinpeng Gao",
    "Yuming Yang",
    "Sirui Wang",
    "Peiguang Li",
    "Kaiwen Wei"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal multi-label emotion recognition (MMER) aims to identify the concurrent presence of multiple emotions in multimodal data. Existing studies primarily focus on improving fusion strategies and modeling modalityto-label dependencies. However, they often overlook the impact of aleatoric uncertainty, which is the inherent noise in the multimodal data and hinders the effectiveness of modality fusion by introducing ambiguity into feature representations. To address this issue and effectively model aleatoric uncertainty, this paper proposes Latent emotional Distribution Decomposition with Uncertainty perception (LDDU) framework from a novel perspective of latent emotional space probabilistic modeling. Specifically, we introduce a contrastive disentangled distribution mechanism within the emotion space to model the multimodal data, allowing for the extraction of semantic features and uncertainty. Furthermore, we design an uncertainty-aware fusion multimodal method that accounts for the dispersed distribution of uncertainty and integrates distribution information. Experimental results show that LDDU achieves state-of-the-art performance on the CMU-MOSEI and M 3 ED datasets, highlighting the importance of uncertainty modeling in MMER. Code is available at https://github. com/201983290498/lddu_mmer.git.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human interactions convey multiple emotions through various channels: micro-expressions, vocal intonations, and text. Multimodal multi-label emotion recognition (MMER) seeks to identify multiple emotions (e.g., happiness, sadness) from multimodal data (e.g., audio, text, and video)  (Zhang et al., 2021) . It could support many downstream applications such as emotion analysis  (Tsai et al., 2019) , human-computer interaction  (Chauhan et al., 2020) , and dialogue systems  (Ghosal et al., 2019) . The main topics of MMER lie in extracting emotion-relevant features by effectively fusing multimodal data and modeling modality-to-label dependencies  (Zhang et al., 2021; Hazarika et al., 2020) . To implement the fusion of multimodal data, some work  (Zhang et al., 2020; Wang et al., 2024b)  employed projection layers to mitigate the modality gap  (Radford et al., 2021) , while other methods  (Zhang et al., 2021; Tsai et al., 2019)  utilized attention mechanisms. Additionally, several studies  (Hazarika et al., 2020; Zhang et al., 2022; Ge et al., 2023; Xu et al., 2024)  decomposed modality features into common and private components. Recently, CARET  (Peng et al., 2024)  introduced emotion space modeling, where emotion-related features were extracted prior to fusion, achieving state-of-the-art performance. Regarding modalityto-label dependencies, many approaches  (Zhang et al., 2021 (Zhang et al., , 2022) )  leveraged Transformer decoders to capture the relationships between label semantic features and fused modality sequence features.\n\nHowever, these approaches primarily focus on semantic features while overlooking aleatoric uncertainty  (Kendall and Gal, 2017) , which repre-sents inherent noise in the data and is commonly modeled using multivariate Gaussian distributions  (Do, 2008)  (for a detailed background, please refer to Appendix A.4). In the context of MMER, such uncertainty primarily arises from factors such as personalized expressions, variations in emotional intensity, and the blending of coexisting emotions  (Zhao et al., 2021) . For instance, as illustrated in Fig.  1 , from a macroscopic perspective, both samples convey happiness, yet case one exhibits more pronounced facial expressions compared to case two. From a distributional perspective, case one demonstrates more concentrated semantic features near the center of the dataset's overall distribution, whereas case two presents features with greater variance, positioned farther from the center. This aleatoric uncertainty introduces ambiguity into semantic feature representations, thereby diminishing the effectiveness of modality fusion in existing MMER approaches  (Gao et al., 2024) .\n\nTo model aleatoric uncertainty in MMER, several challenges need to be addressed: (1) How to represent aleatoric uncertainty: Emotional cues are embedded in multimodal sequences, with each modality contributing differently to emotion expression, making it difficult to extract and disentangle emotional features. When modeled with multivariate Gaussian distributions, samples with the same label often cluster together despite semantic fuzziness. An effective distribution must capture both the central tendency and calibrate variance of emotional features, which is particularly challenging. (2) How to integrate semantic features with aleatoric uncertainty: Higher uncertainty leads to more dispersed distributions, complicating emotion recognition. Without calibrated uncertainty, semantic features can become ambiguous and less informative. Thus, effective strategies for calibrating and integrating uncertainty are crucial to ensure robust and discriminative emotion representations.\n\nTo address these challenges, we propose Latent Distribution Decouple for Uncertainty-Aware MMER (LDDU) from the perspective of latent emotional space probabilistic modeling. For the first challenge, to represent aleatoric uncertainty, LDDU extracts modality-related features using Q-Former-like alignment  (Li et al., 2023) . We then design a distribution decoupling mechanism based on Gaussian distributions to model uncertainty. To further enhance the distinguishability of these distributions, contrastive learning  (Chen et al., 2020)  is employed. For the second challenge, to effectively integrate the distributional information, we draw inspiration from uncertainty learning  (Guo et al., 2017; Moon et al., 2020; Xu et al., 2024)  and develop an uncertainty-aware fusion module, which is accompanied by uncertainty calibration. Experimental results on the CMU-MOSEI and M 3 ED datasets show that LDDU achieves state-of-the-art performance. Specially, it surpasses strong baseline CARAT 4.3% miF1 on CMU-MOSEI under unaligned settings. In summary, the contributions of this work are as follows:\n\n• We introduce latent emotional space probabilistic modeling for MMER. To the best of our knowledge, this is the first work to leverage emotion space distribution for capturing aleatoric uncertainty in MMER.\n\n• We propose LDDU, which models the emotion space to extract emotion features, then uses contrastive disentagled learning to represent latent distributions and recognizes emotions by integrating both semantic features and calibrated uncertainty.\n\n• Experiments on CMU-MOSEI and M 3 ED datasets demonstrate that the proposed LDDU method achieves state-of-the-art performance, with mi-F1 improved 4.3% on the CMU-MOSEI unaligned data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Multi-label Emotion Regression. It aims to infer human emotions from textual, audio, and visual sequences in video clips, often encompassing multiple affective states. The primary challenges in MMER is integrating multimodal data.\n\nEarly studies like MISA  (Hazarika et al., 2020)  address modality heterogeneity by decoupled invariant and modality-specific features for fusion. MMS2S  (Zhang et al., 2020)  and HHMPN  (Zhang et al., 2021)  focused on modeling label-to-label and label-to-modality dependencies using Transformer and GNNs network. Recent approaches  (Peng et al., 2024; Ge et al., 2023; Zhang et al., 2022)  incorporated advanced training techniques; for example, TAILOR  (Zhang et al., 2022)  utilized adversarial learning to differentiate common and private modal features, while AMP  (Wu et al., 2020)  employed masking and parameter perturbation to mitigate modality bias and enhance robustness. However, these works all model from multimodal fusion instead of emotion latent space.\n\nUncertainty-aware Learning and Calibration.\n\nDeep models often overconfidently assign high confidence to incorrect predictions, making uncertainty-aware learning essential to ensure confidence accurately reflects prediction uncertainty  (Guo et al., 2017) . The primary goal is to calibrate model confidence to match the true probability of correctness. There are two main approaches: calibrated uncertainty  (Guo et al., 2017)  and ordinal or ranking-based uncertainty  (Moon et al., 2020) . Calibration methods, such as histogram binning, temperature scaling, and accuracy versus uncertainty calibration  (Zadrozny and Elkan, 2001; Guo et al., 2017; Krishnan and Tickoo, 2020)  3 Methodology",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminary",
      "text": "MMER is typically modeled as a multi-label task. Suppose X v ∈ R sv×dv , X a ∈ R sa×da and X t ∈ R st×dt denote the features of the text, visual, and audio modalities, respectively. In this context, s v , s a , s t denote the length of the feature sequences, while d v , d a ,d t is the dimension of each features sequence. Given a multimodal sequential dataset in joint feature space X v,a,t , denoted as\n\n, the objective of the MMER is to learn the function F: D → Y. Here, N is the size of dataset D and X v i , X a i , X t i represent the visual, audio and textual sequences of the i-th sample. Y ∈ R q represent the emotion space containing q multiple coexisting emotion labels.\n\nIn this section, we describe LDDU framework, which comprises three components (in Figure  2 ).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Uni-Modal Feature Extractor",
      "text": "Follow the work of  Peng et al. (2024) , we conduct experiments on CMU-MOSEI  (Zadeh et al., 2018b)  and M3ED  (Zhao et al., 2022)  datasets. In these two benchmark, facial keypoints X v via the MTCNN algorithm  (Zhang et al., 2016) , acoustic features X a with Covarep  (Degottex et al., 2014)  and text features X t of sample X are extracted using BERT  (Yu et al., 2020) . To capture content sequence dependencies, we employ n v , n a , and n t Transformer layers as unimodal extractors, generating modality visual features",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contrastive Disentangled Representation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Space Modeling",
      "text": "A primary challenge in emotion space modeling is the establishing emotion representations within a unified joint embedding space. Inspired by the Q-Former's structure  (Li et al., 2023) , we introduce trainable emotion embeddings L = [l 1 , l 2 , ..., l q ], where each l i represents an emotion and q is the number of label. Because emotion-related cues may be distributed across different segments of the sequential data, we employ an attention mechanism to automatically extract relevant features for each emotion. Since modality-related features O m and L reside in different feature spaces, we use projection layers to compute the similarity a m ij between frame's feature o m j and the label l i . After obtaining the similarity matrix A m = {a m ij }, Y m is projected to extract modality-specific label-related features Z m ∈ R q×d h , where d h is the dimension of modality-specific label-related features. This process could formalized as follows:\n\nwhere P roj represents the projection layer.\n\nTo facilitate the learning of emotion representations L, we concatenate the multimodal features of i-th sample into\n\nand process them with an MLP-based info classifier employing sigmoid activation functions to generate the final prediction\n\nThe loss function L dir is defined as follows:\n\nwhere BCE(.) is the BCE loss.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contrastive Distentangled Distribution Modeling",
      "text": "This module is composed of distentangled distribution learning (DDL) and contrastive learning (CL). As illustrated in Figure  3 , our architecture incorporates disentangled representation learning (DRL)  (Wang et al., 2024b; Kingma, 2013)  to establish label-related features into latent probabilistic distribution in emotion space. Specifically, we model multimodal emotion representations\n\nwe leverage an encoder (MLP in this paper) and two fully connected layers to obtain the latent distributions\n\n. where µ t i represents the semantic features of the text modality for emotion label i (Tellamekala et al., 2023) and σ t i reflects the distribution region in latent space.\n\nTo ensure that the latent distribution N (µ m i , σ m i ) accurately captures feature differences for each label across modality m, we employ Contrastive Learning (CL). CL could groups similar samples together and enhances the model's ability to distinguish between different classes  (He et al., 2020; Caron et al., 2020) . Formally, given the variations in latent distributions across labels and modalities, we categorize them into 3q potential emotional distributions. For a batch of s B samples B, each sample generates 3q label-related and modality-specific emotion distributions, totaling 3 × q × s B distributions. Each distribution in B + is considered a positive sample if the related sample contains the corresponding emotion. For each positive distribution e ∈ B + , we identify its positive set P e (B) and negative set N e (B) based on labels.\n\nBesides, we promote CL from the following two perspectives. First,  Caron et al. (2020)  ob-serves that a larger batch size can enhance the network abilities by providing more diverse negative samples in CL. We introduce a queue Q of size s q to store the most recent s q emotion distributions. Thus, the final positive and negative sets for each emotion distribution become P e (B ∪ Q) and N e (B ∪ Q), respectively. Besides, similarity calculations between samples must consider both the centers and variances of the decoupled distributions. We represent the distribution e as follows: Finally, we introduce the SupCon loss  (Khosla et al., 2020)  to for each emotion distribution:\n\n∈Pe log e z(e,e + /τ ) e ′ ∈Te e z(e,e ′ )/τ\n\n(5)\n\nwhere T e = P e ∪ N e , and z is the similarity function between emotin distribution. To simplify the process, we calculated cosine similarity on the normalized distribution parameters:\n\nThe final contrastive loss for the entire batch is:\n\n3.4 Uncertainty Aware and Calibration",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Uncertainty-Awared Multimodal Fusion",
      "text": "After modeling the emotional space, it's crucial to integrate latent semantic features with the distribution uncertainty information. We use variance to represent the distribution uncertainty in latent space, as it reflects the degree of dispersion and distribution region. Meanwhile, the center feature represents the semantic features of a sample  (Gao et al., 2024; Tellamekala et al., 2023; Xu et al., 2024) . We hypothesize that when a sample has high aleatoric uncertainty, its semantic features become fuzzier, and the distribution region in latent space becomes more discriminative for emotion recognition. Conversely, when aleatoric uncertainty is low, the semantic features are more discriminative, and the distribution region is narrower. Therefore, the fusion of center feature and variance should depend on the level of aleatoric uncertainty. Firstly, we introduce the i-th sample's prediction ŷdir i of Info Classifier to quantify uncertainty.  Kendall and Gal (2017)  pointed out that aleatoric uncertainty can be measured by the prediction difficulty of the sample. Specifically, if Z i correctly classified by Info Classifier while Z j is misclassified and needs to be decoupled for further classification. We infer that the j-th sample exhibits higher aleatoric uncertainty(i.e., is less informative). Consequently, the uncertainty can be represented as d( ŷi dir , y i ), where ŷi dir is the prediction of Z i .\n\nThen, we integrate the distribution's information by fusing multimodal data. After decoupled, the samples are represented as latent distribu-\n\nfor each modality m. Since E v,a,t and M v,a,t have different semantics, we implement late fusion using gate network. Operationally, (E v ,E a ,E t ) and (M v ,M a ,M t ) are concatenated and passed through final classifier to obtain the predictions ŷi µ and ŷi σ . Semantic mean vector and the variance are dynamically fused according to uncertainty score:\n\nFor a batch of data with size s B , the loss function is as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Uncertainty Calibration",
      "text": "In this section, we impose ordinality constraint  (Moon et al., 2020)  to model the relationship between uncertainty and distribution variance. When well-calibrated, the uncertainty score acts as a proxy for the correctness likelihood of its prediction for the latent distribution. In other words, wellcalibrated uncertainty indicates the expected estimation error, i.e., how far the predicted emotion is expected to lie from its ground truth. It has been confirmed that: frequently forgotten samples are harder to classify, while easier samples are learned earlier in training  (Toneva et al., 2018; Geifman et al., 2018) . As a result, to represent the correctness likelihood values, we use the proportion of samples r i correctly predicted by the Info Classifier during the SGD process  (Shamir and Zhang, 2013; Xu et al., 2024) .\n\nIn our approach, the variance σ i = (σ v i , σ a i , σ t i ) and the prediction error d(ŷ dir i , y i ) from the Info Classifier are strongly correlated with the correctness likelihood values of emotion classification.\n\nThus, the calibration can be formulated as follows:\n\nwhere rk is ranking and Corr is correlation. When the sample contain high uncertainty, the latent distribution variance σ i and the prediction error d i = d(ŷ dir i , y i ) tend to be large, while r i tend to be small. Conversely, when the uncertainty is small, these features are reversed.\n\nFor a batch of size s B , we we compute the variance norm S, distance vector D, and proxy vector R for each sample:\n\nIn order to establish the ranking constraints among S, D and R, we impose ordinality constraints based on soft-ranking  (Tellamekala et al., 2023; Bruch et al., 2019) . Our method employs bidirectional KL divergence to assess mismatching between the softmax distributions of pairs (S, R) and (D, R). Consequently, ordinality calibration loss L ocl can be calculated as follows:\n\nwhere P D , P R , and P S represent the softmax distributions of features S, R, and D, respectively.\n\nOverall, in the whole training process, the training loss of LDDU is as follows:\n\nwhere λ, β, and γ are hyperparameters controlling the weight of each regularization constraint.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets and Evaluation Metrics. We validate the proposed method LDDU on two benchmark: CMU-MOSEI  (Zadeh et al., 2018b)  and M3ED  (Zhao et al., 2022) . CMU-MOSEI consists of 23,453 video segments across 250 topics. Each segment is labeled with multiple emotions, including happiness, sadness, anger, disgust, surprise, and fear. The M 3 ED dataset, designed for dialog emotion recognition, offers greater volume and diversity compared to IEMOCAP  (Busso et al., 2008)  and MELD  (Poria et al., 2018) . It includes 24,449 segments, capturing diverse emotional interactions with seven emotion categories: the above six emotions with neutral. Following previous work  (Zhang et al., 2021 (Zhang et al., , 2022;; Wu et al., 2020; Peng et al., 2024) , in the experiments, we evaluate model performance using accuracy (Acc), precision (P), recall (R), and micro-F1 score (miF1).\n\nBaselines. We compare the LDDU model with two types methods: traditional multimodal methods and multimodal large language model (MLLM) methods. Traditional methods include DFG  (Zadeh et al., 2018a) , RAVEN  (Wang et al., 2019) , MulT  (Tsai et al., 2018) , MISA  (Hazarika et al., 2020) , MMS2S  (Zhang et al., 2020) , HHMPN  (Zhang et al., 2021) , TAILOR  (Zhang et al., 2022) , AMP  (Wu et al., 2020) , and CARAT  (Peng et al., 2024) .\n\nFurthermore, given the significant success of MLLMs in multimodal tasks, we compare LDDU with MLLMs such as  GPT-4o (gpt-4o-2024-11-20 )  (Achiam et al., 2023) , Qwen2-VL-7B  (Wang et al., 2024a) , and AnyGPT  (Zhan et al., 2024) . They respectively correspond to the open-source paradigm, closed-source paradigm, and the omni large language model (LLM). We conduct experiments using raw video clips (treated as unalgined data) from the CMU-MOSEI dataset, maintaining consistent prompts and experimental settings with the framework proposed by  Lian et al. (2024) . Details of the prompts are provided in Appendix A.3.\n\nIn addition, we conducted a comprehensive comparison between the LDDU and existing multi-label classification (MLC) approaches including both classical methods: BR  (Boutell et al., 2004) , LP  (Tsoumakas and Katakis, 2008) , CC  (Read et al., 2011) ; and single-modality methods: SGM  (Yang et al., 2018) , LSAN  (Xiao et al., 2019) , ML-GCN  (Wu et al., 2019) , please see Appendix A.2 and Table  4  for full comparisons. Implementation Details. We set λ = 0.1, β = 0.8, and γ = 0.1, with a batch size of 128. The learning rate is 2e-5 with 30 epochs. More details of all experiences are shown in Appendix A.1.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Results",
      "text": "Main Results. In Table  1  and Table 2 , we compare the performance of our method with various baseline approaches on the CMU-MOSEI and M 3 ED datasets. Different from most baseline methods listed in Table  1 , which use the CTC (Graves Based on Tables  1  and 2 , we can draw the following observations: (1) LDDU outperforms other baseline methods on more crucial metrics such as mi-F1 and accuracy(acc) although recall and precision scores are not the highest on the CMU-MOSEI dataset. Notably, LDDU achieved balanced performance on both aligned and unaligned datasets, with unaligned's accuracy improved by 3% and unaligned's mi-F1 increased by 4.3%. This demonstrates that by modeling the emotional space rather than sequence features, LDDU can better capture emotion-related features. (2) LDDU also achieved significant improvements across all metrics on the M 3 ED dataset, confirming the robustness of our model. (3) TAILOR, CAFET, and the proposed LDDU approach performed better by separating features, emphasizing the importance of considering each modality's unique contributions to emotion recognition in MMER tasks. (4)  2) Effect of contrastive learning. We compared LDDU with the variants without L scl in (3). Performance degradation across metrics confirms the essential role of CL in decoupling. (  4 ) is better than (3), which illustrates a larger batch size can enhance CL. Further, (  5 ) and (  6 ) demonstrates when computing similarity between distributions, both mean value and variance should be considered.\n\n3) Effect of uncertainty calibration. Compared with variants without cilbration, the implementations of constraints  (8, 9, 10, 12)  show enhanced performance. This calibration aligned the variance with uncertainty, generating better predictions.\n\n4) Effect of uncertainty-awared fusion. To modeling aleatoric uncertainty, we integrated the semantic features with the distribution's regional information. (  11 ) and (  12 ) illustrates that both of them contributes to the final classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Further Analysis",
      "text": "Visualization of Emotion Distribution. To evaluate the effectiveness of Contrastive Learning (CL), we used t-SNE (van der  Maaten and Hinton, 2008)  to visualize latent emotion distributions from the CMU-MOSEI test set, excluding samples without specific emotions. As shown in Fig.  4 , panels (a) and (b) display distributions with and without CL, respectively. Without CL, a clear modality gap exists and intra-modality distributions lack distinctiveness. With CL, the 3 × nl emotion distributions across labels and modalities are distinctly separated, enhancing their distinguishability. Consequently, LDDU leveraging CL can more effectively learn emotion distributions across modalities within the joint emotional space, with each cluster representing a specific emotion. Case Study. To validate LDDU's effectiveness, Figure  5  illustrates a representative case where visual/acoustic modalities indicate a transition from sadness to anger, while textual modality explicitly signals anger. While all methods accurately detected sadness and anger, TAILOR and CARET falsely predicted disgust due to its ambiguous emotional cues in overlapping scenarios. In contrast, LDDU effectively modeled emotion-specific uncertainties through latent space Gaussian distributions (distance vector D's value: sad: 0.23, anger: 0.41, disgust: 0.82). We further computed emotion correlation matrices (M 1 -M 3 for methods, M 0 for ground truth) and measure their cosine similarities with M 0 : LDDU achieved 96.7% (vs. 93.3% for TAILOR, 96.1% for CARET), demonstrating superior capability in capturing inter-emotion dependencies. More cases are shown in Appendix A.5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "We propose LDDU, a framework that captures aleatoric uncertainty in MMER through latent emotional space probabilistic modeling. By disentangling semantic features and uncertainty using Gaussian distributions, LDDU mitigates ambiguity arising from variations in emotional intensity and overlapping emotions. Furthermore, an uncertaintyaware fusion module adaptively integrates multimodal features based on their distributional uncertainty. Experimental results on CMU-MOSEI and M 3 ED demonstrate that LDDU achieves state-ofthe-art performance. This work pioneers probabilistic emotion space modeling, providing valuable insights into uncertainty-aware affective computing.",
      "page_start": 1,
      "page_end": 8
    },
    {
      "section_name": "Limitation",
      "text": "While LDDU demonstrates promising performance in MMER, several problems remain to discuss. LDDU models emotion uncertainty using Gaussian distributions in the latent emotion space, effectively capturing inherent ambiguity. However, it does not explicitly utilize emotion intensity labels, as provided in the CMU-MOSEI dataset (quantized into 0, 0.3, 0.6, and 1.0 levels). While this omission ensures fair comparisons with prior work (e.g., TAILOR, CARET), it also limits LDDU's ability to precisely distinguish emotions of varying intensities. As a result, the model may be less effective in disambiguating overlapping emotions, particularly in tasks requiring fine-grained intensity differentiation. Integrating explicit intensity supervision in future iterations could further refine LDDU's predictive",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethical Considerations",
      "text": "Ethical considerations are crucial in multimodal emotion recognition research, particularly with sensitive human data like emotional expressions. In our study, we ensure that all datasets, including CMU-MOSEI and M 3 ED, are publicly available and anonymized to protect individuals' privacy.\n\nWhile our method advances emotion recognition in areas such as human-computer interaction, we acknowledge the potential for misuse, such as manipulation or surveillance. We emphasize the responsible use of these technologies, ensuring they are deployed in contexts that respect privacy.\n\nAdditionally, emotional expressions vary across cultures and individuals, and our model may not fully capture this diversity. We recommend expanding datasets to include a wider range of cultural contexts to avoid biases and misinterpretations.\n\nFinally, we commit to transparency by making our code publicly available for further scrutiny and improvement, ensuring our research aligns with ethical principles and benefits society.  Sicheng Zhao, Guoli Jia, Jufeng Yang, Guiguang Ding, and Kurt Keutzer. 2021 . Emotion recognition from multiple modalities: Fundamentals and methodologies. IEEE Signal Processing Magazine, 38(6):59-73.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Appendix",
      "text": "A.1 Implementation Details\n\nWe set λ = 0.1, β = 0.8, and γ = 0.1, with a batch size of 128. For the uni-modal extraction network, each Transformer consists of 3 layers (l a = l v = l t = 3). The hidden dimensions are 256 for feature Y and 128 for feature Z. The latent emotion distribution has a dimension of 64 for both the distribution centers and variance vectors. The contrastive learning queue Q is sized at 8192. The number of labels (q) is 6 for CMU-MOSEI and 7 for M 3 ED. We optimize all model parameters using the Adam optimizer (Kingma, 2014) with a learning rate of 2 × 10 -5 and a cosine decay schedule with a warm-up rate of 0.1. All experiments are conducted a single GTX A6000 GPU using grid search.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 More Compared Baselines",
      "text": "Despite the advancements in LLM-based and multimodal methods, we conducted a comprehensive and comparative analysis between the LDDU model and existing multi-label classification (MLC) approaches. This comparison includes both classical methods  (BR (Boutell et al., 2004) , LP  (Tsoumakas and Katakis, 2008) , CC  (Read et al., 2011) ) and single-modality methods (SGM  (Yang et al., 2018) , LSAN  (Xiao et al., 2019) , ML-GCN  (Wu et al., 2019) ). The experimental results are presented in Table  4 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.3 Prompts Of Mllm",
      "text": "In this study, we evaluated three multimodal models (GPT-4o, Qwen2-VL-7B, and AnyGPT), using video clips with an average duration of 7-8 seconds.\n\nGPT-4o and Qwen2-VL-7B exhibit strong visual understanding capabilities, representing closedsource and open-source multimodal large language models (MLLMs), respectively. AnyGPT is a versatile any-to-any MLLM capable of processing images, text, and audio. Since all these MLLMs adopt end-to-end architectures, we ensured computational efficiency and consistency by uniformly sampling 8 frames per video clip as input for inference. The specific prompts designed for each model, including task descriptions and format requirements, are detailed in Figure  6 .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.4 Detailed Info Of Uncertainty Caliration",
      "text": "To enhance readers' understanding of aleatoric uncertainty and uncertainty correction, we provide  additional supplementary materials.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.4.1 Aleatoric Uncertainty In Mmer",
      "text": "Aleatoric uncertainty refers to the inherent variability or noise present in the data, arising from factors beyond the model's control. In the context of emotion recognition, it stems from factors such as variations in emotional intensity, individual differences, and the blending of multiple emotions. This form of uncertainty is intrinsic to the data itself.\n\nIn multimodal emotion recognition (MMER), aleatoric uncertainty becomes particularly evident when the same emotion is expressed by different individuals. For example, a person may express happiness through a broad smile (visual modality) but with a neutral tone of voice (audio modality), reflecting differences in emotional intensity and expression. These inconsistencies can introduce conflicting cues that complicate the emotion recognition process. Furthermore, datasets like CMU-MOSEI also contain varying levels of emotion intensity, further contributing to aleatoric uncertainty.\n\nThis type of uncertainty is not confined to emotion recognition alone. In computer vision (CV), it can manifest as blurry faces or imprecise object localization, introducing uncertainty in tasks like object detection. In natural language processing (NLP), aleatoric uncertainty arises from ambiguities in language, where word meanings can shift based on contextual factors. In all these scenarios, probabilistic models are employed to capture and account for such inherent uncertainty, thereby enhancing the robustness of systems in diverse, real-world environments.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.4.2 Uncertainty Calibration",
      "text": "Uncertainty Calibration. Uncertainty Calibration refers to the process of adjusting model predictions to more accurately reflect the true uncertainty associated with them. In machine learning and deep learning, models often provide predictions accompanied by an associated uncertainty; however, these predictions are not always well-calibrated. In other words, the model may exhibit excessive confidence in certain predictions, even when the true uncertainty is high, or it may fail to properly estimate its own uncertainty.\n\nThe primary objective of uncertainty calibration is to align the predicted uncertainty with the actual likelihood of a prediction being correct. In practical terms, this means that if a model is 90% confident in its prediction, it should be correct approximately 90% of the time over a large number of predictions. This calibration process is particularly critical in domains such as emotion recognition, medical diagnosis, and autonomous driving, where accurate uncertainty estimates are essential for reliable decision-making. Several methods can be employed for uncertainty calibration, including temperature scaling, Platt scaling, and Bayesian approaches.\n\nOrdinality Constraint. Ordinality Constraint refers to a form of uncertainty calibration that is based on the ranking of classes. This method assumes that the relationship between classes or labels follows a natural ordinal structure, where labels have an inherent order. For instance, in sentiment analysis, labels such as \"very negative,\" \"negative,\" \"neutral,\" \"positive,\" and \"very positive\" exhibit a natural progression from negative to positive sentiment. Ordinality constraints ensure that the model's predicted probabilities reflect this ranking, adjusting the output so that predictions align with the ordered nature of the classes.\n\nIn our proposed approach, the ordinality constraint is applied to rank the uncertainty of predictions across different labels. By incorporating this constraint, we ensure that the model not only outputs probabilities but also ranks the classes in a manner that respects their inherent order.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.4.3 Uncertainty Caliration In Lddu",
      "text": "Since networks learning variance and mean vectors share similar structures, variance and mean tend to converge and surface feature space collapse without constraints. The key is to ensure that variance vectors accurately reflect uncertainty level. We introduce an ordinality (ranking) constraint  (Moon et al., 2020)  to solve this problem. As shown in Equation 1, ordinality constraint requires predicted confidence κ should correspond to the probability P of correct prediction. In our approach, the variance σ i = (σ v i , σ a i , σ t i ) and the prediction error d(ŷ dir i , y i ) from the Info Classifier jointly represent the sample's confidence. The main challenge is establishing reliable proxy features for P. Inspired by CRL  (Xu et al., 2024) , we use the proportion of samples r i correctly predicted by the Info Classifier during the SGD  (Shamir and Zhang, 2013)  process as a proxy for P. Empirical findings from  Toneva et al. (2018)  and  Geifman et al. (2018)  support our hypothesis: frequently forgotten samples are harder to classify, while easier samples are learned earlier in training.\n\nWhen the sample contain high uncertainty, the latent distribution variance σ i and the prediction error d i = d(ŷ dir i , y i ) tend to be large, while r i tend to be small. Conversely, when the uncertainty is small, these features are reversed. Therefore, the ordinality constraint is: max Corr(rk(\n\n), rk(r i , r j ))\n\n(17\n\nargmax Corr(rk(1 -d i , 1 -d j ), rk(r i , r j ))\n\nwhere Corr represents correlation and rk demotes ranking. In this paper, we impose ordinality constraints based on soft-ranking  (Tellamekala et al., 2023; Bruch et al., 2019) . While  (Tellamekala et al., 2023)  uses KL divergence to measure mismatching of softmax distributions and  (Bruch et al., 2019)  applies softmax cross-entropy for ordinal regression, our method employs bidirectional KL divergence to assess mismatching between the softmax distributions For a batch of size s B , we compute the variance norm S, distance vector D, and proxy vector R for each sample:\n\nInspired by  (Tellamekala et al., 2023; Bruch et al., 2019) , we impose ordinality constraints based on soft-ranking.  While (Tellamekala et al., 2023)  uses KL divergence to measure mismatching of softmax distributions and  (Bruch et al., 2019)  applies softmax cross-entropy for ordinal regression, our method employs bidirectional KL divergence to assess mismatching between the softmax distributions of pairs (S, R) and (D, R). Consequently, ordinality calibration loss L ocl can be calculated as follows: where P D , P R , and P S represent the softmax distributions of features S, R, and D, respectively.\n\nIn summary, the total training loss is as follows:\n\nL total = L cls + λL ocl + βL scl + γL dir (23) where λ, β, and γ are hyperparameters controlling the strength of each regularization constraint.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.5 More Cases For Case Study",
      "text": "Another case is shown in Figure  7 .",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration of aleatoric uncertainty in",
      "page": 1
    },
    {
      "caption": "Figure 1: , from a macroscopic perspective, both sam-",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed LDDU framework consists of three components: (1) the transformer-base unimodal extractor",
      "page": 3
    },
    {
      "caption": "Figure 3: In the latent emotion space, we decouple",
      "page": 4
    },
    {
      "caption": "Figure 3: , our architecture incorpo-",
      "page": 4
    },
    {
      "caption": "Figure 4: The t-SNE visualization of embedding with",
      "page": 8
    },
    {
      "caption": "Figure 4: , panels (a)",
      "page": 8
    },
    {
      "caption": "Figure 5: The case of emotion recognition by multiple",
      "page": 8
    },
    {
      "caption": "Figure 5: illustrates a representative case where vi-",
      "page": 8
    },
    {
      "caption": "Figure 6: Prompts of MLLMs.",
      "page": 13
    },
    {
      "caption": "Figure 7: The case of emotion recognition by multiple",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table 3: , where \"w/o\" means removing,",
      "data": [
        {
          "Approaches\nMethods": "",
          "Aligned": "Acc\nP\nR\nmiF1",
          "Unaligned": "Acc\nP\nR\nmiF1"
        },
        {
          "Approaches\nMethods": "GPT-4o\nLLM-based\nQwen2-VL-7B\nAnyGPT",
          "Aligned": "—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-",
          "Unaligned": "0.352\n0.583\n0.252\n0.196\n0.422\n0.520\n0.355\n0.355\n0.134\n0.251\n0.445\n0.321"
        },
        {
          "Approaches\nMethods": "DFG\nRAVEN\nMulT\nMISA\nMultimodal\nMMS2S\nHHMPN\nTAILOR\nAMP\nCARAT\nLDDU",
          "Aligned": "0.396\n0.595\n0.457\n0.517\n0.416\n0.588\n0.461\n0.517\n0.445\n0.619\n0.465\n0.501\n0.582\n0.430\n0.453\n0.509\n0.475\n0.629\n0.504\n0.516\n0.459\n0.602\n0.496\n0.556\n0.488\n0.641\n0.512\n0.569\n0.484\n0.643\n0.511\n0.569\n0.494\n0.661\n0.581\n0.518\n0.494\n0.647\n0.531\n0.587",
          "Unaligned": "0.386\n0.534\n0.456\n0.494\n0.403\n0.633\n0.429\n0.511\n0.423\n0.636\n0.445\n0.523\n0.571\n0.398\n0.371\n0.450\n0.447\n0.619\n0.462\n0.529\n0.434\n0.591\n0.476\n0.528\n0.460\n0.639\n0.452\n0.529\n0.642\n0.462\n0.459\n0.535\n0.466\n0.652\n0.544\n0.466\n0.496\n0.543\n0.587\n0.638"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: , where \"w/o\" means removing,",
      "data": [
        {
          "Approach": "(1) w/o ESM\n(2) w/o Ldir",
          "Acc\nP\nR\nmiF1": "0.478\n0.663\n0.510\n0.577\n0.491\n0.656\n0.521\n0.580"
        },
        {
          "Approach": "(3) w/o Lscl\n(4) w/o queue Q\n(5) w/o variance µ\n(6) w/o center σ",
          "Acc\nP\nR\nmiF1": "0.679\n0.483\n0.498\n0.575\n0.487\n0.655\n0.487\n0.578\n0.483\n0.628\n0.536\n0.578\n0.492\n0.647\n0.527\n0.581"
        },
        {
          "Approach": "(7) w/o Locl\n(8) ow Corr(S, R)\n(9) ow Corr(D, R)\n(10) ow Corr(D, S)",
          "Acc\nP\nR\nmiF1": "0.483\n0.672\n0.510\n0.580\n0.484\n0.641\n0.532\n0.581\n0.490\n0.647\n0.533\n0.584\n0.492\n0.633\n0.538\n0.582"
        },
        {
          "Approach": "(11) Lcls w/o ˆyµ\n(12) Lcls w/o ˆyσ",
          "Acc\nP\nR\nmiF1": "0.485\n0.666\n0.510\n0.578\n0.543\n0.494\n0.622\n0.580"
        },
        {
          "Approach": "(12) LDDU",
          "Acc\nP\nR\nmiF1": "0.494\n0.587\n0.647\n0.531"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Approaches\nMethods": "",
          "Aligned": "Acc\nP\nR\nmiF1",
          "Unaligned": "Acc\nP\nR\nmiF1"
        },
        {
          "Approaches\nMethods": "GPT-4o\nLLM-based\nQwen2-VL-7B\nAnyGPT",
          "Aligned": "—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-\n—-",
          "Unaligned": "0.352\n0.583\n0.252\n0.196\n0.422\n0.520\n0.355\n0.355\n0.134\n0.251\n0.445\n0.321"
        },
        {
          "Approaches\nMethods": "BR\nClassical\nLP\nCC",
          "Aligned": "0.222\n0.309\n0.515\n0.386\n0.159\n0.231\n0.377\n0.286\n0.225\n0.306\n0.523\n0.386",
          "Unaligned": "0.233\n0.321\n0.545\n0.404\n0.185\n0.252\n0.427\n0.317\n0.235\n0.320\n0.550\n0.404"
        },
        {
          "Approaches\nMethods": "SGM\nDeep-based\nLSAN\nML-GCN",
          "Aligned": "0.455\n0.595\n0.467\n0.523\n0.393\n0.550\n0.459\n0.501\n0.411\n0.546\n0.476\n0.509",
          "Unaligned": "0.449\n0.584\n0.476\n0.524\n0.403\n0.582\n0.460\n0.514\n0.437\n0.573\n0.482\n0.524"
        },
        {
          "Approaches\nMethods": "DFG\nRAVEN\nMulT\nMISA\nMultimodal\nMMS2S\nHHMPN\nTAILOR\nAMP\nCARAT\nLDDU",
          "Aligned": "0.396\n0.595\n0.457\n0.517\n0.416\n0.588\n0.461\n0.517\n0.445\n0.619\n0.465\n0.501\n0.582\n0.430\n0.453\n0.509\n0.475\n0.629\n0.504\n0.516\n0.459\n0.602\n0.496\n0.556\n0.488\n0.641\n0.512\n0.569\n0.484\n0.643\n0.511\n0.569\n0.494\n0.661\n0.581\n0.518\n0.494\n0.647\n0.531\n0.587",
          "Unaligned": "0.386\n0.534\n0.456\n0.494\n0.403\n0.633\n0.429\n0.511\n0.423\n0.636\n0.445\n0.523\n0.571\n0.398\n0.371\n0.450\n0.447\n0.619\n0.462\n0.529\n0.434\n0.591\n0.476\n0.528\n0.460\n0.639\n0.452\n0.529\n0.642\n0.462\n0.459\n0.535\n0.466\n0.652\n0.544\n0.466\n0.496\n0.543\n0.587\n0.638"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Learning multi-label scene classification",
      "authors": [
        "Jiebo Matthew R Boutell",
        "Xipeng Luo",
        "Christopher Shen",
        "Brown"
      ],
      "year": "2004",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "An analysis of the softmax cross entropy loss for learning-to-rank with binary relevance",
      "authors": [
        "Sebastian Bruch",
        "Xuanhui Wang",
        "Michael Bendersky",
        "Marc Najork"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems",
      "authors": [
        "Mathilde Caron",
        "Ishan Misra",
        "Julien Mairal",
        "Priya Goyal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2020",
      "venue": "Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "Dushyant Singh Chauhan",
        "S Dhanush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "7",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "In International conference on machine learning"
    },
    {
      "citation_id": "8",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "9",
      "title": "The multivariate gaussian distribution",
      "authors": [
        "B Chuong",
        "Do"
      ],
      "year": "2008",
      "venue": "Section Notes, Lecture on Machine Learning"
    },
    {
      "citation_id": "10",
      "title": "Embracing unimodal aleatoric uncertainty for robust multimodal fusion",
      "authors": [
        "Zixian Gao",
        "Xun Jiang",
        "Xing Xu",
        "Fumin Shen",
        "Yujie Li",
        "Heng Tao Shen"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Learning robust multi-modal representation for multi-label emotion recognition via adversarial masking and perturbation",
      "authors": [
        "Shiping Ge",
        "Zhiwei Jiang",
        "Zifeng Cheng",
        "Cong Wang",
        "Yafeng Yin",
        "Qing Gu"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "12",
      "title": "Bias-reduced uncertainty estimation for deep neural classifiers",
      "authors": [
        "Yonatan Geifman",
        "Guy Uziel",
        "Ran El-Yaniv"
      ],
      "year": "2018",
      "venue": "Bias-reduced uncertainty estimation for deep neural classifiers",
      "arxiv": "arXiv:1805.08206"
    },
    {
      "citation_id": "13",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "14",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "15",
      "title": "On calibration of modern neural networks",
      "authors": [
        "Chuan Guo",
        "Geoff Pleiss",
        "Yu Sun",
        "Kilian Q Berger"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "16",
      "title": "Misa: Modality-invariant andspecific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "17",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems",
      "authors": [
        "Alex Kendall",
        "Yarin Gal"
      ],
      "year": "2017",
      "venue": "What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Kingma"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "21",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Kingma"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "22",
      "title": "Improving model calibration with accuracy versus uncertainty optimization",
      "authors": [
        "Ranganath Krishnan",
        "Omesh Tickoo"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "24",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition"
    },
    {
      "citation_id": "25",
      "title": "Confidence-aware learning for deep neural networks",
      "authors": [
        "Jooyoung Moon",
        "Jihyo Kim",
        "Younghak Shin",
        "Sangheum Hwang"
      ],
      "year": "2020",
      "venue": "In international conference on machine learning"
    },
    {
      "citation_id": "26",
      "title": "Carat: Contrastive feature reconstruction and aggregation for multi-modal multi-label emotion recognition",
      "authors": [
        "Cheng Peng",
        "Ke Chen",
        "Lidan Shou",
        "Gang Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "28",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "29",
      "title": "Classifier chains for multi-label classification",
      "authors": [
        "Jesse Read",
        "Bernhard Pfahringer",
        "Geoff Holmes",
        "Eibe Frank"
      ],
      "year": "2011",
      "venue": "Machine learning"
    },
    {
      "citation_id": "30",
      "title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
      "authors": [
        "Ohad Shamir",
        "Tong Zhang"
      ],
      "year": "2013",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "31",
      "title": "Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference",
      "authors": [
        "Mahesh Subedar",
        "Ranganath Krishnan",
        "Paulo Meyer",
        "Omesh Tickoo",
        "Jonathan Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "32",
      "title": "Cold fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "W Björn",
        "Elisabeth Schuller",
        "Timo André",
        "Michel Giesbrecht",
        "Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "33",
      "title": "An empirical study of example forgetting during deep neural network learning",
      "authors": [
        "Mariya Toneva",
        "Alessandro Sordoni",
        "Remi Tachet Des Combes",
        "Adam Trischler",
        "Yoshua Bengio",
        "Geoffrey Gordon"
      ],
      "year": "2018",
      "venue": "An empirical study of example forgetting during deep neural network learning",
      "arxiv": "arXiv:1812.05159"
    },
    {
      "citation_id": "34",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "35",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "36",
      "title": "Multilabel classification: An overview",
      "authors": [
        "Grigorios Tsoumakas",
        "Ioannis Katakis"
      ],
      "year": "2008",
      "venue": "Data Warehousing and Mining: Concepts, Methodologies, Tools, and Applications"
    },
    {
      "citation_id": "37",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "38",
      "title": "2024a. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Yang Fan",
        "Kai Dang",
        "Mengfei Du",
        "Xuancheng Ren",
        "Rui Men",
        "Dayiheng Liu",
        "Chang Zhou",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "venue": "2024a. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "39",
      "title": "2024b. Disentangled representation learning",
      "authors": [
        "Xin Wang",
        "Hong Chen",
        "Zihao Wu",
        "Wenwu Zhu"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "41",
      "title": "Adversarial weight perturbation helps robust generalization",
      "authors": [
        "Dongxian Wu",
        "Shu-Tao Xia",
        "Yisen Wang"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "42",
      "title": "Multi-view multi-label learning with viewspecific information extraction",
      "authors": [
        "Xuan Wu",
        "Qing-Guo Chen",
        "Yao Hu",
        "Dengbao Wang",
        "Xiaodong Chang",
        "Xiaobo Wang",
        "Min-Ling Zhang"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "43",
      "title": "Label-specific document representation for multilabel text classification",
      "authors": [
        "Lin Xiao",
        "Xin Huang",
        "Boli Chen",
        "Liping Jing"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "44",
      "title": "Semantic-guided multimodal sentiment decoding with adversarial temporal-invariant learning",
      "authors": [
        "Guoyang Xu",
        "Junqi Xue",
        "Yuxin Liu",
        "Zirui Wang",
        "Min Zhang",
        "Zhenxi Song",
        "Zhiguo Zhang"
      ],
      "year": "2024",
      "venue": "Semantic-guided multimodal sentiment decoding with adversarial temporal-invariant learning",
      "arxiv": "arXiv:2409.00143"
    },
    {
      "citation_id": "45",
      "title": "Sgm: sequence generation model for multi-label classification",
      "authors": [
        "Pengcheng Yang",
        "Xu Sun",
        "Wei Li",
        "Shuming Ma",
        "Wei Wu",
        "Houfeng Wang"
      ],
      "year": "2018",
      "venue": "Sgm: sequence generation model for multi-label classification",
      "arxiv": "arXiv:1806.04822"
    },
    {
      "citation_id": "46",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "47",
      "title": "Prateek Vij, Erik Cambria, and Louis-Philippe Morency. 2018a. Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "49",
      "title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers",
      "authors": [
        "Bianca Zadrozny",
        "Charles Elkan"
      ],
      "year": "2001",
      "venue": "Icml"
    },
    {
      "citation_id": "50",
      "title": "Anygpt: Unified multimodal llm with discrete sequence modeling",
      "authors": [
        "Jun Zhan",
        "Junqi Dai",
        "Jiasheng Ye",
        "Yunhua Zhou",
        "Dong Zhang",
        "Zhigeng Liu",
        "Xin Zhang",
        "Ruibin Yuan",
        "Ge Zhang",
        "Linyang Li",
        "Hang Yan",
        "Jie Fu",
        "Tao Gui",
        "Tianxiang Sun",
        "Yugang Jiang",
        "Xipeng Qiu"
      ],
      "year": "2024",
      "venue": "Anygpt: Unified multimodal llm with discrete sequence modeling",
      "arxiv": "arXiv:2402.12226"
    },
    {
      "citation_id": "51",
      "title": "Multimodal multi-label emotion detection with modality and label dependence",
      "authors": [
        "Dong Zhang",
        "Xincheng Ju",
        "Junhui Li",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "52",
      "title": "Multi-modal multi-label emotion recognition with heterogeneous hierarchical message passing",
      "authors": [
        "Dong Zhang",
        "Xincheng Ju",
        "Wei Zhang",
        "Junhui Li",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "53",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "54",
      "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
      "authors": [
        "Yi Zhang",
        "Mingyuan Chen",
        "Jundong Shen",
        "Chongjun Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "55",
      "title": "M3ed: Multi-modal multi-scene multilabel emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "M3ed: Multi-modal multi-scene multilabel emotional dialogue database",
      "arxiv": "arXiv:2205.10237"
    }
  ]
}