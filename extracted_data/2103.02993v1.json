{
  "paper_id": "2103.02993v1",
  "title": "Speech Emotion Recognition Using Semantic Information",
  "published": "2021-03-04T12:34:25Z",
  "authors": [
    "Panagiotis Tzirakis",
    "Anh Nguyen",
    "Stefanos Zafeiriou",
    "Björn W. Schuller"
  ],
  "keywords": [
    "emotion recognition",
    "deep learning",
    "semantic",
    "paralinguistic",
    "audiotextual information"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a crucial problem manifesting in a multitude of applications such as human computer interaction and education. Although several advancements have been made in the recent years, especially with the advent of Deep Neural Networks (DNN), most of the studies in the literature fail to consider the semantic information in the speech signal. In this paper, we propose a novel framework that can capture both the semantic and the paralinguistic information in the signal. In particular, our framework is comprised of a semantic feature extractor, that captures the semantic information, and a paralinguistic feature extractor, that captures the paralinguistic information. Both semantic and paraliguistic features are then combined to a unified representation using a novel attention mechanism. The unified feature vector is passed through a LSTM to capture the temporal dynamics in the signal, before the final prediction. To validate the effectiveness of our framework, we use the popular SEWA dataset of the AVEC challenge series and compare with the three winning papers. Our model provides state-of-the-art results in the valence and liking dimensions. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic affect recognition is a vital component in human-tohuman communication affecting our social interaction, perception among others  [1] . In order to accomplish a natural interaction between human and machine, intelligent systems need to recognise the emotional state of individuals. However, the task is challenging, as human emotions lack of temporal boundaries and different individuals express emotions in different ways  [2] . In addition, emotions are expressed through multiple modalities. Over the past two decades, a plethora of systems have been proposed that utilise several modalities such as physiological signals, facial expression, speech, and text  [3, 4, 5, 6, 7] . To achieve an accurate emotion recognition system, it is important to consider multiple modalities, as complementary information exists among them  [3] .\n\nCurrent studies exploit Deep Neural Networks (DNNs) to model affect using multiple modalities  [8, 9, 10] . Two modalities that have been extensively used for the emotion recognition task are speech and text  [11, 12] . Whereas the speech signal provides low-level characteristics of the emotions (e. g., prosody), text provides highlevel (semantic) information (e. g., the words \"love\" and \"like\" carry strong emotional content). To this end, several systems have shown that integrating both modalities, strong performance gains can be obtained  [11] .\n\nHowever, one may argue that the textual information is redundant, as it is already included in the speech signal, and as such semantic information can be captured using only the speech modality. To this end, we propose an audiotextual training framework, where the text modality is used during training, but discarded during evaluation. In particular, we train Word2Vec  [13]  and Speech2Vec  [14]  models, and align their two embedding spaces such that Speech2Vec features are as close as possible with the Word2Vec ones  [15] . In addition to the semantic information, we capture low-level characteristics of the speech signal by training a convolution recurrent neural network. The semantic and paralinguistic features are combined to a unified representation and passed through a long short-term memory (LSTM) module that captures the temporal dynamics in the signal, before the final prediction.\n\nTo test the effectiveness of our model, we utilise the Sentiment Analysis in the Wild (SEWA) dataset, which was used in the Audio/Visual Emotion Challenge (AVEC) since 2017  [16] . The dataset provides three continuous affect dimensions: arousal, valence, and likability. Although the arousal and valence dimensions are easily integrated in a single network during the training phase of the models, the likability dimension can cause convergence and generalisation difficulties  [16, 17] . To this end, we propose to use a novel 'disentangled' attention mechanism to fuse the semantic and paralinguistic features such that the information required per affect dimension is disentangled. Our approach provides training stability, and, at the same time, increases the generalisability of the network during evaluation. We compare our framework with the three best performing papers of the competition  [18, 19, 17]  in terms of concordance correlation coefficient (ρc)  [20, 21] , and show that our method provides state-of-the-art results for the valence and likability dimensions.\n\nIn summary, the main contributions of the paper are the following: (a) propose to use the acoustic speech signal to capture semantic information that exists in the text modality, (b) show how to disentangle the information in the network per affect dimensions for stable training and generalisability during the evaluation phase, and (c) produce state-of-the-art results in the valence and likability dimensions using the SEWA dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Several studies have been proposed in the literature for speech emotion recognition  [22, 23, 24] . For example, Trigeorgis et al.  [22]  utilised a convolution neural network to capture the spatial information in the signal, and a recurrent neural network for the temporal ones. In a similar study, Tzirakis et al.  [23]  showed that utilising a deeper architecture with longer input window produces better results. In another study, Neumann et al.  [25]  propose an attentive convolutional neural network (ACNN) that combines CNNs with attention.\n\nIn the past ten years, a plethora of models have been proposed that incorporate more than one modality for the emotion recognition task  [8, 26, 9] . In particular, Tzirakis et al.  [8]  uses both audio and visual information for continuous emotion recognition. Although this study produced good results, it utilises both modalities for the training and evaluation of the model. In a more recent study, Albanie et al.  [9]  transfer the knowledge from the visual information (facial expressions) to the speech model. In another study, Han et al.  [26]  proposed an implicit fusion strategy for audiovisual emotion recognition. In this study, both audio and visual modalities are used for the training of the model and only one for the evaluation of the model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "Our cross-modal framework can leverage the semantic (high-level) information (Sec. 3.1) and the paralinguistic (low-level) dynamics in the speech signal (Sec. 3.2). The low-and high-level feature sets are fused together using a novel attention fusion strategy (Sec. 3.3) before feeding them to a one-layer LSTM module, to captures the temporal dynamics in the signal, for the final frame-level prediction. Fig.  1  depicts the proposed method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Semantic Feature Extractor",
      "text": "To capture the semantic information in the speech signal, we train Word2Vec and Speech2Vec models. The first model uses the text information to extract a semantic vector representation from a given word, whereas the second one the speech. We align their embedding spaces, similar to  [15] , for semantically richer speech representations. Mathematically, we define the speech embedding matrix S = [s1, s2, ..., sm] ∈ R m×ds to be of m vocabulary words with dimension ds, and the text embedding matrix T = [t1, t2, ..., tn] ∈ R n×d t to be of n vocabulary words with dimension dt. Our goal is to learn a linear mapping W ∈ R d t ×ds such that W S is most similar to T .\n\nTo this end, we learn an initial proxy of W via domainadversarial training. The adversarial training is a two-layer game where the generator tries, by computing W , to deceive the discriminator from correctly identifying the embedding space, and making W S and T as similar as possible. Mathematically, the discriminator tries to minimise the following objective:\n\nwhere θD are the parameters of the discriminator, and P θ D (speech = 1|z) is the probability the vector z originates from speech embedding.\n\nOn the other hand, the generator tries to minimise the following objective:\n\nLG\n\nA limitation of the above formulation is that all embedding vectors are treated equally during training. However, words with higher frequency would have better embedding quality in the vector space than less frequent words. To this end, we use the frequent words to create a dictionary that specifies which speech embedding vectors correspond to which text embedding vectors, and refine W :\n\nwhere Sr is a matrix built by k speech vectors from S and Tr is a matrix built by k vectors from T . The solution of Eq. the singular value decomposition of SrT T r , i. .e., SV D(SrT T r ) = U ΣV T .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Paralinguistic Feature Extractor",
      "text": "Our paralinguistic feature extraction network is comprised of three 1-D CNN layers with a rectified linear unit (ReLU) as activation function, and max-pooling operations in-between. Both convolution and pooling operations are performed on the time domain, using the raw waveform as input. Inspired by our previous work  [23] , we perform convolution with small kernel size and stride of one, and a large kernel and stride size for the max-pooling. Table  1  shows the architecture of the network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fusion Strategies",
      "text": "Our last step is to fuse the semantic (xs ∈ R ds ) and paralinguistic (xp ∈ R dp ) speech features, before feeding them to the LSTM. This is performed with two strategies: (i) concatenation, (ii) 'disentangled' attention mechanism.\n\nConcatenation. The first approach is a standard feature-level fusion, i. e., a simple concatenation of the feature vectors. Mathematically,\n\nDisentangled attention mechanism. For our second approach, we propose using attention mechanism to fuse the two modalities. To this end, we perform a linear projection for each of the feature sets such that they are in the same vector space (with dimension du):\n\nwhere Ws ∈ R du×ds , Wp ∈ R du×dp are projection matrices for the semantic and paralinguistic feature sets, respectively. We fuse these features using attention mechanism, i. e.,\n\nwhere q ∈ R du is learnable vector that attends to different features. At this point, we use three fully-connected (FC) layers with linear activation of same dimensionality on top of the output obtained from first attention layer, i. e.,\n\nwhere {Wa, W l , Wv} ∈ R du×du are projection matrices.\n\nWe choose to use three FC layers such that the information flow per emotional dimension (i. e., arousal, valence, and liking) in the network is disentangled. The intuition here is that by adding three additional dense layers, we hope that each of these projections could learn features that suit best for a dimension in our emotion space. In case of a higher number of outputs, more FC layers can be used.\n\nTo fuse the information of the 'disentanngled' vector spaces, we apply an attention layer so that each suited feature set could attend to one another and produce an enriched fusion feature output for final prediction. In particular, we, first, apply attention on a and l; and, finally, on the result with v, i. e., z = Attention(a, l)\n\nx f usion = Attention(z, v).\n\n(7)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "We test the performance of our proposed framework on a timecontinuous emotion recognition dataset for real-world environments. In particular, as outlined, we utilise the Sentiment Analysis in the Wild (SEWA) dataset that was used in the AVEC 2017 challenge  [16] . The dataset consists of 'in-th-wild' audiovisual recordings that were captured from web-cameras and microphones from 32 pairs (i. e., 64 participants) that watched a 90 sec commercial visual and discussed it with their partner for maximum of 3 min. It provides three modalities, namely, audio, visual, and text, for three emotional dimensions: arousal, valence, and liking. The dataset is split into 3 partitions: training (17 pairs), development (7 pairs), and test (8 pairs), and was annotated by 6 German-speaking annotators (3 female, 3 male).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "For training the models, we utilised the Adam optimisation method  [27] , and a fixed learning rate of 10 -4 throughout all experiments. We used a mini-batch of 25 samples with sequence length of 300, and a dropout  [28]  with p = 0.5 for all layers except the recurrent ones to regularise our network. This step is important, as our models have a large amount of parameters and not regularising the network makes it prone on overfitting on the training data. In addition, the LSTM network we use in the training phase is trained with a dropout of 0.5 and a gradient norm clipping of 5.0. Finally, we segment the raw waveform into 10 sec long sequences with sampling rate of 22 050 Hz. Hence, each sequence corresponds to a 22 0500dimension vector.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Objective Function",
      "text": "Our objective function is based on the Concordance Correlation Coefficient (ρc) that was also used in the AVEC 2017 challenge. ρc evaluates the agreement level between the predictions and the gold standard by scaling their correlation coefficient with their mean square difference. Mathematically, the the concordance loss Jc can be defined as follows:\n\nwhere µx = E(x), µy = E(y), σ 2 x = var(x), σ 2 y = var(y), and σ 2 xy = cov(x, y).\n\nOur end-to-end network is trained to predict the arousal, valence, and liking dimensions, and as such, we define the overall loss as follows, L = (L a c + L v c + L l c )/3, where L a c , L v c , and L v c are the concordance loss of the arousal, valence, and liking dimensions, respectively, contributing equally to the loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ablation Study",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparing Vector Spaces",
      "text": "We test the performance of both the semantic and paralinguistic networks, trained independently, and trained jointly, to show the beneficial properties of our proposed framework. Table  2  depicts the results in terms of ρc on the development set of the SEWA dataset. We observe that Word2Vec produces slightly better results than Speech2Vec. However, after aligning their embedding spaces, the aligned Speech2Vec has higher performance than Word2Vec, indicating both that the refinement process makes speech embedding similar to the word ones, and that paralinguistic information exists in the model. Finally, the paralinguistic network, although it produces worse results than the aligned Speech2Vec model, provides the best results for the arousal dimension.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fusion Strategies",
      "text": "We further explore the effectiveness of the attention fusion strategy compared to the simple concatenation. For our experiments we utilised both semantic and paralinguistic deep network models of the proposed method. Table  5 .3.2 depicts the results in terms of ρc on the development set of the SEWA dataset. We observe that the attention method performs superior to the other one on all emotional dimensions, indicating the effectiveness of our approach to model the three emotional dimensions by projecting them to three different spaces before fusing them together with attention. A dash is inserted if the results could not be obtained.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fusion Strategy",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "We compare our proposed framework with the winning papers of the AVEC 2017 challenge. As our model utilises only the audio modality during evaluation, we show, for fairness of comparison, the results of these studies using the audio information. Table  4  depicts the results. First, we observe that our approach provides the best results in the valence dimension with high margin, and the second best in the arousal one. We should note, however, that the network from Huang et al.  [19]  was pretrained on 300 hours of a spontaneous English speech recognition corpus before fine-tuning it to the SEWA dataset. In addition to the features of the network, they also utilise several hand-engineered features. Second, we observe that our approach provides the highest performance in the likability dimension. Our method is able to generalise on this dimension compared to Chen et al  [17]  whose performance drops significantly compared to its performance on the development set. Finally, we should note the high generalisation capability of our approach to model all three emotional dimensions, indicating the effectiveness of the proposed disentangled attention mechanism strategy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a training framework using audio and text information for speech emotion recognition. In particular, we use Word2Vec and Speech2Vec models, and align their embedding spaces for accurate semantic feature extraction using only the speech signal. We combine the semantic and paralinguistic features using a novel attention fusion strategy that first disentangles the information per emotional dimension, and then combines it using attention.\n\nThe proposed model is evaluated on the SEWA dataset and produces state-of-the-art results on the valence and liking dimensions, when compared with the best performing papers submitted to the AVEC 2017 challenge.\n\nIn future work, we intend to use a single network to simultaneously capture the semantic and the paralinguistic information in the speech signal. This will result in simplifying, and at the same time, reducing the number of parameters of the model. Additionally, we intend to investigate the performance of the proposed method on categorical emotion recognition datasets.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed model is comprised of two networks: (a) the semantic feature extractor, that extracts high-level features containing",
      "page": 2
    },
    {
      "caption": "Figure 1: depicts the proposed method.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 2: SEWA dataset results (in terms of ρ ) of the Word2Vec,",
      "data": [
        {
          "Model": "Word2Vec\nSpeech2Vec\nAlign Speech2Vec\nParalinguistic",
          "Arousal\nValence\nLiking": ".434\n.513\n.208\n.433\n.470\n.182\n.453\n.452\n.257\n.508\n.436\n.154",
          "Avg": ".385\n.362\n.387\n.366"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C.-N Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "3",
      "title": "Chapter 18 -Realworld automatic continuous affect recognition from audiovisual signals",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Multimodal Behavior Analysis in the Wild"
    },
    {
      "citation_id": "4",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Mihalis A Nicolaou",
        "G Papaioannou",
        "B Zhao",
        "I Schuller",
        "S Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition: two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "6",
      "title": "Muse 2020 challenge and workshop: Multimodal sentiment analysis, emotion-target engagement and trustworthiness detection in real-life media: Emotional car reviews in-the-wild",
      "authors": [
        "L Stappen",
        "A Baird",
        "G Rizos",
        "P Tzirakis",
        "X Du",
        "F Hafner",
        "L Schumann",
        "A Mallol-Ragolta",
        "B Schuller",
        "I Lefter"
      ],
      "year": "2020",
      "venue": "Proc. ACM International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "10",
      "title": "End2You-The Imperial Toolkit for Multimodal Profiling by End-to-End Learning",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "End2You-The Imperial Toolkit for Multimodal Profiling by End-to-End Learning",
      "arxiv": "arXiv:1802.01115"
    },
    {
      "citation_id": "11",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Proc. IEEE Spoken Language Technology"
    },
    {
      "citation_id": "12",
      "title": "The evolution of sentiment analysis-A review of research topics, venues, and top cited papers",
      "authors": [
        "M Mäntylä",
        "D Graziotin",
        "M Kuutila"
      ],
      "year": "2018",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "13",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Proc. Advances in neural information processing systems (NeurIPS)"
    },
    {
      "citation_id": "14",
      "title": "Speech2vec: A sequenceto-sequence framework for learning word embeddings from speech",
      "authors": [
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised cross-modal alignment of speech and text embedding spaces",
      "authors": [
        "Y.-A Chung",
        "W.-H Weng",
        "S Tong",
        "J Glass"
      ],
      "year": "2018",
      "venue": "Proc. Advances in neural information processing systems (NeurIPS)"
    },
    {
      "citation_id": "16",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt"
      ],
      "year": "2017",
      "venue": "Proc. ACM Multimedia Workshop"
    },
    {
      "citation_id": "17",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "S Chen",
        "Q Jin",
        "J Zhao",
        "S Wang"
      ],
      "year": "2017",
      "venue": "Proc. ACM Multemedia Workshops"
    },
    {
      "citation_id": "18",
      "title": "Investigating word affect features and fusion of probabilistic predictions incorporating uncertainty in avec 2017",
      "authors": [
        "T Dang",
        "B Stasak",
        "Z Huang",
        "S Jayawardena",
        "M Atcheson",
        "M Hayat",
        "P Le",
        "V Sethu",
        "R Goecke",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Proc. ACM Multemedia Workshops"
    },
    {
      "citation_id": "19",
      "title": "Continuous multimodal emotion prediction based on long short term memory recurrent neural network",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "Z Wen",
        "M Yang",
        "J Yi"
      ],
      "year": "2017",
      "venue": "Proc. ACM Multemedia Workshops"
    },
    {
      "citation_id": "20",
      "title": "The interspeech 2018 computational paralinguistics challenge: Atypical & selfassessed affect, crying & heart beats",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "P Marschik",
        "H Baumeister",
        "F Dong",
        "S Hantke",
        "F Pokorny"
      ],
      "year": "2018",
      "venue": "The interspeech 2018 computational paralinguistics challenge: Atypical & selfassessed affect, crying & heart beats"
    },
    {
      "citation_id": "21",
      "title": "End-toend multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "Adieu features? endto-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "Mihalis Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "28",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The Journal of Machine Learning Research"
    }
  ]
}