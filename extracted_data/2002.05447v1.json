{
  "paper_id": "2002.05447v1",
  "title": "Emotion Recognition For In-The-Wild Videos",
  "published": "2020-02-13T11:29:46Z",
  "authors": [
    "Hanyu Liu",
    "Jiabei Zeng",
    "Shiguang Shan",
    "Xilin Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Automated facial expression recognition (FER) in-thewild is a long-standing problem in affective computing and human-computer interaction. To analyze facial expression, psychologists and computer scientists have classified the facial expression into a list of emotion-related categories, such as six basic emotions, i.e., anger, disgust, fear, happiness, sadness, and surprise. Ekman et al.  [1]  have shown that the six basic emotional expressions are universal among human beings. There has been an encouraging progress on facial expression recognition and during the past decades.\n\nIn the the Affective Behavior Analysis in-the-wild (ABAW) 2020 Competition  [2] , the holders provide a large scale in-the-wild database called Aff-Wild2  [3] ,  [4] ,  [5] ,  [6] ,  [7] , including videos annotated with emotion categories, facial action unit  [8] , and valence and arousal  [9]  dimension. In this paper, we present our method used in the expression track in ABAW. In this track, the task is to distinguish seven basic facial expressions (i.e., neutral, anger, disgust, fear, happiness, sadness, surprise) of the person in the given videos. Our method adopts a 101-layer ResNet  [10]  with convolutional block attention module (CBAM)  [11]  to extract frame-by-frame features. Then, the features are fed into a bidirectional recurrent neural network with long short-term memory (BLSTM)  [12]  units.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Deep neural network-based algorithms are widely used in image and video analysis in recent years. Convolutional neural networks (CNN), for example, residual neural network (ResNet), VGGNet  [13] , AlexNet  [14]  are shown effective in image classification and image feature extraction. Long short-term memory network (LSTM)  [12] , a specific improvement of recurrent neural network (RNN), which is capable of capturing serial information, is used in natural language processing as well as video analysis. Meanwhile, the architecture of combination of CNN and RNN is proved to have excellent performance on emotion related tasks. Woo et al. proposed convolutional block attention module (CBAM)  [11] , a lightweight and general attention module boosting the performance of all kinds of CNNs. D. Kollias et al. collected Aff-Wild dataset  [15] , the first dataset with annual annotations for each frame of the videos for facial action unit, facial expression and valence-arousal research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Method",
      "text": "Our method consists of three part: ResNet-101 with CBAM that extracts features for each frame, BLSTM that captures the dynamic features of continuous frames, classification module that makes the decisions. Fig.  1  illustrate the framework of our method. Below, we present the three parts in details. Since ResNet has achieved considerable performance in a lot of computer vision tasks  [10] , we adopt a 101-layer ResNet (ResNet101) to extract visual features from each frame. Considering facial expression appears in particular location in the image, we add a convolutional block attention module (CBAM)  [11]  after each residual block of ResNet101 to introduce channel attention and spatial attention. Fig.  2  illustrates the structure of a residual block with CBAM.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Blstm",
      "text": "Since facial expressions is continuous in the time dimension, we use a Long Short-Term Memory Network (LSTM) to process timing information. Considering that we need to select features for each frame, including the starting frame of the eight frame video clip, we use a bidirectional LSTM here.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Classification Module",
      "text": "Lastly, fully connected layers are applied to classify the features into seven classes based on the features extracted and selected by previous layers.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiment",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "Other than the emotion-annotated part in the provided Aff-Wild2 dataset, we used several internal facial expression datasets (AffectNet  [16] , RAF-DB  [17]   [18] ) and a selfcollected datasets with 300,000 images to pre-train our model.\n\nAff-Wild2: Aff-Wild2 annotated in total 539 videos consisting of 2,595,572 frames with 431 subjects, 265 of which are male and 166 female. The dataset are split into train/validation/test parts in a subject-independent manner, with 253, 71, 233 subjects in each.\n\nAffectNet: AffectNet contains about 440,000 manually annotated facial images collected from Internet search engines. We only used the images with neutral and 6 basic emotions in the training part, including around 280,000 images.\n\nRAF-DB: Real-world Affective Faces Database (RAF-DB) contains around 30,000 facial images annotated with basic or compound expressions. We only used the 12,271 ones in the training part annotated with basic emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Preprocessing",
      "text": "The original videos were first divided into frames. These images were later applied on RetinaFace detector  [19]  to detect all the faced to be analyzed, aligned and cropped into size of 256 × 256. In order to make the external dataset perform better, all the procedures during preprocessing are similar to the official preprocessing except the tools used. For some frames in which human face were not able to be detected by the detector, the corresponding images were removed from the training dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Training",
      "text": "We implemented our model using PyTorch  [20] , on a server with four Nvidia GeForce GTX Titan X GPUs, each with 12GB memory. The model is trained with stochastic gradient descent (SGD) with learning rate 0.0001 and momentum 0.9. Loss function is cross entropy loss. The training batch size is set as 4. At each step during the training, one video from all the videos in the training dataset is selected with equal probability. And then a continuous 8 frame video clip (i.e., without any frame from which faces are unable to be detected) is randomly selected from this video as a batch. The model makes to its best performance usually within 200,000 batches. After every 1000 iteration, we recorded the temporary parameters of the model as a checkpoint.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Evaluation",
      "text": "All the video frames in validation set are arranged into 8 frame clips to be calculated collectively. If the length of a video is not divisible by 8, the last several frames are padding with zeros. The BLSTM part outputs the features for each time step in the clip so that these 8 frames are classified and labeled in a single round. We counted the number of successfully predicted frame as well as the total number of frames processed by our model.\n\nThe final metric S is a combination of accuracy and F 1 formulated as:\n\nwhere Acc is the accuracy which is computed as the ration total number of correctly predicted frames over the total frames. F 1 of computed as unweighted mean of all F 1 of seven categories. The F 1 of a single category is computed as:\n\nWe manually select the parameter with best performance on validation set from all the checkpoints.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Result",
      "text": "We evaluated our method on the validation set of Aff-Wild2 and reported the result of our method in Table  I . The baseline method is MobileNetV2. ResNet+BLSTM is the combination of vanilla ResNet101 and BLSTM.\n\nResNet+CBAM+BLSTM added the CBAM after each layer of ResNet101. As can be seen in Table  I , ResNet+CBAM+BLSTM achieves higher final metric S.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "V. Conclusion",
      "text": "Our proposed method reaches 64.65% accuracy on the validation set, and 43.43% final metric on the validation set, 7.43% higher than the 36% baseline proposed in the competition announcement.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the proposed method.",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrate",
      "page": 1
    },
    {
      "caption": "Figure 2: Structure of the residual block with CBAM",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the structure of a residual block with CBAM.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "liuhanyu@bupt.edu.cn": "introduction to our submis-"
        },
        {
          "liuhanyu@bupt.edu.cn": "expression classiﬁcation track of Af-"
        },
        {
          "liuhanyu@bupt.edu.cn": ""
        },
        {
          "liuhanyu@bupt.edu.cn": ""
        },
        {
          "liuhanyu@bupt.edu.cn": ""
        },
        {
          "liuhanyu@bupt.edu.cn": ""
        },
        {
          "liuhanyu@bupt.edu.cn": "(BLSTM),\nachieving\n64.3%"
        },
        {
          "liuhanyu@bupt.edu.cn": ""
        },
        {
          "liuhanyu@bupt.edu.cn": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "wild is a long-standing problem in affective computing and": "human-computer\ninteraction. To analyze\nfacial\nexpression,"
        },
        {
          "wild is a long-standing problem in affective computing and": "psychologists and computer scientists have classiﬁed the fa-"
        },
        {
          "wild is a long-standing problem in affective computing and": "cial expression into a list of emotion-related categories, such"
        },
        {
          "wild is a long-standing problem in affective computing and": "as\nsix basic emotions,\ni.e., anger, disgust,\nfear, happiness,"
        },
        {
          "wild is a long-standing problem in affective computing and": "sadness, and surprise. Ekman et al.[1] have shown that\nthe"
        },
        {
          "wild is a long-standing problem in affective computing and": "six basic emotional expressions are universal among human"
        },
        {
          "wild is a long-standing problem in affective computing and": "beings. There has been an encouraging progress on facial"
        },
        {
          "wild is a long-standing problem in affective computing and": "expression recognition and during the past decades."
        },
        {
          "wild is a long-standing problem in affective computing and": "In\nthe\nthe\nAffective\nBehavior\nAnalysis\nin-the-wild"
        },
        {
          "wild is a long-standing problem in affective computing and": "(ABAW) 2020 Competition[2],\nthe holders provide a large"
        },
        {
          "wild is a long-standing problem in affective computing and": "scale in-the-wild database called Aff-Wild2[3], [4], [5], [6],"
        },
        {
          "wild is a long-standing problem in affective computing and": "[7],\nincluding\nvideos\nannotated with\nemotion\ncategories,"
        },
        {
          "wild is a long-standing problem in affective computing and": "facial action unit[8], and valence and arousal[9] dimension."
        },
        {
          "wild is a long-standing problem in affective computing and": "In this paper, we present our method used in the expression"
        },
        {
          "wild is a long-standing problem in affective computing and": "track\nin ABAW.\nIn\nthis\ntrack,\nthe\ntask\nis\nto\ndistinguish"
        },
        {
          "wild is a long-standing problem in affective computing and": "seven basic facial expressions\n(i.e., neutral, anger, disgust,"
        },
        {
          "wild is a long-standing problem in affective computing and": "fear, happiness, sadness, surprise) of the person in the given"
        },
        {
          "wild is a long-standing problem in affective computing and": "videos. Our method\nadopts\na\n101-layer ResNet[10] with"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B. Preprocessing": "The original videos were ﬁrst divided into frames. These"
        },
        {
          "B. Preprocessing": "images were\nlater\napplied\non RetinaFace\ndetector[19]\nto"
        },
        {
          "B. Preprocessing": "detect\nall\nthe\nfaced to be\nanalyzed,\naligned and cropped"
        },
        {
          "B. Preprocessing": "into size of 256 × 256. In order to make the external dataset"
        },
        {
          "B. Preprocessing": "perform better, all\nthe procedures during preprocessing are"
        },
        {
          "B. Preprocessing": "similar\nto the ofﬁcial preprocessing except\nthe tools used."
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "For\nsome\nframes\nin which human face were not\nable\nto"
        },
        {
          "B. Preprocessing": "be detected by the detector,\nthe corresponding images were"
        },
        {
          "B. Preprocessing": "removed from the training dataset."
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "C. Training"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "We\nimplemented\nour model\nusing\nPyTorch[20],\non\na"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "server with four Nvidia GeForce GTX Titan X GPUs, each"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "with 12GB memory. The model\nis\ntrained with stochastic"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "gradient descent\n(SGD) with learning rate 0.0001 and mo-"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "mentum 0.9. Loss function is cross entropy loss. The training"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "batch size is set as 4. At each step during the training, one"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "video from all\nthe videos in the training dataset\nis selected"
        },
        {
          "B. Preprocessing": "with equal probability. And then a continuous 8 frame video"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "clip (i.e., without any frame from which faces are unable to"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "be detected) is randomly selected from this video as a batch."
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "The model makes\nto its best performance usually within"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "200,000 batches. After\nevery 1000 iteration, we\nrecorded"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "the temporary parameters of\nthe model as a checkpoint."
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "D. Evaluation"
        },
        {
          "B. Preprocessing": "All\nthe video frames\nin validation set are arranged into"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "8 frame clips to be calculated collectively.\nIf\nthe length of"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "a video is not divisible by 8,\nthe\nlast\nseveral\nframes\nare"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "padding with zeros. The BLSTM part outputs\nthe features"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "for\neach time\nstep in the\nclip so that\nthese 8 frames\nare"
        },
        {
          "B. Preprocessing": "classiﬁed and labeled in a\nsingle\nround. We\ncounted the"
        },
        {
          "B. Preprocessing": "number of successfully predicted frame as well as the total"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "number of\nframes processed by our model."
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "The ﬁnal metric S is a combination of accuracy and F1"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "formulated as:"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "(1)\nS = 0.33Acc + 0.67F1,"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "where Acc is the accuracy which is computed as the ration"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "total number of\ncorrectly predicted frames over\nthe\ntotal"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "frames. F1 of computed as unweighted mean of all F1 of"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "seven categories. The F1 of a single category is computed"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "as:"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "2 · P recision · Recall"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "(2)\nF1 ="
        },
        {
          "B. Preprocessing": "P recision + Recall"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "We manually select\nthe parameter with best performance"
        },
        {
          "B. Preprocessing": "on validation set\nfrom all\nthe checkpoints."
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "E. Result"
        },
        {
          "B. Preprocessing": ""
        },
        {
          "B. Preprocessing": "We\nevaluated our method on the validation set of Aff-"
        },
        {
          "B. Preprocessing": "Wild2 and reported the\nresult of our method in Table\nI."
        },
        {
          "B. Preprocessing": "The\nbaseline method\nis MobileNetV2. ResNet+BLSTM"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table I": ""
        },
        {
          "Table I": "RESULT ON THE VALIDATION SET"
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "S\nMethod\nAcc\nF1"
        },
        {
          "Table I": "baseline[2]\n-\n-\n0.36"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "ResNet+BLSTM\n0.647\n0.281\n0.402"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "ResNet+BLSTM+CBAM\n0.640\n0.333\n0.434"
        },
        {
          "Table I": "S = 0.33Acc + 0.67F1"
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "is\nthe\ncombination\nof\nvanilla ResNet101\nand BLSTM."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "ResNet+CBAM+BLSTM added\nthe\nCBAM after\neach"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "layer\nof\nResNet101.\nAs\ncan\nbe\nseen\nin\nTable\nI,"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "ResNet+CBAM+BLSTM achieves higher ﬁnal metric S."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "V. CONCLUSION"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "Our proposed method reaches 64.65% accuracy on the"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "validation set,\nand 43.43% ﬁnal metric\non the\nvalidation"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "set, 7.43% higher\nthan the 36% baseline proposed in the"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "competition announcement."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "ACKNOWLEDGMENT"
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "The authors would like to thank Xuran Sun for providing"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "us with the pre-trained ResNet FER model and Yuanhang"
        },
        {
          "Table I": "Zhang for assistance."
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "REFERENCES"
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[1]\nP. Ekman, “An argument\nfor basic emotions,” Cognition &"
        },
        {
          "Table I": "emotion, vol. 6, no. 3-4, pp. 169–200, 1992."
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[2] D.\nKollias,\nA.\nSchulc,\nE.\nHajiyev,\nand\nS.\nZafeiriou,"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "“Analysing affective behavior\nin the ﬁrst ABAW 2020 com-"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "petition,” 2020."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[3] D. Kollias\nand\nS.\nZafeiriou,\n“Aff-Wild2:\nExtending\nthe"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "Aff-Wild\ndatabase\nfor\naffect\nrecognition,”\nCoRR,\nvol."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "abs/1811.07770,\n2018.\n[Online]. Available:\nhttp://arxiv.org/"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "abs/1811.07770"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[4] ——,\n“Expression,\naffect,\naction\nunit\nrecognition:\nAff-"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "Wild2,\nmulti-task\nlearning\nand\nArcFace,”\nCoRR,\nvol."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "abs/1910.04855,\n2019.\n[Online]. Available:\nhttp://arxiv.org/"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "abs/1910.04855"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[5] ——,\n“A multi-task\nlearning & generation\nframework:"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "Valence-arousal,\naction\nunits\n&\nprimary\nexpressions,”"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "CoRR,\nvol.\nabs/1811.07771,\n2018.\n[Online].\nAvailable:"
        },
        {
          "Table I": "http://arxiv.org/abs/1811.07771"
        },
        {
          "Table I": ""
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[6] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou,"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "G. Zhao, B. W. Schuller,\nI. Kotsia, and S. Zafeiriou, “Deep"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "affect prediction in-the-wild: Aff-wild database and challenge,"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "International\nJournal\nof\ndeep\narchitectures,\nand\nbeyond,”"
        },
        {
          "Table I": ""
        },
        {
          "Table I": "Computer Vision, vol. 127, no. 6-7, pp. 907–929, 2019."
        },
        {
          "Table I": ""
        },
        {
          "Table I": "[7] D.\nKollias, M.\nA.\nNicolaou,\nI.\nKotsia,\nG.\nZhao,\nand"
        },
        {
          "Table I": "S. Zafeiriou, “Recognition of affect\nin the wild using deep"
        },
        {
          "Table I": "neural networks,” in IEEE Conference on Computer Vision"
        },
        {
          "Table I": "and Pattern Recognition Workshops, 2017, pp. 1972–1979."
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "2",
      "title": "Analysing affective behavior in the first ABAW 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first ABAW 2020 competition"
    },
    {
      "citation_id": "3",
      "title": "Aff-Wild2: Extending the Aff-Wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "4",
      "title": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and ArcFace",
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "5",
      "title": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "6",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "8",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Palo Alto"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "11",
      "title": "Cbam: Convolutional block attention module",
      "authors": [
        "S Woo",
        "J Park",
        "J.-Y Lee",
        "I So Kweon"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "12",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "13",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "14",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Aff-Wild: Valence and arousal 'inthe-wild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "16",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "RetinaFace: Single-stage dense face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "Y Zhou",
        "J Yu",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "20",
      "title": "PyTorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Köpf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Annual Conference on Neural Information Processing Systems"
    }
  ]
}