{
  "paper_id": "2003.02836v2",
  "title": "Guided Generative Adversarial Neural Network For Representation Learning And High Fidelity Audio Generation Using Fewer Labelled Audio Data",
  "published": "2020-03-05T11:01:33Z",
  "authors": [
    "Kazi Nazmul Haque",
    "Rajib Rana",
    "John H. L. Hansen",
    "Björn Schuller"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent improvements in Generative Adversarial Neural Networks (GANs) have shown their ability to generate higher quality samples as well as to learn good representations for transfer learning. Most of the representation learning methods based on GANs learn representations ignoring their post-use scenario, which can lead to increased generalisation ability. However, the model can become redundant if it is intended for a specific task. For example, assume we have a vast unlabelled audio dataset, and we want to learn a representation from this dataset so that it can be used to improve the emotion recognition performance of a small labelled audio dataset. During the representation learning training, if the model does not know the post emotion recognition task, it can completely ignore emotion-related characteristics in the learnt representation. This is a fundamental challenge for any unsupervised representation learning model. In this paper, we aim to address this challenge by proposing a novel GAN framework: Guided Generative Neural Network (GGAN), which guides a GAN to focus on learning desired representations and generating superior quality samples for audio data leveraging fewer labelled samples. Experimental results show that using a very small amount of labelled data as guidance, a GGAN learns significantly better representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Learning a meaningful representation from an unlabelled dataset is a challenging task  [1] . Encouragingly, the advancement of the Generative Adversarial Neural Network (GAN)  [2]  concept has offered great success  [3] ,  [4] ,  [5]  in this field. In particular, the literature shows that the superior generation power of a GAN is useful for learning a good representation  [6] ,  [7] ,  [8] ,  [3] . While the successes of GANs are mainly found in the image generation domain, they do not perform similarly well in the audio domain, because the audio structure is considerably more complicated than the one of an image and successful audio generation depends on generating different temporal scales accurately  [9] , which makes the task harder for a GAN. Recently, some studies  [9] ,  [10] ,  [11]  have shown intriguing results while using a GAN for audio generation. This has encouraged further studies of these methods to learn a representation from unlabelled audio data.\n\nDirect generation of the raw audio waveform is a very complex task. The wavenet generative neural network  [12]  achieved some success in such raw waveform generation; however, the proposed method was computationally expensive and slow. Rather than using GANs to generate a raw waveform, researchers are currently focusing on strategies to generate audio spectrograms and converting these spectrograms to audio  [13] ,  [10] ,  [14] . In this process, if the spectrogram can be generated successfully, the next challenge becomes the conversion of the spectrogram to the audio. The authors introducing TiFGAN  [14]  use a phase-gradient heap integration (PGHI)  [15]  method to reconstruct audio from a spectrogram with a minimal loss.\n\nAmong the GAN architectures, the BigGAN model performs better in terms of image generation, and also in terms of representation learning  [8] . However, to achieve a better result with a BigGAN architecture, we need to provide an enormous amount of data with labels  [16] . Although it is easy to find according enormous open-access datasets, obtaining labels for these is very expensive. The recent work of DeepMind in the context of BigGANs has achieved state of the art (SOTA) generation  [16]  performance using fewer labels; however, these methods are not suitable for representation learning.\n\nTo address the problem of limited availability of labelled data, researchers are focusing on representation learning from unlabelled data. In this paper, we are particularly interested in learning disentangled representations which offers the variations in any dataset to be readily separable in the representation space  [17] ,  [18] ,  [17] ,  [1] ,  [19] . Although some GAN based studies have claimed good results for learning disentangled representation from unlabelled data  [20] ,  [21] ,  [22] ,  [23] ,  [24] ,  [7] ,  [3] , recent work of Google AI shows both theoretically and practically that unsupervised disentangled representation learning is fundamentally impossible and some form of supervision is necessary to achieve this goal  [25] . Therefore, for learning a good representation using GANs, there persists a requirement of good generation as well as of some form of supervision. In this contribution, we address these challenges as follows.\n\n• We propose a new guided unsupervised representation learning model named Guided Generative Adversarial\n\nNeural Network (GGAN), which can guide the GAN model with a small labelled data sample to focus on specific characteristics when learning the representation as well as generating superior quality audio samples. • We evaluate the performance of the newly introduced GGAN applying the widely used Speech Command Dataset (S09) and Librispeech datasets. There, using gender information in Librispeech as a guide, we learn a representation from the S09 set and generate highquality speech commands in male/female voices. A comparison with the existing studies shows that the novel GGAN performs significantly better than the state-of-theart methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background And Related Work",
      "text": "Among the successes in machine learning  [26] ,  [27] ,  [28] ,  [29] ,  [30] ,  [31] ,  [32] ,  [33] ,  [34] , the Generative Adversarial Neural Networks (GANs) have recently brought extraordinary accomplishments in different research domains. Here the GANs are composed of two neural networks, a generator and a discriminator, which are trained based on a minimax game. During training, the discriminator tries to distinguish between real samples from the data distribution and fake samples generated from the generator, while the generator tries to fool the discriminator by producing samples closer to the real sample  [2] . The generator maps any given random continuous sample distribution to the real data distribution. During this mapping, the generator learns significant latent characteristics of the data and tries to disentangle them in the random sample space  [20] .\n\nIn a supervised setting (e. g., a conditional GAN), the generator learns to map the random continuous distribution along with a categorical distribution representing the labels of the dataset, to real samples from the dataset. Accurate labels of the dataset are fed to the discriminator to train these models. Although a GAN performs the best in a fully supervised setting, due to limited availability of labelled data, it is not possible to achieve fully supervised training in most practical scenarios. GANs such as InfoGAN  [22] , which are trained in a completely unsupervised way can avoid the need for labelled data. In the case of the InfoGAN, true labels are not necessary. The generator can learn to map random categorical and continuous samples to real samples, maximising the information between the categorical distribution and generating samples with the help of another small network. However, this method fails to disentangle important features of the data when the complexity of the dataset is very high, such as images with higher resolution.\n\nWhile fully supervised and fully unsupervised training both have their challenges, semi supervision  [35] -a middleground strategy-has received much interest in the past. Semisupervised learning combines a small amount of labelled data with a large amount of unlabelled data during training. Guided unsupervised methods also fall into the category of semisupervised learning. In  [36] , the authors propose a method to guide an InfoGAN. They use a small number of labels to help the InfoGAN to capture a specific representation.\n\nHowever, it fails to perform superior in cases like in complex datasets such as SVHN  [37] , CelebA  [38] , and CIFAR-10  [39] . In another work  [40] , the authors suggest learning a classifier from a partially labelled dataset and then use that classifier for semi-supervision in any GAN architecture. Kumar  [41]  proposed two discriminators in a GAN architecture where one discriminator learns to identify real or fake samples from the unlabelled dataset, and another discriminator learns to identify real or fake samples with their labels from some labelled dataset. In a recent contribution of Google research, the authors explore different semi-supervised methods  [16] . They predict the missing labels of the dataset with the help of a small labelled dataset. First, they train with self supervision, and then fine-tune the classifier with a small labelled dataset. Subsequently, they predict the labels for other missing labelled datasets. They also propose a co-training method for this task, where they train this classifier on top of the discriminator during the training.\n\nFrom the above, we identify two major gaps in the literature of semi-supervision using GANs. First, most of the proposed methods have shown promising results in term of generating higher quality samples from fewer labelled datasets; however, they do not offer any representation learning strategy. Second, most of these latest studies are based on image datasets, and very few researchers have investigated the compatibility of these models in other domains such as audio. To contribute to this field, we propose the GGAN model which is capable of learning powerful representation from an unlabelled audio dataset with some guidance from a minimal amount of labelled samples. The newly introduced GGAN can as well generate higher quality audio samples at the same time.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Proposed Research Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Model Intuition",
      "text": "A generator usually takes random latent vectors as input and generates samples from the real data distribution. In our proposed model, rather than feeding a random latent space to the generator, we feed it with a generated latent space, which is easily classifiable into n categories. In the data distribution, the information of different categories are entangled and not easily separable, but in the latent space, these categories are disentangled, and we can separate them easily. Our aim is that the generator learns to generate different data categories for different latent space categories. However, the challenge is how to force the generator to create such different classes of samples for different categories of latent space. To guide the generator, we hence use some labelled datasets and provide guidance from a classifier network.\n\nIn Fig.  1 , we show the essential connections between the parts of the model. In our model, we have an encoder that takes any random latent vector and random conditional vector to generate a new latent vector, which can be classified into n categories through 'classifier A'. The classifier network tries to predict the random conditional vector given the latent vector. This is achieved as the encoder, and the classifier network work together to maximise the information between the random categorical distribution (random conditional vector) and the generated latent distribution. The generator takes this new latent cector and generates samples from a real data distribution, and 'classifier B' tries to classify this generated sample into n categories. Through classifier B, it is ensured that for different given conditions in the encoder, the generator generates different categories of the sample. Now, the problem for the generator is to generate only the categories that are of our interest. In order that the generator can generate correct categories, we jointly train it with classifier B to classify n different categories of the dataset with the help of a few labelled data samples. Using the labelled data samples, classifier B gets trained to classify our concerned categories; it, therefore, classifies the generated samples into those target categories. As the encoder, the generator, and the classifier B are trained together, the generator tries to minimise the loss of classifier B by matching the classification label of classifier B given a latent vector.\n\nWith the help of classifier B, the generator thus learns to create n numbers of categories of the sample from n different categories of latent space generated by the encoder. As classifier B is trained with the small labelled dataset, it will be able to classify some categories correctly from the dataset at the start of the training. As the training goes on, the generator will generate other similar samples for each of the categories, which will improve the classification accuracy of classifier B.\n\nIn the end, the generator is trained to generate n categories samples from n categories in the latent space generated by the encoder. In the latent space, the categories of the dataset are disentangled and easily classifiable by the classifier network, which makes latent space a powerful representation of the data distribution. We, therefore, connect a feature extractor network into this framework which learns to map real samples to the latent space. The output of the feature extractor then essentially becomes our \"learned representation\". When E learns to map z and c to z e , it can easily ignore the categorical distribution. To address this problem, we introduce a classifier network C e , which takes z e as input and outputs the predicted class ĉe , where the true label is the given categorical sample c. By using the network C e , we force E to maximise the mutual information between c and z e . Suppose we have n categories in the Cat(c), then E learns to create a sample space u(z), which can be classified into n categories by the network C e .\n\n2) Generator: Like any other GAN framework, we have a Generator G, which learns to map z e into sample x ∈ P G , where P G is the generated sample distribution. One of the major goals of G is to generate P G so that it matches the true data distribution P data . Another goal of G is to maximise the mutual information between x and the given random condition c (random categorical sample) ensuring there are categorical variations in the generated samples.\n\n3) First (D 1 ) and Second (D 2 ) Discriminators : The Discriminator D 1 has two parts: a feature extraction part, D, and the real/fake sample identification part D . From D we obtain the features d x, d x , and d x l for x, x, x l , respectively, where a real data sample is x ∈ P data and a labelled data sample resembles x l ∈ P ldata ; P ldata is the labelled data distribution. Here, P ldata can be the subset of the P data or any other data distribution. For a real sample x and a fake sample x, we obtain the output respectively as d x and d x, from D . In our D 1 Network, D optimises the feature learning for both the classification and the discrimination tasks, which renders the optimisation task considerably complex. So, to avoid further complicating of the optimisation task, we use another Discriminator D 2 to merely perform a real and fake sample discrimination.\n\n4) Feature Extractor, Classifier (C x ), and Third Discriminator (D f ): The feature extractor network F learns the representation f x , f x , and f xl for x, x and xl, respectively. Rather than feeding directly samples to F , we feed the features (d x, d x and to d x l ) of these samples from D. This is to make it computationally inexpensive, also to reduce the chance of overfitting as the features from D are constantly changing.\n\nF is trained to map x, x and xl to the latent space u(z). To ensure this, we have another discriminator D f which learns to identify an (f x , x) pair as a fake, and a (z e , x) pair as a real pair. Similarly, rather than feeding directly x and x, we feed their features from D.\n\nThe second Classifier C x learns to classify labelled data as well as it learns to classify the generated image x according to a given categorical, conditional sample c. For the classification of x and xl, we feed the features generated from F to C x .\n\nC. Losses 1) Encoder, Classifier (C e ), and Generator loss: For the Encoder E and the Classifier C e , let the classification loss be EC loss . We have z e = E(z, c), therefore,\n\nFor the Generator G, we have two generation losses coming from the discriminator D 1 and D 2 . For the generator and discriminator, we use the hinge loss. We have x = G(z e ) and d x = D(x), therefore,\n\nThe Generator G has another loss for the classification of the sample. We have f x = F (d x), so the Classification Loss, GC loss for G is,\n\nFor mode collapse, we define a loss named \"Mode Divergence Loss\", M G loss . For calculating this loss, we take two random inputs z 1 and z 2 , and the same conditional code c. For z 1 , z 2 , we get x1 = G(E(z1, c)) and x2 = G(E(z2, c)), respectively.\n\nWe also take two random samples x 1 and x 2 from the real data distribution p data . We calculate the loss based on the feature extracted from D. Let\n\n, and α is a small parameter like .0001, so we get,\n\n(5) Hence, we have a combined loss, ECG loss for E, C, and G. We averaged the G loss1 , G loss2 , and M G loss as all of these are losses for the generation of the sample. The E, C, and G networks are updated to minimise the loss ECG loss .\n\n2) Feature Extractor and Classifier (C x ) loss: We have the feature generation loss, F G loss , coming from the third Discriminator D f , so that F creates features like z e from real data.\n\nWe have two classification losses for C x , one for the labelled sample, Cl loss , and another one for the generated sample, Cg loss . If the label of the real sample is y, and we have\n\nLikewise, the total loss for F and C x is F C loss , which is the sum of the above losses:\n\nWe update the F and C x network to minimise the F C loss .\n\n3) Discriminators loss: The D part of D 1 , and D 2 are two discriminators for identifying the real/fake samples generated from the generator. D 1 also has a part D, which is responsible for generating features for the F and C x networks. It is also optimised to reduce the classification loss, Cl loss , of the real labelled samples. Sample minibatch of m, noise samples {z (1) , . . . , z (2m) } from p z , conditions {c (1) , . . . , c (m) } from Cat(c), data points {x (1) , . . . , x (2m) } from p data and labelled data points {xl (1) , . . . , xl (m) } from P ldata .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "4:",
      "text": "Update the parts D, D of discriminator D 1 by ascending its stochastic gradient:\n\nUpdate the discriminator D 2 by ascending its stochastic gradient: i) .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "6:",
      "text": "Update the discriminator D f by ascending its stochastic gradient:\n\nend for 8:\n\nRepeat step  [3] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "9:",
      "text": "Update the Generator G, Encoder E and Classifier C e by descending its stochastic gradient:\n\nECG loss (i) .\n\n10:\n\nRepeat step  [3] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "11:",
      "text": "Update the Feature Extractor F and Classifier C x by descending its stochastic gradient:\n\n12: end for respectively, are given by,\n\nThe discriminators' weights are updated to maximise these losses. The algorithm to train the whole model is given in Algorithm 1.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Data And Implementation Detail",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "For the validation of the GGAN, we use S09  [42]  and the Librispeech dataset  [43] . In the S09 dataset digits from zero to nine are uttered by 2618 speakers  [42] . Most of the recent studies use the S09 dataset for evaluating their model for audio generation. This dataset is noisy and includes 23,000 samples where many of these samples are poorly labelled. Labels for the speakers and gender are not available in S09.\n\nLibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech. In the dataset, there are 1166 speakers where 564 are female, and 602 are male.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Measurement Metrics",
      "text": "For evaluating generation we use Inception Score (IS)  [44]  and Frchet Inception Distance (FID)  [45] ,  [46] .\n\n1) Inception Score (IS): IS score measures the quality of the generated samples as well as the diversity in the generated sample distribution. Pretrained Inception Network V3  [47]  is used to get the labels for the generated samples. The conditional label distribution p(y|x) is derived from inception network, where x is the generated sample. We want the entropy to be low, which indicates the good quality of the image. It is also expected that the images are diverse, so the marginal label distribution p(y|x)dz should have high entropy. Combining these two requirements, the KL-divergence between the conditional label distribution and the marginal label distribution is computed from metric exp(E x KL(p(y|x)||p(y))). As an inception network is used, it is called Inception Score (IS score). A higher IS score indicates the good quality of the generated samples.\n\n2) Frchet Inception Distance (FID): IS score is computed solely on the generated samples. Frchet Inception Distance (FID) improves the IS score by comparing the statistics of generated samples to real data samples. First the features are extracted for both real and generated samples from the intermediate layer of the inception network. Then the mean µ r , µ g and covariance Σ r , Σ g for real and generated samples are calculated respectively from those features. Finally, the Frchet Distance  [48]  between two multivariate Gaussian distributions (given by the µ r , µ g and Σ r , Σ g ) is calculated using: ||µ r -µ g || 2 + Tr(Σ r + Σ g -2(Σ r Σ g ) 1/2 ). A lower FID score indicates the good quality of the generated samples.\n\nNote that the inception v3 model is trained on imagenet dataset  [49] , which is completely different from audio spectrograms. Therefore, it will not be able to classify spectrograms into any meaningful categories, resulting in poor performance on the calculation of IS and FID score for our datasets. In the \"Adversarial Audio Synthesis\"  [10]  paper, instead of using the pretrained inception v3 network to calculate the IS and FID scores, the authors train a classifier network on S09 spoken digit dataset and obtain good performance. We, therefore, use their pretrained model to calculate both IS and FID scores for our generated samples.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Experimental Setup",
      "text": "Performance of GGAN model is evaluated based on two tasks, Representation Learning and Audio Generation. To evaluate the performance of audio generation, we compare IS and FID score of GGAN with that of supervised and unsupervised BigGAN as well as related research works on S09 dataset. First, the generated log-magnitude spectrograms of the models are converted to audio with the PGHI algorithm and then we convert the audio back to spectrogram to pass through the pretrained classifier to calculate the IS and FID score as the input spectrogram format for the pretrained model is different. Therefore, essentially the evaluation is done based on the generated audio rather than evaluating on the spectrogram directly.\n\nFor representation learning, we compare the GGAN model with unsupervised BigGAN and supervised Convolutional Neural Network (CNN). Our primary goal is to learn representation from unlabelled S09 training dataset so that we can get better classification result on the S09 test dataset. For any GAN model, the latent space captures the representation of the training dataset so to map the real data samples to the latent space we follow the strategy from the 'Adversarially learned inference' paper  [4] .\n\nAfter training the unsupervised BigGAN, we train a feature extraction network to reverse map the sample to latent distribution to get the representation for real samples. Then we train a classifier at the top of the feature extraction network with 1 to 5% randomly sampled labelled dataset from the training dataset and evaluate on the test dataset. A sampling of the training dataset was repeated five times, and the results were averaged. We use the same 1 to 5% labelled data for the training of a CNN network. For this setting, we use the same CNN architecture as feature extraction network. Also, these 1 to 5% labelled data was used during the training of the GGAN model as guidance.\n\nWe take the pretrained D, F and C x networks from GGAN models, pass the test dataset through those pretrained networks to get the prediction for classes C x (F (D(x test ))). Then we compute the classification accuracy on the test dataset. With these experiments, we can evaluate the effectiveness of using a small labelled training data during or after the training of the model. So these experiments will demonstrate if the guidance in GGAN, can improve the performance.\n\nTo evaluate the quality of the representation learnt we visualise the representation space. We then evaluate the disentanglement by observing the linear interpolation in the representation space.\n\nWe also test the possibility of guiding an unsupervised model to learn an attribute of data in a dataset, where the guidance comes from a non-related dataset. Here we have used the whole S09 training data as the unlabelled dataset and Librispeech as the labelled dataset for guidance on the gender. Note that Gender information is not available for the S09 dataset. We have taken 50 male and 50 female speakers from the Librispeech dataset with 5 minutes of audio sample for each speaker. Our expected output from the GGAN is to produce the male and female spoken digits based on the Model Name IS Score FID Score Real (Train Data)  [10]  9.18 ± 0.04 -Real (Test Data)  [10]  8.01 ± 0.24 -TiFGAN  [11]  5.97 26.7 WaveGAN  [10]  4.67 ± 0.01 -SpecGAN  [10]  6.03 ± 0.04 -Supervised BigGAN 7.33 ± .01 24.40 ± 0.5 Unsupervised BigGAN 6.17 ± 0.2 24.72 ± 0.05 GGAN 7.24 ± 0.05 25.75 ± 0.1 guidance from the Librispeech dataset.\n\nFor our generator and discriminator network in the GGAN model, we use the BigGAN  [8]  architecture. We maintain the parameters the same as BigGAN, except we only change the input of the Discriminator and output of the Generator to accommodate the 128 by 256 size log-magnitude spectrogram. During the training of GGAN, we keep the learning rate of the generator and Discriminator equal, which boosts the IS score by 1. The architecture details are given in the supplementary document.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Results",
      "text": "For the unsupervised BigGAN model, we achieve an IS score of 6.17 ± 0.2, and an FID score of 24.72, whereas we receive an IS score of 7.3±0.01, and FID score of 24.40±0.5 for the supervised BigGAN. For our GGAN model, with 5 per cent labelled data as guidance, we achieve a maximum IS score of 7.24 ± 0.05, and an FID score of 25.75 ± 0.1 for the generated samples, which is very close to the performance of the fully supervised BigGAN model and considerably better than unsupervised BigGAN model. Comparison of the IS score and FID score between different models are shown in the table I. We observe that the performance of the proposed GGAN is better than that of other studies reported in the table I. Generated samples for different digit categories are shown in Figure  3 . Notice the diversity in the sample generation for any particular digit category.\n\nA comparison on the test accuracy between the unsupervised BigGAN and GGAN is shown in table II. With a 5 % labelled dataset, we have achieved an accuracy of 92±0.87 %, which is close to the accuracy of the fully supervised CNN model (96.2 %). Therefore, guidance during training helps to learn better representation of the dataset.\n\nAs we have guided the proposed GGAN model with the digit categories, it is expected that in the representation space, the digit categories are disentangled. To investigate this scenario, we visualise the learnt representation in the 2D plain. We take the representation/feature F (D(x test )) of the test dataset passing through the trained D and F networks. Then, the higher dimensional features are visualised with the t-SNE (t-distributed stochastic neighbour embedding)  [50]  visualisation technique and shown in figure  5 . In the figure, we can observe that the features of the similar categories are clustered together and they are easily separable.\n\nTo investigate the quality of the learnt latent space by the generator, we conduct a linear interpolation between two random points in the latent space as in the DCGAN work  [20]  and observe a smooth transition in the generated spectrogram space as well as that after converting them to audio, the transition is smooth. This indicates good learning of latent space by the generator. But for the unsupervised BigGAN, during the linear interpolation, we notice a smooth transition in the generated spectrogram space but after converting them to the audio, we find that the transaction becomes nonsmooth. This indicates that the generator is able to capture the semantics of the spectrogram in the latent space but is unsuccessful to capture the semantics in the audio space. As the unsupervised GAN is trained with spectrograms, it has no idea about the characteristics of the audio samples. But with some guidance, we have shown that the generator of the GGAN can learn important semantics of audio though it is trained with spectrograms.\n\nIn figure  4 , we show the linear interpolation for both the unsupervised BigGAN and the proposed GGAN. We avoid the latent space interpolation for the supervised BigGAN, as the generator of the supervised BigGAN uses both conditions, c and latent space, z, during the sample generation, G(z, c), which discourages the generator to learn any conditional characteristics in the latent space. It instead learns the common attributes in the latent space. In our S09 dataset experiment, the supervised BigGAN does not disentangle the digit cate- We also investigate the effectiveness of the guidance from training data of a completely separate dataset. As unlabelled dataset, we use the S09 dataset and for the labelled dataset part, we use the librispeech dataset with the labels for speaker gender (male and female). We successfully generate samples of 0-9 digits for males and female speakers. We achieve an IS score of 6.52 ± 0.5, and an FID score of 26.21 ± 0.1. Here, although our GGAN model is not guided for digit categories, it shows state of the art performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion And Lesson Learnt",
      "text": "In this contribution, we proposed the novel Guided Generative Adversarial Neural network (GGAN), where we guide an unsupervised GAN network with some labelled data. This allows the network to learn specific attributes while learning the representation of the dataset, which is useful for transfer learning or any other machine learning tasks. As we guide the model during training according to a post-task, the proposed GGAN can be used as a task-aware network that learns taskspecific representation. We showed that the GGAN can learn powerful representations as well as can generate high-quality samples given some labelled data as guidance.\n\nOur model was successfully evaluated based on one-second audio data; it remains a challenge to make it useful for large size audio with higher complexity. However, any long audio can be divided into one-second chunks, and this model can be beneficial for generating high-quality chunks and learn useful representation which can be used for challenging tasks. While more research is necessary to understand this model, our research is likely to motivate others to work on the guided representation learning model where, with some guidance, one model can learn powerful representations from the enormous freely available unlabelled data.\n\nAmong many minor challenges, the main challenge we face is related to the sample generation of the model. There was a severe issue of mode collapse, which we could not satisfyingly fix implementing different techniques in the literature  [44] . However, we received inspiration from the Mode Seeking GAN  [51] , modified the loss function, and created a unique feature loss, which immediately fixed the mode collapse problem. The feature loss calculates the ratio of the difference between the two real samples and two generated samples. If the generator creates very similar or the same samples, it gets penalised and tries to find more modes. Audio samples discussed in this paper, can be found in the GitHub link https://github.com/KnHuq/GGAN.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we show the essential connections between the",
      "page": 2
    },
    {
      "caption": "Figure 1: The connection between different networks named Encoder, Classiﬁer A, Generator, Classiﬁer B, Feature Extractor, and Discriminator is shown in",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of the proposed GGAN model. The model consists",
      "page": 3
    },
    {
      "caption": "Figure 3: Notice the diversity in the sample generation for any",
      "page": 6
    },
    {
      "caption": "Figure 3: The generated log-magnitude spectrograms using the proposed GGAN model for different digit categories is shown in this ﬁgure. Each row shows",
      "page": 7
    },
    {
      "caption": "Figure 4: Generated spectrogram from the interpolation of the latent space from the male sound of digit 2 to the female sound of digit 2 (left to right). The",
      "page": 8
    },
    {
      "caption": "Figure 5: t-SNE visualisation of the learnt representation of the test data of",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model Name": "Real\n(Train Data)\n[10]",
          "IS Score": "9.18 ± 0.04",
          "FID Score": "-"
        },
        {
          "Model Name": "Real\n(Test Data)\n[10]",
          "IS Score": "8.01 ± 0.24",
          "FID Score": "-"
        },
        {
          "Model Name": "TiFGAN [11]",
          "IS Score": "5.97",
          "FID Score": "26.7"
        },
        {
          "Model Name": "WaveGAN [10]",
          "IS Score": "4.67 ± 0.01",
          "FID Score": "-"
        },
        {
          "Model Name": "SpecGAN [10]",
          "IS Score": "6.03 ± 0.04",
          "FID Score": "-"
        },
        {
          "Model Name": "Supervised BigGAN",
          "IS Score": "7.33 ± .01",
          "FID Score": "24.40 ± 0.5"
        },
        {
          "Model Name": "Unsupervised BigGAN",
          "IS Score": "6.17 ± 0.2",
          "FID Score": "24.72 ± 0.05"
        },
        {
          "Model Name": "GGAN",
          "IS Score": "7.24 ± 0.05",
          "FID Score": "25.75 ± 0.1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training\nData Size": "1%",
          "CNN\nNetwork": "82 ± 1.2",
          "Unsupervised\nGAN": "73 ± 1.02",
          "GGAN": "84 ± 2.24"
        },
        {
          "Training\nData Size": "2%",
          "CNN\nNetwork": "83 ± 0.34",
          "Unsupervised\nGAN": "75 ± 0.41",
          "GGAN": "85 ± 1.24"
        },
        {
          "Training\nData Size": "3%",
          "CNN\nNetwork": "83 ± 0.23",
          "Unsupervised\nGAN": "78 ± 0.07",
          "GGAN": "88 ± 0.1"
        },
        {
          "Training\nData Size": "4%",
          "CNN\nNetwork": "84 ± 0.34",
          "Unsupervised\nGAN": "80 ± 0.01",
          "GGAN": "91 ± 0.5"
        },
        {
          "Training\nData Size": "5%",
          "CNN\nNetwork": "84 ± 1.02",
          "Unsupervised\nGAN": "80 ± 1.72",
          "GGAN": "92 ± 0.87"
        },
        {
          "Training\nData Size": "100%",
          "CNN\nNetwork": "96.2",
          "Unsupervised\nGAN": "-",
          "GGAN": "-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Full Name": "Resample",
          "Abbreviation": "RS"
        },
        {
          "Full Name": "Batch normalisation",
          "Abbreviation": "BN"
        },
        {
          "Full Name": "Conditional batch normalisation",
          "Abbreviation": "cBN"
        },
        {
          "Full Name": "Downscale",
          "Abbreviation": "D"
        },
        {
          "Full Name": "Upscale",
          "Abbreviation": "U"
        },
        {
          "Full Name": "Spectral normalisation",
          "Abbreviation": "SN"
        },
        {
          "Full Name": "Input height",
          "Abbreviation": "h"
        },
        {
          "Full Name": "Input width",
          "Abbreviation": "w"
        },
        {
          "Full Name": "True label",
          "Abbreviation": "y"
        },
        {
          "Full Name": "Input channels",
          "Abbreviation": "ci"
        },
        {
          "Full Name": "Output channels",
          "Abbreviation": "co"
        },
        {
          "Full Name": "Number of channels",
          "Abbreviation": "ch"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Shortcut",
          "Kernal\nSize": "[1,1,1]",
          "RS": "U",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "cBN, ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h × w × c {i}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "U",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "cBN, ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "U",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "Addition",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "2h × 2w × c\n{o}"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Shortcut",
          "Kernal\nSize": "[1,1,1]",
          "RS": "U",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "BN, ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h × w × c {i}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "U",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "BN, ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "U",
          "Output\nSize": "2h × 2w × c\n{o}"
        },
        {
          "Layer\nName": "Addition",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "2h × 2w × c\n{o}"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Shortcut",
          "Kernal\nSize": "[1,1,1]",
          "RS": "D",
          "Output\nSize": "h/2 × w/2 × c\n{o}"
        },
        {
          "Layer\nName": "ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h × w × c\n{i}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "-",
          "Output\nSize": "h × w × c\n{o}"
        },
        {
          "Layer\nName": "ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h × w × c\n{o}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "D",
          "Output\nSize": "h/2 × w/2 × c\n{o}"
        },
        {
          "Layer\nName": "Addition",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h/2 × w/2 × c\n{o}"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Shortcut",
          "Kernal\nSize": "[1,1,1]",
          "RS": "D",
          "Output\nSize": "h/2 × w/2 × c {o}"
        },
        {
          "Layer\nName": "ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h × w × c\n{i}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "-",
          "Output\nSize": "h × w × c\n{o}"
        },
        {
          "Layer\nName": "ReLU",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h × w × c\n{o}"
        },
        {
          "Layer\nName": "Convolution",
          "Kernal\nSize": "[3,3,1]",
          "RS": "D",
          "Output\nSize": "h/2 × w/2 × c {o}"
        },
        {
          "Layer\nName": "Addition",
          "Kernal\nSize": "-",
          "RS": "-",
          "Output\nSize": "h/2 × w/2 × c {o}"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input z",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "8 × 4 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "16 × 8 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "32 × 16 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "64 × 32 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "128 × 64 × 16. ch"
        },
        {
          "Layer\nName": "Non-local block",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "128 × 64 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "256 × 128 × 1. ch"
        },
        {
          "Layer\nName": "BN, ReLU",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "Conv [3, 3, 1]",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "Tanh",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "256 × 128 × 1"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input z",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "8 × 4 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "16 × 8 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "32 × 16 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "64 × 32 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "128 × 64 × 16. ch"
        },
        {
          "Layer\nName": "Non-local block",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "128 × 64 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "U",
          "SN": "SN",
          "Output\nSize": "256 × 128 × 1. ch"
        },
        {
          "Layer\nName": "BN, ReLU",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "Conv [3, 3, 1]",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "Tanh",
          "RS": "-",
          "SN": "-",
          "Output\nSize": "256 × 128 × 1"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input\nSpectrogram",
          "RS": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "Non-local block",
          "RS": "-",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "-",
          "Output\nSize": "64 × 32 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "32 × 16 × 2. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "16 × 8 × 4. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "8 × 4 × 8. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock\n(No Shortcut)",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ReLU",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "Global sum pooling",
          "RS": "-",
          "Output\nSize": "1 × 1 × 16. ch"
        },
        {
          "Layer\nName": "Sum(embed(y)h)+(dense → 1)",
          "RS": "-",
          "Output\nSize": "1"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input\nSpectrogram",
          "RS": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "Non-local block",
          "RS": "-",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "-",
          "Output\nSize": "64 × 32 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "32 × 16 × 2. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "16 × 8 × 4. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "8 × 4 × 8. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock\n(No Shortcut)",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ReLU",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "Global sum pooling",
          "RS": "-",
          "Output\nSize": "1 × 1 × 16. ch"
        },
        {
          "Layer\nName": "Dense",
          "RS": "-",
          "Output\nSize": "1"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input\nSpectrogram",
          "RS": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "Non-local block",
          "RS": "-",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "-",
          "Output\nSize": "64 × 32 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "32 × 16 × 2. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "16 × 8 × 4. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "8 × 4 × 8. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock\n(No Shortcut)",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ReLU",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "Global sum pooling",
          "RS": "-",
          "Output\nSize": "1 × 1 × 16. ch"
        },
        {
          "Layer\nName": "Flatten",
          "RS": "-",
          "Output\nSize": "16. ch"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input z",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "1"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input z,\nInput c",
          "Output\nSize": "128 + 10 = 138"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input Feature",
          "Output\nSize": "256"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "256"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "256"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "256"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "256"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "Output\nSize": "10"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer\nName": "Input\nSpectrogram",
          "RS": "-",
          "Output\nSize": "256 × 128 × 1"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "Non-local block",
          "RS": "-",
          "Output\nSize": "128 × 64 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "-",
          "Output\nSize": "64 × 32 × 1. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "32 × 16 × 2. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "16 × 8 × 4. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "8 × 4 × 8. ch"
        },
        {
          "Layer\nName": "ResBlock",
          "RS": "D",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ResBlock\n(No Shortcut)",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "ReLU",
          "RS": "-",
          "Output\nSize": "4 × 2 × 16. ch"
        },
        {
          "Layer\nName": "Global sum pooling",
          "RS": "-",
          "Output\nSize": "1 × 1 × 16. ch"
        },
        {
          "Layer\nName": "Concat with input\nfeature",
          "RS": "-",
          "Output\nSize": "256+128=384"
        },
        {
          "Layer\nName": "Dense",
          "RS": "-",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "ReLU",
          "RS": "-",
          "Output\nSize": "128"
        },
        {
          "Layer\nName": "Dense",
          "RS": "-",
          "Output\nSize": "1"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville",
        "Deep Learning"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Large scale adversarial representation learning",
      "authors": [
        "J Donahue",
        "K Simonyan"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Adversarially learned inference",
      "authors": [
        "V Dumoulin",
        "I Belghazi",
        "B Poole",
        "O Mastropietro",
        "A Lamb",
        "M Arjovsky",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Adversarially learned inference",
      "arxiv": "arXiv:1606.00704"
    },
    {
      "citation_id": "5",
      "title": "Adversarial feature learning",
      "authors": [
        "J Donahue",
        "P Krähenbühl",
        "T Darrell"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "6",
      "title": "Progressive growing of gans for improved quality, stability, and variation",
      "authors": [
        "T Karras",
        "T Aila",
        "S Laine",
        "J Lehtinen"
      ],
      "year": "2017",
      "venue": "Progressive growing of gans for improved quality, stability, and variation"
    },
    {
      "citation_id": "7",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Large scale GAN training for high fidelity natural image synthesis",
      "authors": [
        "A Brock",
        "J Donahue",
        "K Simonyan"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Gansynth: Adversarial neural audio synthesis",
      "authors": [
        "J Engel",
        "K Agrawal",
        "S Chen",
        "I Gulrajani",
        "C Donahue",
        "A Roberts"
      ],
      "year": "2019",
      "venue": "Gansynth: Adversarial neural audio synthesis",
      "arxiv": "arXiv:1902.08710"
    },
    {
      "citation_id": "10",
      "title": "Adversarial audio synthesis",
      "authors": [
        "C Donahue",
        "J Mcauley",
        "M Puckette"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Adversarial generation of time-frequency features with application in audio synthesis",
      "authors": [
        "A Marafioti",
        "N Holighaus",
        "N Perraudin",
        "P Majdak"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "12",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "A Van Den Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "Arxiv"
    },
    {
      "citation_id": "13",
      "title": "Gansynth: Adversarial neural audio synthesis",
      "authors": [
        "J Engel",
        "K Agrawal",
        "S Chen",
        "I Gulrajani",
        "C Donahue",
        "A Roberts"
      ],
      "year": "1902",
      "venue": "CoRR"
    },
    {
      "citation_id": "14",
      "title": "Adversarial generation of time-frequency features with application in audio synthesis",
      "authors": [
        "A Marafioti",
        "N Holighaus",
        "N Perraudin",
        "P Majdak"
      ],
      "year": "2019",
      "venue": "Adversarial generation of time-frequency features with application in audio synthesis",
      "arxiv": "arXiv:1902.04072"
    },
    {
      "citation_id": "15",
      "title": "The large time-frequency analysis toolbox 2.0",
      "authors": [
        "Z Průša",
        "P Søndergaard",
        "N Holighaus",
        "C Wiesmeyr",
        "P Balazs"
      ],
      "year": "2014",
      "venue": "The large time-frequency analysis toolbox 2.0"
    },
    {
      "citation_id": "16",
      "title": "High-fidelity image generation with fewer labels",
      "authors": [
        "M Lucic",
        "M Tschannen",
        "M Ritter",
        "X Zhai",
        "O Bachem",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "High-fidelity image generation with fewer labels"
    },
    {
      "citation_id": "17",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Elements of causal inference: foundations and learning algorithms",
      "authors": [
        "J Peters",
        "D Janzing",
        "B Schölkopf"
      ],
      "year": "2017",
      "venue": "Elements of causal inference: foundations and learning algorithms"
    },
    {
      "citation_id": "19",
      "title": "Recent advances in autoencoder-based representation learning",
      "authors": [
        "M Tschannen",
        "O Bachem",
        "M Lucic"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "21",
      "title": "Infovae: Information maximizing variational autoencoders",
      "authors": [
        "S Zhao",
        "J Song",
        "S Ermon"
      ],
      "year": "2017",
      "venue": "Infovae: Information maximizing variational autoencoders"
    },
    {
      "citation_id": "22",
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "authors": [
        "X Chen",
        "Y Duan",
        "R Houthooft",
        "J Schulman",
        "I Sutskever",
        "P Abbeel"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Adversarial autoencoders",
      "authors": [
        "A Makhzani",
        "J Shlens",
        "N Jaitly",
        "I Goodfellow"
      ],
      "year": "2016",
      "venue": "Adversarial autoencoders"
    },
    {
      "citation_id": "24",
      "title": "Unsupervised speech representation learning using wavenet autoencoders",
      "authors": [
        "J Chorowski",
        "R Weiss",
        "S Bengio",
        "A Van Den Oord"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
      "authors": [
        "F Locatello",
        "S Bauer",
        "M Lui",
        "G Rtsch",
        "S Gelly",
        "B Schlkopf",
        "O Bachem"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Heterogeneous transfer learning for image classification",
      "authors": [
        "Y Zhu",
        "Y Chen",
        "Z Lu",
        "S Pan",
        "G.-R Xue",
        "Y Yu",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "Twenty-Fifth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Transfer learning for visual categorization: A survey",
      "authors": [
        "L Shao",
        "F Zhu",
        "X Li"
      ],
      "year": "2014",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "28",
      "title": "What makes imagenet good for transfer learning?",
      "authors": [
        "M Huh",
        "P Agrawal",
        "A Efros"
      ],
      "year": "2016",
      "venue": "What makes imagenet good for transfer learning?",
      "arxiv": "arXiv:1608.08614"
    },
    {
      "citation_id": "29",
      "title": "Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning",
      "authors": [
        "H.-C Shin",
        "H Roth",
        "M Gao",
        "L Lu",
        "Z Xu",
        "I Nogues",
        "J Yao",
        "D Mollura",
        "R Summers"
      ],
      "year": "2016",
      "venue": "IEEE transactions on medical imaging"
    },
    {
      "citation_id": "30",
      "title": "Gait velocity estimation using time-interleaved between consecutive passive ir sensor activations",
      "authors": [
        "R Rana",
        "D Austin",
        "P Jacobs",
        "M Karunanithi",
        "J Kaye"
      ],
      "year": "2016",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "31",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech",
      "arxiv": "arXiv:1904.03833"
    },
    {
      "citation_id": "32",
      "title": "Multi-task semisupervised adversarial autoencoding for speech emotion",
      "authors": [
        "R Rana",
        "S Latif",
        "S Khalifa",
        "R Jurdak"
      ],
      "year": "2019",
      "venue": "Multi-task semisupervised adversarial autoencoding for speech emotion",
      "arxiv": "arXiv:1907.06078"
    },
    {
      "citation_id": "33",
      "title": "Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation",
      "authors": [
        "W Li",
        "L Duan",
        "D Xu",
        "I Tsang"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Image denoising and restoration with cnn-lstm encoder decoder with direct attention",
      "authors": [
        "K Haque",
        "M Yousuf",
        "R Rana"
      ],
      "year": "2018",
      "venue": "Image denoising and restoration with cnn-lstm encoder decoder with direct attention",
      "arxiv": "arXiv:1801.05141"
    },
    {
      "citation_id": "35",
      "title": "Semi-supervised learning literature survey",
      "authors": [
        "X Zhu"
      ],
      "year": "2005",
      "venue": "Semi-supervised learning literature survey"
    },
    {
      "citation_id": "36",
      "title": "Guiding infogan with semisupervision",
      "authors": [
        "A Spurr",
        "E Aksan",
        "O Hilliges"
      ],
      "year": "2017",
      "venue": "Machine Learning and Knowledge Discovery in"
    },
    {
      "citation_id": "37",
      "title": "Reading digits in natural images with unsupervised feature learning",
      "authors": [
        "Y Netzer",
        "T Wang",
        "A Coates",
        "A Bissacco",
        "B Wu",
        "A Ng"
      ],
      "year": "2011",
      "venue": "Reading digits in natural images with unsupervised feature learning"
    },
    {
      "citation_id": "38",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "39",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "A Krizhevsky",
        "G Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "40",
      "title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks",
      "authors": [
        "J Springenberg"
      ],
      "year": "2015",
      "venue": "Unsupervised and semi-supervised learning with categorical generative adversarial networks"
    },
    {
      "citation_id": "41",
      "title": "Semi-supervised conditional gans",
      "authors": [
        "K Sricharan",
        "R Bala",
        "M Shreve",
        "H Ding",
        "K Saketh",
        "J Sun"
      ],
      "year": "2017",
      "venue": "Semi-supervised conditional gans"
    },
    {
      "citation_id": "42",
      "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
      "authors": [
        "P Warden"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "43",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Improved techniques for training gans",
      "authors": [
        "T Salimans",
        "I Goodfellow",
        "W Zaremba",
        "V Cheung",
        "A Radford",
        "X Chen",
        "X Chen"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "authors": [
        "M Heusel",
        "H Ramsauer",
        "T Unterthiner",
        "B Nessler",
        "S Hochreiter"
      ],
      "year": "2017",
      "venue": "Gans trained by a two time-scale update rule converge to a local nash equilibrium"
    },
    {
      "citation_id": "46",
      "title": "A note on the inception score",
      "authors": [
        "S Barratt",
        "R Sharma"
      ],
      "year": "2018",
      "venue": "A note on the inception score",
      "arxiv": "arXiv:1801.01973"
    },
    {
      "citation_id": "47",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2014",
      "venue": "Going deeper with convolutions"
    },
    {
      "citation_id": "48",
      "title": "The fréchet distance between multivariate normal distributions",
      "authors": [
        "D Dowson",
        "B Landau"
      ],
      "year": "1982",
      "venue": "Journal of multivariate analysis"
    },
    {
      "citation_id": "49",
      "title": "ImageNet: A Large-Scale Hierarchical Image Database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "ImageNet: A Large-Scale Hierarchical Image Database"
    },
    {
      "citation_id": "50",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "51",
      "title": "Mode seeking generative adversarial networks for diverse image synthesis",
      "authors": [
        "Q Mao",
        "H Lee",
        "H Tseng",
        "S Ma",
        "M Yang"
      ],
      "year": "1903",
      "venue": "CoRR"
    }
  ]
}