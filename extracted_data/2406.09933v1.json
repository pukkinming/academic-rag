{
  "paper_id": "2406.09933v1",
  "title": "What Does It Take To Generalize Ser Model Across Datasets? A Comprehensive Benchmark",
  "published": "2024-06-14T11:27:19Z",
  "authors": [
    "Adham Ibrahim",
    "Shady Shehata",
    "Ajinkya Kulkarni",
    "Mukhtar Mohamed",
    "Muhammad Abdul-Mageed"
  ],
  "keywords": [
    "speech emotion recognition",
    "human-computer interaction",
    "computational paralinguistics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is essential for enhancing human-computer interaction in speech-based applications. Despite improvements in specific emotional datasets, there is still a research gap in SER's capability to generalize across realworld situations. In this paper, we investigate approaches to generalize the SER system across different emotion datasets. In particular, incorporate 11 emotional speech datasets and illustrate a comprehensive benchmark on the SER task. We also address the challenge of imbalanced data distribution using oversampling methods when combining SER datasets for training. Furthermore, we explore various evaluation protocols for adeptness in the generalization of SER. Building on this, we explore the potential of Whisper for SER, emphasizing the importance of thorough evaluation. Our approach is designed to advance SER technology by integrating speaker-independent methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are intrinsic characteristics of human communications expressed through speech modality and plays crucial role in human-machine interfaces  [1] . Over the past decades, research has been focused on various feature extraction methods along with machine learning. This includes traditional approaches like Mel-frequency cepstral coefficients (MFCCs)  [2] , linear prediction cepstral coefficients (LPCCs)  [3] , and prosodic features  [4] . Deep learning techniques such as deep neural network DNN  [5] , recurrent neural network (RNN)  [6] , and convolutional neural networks (CNN)  [7]  have also gained prominence. The last three years have seen a considerable exploration of HuBERT  [8]  and Wav2Vec2  [9]  models in SER  [10, 9] , demonstrating their potential for robust emotion recognition.\n\nIEMOCAP  [11]  is considered the benchmark dataset for discrete speech emotion recognition. An extensive evaluation of SER systems has been conducted using IEMOCAP, employing various machine learning methods such as support vector machines (SVM), LSTM, CNN, and ensemble learning  [12, 13] . However, the use of Self-supervised learning (SSL) speech models (e.g., Hubert  [8]  and Wav2Vec2  [9] ) has emerged as the state-of-the-art approach in SER, producing the best performance on the IEMOCAP dataset  [9] . Another very important acted dataset is (RAVDESS)  [14] . RAVDESS was also involved in benchmarking many different SER systems  [15, 16] . A dataset closely similar to RAVDESS is CREMA-D  [17] , which comprises 12 distinct spoken sentences performed by 91 different actors, resulting in a total of 7442 unique audio files.\n\nAmong the more compact SER databases available, the Surrey Audio-Visual Expressed Emotion (SAVEE)  [18]  and the Toronto Emotional Speech Set (TESS)  [19]  stand out. Both datasets have only been benchmarked using traditional machine learning techniques (SVM, LSTM, CNN)  [20, 21, 22] . To the best of our knowledge, these datasets have yet to be tested with newer transformer-based models. MELD dataset has a unique conversational setup of 13,000 utterances from 1,433 dialogues from the TV series Friends. This compilation aims to address the complexities of emotion recognition within conversational contexts, a task known for its challenges. Other SER datasets include ASVP-ESD  [23] , EmoV-DB  [24] , EmoFilm  [25] , JL-Corpus  [26] , and ESD  [27] . Despite the availability of various datasets for speech emotion recognition, there's a consensus that SER still trails behind other tasks  [28, 29, 30] . This shortfall is attributed to the absence of a large, universal dataset capable of bridging the significant disparities among existing datasets and facilitating adaptation to real-world scenarios. A recent study  [16]  investigated the generalization capabilities and the possibility of merging different SER datasets by applying a set of cross-validation experiments, considering both single datasets and combinations of them, on RAVDESS, TESS, CREMA-D, and IEMOCAP. Their results confirm that SER models do not generalize well across datasets (training and testing on different datasets) and that merging datasets can mitigate this problem, as SER performance improves with access to a larger and more varied collection of data points. A key challenge in speech emotion recognition stems from the substantial variance among available datasets, attributed to differences in setup (acted, natural, and elicited), recording quality, and subjective emotion perception by speakers and annotators. Consequently, most systems struggle to generalize across datasets, hindering performance in real-world applications  [16] . Moreover, emotional states influence spoken sentences, compounded by personal attributes like age and gender, posing obstacles for SER generalization and hindering the development of speech-independent systems, particularly in tasks like speech emotion recognition (SER). This challenge is underscored by human behavior, as demonstrated in an experiment by Schuller et al.  [31]  where the accuracy of 12 participants in recognizing expressed emotions decreased from 87.3% with familiar speakers to 64.7% when identifying emotions from speakers they had not previously encountered. Due to variations in emotion classes, emotion labeling, amount of emotional data per dataset, SER more arduous task to generalize. In this paper, we present novel comprehensive benchmarking results across 11 SER datasets and various experimentations showcasing the vital aspects of generalization and performance improvement of SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we highlight our primary contributions: Our SER system based on Whisper, the combination of different datasets, and evaluation strategy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "We illustrate the SER model architecture, which we use throughout this work for various experiments across various SER datasets. This architecture aligns with the current state-ofthe-art (SoTA) and is representative of the current best practices in speech emotion recognition  [10] . Our SER first uses Whisper-based feature extraction  [32]  to map speech into latent representation through a transformerbased encoder-decoder network as shown in Fig 1 . We use the base version of Whisper utilizing both the encoder and decoder to extract fixed-size embedding and fine-tune the whole model with the rest of the network. Thereafter, this latent representation as features is given to feed-forward of fully connected layers: starting from a size of 4096, it sequentially narrows down through layers of 2048, 1024, and 512, with each layer followed by a ReLU activation function to add non-linearity. This setup culminates in an output layer tailored to the number of target classes, making it suitable for classification purposes. To our knowledge, Whisper-based SER has not been explored for ASR tasks. We implement a basic five-layer feed-forward neural network, where the last layer projects into the different emotion labels. Cross-entropy serves as our loss function, and we maintain a constant learning rate of 0.00001. We train all variants of SER on NVIDIA A100-SXM GPUs equipped with 40GB of memory.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets Preparation",
      "text": "Emotional databases in speech emotion recognition are classified into acted, elicited, and natural categories, each with unique characteristics and challenges. Acted databases consist of recordings from trained actors simulating specific emotions, offering control but relying on the actors' skills  [33] . Elicited databases capture genuine emotional responses under designed scenarios requiring ethical consent, while natural databases compile authentic interactions from everyday life, presenting a wide authenticity spectrum but limited emotional range  [34] .\n\nOur study conducts a detailed analysis of individual and combined datasets to understand the impact of similar emotions on model accuracy and generalization on 11 datasets in the SER domain: IEMOCAP  [11] : Approximately 12 hours from 10 speakers, with scripted and improvised dialogues across seven emotion classes (anger, happiness, sadness, excited, frustrated, and neutral). MELD  [35] : 13,708 utterances over 12 hours from the Friends TV series, categorized into seven emotions (anger, disgust, sadness, joy, neutral, surprise, fear) with 10 main speakers and around 290 secondary speakers. ASVP-ESD  [23] : A multi-lingual corpus from movies, YouTube, and real interactions, focusing on English speech with seven emo-\n\ntions, totaling 6.5 hours from main and secondary speakers.\n\nEmoV-DB  [24] : Covers five emotions with five speakers (four in English, one in French), totaling 9.5 hours. TESS  [19] : Features two actresses, covering seven emotion classes over 2800 files in 1.6 hours. EmoFilm  [25] : Samples in English, Italian, and Spanish from 43 movies, focusing on five emotions over 20 minutes. SAVEE  [18] : British-English corpus with 480 utterances from four speakers, spanning seven emotions in 30 minutes. RAVDESS  [14] : Focuses on the speech part with 1440 utterances from 24 actors, covering eight emotions over 1.5 hours. CREMA-D  [17] : Audio-visual dataset with 7442 stimuli from 91 actors, spanning six emotions over 5.3 hours.\n\nJL corpus  [26] : Contains 2400 sentences from four speakers, covering 10 emotions over 1.4 hours. ESD  [27] : Offers 350 parallel utterances in five emotion categories from 10 native English speakers, with over 29 hours recorded.\n\nIn our investigation, we specifically extracted English speech and corresponding labels from mixed-language datasets like ESD  [27] , ASVP-ESD  [23] , and EmoFilm  [25] . It's essential to highlight that the other datasets employed in our study are exclusively in English, ensuring a consistent linguistic foundation for our analysis. Regarding the IEMOCAP dataset, our dataset preparation followed the guidelines outlined in a previous study  [36] , where emotions were categorized into anger, happiness, sadness, and neutral, with excitement categorized under happiness. We retained the original emotion classes for the remaining datasets. Additionally, we downsampled speech across all speech utterances to 16 kHz. Speaker identification for each audio clip in every dataset was accomplished using the information provided in the dataset metadata.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Combining Datasets For The Ser Task",
      "text": "A major challenge in speech emotion recognition lies in the considerable variance among the available datasets. This variance stems from differences in their setup (acted, natural, and elicit), the quality of recordings, and the subjective perception of emotions by different speakers and annotators. Consequently, most speech emotion recognition systems do not generalize well across different datasets (training and testing on different datasets)  [16]  and, by extension, struggle to perform well in real-world applications.\n\nIn an effort to address this challenge,  [16]  showed promising results by combining four of the most famous SER datasets (RAVDESS  [14] , TESS  [19] , CREMA-D  [17] , and IEMOCAP  [11] ). In this work, in addition to these four datasets, we added seven more (MELD  [35] , ASVP-ESD  [23] , EmoV-DB  [24] , EmoFilm  [25] , SAVEE  [18] , JL-Corpus  [26] , and ESD  [27] ) for a total of eleven SER datasets. To the best of our knowledge, combining 11 datasets presents the largest training dataset for the SER task. This study aims to significantly boost performance (against individual datasets) on the SER task, attributable to the considerably larger and more diverse array of training data points.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "Throughout this work, we opted for the leave-one-speaker-out (LOSO) method to evaluate our models. We used accuracy as a way to measure the performance of SER systems. When dealing with datasets where all speakers contributed the same amount of speech, we randomly selected one speaker from each dataset as our test case. Conversely, for datasets where speakers had varying amounts of speech time, such as MELD, ASVP-ESD, and IEMOCAP, we excluded the speaker with the most speech duration. At the end of this process, we ensured that a speaker from every dataset was chosen, guaranteeing a wide-ranging and inclusive evaluation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Study",
      "text": "In our work, we consider three different sets of experiments for training and evaluation: 1. We establish an initial benchmark by training each dataset individually using all their original emotion classes. 2. We use only four emotions (neutral, angry, happy, and sad) to train and test our SER models. We do this for both individual and combined settings. 3. We use five emotions adding surprise to the previous four to train and test our SER models. We do this for both individual and combined settings.\n\nWe used a consistent training setup involving 5-fold crossvalidation. This means we divided the data so that each fold used a different speaker for validation and the rest for training. For datasets with less than five speakers, we adjusted the number of folds to match the number of speakers. Additionally, with the TESS dataset, which has only two speakers, we used one for testing and the other for training, applying 5-fold cross-validation on the training speaker's data. In the case of the EmoFilm dataset, we used 85% of the data for training, still following the 5-fold cross-validation method. For combined datasets, however, we adapted 5-fold cross-validation based on percentage splits across the dataset to ensure even distribution. It is important to note that we leave one speaker from each dataset as a test set right from the start, calculating a weighted average on the speaker-out test set across the 5-folds to ensure uniform and comprehensive evaluation metrics across all experiments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Individual And Combined 4-Emotion Dataset Evaluation",
      "text": "For every dataset, we train a Whisper-based model using the four primary emotions: neutral, angry, happy, and sad. If a dataset lacks one of these emotions, we include from the dataset whichever available subset from these four emotions. Table  1  shows that we considered the most common 4 emotions across all datasets. For the combined dataset, we combined 11 SER datasets into a single comprehensive dataset based on the same 4 emotions. We included audio clips with duration ranging from 2 to 13 seconds, to ensure consistency and comparability across the combined dataset. Speech clips within this duration range are the most common duration of data across all datasets. After combining emotions from various datasets, the data distribution exhibited imbalance, as illustrated in Table  2 . This imbalance in the distribution of emotional categories across the combined dataset could potentially influence both the training and evaluation phases of our model. Explored four techniques to determine if any could improve model generalization: no sampling, randomly downsampling to the lowest category, and oversampling performed using the Synthetic Minority Over-sampling Technique (SMOTE)  [37]  and the Adaptive Synthetic Sampling Method for Imbalanced Data (ADASYN)  [38] . We applied the SMOTE and ADASYN algorithms to the audio data.\n\nThe oversampling process was conducted to balance the distribution of emotion labels, ensuring that low-frequency emotions were over-sampled to match the frequency of the highest emotion category.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Individual And Combined 5-Emotions Dataset Evaluation",
      "text": "This step was taken to ascertain the model's robustness in handling an expanded set of emotion labels. we trained a Whisperbased model on a set of 5 emotions neutral, angry, happy, surprised, and sad. We train the model on each dataset using the same settings as in the individual training described in the previous experiment in Section 3.1. We repeat training and test settings in combined dataset training in the previous experiment, but with 5 emotions neutral, angry, happy, surprised, and sad.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "In this section, we provide a comprehensive overview of speech emotion recognition performance across different datasets, emotion categories, training criteria, and data sampling techniques, offering valuable insights for the generalization of speech emotion recognition. Table  3 . illustrates the performance of the Whisper-based speech emotion recognition system developed under various combinations of dataset, and oversampling techniques. In general, the performance of SER varies across datasets and emotion categories. For instance, datasets like emov db consistently demonstrate high accuracy, while others like MELD exhibit lower performance under all conditions. Overall, using original data distribution or using SMOTE sampling tends to yield better performance compared to downsampling and ADASYN sampling. For instance, mean accuracy for training SER using a combination of 4 emotions demonstrates that using original data distribution and SMOTE sampling provides better results than other conditions. This suggests that maintaining the original sample distribution or using synthetic samples for minority classes can enhance model performance. Furthermore, training systems separately on each dataset for 4 emotion and 5 emotion experiments under performed in comparison to combining datasets. Thereafter, performance varies across datasets and emotion categories. For instance, datasets like emov db consistently showcase high accuracy, while others like MELD display comparatively lower performance. The choice of data sampling technique signifi-  Each dataset exhibits unique characteristics concerning the designing process of the dataset such as emotion label annotation, recording conditions of emotions, speaker demographics, and linguistic variations. etc, have an impact on emotion recognition performance. Hence, the factors such as data quality, diversity, and class distribution influence the effectiveness of trained models. For example, datasets like CREMA-D and emov db, which offer high-quality and diverse speech samples, tend to achieve better performance across different emotion categories. Therefore, it is vital to understand how different training conditions affect model generalization is essential for developing robust speech emotion recognition systems. In conducting our research on 11 SER datasets with Whisper-based models, we encountered several limitations.\n\nInitially, the inherent diversity and imbalance in emotional representation across datasets introduced challenges in standardizing the training process, particularly when reducing the emotion categories to 4 and then to 5 specific emotions. Additionally, the process of combining datasets to improve model resilience encountered challenges in aligning emotional labels and intensities, which could potentially influence the consistency of our findings. The average accuracy findings for both the 4-emotion and 5-emotion experiments consistently demonstrated enhanced performance when utilizing the combined dataset, as opposed to separately training SER on each individual dataset. The comparison between models trained on individual datasets versus a combined dataset approach revealed variations in performance, suggesting that not all datasets contribute equally to model accuracy and generalizability. t-SNE in Figure  2  visualizes the feature embeddings extracted from the Whisper model, representing various emotional states as dis-cerned from multiple speech datasets. It's noteworthy how the datasets SAVEE, ESD, and JL corpus form closely knit clusters, suggesting that their emotional representations in the feature space are similar to each other. On the other hand, the MELD, IEMOCAP, and CREMA-D datasets tend to group closer to one another, indicating a different but consistent internal representation of emotions within these datasets. The RAVDESS and TESS datasets stand out with their unique positioning in the plot, reflecting distinct emotion representation patterns extracted by the Whisper model. Moreover, for each emotion exclusive representation on each dataset reflects the inherent variations in emotions across datasets and its impact on generalization of SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "This study proposes a novel approach to generalize SER model in leaving-one-speaker-out settings. Employing a leave-onespeaker-out methodology highlighted the model's robustness in real-world scenarios, facing speaker variability. Extensive experiments were conducted by combining multiple SER datasets to train a Whisper-based model, followed by testing the model on a single speaker from each dataset, which yielded promising results, indicating a successful generalization across diverse datasets. These findings suggest that through careful dataset combination and targeted model training strategies, we can overcome some of the prevalent challenges in speech emotion recognition, paving the way for more universally applicable SER systems. In conclusion, our work yielded a generalized SER model adept at identifying emotions across a spectrum of datasets. This represents progress in the field, enabling the development of advanced emotion recognition systems capable of effectively handling diverse datasets. Future directions for emotion recognition in low-resource languages involve leveraging transfer learning and unsupervised learning approaches while considering cultural nuances and linguistic expertise.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Whisper based speech emotion recognition system",
      "page": 2
    },
    {
      "caption": "Figure 1: We use the",
      "page": 2
    },
    {
      "caption": "Figure 2: t-SNE visualization of Whisper model’s embeddings showcasing clusters of emotional speech from various datasets.",
      "page": 4
    },
    {
      "caption": "Figure 2: visualizes the feature embeddings extracted from the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset Name": "",
          "4-Emotions WA Test (%)": "Emo. No.",
          "5-Emotions WA Test (%)": "Emo. No.",
          "N-Emotions WA Test (%)": "Emo. No."
        },
        {
          "Dataset Name": "",
          "4-Emotions WA Test (%)": "",
          "5-Emotions WA Test (%)": "",
          "N-Emotions WA Test (%)": ""
        },
        {
          "Dataset Name": "ESD\nMELD\nIEMOCAP\nCREMA-D\nEMOV DB\nASVP-ESD\nTESS\nJL corpus\nRAVDESS\nSAVEE\nEmoFilm",
          "4-Emotions WA Test (%)": "4\n4\n4\n4\n2\n3\n4\n4\n4\n4\n3",
          "5-Emotions WA Test (%)": "5\n5\n5\n4\n2\n4\n5\n4\n5\n5\n3",
          "N-Emotions WA Test (%)": "5\n7\n7\n6\n5\n7\n7\n10\n8\n7\n5"
        },
        {
          "Dataset Name": "Mean",
          "4-Emotions WA Test (%)": "-",
          "5-Emotions WA Test (%)": "-",
          "N-Emotions WA Test (%)": "-"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Expressivity transfer in deep learning based text-to-speech synthesis. (synthèse vocale expressive basée sur un apprentissage profond)",
      "authors": [
        "A Kulkarni"
      ],
      "year": "2022",
      "venue": "Expressivity transfer in deep learning based text-to-speech synthesis. (synthèse vocale expressive basée sur un apprentissage profond)"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition based on mixed MFCC",
      "authors": [
        "P Zhou",
        "X Li",
        "J Li",
        "X Jing"
      ],
      "year": "2013",
      "venue": "Applied Mechanics and Mechanical Engineering III"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using DWT",
      "authors": [
        "S Lalitha",
        "A Mudupu",
        "B Nandyala",
        "R Munagala"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2013",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "venue": "ICASSP IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "13",
      "title": "A study of support vector machines for emotional speech recognition",
      "authors": [
        "N Kurpukdee",
        "S Kasuriya",
        "V Chunwijitra",
        "C Wutiwiwatchai",
        "P Lamsrichan"
      ],
      "year": "2017",
      "venue": "8th international conference of information and communication technology for embedded systems (IC-ICTES)"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "16",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition and deep learning: An extensive validation using convolutional neural networks",
      "authors": [
        "F Rì",
        "F Ciardi",
        "N Conci"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "20",
      "title": "Toronto emotional speech set (tess)-younger talker happy",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (tess)-younger talker happy"
    },
    {
      "citation_id": "21",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "22",
      "title": "Equilibrium optimizer for emotion classification from english speech signals",
      "authors": [
        "L Yue",
        "P Hu",
        "S.-C Chu",
        "J.-S Pan"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in low-resource settings: An evaluation of automatic feature selection methods",
      "authors": [
        "F Haider",
        "S Pollak",
        "P Albert",
        "S Luz"
      ],
      "year": "2021",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "24",
      "title": "Audio,Speech and Vision Processing Lab Emotional Sound database (ASVP-ESD)",
      "authors": [
        "T Dejoli",
        "Q He",
        "W Xie"
      ],
      "year": "2021",
      "venue": "Zenodo"
    },
    {
      "citation_id": "25",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adigwe",
        "N Tits",
        "K Haddad",
        "S Ostadabbas",
        "T Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "26",
      "title": "Emofilm-a multilingual emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Emofilm-a multilingual emotional speech corpus"
    },
    {
      "citation_id": "27",
      "title": "An open source emotional speech corpus for human robot interaction applications",
      "authors": [
        "J James",
        "L Tian",
        "C Watson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "29",
      "title": "Temporal attention convolutional network for speech emotion recognition with latent representation",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang",
        "Y Gao",
        "L Guo",
        "J Dang"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised learning approach to feature analysis for automatic speech emotion recognition",
      "authors": [
        "S Eskimez",
        "Z Duan",
        "W Heinzelman"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Speaker independent speech emotion recognition by ensemble classification",
      "authors": [
        "B Schuller",
        "S Reiter",
        "R Muller",
        "M Al-Hames",
        "M Lang",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "Speaker independent speech emotion recognition by ensemble classification"
    },
    {
      "citation_id": "33",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "ICML"
    },
    {
      "citation_id": "34",
      "title": "Speaker-sensitive emotion recognition via ranking: Studies on acted and spontaneous speech",
      "authors": [
        "H Cao",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2015",
      "venue": "Computer speech & language"
    },
    {
      "citation_id": "35",
      "title": "A review on emotion recognition using speech",
      "authors": [
        "S Basu",
        "J Chakraborty",
        "A Bag",
        "M Aftabuddin"
      ],
      "year": "2017",
      "venue": "International conference on inventive communication and computational technologies (ICICCT)"
    },
    {
      "citation_id": "36",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "37",
      "title": "Speaker-independent speech emotion recognition based on cnn-blstm and multiple svms",
      "authors": [
        "Z.-T Liu",
        "P Xiao",
        "D.-Y Li",
        "M Hao"
      ],
      "year": "2019",
      "venue": "Proc of ICIRA 2019"
    },
    {
      "citation_id": "38",
      "title": "Smote for learning from imbalanced data: progress and challenges, marking the 15-year anniversary",
      "authors": [
        "A Fernández",
        "S Garcia",
        "F Herrera",
        "N Chawla"
      ],
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "39",
      "title": "Adasyn: Adaptive synthetic sampling approach for imbalanced learning",
      "authors": [
        "H He",
        "Y Bai",
        "E Garcia",
        "S Li"
      ],
      "year": "2008",
      "venue": "IEEE International Joint Conference on Neural Networks"
    }
  ]
}