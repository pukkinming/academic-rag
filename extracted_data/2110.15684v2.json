{
  "paper_id": "2110.15684v2",
  "title": "Fusing Asr Outputs In Joint Training For Speech Emotion Recognition",
  "published": "2021-10-29T11:21:17Z",
  "authors": [
    "Yuanchao Li",
    "Peter Bell",
    "Catherine Lai"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Automatic speech recognition",
    "Multi-task learning",
    "Wav2vec 2.0"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Alongside acoustic information, linguistic features based on speech transcripts have been proven useful in Speech Emotion Recognition (SER). However, due to the scarcity of emotion labelled data and the difficulty of recognizing emotional speech, it is hard to obtain reliable linguistic features and models in this research area. In this paper, we propose to fuse Automatic Speech Recognition (ASR) outputs into the pipeline for joint training SER. The relationship between ASR and SER is understudied, and it is unclear what and how ASR features benefit SER. By examining various ASR outputs and fusion methods, our experiments show that in joint ASR-SER training, incorporating both ASR hidden and text output using a hierarchical co-attention fusion approach improves the SER performance the most. On the IEMOCAP corpus, our approach achieves 63.4% weighted accuracy, which is close to the baseline results achieved by combining ground-truth transcripts. In addition, we also present novel word error rate analysis on IEMOCAP and layer-difference analysis of the Wav2vec 2.0 model to better understand the relationship between ASR and SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Despite the rapid development of SER, its use in the wild remains difficult, due in part to the variability and vulnerability of human speech. To address this issue, researchers have proposed combining linguistic (i.e. transcript-derived) and acoustic information to improve the stability of SER. Traditional feature-level fusion and decision-level fusion, as well as the current tensor-level fusion trend, are widely used  [1] . Furthermore, many researchers have also compared the performance of various fusion methods  [2] . All of these works demonstrated the utility of combining linguistic information in SER.\n\nNonetheless, there is limited reliable text information in this area, particularly in real-life SER. First, when compared to ASR, the corpora sizes for SER are relatively small. For example, the popular IEMOCAP  [3]  only has about 12 hours of speech, whereas LibriSpeech  [4]  has about 1,000 hours. SER models built on such limited sized corpora don't generalize well to out-of-domain speech. Second, while previous studies proposed using ASR to generate transcripts for SER  [5] , ASR on emotional speech can often result in relatively high error rates. Previous research has shown that emotion in speech degrades ASR performance, with emotional speech assumed to be a distortion of neutral speech  [6] . However, with the advancement of deep learning technologies, transfer learning for SER from ASR and joint training of ASR and SER have recently emerged  [7, 8] . Nevertheless, the relationship between ASR and SER is still poorly studied, particularly what and how ASR features can benefit SER.\n\nIn this paper, we investigate various ASR outputs and fusion methods for a joint ASR-SER training model. We analyze the Word Error Rate (WER) on IEMOCAP using Wav2vec 2.0 (W2V2), elucidating the reasons why ASR fails on emotional speech, which has not previously been reported. We also compare four ASR outputs from W2V2 and three fusion methods, and find that our proposed hierarchical coattention fusion achieves 63.4% Weighted Accuracy (WA), comparable performance to the baseline result using groundtruth transcripts.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "The relationship between ASR and SER is an important but understudied topic. Although both tasks use speech signals as input, ASR works more at the frame level, whereas SER recognizes emotion on larger timescales. Previous work  [9]  has demonstrated that features from the initial layers of both ASR and SER tasks are transferable and that the relevance of features gradually fades through deep layers. However, ASR features closer to the audio modality can outperform those closer to the text modality for arousal prediction and vice versa for valence prediction  [10] .\n\nThese findings suggest it is possible to use transfer learning for SER from ASR. For example,  [11]  used hidden layer output from a pre-trained ASR model as linguistic features for SER and obtained promising results. Similarly,  [8]  used a single W2V2 model for both ASR and SER tasks and achieved State-Of-The-Art (SOTA) performance on 10-fold Cross-Validation (CV). However, gaps remain on foundational issues such as whether WER varies with emotion type, and where ASR features should enter SER models. In this paper, we build on this line of work, conducting new analyses on WER, ASR outputs, and fusion methods, with the goal of understanding how to best bring ASR into SER research.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experiment Preparation And Preliminary Analysis",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Corpus And Asr Model",
      "text": "In these experiments, we use the IEMOCAP corpus  [3]  as it is widely used in SER research. It consists of five dyadic sessions with ten actors, each with a scripted and improvised multimodal interaction. The corpus contains approximately 12 hours of speech that has been annotated by three annotators using ten emotion classes. We combine happy and excited, and use four categories: angry, happy, neutral, and sad, based on prior research  [12]  . We removed utterances that did not have transcripts, bringing the total number of utterances used in this study to 5500. We use the well-known W2V2 model  [13]  for the ASR model. W2V2 is a self-supervised learning framework composed of three major components: a CNN-based local encoder that extracts a sequence of embeddings from raw audio as latent representation, a Transformer network for context representation, and a quantization module for discretization. In this paper, we use the \"Wav2vec2-base-960h\" model, which has been fine-tuned using 960 hours of Librispeech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminary Analysis Of Wer",
      "text": "We calculate the WER on IEMOCAP to investigate the types of errors that occur in emotional speech (Table  1 ). Surprisingly, we see that the neutral WER is the second highestwe would expect WER for neutral speech to be the lowest as the least emotionally 'distorted'. Furthermore, while Angry and Happy can have similarly variable prosodic characteristics (high f0 and intensity)  [14]  , Angry has the lowest WER while Happy has the highest. To determine the reasons for this, we conducted a word count analysis. The results in Table  1  show that emotion classes with low WER typically have a low ratio of short utterances (word count less than 10). The fact that neutral speech is less prosodically variable may be the reason it has a lower WER than happy, even though its short utterances rate is the highest.\n\nIn fact, we calculated the WER based on the number of words, and found that the shorter the utterance, the higher the WER (see Table  2 ). This finding contradicts a previous finding that longer utterances are more likely to have higher error rates  [5] . This may be due to the fact that, when using W2V2, longer utterances contain more contextual information that the ASR can use to compensate for the negative effects of emotion. However, short utterances, and similarly disfluencies, are clearly very important in understanding affect in spoken dialogues  [15] . We hope that this preliminary analysis and novel findings will aid future ASR-SER research, by motivating the joint training to potentially learn these sorts of complex interactions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction And Encodings",
      "text": "In our model, the raw audio time-series is encoded by the W2V2 model (for ASR), and separately as MFCCs (for SER).\n\nOn the ASR path, the W2V2 representations are then decoded to text by a word-level Connectionist Temporal Classification (CTC) decoder. Both the hidden state output and the text are extracted. A BERT model  [16]  is then used to extract linguistic features from text output. On the SER path, a max pooling with kernel size 2 is conducted on the MFCC features to obtain an acoustic representation.\n\nThe ASR hidden output (short for hidden layer output), text output, and pooled MFCCs, are then encoded using 2layer Bidirectional Long Short-Term Memory (Bi-LSTM) networks. Each layer contains 32 hidden units followed by a dropout with probability of 0.5. A self-attention layer with 16 heads and 64 nodes is applied on the output of the Bi-LSTM, which generates a fixed-length vector as the feature encoding.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Fusion And Emotion Output",
      "text": "We fuse the three encodings from the self-attention layer to produce a final vector for emotion classification. We compared three fusion methods (Fig.  1 ): (a) concatenation, (b) concatenation with co-attention fusion, and (c) our proposed hierarchical co-attention fusion. The co-attention mechanism [17] concatenates two hidden-state vectors which exchange key-value pairs in self-attention, allowing features from one input channel to be incorporated into the other:\n\nwhere W O , W Q i , W K i , and W V i are trainable parameters. Q A represents the query from one input channel, while K B and V B represent the key and value from the other. The value of n is 16, and H C denotes the final concatenated hidden states of co-attention.\n\nHierarchical approaches for emotion recognition have proved useful in fusing different-level or different-type of features in previous works  [18] . Inspired by these works and the hierarchical characteristics of speech  [19] , we propose to fuse the input features from low to high level in a hierarchical manner using co-attention. Because MFCC features are extracted within frames, W2V2 features contain within-utterance context, and BERT features can learn cross-sentence context, we refer to them as frame-level, utterance-level, and dialog-level respectively, and hierarchically fuse them in this order to generate a fixed-length vector. This is passed to a fully connected output layer with Softmax activation function to generate the probability distributions over emotion classes.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "The model is optimized by the multi-task loss function:\n\nwhere L ASR and L SER are the losses for ASR and SER, respectively. We set λ to 0.2, and use cross-entropy as the loss function. We use the Adam optimizer, with a learning rate of 10 -4 and decay rate of 10 -5 . The gradients are clipped with a threshold of 5.0. The batch size is set to 20, and the number of epochs is limited to 100. We perform 5-fold CV and use WA to assess the performance. The experiment results are shown in Table  3 . Here, we present (1) the ground-truth model incorporating human transcripts in the SER model (i.e. no ASR); results of including (2) ASR hidden output or (3) text output; (4) the results of our proposed full model. We used the middle layer ASR output from W2V2 model because of its highest WA (see Table  4 ).\n\nAs expected, the ground-truth model achieves the best performance as the ASR outputs contain recognition errors. Both ASR hidden output and text output help improve the SER performance over the acoustic features, although the difference is small. In general, the performance of co-attention fusion outperforms that of concatenation. It appears that the relatedness of two input channels is learned by attention. We also see that when using ground-truth transcripts, co-attention improves performance more than when using ASR outputs. It is plausible because the incorrectly recognized ASR features may have little or no relatedness to acoustic features, reducing the effectiveness of attention. Finally, our ap- We conducted a layer-difference analysis study to determine which W2V2 layer contributes the most to the SER task.  [20]  examined the layer-specific information in W2V2 intermediate speech representations and found that various acoustic and linguistic properties are encoded in different layers. As such, we compared the hidden-state outputs from the first layer (initial embeddings), middle layer, and final layer and found a surprising difference in SER performance. Table  4  shows that the WA of using the hidden outputs from first and middle layers are close, but decreases significantly when using the final layer. Furthermore, we discovered that the text output outperforms the hidden outputs, which contradicts prior assertions that \"ASR features can be more robust than the text output of ASR\"  [11] . This difference may be specific to W2V2 and requires further investigation, which we leave for future work.\n\nWe present previous works that also use ASR outputs in SER, but note that due to variation in experimental setup, results are not directly comparable.  [21]  used the Google Cloud Speech API to generate transcripts that were then fused with MFCCs for SER. They achieved 69.1% WA, likely due to their low WER (5.53%).  [5]  concatenated GloVe features obtained from Wit.ai API text transcription with acoustic features, obtaining a 62.9% Unweighted Accuracy (UA) using an LSTM based model.  [11]  jointly trained ASR-SER using log mel-scale filter bank outputs and decoder outputs from a pretrained ASR model, and achieved 68.6% WA.  [8]  used a single W2V2 as the training model for both ASR and SER rather than separating two tasks, and used 10-fold CV, which usually yields better results than 5-fold;  [7]  carefully fine-tuned the pre-trained ASR model on IEMOCAP before transferring the ASR features for SER. Note that since our goal is to analyze potential role of ASR in ASR-SER rather than achieve the SOTA performance, we did not focus on fine-tuning the model and parameters, but we expect that the performance of our model can be further improved by parameter tuning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we propose a joint ASR-SER training model that incorporates both ASR hidden output and text output using a hierarchical co-attention fusion approach. On IEMOCAP, it achieves 63.4% WA, which is comparable to the ground-truth transcript model. By conducting WER analysis, we demonstrate situations where ASR can fail on emotional speech. We hope that the findings, which have not previously been reported can contribute to understanding the ASR-SER relationship. In our future work, we will conduct detailed layerwise analysis to understand what meanings the intermediate representations represent as the W2V2 layer becomes deeper. We will also try character-level or hybrid CTC decoders to solve the language model mismatch problem.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ): (a) concatenation, (b)",
      "page": 2
    },
    {
      "caption": "Figure 1: Proposed model with different fusion methods.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "University of Edinburgh, Scotland, UK": "y.li-385@sms.ed.ac.uk, {peter.bell, c.lai}@ed.ac.uk"
        },
        {
          "University of Edinburgh, Scotland, UK": "speech, whereas LibriSpeech [4] has about 1,000 hours. SER"
        },
        {
          "University of Edinburgh, Scotland, UK": "models built on such limited sized corpora don’t generalize"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "well to out-of-domain speech. Second, while previous studies"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "proposed using ASR to generate transcripts for SER [5], ASR"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "on emotional speech can often result\nin relatively high error"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "rates.\nPrevious research has shown that emotion in speech"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "degrades ASR performance, with emotional speech assumed"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "to be a distortion of neutral speech [6]. However, with the"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "advancement of deep learning technologies, transfer learning"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "for SER from ASR and joint\ntraining of ASR and SER have"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "recently emerged [7, 8]. Nevertheless,\nthe relationship be-"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "tween ASR and SER is still poorly studied, particularly what"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "and how ASR features can beneﬁt SER."
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "In this paper, we investigate various ASR outputs and"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "fusion methods\nfor a joint ASR-SER training model. We"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "analyze the Word Error Rate (WER) on IEMOCAP using"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "Wav2vec 2.0 (W2V2), elucidating the reasons why ASR fails"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "on emotional speech, which has not previously been reported."
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "We also compare four ASR outputs\nfrom W2V2 and three"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "fusion methods, and ﬁnd that our proposed hierarchical co-"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "attention fusion achieves 63.4% Weighted Accuracy (WA),"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "comparable performance to the baseline result using ground-"
        },
        {
          "University of Edinburgh, Scotland, UK": "truth transcripts."
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "2. RELATED WORK"
        },
        {
          "University of Edinburgh, Scotland, UK": ""
        },
        {
          "University of Edinburgh, Scotland, UK": "The relationship between ASR and SER is an important but"
        },
        {
          "University of Edinburgh, Scotland, UK": "understudied topic. Although both tasks use speech signals as"
        },
        {
          "University of Edinburgh, Scotland, UK": "input, ASR works more at the frame level, whereas SER rec-"
        },
        {
          "University of Edinburgh, Scotland, UK": "ognizes emotion on larger timescales. Previous work [9] has"
        },
        {
          "University of Edinburgh, Scotland, UK": "demonstrated that features from the initial layers of both ASR"
        },
        {
          "University of Edinburgh, Scotland, UK": "and SER tasks are transferable and that\nthe relevance of fea-"
        },
        {
          "University of Edinburgh, Scotland, UK": "tures gradually fades through deep layers. However, ASR fea-"
        },
        {
          "University of Edinburgh, Scotland, UK": "tures closer to the audio modality can outperform those closer"
        },
        {
          "University of Edinburgh, Scotland, UK": "to the text modality for arousal prediction and vice versa for"
        },
        {
          "University of Edinburgh, Scotland, UK": "valence prediction [10]."
        },
        {
          "University of Edinburgh, Scotland, UK": "These ﬁndings suggest it is possible to use transfer learn-"
        },
        {
          "University of Edinburgh, Scotland, UK": "ing for SER from ASR. For example, [11] used hidden layer"
        },
        {
          "University of Edinburgh, Scotland, UK": "output\nfrom a pre-trained ASR model as linguistic features"
        },
        {
          "University of Edinburgh, Scotland, UK": "for SER and obtained promising results. Similarly,\n[8] used"
        },
        {
          "University of Edinburgh, Scotland, UK": "a\nsingle W2V2 model\nfor both ASR and SER tasks\nand"
        },
        {
          "University of Edinburgh, Scotland, UK": "achieved State-Of-The-Art\n(SOTA) performance on 10-fold"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: ). Surpris- totextbyaword-levelConnectionistTemporalClassification",
      "data": [
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "tional issues such as whether WER varies with emotion type,",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "and where ASR features should enter SER models.",
          "gaps": "",
          "remain on founda-": "In this"
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "paper, we build on this line of work, conducting new analyses",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "on WER, ASR outputs, and fusion methods, with the goal of",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "understanding how to best bring ASR into SER research.",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "3. EXPERIMENT PREPARATION AND",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "",
          "remain on founda-": ""
        },
        {
          "Cross-Validation (CV). However,": "",
          "gaps": "PRELIMINARY ANALYSIS",
          "remain on founda-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: ). Surpris- totextbyaword-levelConnectionistTemporalClassification",
      "data": [
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "",
          "44.8%": "Table 2. WER according to different number of words N.",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "N",
          "44.8%": "Ang",
          "55.7%": "Hap",
          "60.5%": "Neu",
          "52.7%": "Sad"
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "≤ 10",
          "44.8%": "26.5%",
          "55.7%": "54.8%",
          "60.5%": "48.4%",
          "52.7%": "47.2%"
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "11-20",
          "44.8%": "21.8%",
          "55.7%": "37.5%",
          "60.5%": "36.1%",
          "52.7%": "30.4%"
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "21-30",
          "44.8%": "21.4%",
          "55.7%": "31.0%",
          "60.5%": "27.2%",
          "52.7%": "22.6%"
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        },
        {
          "Short%": "≥ 30",
          "44.8%": "20.9%",
          "55.7%": "31.0%",
          "60.5%": "26.1%",
          "52.7%": "20.1%"
        },
        {
          "Short%": "",
          "44.8%": "",
          "55.7%": "",
          "60.5%": "",
          "52.7%": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: . Here, we",
      "data": [
        {
          "Fig. 1. Proposed model with different fusion methods.": "[17] concatenates two hidden-state vectors which exchange"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "key-value pairs in self-attention, allowing features from one"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "input channel to be incorporated into the other:"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "(1)\nHC = Concat(HC1 , HC2 )"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "(2)\nHCi = M ultiHead(QA, KB, VB)W O"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "(3)\n= Concat(head1, ..., headn)"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ")\n(4)\nheadi = Attention(QAW Q\n, KBW K\n, VBW V"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "where W O, W Q\n, W K\n, and W V\nare trainable parameters. QA"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "i\ni\ni"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "represents the query from one input channel, while KB and"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "the key and value from the other. The value of\nVB represent"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "n is 16, and HC denotes the ﬁnal concatenated hidden states"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "of co-attention."
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": ""
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "Hierarchical\napproaches\nfor\nemotion\nrecognition\nhave"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "proved useful in fusing different-level or different-type of fea-"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "tures in previous works [18]. Inspired by these works and the"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "hierarchical characteristics of speech [19], we propose to fuse"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "the input features from low to high level in a hierarchical man-"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "ner using co-attention. Because MFCC features are extracted"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "within frames, W2V2 features contain within-utterance con-"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "text, and BERT features can learn cross-sentence context, we"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "refer to them as frame-level, utterance-level, and dialog-level"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "respectively, and hierarchically fuse them in this order to gen-"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "erate a ﬁxed-length vector. This is passed to a fully connected"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "output\nlayer with Softmax activation function to generate the"
        },
        {
          "Fig. 1. Proposed model with different fusion methods.": "probability distributions over emotion classes."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: We hope that the findings, which have not previously been",
      "data": [
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": "Feature"
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": "Acoustic + Ground-truth transcripts"
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": ""
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": "Acoustic + Hidden output (middle layer)"
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": ""
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": "Acoustic + Text output"
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": ""
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": "Acoustic + Hidden output (middle layer) + Text output"
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": ""
        },
        {
          "Table 3. Performance comparison of using different models, features and fusion methods.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: We hope that the findings, which have not previously been",
      "data": [
        {
          "Co-attention\n62.9%": "63.4%\nHierarchical co-attention"
        },
        {
          "Co-attention\n62.9%": "their\nlow WER (5.53%).\n[5] concatenated GloVe features"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "obtained from Wit.ai API text transcription with acoustic fea-"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "tures, obtaining a 62.9% Unweighted Accuracy (UA) using an"
        },
        {
          "Co-attention\n62.9%": "LSTM based model.\n[11] jointly trained ASR-SER using log"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "mel-scale ﬁlter bank outputs and decoder outputs from a pre-"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "trained ASR model, and achieved 68.6% WA. [8] used a sin-"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "gle W2V2 as the training model for both ASR and SER rather"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "than separating two tasks, and used 10-fold CV, which usu-"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "ally yields better results than 5-fold;\n[7] carefully ﬁne-tuned"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "the pre-trained ASR model on IEMOCAP before transferring"
        },
        {
          "Co-attention\n62.9%": "the ASR features for SER. Note that since our goal\nis to an-"
        },
        {
          "Co-attention\n62.9%": "alyze potential role of ASR in ASR-SER rather than achieve"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "the SOTA performance, we did not focus on ﬁne-tuning the"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "model and parameters, but we expect that the performance of"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "our model can be further improved by parameter tuning."
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "6. CONCLUSIONS AND FUTURE WORK"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "In this paper, we propose a joint ASR-SER training model that"
        },
        {
          "Co-attention\n62.9%": "incorporates both ASR hidden output and text output using a"
        },
        {
          "Co-attention\n62.9%": "hierarchical co-attention fusion approach. On IEMOCAP,\nit"
        },
        {
          "Co-attention\n62.9%": "achieves 63.4% WA, which is comparable to the ground-truth"
        },
        {
          "Co-attention\n62.9%": "transcript model. By conducting WER analysis, we demon-"
        },
        {
          "Co-attention\n62.9%": "strate situations where ASR can fail on emotional\nspeech."
        },
        {
          "Co-attention\n62.9%": "We hope that\nthe ﬁndings, which have not previously been"
        },
        {
          "Co-attention\n62.9%": "reported can contribute to understanding the ASR-SER rela-"
        },
        {
          "Co-attention\n62.9%": "tionship.\nIn our future work, we will conduct detailed layer-"
        },
        {
          "Co-attention\n62.9%": "wise analysis to understand what meanings the intermediate"
        },
        {
          "Co-attention\n62.9%": "representations represent as the W2V2 layer becomes deeper."
        },
        {
          "Co-attention\n62.9%": "We will also try character-level or hybrid CTC decoders to"
        },
        {
          "Co-attention\n62.9%": "solve the language model mismatch problem."
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "7. ACKNOWLEDGEMENTS"
        },
        {
          "Co-attention\n62.9%": ""
        },
        {
          "Co-attention\n62.9%": "The authors would like to thank Dr.\nRicheng Duan from"
        },
        {
          "Co-attention\n62.9%": "A∗STAR I2R and Han Feng from Huawei\nfor valuable dis-"
        },
        {
          "Co-attention\n62.9%": "cussion."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "to-End Speech Emotion Recognition Combined with"
        },
        {
          "8. REFERENCES": "and Manfred Lang,",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Acoustic-to-Word ASR Model,”\nin INTERSPEECH,"
        },
        {
          "8. REFERENCES": "“Speech emotion recognition combining acoustic fea-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "2020, pp. 501–505."
        },
        {
          "8. REFERENCES": "tures and linguistic information in a hybrid support vec-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[12] Yuanchao Li, Tianyu Zhao, and Tatsuya Kawahara, “Im-"
        },
        {
          "8. REFERENCES": "tor machine-belief network architecture,”\nin Proceed-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "proved End-to-End Speech Emotion Recognition Using"
        },
        {
          "8. REFERENCES": "ings of ICASSP 2004. IEEE, 2004.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Self Attention Mechanism and Multitask Learning,”\nin"
        },
        {
          "8. REFERENCES": "Jilt Sebastian, Piero Pierucci, et al., “Fusion Techniques",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Proceedings of Interspeech 2019, 2019, pp. 2803–2807."
        },
        {
          "8. REFERENCES": "for Utterance-Level Emotion Recognition Combining",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[13] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,"
        },
        {
          "8. REFERENCES": "Speech and Transcripts,” in Proceedings of Interspeech",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "and Michael Auli, “Wav2vec 2.0: A framework for self-"
        },
        {
          "8. REFERENCES": "2019, 2019, pp. 51–55.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "supervised learning of speech representations,”\nin Ad-"
        },
        {
          "8. REFERENCES": "Chi-Chun Lee, Abe",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "vances in Neural Information Processing Systems, 2020,"
        },
        {
          "8. REFERENCES": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "pp. 12449–12460."
        },
        {
          "8. REFERENCES": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[14] Astrid Paeschke and Walter F Sendlmeier,\n“Prosodic"
        },
        {
          "8. REFERENCES": "“IEMOCAP: Interactive emotional dyadic motion cap-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "characteristics of emotional speech: Measurements of"
        },
        {
          "8. REFERENCES": "ture database,”\nLanguage Resources and Evaluation,",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "fundamental\nfrequency movements,”\nin ISCA Tutorial"
        },
        {
          "8. REFERENCES": "vol. 42, no. 4, pp. 335–359, 2008.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "and Research Workshop on Speech and Emotion, 2000."
        },
        {
          "8. REFERENCES": "[4] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[15] Leimin Tian,\nJohanna D Moore,\nand Catherine Lai,"
        },
        {
          "8. REFERENCES": "jeev Khudanpur,\n“Librispeech: an asr corpus based on",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "“Emotion\nrecognition\nin\nspontaneous\nand\nacted\ndia-"
        },
        {
          "8. REFERENCES": "public domain audio books,” in Proceedings of ICASSP",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "logues,”\nin Proceedings of ACII 2015. IEEE, 2015, pp."
        },
        {
          "8. REFERENCES": "2015, 2015, pp. 5206–5210.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "698–704."
        },
        {
          "8. REFERENCES": "[5] Saurabh Sahu, Vikramjit Mitra, Nadee Seneviratne, and",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[16]\nJacob Devlin, Ming-Wei Chang, Kenton\nLee,\nand"
        },
        {
          "8. REFERENCES": "Carol Espy-Wilson, “Multi-Modal Learning for Speech",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Kristina Toutanova,\n“BERT:\nPre-training\nof\ndeep"
        },
        {
          "8. REFERENCES": "Emotion Recognition: An Analysis and Comparison of",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "bidirectional\ntransformers for language understanding,”"
        },
        {
          "8. REFERENCES": "ASR Outputs with Ground Truth Transcription,” in Pro-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Proceedings\nof NAACL-HLT\n2019,\npp.\n4171–4186,"
        },
        {
          "8. REFERENCES": "ceedings of Interspeech 2019, 2019, pp. 3302–3306.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "2019."
        },
        {
          "8. REFERENCES": "A computational model\nfor the auto-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[17]\nJiasen\nLu,\nDhruv Batra,\nDevi\nParikh,\nand\nStefan"
        },
        {
          "8. REFERENCES": "matic recognition of affect in speech, Ph.D. thesis, Mas-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Lee,\n“ViLBERT: Pretraining\ntask-agnostic\nvisiolin-"
        },
        {
          "8. REFERENCES": "sachusetts Institute of Technology, 2004.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "guistic representations\nfor vision-and-language tasks,”"
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "in Advances in neural\ninformation processing systems,"
        },
        {
          "8. REFERENCES": "[7] Sitong Zhou and Homayoon Beigi, “A transfer learning",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "2019."
        },
        {
          "8. REFERENCES": "method for speech emotion recognition from automatic",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "speech recognition,”\narXiv preprint arXiv:2008.02863,",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[18] Leimin Tian, Johanna Moore, and Catherine Lai, “Rec-"
        },
        {
          "8. REFERENCES": "2020.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "ognizing emotions\nin spoken dialogue with hierarchi-"
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "cally fused acoustic and lexical features,”\nin Proceed-"
        },
        {
          "8. REFERENCES": "[8] Xingyu Cai, Jiahong Yuan, Renjie Zheng, Liang Huang,",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "ings of SLT 2016. IEEE, 2016, pp. 565–572."
        },
        {
          "8. REFERENCES": "and Kenneth Church,\n“Speech emotion recognition",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "with multi-task learning,” in Proceedings of Interspeech",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[19] Santiago Pascual, Mirco Ravanelli, Joan Serra, Antonio"
        },
        {
          "8. REFERENCES": "2021, 2021.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Bonafonte,\nand Yoshua Bengio,\n“Learning problem-"
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "agnostic\nspeech\nrepresentations\nfrom multiple\nself-"
        },
        {
          "8. REFERENCES": "[9] Haytham M Fayek, Margaret Lech, and Lawrence Cave-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "supervised tasks,”\nin Proceedings of Interspeech 2019,"
        },
        {
          "8. REFERENCES": "don, “On the Correlation and Transferability of Features",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "2019."
        },
        {
          "8. REFERENCES": "Between Automatic Speech Recognition and Speech",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "Interspeech\nEmotion Recognition,”\nin Proceedings of",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[20] Ankita\nPasad,\nJu-Chieh Chou,\nand Karen Livescu,"
        },
        {
          "8. REFERENCES": "2016, 2016, pp. 3618–3622.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "“Layer-wise analysis of a self-supervised speech repre-"
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "sentation model,” in Proceedings of ASRU 2021, 2021."
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "based features for emotion recognition: A transfer learn-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "[21] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,"
        },
        {
          "8. REFERENCES": "ing approach,” in Proceedings of Grand Challenge and",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "“Multimodal\nspeech emotion recognition using audio"
        },
        {
          "8. REFERENCES": "Workshop on Human Multimodal Language (Challenge-",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "and text,”\nin 2018 IEEE Spoken Language Technology"
        },
        {
          "8. REFERENCES": "HML). 2018, pp. 48–52, Association for Computational",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        },
        {
          "8. REFERENCES": "",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": "Workshop (SLT). IEEE, 2018, pp. 112–118."
        },
        {
          "8. REFERENCES": "Linguistics.",
          "[11] Han Feng, Sei Ueno,\nand Tatsuya Kawahara,\n“End-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2004",
      "venue": "Proceedings of ICASSP 2004"
    },
    {
      "citation_id": "3",
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "authors": [
        "Jilt Sebastian",
        "Piero Pierucci"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech 2019"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proceedings of ICASSP 2015"
    },
    {
      "citation_id": "6",
      "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
      "authors": [
        "Saurabh Sahu",
        "Vikramjit Mitra",
        "Nadee Seneviratne",
        "Carol Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "7",
      "title": "A computational model for the automatic recognition of affect in speech",
      "authors": [
        "Raul Fernandez"
      ],
      "year": "2004",
      "venue": "A computational model for the automatic recognition of affect in speech"
    },
    {
      "citation_id": "8",
      "title": "A transfer learning method for speech emotion recognition from automatic speech recognition",
      "authors": [
        "Sitong Zhou",
        "Homayoon Beigi"
      ],
      "year": "2020",
      "venue": "A transfer learning method for speech emotion recognition from automatic speech recognition",
      "arxiv": "arXiv:2008.02863"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "Xingyu Cai",
        "Jiahong Yuan",
        "Renjie Zheng",
        "Liang Huang",
        "Kenneth Church"
      ],
      "year": "2021",
      "venue": "Proceedings of Interspeech 2021"
    },
    {
      "citation_id": "10",
      "title": "On the Correlation and Transferability of Features Between Automatic Speech Recognition and Speech Emotion Recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2016",
      "venue": "Proceedings of Interspeech 2016"
    },
    {
      "citation_id": "11",
      "title": "ASRbased features for emotion recognition: A transfer learning approach",
      "authors": [
        "Noé Tits",
        "Kevin Haddad",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "12",
      "title": "Endto-End Speech Emotion Recognition Combined with Acoustic-to-Word ASR Model",
      "authors": [
        "Han Feng",
        "Sei Ueno",
        "Tatsuya Kawahara"
      ],
      "year": "2020",
      "venue": "Endto-End Speech Emotion Recognition Combined with Acoustic-to-Word ASR Model"
    },
    {
      "citation_id": "13",
      "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
      "authors": [
        "Yuanchao Li",
        "Tianyu Zhao",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech 2019"
    },
    {
      "citation_id": "14",
      "title": "Wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Prosodic characteristics of emotional speech: Measurements of fundamental frequency movements",
      "authors": [
        "Astrid Paeschke",
        "Walter Sendlmeier"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop on Speech and Emotion"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in spontaneous and acted dialogues",
      "authors": [
        "Leimin Tian",
        "Johanna Moore",
        "Catherine Lai"
      ],
      "year": "2015",
      "venue": "Proceedings of ACII 2015"
    },
    {
      "citation_id": "17",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "18",
      "title": "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Recognizing emotions in spoken dialogue with hierarchically fused acoustic and lexical features",
      "authors": [
        "Leimin Tian",
        "Johanna Moore",
        "Catherine Lai"
      ],
      "year": "2016",
      "venue": "Proceedings of SLT 2016"
    },
    {
      "citation_id": "20",
      "title": "Learning problemagnostic speech representations from multiple selfsupervised tasks",
      "authors": [
        "Santiago Pascual",
        "Mirco Ravanelli",
        "Joan Serra",
        "Antonio Bonafonte",
        "Yoshua Bengio"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech 2019"
    },
    {
      "citation_id": "21",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "Ankita Pasad",
        "Ju-Chieh Chou",
        "Karen Livescu"
      ],
      "year": "2021",
      "venue": "Proceedings of ASRU 2021"
    },
    {
      "citation_id": "22",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    }
  ]
}