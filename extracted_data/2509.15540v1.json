{
  "paper_id": "2509.15540v1",
  "title": "Beyond Words: Enhancing Desire, Emotion, And Sentiment Recognition With Non-Verbal Cues",
  "published": "2025-09-19T02:49:47Z",
  "authors": [
    "Wei Chen",
    "Tongguan Wang",
    "Feiyue Xue",
    "Junkai Li",
    "Hui Liu",
    "Ying Sha"
  ],
  "keywords": [
    "Enhancing Desire",
    "Emotion",
    "and Sentiment Recognition with Non-Verbal Cues Human Desire Understanding",
    "Sentiment Analysis",
    "Emotion Recognition",
    "Multimodal Learning",
    "Multimodal Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Social media platforms such as Twitter and Instagram have become widely used channels for sharing personal experiences and opinions. On these platforms, users post multimodal content including images, text, and audio, making emotion expression and sentiment analysis important topics in natural language processing and computational social science. Over the past decades, research has progressed from text-only emotion and sentiment analysis  [1, 2, 3]  to multimodal approaches  [4, 5, 6] , which have shown wide applicability in dialogue understanding, public opinion monitoring, and human-computer interaction. However, the specific problem of desire understanding, which an underlying driver of human emotion, has received comparatively little dedicated attention.\n\nDesire is a fundamental human intention that reflects a strong wish for certain objects or states, and it constitutes one of the traits distinguishing humans from other animals  [7] . Desire interacts closely with emotion and sentiment. It can shape affective life experiences, and conversely, sentiment and emotional states may modulate the expression or strength of desire. Such three tasks form interconnected and essential components of the human experience, driving our actions and decisions. For example, in Figure  1  (a), a couple smiling and preparing food in a kitchen can be interpreted as expressing a \"romance\" desire, which explains positive sentiment and happy emotion. Figure  1  (b) depicts a man's exaggerated movements to avoid security cameras. It can be understood as fear and negative sentiment driven by a desire for safety and privacy. Therefore, if a machine were capable of accurately inferring such desire intents, it would move research closer to recognizing human emotional intelligence  [8] .\n\nWith the rapid development of multimodal learning and the prevalence of social media data, multimodal sentiment analysis and emotion recognition have attracted increasing attention. However, most existing methods still concentrate on enhancing and modeling verbal cues (e.g., via graph neural networks), such as  [9, 10] , while image-based non-verbal cues are often treated only as features to be extracted and fused superficially. In practice, non-verbal cues play a crucial role. For example, the grimacing expression in Figure  1  (c) could indicate disgust toward broccoli or be part of a playful interaction with family; without image context, text-based inference may be ambiguous. Whereas incorporating image information can lead to a correct interpretation of the emotion, such as disgust.\n\nMotivated by these observations, we propose SyDES, a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment recognition. The framework emphasizes deep utilization of non-verbal visual cues while ensuring that verbal cues remain effectively exploited. Specifically, the input image is processed as both a lowresolution version and a high-resolution version using a shared image encoder. The low-resolution image provides global visual representations for cross-modal alignment. The highresolution image is processed by mixed-scale image strategy to get high-resolution sub-images, and these sub-images are modeled with masked image modeling to encourage the encoder to better learn fine-grained local features. A text-guided image decoder is introduced so that image reconstruction can be guided by textual semantics. Conversely, an image-guided text decoder is employed so that text decoding can incorporate multi-scale visual information. In the meantime, we design a set of loss functions (e.g., local-global semantic similarity loss, cross-modal feature-distribution consistency loss) to allow reconstructed image to maintain modal alignment of fine-grained local visual features and global visual representations, thereby avoid over-reliance on specific regional features. These mechanisms enable mutual guidance and semantic alignment of textual representations, local visual features and global visual features. The fused text outputs are then passed to a lightweight multi-layer perception (MLP) for downstream prediction (see Figure  2  and Section 3 for details).\n\nTo evaluate our approach, extensive experiments are conducted on MSED dataset, the first multimodal dataset that includes benchmarks for desire understanding, emotion recognition, and sentiment analysis. In addition to desire task, evaluations are performed on the emotion and sentiment recognition. Results show that our proposed SyDES outperforms existing methods across all three tasks, with F1-score improve-ments of approximately 0.9% for sentiment analysis, 0.6% for emotion recognition, and 1.1% for desire understanding. These findings demonstrate the effectiveness of a vision-driven modal guidance strategy for intentive representation and generalization. Our contributions can be summarized:\n\n(1) We introduce SyDES, a symmetrical bidirectional multimodal learning framework that enhances the utilization of non-verbal cues while preserving the effectiveness of textual signals, thereby improving desire understanding, emotion recognition, and sentiment analysis.\n\n(2) To reconcile the differing objectives of modal alignment and masked image modeling, we design a set of loss functions that balance these objectives, enabling the model to learn fine-grained visual features while maintain semanticlevel modal consistency.\n\n(3) We provide comprehensive experiments and ablation studies on the MSED dataset, validating the effectiveness and generalization of our proposed SyDES for multimodal desire understanding, emotion recognition, and sentiment analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2. Sentiment Analysis And Emotion Recognition",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2.1.1. Text-Only Sentiment Analysis",
      "text": "Sentiment analysis  [1, 11, 12, 13]  has long been a central research topic in NLP. Early researches often operated in a unimodal setting. For example. Taboada et al.  [14]  proposed a lexicon-based method called the Semantic Orientation CALculator, which uses a lexicon annotated with semantic orientation and incorporates negation handling to determine polarity. Pang et al.  [11]  and Gamallo et al.  [15]  applied classical machine learning techniques for text sentiment classification, including support vector machine (SVM) and Naïve Bayes. Kim et al.  [1]  was among the first to apply convolutional neural networks to text classification. Tai et al.  [12]  considered syntactic structures of natural language and proposed the tree-structured long short-term memory (Tree-LSTM) to model hierarchical dependencies. Yang et al.  [2]  designed a Hierarchical Attention Network (HAN) for document-level sentiment classification, which introduces the attention mechanism to assist networks in selecting important words and sentences. Similarity, Wang et al.  [16]  integrated attention into LSTM to strengthen aspect-level sentiment associations within sentences. More recently, Singh et al.  [13]  applied BERT to sentiment analysis on COVID-related tweets. These approaches predominantly rely on unimodal signals. In real-world social media scenarios, however, text is often accompanied by images; relying solely on text is therefore insufficient because multimodal cues typically provide complementary information to facilitate more accurate prediction.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Sentiment Analysis And Emotion Recognition",
      "text": "Multimodal sentiment classification  [17, 18, 19]  has attracted increasing attention as researchers seek to jointly analyze sentiment from multiple modalities. You et al.  [17]  introduced cross-modal consistency regression (CCR) to jointly classify sentiment using image and text features. Xu et al.  [4]  leveraged scene-level visual cues and attention mechanisms to identify important words and sentences. Hu et al.  [20]  focused on user's latent emotion states. From a modality-interaction perspective, Xu et al.  [18]  explored iterative relations between image and text. Huang et al.  [19]  used hybrid fusion strategies to enable between unimodal and cross-modal representations. Li et al.  [21]  applied contrastive learning and data augmentation to align and fuse the token-level features of text and image. Meanwhile, multimodal emotion recognition  [22, 23, 24] , which targets finer-gained affective states than coarse sentiment, has also been widely studied. To capture reader's emotional reactions to new articles, Guo et al.  [25]  introduced two multimodal news datasets and proposed a layout-driven network. Nemaiti et al.  [26]  proposed a hybrid method for latent-space data fusion. Zhang et al.  [23]  used manifold learning and deep convolutional networks to extract low-dimensional emotion-relevant features. Xu et al.  [27]  adopted image captions as semantic cues for multimodal prediction. Yang et al.  [5]  updated imagetext features within a memory network and introduced a multiview attention network for multimodal integration. Despite these advances, most of the existing approaches simply leveraged holistic or local features extracted from different modalities to predict multimodal sentiments, which leads to suboptimal performance.\n\nThe emergence of graph neural networks (GNNs)  [28, 29]  has opened new directions for mining relationships among verbal cues. Yang et al.  [9]  observed that emotional expression exhibits specific global features and introduced a multi-channel GNN to model global attributes, aiming to mine commonalities in language signals. Similarly, Zhang et al.  [30]  proposed a multi-task interactive graph-attention network with local-global context connection module to model contextual relationships. Wang et al.  [31]  enriched textual representations by leveraging contextual world knowledge from large multimodal models. However, these approaches predominantly focus on mining ver-bal cues and often underutilize the rich information contained in non-verbal cues.\n\nMotived by the importance of non-verbal cues, we hope to attain fine-gained features from image contextual information. Masked image modeling paradigm introduced by He et al.  [32]  has shown that an encoder can be encouraged, via a reconstruction-based self-supervised objective, to learn richer local representations. This insight motivates our proposal of a symmetrical bidirectional multimodal learning framework to ensure more effective exploitation of image-based non-verbal cues for desire understanding, emotion recognition, and sentiment analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Desire Understanding",
      "text": "Multimodal desire understanding concerns recognizing desires or intentions expressed in both textual and visual expression, and it remains an underexplored problem. Existing automated analyses of desire largely originate from psychology and philosophy. Lim et al.  [33]  developed a desire-understanding system based on four emotional states in audio and gestural cues. Cacioppo et al.  [34]  designed a multi-level kernel-density fMRI analysis to investigate differences and correlations between sexual desire and love. Schutte et al.  [35]  conducted a meta-analysis on 2,692 participants to examine links between curiosity and creativity. Hoppe et al.  [36]  estimated different levels of curiosity using eye-movement data and SVM. Yavuz et al.  [37]  proposed a data-mining approach for desire and intent using neural networks and Bayesian networks. Chauhan et al.  [38]  presented a multi-task multimodal deep attentive framework for offense, motivation, and sentiment analysis. Nevertheless, these researches commonly lack support for large-scale multimodal social media data and often do not fully exploit both visual and textual channels.\n\nTo address these limitations, Jia et al.  [39]  introduced MSED dataset, the first multimodal dataset for desire understanding. MSED collects image-text pairs from social media and defines three sub-tasks, including desire understanding, emotion recognition, and sentiment analysis. They also provide various strong baselines based on different combinations of feature representations using various image and text encoders. Aziz et al.  [40]  attempted to combine two multimodal models into a unified architecture (MMTF-DES) to achieve ensemble-like performance and reported improved results on desire understanding. However, this strategy increases training and inference cost. Motivated by the need to fully exploit image-based non-verbal cues while maintaining computational efficiency, we propose a symmetrical bidirectional multimodal learning framework, offering a new method for multimodal desire understanding.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Sydes",
      "text": "Figure  2  illustrates overall architecture of SyDES. In this section, we first introduce the mixed-scale image strategy and motivate its necessity. We then describe in detail each component of the architecture, including the image encoder, text encoder, text-guided image decoder, and image-guided text decoder. Next, we present the loss functions used during training and explain how they support downstream performance. Finally, we discuss the two-stage training paradigm and its rationale.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mixed-Scale Image Strategy",
      "text": "High-resolution images can capture richer region-level features and local details. However, many multimodal models conventionally operate on 224×224 images, which constrains their capacity for fine-grained visual perception. Directly increasing the encoder input resolution (e.g., processing all inputs at 448 × 448) dramatically increases computational cost.\n\nTo balance fine-grained feature extraction and computational efficiency, we adopt a mixed-scale sub-images strategy to get detailed high-resolution visual features. Given a highresolution image I i from batch size N, e.g., 448 × 448, we produce one downsampled low-resolution image and four highresolution sub-images, resulting in five 224 × 224 images per original image. This provides four sets of fine-grained local features and one global representation while controlling computational overhead. Formally:\n\nwhere DownSample may use bilinear interpolation, and Crop n denotes corner or predefined cropping. Each 224×224 image is treated as an independent input to the image encoder to obtain global and local representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "Our proposed SyDES comprises four modules: an image encoder, a text encoder, a text-guided image decoder, and an image-guided text decoder, together with a lightweight MLP for downstream prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Image Encoder",
      "text": "We employ a vision transformer  [41]  as the image encoder in order to extract global representation from low-resolution images and capture local details from high-resolution sub-images. We attain I (1)   i and I (2,n)   i after the mixed-scale image strategy. Each 224 × 224 image is split into P patches and embedded as:\n\nwhere C 1 denotes the image embedding dimension and v cls is the CLS token. For high-resolution sub-images I (2,n) i , we apply a binary mask vector m i,n ∈ {0, 1} P with m i,n [P] = 1 indicating that patch p is masked. Following the Masked Auto-Encoder (MAE)  [32] , we set the mask ratio to m ∈ [0, 1] and set the keep ratio to r = 1m. The set of unmasked indices is\n\nSelecting the unmasked tokens yields:\n\nBoth P (1)  emb,i and P (2,n)  emb,i are fed into the shared image encoder to produce 1 , where the first row of V (1)   i corresponds to the low-resolution CLS visual feature",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Text Encoder",
      "text": "As the text encoder, we adopt a casual masked transformer encoder to model text inputs. Text inputs are tokenized by BPE into an embedding sequence of length S represented as W emb,i . The sequence is encoded by a causal masked transformer to avoid future information leakage and attain encoding features:\n\nthe CLS text feature and C 2 denotes the text embedding dimension.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text-Guided Image Decoder",
      "text": "To recover masked patches from P (2,n)  emb,i and make reconstruction aware of textual semantics, we adopt a text-guided imaged decoder. Specifically, we utilize cross-attention transformer modules to deeply fuse image and text information. We project V (2,n)   i to the text embedding dimension C 2 , and introduce mP shared and learnable mask tokens P mask ∈ R mP×C 2 . The decoder input is:\n\nTo achieve stable modal fusion, a gate-based fusion mechanism is used to combine intermediate image projections and text features:\n\nAs utilizing D in i,n as the query and\n\nas the key and value, we leverage textual semantic information to compel the masked image reconstruction process to perceive verbal cues, thereby facilitating the generation of reconstructed images with crossmodal characteristics. The image decoder predicts the pixel values of masked image patches and a sub-image-level global representation:\n\nwhere X p i,n denotes the predicted pixel value for the p-th masked patch in the n-th sub-image of the i-th high-resolution image, M i,n represents the set of masked indices in the n-th sub-image of the i-th high-resolution image, and ImgDec refers to the textguided image decoder. Furthermore, the decoder outputs a set of sub-image-level global representation z i,n ∈ R n×C 2 , which is used for subsequent aggregation and similarity loss calculation to better maintain modal alignment between fine-grained local features and global visual representations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Image-Guided Text Decoder",
      "text": "Similarity, to enable the text decoder to incorporate multiscale visual cues, we adopt an image-guided text decoder. More specifically, we use non-CLS text tokens as the query, and the concatenation of V (1)  i and V (2,n)   i as the key and value. This enables the fusion of textual and multi-scale visual information from low-resolution images and high-resolution sub-images. Ultimately, we obtain a multimodal feature W i ∈ R (S -1)×C 2 , which is fed into a lightweight MLP to produce the final prediction y i . The overall decoding strategy can be formally expressed as:\n\nwhere [•] denotes the concatenation operation, and TextDec refers to the image-guided text decoder.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Summary",
      "text": "Low-resolution images provide global visual representation, masked image modeling encourages learning of fine-grained local features from high-resolution sub-images. The textguided image decoder enforces text-aware reconstruction, and the image-guided text decoder enables the text modality to assimilate multi-scale visual cues. This symmetrical bidirectional fusion thus exploits non-verbal visual cues at both local and global representations while preserving effective use of verbal cues, yielding mutual guidance and semantic alignment between different modalities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Loss Functions",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reconstruction Loss",
      "text": "We need to calculate error between predicted pixel values and real pixel values when utilizing masked image modeling. We minimize mean square error over in the pixel space between the masked image tokens in the high-resolution sub-images and the reconstructed tokens, and use it as the reconstruction loss to optimize fine-grained feature extraction capability of the image encoder:\n\nwhere X p i,n is the RGB pixel value of the high-resolution subimages, and X p i,n is the predicted pixel value. |M i,n | denotes the number of masked tokens in the n-th sub-image of the i-th highresolution sub-images, and ∥ • ∥ 2 refers to the L 2 loss.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Image-Text Contrastive Loss",
      "text": "To facilitate the alignment of image and text modalities in a shared semantic space, we introduce an image-text contrastive loss. Given the normalized global visual representation v cls i from the low-resolution image and the global text representation w cls i , the loss is defined to align the image and text representation:\n\nwhere ⟨•, •⟩ denotes the inner product, and τ is a temperature hyperparameter. This loss encourages matched image-text pairs to cluster together in the semantic space while pushing unmatched pairs apart, thereby enhancing cross-modal consistency.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Local-Global Semantic Similarity Loss",
      "text": "Although using high-resolution sub-images as input in masked image modeling helps extract fine-grained local features, it may also cause the reconstructed tokens to deviate from the global visual semantic information from low-resolution images, resulting in over rely local information. To enforce consistency between the reconstructed local features and the global visual representations, we perform learnable weighted aggregation on the sub-image-level global representation {z i,n } 4 n=1 to obtain:\n\nare learnable parameters, and the MLP projects the aggregated feature into same semantic space as v cls i . After normalizing to all features, the similarity loss is computed as:\n\nThis loss ensures that the reconstructed local pixel features from high-resolution sub-images remain consistent with the global image semantics from low-resolution images, achieving a balance between local and global features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Modal Feature-Distribution Consistency Loss",
      "text": "There exists a conflict between the objectives of the contrastive loss (semantic-level) and the reconstruction loss (pixellevel). The former emphasizes cross-modal semantic alignment, while the latter focuses on pixel-level reconstruction quality without explicitly enforcing semantic consistency across modalities. To mitigate this, we constrain the semantic distribution consistency between the text to reconstructed image features and the text to low-resolution image features. Specifically, the similarity distribution between the aggregated reconstructed image feature P agg,i and the global text representation w cls i is defined as:\n\nSimilarly, the similarity distribution S (v cls i , w cls i ) is defined for the low-resolution image feature and the global text representation. By minimizing the KL divergence between the two distributions and adding an entropy regularization term for robustness, we obtain the following loss:\n\nwhere KL denotes the relative entropy loss function, H represents the entropy function.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Classification Loss",
      "text": "For downstream tasks, such as desire understanding, emotion recognition, and sentiment analysis, we use the standard crossentropy loss:\n\nwhere y is the ground-truth label and y i is the predicted label.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Two-Stage Training Strategy",
      "text": "To balance semantic alignment with pixel-level reconstruction, we adopt a two-stage training strategy and selectively freeze or unfreeze model components in each stage to steer learning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Training Stage",
      "text": "The pre-training stage focuses on masked image modeling, giving priority to training the image encoder and the text-guided image decoder. In this stage, we freeze the image-guided text decoder and the MLP, and only update the text encoder, the image encoder, and the text-guided image decoder. This design allows us to extract fine-grained details from high-resolution sub-images while constraining modal consistency at the semantic level. As discussed in Section 3.3.3 and Section 3.3.4, we also consider the sub-image-level global visual representation of reconstructed image tokens and their cross-modal alignment. Therefore, the overall loss used in this stage is:\n\nwhere λ rec , λ si , λ dc and λ itc are hyperparameter weights that balance the contributions of each loss term.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Tuning Stage",
      "text": "The fine-tuning stage targets downstream tasks such as desire understanding. Its goal is to train the image-guided text decoder to fully leverage both local and global features produced by the image encoder, and to use an MLP to map the fused multimodal representation to task-specific outputs. During fine-tuning we freeze the image encoder and text-guided image decoder, and train the text encoder, the image-guided text decoder, and the MLP. The overall loss for this stage is:\n\nwhere λ cls and λ itc are hyperparameter weights. Retaining the ITC term helps preserve cross-modal alignment stability during fine-tuning. To validate our method, we evaluate the model on the publicly available multimodal dataset MSED  [39] . MSED is the first multimodal, multi-task dataset for sentiment analysis, emotion recognition, and desire understanding and contains 9, 190 image-text pairs annotated in English. The samples are collected from social media platforms, including Twitter, Getty Image, and Flickr. The dataset defines three downstream tasks: sentiment analysis, emotion recognition, and desire understanding. Each multimodal sample is manually labeled with sentiment class (positive, neutral, negative), an emotion class (happiness, sad, neutral, disgust, anger, and fear), and a desire class (family, romance, vengeance, curiosity, tranquility, socialcontact, and none). The data are split into train, validation, and test sets with ratios of 70%, 10%, and 20%, yielding 6, 127 training instances, 1, 021 validation instances, and 2, 024 test instances. The detailed statistics of the MSED dataset are listed in Table  1 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "All three downstream tasks are classification problems. We therefore report standard classification metrics: Precision (P), Recall (R), Macro-F1-score (F1), and Weighted Accuracy (Acc).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Details",
      "text": "All experiments run on NVIDIA V100 with CUDA 11.0 and PyTorch 2.1.2  [42] . In pre-training stage, we initialize the image encoder and the text encoder from CLIP  [43]  pre-training weights provided by OpenAI 1 , and initialize the text-guided image decoder from pre-training weights 2 in  [44] . We use the AdamW  [45]  optimizer. The initial learning rates for the image encoder, the text encoder, and text-guided image decoder are set to 5e -6, 5e -5, and 1e -4, respectively. Weight decay is 0.01. The learning rate follows a cosine decay schedule with a 15% warmup period. We train for 50 epochs with batch size 64. Loss weights are set to λ rec = 1, λ si = 0.5, λ dc = 0.025, and λ itc = 0.5. The masking ratio m is 0.75 during pre-training.\n\nDuring fine-tuning stage, we keep the overall setup and finetune the pretrained model separately for desire understanding, emotion recognition, and sentiment analysis. Specifically, the initial learning rates for the text encoder, image-guided text decoder, and MLP are 1e -4, 2e -4, and 1e -4, respectively, with a 10% warmup. Fine-tuning uses batch size 64 for 50 epochs. Loss weights are λ cls = 1 and λ itc = 0.4. In fine-tuning, images are no longer masked, we feed the full low-resolution image along with high-resolution sub-images. The masking ratio m is thus set to 0.00. 1 https://github.com/openai/CLIP 2 https://huggingface.co/laion/mscoco_finetuned_ CoCa-ViT-L-14-laion2B-s13B-b90k",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline",
      "text": "To facilitate subsequent ablation analysis and comparison, we use the SyDES architecture trained directly with classification loss without pre-training stage. We denote this variant as SyDES-B (the SyDES baseline).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With Different Modality Baseline Models",
      "text": "We compared different modality baseline models on three downstream tasks, including desire understanding, emotion recognition, and sentiment analysis, to evaluate the effectiveness of our proposed SyDES. For the textual modality, we employed BERTweet  [46]  model as the baseline modal since text data are annotated from social media platforms. For the image modality, we used the classic backbone network ResNet  [47] . The multimodal baseline model, SyDES-B, was included to demonstrate the advantage of multimodal fusion. We also present the results of our proposed SyDES method.\n\nThe experiment results for different modality baseline models are shown in Table  2 . In sentiment analysis, BERTweet achieved an F1-score of 82.49%, ResNet 70.64%, SyDES-B 86.50%, and SyDES 89.19%. For emotion recognition, the F1-scores were 78.34% for BERTweet, 56.40% for ResNet, 80.80% for SyDES-B, and 84.74% for SyDES. In desire understanding, BERTweet scored 78.86%, ResNet 49.20%, SyDES-B 72.27%, and SyDES reached 84.02%.\n\nAnalysis of these results leads to three main conclusions: (1) Multimodal models consistently outperform unimodal models across mostly tasks. For instance, in emotion recognition, SyDES-B improved the F1-score by 3.14% gains over BERTweet, indicating the benefit of leveraging multiple modalities in sentiment-related tasks. (  2 ) The unimodal image model (e.g., ResNet) consistently underperformed compared to the unimodal text model (e.g., BERTweet), suggesting limitations in capturing fine-grained visual semantics and underutilization of non-verbal cues. This further highlights the necessity of our proposed method. (3) Our proposed SyDES consistently surpassed SyDES-B in all tasks, validating the effectiveness of the non-verbal cues mining mechanism introduced during pretraining.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "We propose SyDES, a symmetrical bidirectional multimodal learning framework that leverages masked image modeling The comparative performance of our proposed SyDES on test data against other SOTA methods across all tasks is presented in Table  3 . The results indicate that SyDES achieves competitive performance across all three tasks. In terms of the primary metric F1-score, our proposed SyDES achieves 84.02% in desire understanding, 84.74% in emotion recognition, and 89.19% in sentiment analysis, surpassing the previous best model, MMTF-DES  [40] , by 1.1%, 0.6%, and 0.9% gains, respectively. It is worth noting that MMTF-DES relies on integrating multiple multimodal Transformer encoders (e.g., ViLT and VAuLT), which entails considerably higher training costs. In contrast, SyDES extracts non-verbal cues from images effectively while maintaining lower computational overhead. The experimental results validate this trade-off: the improvement on the desire understanding task, which depends more heavily on non-verbal cues, is particularly pronounced in terms of the F1-score.\n\nWe further observe that sentiment analysis generally yields higher performance than for the emotion recognition and desire understanding tasks. A possible reason for this difference in performance may be attributed to the nature of the three tasks. Sentiment analysis aim to identify the overall emotion or opinion expressed in an image-text pair. In contrast, desire understanding and emotion recognition require fine-grained detection of specific signals such as a person's gestures or facial expressions that are inherently embedded in images. For example, there exists the exaggerated motion and frightened expression of the man in Figure  1 (b) . These subtle cues are inherently more challenging to capture. Therefore, sentiment analysis may be an easier and more straightforward task for model, while desire understanding and emotion recognition may be more complicated and nuanced. Our proposed SyDES enhances local detail extraction, resulting in particularly notable gains in desire understanding, but its performance remains slightly below that of sentiment analysis. This suggests there is still room for improvement and underscores the need for further research into desire understanding and emotion recognition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "4. Ablation Studies",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "4.4.1. Loss Functions",
      "text": "We adopt a two-stage training strategy to facilitate crossmodal fusion between textual features and both local and global visual features. As summarized in Table  4 , we systematically evaluate the impact of different loss combinations on the three sub-tasks. The first row corresponds to the model fin-tuned directly without pre-training stage (i.e., SyDES-B as described in Section 4.1.4). During pre-training, four loss functions are used, including L rec , L si , L dc , and L itc . Results show that using only L rec for masked image modeling leads to performance even worse than SyDES-B. This indicates that reconstructing high-resolution sub-images alone may introduce a semantic mismatch with the low-resolution image due to inadequate modal alignment, resulting in modal inconsistency that harms fine-tuning. Gradually incorporating L itc , L si , and L dc consistently improves performance across all tasks. For example, in desire understanding, the F1-score increases to 81.44% with L itc , to 82.24% with L si , and finally to 84.02% with L dc . This suggests that cross-modal alignment and semantic/consistency constraints are critical to bridging the gap between reconstructed sub-images and the global image semantics.\n\nDuring fine-tuning, two loss functions are used, including L itc and L cls . We observe that that L itc is important to preserve pre-training gains. Removing L itc during fine-tuning causes the F1-scores to drop from 84.02%, 84.74%, and 89.19% to 77.09%, 83.21%, and 86.16%, respectively. In conclusion, using all proposed loss functions yields the best performance across desire understanding, emotion recognition, and sentiment analysis, confirming the complementary effects of the loss terms.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ratio Of Masked Tokens",
      "text": "As shown in Table  5 , we investigate the impact by setting different ratios of masked tokens. Specifically, 0.25, 0.50, 0.75, and 0.90 are tested. Experimental results demonstrate that a masking ratio of 75% yields the best average performance across most tasks. Although a ratio of 25% achieves slightly better results in emotion recognition, the 75% ratio is overall   more suitable considering its substantially lower computational cost while maintain competitive performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Performance Analysis",
      "text": "To better understand the advantage of our proposed SyDES, we selected representative examples from the MSED dataset across three sub-tasks. These examples are misclassified by SyDES-B but correctly classified by SyDES. As illustrated in Figure  3 , we compare the predictions of SyDES-B and our proposed SyDES, including example stimuli, ground-truth labels, and predicted probabilities. Across all three tasks, our proposed SyDES produces correct predictions with high confidence. For instance, in Example 1 of the sentiment analysis task, the image caption is \"Brother and sister exploring with flashlight.\", it provides limited information. But our proposed SyDES achieved a high confidence score of 96.21% by effectively leveraging visual cues. This evidence indicates that, compared with models that rely mainly on transfer learning with pre-trained weights (e.g., SyDES-B), the non-verbal cues mining and symmetric bidirectional multimodal learning mechanism introduced in the pre-training stage of our proposed SyDES are effective. It substantially improves robustness and accuracy, especially when textual information is limited.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cross-Modal Attention Heatmap Analysis",
      "text": "To intuitively illustrate the perception ability of our proposed SyDES toward textual and image modalities, we visualized the attention localization map for the last layer in the image encoder, and compared them with those of SyDES-B. As depicted in Figure  4 , our proposed SyDES focuses more accurately on fine-grained visual regions corresponding to informative words in the text. For example, in the case of Image 1, SyDES shows concentrated attention on regions related to \"boy\" and \"bike\", whereas SyDES-B neglects these essential details. Similarity, in the case of Image 3, SyDES-B perceives the concept of \"family\" vaguely, while our proposed SyDES clearly identifies the five-person \"family\" and the \"beach\" scene. This improved perception can be attributed to two mechanisms: (1) the masked image modeling on high-resolution sub-images, which enhances local detail sensitivity; (2) the text-guided crossmodal reconstruction, which strengthens cross-modal semantic alignment. These results further indicate that our proposed SyDES effectively exploits non-verbal visual cues.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Reconstructed Image Visualization",
      "text": "We reconstructed and stitched the high-resolution subimages, and visualized the masked image patches to inspect the practical effect in Figure  5 . Each sample consists of the original image, the randomly masked image, and the reconstructed image by the text-guided image decoder. The results show that, even with complex image content (e.g., people, gesture, lighting, and natural scenes in the example 1), our proposed SyDES can reconstruct contextual content reasonably well, owing to masked image modeling using high-resolution sub-images and  text-guided decoding. However, we also observe limitations in reconstructing fine facial details. This is likely because faces occupy a small fraction of the overall image and contain intricate features (e.g., eyes, nose, mouth) that require more finegrained reconstruction methods. The local multi-scale reconstruction proposed in  [48]  appears promising and warrants further exploration to address these limitations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "Although we thoroughly explore non-verbal cues in images through a masked image modeling and our proposed method demonstrated effectiveness on the MSED dataset, several limitations remain:\n\n(1) Capturing fine-grained local features is still insufficient. As shown in Section 4.5.2, reconstructions of small but semantically important region such as faces are limited, indicating that there is still room for improvement in perceiving and reconstructing fine-grained regions.\n\n(2) The exploitation of textual information could be further enhanced. Textual descriptions often obtain abundant aspectlevel expressions (e.g., opinion words). Effectively leveraging such terms, integrating and aligning them with visual information represents a promising direction for future research.\n\n(3) The current method focuses only on image-text pairs. Real-world multimodal data involve additional modalities (e.g., video and audio), which provide richer non-verbal cues. Extending our method to incorporate more modalities while balancing computational cost and performance constitutes an important direction for further investigation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "To address the under-exploration of desire understanding and of non-verbal cues in multimodal sentiment analysis, we propose a symmetric bidirectional multimodal learning framework for desire, emotion, and sentiment recognition. The framework employs a masked image modeling to boosting extract fine-grained local features from high-resolution sub-images and capturing global visual representations from low-resolution images. More, we introduce a text-guided image decoder and an image-guided text decoder so that facilitate semantic alignment and modal fusion from local and global features during decoding, thereby enhancing the discriminative power of multimodal representations. Extensive experiments on the MSED dataset across three sub-tasks, demonstrate the effectiveness of our proposed method. We expect this work to offer insights for multimodal desire understanding and to inspire further research in this field.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), a couple smiling and preparing food in",
      "page": 1
    },
    {
      "caption": "Figure 1: (b) depicts a man’s exaggerated movements to avoid security",
      "page": 1
    },
    {
      "caption": "Figure 1: (c) could indicate disgust",
      "page": 1
    },
    {
      "caption": "Figure 1: Examples of multimodal desire, emotion, and sentiment.",
      "page": 2
    },
    {
      "caption": "Figure 2: and Section 3 for details).",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates overall architecture of SyDES. In this",
      "page": 3
    },
    {
      "caption": "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-",
      "page": 4
    },
    {
      "caption": "Figure 1: (b). These subtle cues are inherently",
      "page": 8
    },
    {
      "caption": "Figure 3: Performance analysis of our proposed SyDES on desire understand-",
      "page": 9
    },
    {
      "caption": "Figure 3: , we compare the predictions of SyDES-B and our pro-",
      "page": 9
    },
    {
      "caption": "Figure 4: , our proposed SyDES focuses more accurately on",
      "page": 9
    },
    {
      "caption": "Figure 5: Each sample consists of the origi-",
      "page": 9
    },
    {
      "caption": "Figure 4: Qualitative analysis of attention localization map in our proposed SyDES. We visualize the attention localization map from the last layer in the image",
      "page": 10
    },
    {
      "caption": "Figure 5: Visualization of reconstructed images using our proposed SyDES.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Abstract"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Desire, as an intention that drives human behavior,\nis closely related to both emotion and sentiment. Multimodal\nlearning has"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as com-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "plementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "capture intention-related representations in the image.\nSpecifically,\nlow-resolution images are used to obtain global visual\nrep-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "resentations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information."
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Additionally,\nto balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition,"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES."
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Keywords: Human Desire Understanding, Sentiment Analysis, Emotion Recognition, Multimodal Learning, Multimodal Fusion"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "1.\nIntroduction\nstates may modulate the expression or strength of desire. Such"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "three tasks form interconnected and essential components of the"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Social media platforms such as Twitter and Instagram have"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "human experience, driving our actions and decisions. For ex-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "become widely used channels for sharing personal experiences"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "ample,\nin Figure 1 (a), a couple smiling and preparing food in"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "and opinions. On these platforms, users post multimodal con-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "a kitchen can be interpreted as expressing a \"romance\" desire,"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "tent including images, text, and audio, making emotion expres-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "which explains positive sentiment and happy emotion. Figure 1"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "sion and sentiment analysis important topics in natural language"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "(b) depicts a man’s exaggerated movements to avoid security"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "processing and computational\nsocial\nscience.\nOver\nthe past"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "cameras.\nIt can be understood as fear and negative sentiment"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "decades,\nresearch has progressed from text-only emotion and"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "driven by a desire for safety and privacy. Therefore,\nif a ma-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "sentiment analysis [1, 2, 3] to multimodal approaches [4, 5, 6],"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "chine were capable of accurately inferring such desire intents,"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "which have shown wide applicability in dialogue understand-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "it would move research closer to recognizing human emotional"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "ing, public opinion monitoring, and human-computer\ninterac-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "intelligence [8]."
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "tion. However,\nthe specific problem of desire understanding,"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "which an underlying driver of human emotion, has\nreceived\nWith the rapid development of multimodal\nlearning and the"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "comparatively little dedicated attention.\nprevalence of social media data, multimodal sentiment analy-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Desire is a fundamental human intention that reflects a strong\nsis and emotion recognition have attracted increasing attention."
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "wish for certain objects or states, and it constitutes one of the\nHowever, most existing methods still concentrate on enhanc-"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "traits distinguishing humans from other animals [7]. Desire in-\ning and modeling verbal cues (e.g., via graph neural networks),"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "teracts closely with emotion and sentiment.\nIt can shape affec-\nsuch as [9, 10], while image-based non-verbal cues are often"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "tive life experiences, and conversely, sentiment and emotional\ntreated only as features to be extracted and fused superficially."
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "In practice, non-verbal cues play a crucial role. For example,"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "the grimacing expression in Figure 1 (c) could indicate disgust"
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "∗Corresponding author."
        },
        {
          "dCollege of Informatics, Huazhong Agricultural University, No.1 Shizishan Street, Hongshan District, Wuhan, 430070, Hubei, China": "Email address: shaying@mail.hzau.edu.cn (Ying Sha)\ntoward broccoli or be part of a playful\ninteraction with family;"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "guidance strategy for\nintentive representation and generaliza-"
        },
        {
          "interpretation of the emotion, such as disgust.": "a\nMotivated\nby\nthese\nobservations, we\npropose SyDES,",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "tion. Our contributions can be summarized:"
        },
        {
          "interpretation of the emotion, such as disgust.": "Symmetrical Bidirectional Multimodal Learning Framework",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "for Desire, Emotion, and Sentiment recognition.\nThe frame-",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "(1) We introduce SyDES, a symmetrical bidirectional multi-"
        },
        {
          "interpretation of the emotion, such as disgust.": "work emphasizes deep utilization of non-verbal visual\ncues",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "modal\nlearning framework that enhances the utilization of"
        },
        {
          "interpretation of the emotion, such as disgust.": "while ensuring that verbal cues\nremain effectively exploited.",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "non-verbal cues while preserving the effectiveness of tex-"
        },
        {
          "interpretation of the emotion, such as disgust.": "Specifically,\nthe\ninput\nimage\nis\nprocessed\nas\nboth\na\nlow-",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "tual signals, thereby improving desire understanding, emo-"
        },
        {
          "interpretation of the emotion, such as disgust.": "resolution version and a high-resolution version using a shared",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "tion recognition, and sentiment analysis."
        },
        {
          "interpretation of the emotion, such as disgust.": "image encoder. The low-resolution image provides global vi-",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "sual\nrepresentations\nfor\ncross-modal\nalignment.\nThe\nhigh-",
          "findings demonstrate the effectiveness of a vision-driven modal": "(2) To reconcile the differing objectives of modal alignment"
        },
        {
          "interpretation of the emotion, such as disgust.": "resolution image is processed by mixed-scale image strategy",
          "findings demonstrate the effectiveness of a vision-driven modal": "and masked image modeling, we design a set of loss func-"
        },
        {
          "interpretation of the emotion, such as disgust.": "to get high-resolution sub-images,\nand these sub-images are",
          "findings demonstrate the effectiveness of a vision-driven modal": "tions that balance these objectives, enabling the model\nto"
        },
        {
          "interpretation of the emotion, such as disgust.": "modeled with masked image modeling to encourage the en-",
          "findings demonstrate the effectiveness of a vision-driven modal": "learn fine-grained visual features while maintain semantic-"
        },
        {
          "interpretation of the emotion, such as disgust.": "coder to better learn fine-grained local features. A text-guided",
          "findings demonstrate the effectiveness of a vision-driven modal": "level modal consistency."
        },
        {
          "interpretation of the emotion, such as disgust.": "image decoder\nis introduced so that\nimage reconstruction can",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "(3) We provide comprehensive experiments and ablation stud-"
        },
        {
          "interpretation of the emotion, such as disgust.": "be guided by textual semantics. Conversely, an image-guided",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "ies on the MSED dataset, validating the effectiveness and"
        },
        {
          "interpretation of the emotion, such as disgust.": "text decoder is employed so that text decoding can incorporate",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "generalization of our proposed SyDES for multimodal de-"
        },
        {
          "interpretation of the emotion, such as disgust.": "multi-scale visual\ninformation.\nIn the meantime, we design a",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "sire\nunderstanding,\nemotion\nrecognition,\nand\nsentiment"
        },
        {
          "interpretation of the emotion, such as disgust.": "set of loss functions (e.g., local-global semantic similarity loss,",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "",
          "findings demonstrate the effectiveness of a vision-driven modal": "analysis."
        },
        {
          "interpretation of the emotion, such as disgust.": "cross-modal feature-distribution consistency loss) to allow re-",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        },
        {
          "interpretation of the emotion, such as disgust.": "constructed image to maintain modal alignment of fine-grained",
          "findings demonstrate the effectiveness of a vision-driven modal": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "ments of approximately 0.9% for sentiment analysis, 0.6% for"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "emotion recognition, and 1.1% for desire understanding. These"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "findings demonstrate the effectiveness of a vision-driven modal"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "guidance strategy for\nintentive representation and generaliza-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "tion. Our contributions can be summarized:"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "(1) We introduce SyDES, a symmetrical bidirectional multi-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "modal\nlearning framework that enhances the utilization of"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "non-verbal cues while preserving the effectiveness of tex-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "tual signals, thereby improving desire understanding, emo-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "tion recognition, and sentiment analysis."
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "(2) To reconcile the differing objectives of modal alignment"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "and masked image modeling, we design a set of loss func-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "tions that balance these objectives, enabling the model\nto"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "learn fine-grained visual features while maintain semantic-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "level modal consistency."
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "(3) We provide comprehensive experiments and ablation stud-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "ies on the MSED dataset, validating the effectiveness and"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "generalization of our proposed SyDES for multimodal de-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "sire\nunderstanding,\nemotion\nrecognition,\nand\nsentiment"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "analysis."
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "2. Related Work"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "2.1.\nSentiment Analysis and Emotion Recognition"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "2.1.1. Text-only sentiment analysis"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "Sentiment analysis [1, 11, 12, 13] has long been a central"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "research topic in NLP. Early researches often operated in a uni-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": ""
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "modal setting.\nFor example.\nTaboada et al.\n[14] proposed a"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "lexicon-based method called the Semantic Orientation CALcu-"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "lator, which uses a lexicon annotated with semantic orientation"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "and incorporates negation handling to determine polarity. Pang"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "et al.\n[11] and Gamallo et al.\n[15] applied classical machine"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "learning techniques for text sentiment classification,\nincluding"
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "support vector machine (SVM) and Naïve Bayes. Kim et al."
        },
        {
          "Figure 1: Examples of multimodal desire, emotion, and sentiment.": "[1] was among the first to apply convolutional neural networks"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "tures of natural language and proposed the tree-structured long",
          "bal cues and often underutilize the rich information contained": "in non-verbal cues."
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "short-term memory (Tree-LSTM) to model hierarchical depen-",
          "bal cues and often underutilize the rich information contained": "Motived by the\nimportance of non-verbal\ncues, we hope"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "dencies. Yang et al.\n[2] designed a Hierarchical Attention Net-",
          "bal cues and often underutilize the rich information contained": "to attain fine-gained features from image contextual\ninforma-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "work (HAN) for document-level sentiment classification, which",
          "bal cues and often underutilize the rich information contained": "tion. Masked image modeling paradigm introduced by He et"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "introduces the attention mechanism to assist networks in select-",
          "bal cues and often underutilize the rich information contained": "al.\n[32] has shown that an encoder can be encouraged, via a"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "ing important words and sentences. Similarity, Wang et al. [16]",
          "bal cues and often underutilize the rich information contained": "reconstruction-based self-supervised objective,\nto learn richer"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "integrated attention into LSTM to strengthen aspect-level sen-",
          "bal cues and often underutilize the rich information contained": "local\nrepresentations.\nThis insight motivates our proposal of"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "timent associations within sentences. More recently, Singh et",
          "bal cues and often underutilize the rich information contained": "a symmetrical bidirectional multimodal\nlearning framework to"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "al. [13] applied BERT to sentiment analysis on COVID-related",
          "bal cues and often underutilize the rich information contained": "ensure more effective exploitation of\nimage-based non-verbal"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "tweets. These approaches predominantly rely on unimodal sig-",
          "bal cues and often underutilize the rich information contained": "cues for desire understanding, emotion recognition, and senti-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "nals.\nIn real-world social media scenarios, however,\ntext\nis of-",
          "bal cues and often underutilize the rich information contained": "ment analysis."
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "ten accompanied by images; relying solely on text\nis therefore",
          "bal cues and often underutilize the rich information contained": ""
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "insufficient because multimodal cues typically provide comple-",
          "bal cues and often underutilize the rich information contained": "2.2. Multimodal Desire Understanding"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "mentary information to facilitate more accurate prediction.",
          "bal cues and often underutilize the rich information contained": "Multimodal desire understanding concerns recognizing de-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "",
          "bal cues and often underutilize the rich information contained": "sires or intentions expressed in both textual and visual expres-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "2.1.2. Multimodal sentiment analysis and emotion recognition",
          "bal cues and often underutilize the rich information contained": "sion, and it remains an underexplored problem. Existing auto-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "Multimodal sentiment classification [17, 18, 19] has attracted",
          "bal cues and often underutilize the rich information contained": "mated analyses of desire largely originate from psychology and"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "increasing attention as researchers seek to jointly analyze sen-",
          "bal cues and often underutilize the rich information contained": "philosophy. Lim et al.\n[33] developed a desire-understanding"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "timent\nfrom multiple modalities. You et al.\n[17]\nintroduced",
          "bal cues and often underutilize the rich information contained": "system based on four emotional\nstates\nin audio and gestural"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "cross-modal consistency regression (CCR)\nto jointly classify",
          "bal cues and often underutilize the rich information contained": "cues. Cacioppo et al. [34] designed a multi-level kernel-density"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "sentiment using image and text\nfeatures. Xu et al.\n[4]\nlever-",
          "bal cues and often underutilize the rich information contained": "fMRI analysis\nto investigate differences and correlations be-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "aged scene-level visual cues and attention mechanisms to iden-",
          "bal cues and often underutilize the rich information contained": "tween sexual desire and love.\nSchutte et al.\n[35] conducted"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "tify important words and sentences. Hu et al.\n[20] focused on",
          "bal cues and often underutilize the rich information contained": "a meta-analysis on 2,692 participants to examine links between"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "user’s latent emotion states. From a modality-interaction per-",
          "bal cues and often underutilize the rich information contained": "curiosity and creativity. Hoppe et al.\n[36] estimated different"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "spective, Xu et al. [18] explored iterative relations between im-",
          "bal cues and often underutilize the rich information contained": "levels of curiosity using eye-movement data and SVM. Yavuz"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "age and text. Huang et al.\n[19] used hybrid fusion strategies to",
          "bal cues and often underutilize the rich information contained": "et al.\n[37] proposed a data-mining approach for desire and in-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "enable between unimodal and cross-modal representations. Li",
          "bal cues and often underutilize the rich information contained": "tent using neural networks and Bayesian networks. Chauhan et"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "et al. [21] applied contrastive learning and data augmentation to",
          "bal cues and often underutilize the rich information contained": "al. [38] presented a multi-task multimodal deep attentive frame-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "align and fuse the token-level features of text and image. Mean-",
          "bal cues and often underutilize the rich information contained": "work for offense, motivation, and sentiment analysis. Never-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "while, multimodal emotion recognition [22, 23, 24], which tar-",
          "bal cues and often underutilize the rich information contained": "theless, these researches commonly lack support for large-scale"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "gets finer-gained affective states than coarse sentiment, has also",
          "bal cues and often underutilize the rich information contained": "multimodal social media data and often do not fully exploit both"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "been widely studied. To capture reader’s emotional\nreactions",
          "bal cues and often underutilize the rich information contained": "visual and textual channels."
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "to new articles, Guo et al.\n[25]\nintroduced two multimodal",
          "bal cues and often underutilize the rich information contained": "To address these limitations, Jia et al. [39] introduced MSED"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "news datasets and proposed a layout-driven network. Nemaiti",
          "bal cues and often underutilize the rich information contained": "dataset,\nthe first multimodal dataset\nfor desire understanding."
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "et al.\n[26] proposed a hybrid method for latent-space data fu-",
          "bal cues and often underutilize the rich information contained": "MSED collects image-text pairs from social media and defines"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "sion. Zhang et al. [23] used manifold learning and deep convo-",
          "bal cues and often underutilize the rich information contained": "three sub-tasks, including desire understanding, emotion recog-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "lutional networks to extract\nlow-dimensional emotion-relevant",
          "bal cues and often underutilize the rich information contained": "nition, and sentiment analysis. They also provide various strong"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "features. Xu et al.\n[27] adopted image captions as semantic",
          "bal cues and often underutilize the rich information contained": "baselines based on different combinations of feature represen-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "cues for multimodal prediction. Yang et al.\n[5] updated image-",
          "bal cues and often underutilize the rich information contained": "tations using various image and text encoders. Aziz et al.\n[40]"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "text features within a memory network and introduced a multi-",
          "bal cues and often underutilize the rich information contained": "attempted to combine two multimodal models into a unified ar-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "view attention network for multimodal\nintegration.\nDespite",
          "bal cues and often underutilize the rich information contained": "chitecture (MMTF-DES) to achieve ensemble-like performance"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "these advances, most of the existing approaches simply lever-",
          "bal cues and often underutilize the rich information contained": "and reported improved results on desire understanding. How-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "aged holistic or local features extracted from different modali-",
          "bal cues and often underutilize the rich information contained": "ever,\nthis strategy increases training and inference cost. Moti-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "ties to predict multimodal sentiments, which leads to subopti-",
          "bal cues and often underutilize the rich information contained": "vated by the need to fully exploit image-based non-verbal cues"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "mal performance.",
          "bal cues and often underutilize the rich information contained": "while maintaining computational efficiency, we propose a sym-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "The emergence of graph neural networks (GNNs)\n[28, 29]",
          "bal cues and often underutilize the rich information contained": "metrical bidirectional multimodal learning framework, offering"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "has opened new directions for mining relationships among ver-",
          "bal cues and often underutilize the rich information contained": "a new method for multimodal desire understanding."
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "bal cues. Yang et al.\n[9] observed that emotional expression",
          "bal cues and often underutilize the rich information contained": ""
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "exhibits specific global features and introduced a multi-channel",
          "bal cues and often underutilize the rich information contained": ""
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "",
          "bal cues and often underutilize the rich information contained": "3. SyDES"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "GNN to model global attributes, aiming to mine commonali-",
          "bal cues and often underutilize the rich information contained": ""
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "ties in language signals. Similarly, Zhang et al. [30] proposed a",
          "bal cues and often underutilize the rich information contained": "Figure 2 illustrates overall architecture of SyDES.\nIn this"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "multi-task interactive graph-attention network with local-global",
          "bal cues and often underutilize the rich information contained": "section, we first\nintroduce the mixed-scale image strategy and"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "context connection module to model contextual\nrelationships.",
          "bal cues and often underutilize the rich information contained": "motivate its necessity. We then describe in detail each com-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "Wang et al.\n[31] enriched textual\nrepresentations by leverag-",
          "bal cues and often underutilize the rich information contained": "ponent of\nthe architecture,\nincluding the image encoder,\ntext"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "ing contextual world knowledge from large multimodal models.",
          "bal cues and often underutilize the rich information contained": "encoder, text-guided image decoder, and image-guided text de-"
        },
        {
          "to text classification. Tai et al.\n[12] considered syntactic struc-": "However, these approaches predominantly focus on mining ver-",
          "bal cues and often underutilize the rich information contained": "coder. Next, we present the loss functions used during training"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "guided text decoder."
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "and explain how they support downstream performance.\nFi-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "nally, we discuss the two-stage training paradigm and its ratio-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "nale."
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "3.1. Mixed-Scale Image Strategy"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "High-resolution images can capture richer\nregion-level\nfea-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "tures and local details. However, many multimodal models con-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "ventionally operate on 224×224 images, which constrains their"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "capacity for fine-grained visual perception. Directly increas-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "ing the encoder input resolution (e.g., processing all\ninputs at"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "448 × 448) dramatically increases computational cost."
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "To\nbalance fine-grained\nfeature\nextraction\nand\ncomputa-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "tional efficiency, we adopt a mixed-scale sub-images strategy"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "to get detailed high-resolution visual\nfeatures. Given a high-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "resolution image Ii from batch size N, e.g., 448 × 448, we pro-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "duce one downsampled low-resolution image and four high-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "resolution sub-images,\nresulting in five 224 × 224 images per"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "original\nimage.\nThis provides four sets of fine-grained local"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "features and one global representation while controlling com-"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": "putational overhead. Formally:"
        },
        {
          "Figure 2: Overall architecture of SyDES. The model consists of four core modules: an image encoder, a text encoder, a text-guided image decoder, and an image-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.2.3. Text-guided Image Decoder": "To recover masked patches from (cid:101)P(2,n)",
          "the image-guided text decoder enables the text modality to as-": "similate multi-scale visual cues. This symmetrical bidirectional"
        },
        {
          "3.2.3. Text-guided Image Decoder": "emb,i and make reconstruc-",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "tion aware of textual semantics, we adopt a text-guided imaged",
          "the image-guided text decoder enables the text modality to as-": "fusion thus exploits non-verbal visual cues at both local and"
        },
        {
          "3.2.3. Text-guided Image Decoder": "decoder.\nSpecifically, we utilize cross-attention transformer",
          "the image-guided text decoder enables the text modality to as-": "global\nrepresentations while preserving effective use of ver-"
        },
        {
          "3.2.3. Text-guided Image Decoder": "modules to deeply fuse image and text information. We project",
          "the image-guided text decoder enables the text modality to as-": "bal cues, yielding mutual guidance and semantic alignment be-"
        },
        {
          "3.2.3. Text-guided Image Decoder": "V (2,n)\nto the text embedding dimension C2, and introduce mP",
          "the image-guided text decoder enables the text modality to as-": "tween different modalities."
        },
        {
          "3.2.3. Text-guided Image Decoder": "i",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "shared and learnable mask tokens Pmask ∈ RmP×C2 . The decoder",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "input is:",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "3.3. Loss Functions"
        },
        {
          "3.2.3. Text-guided Image Decoder": "Din\n)] ∈ R(mP+rP+1)×C2\n= [Pmask; Proj(V (2,n)",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "i,n\ni",
          "the image-guided text decoder enables the text modality to as-": "3.3.1. Reconstruction Loss"
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "We need to calculate error between predicted pixel values and"
        },
        {
          "3.2.3. Text-guided Image Decoder": "To achieve stable modal\nfusion, a gate-based fusion mech-",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "real pixel values when utilizing masked image modeling. We"
        },
        {
          "3.2.3. Text-guided Image Decoder": "anism is used to combine intermediate image projections and",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "minimize mean square error over\nin the pixel space between"
        },
        {
          "3.2.3. Text-guided Image Decoder": "text features:",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "the masked image tokens in the high-resolution sub-images and"
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "the reconstructed tokens, and use it as the reconstruction loss to"
        },
        {
          "3.2.3. Text-guided Image Decoder": "W comb\n= Gate(Wi, Din\ni,n) ∈ R(rP+mP+1)×C2\ni,n",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "optimize fine-grained feature extraction capability of the image"
        },
        {
          "3.2.3. Text-guided Image Decoder": "",
          "the image-guided text decoder enables the text modality to as-": "encoder:"
        },
        {
          "3.2.3. Text-guided Image Decoder": "as the key and value,\ni,n as the query and W comb",
          "the image-guided text decoder enables the text modality to as-": ""
        },
        {
          "3.2.3. Text-guided Image Decoder": "we leverage textual semantic information to compel the masked",
          "the image-guided text decoder enables the text modality to as-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "obtain:": "ei,n = uT tanh (cid:0)zi,nWz + b(cid:1)",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "αi,n = (cid:2)softmax (cid:0)[ei,1, . . . , ei,4](cid:1)(cid:3)\nn",
          "3.4. Two-stage Training Strategy": "To balance semantic alignment with pixel-level\nreconstruc-"
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "tion, we\nadopt\na\ntwo-stage\ntraining strategy and selectively"
        },
        {
          "obtain:": "4",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "(cid:88)",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "\n",
          "3.4. Two-stage Training Strategy": "freeze or unfreeze model components\nin each stage to steer"
        },
        {
          "obtain:": "αi,nzi,n\nPagg,i = MLP",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "learning."
        },
        {
          "obtain:": "n=1",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "here, Wz, u, b are learnable parameters, and the MLP projects",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "the aggregated feature into same semantic space as vcls\n. After",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i",
          "3.4. Two-stage Training Strategy": "3.4.1. Pre-training Stage"
        },
        {
          "obtain:": "normalizing to all features, the similarity loss is computed as:",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "The pre-training stage focuses on masked image modeling,"
        },
        {
          "obtain:": "N",
          "3.4. Two-stage Training Strategy": "giving priority to training the image encoder and the text-guided"
        },
        {
          "obtain:": "(cid:88)",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "∥vcls\n(4)\nLsi = 1\n− Pagg,i∥2",
          "3.4. Two-stage Training Strategy": "image decoder.\nIn this stage, we freeze the image-guided text"
        },
        {
          "obtain:": "i",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "N",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i=1",
          "3.4. Two-stage Training Strategy": "decoder and the MLP, and only update the text encoder, the im-"
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "age encoder, and the text-guided image decoder. This design"
        },
        {
          "obtain:": "This loss ensures that\nthe reconstructed local pixel\nfeatures",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "allows us\nto extract fine-grained details\nfrom high-resolution"
        },
        {
          "obtain:": "from high-resolution sub-images\nremain consistent with the",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "sub-images while constraining modal consistency at the seman-"
        },
        {
          "obtain:": "global image semantics from low-resolution images, achieving",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "tic level. As discussed in Section 3.3.3 and Section 3.3.4, we"
        },
        {
          "obtain:": "a balance between local and global features.",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "also consider\nthe sub-image-level global visual\nrepresentation"
        },
        {
          "obtain:": "3.3.4. Cross-Modal Feature-Distribution Consistency Loss",
          "3.4. Two-stage Training Strategy": "of reconstructed image tokens and their cross-modal alignment."
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "Therefore, the overall loss used in this stage is:"
        },
        {
          "obtain:": "There exists a conflict between the objectives of\nthe con-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "trastive loss (semantic-level) and the reconstruction loss (pixel-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "(7)\nLp = λrecLrec + λsiLsi + λdcLdc + λitcLitc"
        },
        {
          "obtain:": "level).\nThe former emphasizes cross-modal\nsemantic align-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "ment,\nwhile\nthe\nlatter\nfocuses\non\npixel-level\nreconstruc-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "that\nwhere λrec, λsi, λdc and λitc are hyperparameter weights"
        },
        {
          "obtain:": "tion quality without explicitly enforcing semantic consistency",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "balance the contributions of each loss term."
        },
        {
          "obtain:": "across modalities.\nTo mitigate this, we constrain the seman-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "tic distribution consistency between the text\nto reconstructed",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "image features and the text\nto low-resolution image features.",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "3.4.2. Fine-tuning Stage"
        },
        {
          "obtain:": "Specifically,\nthe similarity distribution between the aggregated",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "The fine-tuning stage targets downstream tasks such as desire"
        },
        {
          "obtain:": "reconstructed image feature Pagg,i and the global text represen-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "understanding. Its goal is to train the image-guided text decoder"
        },
        {
          "obtain:": "tation wcls\nis defined as:",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "to fully leverage both local and global features produced by the"
        },
        {
          "obtain:": "(cid:16)\n(cid:17)",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "image encoder, and to use an MLP to map the fused multimodal"
        },
        {
          "obtain:": "⟩/τ\nexp\n⟨Pagg,i, wcls",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": ") =\nS (Pagg,i, wcls",
          "3.4. Two-stage Training Strategy": "representation to task-specific outputs. During fine-tuning we"
        },
        {
          "obtain:": "(cid:16)\n(cid:17)",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "(cid:80)N",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "⟩/τ\n⟨Pagg,i, wcls",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "j=1 exp\nj",
          "3.4. Two-stage Training Strategy": "freeze the image encoder and text-guided image decoder, and"
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "train the text encoder,\nthe image-guided text decoder, and the"
        },
        {
          "obtain:": ", wcls\nSimilarly, the similarity distribution S (vcls\n) is defined for",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i\ni",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "MLP. The overall loss for this stage is:"
        },
        {
          "obtain:": "the low-resolution image feature and the global\ntext represen-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "tation. By minimizing the KL divergence between the two dis-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "(8)\nL f = λclsLcls + λitcLitc"
        },
        {
          "obtain:": "tributions and adding an entropy regularization term for robust-",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "ness, we obtain the following loss:",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "",
          "3.4. Two-stage Training Strategy": "where λcls and λitc are hyperparameter weights. Retaining the"
        },
        {
          "obtain:": "N",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "(cid:88)",
          "3.4. Two-stage Training Strategy": "ITC term helps preserve cross-modal alignment stability during"
        },
        {
          "obtain:": ", wcls\n), S (vcls\n))\nLdc = 1\nKL(S (Pagg,i, wcls",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i\ni",
          "3.4. Two-stage Training Strategy": "fine-tuning."
        },
        {
          "obtain:": "N",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "i=1",
          "3.4. Two-stage Training Strategy": ""
        },
        {
          "obtain:": "(5)",
          "3.4. Two-stage Training Strategy": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "i\ni"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "N"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "i=1"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "(5)"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "N"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "(cid:88)"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "+ 1"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "))\nH(S (Pagg,i, wcls"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "N"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "i=1"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "where KL denotes the relative entropy loss function, H repre-"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "sents the entropy function."
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "3.3.5. Classification Loss"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "For downstream tasks, such as desire understanding, emotion"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "recognition, and sentiment analysis, we use the standard cross-"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "entropy loss:"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "N"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "(cid:88)"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "(6)\nLcls = 1"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "CrossEntropy(yi,(cid:98)yi)"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "N"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "i=1"
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": ""
        },
        {
          ", wcls\n), S (vcls\n))\nKL(S (Pagg,i, wcls": "is the predicted label.\nwhere y is the ground-truth label and (cid:98)yi"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: In sentiment analysis, BERTweet",
      "data": [
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "4.1. Experiment Setup"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "4.1.1. Dataset"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "To validate our method, we evaluate the model on the pub-"
        },
        {
          "4. Experiments": "licly available multimodal dataset MSED [39]. MSED is the"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "first multimodal, multi-task dataset for sentiment analysis, emo-"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "tion recognition, and desire understanding and contains 9, 190"
        },
        {
          "4. Experiments": "image-text pairs annotated in English.\nThe samples are col-"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "lected from social media platforms,\nincluding Twitter, Getty"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "Image, and Flickr. The dataset defines three downstream tasks:"
        },
        {
          "4. Experiments": "sentiment analysis, emotion recognition, and desire understand-"
        },
        {
          "4. Experiments": "ing. Each multimodal sample is manually labeled with senti-"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "ment class (positive, neutral, negative), an emotion class (hap-"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "sad, neutral, disgust, anger, and fear),\nand a desire"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "class (family, romance, vengeance, curiosity, tranquility, social-"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "contact, and none). The data are split into train, validation, and"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "test\nsets with ratios of 70%, 10%, and 20%, yielding 6, 127"
        },
        {
          "4. Experiments": "training instances, 1, 021 validation instances, and 2, 024 test"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "instances. The detailed statistics of the MSED dataset are listed"
        },
        {
          "4. Experiments": "in Table 1."
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "4.1.2. Evaluation Metrics"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "All\nthree downstream tasks are classification problems. We"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "therefore report standard classification metrics: Precision (P),"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "Recall\n(R), Macro-F1-score\n(F1),\nand Weighted Accuracy"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "(Acc)."
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "4.1.3. Training Details"
        },
        {
          "4. Experiments": ""
        },
        {
          "4. Experiments": "All experiments run on NVIDIA V100 with CUDA 11.0 and"
        },
        {
          "4. Experiments": "PyTorch 2.1.2 [42].\nIn pre-training stage, we initialize the im-"
        },
        {
          "4. Experiments": "age encoder and the text encoder from CLIP [43] pre-training"
        },
        {
          "4. Experiments": "weights provided by OpenAI1, and initialize the text-guided im-"
        },
        {
          "4. Experiments": "age decoder\nfrom pre-training weights2\nin [44]. We use the"
        },
        {
          "4. Experiments": "AdamW [45] optimizer. The initial learning rates for the image"
        },
        {
          "4. Experiments": "encoder,\nthe text encoder, and text-guided image decoder are"
        },
        {
          "4. Experiments": "set\nto 5e − 6, 5e − 5, and 1e − 4,\nrespectively. Weight decay"
        },
        {
          "4. Experiments": "is 0.01. The learning rate follows a cosine decay schedule with"
        },
        {
          "4. Experiments": "a 15% warmup period. We train for 50 epochs with batch size"
        },
        {
          "4. Experiments": "64. Loss weights are set to λrec = 1, λsi = 0.5, λdc = 0.025, and"
        },
        {
          "4. Experiments": "λitc = 0.5. The masking ratio m is 0.75 during pre-training."
        },
        {
          "4. Experiments": "During fine-tuning stage, we keep the overall setup and fine-"
        },
        {
          "4. Experiments": "tune the pretrained model separately for desire understanding,"
        },
        {
          "4. Experiments": "emotion recognition, and sentiment analysis. Specifically,\nthe"
        },
        {
          "4. Experiments": "initial learning rates for the text encoder, image-guided text de-"
        },
        {
          "4. Experiments": "coder, and MLP are 1e − 4, 2e − 4, and 1e − 4, respectively, with"
        },
        {
          "4. Experiments": "a 10% warmup. Fine-tuning uses batch size 64 for 50 epochs."
        },
        {
          "4. Experiments": "Loss weights are λcls = 1 and λitc = 0.4. In fine-tuning, images"
        },
        {
          "4. Experiments": "are no longer masked, we feed the full\nlow-resolution image"
        },
        {
          "4. Experiments": "along with high-resolution sub-images. The masking ratio m is"
        },
        {
          "4. Experiments": "thus set to 0.00."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: , we systematically",
      "data": [
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "Desire Understanding"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": ""
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "R"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "52.02"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "50.64"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "68.00"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "75.50"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "80.20"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "-"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "82.01"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "84.07"
        },
        {
          "Table 3: Comparison of SyDES and other SOTA methods on the MSED dataset": "+2.50"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: , we systematically",
      "data": [
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "84.07\n84.02\n88.32\nSyDES (Ours)\n84.09",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "84.92\n84.81\n84.74\n85.96\n89.28\n89.13\n89.19\n89.37"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "%Gains\n-0.20\n+2.50\n+1.10\n+1.60",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "+0.60\n+0.20\n+0.60\n+2.20\n+1.10\n+0.50\n+0.90\n+1.10"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "to effectively integrate\ntextual\nfeatures with both local\nand",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "desire understanding and emotion recognition."
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "global visual\nfeatures\nfor desire understanding.\nTo evaluate",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": ""
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "SyDES and validate the effectiveness of non-verbal cues mining",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "4.4. Ablation Studies"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "mechanism, we conducted experiments on the MSED dataset,",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "4.4.1. Loss Functions"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "which comprises three sub-tasks,\nincluding desire understand-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": ""
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "We adopt a two-stage training strategy to facilitate cross-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "ing, emotion recognition, and sentiment analysis.",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": ""
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "modal fusion between textual features and both local and global"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "The comparative performance of our proposed SyDES on",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "visual features. As summarized in Table 4, we systematically"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "test data against other SOTA methods across all\ntasks is pre-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "evaluate the impact of different\nloss combinations on the three"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "sented in Table 3.\nThe results indicate that SyDES achieves",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "sub-tasks. The first row corresponds to the model fin-tuned di-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "competitive performance across all\nthree tasks.\nIn terms of",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "rectly without pre-training stage (i.e., SyDES-B as described"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "the primary metric F1-score, our proposed SyDES achieves",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "in Section 4.1.4). During pre-training,\nfour loss functions are"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "84.02% in desire understanding, 84.74% in emotion recogni-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "used,\nResults\nshow that\nincluding Lrec, Lsi, Ldc,\nand Litc."
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "tion, and 89.19% in sentiment analysis, surpassing the previous",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "for masked image modeling leads\nto perfor-\nusing only Lrec"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "best model, MMTF-DES [40], by 1.1%, 0.6%, and 0.9% gains,",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "mance even worse than SyDES-B. This indicates that\nrecon-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "respectively.\nIt is worth noting that MMTF-DES relies on inte-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "structing high-resolution sub-images alone may introduce a se-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "grating multiple multimodal Transformer encoders (e.g., ViLT",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "mantic mismatch with the low-resolution image due to inade-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "and VAuLT), which entails considerably higher training costs.",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "quate modal alignment,\nresulting in modal\ninconsistency that"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "In contrast, SyDES extracts non-verbal cues from images effec-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "harms fine-tuning. Gradually incorporating Litc, Lsi, and Ldc"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "tively while maintaining lower computational overhead.\nThe",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "consistently improves performance across all\ntasks. For exam-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "experimental\nresults validate this\ntrade-off:\nthe improvement",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "ple,\nin desire understanding,\nthe F1-score increases to 81.44%"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "on the desire understanding task, which depends more heavily",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "and finally to 84.02% with\nwith Litc,\nto 82.24% with Lsi,"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "on non-verbal cues,\nis particularly pronounced in terms of the",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "This\nsuggests\nthat\ncross-modal\nalignment\nand seman-\nLdc."
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "F1-score.",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "tic/consistency constraints are critical\nto bridging the gap be-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "We further observe that sentiment analysis generally yields",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "tween reconstructed sub-images and the global\nimage seman-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "higher performance than for the emotion recognition and desire",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "tics."
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "understanding tasks. A possible reason for\nthis difference in",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "During fine-tuning,\ntwo loss\nfunctions are used,\nincluding"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "performance may be attributed to the nature of the three tasks.",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "Litc and Lcls. We observe that that Litc is important to preserve"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "Sentiment analysis aim to identify the overall emotion or opin-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "pre-training gains.\nRemoving Litc during fine-tuning causes"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "ion expressed in an image-text pair.\nIn contrast, desire under-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "the F1-scores\nto drop from 84.02%, 84.74%, and 89.19% to"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "standing and emotion recognition require fine-grained detection",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "77.09%,\n83.21%,\nand 86.16%,\nrespectively.\nIn conclusion,"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "of specific signals such as a person’s gestures or facial expres-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "using all proposed loss functions yields the best performance"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "sions that are inherently embedded in images.\nFor example,",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "across desire understanding,\nemotion recognition,\nand senti-"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "there exists the exaggerated motion and frightened expression",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "ment analysis, confirming the complementary effects of the loss"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "of\nthe man in Figure 1 (b).\nThese subtle cues are inherently",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "terms."
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "more challenging to capture. Therefore, sentiment analysis may",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": ""
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "be an easier and more straightforward task for model, while de-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "4.4.2. Ratio of Masked Tokens"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "sire understanding and emotion recognition may be more com-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "As shown in Table 5, we investigate the impact by setting"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "plicated and nuanced. Our proposed SyDES enhances local de-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "different ratios of masked tokens. Specifically, 0.25, 0.50, 0.75,"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "tail extraction,\nresulting in particularly notable gains in desire",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "and 0.90 are\ntested.\nExperimental\nresults demonstrate\nthat"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "understanding, but its performance remains slightly below that",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "a masking ratio of 75% yields\nthe best average performance"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "of sentiment analysis. This suggests there is still room for im-",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "across most\ntasks. Although a ratio of 25% achieves slightly"
        },
        {
          "84.23\nMMTF-DES [40]\n82.01\n83.11\n86.97": "provement and underscores the need for\nfurther\nresearch into",
          "84.39\n84.64\n84.26\n84.13\n88.27\n88.68\n88.44\n88.44": "better\nresults in emotion recognition,\nthe 75% ratio is overall"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": ""
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "P"
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "80.77"
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "76.17"
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "81.43"
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "82.28"
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "78.55"
        },
        {
          "Table 4: Ablation study on the loss functions used in the two training stages": "84.09"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "0.90\n81.31\n82.82\n82.02\n86.48",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "84.32\n83.31\n83.78\n85.41\n88.54\n87.93\n88.17\n88.39"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "pre-training stage of our proposed SyDES are effective.\nIt sub-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "stantially improves robustness and accuracy, especially when"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "textual information is limited."
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "4.5. Visualization"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "4.5.1. Cross-Modal Attention Heatmap Analysis"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "To intuitively illustrate the perception ability of our proposed"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "SyDES toward textual and image modalities, we visualized the"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "attention localization map for\nthe last\nlayer\nin the image en-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "coder, and compared them with those of SyDES-B. As depicted"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "Figure 3: Performance analysis of our proposed SyDES on desire understand-",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "in Figure 4, our proposed SyDES focuses more accurately on"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "ing, emotion recognition, and sentiment analysis tasks. A × indicates correct",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "fine-grained visual regions corresponding to informative words"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "classification and a ✓ represents misclassification.",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "in the text. For example, in the case of Image 1, SyDES shows"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "concentrated attention on regions related to “boy” and “bike”,"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "whereas SyDES-B neglects\nthese essential details.\nSimilar-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "more suitable considering its substantially lower computational",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "ity,\nin the case of\nImage 3, SyDES-B perceives\nthe concept"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "cost while maintain competitive performance.",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "of “family” vaguely, while our proposed SyDES clearly iden-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "tifies the five-person “family” and the “beach” scene. This im-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "4.4.3. Performance Analysis",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "proved perception can be attributed to two mechanisms: (1) the"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "masked image modeling on high-resolution sub-images, which"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "To better understand the advantage of our proposed SyDES,",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "enhances\nlocal\ndetail\nsensitivity;\n(2)\nthe\ntext-guided\ncross-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "we selected representative examples\nfrom the MSED dataset",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "modal\nreconstruction, which strengthens cross-modal seman-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "across\nthree sub-tasks.\nThese examples are misclassified by",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "tic alignment. These results further indicate that our proposed"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "SyDES-B but correctly classified by SyDES. As illustrated in",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "SyDES effectively exploits non-verbal visual cues."
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "Figure 3, we compare the predictions of SyDES-B and our pro-",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "posed SyDES,\nincluding example stimuli, ground-truth labels,",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": ""
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "and predicted probabilities. Across all three tasks, our proposed",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "4.5.2. Reconstructed Image Visualization"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "SyDES produces correct predictions with high confidence. For",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "We\nreconstructed\nand\nstitched\nthe\nhigh-resolution\nsub-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "instance, in Example 1 of the sentiment analysis task, the image",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "images, and visualized the masked image patches to inspect the"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "caption is “Brother and sister exploring with flashlight.”, it pro-",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "practical effect\nin Figure 5. Each sample consists of the origi-"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "vides limited information. But our proposed SyDES achieved",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "nal\nimage,\nthe randomly masked image, and the reconstructed"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "a high confidence score of 96.21% by effectively leveraging vi-",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "image by the text-guided image decoder. The results show that,"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "sual cues. This evidence indicates that, compared with models",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "light-\neven with complex image content (e.g., people, gesture,"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "that\nrely mainly on transfer\nlearning with pre-trained weights",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "ing, and natural scenes in the example 1), our proposed SyDES"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "(e.g., SyDES-B),\nthe non-verbal cues mining and symmetric",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "can reconstruct contextual content\nreasonably well, owing to"
        },
        {
          "84.09\n84.07\n84.02\n88.32\n0.75": "bidirectional multimodal learning mechanism introduced in the",
          "85.96\n89.28\n89.13\n89.19\n89.37\n84.92\n84.81\n84.74": "masked image modeling using high-resolution sub-images and"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ities while balancing computational cost and performance": "constitutes an important direction for further investigation."
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "To address the under-exploration of desire understanding and"
        },
        {
          "ities while balancing computational cost and performance": "of non-verbal cues in multimodal sentiment analysis, we pro-"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "pose a symmetric bidirectional multimodal learning framework"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "and sentiment\nrecognition.\nThe frame-"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "work employs a masked image modeling to boosting extract"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "fine-grained local features from high-resolution sub-images and"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "capturing global visual representations from low-resolution im-"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "ages. More, we introduce a text-guided image decoder and an"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "image-guided text decoder so that facilitate semantic alignment"
        },
        {
          "ities while balancing computational cost and performance": "and modal fusion from local and global features during decod-"
        },
        {
          "ities while balancing computational cost and performance": "ing, thereby enhancing the discriminative power of multimodal"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "representations. Extensive experiments on the MSED dataset"
        },
        {
          "ities while balancing computational cost and performance": "across three sub-tasks, demonstrate the effectiveness of our pro-"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "this work to offer insights for mul-"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": "timodal desire understanding and to inspire further research in"
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": ""
        },
        {
          "ities while balancing computational cost and performance": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion classification with multi-level semantic reason-": "ing network, IEEE Transactions on Multimedia 25 (2023)"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "6868–6880."
        },
        {
          "emotion classification with multi-level semantic reason-": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "emotion classification with multi-level semantic reason-": "CorpusID:253313650"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[11] B. Pang, L. Lee, S. Vaithyanathan, Thumbs up? sentiment"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "classification using machine learning techniques,\narXiv"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "preprint cs/0205070 (2002)."
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[12] K. S. Tai, R. Socher, C. D. Manning, Improved semantic"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "representations\nfrom\ntree-structured\nlong\nshort-term"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "memory networks, ArXiv abs/1503.00075 (2015)."
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "CorpusID:3033526"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[13] M. Singh, A. K.\nJakhar, S. Pandey, Sentiment analysis"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "on the impact of coronavirus in social\nlife using the bert"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "model, Social Network Analysis and Mining 11 (2021)."
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "CorpusID:232293517"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[14] M. Taboada, J. Brooke, M. Tofiloski, K. Voll, M. Stede,"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "Lexicon-based methods for sentiment analysis, Computa-"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "tional linguistics 37 (2) (2011) 267–307."
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[15] P. Gamallo, M. Garcia, Citius: A naive-bayes strategy for"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "sentiment analysis on english tweets,\nin: Proceedings of"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "the 8th international Workshop on Semantic Evaluation"
        },
        {
          "emotion classification with multi-level semantic reason-": "(SemEval 2014), 2014, pp. 171–175."
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[16] Y. Wang, M. Huang, X. Zhu, L. Zhao, Attention-based"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "lstm for\naspect-level\nsentiment\nclassification,\nin:\nCon-"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "ference\non\nEmpirical Methods\nin Natural\nLanguage"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "Processing, 2016."
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "emotion classification with multi-level semantic reason-": "CorpusID:18993998"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[17] Q. You, J. Luo, H. Jin, J. Yang, Cross-modality consistent"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "regression\nfor\njoint\nvisual-textual\nsentiment\nanalysis"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "of\nsocial multimedia,\nProceedings\nof\nthe Ninth ACM"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "International Conference on Web Search and Data Mining"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "(2016)."
        },
        {
          "emotion classification with multi-level semantic reason-": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "emotion classification with multi-level semantic reason-": "CorpusID:7928793"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "[18] N. Xu, W. Mao, G. Chen, A co-memory network for"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "multimodal\nsentiment\nanalysis, The\n41st\nInternational"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "ACM SIGIR Conference on Research & Development\nin"
        },
        {
          "emotion classification with multi-level semantic reason-": ""
        },
        {
          "emotion classification with multi-level semantic reason-": "Information Retrieval (2018)."
        },
        {
          "emotion classification with multi-level semantic reason-": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "emotion classification with multi-level semantic reason-": "CorpusID:195351351"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "sentiment detection based on multi-channel graph neural"
        },
        {
          "Acknowledgments": "This work is\nsupported by the National Natural Science",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "networks,\nin:\nAnnual Meeting of\nthe Association for"
        },
        {
          "Acknowledgments": "Foundation of China (Grant No.\n62272188),\nthe Fundamen-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "Computational Linguistics, 2021."
        },
        {
          "Acknowledgments": "tal Research Funds\nfor\nthe Central Universities\n(Grant No.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "2662021JC008), and the 2023 Independent Science and Tech-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:236460184"
        },
        {
          "Acknowledgments": "nology Innovation Fund Project of Huazhong Agricultural Uni-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "versity (Grant No. 2662023XXPY005).",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[10] T. Zhu, L. Li,\nJ. Yang, S. Zhao, X. Xiao, Multimodal"
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "emotion classification with multi-level semantic reason-"
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "ing network, IEEE Transactions on Multimedia 25 (2023)"
        },
        {
          "Acknowledgments": "References",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "6868–6880."
        },
        {
          "Acknowledgments": "[1] Y. Kim,\nConvolutional\nneural\nnetworks\nfor\nsentence",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "classification,\nin: Conference on Empirical Methods\nin",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:253313650"
        },
        {
          "Acknowledgments": "Natural Language Processing, 2014.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[11] B. Pang, L. Lee, S. Vaithyanathan, Thumbs up? sentiment"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "classification using machine learning techniques,\narXiv"
        },
        {
          "Acknowledgments": "CorpusID:9672033",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "preprint cs/0205070 (2002)."
        },
        {
          "Acknowledgments": "[2] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, E. H. Hovy,",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[12] K. S. Tai, R. Socher, C. D. Manning, Improved semantic"
        },
        {
          "Acknowledgments": "Hierarchical attention networks for document classifica-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "representations\nfrom\ntree-structured\nlong\nshort-term"
        },
        {
          "Acknowledgments": "tion,\nin: North American Chapter of the Association for",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "memory networks, ArXiv abs/1503.00075 (2015)."
        },
        {
          "Acknowledgments": "Computational Linguistics, 2016.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:3033526"
        },
        {
          "Acknowledgments": "CorpusID:6857205",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[13] M. Singh, A. K.\nJakhar, S. Pandey, Sentiment analysis"
        },
        {
          "Acknowledgments": "[3] G. Liu,\nJ. Guo, Bidirectional\nlstm with attention mech-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "on the impact of coronavirus in social\nlife using the bert"
        },
        {
          "Acknowledgments": "anism and\nconvolutional\nlayer\nfor\ntext\nclassification,",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "model, Social Network Analysis and Mining 11 (2021)."
        },
        {
          "Acknowledgments": "Neurocomputing 337 (2019) 325–338.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:232293517"
        },
        {
          "Acknowledgments": "CorpusID:127325665",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[14] M. Taboada, J. Brooke, M. Tofiloski, K. Voll, M. Stede,"
        },
        {
          "Acknowledgments": "[4] N. Xu, W. Mao, Multisentinet: A deep semantic network",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "Lexicon-based methods for sentiment analysis, Computa-"
        },
        {
          "Acknowledgments": "for multimodal\nsentiment\nanalysis, Proceedings of\nthe",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "tional linguistics 37 (2) (2011) 267–307."
        },
        {
          "Acknowledgments": "2017 ACM on Conference on Information and Knowl-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "edge Management (2017).",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[15] P. Gamallo, M. Garcia, Citius: A naive-bayes strategy for"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "sentiment analysis on english tweets,\nin: Proceedings of"
        },
        {
          "Acknowledgments": "CorpusID:29030535",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "the 8th international Workshop on Semantic Evaluation"
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "(SemEval 2014), 2014, pp. 171–175."
        },
        {
          "Acknowledgments": "[5] X. Yang, S. Feng, D. Wang, Y. Zhang,\nImage-text mul-",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "timodal emotion classification via multi-view attentional",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[16] Y. Wang, M. Huang, X. Zhu, L. Zhao, Attention-based"
        },
        {
          "Acknowledgments": "network,\nIEEE Transactions on Multimedia 23 (2020)",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "lstm for\naspect-level\nsentiment\nclassification,\nin:\nCon-"
        },
        {
          "Acknowledgments": "4014–4026.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "ference\non\nEmpirical Methods\nin Natural\nLanguage"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "Processing, 2016."
        },
        {
          "Acknowledgments": "CorpusID:229272644",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "[6] Z. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang,",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:18993998"
        },
        {
          "Acknowledgments": "A.\nZadeh,\nL.\nphilippe Morency,\nEfficient\nlow-rank",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[17] Q. You, J. Luo, H. Jin, J. Yang, Cross-modality consistent"
        },
        {
          "Acknowledgments": "multimodal\nfusion with modality-specific factors, ArXiv",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "regression\nfor\njoint\nvisual-textual\nsentiment\nanalysis"
        },
        {
          "Acknowledgments": "abs/1806.00064 (2018).",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "of\nsocial multimedia,\nProceedings\nof\nthe Ninth ACM"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "International Conference on Web Search and Data Mining"
        },
        {
          "Acknowledgments": "CorpusID:44131945",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "(2016)."
        },
        {
          "Acknowledgments": "[7] P. Portner, A. Rubinstein, Desire, belief,\nand semantic",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "composition:\nvariation\nin mood\nselection with\ndesire",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:7928793"
        },
        {
          "Acknowledgments": "predicates, Natural Language Semantics 28 (2020) 343 –",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "[18] N. Xu, W. Mao, G. Chen, A co-memory network for"
        },
        {
          "Acknowledgments": "393.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "multimodal\nsentiment\nanalysis, The\n41st\nInternational"
        },
        {
          "Acknowledgments": "URL\nhttps://api.semanticscholar.org/",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "ACM SIGIR Conference on Research & Development\nin"
        },
        {
          "Acknowledgments": "CorpusID:226352313",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": ""
        },
        {
          "Acknowledgments": "",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "Information Retrieval (2018)."
        },
        {
          "Acknowledgments": "[8] W. Hofmann, L. F. Nordgren, The psychology of desire,",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Acknowledgments": "Guilford Publications, 2015.",
          "[9] X. Yang,\nS.\nFeng, Y. Zhang, D. Wang, Multimodal": "CorpusID:195351351"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "modality-gated networks\nfor\nimage-text\nsentiment anal-",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:20067030"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "ysis,\nACM Transactions\non Multimedia\nComputing,",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "[28] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Communications, and Applications (TOMM) 16 (2020) 1",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "G. Monfardini, The graph neural network model,\nIEEE"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "– 19.",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "Transactions on Neural Networks 20 (2009) 61–80."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:218517893",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:206756462"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[20] A. Hu, S. Flaxman, Multimodal\nsentiment\nanalysis\nto",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "[29] X. Liu, W. Liu, M. Zhang, J. Chen, L. Gao, C. C. Yan,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "explore the structure of emotions, Proceedings of the 24th",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "T. Mei,\nSocial\nrelation\nrecognition\nfrom videos\nvia"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "ACM SIGKDD International Conference on Knowledge",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "multi-scale spatial-temporal\nreasoning, 2019 IEEE/CVF"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Discovery & Data Mining (2018).",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "Conference on Computer Vision and Pattern Recognition"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "(CVPR) (2019) 3561–3569."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:44075392",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[21] Z. Li, B. Xu, C. Zhu, T. Zhao, Clmlf:\nA contrastive",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:198118474"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "learning and multi-layer\nfusion method for multimodal",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "[30] Y. Zhang, A.\nJia, B. Wang, P. Zhang, D. Zhao, P. Li,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "sentiment detection, ArXiv abs/2204.05515 (2022).",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "Y\n. Hou, X. Jin, D. Song, J. Qin, M3gat: A multi-modal,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "multi-task interactive graph attention network for\ncon-"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:248119031",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "versational\nsentiment analysis and emotion recognition,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[22] H.\nRanganathan,\nS.\nChakraborty,\nS.\nPanchanathan,",
          "URL\nhttps://api.semanticscholar.org/": "ACM Transactions on Information Systems 42 (2023) 1 –"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Multimodal\nemotion\nrecognition\nusing\ndeep\nlearning",
          "URL\nhttps://api.semanticscholar.org/": "32."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "architectures, 2016 IEEE Winter Conference on Applica-",
          "URL\nhttps://api.semanticscholar.org/": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "tions of Computer Vision (WACV) (2016) 1–9.",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:258788073"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "[31] W. Wang, L. Ding, L. Shen, Y. Luo, H. Hu, D. Tao,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:6182290",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "Wisdom:\nImproving multimodal\nsentiment\nanalysis by"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[23] Y. Zhang, C. Cheng, Y. Zhang, Multimodal\nemotion",
          "URL\nhttps://api.semanticscholar.org/": "fusing contextual world knowledge, Proceedings of\nthe"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "recognition based on manifold learning and convolution",
          "URL\nhttps://api.semanticscholar.org/": "32nd ACM International\nConference\non Multimedia"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "neural network, Multimedia Tools and Applications 81",
          "URL\nhttps://api.semanticscholar.org/": "(2024)."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "(2022) 33253 – 33268.",
          "URL\nhttps://api.semanticscholar.org/": "URL"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "https://api.semanticscholar.org/"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:266977237"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:248252616",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "[32] K. He, X. Chen, S. Xie, Y. Li, P. Doll’ar, R. B. Girshick,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[24] H.-D. Le, G. Lee, S. hyung Kim, S. won Kim, H.-J.",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "Masked autoencoders are scalable vision learners, 2022"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Yang, Multi-label multimodal emotion recognition with",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "transformer-based fusion and emotion-level\nrepresenta-",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "Recognition (CVPR) (2021) 15979–15988."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "tion learning, IEEE Access 11 (2023) 14742–14751.",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:243985980"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:256944875",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "[33] A. Lim, T. Ogata, H. G. Okuno, The desire model: Cross-"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[25] W. Guo, Y. Zhang, X. Cai, L. Meng, J. Yang, X. Yuan,",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "modal emotion analysis and expression for robots, Infor-"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Ld-man: Layout-driven multimodal attention network for",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "mation Processing Society of Japan 5 (4) (2012)."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "online news sentiment recognition, IEEE Transactions on",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Multimedia 23 (2021) 1785–1798.",
          "URL\nhttps://api.semanticscholar.org/": "[34] S. Cacioppo, F. Bianchi-Demicheli, C. Frum, J. G. Pfaus,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": "J. W. Lewis, The common neural bases between sexual"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:226742001",
          "URL\nhttps://api.semanticscholar.org/": "desire and love: a multilevel kernel density fmri analysis,"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "",
          "URL\nhttps://api.semanticscholar.org/": "The journal of sexual medicine 9 (4) (2012) 1048–1054."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[26] S. Nemati, R. Rohani, M. E. Basiri, M. Abdar, N. Y. Yen,",
          "URL\nhttps://api.semanticscholar.org/": ""
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "V\n. Makarenkov, A hybrid latent space data fusion method",
          "URL\nhttps://api.semanticscholar.org/": "[35] N. S. Schutte,\nJ. M. Malouff, A meta-analysis of\nthe"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "for multimodal\nemotion\nrecognition,\nIEEE Access\n7",
          "URL\nhttps://api.semanticscholar.org/": "relationship between curiosity and creativity, The Journal"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "(2019) 172948–172964.",
          "URL\nhttps://api.semanticscholar.org/": "of Creative Behavior (2019)."
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "URL\nhttps://api.semanticscholar.org/",
          "URL\nhttps://api.semanticscholar.org/": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "CorpusID:209320752",
          "URL\nhttps://api.semanticscholar.org/": "CorpusID:199157255"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "[27] N. Xu, Analyzing multimodal public sentiment based on",
          "URL\nhttps://api.semanticscholar.org/": "[36] S. Hoppe, T. Loetscher, S. Morey, A. Bulling, Recogni-"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "hierarchical\nsemantic\nattentional\nnetwork,\n2017\nIEEE",
          "URL\nhttps://api.semanticscholar.org/": "tion of curiosity using eye movement analysis, Adjunct"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "International Conference\non\nIntelligence\nand\nSecurity",
          "URL\nhttps://api.semanticscholar.org/": "Proceedings of\nthe 2015 ACM International\nJoint Con-"
        },
        {
          "[19] F. Huang, K. Wei,\nJ. Weng,\nZ. Li, Attention-based": "Informatics (ISI) (2017) 152–154.",
          "URL\nhttps://api.semanticscholar.org/": "ference\non\nPervasive\nand Ubiquitous Computing\nand"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Proceedings of the 2015 ACM International Symposium": "on Wearable Computers (2015).",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong,"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "J. Miller, H. Hajishirzi, A. Farhadi, L. Schmidt, Openclip,"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:15967389",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "if you use this software, please cite it as below. (Jul. 2021)."
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "doi:10.5281/zenodo.5143773."
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[37] Ö. Yavuz, A. Karahoca, D. Karahoca, A data mining",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "URL https://doi.org/10.5281/zenodo.5143773"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "approach for desire and intention to participate in virtual",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "communities,\nInternational\nJournal\nof\nElectrical\nand",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "[45]\nI. Loshchilov, F. Hutter, Decoupled weight decay reg-"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "Computer Engineering (IJECE) (2019).",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "ularization,\nin:\nInternational Conference\non Learning"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "Representations, 2017."
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:208980057",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "CorpusID:53592270"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[38] D. S. Chauhan, S. Dhanush, A. Ekbal, P. Bhattacharyya,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "All-in-one: A deep attentive multi-task learning frame-",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "[46] D. Q. Nguyen,\nT. Vu, A. G.-T. Nguyen,\nBertweet:"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "work for humour, sarcasm, offensive, motivation, and sen-",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "A pre-trained\nlanguage model\nfor\nenglish\ntweets,\nin:"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "timent on memes,\nin: Proceedings of\nthe 1st conference",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "Conference on Empirical Methods in Natural Language"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "of the Asia-Pacific chapter of the association for compu-",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "Processing, 2020."
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "tational linguistics and the 10th international joint confer-",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "ence on natural language processing, 2020, pp. 281–290.",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "CorpusID:218719869"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[39] A. Jia, Y. He, Y. Zhang, S. Uprety, D. Song, C. Lioma,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "[47] K. He, X. Zhang, S. Ren, J. Sun, Deep residual\nlearning"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "Beyond\nemotion:\nA multi-modal\ndataset\nfor\nhuman",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "for\nimage recognition, 2016 IEEE Conference on Com-"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "desire understanding,\nin: North American Chapter of the",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "puter Vision\nand Pattern Recognition\n(CVPR)\n(2015)"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "Association for Computational Linguistics, 2022.",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "770–778."
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:250391079",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "CorpusID:206594692"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "[48] H. Wang, Y. Tang, Y. Wang,\nJ. Guo, Z. Deng, K. Han,"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[40] A. Aziz, N. K. Chowdhury, M. A. Kabir, A. N. Chy, M. J.",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "Masked image modeling with local multi-scale\nrecon-"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "Siddique, Mmtf-des: A fusion of multimodal transformer",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "struction,\n2023\nIEEE/CVF Conference\non Computer"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "models\nfor desire,\nemotion,\nand sentiment\nanalysis of",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "Vision and Pattern Recognition (CVPR)\n(2023) 2122–"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "social media data, Neurocomputing 623 (2025) 129376.",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "2131."
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "URL\nhttps://api.semanticscholar.org/"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:275561806",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": "CorpusID:257427476"
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[41] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "G. Heigold,\nS. Gelly,\nJ. Uszkoreit, N. Houlsby, An",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "image\nis worth 16x16 words:\nTransformers\nfor\nimage",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "recognition at scale, ArXiv abs/2010.11929 (2020).",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:225039882",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[42] A.\nPaszke,\nS. Gross,\nF. Massa, A.\nLerer,\nJ. Brad-",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "M. Raison, A.\nTejani,\nS. Chilamkurthy,\nB.\nSteiner,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "L. Fang,\nJ. Bai, S. Chintala, Pytorch:\nAn imperative",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "style,\nhigh-performance\ndeep\nlearning\nlibrary, ArXiv",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "abs/1912.01703 (2019).",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:202786778",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "[43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark,",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "G. Krueger,\nI. Sutskever, Learning\ntransferable\nvisual",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "models\nfrom natural\nlanguage supervision,\nin:\nInterna-",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "tional Conference on Machine Learning, 2021.",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "URL\nhttps://api.semanticscholar.org/",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        },
        {
          "Proceedings of the 2015 ACM International Symposium": "CorpusID:231591445",
          "[44] G.\nIlharco, M. Wortsman, R. Wightman, C. Gordon,": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Z Yang",
        "D Yang",
        "C Dyer",
        "X He",
        "A Smola",
        "E Hovy"
      ],
      "year": "2016",
      "venue": "North American Chapter"
    },
    {
      "citation_id": "3",
      "title": "Bidirectional lstm with attention mechanism and convolutional layer for text classification",
      "authors": [
        "G Liu",
        "J Guo"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "4",
      "title": "Multisentinet: A deep semantic network for multimodal sentiment analysis",
      "authors": [
        "N Xu",
        "W Mao"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "5",
      "title": "Image-text multimodal emotion classification via multi-view attentional network",
      "authors": [
        "X Yang",
        "S Feng",
        "D Wang",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors"
    },
    {
      "citation_id": "7",
      "title": "Desire, belief, and semantic composition: variation in mood selection with desire predicates",
      "authors": [
        "P Portner",
        "A Rubinstein"
      ],
      "year": "2020",
      "venue": "Natural Language Semantics"
    },
    {
      "citation_id": "8",
      "title": "The psychology of desire",
      "authors": [
        "W Hofmann",
        "L Nordgren"
      ],
      "year": "2015",
      "venue": "The psychology of desire"
    },
    {
      "citation_id": "9",
      "title": "Multimodal sentiment detection based on multi-channel graph neural networks",
      "authors": [
        "X Yang",
        "S Feng",
        "Y Zhang",
        "D Wang"
      ],
      "year": "2021",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Multimodal emotion classification with multi-level semantic reasoning network",
      "authors": [
        "T Zhu",
        "L Li",
        "J Yang",
        "S Zhao",
        "X Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Thumbs up? sentiment classification using machine learning techniques",
      "authors": [
        "B Pang",
        "L Lee",
        "S Vaithyanathan"
      ],
      "year": "2002",
      "venue": "Thumbs up? sentiment classification using machine learning techniques"
    },
    {
      "citation_id": "12",
      "title": "Improved semantic representations from tree-structured long short-term memory networks",
      "authors": [
        "K Tai",
        "R Socher",
        "C Manning"
      ],
      "year": "2015",
      "venue": "Improved semantic representations from tree-structured long short-term memory networks"
    },
    {
      "citation_id": "13",
      "title": "Sentiment analysis on the impact of coronavirus in social life using the bert model",
      "authors": [
        "M Singh",
        "A Jakhar",
        "S Pandey"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "14",
      "title": "Lexicon-based methods for sentiment analysis",
      "authors": [
        "M Taboada",
        "J Brooke",
        "M Tofiloski",
        "K Voll",
        "M Stede"
      ],
      "year": "2011",
      "venue": "Computational linguistics"
    },
    {
      "citation_id": "15",
      "title": "Citius: A naive-bayes strategy for sentiment analysis on english tweets",
      "authors": [
        "P Gamallo",
        "M Garcia"
      ],
      "year": "2014",
      "venue": "Proceedings of the 8th international Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "16",
      "title": "Attention-based lstm for aspect-level sentiment classification",
      "authors": [
        "Y Wang",
        "M Huang",
        "X Zhu",
        "L Zhao"
      ],
      "year": "2016",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining"
    },
    {
      "citation_id": "18",
      "title": "A co-memory network for multimodal sentiment analysis",
      "authors": [
        "N Xu",
        "W Mao",
        "G Chen"
      ],
      "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (2018)"
    },
    {
      "citation_id": "19",
      "title": "Attention-based modality-gated networks for image-text sentiment analysis",
      "authors": [
        "F Huang",
        "K Wei",
        "J Weng",
        "Z Li"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "20",
      "title": "Multimodal sentiment analysis to explore the structure of emotions",
      "authors": [
        "A Hu",
        "S Flaxman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "21",
      "title": "Clmlf: A contrastive learning and multi-layer fusion method for multimodal sentiment detection",
      "authors": [
        "Z Li",
        "B Xu",
        "C Zhu",
        "T Zhao"
      ],
      "year": "2022",
      "venue": "Clmlf: A contrastive learning and multi-layer fusion method for multimodal sentiment detection"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Multimodal emotion recognition based on manifold learning and convolution neural network",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "24",
      "title": "Multi-label multimodal emotion recognition with transformer-based fusion and emotion-level representation learning",
      "authors": [
        "H.-D Le",
        "G Lee",
        "S Kim",
        "S Kim",
        "H.-J Yang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "Ld-man: Layout-driven multimodal attention network for online news sentiment recognition",
      "authors": [
        "W Guo",
        "Y Zhang",
        "X Cai",
        "L Meng",
        "J Yang",
        "X Yuan"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "A hybrid latent space data fusion method for multimodal emotion recognition",
      "authors": [
        "S Nemati",
        "R Rohani",
        "M Basiri",
        "M Abdar",
        "N Yen",
        "V Makarenkov"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Analyzing multimodal public sentiment based on hierarchical semantic attentional network",
      "authors": [
        "N Xu"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Intelligence and Security Informatics (ISI"
    },
    {
      "citation_id": "28",
      "title": "URL",
      "venue": "URL"
    },
    {
      "citation_id": "29",
      "title": "The graph neural network model",
      "authors": [
        "F Scarselli",
        "M Gori",
        "A Tsoi",
        "M Hagenbuchner",
        "G Monfardini"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "30",
      "title": "Social relation recognition from videos via multi-scale spatial-temporal reasoning",
      "authors": [
        "X Liu",
        "W Liu",
        "M Zhang",
        "J Chen",
        "L Gao",
        "C Yan",
        "T Mei"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "31",
      "title": "M3gat: A multi-modal, multi-task interactive graph attention network for conversational sentiment analysis and emotion recognition",
      "authors": [
        "Y Zhang",
        "A Jia",
        "B Wang",
        "P Zhang",
        "D Zhao",
        "P Li",
        "Y Hou",
        "X Jin",
        "D Song",
        "J Qin"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Information Systems"
    },
    {
      "citation_id": "32",
      "title": "Improving multimodal sentiment analysis by fusing contextual world knowledge",
      "authors": [
        "W Wang",
        "L Ding",
        "L Shen",
        "Y Luo",
        "H Hu",
        "D Tao",
        "Wisdom"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Doll'ar",
        "R Girshick"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "The desire model: Crossmodal emotion analysis and expression for robots",
      "authors": [
        "A Lim",
        "T Ogata",
        "H Okuno"
      ],
      "year": "2012",
      "venue": "Information Processing Society of Japan"
    },
    {
      "citation_id": "35",
      "title": "The common neural bases between sexual desire and love: a multilevel kernel density fmri analysis",
      "authors": [
        "S Cacioppo",
        "F Bianchi-Demicheli",
        "C Frum",
        "J Pfaus",
        "J Lewis"
      ],
      "year": "2012",
      "venue": "The journal of sexual medicine"
    },
    {
      "citation_id": "36",
      "title": "A meta-analysis of the relationship between curiosity and creativity",
      "authors": [
        "N Schutte",
        "J Malouff"
      ],
      "year": "2019",
      "venue": "The Journal of Creative Behavior"
    },
    {
      "citation_id": "37",
      "title": "Recognition of curiosity using eye movement analysis",
      "authors": [
        "S Hoppe",
        "T Loetscher",
        "S Morey",
        "A Bulling"
      ],
      "year": "2015",
      "venue": "Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and 12 Proceedings of the 2015 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "38",
      "title": "A data mining approach for desire and intention to participate in virtual communities",
      "authors": [
        "Ö Yavuz",
        "A Karahoca",
        "D Karahoca"
      ],
      "year": "2019",
      "venue": "International Journal of Electrical and Computer Engineering (IJECE"
    },
    {
      "citation_id": "39",
      "title": "Proceedings of the 1st conference of the Asia-Pacific chapter of the association for computational linguistics and the 10th international joint conference on natural language processing",
      "authors": [
        "D Chauhan",
        "S Dhanush",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st conference of the Asia-Pacific chapter of the association for computational linguistics and the 10th international joint conference on natural language processing"
    },
    {
      "citation_id": "40",
      "title": "Beyond emotion: A multi-modal dataset for human desire understanding",
      "authors": [
        "A Jia",
        "Y He",
        "Y Zhang",
        "S Uprety",
        "D Song",
        "C Lioma"
      ],
      "year": "2022",
      "venue": "North American Chapter"
    },
    {
      "citation_id": "41",
      "title": "Mmtf-des: A fusion of multimodal transformer models for desire, emotion, and sentiment analysis of social media data",
      "authors": [
        "A Aziz",
        "N Chowdhury",
        "M Kabir",
        "A Chy",
        "M Siddique"
      ],
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "42",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "43",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Köpf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, high-performance deep learning library"
    },
    {
      "citation_id": "44",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "45",
      "title": "Openclip, if you use this software, please cite it as below",
      "authors": [
        "G Ilharco",
        "M Wortsman",
        "R Wightman",
        "C Gordon",
        "N Carlini",
        "R Taori",
        "A Dave",
        "V Shankar",
        "H Namkoong",
        "J Miller",
        "H Hajishirzi",
        "A Farhadi",
        "L Schmidt"
      ],
      "year": "2021",
      "venue": "Openclip, if you use this software, please cite it as below",
      "doi": "10.5281/zenodo.5143773"
    },
    {
      "citation_id": "46",
      "title": "International Conference on Learning Representations",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "47",
      "title": "Bertweet: A pre-trained language model for english tweets",
      "authors": [
        "D Nguyen",
        "T Vu",
        "-T Nguyen"
      ],
      "year": "2020",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "48",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "49",
      "title": "Masked image modeling with local multi-scale reconstruction",
      "authors": [
        "H Wang",
        "Y Tang",
        "Y Wang",
        "J Guo",
        "Z Deng",
        "K Han"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}