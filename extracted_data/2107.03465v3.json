{
  "paper_id": "2107.03465v3",
  "title": "An Audiovisual And Contextual Approach For Categorical And Continuous Emotion Recognition In-The-Wild",
  "published": "2021-07-07T20:13:17Z",
  "authors": [
    "Panagiotis Antoniadis",
    "Ioannis Pikoulis",
    "Panagiotis P. Filntisis",
    "Petros Maragos"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work we tackle the task of video-based audiovisual emotion recognition, within the premises of the 2nd Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions, head/body orientation and low image resolution constitute factors that can potentially hinder performance in case of methodologies that solely rely on the extraction and analysis of facial features. In order to alleviate this problem, we leverage both bodily and contextual features, as part of a broader emotion recognition framework. We choose to use a standard CNN-RNN cascade as the backbone of our proposed model for sequence-to-sequence (seq2seq) learning. Apart from learning through the RGB input modality, we construct an aural stream which operates on sequences of extracted mel-spectrograms. Our extensive experiments on the challenging and newly assembled Aff-Wild2 dataset verify the validity of our intuitive multi-stream and multi-modal approach towards emotion recognition \"in-the-wild\". Emphasis is being laid on the the beneficial influence of the human body and scene context, as aspects of the emotion recognition process that have been left relatively unexplored up to this point. All the code was implemented using PyTorch 1 and is publicly available 2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic affect recognition constitutes a subject of rigorous studies across several scientific disciplines and bears immense practical importance as it has extensive applications in environments that involve human-robot cooperation, sociable robotics, medical treatment, psychiatric patient surveillance and many other human-computer interaction scenarios.\n\nRepresenting human emotions has been a basic topic of research in psychology. While the cultural and ethnic background of a person can affect their expressive style, Ekman indicated that humans perceive certain basic emotions in the same way regardless of their culture  [10, 9] . These six universal facial expressions (happiness, sadness, surprise, fear, disgust and anger) constitute the categorical model. Contempt was subsequently added as one of the basic emotions  [29] . Due to its direct and intuitive definition of facial expressions, the categorical model is used in the majority of emotion recognition algorithms  [16, 12, 42, 1]  and largescale databases (MMI  [34] , AFEW  [8] , FER-Wild  [32] , etc). However, the subjectivity and ambiguity of restricting human emotion to discrete categories result in large intraclass variations and small inter-class differences.\n\nRecently, the dimensional model proposed by Russell  [37]  has gained a lot of attention where emotion is described using a set of two latent dimensions that are valence (how pleasant or unpleasant a feeling is) and arousal (how likely is the person to take action under the emotional state). Another dimension called dominance is used sometimes to know whether the person is controlling the situation or not. Since a continuous representation can distinguish between subtly different displays of affect and encode small changes in the intensity, some recent algorithms  [33, 4, 22]  and databases (Aff-Wild  [39, 21] , Aff-Wild2  [22] , OMG-Emotion  [2] , AFEW-VA  [24] , etc) have utilized the dimensional model for uncontrolled emotion recognition. Even so, predicting a 2-dimensional continuous value instead of a category increases the task complexity by a lot and lacks intuitiveness.\n\nThe remainder of the paper is structured as follows: Firstly, we provide an overview of the latest and most notable related work in the domain of video-based emotion recognition in-the-wild. Subsequently, we analyze our proposed model architecture. Next, we present our experimental results on the Aff-Wild2 dataset, followed by conclusive remarks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition has been extensively studied for many years using different representations of human emotion, like basic facial expressions, action units and valencearousal.\n\nRecently, many studies have tried to leverage all emotion representations and jointly learn these three facial behavior tasks. Kollias et al.  [19]  proposed the FaceBehaviorNet, the first study that considered joint-learning of all facial behaviour tasks, in a single holistic framework. They utilized many publicly available emotion databases and proposed two strategies for coupling the tasks during training. Later, Kollias et al. released the Aff-Wild2 dataset  [22] , the first large scale in-the-wild database containing annotations for all three main behavior tasks. They also proposed multitask learning models that employ both visual and audio modalities and suggested using the ArcFace loss  [7]  for expression recognition. In an additional work, Kollias et al.  [23, 20]  studied the problem of non-overlapping annotations in multitask learning datasets. They explored task-relatedness and proposed a novel distribution matching approach, in which knowledge exchange is enabled between tasks, via matching of their predictions' distributions. Last year, the First Affective Behavior Analysis in-the-wild (ABAW) Competition  [18]  was held in conjunction with the IEEE Conference on Face and Gesture Recognition 2020. The competition contributed in advancing the state-of-the-art methods on the dimensional, categorical and facial action unit analysis and recognition on the basis of the Aff-Wild2 dataset.\n\nAlthough, the aforementioned methodologies boast relatively high recognition scores on the premise of facial expression analysis, they often neglect the usage of other supplementary sources of affective information, such as the body and context. Related works  [25, 30, 11, 36]  from the field of context-based visual emotion recognition, follow a more holistic approach towards solving the \"in-the-wild\" version of the current problem. Kosti et al.  [25]  introduced the EMotions In Context (EMOTIC) dataset, the first largescale image database for context-based emotion recognition, annotated on the basis of an extended emotional corpus with 26 discrete categories and VAD dimensions. A baseline model was also provided, consisting of two Con-vNet feature extractors (one for each of the body and context input streams) and one fusion network. Mittal et al.  [30]  surpassed baseline performance by fusing multiple input modalities, including the face, pose, inter-agent interactions and socio-dynamic context, effectively forming the EmotiCon framework. Furthermore, researchers have extended the concept of context-based visual emotion recognition in the dynamic setting of video sequences. Filntisis et al.  [11]  improved upon the baseline performance of  [28]  relative to the Body Language Dataset (BoLD) by incorporating a contextual feature encoding branch and a visual-semantic embedding loss based on Global Vectors (GloVe)  [35]  word embeddings. Lastly, Pikoulis et al.  [36]  achieved state-of-the-art performance on BoLD through an intuitive approach that included the use of scene and attribute characteristics as well as multi-stream optical flow.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "A complete schematic diagram of our proposed model is shown in Fig.  1 . Firstly, we will present the structure of the sub-network regarding the RGB visual modality, along with our proposed extensions for the enhancement of emotion understanding. Next, we will do the same for the aural model, and finally we will present the unified audio-visual architecture.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Seq2Seq",
      "text": "In order to leverage temporal information and emotion labels throughout each video, our method takes as input sequences of frames that contain either visual or aural information (which will be described in the next subsections). After extracting intermediate feature representations for each sequence of frames, we use a standard LSTM (Long Short-Term Memory)  [14]  model in order to map the extracted feature sequences features to their respective output labels.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Visual Model",
      "text": "A single RGB image usually encodes static appearance at a specific point in time but lacks the contextual information about previous and next frames. We aspire to enhance the descriptive capacity of the extracted deep visual embeddings through the feature level combination of multiple feature extractors that focus on different parts of the human instance. In our implementation, we utilize the human face as our primary source of affective information, while we also make use of the body and surrounding depicted environment, in a supplementary manner. For all of the subsequent convolutional branches, we use a standard 50-layer ResNet  [13] , as our feature extractor backbone. The ResNet-50 variant produces 2048-dim deep feature vector representations for each given input image. All ConvNet backbones are pre-trained using various task-specific datasets. As it will be discussed later on, pre-training constitutes the main differentiating factor among the multiple ConvNet backbones that comprise our visual model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Face",
      "text": "The face is commonly perceived as the window to the human soul and the most expressive source of visual affective information. We introduce an input stream which explicitly operates on the aligned face crops of the primary depicted human agents. The localization, extraction and alignment of The ConvNet backbone of the face branch receives manual pre-training on AffectNet  [31]  which constitutes the largest facial expression database, containing over 1M images, annotated on both categorical and dimensional level. We pre-trained the face branch on AffectNet for 5 epochs using a batch size of 64 and a learning rate of 0.001 achieving 64.09% validation accuracy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Context",
      "text": "We incorporate a context stream in the form of RGB frames whose primary depicted agents have been masked out. For the acquisition of the masks we use body bounding boxes and multiply them element-wise with a constant value of zero. Prior to the acquisition of the body bounding boxes and the corresponding masks, we calculate the 2D coordinates for 25 joints of the body of the primary depicted agent using the BODY25 model of the OpenPose  [3]  publicly available toolkit. Let {(x\n\nn=1 be the detected set of horizontal and vertical joint coordinates of a given agent, at frame t. The bounding box of the agent bbox agent within a given image I of height H and width W , is calculated as follows:\n\nAll joints with a detection confidence score that is less than 10%, are discarded. The masked image I mask for a given input image I is calculated as follows:\n\nwhere the tuple (i, j) corresponds to all valid pixel locations. Throughout our experiments we set λ x = 0.1 and λ y = 0.25.\n\nContextual feature extraction is a scene-centric task, and therefore we choose to initialize the corresponding ConvNet backbone using the Places365-Standard  [43] , a large-scale database of photographs, labeled with scene semantic categories. The pre-trained model is publicly available 3  .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Body",
      "text": "We incorporate an additional input stream that focuses solely on encoding bodily expressions of emotion, with the aim of alleviating the problem of undetected or misaligned face crops. The contribution of the body branch in the emotion recognition process becomes more evident during frames where the corresponding face crops are not existent. The body stream operates either on the bounding of the depicted agents or the entire image. The ConvNet feature extractor is pre-trained on the object-centric ImageNet  [6]  database. The pre-trained weights are publicly available  4  .\n\nThe early feature fusion of all of the aforementioned input streams results in a 6144-dim concatenated feature vec-tor. Subsequently, the fused features are fed into a bidirectional, single-layer LSTM, with 512 hidden units, for further temporal modeling.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Aural Model",
      "text": "In the branch that incorporates audio information, starting with a sequence of input waveforms, we extract the mel-spectrogram representation of each. Then, we use a 18-layer ResNet model pretrained on the ImageNet dataset to extract a 512 feature vector for each input waveform. Finally, using an LSTM layer, we map the feature sequence to labels (either expression labels in the case of Track 2 or VA labels in the case of Track 1).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Functions",
      "text": "For Track 1 of the ABAW competition, we use both a standard mean-squared error L mse as well as loss term based on the concordance correlation coefficient (CCC). The latter is defined as:\n\nwhere s x and s y denote the variance of the predicted and ground truth values respectively, x and ȳ are the corresponding mean values and s xy is the respective covariance value. The range of CCC is from -1 (perfect disagreement) to 1 (perfect agreement). Hence, in our case we define L ccc as:\n\nwhere ρ v and ρ a are the respective CCC of valence and arousal.\n\nFor Track 2, we use a standard cross-entropy function L ce . We also enforce semantic congruity between the extracted visual embeddings and the categorical label word embeddings from a 300-dim GloVe  [35]  model, pre-trained on Wikipedia and Gigaword 5 data, in the same manner as in  [11] . More specifically, given an input sample x, we transform the concatenated visual embeddings f v (x) into the same dimensionality as the word embeddings f t (y) through a linear transformation W emb , with y being the ground truth target label. We later apply an MSE loss between the transformed visual embeddings and the word embeddings which correspond to the ground truth emotional label and denote this term as L emb :\n\nwhere • is the Iverson bracket and c is the class index. The whole network can be trained in an end-toend manner by minimizing the combined loss function L = L ce + λ emb L emb . For simplicity, we set λ emb = 1.0 during all of our subsequent experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Validation Set",
      "text": "Tables  1  and 2  present our results on the Aff-Wild2 validation set, for the Expression and Valence-Arousal subchallenges respectively, together with a performance comparison with the baseline and top entries from last year's ABAW competition  [18] . Method CCC-V CCC-A Total Baseline  [17]  0.23 0.21 0.22 NISL2020  [5]  0.335 0.515 0.425 ICT-VIPL  [41]  0.32 0.55 0.435 TNT  [26]  0 The letter F denotes a visual branch trained using only the cropped face in the input video, while BCF denotes the visual branch that incorporates both bodily as well as contextual features, as seen in Figure  1 . Audio denotes the results of the aural branch. In both tables we also include the results of the weighted average late score fusion among all different combinations of F, BCF, and Audio. We readily see that including the body and the context as additional, supplementary information results in a significant performance boost, especially in the case of the Expression subchallenge. Furthermore, the fusion of any two methods results in increased performance, when compared to the single branches. Finally, in both Tables, the fusion of all three",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Test Set",
      "text": "Table  3  presents an extensive performance comparison among the proposed methodologies of  [23] , the top entries of this year's ABAW2 competition as well as our own models.\n\nIt is quite evident that our proposed model, even though it manages to surpass the baseline  [17]  on the validation set by a large margin, it does not come close to the performance, exhibited by the top submissions  [5, 38, 15, 40] . There two main reasons that account for this observed behavior. Firstly, we presume that the distribution of the test set greatly differs from that of the validation set, leading to a significant difference in performance, relative to the two sets. Furthermore, we followed a more holistic and highlevel approach to the problem, whereas the aforementioned teams laid their emphasis explicitly on the facial stream and disregarded all other sources of affective information, such as the body, context or even the aural modality. In that way, the production of the best facial expression model basically lies outside the scope of the current analysis. However, through our experiments we verified the beneficial influence of both the body and context, as supplementary streams that encode valuable information, especially in the case when the applied face extraction and alignment algorithms fail to due to extreme head or body orientations, low illumination and image quality.\n\nMore specifically, on the Expression sub-challenge, the sole usage of the facial stream, led to a total score of 0.405, while the inclusion of both bodily and contextual features raised the total score to 0.423. With the introduction of the aural stream, our model reached a maximum of 0.437 on the test set. On the VA sub-challenge we notice a slightly different but similar behavior. The introduction of contextual and bodily features decreased the performance of our model significantly along the valence dimension but increased it along the arousal dimension. This can be justified by the fact that the latter is highly correlated with body motion while the former is dominated by facial expressions and AUs. The inclusion of the aural stream, once again led to a significant boost in overall performance, resulting in more than 10% increase in CCC-A and a total score of 0.416.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "We presented our method for the \"2nd Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW2)\" challenge. Apart from using only face-cropped images, we leverage both contextual (scene), bodily features, as well as the audio of the video, to further enhance our model's perception. Our results show that fusion of the different streams of information results in significant performance increase, when compared to both single streams, as well as previous best published results on the validation set. Even though our model does not come close to the performance of this year's top entries, we have successfully highlighted the beneficial influence of the body and context input streams in the emotion recognition process, and as a result, propose our multi-stream and multi-modal approach as a potential way to further improve methodologies that solely rely on the analysis of facial expressions.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Firstly, we will present the structure of",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of our proposed method.",
      "page": 3
    },
    {
      "caption": "Figure 1: Audio denotes the re-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "Abstract",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "Representing human emotions has been a basic topic of"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "research in psychology. While the cultural and ethnic back-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "In this work we tackle the task of video-based audio-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "ground of a person can affect their expressive style, Ekman"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "visual emotion recognition, within the premises of\nthe 2nd",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "indicated that humans perceive certain basic emotions in the"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "Workshop and Competition on Affective Behavior Anal-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "same way regardless of their culture [10, 9]. These six uni-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "ysis\nin-the-wild (ABAW2).\nPoor\nillumination conditions,",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "versal facial expressions (happiness, sadness, surprise, fear,"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "head/body orientation and low image resolution constitute",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "disgust and anger) constitute the categorical model. Con-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "factors that can potentially hinder performance in case of",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "tempt was subsequently added as one of the basic emotions"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "methodologies that solely rely on the extraction and analy-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "[29]. Due to its direct and intuitive deﬁnition of facial ex-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "sis of facial features.\nIn order to alleviate this problem, we",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "pressions,\nthe categorical model\nis used in the majority of"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "leverage both bodily and contextual\nfeatures, as part of a",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "emotion recognition algorithms [16, 12, 42, 1] and large-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "broader emotion recognition framework. We choose to use",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "scale databases\n(MMI\n[34], AFEW [8], FER-Wild [32],"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "a standard CNN-RNN cascade as the backbone of our pro-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "etc). However, the subjectivity and ambiguity of restricting"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "posed model\nfor sequence-to-sequence (seq2seq) learning.",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "human emotion to discrete categories result\nin large intra-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "Apart\nfrom learning through the RGB input modality, we",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "class variations and small inter-class differences."
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "construct an aural stream which operates on sequences of",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": ""
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "extracted mel-spectrograms. Our extensive experiments on",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "Recently,\nthe dimensional model proposed by Russell"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "the challenging and newly assembled Aff-Wild2 dataset ver-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "[37] has gained a\nlot of\nattention where\nemotion is de-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "ify the validity of our intuitive multi-stream and multi-modal",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "scribed using a set of\ntwo latent dimensions\nthat are va-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "approach towards emotion recognition “in-the-wild”. Em-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "lence (how pleasant or unpleasant a feeling is) and arousal"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "phasis is being laid on the the beneﬁcial\ninﬂuence of\nthe",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "(how likely is the person to take action under the emotional"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "human body and scene\ncontext,\nas aspects of\nthe\nemo-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "state). Another dimension called dominance is used some-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "tion recognition process that have been left relatively un-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "times to know whether the person is controlling the situation"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "explored up to this point.\nAll\nthe code was implemented",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "or not.\nSince a continuous representation can distinguish"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "using PyTorch1 and is publicly available2.",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "between subtly different displays of affect and encode small"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "changes in the intensity, some recent algorithms [33, 4, 22]"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "and databases (Aff-Wild [39, 21], Aff-Wild2 [22], OMG-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "Emotion [2], AFEW-VA [24], etc) have utilized the dimen-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "1. Introduction",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": ""
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "sional model\nfor uncontrolled emotion recognition.\nEven"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "Automatic affect recognition constitutes a subject of rig-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "so, predicting a 2-dimensional continuous value instead of"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "orous studies across several scientiﬁc disciplines and bears",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "a category increases the task complexity by a lot and lacks"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "immense practical\nimportance as it has extensive applica-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "intuitiveness."
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "tions\nin environments\nthat\ninvolve human-robot coopera-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": ""
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "tion,\nsociable robotics, medical\ntreatment, psychiatric pa-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "The\nremainder of\nthe paper\nis\nstructured as\nfollows:"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "tient surveillance and many other human-computer interac-",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "Firstly, we provide an overview of the latest and most no-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "tion scenarios.",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "table related work in the domain of video-based emotion"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "recognition in-the-wild. Subsequently, we analyze our pro-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "*Equal contribution",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": ""
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "posed model architecture. Next, we present our experimen-"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "1https://pytorch.org/",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": ""
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "2https://github.com/PanosAntoniadis/",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "tal results on the Aff-Wild2 dataset, followed by conclusive"
        },
        {
          "{pantoniadis97,\npikoulis.giannis}@gmail.com,": "NTUA-ABAW2021",
          "filby@central.ntua.gr,\nmaragos@cs.ntua.gr": "remarks."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "[35] word embeddings. Lastly, Pikoulis et al. [36] achieved"
        },
        {
          "2. Related Work": "Emotion recognition has been extensively studied for",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "state-of-the-art performance on BoLD through an intuitive"
        },
        {
          "2. Related Work": "many years using different representations of human emo-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "approach that\nincluded the use of scene and attribute char-"
        },
        {
          "2. Related Work": "tion, like basic facial expressions, action units and valence-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "acteristics as well as multi-stream optical ﬂow."
        },
        {
          "2. Related Work": "arousal.",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "Recently, many studies have tried to leverage all emotion",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "3. Method"
        },
        {
          "2. Related Work": "representations and jointly learn these three facial behavior",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "tasks. Kollias et al.\n[19] proposed the FaceBehaviorNet,",
          "semantic embedding loss based on Global Vectors (GloVe)": "A complete schematic diagram of our proposed model is"
        },
        {
          "2. Related Work": "the ﬁrst study that considered joint-learning of all facial be-",
          "semantic embedding loss based on Global Vectors (GloVe)": "shown in Fig.\n1.\nFirstly, we will present\nthe structure of"
        },
        {
          "2. Related Work": "haviour tasks, in a single holistic framework. They utilized",
          "semantic embedding loss based on Global Vectors (GloVe)": "the sub-network regarding the RGB visual modality, along"
        },
        {
          "2. Related Work": "many publicly available emotion databases and proposed",
          "semantic embedding loss based on Global Vectors (GloVe)": "with our proposed extensions for the enhancement of emo-"
        },
        {
          "2. Related Work": "two strategies for coupling the tasks during training. Later,",
          "semantic embedding loss based on Global Vectors (GloVe)": "tion understanding. Next, we will do the same for the aural"
        },
        {
          "2. Related Work": "Kollias et al.\nreleased the Aff-Wild2 dataset [22],\nthe ﬁrst",
          "semantic embedding loss based on Global Vectors (GloVe)": "model, and ﬁnally we will present\nthe uniﬁed audio-visual"
        },
        {
          "2. Related Work": "large scale in-the-wild database containing annotations for",
          "semantic embedding loss based on Global Vectors (GloVe)": "architecture."
        },
        {
          "2. Related Work": "all three main behavior tasks. They also proposed multitask",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "3.1. Seq2Seq"
        },
        {
          "2. Related Work": "learning models that employ both visual and audio modali-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "ties and suggested using the ArcFace loss [7] for expression",
          "semantic embedding loss based on Global Vectors (GloVe)": "In order\nto leverage temporal\ninformation and emotion"
        },
        {
          "2. Related Work": "recognition.\nIn an additional work, Kollias et al.\n[23, 20]",
          "semantic embedding loss based on Global Vectors (GloVe)": "labels\nthroughout each video, our method takes as\ninput"
        },
        {
          "2. Related Work": "studied the problem of non-overlapping annotations in mul-",
          "semantic embedding loss based on Global Vectors (GloVe)": "sequences of\nframes that contain either visual or aural\nin-"
        },
        {
          "2. Related Work": "titask learning datasets. They explored task-relatedness and",
          "semantic embedding loss based on Global Vectors (GloVe)": "formation (which will be described in the next\nsubsec-"
        },
        {
          "2. Related Work": "proposed a novel distribution matching approach,\nin which",
          "semantic embedding loss based on Global Vectors (GloVe)": "tions). After extracting intermediate feature representations"
        },
        {
          "2. Related Work": "knowledge exchange is enabled between tasks, via match-",
          "semantic embedding loss based on Global Vectors (GloVe)": "for\neach sequence of\nframes, we use\na\nstandard LSTM"
        },
        {
          "2. Related Work": "ing of\ntheir predictions’ distributions. Last year,\nthe First",
          "semantic embedding loss based on Global Vectors (GloVe)": "(Long Short-Term Memory) [14] model in order to map the"
        },
        {
          "2. Related Work": "Affective Behavior Analysis in-the-wild (ABAW) Competi-",
          "semantic embedding loss based on Global Vectors (GloVe)": "extracted feature sequences features to their respective out-"
        },
        {
          "2. Related Work": "tion [18] was held in conjunction with the IEEE Conference",
          "semantic embedding loss based on Global Vectors (GloVe)": "put labels."
        },
        {
          "2. Related Work": "on Face and Gesture Recognition 2020.\nThe competition",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "3.2. Visual Model"
        },
        {
          "2. Related Work": "contributed in advancing the state-of-the-art methods on the",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "dimensional, categorical and facial action unit analysis and",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "A single RGB image usually encodes static appearance"
        },
        {
          "2. Related Work": "recognition on the basis of the Aff-Wild2 dataset.",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "at a speciﬁc point in time but lacks the contextual informa-"
        },
        {
          "2. Related Work": "Although,\nthe aforementioned methodologies boast rel-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "tion about previous and next frames. We aspire to enhance"
        },
        {
          "2. Related Work": "atively high recognition scores on the premise of facial ex-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "the descriptive capacity of the extracted deep visual embed-"
        },
        {
          "2. Related Work": "pression analysis, they often neglect the usage of other sup-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "dings through the feature level combination of multiple fea-"
        },
        {
          "2. Related Work": "plementary sources of affective information,\nsuch as\nthe",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "ture extractors that focus on different parts of the human in-"
        },
        {
          "2. Related Work": "body and context. Related works [25, 30, 11, 36] from the",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "stance. In our implementation, we utilize the human face as"
        },
        {
          "2. Related Work": "ﬁeld of context-based visual emotion recognition, follow a",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "our primary source of affective information, while we also"
        },
        {
          "2. Related Work": "more holistic approach towards\nsolving the “in-the-wild”",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "make use of\nthe body and surrounding depicted environ-"
        },
        {
          "2. Related Work": "version of the current problem. Kosti et al.\n[25] introduced",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "ment, in a supplementary manner. For all of the subsequent"
        },
        {
          "2. Related Work": "the EMotions In Context (EMOTIC) dataset, the ﬁrst large-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "convolutional branches, we use a standard 50-layer ResNet"
        },
        {
          "2. Related Work": "scale image database for context-based emotion recogni-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "[13],\nas our\nfeature extractor backbone.\nThe ResNet-50"
        },
        {
          "2. Related Work": "tion,\nannotated\non\nthe\nbasis\nof\nan\nextended\nemotional",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "variant produces 2048-dim deep feature vector representa-"
        },
        {
          "2. Related Work": "corpus with 26 discrete categories and VAD dimensions.",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "tions for each given input\nimage. All ConvNet backbones"
        },
        {
          "2. Related Work": "A baseline model was also provided, consisting of two Con-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "are pre-trained using various task-speciﬁc datasets. As it"
        },
        {
          "2. Related Work": "vNet feature extractors (one for each of the body and con-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "will be discussed later on, pre-training constitutes the main"
        },
        {
          "2. Related Work": "text\ninput streams) and one fusion network. Mittal et al.",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "differentiating factor among the multiple ConvNet back-"
        },
        {
          "2. Related Work": "[30] surpassed baseline performance by fusing multiple in-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "bones that comprise our visual model."
        },
        {
          "2. Related Work": "put modalities,\nincluding the face, pose,\ninter-agent\ninter-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "actions and socio-dynamic context, effectively forming the",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "",
          "semantic embedding loss based on Global Vectors (GloVe)": "3.2.1\nFace"
        },
        {
          "2. Related Work": "EmotiCon framework.\nFurthermore,\nresearchers have ex-",
          "semantic embedding loss based on Global Vectors (GloVe)": ""
        },
        {
          "2. Related Work": "tended the concept of context-based visual emotion recog-",
          "semantic embedding loss based on Global Vectors (GloVe)": "The face is commonly perceived as the window to the hu-"
        },
        {
          "2. Related Work": "nition in the dynamic setting of video sequences. Filntisis",
          "semantic embedding loss based on Global Vectors (GloVe)": "man soul and the most expressive source of visual affective"
        },
        {
          "2. Related Work": "et al.\n[11] improved upon the baseline performance of [28]",
          "semantic embedding loss based on Global Vectors (GloVe)": "information. We introduce an input stream which explicitly"
        },
        {
          "2. Related Work": "relative to the Body Language Dataset (BoLD) by incorpo-",
          "semantic embedding loss based on Global Vectors (GloVe)": "operates on the aligned face crops of the primary depicted"
        },
        {
          "2. Related Work": "rating a contextual\nfeature encoding branch and a visual-",
          "semantic embedding loss based on Global Vectors (GloVe)": "human agents. The localization, extraction and alignment of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1. Overview of our proposed method.": "face regions per frame has been carried out by the ofﬁcial"
        },
        {
          "Figure 1. Overview of our proposed method.": "distributors of the Aff-Wild2 dataset priorly. During frames"
        },
        {
          "Figure 1. Overview of our proposed method.": "where face detection and alignment has failed and the cor-"
        },
        {
          "Figure 1. Overview of our proposed method.": "responding face crops are missing, we feed the ConvNet"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "feature extractor with an input\ntensor of appropriate size,"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "ﬁlled with zeros."
        },
        {
          "Figure 1. Overview of our proposed method.": "The ConvNet backbone of the face branch receives man-"
        },
        {
          "Figure 1. Overview of our proposed method.": "ual pre-training on AffectNet\n[31] which constitutes\nthe"
        },
        {
          "Figure 1. Overview of our proposed method.": "largest facial expression database, containing over 1M im-"
        },
        {
          "Figure 1. Overview of our proposed method.": "ages, annotated on both categorical and dimensional\nlevel."
        },
        {
          "Figure 1. Overview of our proposed method.": "We pre-trained the face branch on AffectNet\nfor 5 epochs"
        },
        {
          "Figure 1. Overview of our proposed method.": "using a batch size of 64 and a learning rate of 0.001 achiev-"
        },
        {
          "Figure 1. Overview of our proposed method.": "ing 64.09% validation accuracy."
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "3.2.2\nContext"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "We incorporate a context stream in the form of RGB frames"
        },
        {
          "Figure 1. Overview of our proposed method.": "whose primary depicted agents have been masked out. For"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "the acquisition of the masks we use body bounding boxes"
        },
        {
          "Figure 1. Overview of our proposed method.": "and multiply them element-wise with a constant value of"
        },
        {
          "Figure 1. Overview of our proposed method.": "zero. Prior to the acquisition of the body bounding boxes"
        },
        {
          "Figure 1. Overview of our proposed method.": "and the corresponding masks, we calculate the 2D coor-"
        },
        {
          "Figure 1. Overview of our proposed method.": "dinates for 25 joints of\nthe body of\nthe primary depicted"
        },
        {
          "Figure 1. Overview of our proposed method.": "agent using the BODY25 model of the OpenPose [3] pub-"
        },
        {
          "Figure 1. Overview of our proposed method.": ")}25\nlicly available toolkit. Let {(x(t)\nn , y(t)\nn=1 be the detected"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "set of horizontal and vertical\njoint coordinates of a given"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "agent, at frame t. The bounding box of the agent bboxagent"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "within a given image I of height H and width W , is calcu-"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "lated as follows:"
        },
        {
          "Figure 1. Overview of our proposed method.": ""
        },
        {
          "Figure 1. Overview of our proposed method.": "z(t)\nz(t)\n),\nz ≡ x or z ≡ y\nez = λz(max"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "to extract a 512 feature vector for each input waveform. Fi-"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "nally, using an LSTM layer, we map the feature sequence to"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "labels (either expression labels in the case of Track 2 or VA"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "labels in the case of Track 1)."
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "3.4. Loss Functions"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "For Track 1 of\nthe ABAW competition, we use both"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "a standard mean-squared error Lmse as well as loss term"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "based on the concordance correlation coefﬁcient"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "The latter is deﬁned as:"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "2sxy"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "ρc ="
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "s2\nx + s2\ny + (¯x − ¯y)2"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": ""
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "the predicted and\nwhere sx and sy denote the variance of"
        },
        {
          "18-layer ResNet model pretrained on the ImageNet dataset": "ground truth values respectively, ¯x and ¯y are the correspond-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "CCC-V"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.23"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.335"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.32"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.493"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.243"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.330"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.344"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.358"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.366"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.382"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": "0.386"
        },
        {
          "Table 2. Results on the Aff-Wild2 validation set, for the Valence-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "tional, single-layer LSTM, with 512 hidden units,\nfor\nfur-",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "4.1. Validation Set"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "ther temporal modeling.",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Tables 1 and 2 present our results on the Aff-Wild2 val-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "3.3. Aural Model",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "idation set,\nfor\nthe Expression and Valence-Arousal\nsub-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "In the branch that\nincorporates audio information, start-",
          "4. Experimental Results": "challenges respectively,\ntogether with a performance com-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "ing with a sequence of\ninput waveforms, we extract\nthe",
          "4. Experimental Results": "parison with the baseline and top entries from last year’s"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "mel-spectrogram representation of each.\nThen, we use a",
          "4. Experimental Results": "ABAW competition [18]."
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "18-layer ResNet model pretrained on the ImageNet dataset",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Table 1. Results on the Aff-Wild2 validation set,\nfor the Expres-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "to extract a 512 feature vector for each input waveform. Fi-",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "sion sub-challenge."
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "nally, using an LSTM layer, we map the feature sequence to",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Method\nAccuracy\nTotal\nF1 Score"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "labels (either expression labels in the case of Track 2 or VA",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Baseline [17]\n0.30\n0.50\n0.366"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "labels in the case of Track 1).",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "NISL2020 [5]\n-\n-\n0.493"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "3.4. Loss Functions",
          "4. Experimental Results": "ICT-VIPL [27]\n0.33\n0.64\n0.434"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "TNT [26]\n-\n-\n0.546"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "For Track 1 of\nthe ABAW competition, we use both",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Audio\n0.375\n0.495\n0.415"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "a standard mean-squared error Lmse as well as loss term",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Visual (F)\n0.453\n0.584\n0.496"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "based on the concordance correlation coefﬁcient\n(CCC).",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "0.640\nVisual (BCF)\n0.517\n0.558"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "The latter is deﬁned as:",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "0.532\n0.567\n0.639\nVisual (BCF) + Lemb"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Visual (F + BCF)\n0.543\n0.657\n0.580"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "2sxy",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "(3)\nρc =",
          "4. Experimental Results": "Audio + Visual (F)\n0.519\n0.645\n0.561"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "s2\nx + s2\ny + (¯x − ¯y)2",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Audio + Visual (BCF)\n0.536\n0.654\n0.575"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "the predicted and\nwhere sx and sy denote the variance of",
          "4. Experimental Results": "0.555\n0.668\n0.592\nAudio + Visual (F + BCF)"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "ground truth values respectively, ¯x and ¯y are the correspond-",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "ing mean values and sxy is the respective covariance value.",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Table 2. Results on the Aff-Wild2 validation set, for the Valence-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "The range of CCC is from -1 (perfect disagreement)\nto 1",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Arousal sub-challenge."
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "(perfect agreement). Hence, in our case we deﬁne Lccc as:",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Method\nCCC-V\nCCC-A\nTotal"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "ρv + ρa",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Baseline [17]\n0.23\n0.21\n0.22"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "(4)\nLccc = 1 −",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "2",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "NISL2020 [5]\n0.335\n0.515\n0.425"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "are the respective CCC of valence and\nwhere ρv\nand ρa",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "ICT-VIPL [41]\n0.32\n0.55\n0.435"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "arousal.",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "TNT [26]\n0.493\n0.613\n0.553"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "For Track 2, we use a standard cross-entropy function",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Audio\n0.243\n0.400\n0.322"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "Lce. We also enforce semantic congruity between the ex-",
          "4. Experimental Results": "Visual (F)\n0.330\n0.539\n0.435"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "tracted visual embeddings and the categorical\nlabel word",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "0.344\n0.550\n0.447\nVisual (BCF)"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "embeddings from a 300-dim GloVe [35] model, pre-trained",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Visual (F + BCF)\n0.358\n0.597\n0.478"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "on Wikipedia and Gigaword 5 data,\nin the same manner",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Audio + Visual (F)\n0.366\n0.582\n0.474"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "as\nin [11]. More speciﬁcally, given an input\nsample x,",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "Audio + Visual (BCF)\n0.382\n0.586\n0.484"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "we transform the concatenated visual embeddings fv(x)",
          "4. Experimental Results": "0.386\n0.616\n0.502\nAudio + Visual (F + BCF)"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "into the same dimensionality as the word embeddings ft(y)",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "through a linear\ntransformation Wemb, with y being the",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "The letter F denotes a visual branch trained using only"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "ground truth target\nlabel. We later apply an MSE loss be-",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "the cropped face in the input video, while BCF denotes the"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "tween the transformed visual embeddings and the word em-",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "visual branch that\nincorporates both bodily as well as con-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "beddings which correspond to the ground truth emotional",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "textual features, as seen in Figure 1. Audio denotes the re-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "label and denote this term as Lemb:",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "sults of the aural branch.\nIn both tables we also include the"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "results of the weighted average late score fusion among all"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "(cid:88) c\n2 2\ny = c\n(5)\nft(y)(cid:13)\nLemb = (cid:13)\n(cid:13)Wembfv(x) −",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "different combinations of F, BCF, and Audio. We readily"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "(cid:74)\n(cid:75)",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "",
          "4. Experimental Results": "see that\nincluding the body and the context as additional,"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "·\nc\nwhere\nis\nthe\nIverson\nbracket\nand\nis\nthe\nclass\nin-",
          "4. Experimental Results": "supplementary information results\nin a signiﬁcant perfor-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "(cid:74)\n(cid:75)",
          "4. Experimental Results": ""
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "dex.\nThe whole network can be\ntrained\nin an end-to-",
          "4. Experimental Results": "mance boost, especially in the case of the Expression sub-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "end manner\nby minimizing\nthe\ncombined\nloss\nfunction",
          "4. Experimental Results": "challenge. Furthermore,\nthe fusion of any two methods re-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "L = Lce + λembLemb.\nFor simplicity, we set λemb = 1.0",
          "4. Experimental Results": "sults in increased performance, when compared to the sin-"
        },
        {
          "tor. Subsequently,\nthe fused features are fed into a bidirec-": "during all of our subsequent experiments.",
          "4. Experimental Results": "gle branches. Finally, in both Tables, the fusion of all three"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: presents an extensive performance comparison",
      "data": [
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "models."
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": ""
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "Method"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": ""
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "MT-VGG [23]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "V-MT-VGG-GRU [23]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "A-MT-VGG-GRU [23]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "A/V-MT-VGG-GRU [23]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "NISL-2021 [5]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "Maybe Next Time [38]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "CPIC-DIR2021 [15]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "Netease Fuxi Virtual Human [40]"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "Visual (F)"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "Visual (BCF)"
        },
        {
          "Table 3. Results on the Aff-Wild2 test set relative to baseline methodologies, the top entries of the ABAW2 competition and our proposed": "Audio + Visual (F + BCF)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: presents an extensive performance comparison",
      "data": [
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "0.464\n0.416\nAudio + Visual (F + BCF)\n0.368",
          "0.329\n0.611\n0.423\n-\n-\n-": "0.337\n0.642\n0.437\n-\n-\n-"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "different models results in the best scores,\ni.e.\n0.592 for",
          "0.329\n0.611\n0.423\n-\n-\n-": "and bodily features decreased the performance of our model"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "the Expression sub-challenge and 0.502 for\nthe VA sub-",
          "0.329\n0.611\n0.423\n-\n-\n-": "signiﬁcantly along the valence dimension but\nincreased it"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "challenge, on the basis of the Aff-Wild2 validation set.",
          "0.329\n0.611\n0.423\n-\n-\n-": "along the arousal dimension. This can be justiﬁed by the"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "fact\nthat\nthe latter\nis highly correlated with body motion"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "4.2. Test Set",
          "0.329\n0.611\n0.423\n-\n-\n-": "while the former\nis dominated by facial expressions and"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "AUs. The inclusion of the aural stream, once again led to a"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "Table 3 presents an extensive performance comparison",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "signiﬁcant boost\nin overall performance, resulting in more"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "among the proposed methodologies of\n[23],\nthe\ntop en-",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "than 10% increase in CCC-A and a total score of 0.416."
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "tries of this year’s ABAW2 competition as well as our own",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "models.",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "5. Conclusion"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "It is quite evident that our proposed model, even though",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "it manages\nto surpass\nthe baseline [17] on the validation",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "We presented our method for\nthe “2nd Workshop and"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "set by a large margin,\nit does not come close to the per-",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "Competition on Affective Behavior Analysis\nin-the-wild"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "formance, exhibited by the top submissions [5, 38, 15, 40].",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "(ABAW2)” challenge. Apart from using only face-cropped"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "There two main reasons that account for this observed be-",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "images, we leverage both contextual\n(scene), bodily fea-"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "havior. Firstly, we presume that\nthe distribution of the test",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "tures, as well as the audio of the video,\nto further enhance"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "set greatly differs from that of the validation set, leading to",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "our model’s perception. Our results show that fusion of the"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "a signiﬁcant difference in performance,\nrelative to the two",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "different streams of\ninformation results in signiﬁcant per-"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "sets.\nFurthermore, we followed a more holistic and high-",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "formance increase, when compared to both single streams,"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "level approach to the problem, whereas the aforementioned",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "as well as previous best published results on the validation"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "teams laid their emphasis explicitly on the facial stream and",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "set. Even though our model does not come close to the per-"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "disregarded all other sources of affective information, such",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "formance of\nthis year’s\ntop entries, we have successfully"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "as the body, context or even the aural modality. In that way,",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "highlighted the beneﬁcial inﬂuence of the body and context"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "the production of the best facial expression model basically",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "input streams in the emotion recognition process, and as a"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "lies outside the scope of\nthe current analysis.\nHowever,",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "result, propose our multi-stream and multi-modal approach"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "through our experiments we veriﬁed the beneﬁcial inﬂuence",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "as a potential way to further\nimprove methodologies that"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "of both the body and context, as supplementary streams that",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "",
          "0.329\n0.611\n0.423\n-\n-\n-": "solely rely on the analysis of facial expressions."
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "encode valuable information, especially in the case when",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "the applied face extraction and alignment algorithms fail to",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "due to extreme head or body orientations,\nlow illumination",
          "0.329\n0.611\n0.423\n-\n-\n-": "References"
        },
        {
          "Visual (BCF)\n0.316\n0.352\n0.334": "and image quality.",
          "0.329\n0.611\n0.423\n-\n-\n-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: presents an extensive performance comparison",
      "data": [
        {
          "of both the body and context, as supplementary streams that": ""
        },
        {
          "of both the body and context, as supplementary streams that": "encode valuable information, especially in the case when"
        },
        {
          "of both the body and context, as supplementary streams that": "the applied face extraction and alignment algorithms fail to"
        },
        {
          "of both the body and context, as supplementary streams that": "due to extreme head or body orientations,\nlow illumination"
        },
        {
          "of both the body and context, as supplementary streams that": "and image quality."
        },
        {
          "of both the body and context, as supplementary streams that": ""
        },
        {
          "of both the body and context, as supplementary streams that": "More speciﬁcally, on the Expression sub-challenge,\nthe"
        },
        {
          "of both the body and context, as supplementary streams that": ""
        },
        {
          "of both the body and context, as supplementary streams that": "sole usage of the facial stream, led to a total score of 0.405,"
        },
        {
          "of both the body and context, as supplementary streams that": ""
        },
        {
          "of both the body and context, as supplementary streams that": "while the inclusion of both bodily and contextual\nfeatures"
        },
        {
          "of both the body and context, as supplementary streams that": "raised the total score to 0.423. With the introduction of the"
        },
        {
          "of both the body and context, as supplementary streams that": ""
        },
        {
          "of both the body and context, as supplementary streams that": "aural stream, our model reached a maximum of 0.437 on the"
        },
        {
          "of both the body and context, as supplementary streams that": "test set. On the VA sub-challenge we notice a slightly dif-"
        },
        {
          "of both the body and context, as supplementary streams that": "ferent but similar behavior. The introduction of contextual"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "OpenPose: Realtime multi-person 2D pose estimation using",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "affective behavior in the ﬁrst ABAW 2020 competition.\nIn"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "part afﬁnity ﬁelds. IEEE Trans. on Pattern Analysis and Ma-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "15th IEEE Int. Conf. on Automatic Face and Gesture Recog-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "chine Intelligence (TPAMI), 43:172–186, 2021.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "nition (FG), 2020."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[4] Wei-Yi Chang,\nShih-Huan Hsu,\nand\nJen-Hsien Chien.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[19] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "FATAUVA-Net:\nAn\nintegrated\ndeep\nlearning\nframework",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "for\nfacial\nattribute\nrecognition,\naction unit detection,\nand",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "IEEE Conf. on Com-\nvalence-arousal estimation.\nIn Proc.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "arXiv:1910.11111, 2019."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "puter Vision and Pattern Recognition Workshops (CVPRW),",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[20] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "2017.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[5] Didan Deng, Zhaokang Chen, and Bertram E Shi. Multitask",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "arXiv preprint\ntask learning:\nA large-scale\nface\nstudy."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "emotion recognition with incomplete labels.\nIn 15th IEEE",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "arXiv:2105.03790, 2021."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Int. Conf. on Automatic Face and Gesture Recognition (FG).",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "IEEE, 2020.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[6]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "in-the-wild: Aff-Wild database and challenge, deep architec-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Fei.\nImageNet: A large-scale hierarchical\nimage database.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "tures, and beyond.\nInt. Journal of Computer Vision (IJCV),"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "In IEEE Conf. Computer Vision and Pattern Recognition",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "pages 1–23, 2019."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "(CVPR). IEEE, 2009.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[22] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[7]\nJiankang Deng,\nJia Guo,\nNiannan Xue,\nand\nStefanos",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "action unit recognition: Aff-Wild2, multi-task learning and"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Zafeiriou. ArcFace: Additive angular margin loss for deep",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "ArcFace. arXiv preprint arXiv:1910.04855, 2019."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "the IEEE/CVF Conf. on Com-\nface recognition.\nIn Proc. of",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[23] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "puter Vision and Pattern Recognition (CVPR), 2019.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[8] Abhinav Dhall, Roland Goecke, Jyoti Joshi, Michael Wag-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "ner, and Tom Gedeon. Emotion recognition in the wild chal-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[24]\nJean Kossaiﬁ, Georgios Tzimiropoulos, Sinisa Todorovic,"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "lenge 2013.\nIn Proc. 15th ACM Int. Conf. on Multimodal",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "and Maja Pantic.\nAFEW-VA database\nfor\nvalence\nand"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Interaction, 2013.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Image and Vision Comput-\narousal estimation in-the-wild."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[9]\nPaul Ekman. Strong evidence for universals in facial expres-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "ing, 65:23–36, 2017."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "sions: A reply to Russell’s mistaken critique. Psychological",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[25] Ronak Kosti,\nJose M Alvarez, Adria Recasens, and Agata"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "bulletin, 1994.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Lapedriza.\nContext\nbased\nemotion\nrecognition\nusing"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[10]\nPaul Ekman and Wallace V Friesen. Constants across cul-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "IEEE Trans. on Pattern Analysis and Ma-\nEMOTIC dataset."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Journal of Personality and\ntures in the face and emotion.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "chine Intelligence (TPAMI), 42:2755–2766, 2019."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Social Psychology, 17:124, 1971.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[26]\nFelix Kuhnke, Lars Rumberg, and J¨orn Ostermann.\nTwo-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[11]\nP. P. Filntisis, N. Efthymiou, G. Potamianos, and P. Maragos.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "stream aural-visual affect analysis in the wild.\nIn 15th IEEE"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Emotion understanding in videos through body, context, and",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Int. Conf. on Automatic Face and Gesture Recognition (FG)."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "visual-semantic embedding loss.\nIn Proc. 16th Eur. Conf.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "IEEE, 2020."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Computer Vision Workshops (ECCVW) - Workshop on Bodily",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[27] Hanyu Liu,\nJiabei Zeng, Shiguang Shan, and Xilin Chen."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Expressed Emotion Understanding (BEEU), 2020.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "arXiv preprint\nEmotion recognition for in-the-wild videos."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[12] Behzad Hasani and Mohammad H Mahoor.\nFacial expres-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "arXiv:2002.05447, 2020."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "sion recognition using enhanced deep 3D convolutional neu-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[28] Y. Luo, J. Ye, R. B. Adams, J. Li, M. G. Newman, and J. Z."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "ral networks.\nIn Proc. IEEE Conf on Computer Vision and",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Wang. ARBEE: Towards automated recognition of bodily"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Pattern Recognition Workshops (CVPRW), 2017.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Int. Journal of Computer\nexpression of emotion in the wild."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Vision (IJCV), 128:1–25, 2019."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "for image recognition. In Proc. IEEE Conf. Computer Vision",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[29] David Matsumoto. More evidence for the universality of a"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "and Pattern Recognition (CVPR), 2016.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "contempt expression. Motivation and Emotion, 16:363–368,"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[14]\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "1992."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "memory. Neural Computation, 9(8):1735–1780, 1997.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": ""
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[30] T. Mittal,\nP. Guhan, U. Bhattacharya,\nR. Chandra, A."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[15] Yue\nJin, Tianqing Zheng, Chao Gao,\nand Guoqiang Xu.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Bera, and D. Manocha.\nEmotiCon: Context-aware multi-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "A multi-modal\nand multi-task\nlearning method\nfor\nac-",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "modal emotion recognition using Frege’s principle.\nIn Proc."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "arXiv\npreprint\ntion\nunit\nand\nexpression\nrecognition.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "IEEE/CVF Conf. on Computer Vision and Pattern Recogni-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "arXiv:2107.04187, 2021.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "tion (CVPR), 2020."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[16] Heechul Jung, Sihaeng Lee, Junho Yim, Sunjeong Park, and",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[31] A. Mollahosseini, B. Hasani, and M. H. Mahoor.\nAffect-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Junmo Kim.\nJoint ﬁne-tuning in deep neural networks for",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "Net: A database for facial expression, valence, and arousal"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "IEEE Int. Conf. on\nfacial expression recognition.\nIn Proc.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "computing in the wild.\nIEEE Trans. Affective Computing,"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Computer Vision (ICCV), 2015.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "10:18–31, 2017."
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "[17] Dimitrios Kollias, Irene Kotsia, Elnar Hajiyev, and Stefanos",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "[32] Ali Mollahosseini, Behzad Hasani, Michelle J Salvador, Ho-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "Zafeiriou. Analysing affective behavior in the second abaw2",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "jjat Abdollahi, David Chan, and Mohammad H Mahoor. Fa-"
        },
        {
          "[3] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.": "competition. arXiv preprint arXiv:2106.15318, 2021.",
          "[18] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing": "cial expression recognition from world wild web.\nIn Proc."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Workshops (CVPRW), 2016."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[33] Mihalis A Nicolaou, Hatice Gunes, and Maja Pantic. Con-"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "tinuous prediction of spontaneous affect from multiple cues"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "IEEE Trans. on Af-\nand modalities in valence-arousal space."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "fective Computing, 2:92–105, 2011."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[34] Maja Pantic, Michel Valstar, Ron Rademaker,\nand Ludo"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Maat. Web-based database for facial expression analysis.\nIn"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "IEEE Int. Conf. on Multimedia and Expo (ICME), 2005."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[35]\nJ. Pennington, R. Socher, and C. D. Manning. GloVe: Global"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "vectors for word representation. In Proc. 2014 Conf. Empiri-"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "cal Methods Natural Language Processing (EMNLP), 2014."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[36]\nIoannis Pikoulis, Panagiotis P Filntisis,\nand Petros Mara-"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "gos.\nLeveraging semantic scene characteristics and multi-"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "stream convolutional architectures in a contextual approach"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "for video-based visual emotion recognition in the wild. arXiv"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "preprint arXiv:2105.07484, 2021."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Journal of\n[37]\nJames A Russell. A circumplex model of affect."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Personality and Social Psychology, 39(6):1161, 1980."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[38]\nPhan Tran Dac Thinh, Hoang Manh Hung, Hyung-Jeong"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Yang,\nSoo-Hyung Kim,\nand Guee-Sang Lee.\nEmotion"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "recognition with incomplete labels using modiﬁed multi-task"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "learning technique. arXiv preprint arXiv:2107.04192, 2021."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[39]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A. Nicolaou,"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Athanasios Papaioannou, Guoying Zhao, and Irene Kotsia."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Aff-Wild: Valence and arousal\n’in-the-wild’ challenge.\nIn"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Proc. of\nthe IEEE Conf. on Computer Vision and Pattern"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Recognition Workshops (CVPRW), 2017."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[40] Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Zhang,\nand Yu Ding.\nPrior aided streaming network for"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "multi-task affective recognitionat\nthe 2nd abaw2 competi-"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "tion. arXiv preprint arXiv:2107.03708, 2021."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[41] Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, and Shiguang"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Shan. M 3T : Multi-modal continuous valence-arousal esti-"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "mation in the wild.\nIn 15th IEEE Int. Conf. on Automatic"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Face and Gesture Recognition (FG). IEEE, 2020."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[42] Xiangyun Zhao, Xiaodan Liang, Luoqi Liu, Teng Li, Yugang"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Han, Nuno Vasconcelos, and Shuicheng Yan.\nPeak-piloted"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "deep network for facial expression recognition. In Eur. Conf."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "on Computer Vision (ECCV). Springer, 2016."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "[43] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "and Antonio Torralba. Places: A 10 million image database"
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "IEEE Trans. on Pattern Analysis and\nfor scene recognition."
        },
        {
          "IEEE Conf. on Computer Vision and Pattern Recognition": "Machine Intelligence (TPAMI), 40:1452–1464, 2018."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Exploiting emotional dependencies with graph convolutional networks for facial expression recognition",
      "authors": [
        "Panagiotis Antoniadis",
        "P Panagiotis",
        "Petros Filntisis",
        "Maragos"
      ],
      "year": "2021",
      "venue": "Exploiting emotional dependencies with graph convolutional networks for facial expression recognition",
      "arxiv": "arXiv:2106.03487"
    },
    {
      "citation_id": "2",
      "title": "The OMG-Emotion behavior dataset",
      "authors": [
        "Pablo Barros",
        "Nikhil Churamani",
        "Egor Lakomkin",
        "Henrique Siqueira",
        "Alexander Sutherland",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "IEEE Int. Joint Conf. on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "3",
      "title": "OpenPose: Realtime multi-person 2D pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "G Hidalgo",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
      "citation_id": "4",
      "title": "FATAUVA-Net: An integrated deep learning framework for facial attribute recognition, action unit detection, and valence-arousal estimation",
      "authors": [
        "Wei-Yi Chang",
        "Shih-Huan Hsu",
        "Jen-Hsien Chien"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "5",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "15th IEEE Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "6",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "7",
      "title": "ArcFace: Additive angular margin loss for deep face recognition",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Niannan Xue",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition in the wild chal-2013",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Jyoti Joshi",
        "Michael Wagner",
        "Tom Gedeon"
      ],
      "year": "2013",
      "venue": "Proc. 15th ACM Int. Conf. on Multimodal Interaction"
    },
    {
      "citation_id": "9",
      "title": "Strong evidence for universals in facial expressions: A reply to Russell's mistaken critique",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1994",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "10",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "11",
      "title": "Emotion understanding in videos through body, context, and visual-semantic embedding loss",
      "authors": [
        "P Filntisis",
        "N Efthymiou",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2020",
      "venue": "Proc. 16th Eur. Conf. Computer Vision Workshops (ECCVW) -Workshop on Bodily Expressed Emotion Understanding (BEEU)"
    },
    {
      "citation_id": "12",
      "title": "Facial expression recognition using enhanced deep 3D convolutional neural networks",
      "authors": [
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conf on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "15",
      "title": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "authors": [
        "Jin Yue",
        "Tianqing Zheng",
        "Chao Gao",
        "Guoqiang Xu"
      ],
      "year": "2021",
      "venue": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "arxiv": "arXiv:2107.04187"
    },
    {
      "citation_id": "16",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "Heechul Jung",
        "Sihaeng Lee",
        "Junho Yim",
        "Sunjeong Park",
        "Junmo Kim"
      ],
      "year": "2015",
      "venue": "Proc. IEEE Int. Conf. on Computer Vision (ICCV)"
    },
    {
      "citation_id": "17",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Irene Kotsia",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Analysing affective behavior in the second abaw2 competition",
      "arxiv": "arXiv:2106.15318"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the first ABAW 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "15th IEEE Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "19",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "20",
      "title": "Distribution matching for heterogeneous multitask learning",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "A large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "21",
      "title": "Deep affect prediction in-the-wild: Aff-Wild database and challenge, deep architectures, and beyond. Int",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "Journal of Computer Vision (IJCV)"
    },
    {
      "citation_id": "22",
      "title": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and ArcFace",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "23",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "24",
      "title": "AFEW-VA database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "25",
      "title": "Context based emotion recognition using EMOTIC dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
      "citation_id": "26",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "15th IEEE Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition for in-the-wild videos",
      "authors": [
        "Hanyu Liu",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2020",
      "venue": "Emotion recognition for in-the-wild videos",
      "arxiv": "arXiv:2002.05447"
    },
    {
      "citation_id": "28",
      "title": "ARBEE: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2019",
      "venue": "Int. Journal of Computer Vision (IJCV)"
    },
    {
      "citation_id": "29",
      "title": "More evidence for the universality of a contempt expression",
      "authors": [
        "David Matsumoto"
      ],
      "year": "1992",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "30",
      "title": "EmotiCon: Context-aware multimodal emotion recognition using Frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Affect-Net: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Facial expression recognition from world wild web",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Michelle Salvador",
        "Hojjat Abdollahi",
        "David Chan",
        "Mohammad Mahoor"
      ],
      "venue": "Proc"
    },
    {
      "citation_id": "33",
      "title": "IEEE Conf. on Computer Vision and Pattern Recognition Workshops",
      "year": "2016",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "34",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "Maja Pantic",
        "Michel Valstar",
        "Ron Rademaker",
        "Ludo Maat"
      ],
      "year": "2005",
      "venue": "IEEE Int. Conf. on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "36",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. 2014 Conf. Empirical Methods Natural Language Processing"
    },
    {
      "citation_id": "37",
      "title": "Leveraging semantic scene characteristics and multistream convolutional architectures in a contextual approach for video-based visual emotion recognition in the wild",
      "authors": [
        "Ioannis Pikoulis",
        "P Panagiotis",
        "Petros Filntisis",
        "Maragos"
      ],
      "year": "2021",
      "venue": "Leveraging semantic scene characteristics and multistream convolutional architectures in a contextual approach for video-based visual emotion recognition in the wild",
      "arxiv": "arXiv:2105.07484"
    },
    {
      "citation_id": "38",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition with incomplete labels using modified multi-task learning technique",
      "authors": [
        "Phan Tran",
        "Dac Thinh",
        "Manh Hoang",
        "Hyung-Jeong Hung",
        "Soo-Hyung Yang",
        "Guee-Sang Kim",
        "Lee"
      ],
      "year": "2021",
      "venue": "Emotion recognition with incomplete labels using modified multi-task learning technique",
      "arxiv": "arXiv:2107.04192"
    },
    {
      "citation_id": "40",
      "title": "Aff-Wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "Mihalis Nicolaou",
        "Athanasios Papaioannou",
        "Guoying Zhao",
        "Irene Kotsia"
      ],
      "year": "2017",
      "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "41",
      "title": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
      "authors": [
        "Wei Zhang",
        "Zunhu Guo",
        "Keyu Chen",
        "Lincheng Li",
        "Zhimeng Zhang",
        "Yu Ding"
      ],
      "year": "2021",
      "venue": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
      "arxiv": "arXiv:2107.03708"
    },
    {
      "citation_id": "42",
      "title": "M 3 T : Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Yuan-Hang Zhang",
        "Rulin Huang",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2020",
      "venue": "15th IEEE Int. Conf. on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "43",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "Xiangyun Zhao",
        "Xiaodan Liang",
        "Luoqi Liu",
        "Teng Li",
        "Yugang Han",
        "Nuno Vasconcelos",
        "Shuicheng Yan"
      ],
      "year": "2016",
      "venue": "Eur. Conf. on Computer Vision (ECCV)"
    },
    {
      "citation_id": "44",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)"
    }
  ]
}