{
  "paper_id": "2112.12389v1",
  "title": "S+Page: A Speaker And Position-Aware Graph Neural Network Model For Emotion Recognition In Conversation",
  "published": "2021-12-23T07:25:02Z",
  "authors": [
    "Chen Liang",
    "Chong Yang",
    "Jing Xu",
    "Juyang Huang",
    "Yongliang Wang",
    "Yang Dong"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) has attracted much attention in recent years for its necessity in widespread applications. Existing ERC methods mostly model the self and inter-speaker context separately, posing a major issue for lacking enough interaction between them. In this paper, we propose a novel Speaker and Position-Aware Graph neural network model for ERC (S+PAGE), which contains three stages to combine the benefits of both Transformer and relational graph convolution network (R-GCN) for better contextual modeling. Firstly, a two-stream conversational Transformer is presented to extract the coarse self and inter-speaker contextual features for each utterance. Then, a speaker and position-aware conversation graph is constructed, and we propose an enhanced R-GCN model, called PAG, to refine the coarse features guided by a relative positional encoding. Finally, both of the features from the former two stages are input into a conditional random field layer to model the emotion transfer. Extensive experiments demonstrate that our model achieves state-of-the-art performance on three ERC datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC), which aims to identify the emotion of each utterance in a conversation, is a task arousing increasing interests in many fields. With the prevalence of social media and intelligent assistants, ERC has great potential applications in several areas, such as emotional chatbots, sentiment analysis of comments in social media and healthcare intelligence, for understanding emotions in the conversation with emotion dynamics and generating emotionally coherent responses. ERC remains a challenge. Both lexicon-based  (Wu, Chuang, and Lin 2006; Shaheen et al. 2014 ) and deep learning-based  (Colnerič and Demšar 2018)  text emotion recognition methods that treat each utterance individually fail in this task as these works ignore some conversation-specific characteristics.\n\nEmpirical results show that conversational context plays an important role in the ERC task  (Poria et al. 2019) . As demonstrated in Figure  1 , the third utterance by speaker A will be assigned a wrong emotion label if the history conversation information is blind to the model. In the past few years, recurrent neural network (RNN)-based solutions, such as CMN  (Hazarika et al. 2018b) , ICON  (Hazarika et al. 2018a)  and DialogueRNN  (Majumder et al. 2019) , have Figure  1 : A dialogue from IEMOPCAP, in which the emotion of the last utterance by speaker A will be wrongly classified if the dialogue context is not taken into consideration.\n\ndominated this field due to the sequential nature of conversational context. Nonetheless, they share some inherent limitations: 1) RNN model performs poorly in grasping distant contextual information; 2) RNN-based methods are not capable of handling large-scale multiparty conversations.\n\nWith the rise of graph neural network (GNN)  (Wu et al. 2020 ) in many natural language processing (NLP) tasks, researchers pay increasing attention to GNN-based ERC methods recently. Instead of modeling only sequential data recurrently in RNN, GNN is designed to capture all kinds of graph structure information via various aggregation algorithms. Existing GNN-based ERC methods, such as  Di-alogueGCN (Ghosal et al. 2019) , RGAT  (Ishiwatari et al. 2020 ) and DAG-ERC  (Shen et al. 2021) , which are the state of the art, have demonstrated the superiority of GNN in modeling conversational context. A directed graph is constructed on each dialogue in these methods, where the nodes denote the individual utterances, and the edges indicate relationships between utterances. However, we notice that these methods do not work well on modeling speaker-specific context, which is also important in the ERC task. For example, in Figure  1  the third utterance spoken by speaker A is more influenced by speaker A's prior utterances rather than the second utterance spoken by speaker B, even though the latter is closer. Thus, in contextual modeling, we should consider both the emotional influence that speakers have on themselves during a conversation, i.e., self-speaker context, and context on the entire conversation flow, i.e., inter-speaker context, as well as the interaction between them.\n\nOn the other hand, none of the existing methods actually exploit the fine-grained temporal information, i.e., the relative distance between utterances, because the original graph aggregation algorithms do not propagate distance-related message. We surmise that the relative distance is helpful to grasp more temporal information, and thus can further improve the performance.\n\nIn this paper, we propose a novel Speaker and Position-Aware GNN model for ERC (S+PAGE) to settle the above drawbacks of exisiting methods. Our model contains three stages to fully consider both self and inter-speaker context. Specifically, given a sequence of utterances in the same dialogue, we first leverage a Two-Stream Conversational Transformer (TSCT) with the attentive masking mechanism to get both the same speaker's and the whole dialogue's contextual features. Then, guided by the speaker identity and utterance order, we construct a speaker and positionaware conversation graph. The Position-Aware GNN model (PAG), which is an enhanced relational graph convolution network (R-GCN), is utilized to refine the contextual features with self and inter-speaker dependency. Particularly, we introduce relational relative positional encoding in the aggregation algorithm, in order to make PAG capable of capturing fine-grained temporal information. Finally, the global transfer of emotion labels is modeled by a conditional random field (CRF) layer with the features from both TSCT and PAG. Experimental results demonstrate the superiority of our model compared with state-of-the-art models. Ablation study illustrates the effectiveness of the proposed Transformer and position-aware graph structure in modeling the conversational context. To conclude, our contributions are as follows:\n\n• We propose a novel graph neural network-based ERC method, called S+PAGE, to incorporate all kinds of conversational context information in the model at the same time.\n\n• We present a two-stream conversational Transformer architecture to extract both conversational and speakerspecific context-aware features, which is also capable of handling multiparty conversations on large scale with high efficiency. • We apply a relational relative positional encoding on the graph neural network to capture fine-grained temporal information in a conversation. • We conduct extensive experiments on several ERC datasets, which demonstrate that our proposed model can significantly promote the performance.\n\n2 Related Works",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Emotion recognition in conversation is a popular area in NLP. Many ERC datasets have been scripted and annotated in the past few years, such as IEMOCAP  (Busso et al. 2008) , MELD  (Poria et al. 2018) , DailyDialog  (Li et al. 2017) , EmotionLines  (Chen et al. 2018)  and EmoryNLP  (Zahiri and Choi 2018) . IEMOCAP, MELD, and EmoryNLP are multimodal datasets, containing acoustic, visual and textual information, while the remaining two datasets are textual.\n\nIn recent years, ERC solutions are mostly deep learningbased models. CMN  (Hazarika et al. 2018b ) and ICON  (Hazarika et al. 2018a ) utilize gated recurrent unit (GRU) and memory networks to capture the dialogue dynamics. In IAAN  (Yeh, Lin, and Lee 2019)  and DialgueRNN  (Majumder et al. 2019) , attention mechanisms are applied to interact between the party state and global state. With the rise of Transformer and graph neural networks in NLP tasks, many works have also introduce them into the ERC task.  Zhong, Wang, and Miao (2019)  propose KET, which is a structure of hierarchical Transformers assisted by external commonsense knowledge. DialogueXL  (Shen et al. 2020)  applies dialogue-aware self-attention to deal with the multiparty structures. In  DialogueGCN (Ghosal et al. 2019 ) and RGAT  (Ishiwatari et al. 2020) , GCN  (Kipf and Welling 2016)  and GAT  (Veličković et al. 2017 ) are applied to refine the features with speaker dependencies and temporal information. DAG-ERC  (Shen et al. 2021 ) applies a directed acyclic graph for conversation representation and it achieves the state-of-the-art performance on multiple ERC datasets.  Vaswani et al. (2017)  first propose Transformer for machine translation task, whose success subsequently has been proved in various down-stream NLP tasks. Self-attention mechanisms endow Transformer with the ability of capturing longer-range dependency among elements of an input sequence than the RNN structure.  Beltagy, Peters, and Cohan (2020)  propose a novel self-attention mechanism for feature extraction of long documents. Pre-trained models such as BERT  (Devlin et al. 2018)  and GPT  (Brown et al. 2020)  use Transformer encoder and decoder respectively to learn representations on large-scale datasets. In our model, we present a modified Transformer structure to encode the contextual information in a conversation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Transformer",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Graph Neural Network",
      "text": "Graph neural network has attracted a lot of attention in recent years, which learns a target node's representation by propagating neighbor information in the graph.  Kipf and Welling (2016)  propose a simple and well-behaved layer-wise propagation rule for neural network models and demonstrate its effectiveness in semi-supervised classification tasks. Better aggregation methods for large graphs are proposed in GAT  (Veličković et al. 2017 ) and Graph-Sage  (Hamilton, Ying, and Leskovec 2017) .  Schlichtkrull et al. (2018)  propose R-GCN to deal with the highly multirelational data characteristic by assigning different aggregation structures for each relation type.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The framework of our model is shown in Figure  2 . We decompose the emotion classification procedure into three stages, i.e., contextual modeling, speaker dependency modeling, and global consistency modeling. In the first stage, we present a novel conversation-specific Transformer to get coarse contextual features as well as cues of speaker information. Then, a graph neural network is proposed to re-\n\nGraph Layer 0",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contextual Modeling",
      "text": "Speaker Dependency Modeling Consistency Modeling\n\nfine the features with speaker dependency and temporal information guided by relational relative positional features. Subsequently, we employ conditional random field to model the context of global emotion consistency, i.e., the emotion transfer. The interaction between self and inter-speaker context is involved in both of the former two stages.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition",
      "text": "The ERC task is to predict emotion labels (e.g., Happy, Sad, Neutral, Angry, Excited, and Frustrated) for utterances u 1 ; u 2 ; • • • ; u N , where N denotes the number of utterances in a conversation. Let S be the number of speakers in a given dataset. P is a mapping function, and p s = P (u i ) denotes utterance u i uttered by speaker p s , where s ∈ {1, • • • , S}.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Utterance Encoding",
      "text": "Following previous works  (Ghosal et al. 2019; Majumder et al. 2019) , we use a simple architecture consisting of a single convolutional layer followed by a max-pooling layer and a fully connected layer to extract context-independent textual features of each utterance. The input of this network is the 300 dimensional pre-trained 840B GloVe vectors (Pennington, Socher, and Manning 2014). We use the output features, denoted as u i , as the representation of each utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contextual Modeling",
      "text": "Two-Stream Conversational Transformer We present a Two-Stream Conversational Transformer (TSCT) to better extract the contextual representation of each utterance in a conversation, which is also capable of handling multi-party conversations efficiently. The structure of TSCT is shown in Figure  3 . The collection of utterance representations U = u 1 ; u 2 ; • • • ; u N is taken as the input. We design a multi-head self-attention mechanism, composed of two streams, i.e., the inter-speaker self-attention stream and the intra-speaker selfattention stream.\n\nInter-Speaker Self-Attention The inter-speaker selfattention is the same as the self-attention in vanilla Transformer, in which each utterance can attend to all positions in the dialogue as shown in Figure  3 (b). It is calculated as:\n\nwhere W l iq , W l ik and W l iv are three learnable weight matrices for attention head i at layer l.\n\nIntra-Speaker Self-Attention The intra-speaker selfattention models speaker-specific contextual information by only computing attention on the same speaker's utterances in a dialogue. In this way, the model is able to capture the emotional influence that speakers have on themselves during the conversation. It is implemented by the attentive masking strategy as illustrated in Figure  3 (c) and formulated as:\n\nwhere m ∈ R N ×N is the attentive masking matrix. The elements of m are set as below:\n\nwhere P (•) is the function that maps the utterance and its corresponding speaker.\n\nEach attention head i of the l-th layer in TSCT, denoted as head l i , is the concatenation of the f i and z i , and the output of the multi-head attention can be formulated as follows:\n\nwhere denotes concatenation operation. M is the number of attention heads, while 1 ≤ i ≤ M . Following the structure of the original Transformer, the output of the TSCT layer can be generated by passing M ultiHead l i through a normalization layer followed by a feed-forward network:\n\nTSCT Layer",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speaker Dependency Modeling",
      "text": "After extracting the contextual features, we construct a conversation graph based on the speaker identity and the order of utterances. We design a novel position-aware graph neural network to capture both speaker dependency and temporal information in a dialogue by introducing relative positional encoding.\n\nGraph Architecture We construct a directed graph, G = (V, E, R, W), for each dialogue with N utterances. The nodes in the graph are the utterances in the conversation, i.e., V = {u 1 ; u 2 ; • • • , u N }. (v i , v j , r ij ) ∈ E denotes a labeled edge (relation), where r ij ∈ R is a relation type, defined according to speaker identity and relative distance. W represents the set of edge weights.\n\nNodes Feature vector g i of each node v i is initialized as the output of the TSCT layer, i.e., h i . Feature vector g i is modified by the aggregation algorithm through the stacked graphical layers in GNN. The output feature is described as g l i , where l denotes the number of layers.\n\nEdges Instead of only focusing on past utterances, we take converse influence into account  (Ghosal et al. 2019 ).\n\nWe construct edges E with a sliding window for each utterance. The window sizes p and f denote the number of past and future utterances from the target utterance. Each utterance node v i has an edge with p utterances of the past: and have equal distances from the target utterance. For example, in Figure  4  u 1 and u 5 belong to the same edge type, while u 1 and u 6 belong to different edge types.\n\nEdge Weights Edge weights are computed by an attention mechanism. The particular attentional setup in our model closely follows the work of GAT  (Veličković et al. 2017) . The input of the attention module is a set of node features, i.e., g = g 1 , g 2 , ..., g N ∈ R F , where F is the dimension of each node feature. Motivated by  (Shaw, Uszkoreit, and Vaswani 2018) , which shows that absolute positional encoding is not effective for the model to capture the information of relative word order, we inject relative positional encoding into the attention mechanism. Fully expanded out, the edge weights, denoted as α ij ∈ W, computed by the positionaware graph attention mechanism can be described as:\n\nwhere W ∈ R F ×F is a weight matrix applied to every node, and F is the dimension of β ij . N i is the number of nodes linked with node i. a ∈ R 2F is a parametrized weight vector. • T represents transposition, and is the concatenation operation. β ij denotes the relative position representation between utterance i and utterance j in a dialogue, which is encoded by a learnable embedding matrix E p :\n\nwhere o(•) is a mapping function between utterance and its absolute position in the dialogue sequence. Notice that we use a signed relative position instead of the relative distance in definition of edge type to keep the temporal order. Thus, we have E p ∈ R w * F , where w = p + f denotes the whole window size.\n\nPAG We introduce a novel graph neural network to propagate utterance features, named Position-Aware Graph neural network (PAG). Inspired by R-GCN  (Schlichtkrull et al. 2018) , we define the following aggregation algorithm to calculate the forward-pass update of a node in the graph:\n\nwhere g l i is the hidden state of node i in the l-th layer. N r i denotes the set of neighbors of utterance i under the edge type r ∈ R. c i,r is a normalization constant, and we set c i,r = |N r i | in our experiment. W l r and W l o are learnable weight matrices, and σ(•) is an activation function. Different from R-GCN, we use edge weights calculated by Equation  7 to involve fine-grained temporal information in a conversation.\n\nFurthermore, between the stacked layers of PAG, we employ a fusion function, which is formulated as:\n\nwhere l ≥ 1. The fusion function is designed as a gated sum of two features:\n\nwhere a and b denote the feature vectors, and W z is a trainable weight matrix.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Consistency Modeling",
      "text": "Conditional Random Field To model the label consistency, i.e., the emotion transfer on a dialogue, a linear chain conditional random field is employed to yield final emotion tags of each utterance. Following the description of  (Lample et al. 2016) , for an input set of utterances U = u 1 , u 2 , ..., u n and a sequence of tag predictions y = y 1 , y 2 , .., y n , y i ∈ 1, • • • , K (K is the number of emotion tags), the score of the sequence is defined as:\n\nwhere T ∈ R K×K is the trainable transition matrix of the labels, and Q ∈ R n×K denotes the emmision matrix. In our model, P is computed by the concatenation of the last two modules' output features, following a linear layer and a softmax function. The model is trained to maximize the log-probability of the correct tag sequence:\n\nwhere Y is the set of all possible tag sequences. In the evaluation procedure, Equation 15 is computed by the Viterbi algorithm  (Rabiner 1989 ) to get the maximumprobability label sequence.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we present the datasets, baselines, metrics and experimental settings.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our S+PAGE model on four benchmark datasets -IEMOCAP, MELD, DailyDialog and EmoryNLP. For this work, we only consider emotion recognition based on textual features. The statistic of them is shown in Table  1 .\n\nIEMOCAP  (Busso et al. 2008 ) is a audiovisual dataset. It consists of dyadic conversations where actors perform improvisations or scripted scenarios. Each conversation is segmented into utterances, which are annotated with one of the six emotion labels, which are happy, sad, neutral, angry, excited, and frustrated.\n\nDailyDialogue  (Li et al. 2017 ) is a human-written multiturn dyadic dialogue dataset, reflecting our daily communication way and covering various topics about our daily life. Emotion labels in it contain neutral, happiness, surprise, sadness, anger, disgust, and fear.\n\nMELD  (Poria et al. 2018 ) is a multi-modal emotion classification dataset. It is a multi-party dialogue dataset created from scripts of the Friends TV series. Each utterance is annotated as one of the seven emotion classes: anger, disgust, sadness, joy, surprise, fear or neutral. EmoryNLP  (Zahiri and Choi 2018)  is also collected from Friends TV series, but it is different from MELD in the choice of scenes and emotion labels. The emotion labels include neutral, sad, mad, scared, powerful, peaceful, and joyful.\n\nFor the evaluation metrics, we choose micro-averaged F1 for DailyDialog and weighted-average F1 for the other datasets, following previous works  (Ishiwatari et al. 2020; Shen et al. 2021; Zhong, Wang, and Miao 2019) .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Baseline Methods",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "We set the initial learning rate of 1e-4 in the Transformer layers, 2e-3 in the PAG layers and 2e-2 in the CRF layer. AdamW optimizer is used under a scheduled learning rate following  (Vaswani et al. 2017) . The number of dimensions of the utterance representations and contextual embeddings is set to 300. We set the dropout rate and number of attention head in TSCT to be 0.1 and 8 repectively. 3-head attention is used during calculating the edge weights. We also conduct experiments with different window sizes and PAG layers. We choose the hyper-parameters that achieve the best score on each dataset by using development data. The training and testing process is run on a single Tesla P100 GPU with 32G memory. The reported results of our implemented models are all based on the average score of 5 random runs on the test sets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overall Performance",
      "text": "We compare our model with the baseline methods, and the results are reported in As shown in the table, all GNN-based models outperform RNN-based models. Compared with existing GNNbased models, our model even has considerable improvement. There are three main advantages that contribute to our performance: 1) contextual modeling with both self and inter-speaker dependency, 2) the presence of relative positional encoding in GNN, 3) consistency modeling of global emotion transfer.\n\nWe find that the improvement on MELD and EmoryNLP is not significant. These two datasets are constructed based on Friends TV series, which involve plenty of commonsense knowledge. Thus, pre-trained and knowledgeenhanced models, such as RGAT, DAG-ERC, KET have good results. However, our model still achieves competitive performance compared to these models as shown in the table.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "For better understanding the contribution of each component in our proposed model, we conduct experiments by replacing TSCT with the vanilla Transformer, and removing PAG and CRF from our model respectively. The results on IEMO-CAP and MELD are shown in table 3. We can observe that when TSCT is removed, the weighted F1 score drop more on MELD than that on IEMOCAP. This shows the superiority of TSCT on contextual feature extraction of multi-party conversations, as there are more speakers in dialogues of MELD. Removal of PAG leads to significant drop on both datasets, which implies the importance of PAG to refine the contextual features with speaker dependency and temporal information. Meanwhile, after removing CRF layer, we observe the performance degradation. It implies that the modeling of label consistency is essential in the ERC task. To sum up, all of the three components contribute to the performance improvement of S+PAGE.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pag Model Depth",
      "text": "We further explore the relationship between model performance and the depth of PAG. Stacking too many layers of GNN leads to performance degradation due to the oversmoothing problems  (Li, Han, and Wu 2018) . As shown in Figure  5 , we set different number of layers of PAG on IEMOCAP, and compare the performance with Diaglog-GCN and DAG-ERC. As can be noted from Figure  5 , all the models suffer from performance degradation when the model depth grows. However, the curve of PAG's F1 score descends more slowly than the other methods, which indicates the robustness of our model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Whether Pag Outperforms Other Graph Structures?",
      "text": "We conduct experiments on IEMOCAP by replacing PAG with the graph structures in DialogueGCN, RGAT and DAG-ERC respectively. As shown in Table  4 , S+PAGE still outperforms the other methods significantly. Notice that both DialogueGCN and RGAT with our contextual and consistency modeling perform better than their original versions. It proves the effectiveness of our none-graph parts.\n\nIn addition, the performance declines when PAG is replaced by the DAG in DAG-ERC. We think the main reason is that we do not use RoBERTa  (Liu et al. 2019)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Window Size",
      "text": "We analyze the influence of past and future window sizes by conducting experiments with window size w of (4, 4), (6, 6), (8, 8), (10, 10), (20, 20), (30, 30), (40, 40) on IEMOCAP dataset. As shown in Figure  6 , the F1 score of S+PAGE, RGAT and DialogueGCN significantly increase, when the window sizes expand from 4 to 10. The reason is that useful contextual information keeps growing with the increasing of w. However, after window sizes exceed 20, the F1 score drops for both DialogueGCN and RGAT. The reason is that the amount of useless long-range dependency increases when the window size continuously grows, which hinders the models from efficiently capturing crucial context. In contrast, the performance of S+PAGE fluctuates in a relatively narrow range, which shows the robustness of our model on varied window sizes. We can infer that the relative positional encoding endows capability of distinguishing critical contextual information to our model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel graph neural networkbased model, named S+PAGE, for emotion recognition in conversation (ERC). Specifically, S+PAGE contains three parts, i.e., contextual modeling, speaker dependency modeling, consistency modeling, to incorporate all kinds of contextual information. We present a new Transformer structure with two-stream attention mechanism to better capture the self and inter-speaker conversational context. In speaker dependency modeling, we introduce a novel GNN model, named PAG, to get fine-grained temporal information guided by the relative positional encoding. Furthermore, we use a CRF layer to model emotion transfer in the consistency modeling part. Experiments demonstrate that our model achieves competitive results on several ERC benchmarks. The effectiveness of the proposed two-stream conversational Transformer and position-aware graph neural network is also proved by extensive ablation study.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the third utterance by speaker",
      "page": 1
    },
    {
      "caption": "Figure 1: A dialogue from IEMOPCAP, in which the emo-",
      "page": 1
    },
    {
      "caption": "Figure 1: the third utterance spoken by speaker A is more",
      "page": 1
    },
    {
      "caption": "Figure 2: The framework of S+PAGE.",
      "page": 3
    },
    {
      "caption": "Figure 3: The collection of utterance representations U =",
      "page": 3
    },
    {
      "caption": "Figure 3: (b). It is calculated as:",
      "page": 3
    },
    {
      "caption": "Figure 3: (c) and formulated as:",
      "page": 3
    },
    {
      "caption": "Figure 3: (a) Structure of the two-stream conversational Transformer, (b) Inter-speaker self-attention, (c) Intra-speaker self-",
      "page": 4
    },
    {
      "caption": "Figure 4: An example of dialogue graph construction. Dif-",
      "page": 4
    },
    {
      "caption": "Figure 4: u1 and u5 belong to the same edge type,",
      "page": 4
    },
    {
      "caption": "Figure 5: Results of varying depths of GNN.",
      "page": 7
    },
    {
      "caption": "Figure 5: , we set different number of layers of PAG on",
      "page": 7
    },
    {
      "caption": "Figure 6: Results of varying window sizes.",
      "page": 7
    },
    {
      "caption": "Figure 6: , the F1 score of S+PAGE,",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "CNN\nCNN+cLSTM\nDialogueRNN\nKET",
          "IEMOCAP": "48.18\n54.95\n62.75\n59.56",
          "MELD": "55.86\n56.87\n57.03\n58.18",
          "DailyDialog": "49.34\n50.24\n-\n53.37",
          "EmoryNLP": "32.59\n32.89\n-\n33.95"
        },
        {
          "Model": "DialogueGCN\nRGAT\nDAG-ERC",
          "IEMOCAP": "64.18\n65.22\n68.03",
          "MELD": "58.10\n60.91\n63.65",
          "DailyDialog": "-\n54.31\n59.33",
          "EmoryNLP": "-\n34.42\n39.02"
        },
        {
          "Model": "S+PAGE",
          "IEMOCAP": "68.72",
          "MELD": "63.32",
          "DailyDialog": "64.07",
          "EmoryNLP": "39.14"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Longformer: The long-document transformer",
      "authors": [
        "I Beltagy",
        "M Peters",
        "A Cohan"
      ],
      "year": "2020",
      "venue": "Longformer: The long-document transformer",
      "arxiv": "arXiv:2004.05150"
    },
    {
      "citation_id": "2",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "S.-Y Chen",
        "C.-C Hsu",
        "C.-C Kuo",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition on twitter: Comparative study and training a unison model",
      "authors": [
        "N Colnerič",
        "J Demšar"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2018",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "R Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "9",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. North American Chapter"
    },
    {
      "citation_id": "10",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "12",
      "title": "Neural architectures for named entity recognition",
      "authors": [
        "G Lample",
        "M Ballesteros",
        "S Subramanian",
        "K Kawakami",
        "C Dyer"
      ],
      "year": "2016",
      "venue": "Neural architectures for named entity recognition",
      "arxiv": "arXiv:1603.01360"
    },
    {
      "citation_id": "13",
      "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
      "authors": [
        "Q Li",
        "Z Han",
        "X.-M ; Wu",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Thirty-Second AAAI conference on artificial intelligence",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "14",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "15",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "17",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "18",
      "title": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "A tutorial on hidden Markov models and selected applications in speech recognition",
      "authors": [
        "L Rabiner"
      ],
      "year": "1989",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "21",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "M Schlichtkrull",
        "T Kipf",
        "P Bloem",
        "Van Den",
        "R Berg",
        "I Titov",
        "M Welling"
      ],
      "year": "2018",
      "venue": "European semantic web conference"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from text based on automatically generated rules",
      "authors": [
        "S Shaheen",
        "W El-Hajj",
        "H Hajj",
        "S Elbassuoni"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Data Mining Workshop"
    },
    {
      "citation_id": "23",
      "title": "Selfattention with relative position representations",
      "authors": [
        "P Shaw",
        "J Uszkoreit",
        "A Vaswani"
      ],
      "year": "2018",
      "venue": "Selfattention with relative position representations",
      "arxiv": "arXiv:1803.02155"
    },
    {
      "citation_id": "24",
      "title": "Di-alogXL: All-in-one XLNet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2020",
      "venue": "Di-alogXL: All-in-one XLNet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "25",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition from text using semantic labels and separable mixture models",
      "authors": [
        "C.-H Wu",
        "Z.-J Chuang",
        "Y.-C Lin"
      ],
      "year": "2006",
      "venue": "ACM transactions on Asian language information processing"
    },
    {
      "citation_id": "29",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Z Wu",
        "S Pan",
        "F Chen",
        "G Long",
        "C Zhang",
        "S Philip"
      ],
      "year": "2020",
      "venue": "A comprehensive survey on graph neural networks"
    },
    {
      "citation_id": "30",
      "title": "An interactionaware attention network for speech emotion recognition in spoken dialogs",
      "authors": [
        "S.-L Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "32",
      "title": "Knowledgeenriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Knowledgeenriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    }
  ]
}