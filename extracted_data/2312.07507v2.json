{
  "paper_id": "2312.07507v2",
  "title": "Nac-Tcn: Temporal Convolutional Networks With Causal Dilated Neighborhood Attention For Emotion Understanding",
  "published": "2023-12-12T18:41:30Z",
  "authors": [
    "Alexander Mehta",
    "William Yang"
  ],
  "keywords": [
    "â€¢ General and reference â†’ Experimentation",
    "Performance",
    "â€¢ Human-centered computing â†’ Collaborative and social computing devices",
    "â€¢ Computer systems organization â†’ Neural networks",
    "â€¢ Computing methodologies â†’ Scene understanding",
    "Vision for robotics",
    "Activity recognition and understanding",
    "Computer vision tasks",
    "Computer vision",
    "Computer vision problems",
    "Machine learning approaches Temporal Convolutional Networks, Video Understanding, Recurrent Neural Networks, Attention-Based Video Models, Emotion Recognition, AFEW-VA, AffWild2, EmoReact"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the task of emotion recognition from videos, a key improvement has been to focus on emotions over time rather than a single frame. There are many architectures to address this task such as GRUs, LSTMs, Self-Attention, Transformers, and Temporal Convolutional Networks (TCNs). However, these methods suffer from high memory usage, large amounts of operations, or poor gradients. We propose a method known as Neighborhood Attention with Convolutions TCN (NAC-TCN) which incorporates the benefits of attention and Temporal Convolutional Networks while ensuring that causal relationships are understood which results in a reduction in computation and memory cost. We accomplish this by introducing a causal version of Dilated Neighborhood Attention while incorporating it with convolutions. Our model achieves comparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, and GRUs while requiring fewer parameters on standard emotion recognition datasets. We publish our code online for easy reproducibility and use in other projects -Github Link.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The study of emotion recognition has gained significant importance due to its widespread applications in various disciplines, including Human-Computer Interaction. Socially assistive robots, medical diagnosis of disorders such as PTSD  [50] , and software usability testing  [23]  in particular rely heavily on accurate emotion recognition. For instance, a better understanding of human emotions enables socially assistive robots to comprehend and respond appropriately to various scenarios, leading to effective assistance  [2] . As a result, the development of advanced emotion recognition techniques holds immense potential in revolutionizing various aspects of our lives.\n\nA common issue with the task of emotion recognition and understanding is lack of data. This is commonly due to the fact that annotation is a non-trivial task. This means that architectures must be expressive enough without large data sizes. The classification of emotions from video inputs is a task that has been extensively studied in the field of computer vision. A common approach to this problem involves performing classification directly on individual frames using convolutional networks  [49] . However, this approach ignores the temporal aspect of emotion, which is critical to its accurate recognition. Humans exhibit emotions over a period of time, and considering this temporal aspect can lead to significant improvements in performance through contextual understanding  [46] .\n\nTo incorporate temporal information into emotion recognition from video inputs, various techniques have been proposed, including Recurrent Neural Networks (RNNs)  [48]  and Transformers  [8, 16, 61] . While GRUs and LSTMs are effective in modeling temporal dependencies, they suffer from slow training times, unstable training, and high computational costs. This is because their gradients flow through time rather than in parallel  [22] . Transformers, on the other hand, typically suffer have a large parameter size and require more operations due to their self-attention mechanism attending to a large receptive field. Commonly the two models are combined for optimal performance  [58] .\n\nAn alternative solution that has gained popularity is the Temporal Convolutional Network (TCN)  [5, 44, 59] . The TCN allows for the modeling of temporal dependencies in video tasks and time-series tasks similar to RNNs and LSTMs, but with more stable gradients and higher efficiency at large receptive fields due to parallelized computation of gradients  [5, 34] . The receptive field of the TCN can be easily adjusted with the number of layers, kernel size, and dilation factor. The TCN is a promising method for video tasks due to its efficiency benefits while understanding temporal dependencies  [3]  for emotional understanding.\n\nTemporal Convolutional Networks do pose performance issues in regard to it's understanding complex relationships in the short and long term of a sequence and more irregular sequences  [40] . Models attempt to address this, but fall short in terms of model size and performance due to the large size of self attention or auxiliary models  [6, 40, 61] . TCAN attempts to intertwine attention layers  [17]  which leads to larger models.\n\nIn this paper we introduce a new Temporal Convolutional Network known as NAC-TCN which addresses concerns about complex temporal relationships while maintaining or improving the benefits of TCNs -stable gradients and computational efficiency.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "1.1.1 Recurrent Networks. Long-Short Term Memory Units (LSTMs) are a response to the hard to train nature of RNNs  [45] . RNNs store a hidden state that can be used in order to represent all prior knowledge. Though this may be a strong representation, there are often training issues in long sequences. LSTMs include a forget gate that can allow for data to not be stored in the cells memory unit state. The LSTM has an input, output, and forget gate. LSTM models suffer from a large amount of operations per cell. Gated Recurrent Units  [9]  attempt to solve this by not holding a memory unit, instead only having update and forget gates. Despite the less complex structure, it has similar performance to LSTMs  [11] .\n\n1.1.2 Attention and Transformer. Self Attention  [9, 54]  was introduced for language tasks. Instead of simply holding a weight and bias, self attention focuses on parts of the input and weights sequential inputs based on a query, key, and value matrix. These attention models can be used in conjunction with recurrent models for better performance (Sec. 1.1.1).\n\nTransformers use layered attention with encoders and decoders  [12, 54] . This has been commonly used in NLP tasks, but recent advances have moved towards it's application in computer vision  [4] .\n\nThese 2 methods represent a divergence in machine learninguse of classical recurrent methods or a move to the more computationally heavy transformer. In this paper, we hope to show a third path that can incorporate the benefits of both while alleviating their pitfalls.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Background 2.1 Temporal Convolutional Networks",
      "text": "The Temporal Convolutional Network  [5, 44, 59 ] is a convolutional representation of temporal data. It contains 2 commonly used parts, the main casual convolution network (Sec. 2.1.2) and dilated convolution (Sec. 2.1.1) in order to create a Dilated Temporal Convolutional Network.\n\n2.1.1 Dilated Convolution. The dilation (Ã  trous Convolution)  [59]  allows for a model to have a larger receptive field without increasing parameters. Dilated convolution is achieved by introducing \"holes\" between the points addressed by the kernel, resulting in a larger receptive field. The term \"gaps\" will be used to refer to any method of expanding a kernel through gaps to widen the receptive field.\n\n2.1.2 Dilated Temporal Convolutional Network. Introduced in WaveNet  [44] , a Dilated Temporal Convolutional Network is a temporal network model that computes timesteps in parallel rather than sequentially. This fundamentally alters how the model addresses backpropagation through time by performing backpropogation for all time steps at once rather than following a temporal gradient flow. A casual convolution is used in order to prevent leakage from the past into future steps. (1)\n\nA dilated convolution is used in order to allow a network to understand time steps from previous steps efficiently and exponentially increase the receptive field  [59] . Without dilated convolutions, TCNs would have a linear receptive field to prior steps. With dilations, the receptive field to previous timesteps (frames) can be calculated as\n\nwhere ğ‘‘ is the dilation factor, ğ‘˜ is the kernel size, and ğ‘› is the number of hidden layers. Commonly, a dilation factor of 2 is used in order to achieve an exponential receptive field  [5] . Dilated Temporal Convolutional Networks (TCNs) allow for a large amount of temporal data to be processed with low computation through a large receptive field. TCNs allow for parallel computation, a large receptive field, and helps avoid vanishing or exploding gradients due to its backpropagation not being parallel to the temporal sequence, but rather perpendicular. Dilated TCNs have achieved impressive results replicating the long-term memory understanding of other architectures like LSTMs and RNNs such as the copying memory task  [5] . The TCN has also been adopted for action segmentation achieving state-of-the-art results on action detection  [35] . TCNs have also been explored in emotion analysis, achieving results above those of LSTMs and RNNs on emotion based tasks  [61] . TCNs commonly consist of temporal blocks, which are made up of two convolutional layers that are stacked on top of each other. The purpose of stacking these layers is to ensure that the input data is first scaled to the expected size and then passed through a convolutional layer of the output size.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Neighborhood Attention",
      "text": "Neighborhood Attention (NA) is an attention method that utilizes a sliding window technique similar to a convolution which views the time series at incriments like a convolution instead of all at once such as self-attention. This is similar to methods such as the SWIN transformer  [20, 38]  but the main difference comes from how NA allows for overlapping sgements, a method showed to improve performance by ensuring translation equivariance over similar methods  [19, 20] . NA was introduced in order to address poor efficiency of self-attention and sliding window techniques by using a tiled algorithm and efficient CUDA kernels published in the Nğ´ğ‘‡ğ‘‡ ğ¸ğ‘ library [1].\n\nDilated Neighborhood Attention (DiNA). is a method introduced to further address the performance of attention  [19] . This dilated transformer works similar to dilated (also known as Ã  trous) convolutions  [59] . This improves performance beyond Neighborhood attention by attending to a higher receptive field in less operations than a normal transformer. When ğ‘‘ is a dilation value and ğ‘˜ is a neighborhood (kernel) size, DiNA reduces time complexity of self-attention from ğ‘‚ (ğ‘› 2 ğ‘‘) to ğ‘‚ (ğ‘›ğ‘‘ğ‘˜).  [17]  is a TCN-based model that intertwines attention to maintain receptive field while providing an attention mechanism. This model has seen improvements over the TCN on language datasets. Although the method has seen improvements over the TCN, it leads to a significant parameter increase and it doesn't preserve the casual nature of the TCN, meaning that information flows freely between layers, and loses the temporal property due to leakage.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Tcan. Tcan",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "To enhance the representation of temporal dependencies and their importance in emotional understanding, we propose a method that extends TCNs by incorporating the attention features of Neighborhood Attention while maintaining causality. We introduce our proposed archetecture that achieves this along with memory and runtime benefits in this section.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Nac-Tcn Formulation",
      "text": "The Neighborhood Attention with feature extracting Convolutions TCN (NAC-TCN), is a deep learning based approach that utilizes Dilated Neighborhood Attention to enforce causality and combines convolutional operations and self-attention. Our proposed NAC-TCN method incorporates neighborhood self-attention layers within Temporal Blocks with 1D Convolutional Layers to allow the TCN to identify the most important frames through Neighborhood Attention and create local filters through 1D Convolutions. A combination of convolution and attention layers has been shown to produce improved results  [57] . Our method makes use of Dilated Neighborhood Attention  [19, 20, 56] , and shifting inputs to maintain causality. 1x1 convolutions  [37]  are added on the input of each temporal block in order to ensure that residual connections have the same tensor shape in a similar fashion to the original TCN. The use of Dilated Neighborhood Attention not only keeps causality in the TCN, but also reduces operations and parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Block",
      "text": "A NAC-TCN temporal block is parametrized by its kernel size ğ‘˜, dilation value ğ‘‘, input ğ‘¥, time step ğ‘¡, convolution ğ‘“ ğ‘˜ and the DiNA operation (Eq. (  6 )) and can be described as\n\nIn between each convolution, an activation (ReLU) is applied and followed by a 1D Spatial Dropout Layer which allows for feature independence between channels of the model  [52] . This reflects the primary diagram in Sec. 3 which shows the Temporal Block structure where a Convolution is followed by Dropout and ReLU with a 1x1 convolution as described in Sec. 3.4.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Motivation For Convolution And Neighborhood",
      "text": "Attention Stacking. We wish to create high performing low operational cost models. Adding convolutions achieves this twofold: being able to reduce dimensionality through downsampling (a feature that doesn't exist in NAT) and using a convolution over an attention block with fewer parameters. Additionally, our motivation for stacking convolutions and Di-NAT comes from the benefits of convolutions that have been understated by recent works. Prior work has noted that though attention based methods outperform, they mainly do on a very large scale  [14] . Primarily, lack of convolutions loses the benefit of quick and easy translational equivariance and requiring larger datasets/training time to perform as expected or a use of regularization  [53] . This becomes important in domains such as emotion recognition, as datasets are tedious to collect as they require expert annotators and a mix of annotators requires agreement in labeling, which can be sometimes subjective 1  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Causal Dilated Neighborhood Attention",
      "text": "We extend the dilated neighborhood attention structure introduced by Hassani and Shi  [19]  (Sec. 2.2). Their Dilated Neighborhood Attention is modeled by attention weights A (ğ‘˜,ğ›¿ ) ğ‘– for a dilated by ğ›¿ DiNA layer\n\nFor each sliding window, we can define the output of each pixel by\n\nwhere ğ·ğ‘–ğ‘ ğ´ is applied to each element ğ‘–, where ğ‘– âˆˆ R 1Ã—ğ‘› , and the output is then ğ·ğ‘–ğ‘ ğ´(ğ‘–). ğ‘‰ , ğ‘„, and ğ¾ are all calculated in the same manner as self-attention, as ğ·ğ‘–ğ‘ ğ´(ğ‘–) tends to simply self-attention as you increase ğ‘˜ and decrease ğ‘‘ to 1. In order to stop temporal leakage, we must ensure that for an input (ğ‘¥ ğ‘¡ |ğ‘¥ 0 , ..., ğ‘¥ ğ‘¡ -1 ) to each temporal block, the output (ğ‘¦ ğ‘¡ |ğ‘¦ 0 , ..., ğ‘¦ ğ‘¡ -1 ) must be influenced by a time step at least 1 less than ğ‘¡. To ensure this, we change ğœŒ ğ›¿ ğ‘— (ğ‘–) to represent the nearest neighbor to the left of the ğ‘–-th value with a dilation ğ›¿. This would mean that for a neighborhood ğ‘˜, the farthest value referenced is ğ›¿ â€¢ (ğ‘˜ -1) to the left of the input sequence (not including the ğ‘¥ ğ‘¡ itself), rather than\n\n. In order to ensure this, we pad DiNA ğ›¿ ğ‘˜ (ğ‘–) and the convolutions using casual padding in order to make sure that timesteps are not influenced by the future, then removing padding before the next temporal block to ensure length consistency. In implementation, this is a standard ğ›¿ â€¢ (ğ‘˜ -1) zero padding followed by removing ğ›¿ â€¢ (ğ‘˜ -1) elements to the right, removing future timesteps.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Residual Connections",
      "text": "Since a network requiring a large receptive field will require an increase in layers, a residual connection  [21]  is added to address vanishing and shattering gradients problems  [7, 55, 60] , improve the loss landscape  [36]  leading to more stable training and better results. Residual layers are simply described as\n\nSince Temporal Blocks commonly upscale or downscale inputs, the residual layer in the NAC-TCN Temporal Block is\n\nwhere ğº (ğ‘¥) is an optional 1x1 convolution used when scaling of channels is required. The 1x1 convolution impact is twofold: reducing dimensions of the network in later layers and providing a way for the model to translate features from one layer to another while maintaining the same overall information as previous layers. DiNA is not used for this 1x1 convolution because of its inability perform dimension scaling. NAC-TCN reduces parameters compared to the multi-model approach proposed by others  [18, 62, 64] , the original TCN, and TCAN  [17] . This can be attributed to the fact that attention operations, which solely consider the kernel size of the neighborhood, are intertwined with the convolution operations, leading to a decrease in parameters when compared to traditional combined structures.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In order to evaluate the effectiveness of our TCN methods, we used a variety of emotion and action recognition datasets, where newer temporal information is more relevant than the past. The ğ‘Ÿğ‘’ğ‘”ğ‘›ğ‘’ğ‘¡_ğ‘¦_400ğ‘šğ‘“ image encoder  [47]  is used as an encoder for all the datasets to ensure that the NAC-TCN is the main factor tested.\n\nThe AffWild2 dataset.  [23] [24] [25] [26] [27] [28] [30] [31] [32]  supplies 1,500,000+ annotated video frames of the valence and arousal metric in 341 videos. A video length of 256 frames is used. Due to the fact that valence and arousal are between [-1, 1], tanh is applied to the model output. The valence and arousal scores are evaluated and trained on the Concordance Correlation Coefficient (CCC) metric\n\nwhere ğ‘¥ and ğ‘¦ are predictions and ground truths, 2ğ‘  ğ‘¥ and 2ğ‘  ğ‘¦ are variances, and x and È³ are the mean values.\n\nThe EmoReact dataset.  [43]  provides videos of children annotated for 8 different emotions. Sequence length of 128 is used. The classes are not mutually exclusive and imbalanced. A random sampler and binary cross entropy are used to address these issues. The same CNN encoder model from the AffWild2 experiments is used. Evaluation is done using Area under the precision-recall curve (AUC-ROC) to follow similar methodology to prior studies. AUC-ROC tells us for different threshold how a model performs by ploting False Positive (FP) rate and True Positive (TP) rate and defining AUC-ROC as the area under this curve. AFEW-VA.  [13, 33]  provides valence and arousal annotations to popular films. These annotations are integers ranging from  [-10,10] . These are converted to mood labels to compare to prior works  [41] . Accuracy is used as the evaluation metric. The sequence length is 32, as videos are much shorter compared to other datasets.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Model Testing Methodology",
      "text": "The baseline GRU and LSTM hyperparameters for the AffWild2 dataset are chosen to match the prior models tested on the dataset. Other models such as TCN  [5]  and TCAN  [17]  used the same hyperparameters as the large NAC-TCN. GRU and LSTM blocks are concatenated with attention based models in order to create an ensemble of the two for testing.\n\nFor each evaluated dataset, NAC-TCN was tested in two sizes. The larger size attempted to use similar hyperparameters to the GRU models while ensuring optimal receptive field (1) through ğ‘˜, ğ‘‘, and number of layers. The optimal receptive field for all models besides AFEW-VA and AffWild2 were the length of the sequence, as only the final item was annotated. AFEW-VA used the entire sequence length 32 and AffWild2 used 256 based on prior literature. The model smaller size still ensured the optimal receptive field, but attempted to be a equal to smaller size than the GRU and LSTM models through adjusting previously mentioned hyperparameters along with the number of channels for the convolutional layers. This approach allowed us to conduct comparative tests while highlighting the versatility of the NAC-TCN model in terms of memory and computational cost.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "We use an Adam Optimizer with a base learning of 0.001 alongside an annealing cosine scheduler. We use a batch size of 16 for the AffWild2, EmoReact, and AFEW-VA datasets and all models were trained for 10 epochs. AFEW and AffWild2 all used subject based k-fold cross validation to ensure that information leakage did not occur between testing and training. The validation dataset of AffWild2 was used as the evaluation set and kept separate from training data. The EmoReact dataset had preset train and test splits that were used to be in line with the performance of prior models. The best performing model was selected for each method. The same random seed value was selected to ensure reproducibility. The number of heads for DiNAT was selected using hyperparameter search ({2 ğ‘› | 1 â‰¤ ğ‘› â‰¤ 3}). Note that for AffWild2, we build off the open source library provided by  Nguyen 22  for testing our new model to ensure consistency when comparing to other methods  [42] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Metrics",
      "text": "In addition to the per-dataset metrics, both operations and parameters are recorded. Operations are measured in MACs, or Multiply-Accumulate operations 2 . Both of these were measured using the Pytorch FLOPS Counter  [51] . MACs represent the common operation of\n\nwhere ğ‘‹ is the original input, ğ‘Š is a weight, and ğµ is a bias. We commonly report MMac, or MegaMACs (10 6 MACs).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "In this section, we report the performance of our proposed NAC-TCN architectures against the baseline GRU, LSTM, TCN, and attention models with both performance and efficiency. In addition, we compare against state-of-the-art models on the respective datasets where relevant. As reported in Tab. 1, our proposed NAC-TCN architecture outperforms the other temporal-based models, while using a smaller model size. Additionally, we achieve the highest performance at smaller model sizes. The NAC-TCN, achieved through either a simple single layer setup or with self-attention, exhibited higher performance than the base TCN, which indicates that NAC-TCN can better learn temporal representations with less memory.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Affwild2",
      "text": "It should be noted that superior performance has exhibited in recent studies. However, conducting a direct comparison is challenging due to the utilization of disparate datasets and the employment of multi-sensor methodologies during the training process. Notably, participants in the latest AffWild2 challenge have surpassed reported results through four-encoder model, which incorporates audio and image encoders  [39] . Other participants have surpassed prior state-of-the-art results with use of linguistic models from extracted words  [63] . For our purposes, we have achieved a stateof-the-art result in the chosen set of input modalities and encoder choice.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emoreact",
      "text": "Results on EmoReact  [43]  (Tab. 2) show that with less modalities, NAC-TCN Small outperforms other models without multiple modalities or increased training data. This is done with a decrease in parameters and operations. This indicates that a better performing architecture like NAC-TCN may be actually outperform even with less data. NAC-TCN may be more prone to overfitting, given that with similar parameters to GRU and LSTM, it performed similarly and worse to a larger TCN. This highlights that NAC-TCN can be more expressive with the same hyperparamters, hence strong of the smaller model. AFEW-VA results show that the larger NAC-TCN is able to outperform other methods. The smaller model results in similar performance to TCNs but with smaller memory footprint. It is important to note that the disparity between the models is minimal, within a Â± 2% range. Consequently, the AFEW-VA dataset should be regarded primarily as a validation of the NAC-TCN's capacity to maintain performance levels akin to those of more expansive models. Nevertheless, NAC-TCN outperforms the 1-CNN model which uses attention  [41] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Afew-Va",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "In order to understand the impact of different choices we made in design and experimentation, we perform several ablation studies. The study reveals that the residual connection is critical to training. Without residual connection, the model saw a significant performance decrease (Tab. 4). timesteps, where an acausal model would weight based on ğ‘˜ 2 on each side. We find that the causal relation is important in both datasets, but is more dramatic in the EmoReact dataset. This suggests that emotions are better learned when future information is unknown. This phenomenon of better learning with less information can be attributed to two potential reasons. Firstly, emotions inherently involve a causal process, wherein per-frame annotations occur continuously, thereby influencing annotators' decisions based on prior frames rather than knowledge of future frames. This can lead to different understandings depending on what context is used. Secondly, the disparity between the datasets stems from the variation in label format. AffWild2 employs per-frame labels, allowing for non-causal predictions of adjacent frames, whereas EmoReact utilizes end-of-video labels, thereby elevating the significance of causality (the last frame culminating in information from previous frames rather than â„/2 prior frames). We find that prior literature commonly uses causal relationships over acausal with better results, making it an interesting point of discussion for future work.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Limitations",
      "text": "Although our method outperformed on many datasets, performance on AFEW-VA is notably similar to other temporal models. Given AFEW-VA is a smaller dataset, this may indicate that NAC-TCN outperforms other models on larger datasets with more oracle access. Multi-model & pretraining approaches that could perform better were not studied due to hardware limitations and simplicity in results. Our model also holds many of the same flaws of modern TCN based methods, such as higher memory during evaluation (needing the whole sequence instead of hidden state) and poor transfer learning with different ğ‘˜ or ğ‘‘ values.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Contribution",
      "text": "In this paper, we presented an alternative to the Temporal Convolutional Network that allows for attention while decreasing parameters and number of MAC operations. Experimental evaluation revealed improvements over classical methods such as GRUs, LSTMs, and Attention-based methods at a lower computational cost. Our method outperforms common temporal methods, improves on the benefits of the TCN, and performs similarly at an efficiency benefit while maintaining the same TCN controls over memory usage.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The NAC-TCN combines Dilated Temporal Convolu-",
      "page": 1
    },
    {
      "caption": "Figure 2: TCN architecture",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "alexandermehta@outlook.com": "Independent Researcher",
          "yangwill@seas.upenn.edu": "University of Pennsylvania"
        },
        {
          "alexandermehta@outlook.com": "USA",
          "yangwill@seas.upenn.edu": "USA"
        },
        {
          "alexandermehta@outlook.com": "ABSTRACT",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "In the task of emotion recognition from videos, a key improve-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "ment has been to focus on emotions over time rather than a single",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "frame. There are many architectures to address this task such as",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "GRUs, LSTMs, Self-Attention, Transformers, and Temporal Con-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "volutional Networks (TCNs). However, these methods suffer from",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "high memory usage, large amounts of operations, or poor gradients.",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "We propose a method known as Neighborhood Attention with",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "Convolutions TCN (NAC-TCN) which incorporates the benefits",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "of attention and Temporal Convolutional Networks while ensur-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "ing that causal relationships are understood which results in a",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "reduction in computation and memory cost. We accomplish this by",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "introducing a causal version of Dilated Neighborhood Attention",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "while incorporating it with convolutions. Our model achieves com-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "parable, better, or state-of-the-art performance over TCNs, TCAN,",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "LSTMs, and GRUs while requiring fewer parameters on standard",
          "yangwill@seas.upenn.edu": "Figure 1: The NAC-TCN combines Dilated Temporal Convolu-"
        },
        {
          "alexandermehta@outlook.com": "emotion recognition datasets. We publish our code online for easy",
          "yangwill@seas.upenn.edu": "tions with Dilated Neighborhood Attention to better capture"
        },
        {
          "alexandermehta@outlook.com": "reproducibility and use in other projects â€“ Github Link.",
          "yangwill@seas.upenn.edu": "temporal relationships in video inputs through contextual"
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "weighting using Dilated Neighborhood Attention. Our pro-"
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "posed architecture achieves better performance with smaller"
        },
        {
          "alexandermehta@outlook.com": "CCS CONCEPTS",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "model size."
        },
        {
          "alexandermehta@outlook.com": "â€¢ General and reference â†’ Experimentation; Performance;",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "Human-centered computing â†’ Collaborative and social com-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "puting devices; â€¢ Computer systems organization â†’ Neural",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "annotation is a non-trivial task. This means that architectures must"
        },
        {
          "alexandermehta@outlook.com": "networks; â€¢ Computing methodologies â†’ Scene understand-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "be expressive enough without large data sizes. The classification"
        },
        {
          "alexandermehta@outlook.com": "ing; Vision for robotics; Activity recognition and understanding;",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "of emotions from video inputs is a task that has been extensively"
        },
        {
          "alexandermehta@outlook.com": "Computer vision tasks; Computer vision; Computer vision",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "studied in the field of computer vision. A common approach to this"
        },
        {
          "alexandermehta@outlook.com": "problems; Machine learning approaches.",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "problem involves performing classification directly on individual"
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "frames using convolutional networks[49]. However, this approach"
        },
        {
          "alexandermehta@outlook.com": "KEYWORDS",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "ignores the temporal aspect of emotion, which is critical\nto its"
        },
        {
          "alexandermehta@outlook.com": "Temporal Convolutional Networks, Video Understanding, Recur-",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "accurate recognition. Humans exhibit emotions over a period of"
        },
        {
          "alexandermehta@outlook.com": "rent Neural Networks, Attention-Based Video Models, Emotion",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "time, and considering this temporal aspect can lead to significant"
        },
        {
          "alexandermehta@outlook.com": "Recognition, AFEW-VA, AffWild2, EmoReact",
          "yangwill@seas.upenn.edu": ""
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "improvements in performance through contextual understanding"
        },
        {
          "alexandermehta@outlook.com": "",
          "yangwill@seas.upenn.edu": "[46]."
        },
        {
          "alexandermehta@outlook.com": "1\nINTRODUCTION",
          "yangwill@seas.upenn.edu": "To incorporate temporal information into emotion recognition"
        },
        {
          "alexandermehta@outlook.com": "The study of emotion recognition has gained significant importance",
          "yangwill@seas.upenn.edu": "from video inputs, various techniques have been proposed, includ-"
        },
        {
          "alexandermehta@outlook.com": "due to its widespread applications in various disciplines, including",
          "yangwill@seas.upenn.edu": "ing Recurrent Neural Networks (RNNs)\n[48] and Transformers"
        },
        {
          "alexandermehta@outlook.com": "Human-Computer Interaction. Socially assistive robots, medical",
          "yangwill@seas.upenn.edu": "[8, 16, 61]. While GRUs and LSTMs are effective in modeling tem-"
        },
        {
          "alexandermehta@outlook.com": "diagnosis of disorders such as PTSD [50], and software usability",
          "yangwill@seas.upenn.edu": "poral dependencies, they suffer from slow training times, unstable"
        },
        {
          "alexandermehta@outlook.com": "testing [23] in particular rely heavily on accurate emotion recog-",
          "yangwill@seas.upenn.edu": "training, and high computational costs. This is because their gradi-"
        },
        {
          "alexandermehta@outlook.com": "nition. For instance, a better understanding of human emotions",
          "yangwill@seas.upenn.edu": "ents flow through time rather than in parallel [22]. Transformers,"
        },
        {
          "alexandermehta@outlook.com": "enables socially assistive robots to comprehend and respond appro-",
          "yangwill@seas.upenn.edu": "on the other hand, typically suffer have a large parameter size and"
        },
        {
          "alexandermehta@outlook.com": "priately to various scenarios, leading to effective assistance [2]. As",
          "yangwill@seas.upenn.edu": "require more operations due to their self-attention mechanism at-"
        },
        {
          "alexandermehta@outlook.com": "a result, the development of advanced emotion recognition tech-",
          "yangwill@seas.upenn.edu": "tending to a large receptive field. Commonly the two models are"
        },
        {
          "alexandermehta@outlook.com": "niques holds immense potential in revolutionizing various aspects",
          "yangwill@seas.upenn.edu": "combined for optimal performance [58]."
        },
        {
          "alexandermehta@outlook.com": "of our lives.",
          "yangwill@seas.upenn.edu": "An alternative solution that has gained popularity is the Tempo-"
        },
        {
          "alexandermehta@outlook.com": "A common issue with the task of emotion recognition and un-",
          "yangwill@seas.upenn.edu": "ral Convolutional Network (TCN) [5, 44, 59]. The TCN allows for the"
        },
        {
          "alexandermehta@outlook.com": "derstanding is lack of data. This is commonly due to the fact that",
          "yangwill@seas.upenn.edu": "modeling of temporal dependencies in video tasks and time-series"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Mehta and Yang": "2.1.1\nDilated Convolution. The dilation (Ã  trous Convolution) [59]"
        },
        {
          "Mehta and Yang": "allows for a model to have a larger receptive field without increasing"
        },
        {
          "Mehta and Yang": "parameters. Dilated convolution is achieved by introducing \"holes\""
        },
        {
          "Mehta and Yang": "between the points addressed by the kernel, resulting in a larger"
        },
        {
          "Mehta and Yang": "receptive field. The term â€œgapsâ€ will be used to refer to any method"
        },
        {
          "Mehta and Yang": "of expanding a kernel through gaps to widen the receptive field."
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "Introduced in WaveNet"
        },
        {
          "Mehta and Yang": "2.1.2\nDilated Temporal Convolutional Network."
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "[44], a Dilated Temporal Convolutional Network is a temporal"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "network model\nthat computes timesteps in parallel rather than"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "sequentially. This fundamentally alters how the model addresses"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "backpropagation through time by performing backpropogation for"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "all time steps at once rather than following a temporal gradient"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "flow. A casual convolution is used in order to prevent leakage from"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "the past into future steps."
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "Figure 2: TCN architecture"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "A TCN layer can be described as follows for an input sequence"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "ğ‘¥, dilation ğ‘‘, length ğ‘–, dilated convolution âˆ—ğ‘‘ , and filter ğ‘“"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "ğ‘– âˆ’1"
        },
        {
          "Mehta and Yang": "(1)"
        },
        {
          "Mehta and Yang": "ğ¹ (ğ‘¥ğ‘¡ ) = (ğ‘¥ âˆ—ğ‘‘ ğ‘“ )(ğ‘¡) =\nğ‘“ (ğ‘¡) Â· ğ‘¥ğ‘  âˆ’ğ‘‘âˆ—ğ‘– ."
        },
        {
          "Mehta and Yang": "âˆ‘ï¸ ğ‘›"
        },
        {
          "Mehta and Yang": "=0"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "A dilated convolution is used in order to allow a network to un-"
        },
        {
          "Mehta and Yang": "derstand time steps from previous steps efficiently and exponen-"
        },
        {
          "Mehta and Yang": "tially increase the receptive field [59]. Without dilated convolutions,"
        },
        {
          "Mehta and Yang": "TCNs would have a linear receptive field to prior steps. With di-"
        },
        {
          "Mehta and Yang": "lations, the receptive field to previous timesteps (frames) can be"
        },
        {
          "Mehta and Yang": "calculated as"
        },
        {
          "Mehta and Yang": "ğ‘›âˆ’1"
        },
        {
          "Mehta and Yang": "âˆ‘ï¸ ğ‘–\n(2)\nğ‘‘ğ‘– (ğ‘˜ âˆ’ 1),\nğ‘…ğ¹ (ğ‘›, ğ‘‘, ğ‘˜) = 1 +"
        },
        {
          "Mehta and Yang": "=0"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "where ğ‘‘ is the dilation factor, ğ‘˜ is the kernel size, and ğ‘› is the"
        },
        {
          "Mehta and Yang": "number of hidden layers. Commonly, a dilation factor of 2 is used"
        },
        {
          "Mehta and Yang": "in order to achieve an exponential receptive field [5]."
        },
        {
          "Mehta and Yang": "Dilated Temporal Convolutional Networks (TCNs) allow for"
        },
        {
          "Mehta and Yang": "a large amount of temporal data to be processed with low com-"
        },
        {
          "Mehta and Yang": "putation through a large receptive field. TCNs allow for parallel"
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": "computation, a large receptive field, and helps avoid vanishing or"
        },
        {
          "Mehta and Yang": "exploding gradients due to its backpropagation not being parallel"
        },
        {
          "Mehta and Yang": "to the temporal sequence, but rather perpendicular. Dilated TCNs"
        },
        {
          "Mehta and Yang": "have achieved impressive results replicating the long-term memory"
        },
        {
          "Mehta and Yang": "understanding of other architectures like LSTMs and RNNs such"
        },
        {
          "Mehta and Yang": "as the copying memory task [5]. The TCN has also been adopted"
        },
        {
          "Mehta and Yang": "for action segmentation achieving state-of-the-art results on action"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "detection [35]. TCNs have also been explored in emotion analy-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "sis, achieving results above those of LSTMs and RNNs on emotion"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "based tasks [61]. TCNs commonly consist of temporal blocks, which"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "are made up of two convolutional layers that are stacked on top of"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "each other. The purpose of stacking these layers is to ensure that"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "the input data is first scaled to the expected size and then passed"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "through a convolutional layer of the output size."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "2.2\nNeighborhood Attention"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Neighborhood Attention (NA) is an attention method that utilizes"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "a sliding window technique similar to a convolution which views"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "the time series at incriments like a convolution instead of all at"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "once such as self-attention. This is similar to methods such as the"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "SWIN transformer [20, 38] but\nthe main difference comes from"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "how NA allows for overlapping sgements, a method showed to"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "improve performance by ensuring translation equivariance over"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "similar methods [19, 20]. NA was introduced in order to address"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "poor efficiency of self-attention and sliding window techniques by"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "using a tiled algorithm and efficient CUDA kernels published in the"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Nğ´ğ‘‡ğ‘‡ ğ¸ğ‘ library [1]."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "is a method introduced"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Dilated Neighborhood Attention (DiNA)."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "to further address the performance of attention [19]. This dilated"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "transformer works similar to dilated (also known as Ã  trous) con-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "volutions [59]. This improves performance beyond Neighborhood"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "attention by attending to a higher receptive field in less operations"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "than a normal\ntransformer. When ğ‘‘ is a dilation value and ğ‘˜ is"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "a neighborhood (kernel) size, DiNA reduces time complexity of"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "self-attention from ğ‘‚ (ğ‘›2ğ‘‘) to ğ‘‚ (ğ‘›ğ‘‘ğ‘˜)."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "2.2.1\nTCAN. TCAN [17] is a TCN-based model that intertwines"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "attention to maintain receptive field while providing an attention"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "mechanism. This model has seen improvements over the TCN on"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "language datasets. Although the method has seen improvements"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "over the TCN,\nit leads to a significant parameter increase and it"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "doesnâ€™t preserve the casual nature of the TCN, meaning that infor-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "mation flows freely between layers, and loses the temporal property"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "due to leakage."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "3\nMETHODS"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "To enhance the representation of temporal dependencies and their"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "importance in emotional understanding, we propose a method that"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "extends TCNs by incorporating the attention features of Neigh-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "borhood Attention while maintaining causality. We introduce our"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "proposed archetecture that achieves this along with memory and"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "runtime benefits in this section."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "3.1\nNAC-TCN Formulation"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "The Neighborhood Attention with feature extracting Convolutions"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "TCN (NAC-TCN), is a deep learning based approach that utilizes"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Dilated Neighborhood Attention to enforce causality and com-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "bines convolutional operations and self-attention. Our proposed"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "NAC-TCN method incorporates neighborhood self-attention layers"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "within Temporal Blocks with 1D Convolutional Layers to allow the"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "TCN to identify the most important frames through Neighborhood"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Attention and create local filters through 1D Convolutions. A com-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "bination of convolution and attention layers has been shown to"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "omitted for simplicity.",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "is bias corre-\nwhere ğœŒğ›¿\nis ğ‘–â€™s\nğ‘—-th nearest neighbor, ğµ",
          "are\n(b) NAC-TCN Temporal Block": "Since Temporal Blocks commonly upscale or downscale inputs, the"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘— (ğ‘–)\n(ğ‘–,ğœŒğ›¿",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘˜ (ğ‘– ) )",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "residual layer in the NAC-TCN Temporal Block is"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "sponding to two tokens ğ‘– and ğ‘—, ğ‘„ and ğ¾ are query and key projec-",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "tions of ğ‘‹ . The neighboring values of each neighborhood of size ğ‘˜",
          "are\n(b) NAC-TCN Temporal Block": "(8)"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "ğ» (ğ‘¥) = ğ¹ (ğ‘¥) + ğº (ğ‘¥)."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "dilated by a value ğ›¿ is",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "where ğº (ğ‘¥) is an optional 1x1 convolution used when scaling of"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "channels is required. The 1x1 convolution impact is twofold: reduc-"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(cid:105)ğ‘‡",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(ğ‘˜,ğ›¿ )\n(cid:104)ğ‘‰ ğ‘‡\nğ‘‰ ğ‘‡\n. . .\nğ‘‰ ğ‘‡",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(5)\n.\n=",
          "are\n(b) NAC-TCN Temporal Block": "ing dimensions of the network in later layers and providing a way"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "V\nğœŒğ›¿\nğœŒğ›¿\nğœŒğ›¿",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘–\n1 (ğ‘– )\n2 (ğ‘– )\nğ‘˜ (ğ‘– )",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "for the model to translate features from one layer to another while"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "For each sliding window, we can define the output of each pixel",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "maintaining the same overall information as previous layers. DiNA"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "by",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "is not used for this 1x1 convolution because of its inability perform"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(ğ‘˜,ğ›¿ )",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(cid:32) A",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(ğ‘˜,ğ›¿ )",
          "are\n(b) NAC-TCN Temporal Block": "dimension scaling."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "DiNAğ›¿\n(6)",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘– âˆšï¸\nV\nğ‘–\nğ‘˜ (ğ‘–) = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "NAC-TCN reduces parameters compared to the multi-model ap-"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘‘ğ‘˜",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "proach proposed by others [18, 62, 64], the original TCN, and TCAN"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "where ğ·ğ‘–ğ‘ ğ´ is applied to each element ğ‘–, where ğ‘– âˆˆ R1Ã—ğ‘›, and the",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "[17]. This can be attributed to the fact that attention operations,"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "output is then ğ·ğ‘–ğ‘ ğ´(ğ‘–). ğ‘‰ , ğ‘„, and ğ¾ are all calculated in the same",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "which solely consider the kernel size of the neighborhood, are in-"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "manner as self-attention, as ğ·ğ‘–ğ‘ ğ´(ğ‘–) tends to simply self-attention",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "tertwined with the convolution operations, leading to a decrease in"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "as you increase ğ‘˜ and decrease ğ‘‘ to 1.",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "parameters when compared to traditional combined structures."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "In order to stop temporal leakage, we must ensure that for an in-",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "put (ğ‘¥ğ‘¡ |ğ‘¥0, ..., ğ‘¥ğ‘¡ âˆ’1) to each temporal block, the output (ğ‘¦ğ‘¡ |ğ‘¦0, ..., ğ‘¦ğ‘¡ âˆ’1)",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "4\nEXPERIMENTS"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "must be influenced by a time step at least 1 less than ğ‘¡. To ensure",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "In order to evaluate the effectiveness of our TCN methods, we"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "this, we change ğœŒğ›¿",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘— (ğ‘–) to represent the nearest neighbor to the left",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "used a variety of emotion and action recognition datasets, where"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "of the ğ‘–-th value with a dilation ğ›¿. This would mean that for a neigh-",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "newer temporal\ninformation is more relevant than the past. The"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "borhood ğ‘˜, the farthest value referenced is ğ›¿ Â· (ğ‘˜ âˆ’ 1) to the left of",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "image encoder [47] is used as an encoder for all\nğ‘Ÿğ‘’ğ‘”ğ‘›ğ‘’ğ‘¡_ğ‘¦_400ğ‘šğ‘“"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "itself), rather than ğ›¿ Â· (ğ‘˜ âˆ’1)\n.\nthe input sequence (not including the ğ‘¥ğ‘¡",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "2",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "the datasets to ensure that the NAC-TCN is the main factor tested."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "In order to ensure this, we pad DiNAğ›¿",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ‘˜ (ğ‘–) and the convolutions",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "using casual padding in order to make sure that timesteps are not",
          "are\n(b) NAC-TCN Temporal Block": "[23â€“28, 30â€“32] supplies 1,500,000+ anno-"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "The AffWild2 dataset."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "influenced by the future, then removing padding before the next",
          "are\n(b) NAC-TCN Temporal Block": "tated video frames of the valence and arousal metric in 341 videos."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "temporal block to ensure length consistency. In implementation,",
          "are\n(b) NAC-TCN Temporal Block": "A video length of 256 frames is used. Due to the fact that valence"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "this is a standard ğ›¿ Â· (ğ‘˜ âˆ’ 1) zero padding followed by removing",
          "are\n(b) NAC-TCN Temporal Block": "and arousal are between [âˆ’1, 1], tanh is applied to the model output."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "The valence and arousal scores are evaluated and trained on the"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "ğ›¿ Â· (ğ‘˜ âˆ’ 1) elements to the right, removing future timesteps.",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "Concordance Correlation Coefficient (CCC) metric"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "3.4\nResidual Connections",
          "are\n(b) NAC-TCN Temporal Block": "2ğ‘ ğ‘¥ ğ‘¦"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "(9)\n,\nğ¶ğ¶ğ¶ ="
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "Since a network requiring a large receptive field will require an",
          "are\n(b) NAC-TCN Temporal Block": "ğ‘ 2\nğ‘¥ + ğ‘ 2\nğ‘¦ + ( Â¯ğ‘¥ âˆ’ Â¯ğ‘¦)2"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "increase in layers, a residual connection [21] is added to address",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "where ğ‘¥ and ğ‘¦ are predictions and ground truths, 2ğ‘ ğ‘¥ and 2ğ‘ ğ‘¦ are"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "vanishing and shattering gradients problems [7, 55, 60], improve",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "variances, and Â¯ğ‘¥ and Â¯ğ‘¦ are the mean values."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "the loss landscape [36] leading to more stable training and better",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "results. Residual layers are simply described as",
          "are\n(b) NAC-TCN Temporal Block": "[43] provides videos of children annotated"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "The EmoReact dataset."
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "for 8 different emotions. Sequence length of 128 is used. The classes"
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "(7)\nğ» (ğ‘¥) = ğ¹ (ğ‘¥) + ğ‘¥ .",
          "are\n(b) NAC-TCN Temporal Block": ""
        },
        {
          "(a) ğ›¼ represents the Attention. Convolutions and Attention for timesteps other than ğ‘¡ğ‘“": "",
          "are\n(b) NAC-TCN Temporal Block": "are not mutually exclusive and imbalanced. A random sampler and"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "concatenated with attention based models in order to create an",
          "Table 1: Results on AffWild2 Validation. All models repro-": "duced besides competition provided baseline. Bold denotes"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "ensemble of the two for testing.",
          "Table 1: Results on AffWild2 Validation. All models repro-": "indicates the highest performing model."
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "For each evaluated dataset, NAC-TCN was tested in two sizes.",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "The larger size attempted to use similar hyperparameters to the",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "CCC â†‘"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "GRU models while ensuring optimal receptive field (1) through ğ‘˜,",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.17"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "ğ‘‘, and number of layers. The optimal receptive field for all models",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.20"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "besides AFEW-VA and AffWild2 were the length of the sequence,",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.41"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "as only the final",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.42"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "sequence length 32 and AffWild2 used 256 based on prior literature.",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.42"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "The model smaller size still ensured the optimal receptive field, but",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.44"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "attempted to be a equal to smaller size than the GRU and LSTM",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.44"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "models through adjusting previously mentioned hyperparameters",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.46"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "along with the number of channels for the convolutional",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.48"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "This approach allowed us to conduct comparative tests while high-",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "lighting the versatility of the NAC-TCN model in terms of memory",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.48"
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "and computational cost.",
          "Table 1: Results on AffWild2 Validation. All models repro-": ""
        },
        {
          "perparameters as the large NAC-TCN. GRU and LSTM blocks are": "",
          "Table 1: Results on AffWild2 Validation. All models repro-": "0.52"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "binary cross entropy are used to address these issues. The same CNN"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "encoder model from the AffWild2 experiments is used. Evaluation"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "is done using Area under the precision-recall curve (AUC-ROC) to"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "follow similar methodology to prior studies. AUC-ROC tells us for"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "different threshold how a model performs by ploting False Positive"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "(FP) rate and True Positive (TP) rate and defining AUC-ROC as the"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "area under this curve."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "[13, 33] provides valence and arousal annotations to"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "AFEW-VA."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "popular films. These annotations are integers ranging from [-10,10]."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "These are converted to mood labels to compare to prior works [41]."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Accuracy is used as the evaluation metric. The sequence length is"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "32, as videos are much shorter compared to other datasets."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "4.1\nModel Testing Methodology"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "The baseline GRU and LSTM hyperparameters for the AffWild2"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "dataset are chosen to match the prior models tested on the dataset."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Other models such as TCN [5] and TCAN [17] used the same hy-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "perparameters as the large NAC-TCN. GRU and LSTM blocks are"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "concatenated with attention based models in order to create an"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "ensemble of the two for testing."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "For each evaluated dataset, NAC-TCN was tested in two sizes."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "The larger size attempted to use similar hyperparameters to the"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "GRU models while ensuring optimal receptive field (1) through ğ‘˜,"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "ğ‘‘, and number of layers. The optimal receptive field for all models"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "besides AFEW-VA and AffWild2 were the length of the sequence,"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "as only the final\nitem was annotated. AFEW-VA used the entire"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "sequence length 32 and AffWild2 used 256 based on prior literature."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "The model smaller size still ensured the optimal receptive field, but"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "attempted to be a equal to smaller size than the GRU and LSTM"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "models through adjusting previously mentioned hyperparameters"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "along with the number of channels for the convolutional\nlayers."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "This approach allowed us to conduct comparative tests while high-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "lighting the versatility of the NAC-TCN model in terms of memory"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "and computational cost."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "4.2\nImplementation Details"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "We use an Adam Optimizer with a base learning of 0.001 along-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "side an annealing cosine scheduler. We use a batch size of 16 for"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "the AffWild2, EmoReact, and AFEW-VA datasets and all models"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "were trained for 10 epochs. AFEW and AffWild2 all used subject"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "based k-fold cross validation to ensure that information leakage"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "did not occur between testing and training. The validation dataset"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "of AffWild2 was used as the evaluation set and kept separate from"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "training data. The EmoReact dataset had preset train and test splits"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "that were used to be in line with the performance of prior models."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "The best performing model was selected for each method. The same"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "random seed value was selected to ensure reproducibility. The num-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "ber of heads for DiNAT was selected using hyperparameter search"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "({2ğ‘› | 1 â‰¤ ğ‘› â‰¤ 3}). Note that for AffWild2, we build off the open"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "source library provided by Nguyen 22 for testing our new model to"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "ensure consistency when comparing to other methods [42]."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": ""
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "Method"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "Attn."
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "SVM [43]"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "SVM [43]"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "GRU"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "LSTM"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "GRU/Attn."
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "LSTM /Attn."
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "TCN"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "TSN [15]"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "TCAN"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "NAC-TCN (sm)"
        },
        {
          "Table 2: Results on the EmoReact Dataset. Bold denotes indicates the highest performing model. Two variations of our proposed": "NAC-TCN (lg)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NAC-TCN (sm)\n0.86\n1.5": "NAC-TCN (lg)\n0.78\n7.8",
          "453": "2500",
          "âœ“": "âœ“"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "extracted words [63]. For our purposes, we have achieved a state-",
          "453": "",
          "âœ“": "to note that the disparity between the models is minimal, within a Â±"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "of-the-art result in the chosen set of input modalities and encoder",
          "453": "",
          "âœ“": "2% range. Consequently, the AFEW-VA dataset should be regarded"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "choice.",
          "453": "",
          "âœ“": "primarily as a validation of the NAC-TCNâ€™s capacity to maintain"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "",
          "453": "",
          "âœ“": "performance levels akin to those of more expansive models. Nev-"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "5.2\nEmoReact",
          "453": "",
          "âœ“": "ertheless, NAC-TCN outperforms the 1-CNN model which uses"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "Results on EmoReact [43] (Tab. 2) show that with less modalities,",
          "453": "",
          "âœ“": "attention [41]."
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "NAC-TCN Small outperforms other models without multiple modal-",
          "453": "",
          "âœ“": ""
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "",
          "453": "",
          "âœ“": "5.4\nAblation Studies"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "ities or increased training data. This is done with a decrease in pa-",
          "453": "",
          "âœ“": ""
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "rameters and operations. This indicates that a better performing",
          "453": "",
          "âœ“": "In order to understand the impact of different choices we made in"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "architecture like NAC-TCN may be actually outperform even with",
          "453": "",
          "âœ“": "design and experimentation, we perform several ablation studies."
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "less data. NAC-TCN may be more prone to overfitting, given that",
          "453": "",
          "âœ“": ""
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "with similar parameters to GRU and LSTM, it performed similarly",
          "453": "",
          "âœ“": ""
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "",
          "453": "",
          "âœ“": "Table 4: Ablation study comparing residual connection on"
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "and worse to a larger TCN. This highlights that NAC-TCN can be",
          "453": "",
          "âœ“": ""
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "",
          "453": "",
          "âœ“": "NAC-TCN small on the AffWild2 dataset."
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "more expressive with the same hyperparamters, hence strong of",
          "453": "",
          "âœ“": ""
        },
        {
          "NAC-TCN (sm)\n0.86\n1.5": "the smaller model.",
          "453": "",
          "âœ“": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "Method"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "Attn."
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "1-CNN [41]"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "TS (Mood/Î”)[41]"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "TCN"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "GRU"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "LSTM"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "TCAN"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "LSTM/Attn"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "GRU/Attn"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": "NAC-TCN (lg)"
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        },
        {
          "Table 3: Results on AFEW-VA dataset mood labels. Note that": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "timesteps, where an acausal model would weight based on ğ‘˜"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "2 on"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "each side. We find that\nthe causal relation is important\nin both"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "datasets, but is more dramatic in the EmoReact dataset. This sug-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "gests that emotions are better learned when future information is"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "unknown. This phenomenon of better learning with less informa-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "tion can be attributed to two potential reasons. Firstly, emotions"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "inherently involve a causal process, wherein per-frame annota-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "tions occur continuously, thereby influencing annotatorsâ€™ decisions"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "based on prior frames rather than knowledge of future frames. This"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "can lead to different understandings depending on what context"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "is used. Secondly, the disparity between the datasets stems from"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "the variation in label format. AffWild2 employs per-frame labels,"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "allowing for non-causal predictions of adjacent frames, whereas"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "EmoReact utilizes end-of-video labels, thereby elevating the sig-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "nificance of causality (the last frame culminating in information"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "from previous frames rather than â„/2 prior frames). We find that"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "prior literature commonly uses causal relationships over acausal"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "with better results, making it an interesting point of discussion for"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "future work."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "6\nDISCUSSION"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "6.1\nLimitations"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Although our method outperformed on many datasets, performance"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "on AFEW-VA is notably similar to other temporal models. Given"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "AFEW-VA is a smaller dataset, this may indicate that NAC-TCN"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "outperforms other models on larger datasets with more oracle"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "access. Multi-model & pretraining approaches that could perform"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "better were not studied due to hardware limitations and simplicity"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "in results. Our model also holds many of the same flaws of modern"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "TCN based methods, such as higher memory during evaluation"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "(needing the whole sequence instead of hidden state) and poor"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "transfer learning with different ğ‘˜ or ğ‘‘ values."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "6.2\nContribution"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "In this paper, we presented an alternative to the Temporal Convolu-"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "tional Network that allows for attention while decreasing parameters"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "and number of MAC operations. Experimental evaluation revealed"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "improvements over classical methods such as GRUs, LSTMs, and"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Attention-based methods at a lower computational cost. Our method"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "outperforms common temporal methods, improves on the benefits"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "of the TCN, and performs similarly at an efficiency benefit while"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "maintaining the same TCN controls over memory usage."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": ""
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "REFERENCES"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Natten â€“ neighborhood attention extension.\nhttps://github.com/SHI-Labs/\n[1]"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "NATTEN, 2023."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "[2]\nAbdollahi, H., Mahoor, M., Zandie, R., Sewierski, J., and Qualls, S. Artificial"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "emotional intelligence in socially assistive robots for older adults: A pilot study."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "IEEE Transactions on Affective Computing (2022), 1â€“1."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "[3]\nArmandika, F., Djamal, E. C., Nugraha, F., and Kasyidi, F. Dynamic hand"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "gesture recognition using temporal-stream convolutional neural networks.\nIn"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "2020 7th International Conference on Electrical Engineering, Computer Sciences"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "and Informatics (EECSI) (2020), pp. 132â€“136."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "[4]\nArnab, A., Dehghani, M., Heigold, G., Sun, C., LuÄiÄ‡, M., and Schmid, C."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Vivit: A video vision transformer.\nIn Proceedings of the IEEE/CVF International"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "Conference on Computer Vision (ICCV) (October 2021), pp. 6836â€“6846."
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "[5]\nBai, S., Kolter,\nJ. Z., and Koltun, V.\nAn empirical evaluation of generic"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "convolutional and recurrent networks for sequence modeling. arXiv:1803.01271"
        },
        {
          "NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention": "(2018)."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Mehta and Yang": "Zhang, S., An, R., Ding, Y., and Guan, C. Continuous emotion recognition"
        },
        {
          "Mehta and Yang": "using visual-audio-linguistic information: A technical report for abaw3, 2022."
        },
        {
          "Mehta and Yang": "Zhang, S., Zhao, Z., and Guan, C. Multimodal continuous emotion recognition:"
        },
        {
          "Mehta and Yang": "A technical report for abaw5. arXiv preprint arXiv:2303.10335 (2023)."
        },
        {
          "Mehta and Yang": "Zhao, Y., Wang, D., Xu, B., and Zhang, T. Monaural speech dereverberation"
        },
        {
          "Mehta and Yang": "using temporal convolutional networks with self attention.\nIEEE ACM Trans."
        },
        {
          "Mehta and Yang": "Audio Speech Lang. Process. 28 (May 2020), 1598â€“1607."
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        },
        {
          "Mehta and Yang": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Artificial emotional intelligence in socially assistive robots for older adults: A pilot study",
      "authors": [
        "H Abdollahi",
        "M Mahoor",
        "R Zandie",
        "J Sewierski",
        "S Qualls"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Dynamic hand gesture recognition using temporal-stream convolutional neural networks",
      "authors": [
        "F Armandika",
        "E Djamal",
        "F Nugraha",
        "F Kasyidi"
      ],
      "year": "2020",
      "venue": "In 2020 7th International Conference on Electrical Engineering"
    },
    {
      "citation_id": "3",
      "title": "Vivit: A video vision transformer",
      "authors": [
        "A Arnab",
        "M Dehghani",
        "G Heigold",
        "C Sun",
        "M LuÄiÄ‡",
        "C Schmid"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "4",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "5",
      "title": "Trellis networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR"
    },
    {
      "citation_id": "6",
      "title": "The shattered gradients problem: If resnets are the answer, then what is the question?",
      "authors": [
        "D Balduzzi",
        "M Frean",
        "L Leary",
        "J Lewis",
        "K Ma",
        "-D Mcwilliams"
      ],
      "year": "2018",
      "venue": "The shattered gradients problem: If resnets are the answer, then what is the question?"
    },
    {
      "citation_id": "7",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "End-to-end object detection with transformers"
    },
    {
      "citation_id": "8",
      "title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van MerriÃ«nboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "9",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "M2fnet: Multi-modal fusion network for emotion recognition in conversation"
    },
    {
      "citation_id": "10",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "11",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2019",
      "venue": "Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "12",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "13",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "14",
      "title": "Visual robotic perception system with incremental learning for child-robot interaction scenarios",
      "authors": [
        "N Efthymiou",
        "P Filntisis",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2021",
      "venue": "Technologies"
    },
    {
      "citation_id": "15",
      "title": "Deep self-attention network for facial emotion recognition",
      "authors": [
        "A Gupta",
        "S Arunachalam",
        "R Balakrishnan"
      ],
      "year": "2020",
      "venue": "Third International Conference on Computing and Network Communications"
    },
    {
      "citation_id": "16",
      "title": "Temporal convolutional attention-based network for sequence modeling",
      "authors": [
        "H Hao",
        "Y Wang",
        "Y Xia",
        "J Zhao",
        "F Shen"
      ],
      "year": "2020",
      "venue": "Temporal convolutional attention-based network for sequence modeling",
      "arxiv": "arXiv:2002.12530"
    },
    {
      "citation_id": "17",
      "title": "Temporal convolutional attention-based network for sequence modeling",
      "authors": [
        "H Hao",
        "Y Wang",
        "Y Xia",
        "J Zhao",
        "F Shen"
      ],
      "year": "2020",
      "venue": "Temporal convolutional attention-based network for sequence modeling"
    },
    {
      "citation_id": "18",
      "title": "Dilated neighborhood attention transformer",
      "authors": [
        "A Hassani",
        "H Shi"
      ],
      "venue": "Dilated neighborhood attention transformer"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "A Hassani",
        "S Walton",
        "J Li",
        "S Li",
        "H Shi"
      ],
      "venue": ""
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "21",
      "title": "An empirical exploration of recurrent network architectures",
      "authors": [
        "R Jozefowicz",
        "W Zaremba",
        "I Sutskever"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition and its applications",
      "authors": [
        "A KoÅ‚akowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wrobel"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3"
    },
    {
      "citation_id": "23",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "24",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "25",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "26",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "27",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition"
    },
    {
      "citation_id": "29",
      "title": "Expression, affect, action unit recognition: Affwild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Affwild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "30",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "31",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "32",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "33",
      "title": "Temporal convolutional networks for action segmentation and detection",
      "authors": [
        "C Lea",
        "M Flynn",
        "R Vidal",
        "A Reiter",
        "G Hager"
      ],
      "year": "2016",
      "venue": "Temporal convolutional networks for action segmentation and detection"
    },
    {
      "citation_id": "34",
      "title": "Temporal convolutional networks for action segmentation and detection",
      "authors": [
        "C Lea",
        "M Flynn",
        "R Vidal",
        "A Reiter",
        "G Hager"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Visualizing the loss landscape of neural nets",
      "authors": [
        "H Li",
        "Z Xu",
        "G Taylor",
        "C Studer",
        "T Goldstein"
      ],
      "year": "2018",
      "venue": "Visualizing the loss landscape of neural nets"
    },
    {
      "citation_id": "36",
      "title": "Network in network",
      "authors": [
        "M Lin",
        "Q Chen",
        "S Yan"
      ],
      "year": "2014",
      "venue": "Network in network"
    },
    {
      "citation_id": "37",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Swin transformer: Hierarchical vision transformer using shifted windows"
    },
    {
      "citation_id": "38",
      "title": "Multi-modal emotion estimation for in-the-wild videos",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "Y Deng",
        "R Li",
        "Y Wu",
        "J Zhao"
      ],
      "year": "2022",
      "venue": "Multi-modal emotion estimation for in-the-wild videos",
      "arxiv": "arXiv:2203.13032"
    },
    {
      "citation_id": "39",
      "title": "Early recognition of sepsis with gaussian process temporal convolutional networks and dynamic time warping",
      "authors": [
        "M Moor",
        "M Horn",
        "B Rieck",
        "D Roqeiro",
        "K Borgwardt"
      ],
      "year": "2019",
      "venue": "Machine Learning for Healthcare Conference"
    },
    {
      "citation_id": "40",
      "title": "Focus on change: Mood prediction by learning emotion changes via spatio-temporal attention",
      "authors": [
        "S Narayana",
        "R Subramanian",
        "I Radwan",
        "R Goecke"
      ],
      "year": "2023",
      "venue": "Focus on change: Mood prediction by learning emotion changes via spatio-temporal attention"
    },
    {
      "citation_id": "41",
      "title": "An ensemble approach for facial behavior analysis in-the-wild video",
      "authors": [
        "H.-H Nguyen",
        "V.-T Huynh",
        "S.-H Kim"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "42",
      "title": "Emoreact: a multimodal approach and dataset for recognizing emotional responses in children",
      "authors": [
        "B Nojavanasghari",
        "T BaltruÅ¡aitis",
        "C Hughes",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th acm international conference on multimodal interaction"
    },
    {
      "citation_id": "43",
      "title": "A generative model for raw audio",
      "authors": [
        "A Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu",
        "Wavenet"
      ],
      "year": "2016",
      "venue": "A generative model for raw audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "44",
      "title": "On the difficulty of training recurrent neural networks",
      "authors": [
        "R Pascanu",
        "T Mikolov",
        "Y Bengio"
      ],
      "year": "2012",
      "venue": "30th International Conference on Machine Learning, ICML 2013"
    },
    {
      "citation_id": "45",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversation: Research challenges, datasets, and recent advances"
    },
    {
      "citation_id": "46",
      "title": "Designing network design spaces",
      "authors": [
        "I Radosavovic",
        "R Kosaraju",
        "R Girshick",
        "K He",
        "P DollÃ¡r"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "47",
      "title": "Emotion recognition on large video dataset based on convolutional feature extractor and recurrent neural network",
      "authors": [
        "D Rangulov",
        "M Fahim"
      ],
      "year": "2020",
      "venue": "2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS)"
    },
    {
      "citation_id": "48",
      "title": "Facial emotion recognition: A multi-task approach using deep learning",
      "authors": [
        "A Saroop",
        "P Ghugare",
        "S Mathamsetty",
        "V Vasani"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: A multi-task approach using deep learning"
    },
    {
      "citation_id": "49",
      "title": "Deep learning-based classification of posttraumatic stress disorder and depression following trauma utilizing visual and auditory markers of arousal and mood",
      "authors": [
        "K Schultebraucks",
        "V Yadav",
        "A Shalev",
        "G Bonanno",
        "I Galatzer-Levy"
      ],
      "year": "2022",
      "venue": "Psychological Medicine"
    },
    {
      "citation_id": "50",
      "title": "Flops-counter pytorch",
      "authors": [
        "V Sovrasov"
      ],
      "year": "2021",
      "venue": "Flops-counter pytorch"
    },
    {
      "citation_id": "51",
      "title": "Efficient object localization using convolutional networks",
      "authors": [
        "J Tompson",
        "R Goroshin",
        "A Jain",
        "Y Lecun",
        "C Bregler"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "52",
      "title": "Training data-efficient image transformers and distillation through attention",
      "authors": [
        "H Touvron",
        "M Cord",
        "M Douze",
        "F Massa",
        "A Sablayrolles",
        "H JÃ©gou"
      ],
      "year": "2021",
      "venue": "Training data-efficient image transformers and distillation through attention"
    },
    {
      "citation_id": "53",
      "title": "",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "54",
      "title": "Residual networks behave like ensembles of relatively shallow networks",
      "authors": [
        "A Veit",
        "M Wilber",
        "S Belongie"
      ],
      "year": "2016",
      "venue": "Residual networks behave like ensembles of relatively shallow networks"
    },
    {
      "citation_id": "55",
      "title": "",
      "authors": [
        "S Walton",
        "A Hassani",
        "X Xu",
        "Z Wang",
        "H Shi",
        "Stylenat"
      ],
      "venue": ""
    },
    {
      "citation_id": "56",
      "title": "Co-scale conv-attentional image transformers",
      "authors": [
        "W Xu",
        "Y Xu",
        "T Chang",
        "Z Tu"
      ],
      "year": "2021",
      "venue": "Co-scale conv-attentional image transformers"
    },
    {
      "citation_id": "57",
      "title": "Attention boosted deep networks for video classification",
      "authors": [
        "J You",
        "J Korhonen"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "58",
      "title": "Multi-scale context aggregation by dilated convolutions",
      "authors": [
        "F Yu",
        "V Koltun"
      ],
      "year": "2016",
      "venue": "Multi-scale context aggregation by dilated convolutions"
    },
    {
      "citation_id": "59",
      "title": "Norm-preservation: Why residual networks can become extremely deep?",
      "authors": [
        "A Zaeemzadeh",
        "N Rahnavard",
        "M Shah"
      ],
      "year": "2020",
      "venue": "Norm-preservation: Why residual networks can become extremely deep?"
    },
    {
      "citation_id": "60",
      "title": "Continuous emotion recognition using visual-audio-linguistic information",
      "authors": [
        "S Zhang",
        "R An",
        "Y Ding",
        "C Guan"
      ],
      "year": "2022",
      "venue": "A technical report for abaw"
    },
    {
      "citation_id": "61",
      "title": "Continuous emotion recognition using visual-audio-linguistic information",
      "authors": [
        "S Zhang",
        "R An",
        "Y Ding",
        "C Guan"
      ],
      "year": "2022",
      "venue": "A technical report for abaw"
    },
    {
      "citation_id": "62",
      "title": "Multimodal continuous emotion recognition: A technical report for abaw5",
      "authors": [
        "S Zhang",
        "Z Zhao",
        "C Guan"
      ],
      "year": "2023",
      "venue": "Multimodal continuous emotion recognition: A technical report for abaw5",
      "arxiv": "arXiv:2303.10335"
    },
    {
      "citation_id": "63",
      "title": "Monaural speech dereverberation using temporal convolutional networks with self attention",
      "authors": [
        "Y Zhao",
        "D Wang",
        "B Xu",
        "T Zhang"
      ],
      "year": "2020",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    }
  ]
}