{
  "paper_id": "2503.18998v1",
  "title": "Face: Few-Shot Adapter With Cross-View Fusion For Cross-Subject Eeg Emotion Recognition",
  "published": "2025-03-24T03:16:52Z",
  "authors": [
    "Haiqi Liu",
    "C. L. Philip Chen",
    "Tong Zhang"
  ],
  "keywords": [
    "EEG emotion recognition",
    "few shot learning",
    "adapter",
    "cross-view fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-subject EEG emotion recognition is challenged by significant inter-subject variability and intricately entangled intra-subject variability. Existing works have primarily addressed these challenges through domain adaptation or generalization strategies. However, they typically require extensive target subject data or demonstrate limited generalization performance to unseen subjects. Recent few-shot learning paradigms attempt to address these limitations but often encounter catastrophic overfitting during subject-specific adaptation with limited samples. This article introduces the few-shot adapter with a crossview fusion method called FACE for cross-subject EEG emotion recognition, which leverages dynamic multi-view fusion and effective subject-specific adaptation. Specifically, FACE incorporates a cross-view fusion module that dynamically integrates global brain connectivity with localized patterns via subject-specific fusion weights to provide complementary emotional information. Moreover, the few-shot adapter module is proposed to enable rapid adaptation for unseen subjects while reducing overfitting by enhancing adapter structures with meta-learning. Experimental results on three public EEG emotion recognition benchmarks demonstrate FACE's superior generalization performance over state-of-the-art methods. FACE provides a practical solution for cross-subject scenarios with limited labeled data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "U NDERSTANDING Human emotions is fundamental and crucial to advancing fields such as human-computer interaction  [1]  and mental health  [2] . Electroencephalography (EEG) has recently emerged as a remarkable tool for capturing subject's neural responses to emotional states  [3] . EEG-based emotion recognition remains challenging due to the substantial inter-subject variance in brain activity patterns  [4] ,  [5] . Additionally, intra-subject variance arises from the non-stationary nature of EEG signals, which exhibit variations in frequency and amplitude over time within the same subject. In most cases, these variances entangle with each other, diminishing the ability of models trained on one group to generalize effectively to unseen subjects.\n\nExisting studies are mainly dedicated to addressing the inter-subject variance through domain-adaptation and domaingeneralization strategies. Domain-adaptation methods learn subject-specific representations by aligning the marginal distributions of source and target subjects  [6] . Although these methods  [5] ,  [7] ,  [8]  have achieved notable results, they require large amounts of unlabeled target subject data during training. This requirement restricts their applicability in scenarios with scarce or non-specific target subject data  [9] . In contrast, domain-generalization methods attempt to learn subjectinvariant representations from source data  [10] . However, they often struggle to generalize to unseen target domains due to their dependence on source data diversity  [11] . Moreover, the absence of target domain data in training makes it challenging for these methods to estimate subtle domain shifts and capture target subject-specific patterns. Recently, few-shot learning  [12]  (FSL) approaches have emerged as a promising alternative to address this issue. These methods  [13] -  [15]  leverage the FSL method to learn a whole subject-agnostic initialization that can quickly adapt to new subjects using only a few labeled samples (See Fig.  1 ). Nonetheless, the scarcity of labeled EEG data from target subjects often leads to catastrophic overfitting when performing whole model parameter updates with limited training steps. These limitations result in suboptimal performance for novel subjects. Based on these observations, this article encourages a partial parameter update strategy for target subject adaptation, prioritizing rapid adaptation over full model fine-tuning. We introduce the Few-shot Adapter module which incorporates the adapter structure with the meta-learning paradigm. This design facilitates capturing subject-specific emotional discriminative cues while reducing overfitting under few-shot conditions.\n\nFurthermore, addressing intra-individual variance in EEG signals remains challenging  [16] . Earlier studies attempted to mitigate this issue by introducing statistical features such as power spectral density (PSD) and phase-locking value (PLV) to capture complementary information of the EEG signal  [17] . However, these methods suffer from heterogeneous feature spaces that lead to redundant information and noise accumulation  [18] . Recent work has shifted toward learning discriminative patterns automatically, leveraging multi-scale learning  [19] , contrastive learning  [20] , or spatial-temporal attention  [21]  within graph convolutional networks (GCNs). Although these approaches can reveal complex brain functional connectivity associated with emotion, they typically rely on a single view information or uniform fusion weights across all unseen subjects, limiting their capacity to fully exploit subject-specific discriminative details. It is reasonable to assume that fusion weights for different views should also be subject-specific because inter-subject variance is inherently coupled with intra-subject variance in EEG signals. To address this gap, this article proposes a cross-view fusion module that dynamically learns a unified representation across different views with subject-specific fusion weights.\n\nTo address the above challenges, this article proposes a few-shot adapter with a cross-view fusion framework named FACE for cross-subject EEG emotion recognition. Departing from existing FSL-based methods in this domain, FACE innovatively incorporates complementary spatial view information and encourages subject-specific adaptation from both the feature fusion and adjustment aspects only, thereby achieving superior cross-subject generalization. Specifically, FACE comprises two key components: a cross-view fusion (CVF) module and a few-shot adapter (FSA) module. Recent studies  [22] ,  [23]  have demonstrated that emotional processing depends on both the activity of specific brain regions and the coordination of large-scale brain networks. Building on this insight, the CVF module extends the conventional GCN view by introducing a cross-view architecture. The GCN view extracts global connectivity patterns, while a complementary local view identifies region-specific emotional information. The CVF module projects two view representations into a shared space and performs subject-specific complementary fusion with meta-learning. Moreover, inspired by the adapter structure in the computer vision field, this article introduces a FSA module to enable rapid calibration for unseen subjects in the EEG emotion recognition task. Due to the significant intersubject variability in EEG signals, this article incorporates batch normalization layers into each residual adapter layer to statistically regularize inter-subject variability. Moreover, the adapter employs a meta-learning paradigm to achieve more effective calibration. To the best of our knowledge, this is the first work to apply an adapter mechanism to address cross-domain EEG emotion recognition. Comprehensive evaluations on three widely used benchmarks (SEED, SEED-IV, and SEED-V) demonstrate that FACE outperforms state-of-the-art cross-subject EEG emotion recognition methods.\n\nIn summary, this work presents the following contributions: 1) This article proposes FACE, a novel method to enhance cross-subject EEG emotion recognition through subject-specific adaptation via joint feature fusion and adjustment. Extensive experiments on three widely used benchmarks demonstrate that FACE achieves comparable performance with state-of-the-art approaches.\n\n2) The CVF module effectively combines global brain connectivity and local patterns, utilizing meta-learned cross-view fusion. It dynamically provides comprehensive subject-specific unified representation.\n\n3) The FSA module incorporates the adapter structure and meta-learning paradigm to enable rapid subject-specific emotional feature adjustment for unseen subjects. The remainder of the article is organized as follows. Section II reviews the relevant literature. Section III details the proposed FACE model. Section IV presents the experimental results and comparative analysis. Section V critically examines the impact of each module and the parameter settings. Finally, Section VI summarizes our contributions and discusses future research directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Cross-Subject Eeg Emotion Recognition",
      "text": "Cross-subject EEG emotion recognition has garnered significant attention due to its pivotal role in brain-computer interfaces  [24] . The task is particularly challenging due to the inherent non-stationarity of EEG signals and the complexity of human emotional expression. Early approaches to this task focused on handcrafted statistical features combined with simple classifiers, which yielded limited performance  [25] .\n\nRecent works  [19] ,  [20] ,  [26]  primarily address this issue in a supervised learning manner. Song et al. proposed the instanceadaptive graph network, which explores dynamic connectivity across brain regions  [26] . GMSS  [20]  leveraged multi-task self-supervision for generalized representations. Jin et al. introduced the pyramidal GCN, which fully exploits multi-scale information to improve performance  [19] . More recently, semisupervised learning paradigms have been applied to EEG emotion recognition to refine individual-specific mappings better. Li et al. combined meta-learning and semi-supervised learning to facilitate subject-adaptation representations  [27] . Guarneros et al. proposed semi-supervised multi-source joint distribution adaptation to align distributions across different subjects  [28] . Despite these advances, supervised learning methods fail to explicitly model inter-subject variability, often resulting in poor performance for specific subjects. Additionally, semisupervised methods require extensive unlabeled target data and suffer from multi-objective optimization instability. To address these issues, this article investigates the problem of few-shot cross-subject emotion recognition, relaxing the requirement for unlabeled data and learning subject-specific emotional representations with few labeled data from the target subject.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Inter-Subject Variance In Eeg Emotion Recognition",
      "text": "Inter-subject variability in EEG emotion recognition mainly arises from individual differences in brain structure and cognitive processes. These variations pose significant challenges for developing generalized EEG emotion recognition models. Recently, Domain Adaptation (DA) and Domain Generalization (DG) techniques have emerged as promising solutions to mitigate the impact of inter-subject variance  [29] . DA methods  [5] ,  [7] ,  [8] ,  [30]  aim to bridge the distributional gaps between source and target subjects to improve the performance of the target subject. They leverage various divergence metrics (e.g. MMD  [31] ) to qualify and minimize distributional shifts. Chen et al. introduced a multi-source marginal distribution strategy to preserve the marginal distributions of EEG data during adaptation  [7] . Li et al. proposed a dynamic domain adaptation approach, which dynamically adapts domains based on class-aware information  [8] . Wu et al. developed the graph orthogonal purification network, which aligns the distribution of emotional features across dual spaces  [5] . However, DA methods are limited in applicability due to their reliance on target subject information during training. Unlike DA, DG methods  [4] ,  [32] -  [34]  attempt to learn generalized models that perform well across multiple domains without relying on explicit adaptation to specific target domains. Chen et al. proposed a graph domain disentanglement network to learn subject-invariant representations  [4] . Ugan  [33]  was introduced to model and constrain potential uncertain statistical shifts across individuals comprehensively. Liu et al. developed a sparse mixture of graph experts model, which learns transferable features by decomposing brain regions into simpler functional units  [34] . Despite their effectiveness, these methods remain in suboptimal performance due to the absence of target domain information. This fundamental limitation drives our exploration of few-shot learning paradigms for EEG-based emotion recognition, particularly focusing on scenarios where only minimal labeled target subject data is available.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Few-Shot Learning",
      "text": "Few-shot learning (FSL) is an advanced paradigm that facilitates rapid model adaptation to novel classes with only a few labeled examples  [7] ,  [35] ,  [36] . This paradigm has recently gained attraction in brain-computer interface research. Ning et al. pioneered its application to cross-subject EEG-based emotion recognition by employing a prototypical network  [37]  for efficient cross-subject generalization  [38] . Zhang et al. developed EmoDSN  [39] , a deep Siamese network architecture that achieves rapid convergence with minimal training data while maintaining robust performance in fine-grained emotion recognition tasks. These metric-learning-based methods, which focus on learning a metric space that generalizes to new subjects without requiring fine-tuning, exhibit similarities with DG methods  [11] . Current research predominantly focuses on meta-learning strategies for cross-subject EEG emotion recognition. MetaEmotionNet  [40]  and MAML-EEG  [14]  adapted the model-agnostic meta-learning  [41]  framework to EEG emotion recognition, demonstrating promising performance on novel subjects. FreAML  [13]  introduced a Bayesian meta-learning framework that derives task-specific model parameters from the frequency domain. SIML  [15]  further hybridized conventional supervised learning with meta-learning to optimize EEG classifier training. These methods generally aim to learn a subject-agnostic initialization that can quickly adapt to new subjects with few labeled samples. Despite these advances, existing methods face inherent limitations in computational efficiency and adaptation flexibility when handling heterogeneous neurophysiological patterns. Differently, this article advocates a partial parameter update strategy for target subject adaptation, emphasizing rapid adaptation while reducing overfitting over exhaustive model fine-tuning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. The Adopted Methodology",
      "text": "This section introduces a few-shot adapter with a cross-view fusion approach for cross-subject EEG emotion recognition. The overall architecture of the proposed FACE framework is illustrated in Fig.  2 . FACE consists of two core components: the few-shot adapter module and the cross-view fusion module. The following sections provide a detailed explanation of each component.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Problem Definition",
      "text": "In the cross-subject EEG emotion recognition task, each subject can be considered a distinct domain due to substantial inter-subject variance. Accordingly, the data is divided into a source domain for training and a target domain for testing. Let the source domain be defined as\n\nwhere S i ∈ S source denotes the i-th source subject, x i ∈ X represents EEG signals, and y i ∈ Y is the corresponding emotion label. The objective of cross-subject emotion recognition is to learn a model f θ : X → Y trained on D s that generalizes to unseen target subjects S t ∈ S target , where S source ∩ S target = ∅. The core challenge arises from domain shifts between source and target subjects, characterized by P (X | S s ) ̸ = P (X | S t ) , where S s ∈ S source . Existing methods often leverage auxiliary data from the target domain, such as few labeled D l t = {(x i , y i , S t ) , i = 1, .., j} and D u t = {x, S t } , to mitigate distribution shifts. The few shot crosssubject emotion recognition extends this task by imposing a strict few-shot constraint: For target subject S t , the model must perform few-steps adaptation utilizing only a small support set\n\nwhere k (k ≤ 10) represents the number of labeled samples per emotion class c, and generalize to the query set\n\nHere, M represents the total number of remaining unlabeled samples from the target subject after excluding D sup t . This paradigm is referred to as a c-way k-shot task. Formally, the model f θ initially trained on the source domain D s must generalize to target subject S t via few steps optimization using limited labeled samples D sup t , such that:\n\nwhere f θ ′ is the updated model and L(•) is the loss function, i.e., cross-entropy loss.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Spatial Input",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cnn Encoder",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Graph Input",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Gcn Backbone",
      "text": "Current methodologies for EEG-based emotion recognition commonly employ GCNs to capture discriminative brain topology from EEG signals. This article employs the dynamic GCN (DGCN)  [42]  as the main encoder. EEG signals are pre-processed into differential entropy (DE) features across multiple frequency bands, forming representative feature X ∈ R N ×C×B where N , C and B denote the number of samples, channels, and frequency bands, respectively. Each EEG sample x ∈ X can be represented as a graph G = (V, E), where the node set V = {v 1 , . . . , v C } corresponds to EEG channels, and the edges E are weighted by a dynamic adjacency matrix A d ∈ R C×C that quantifies pairwise channel dependencies. The dynamic adjacency matrix can be formulated as:\n\nwhere σ 1 and σ 2 denote different activation functions. A 0 represents a randomly initialized matrix, and\n\nr ×C) are learnable projection matrices with a reduction ratio r.\n\nThe dynamic GCN processes X through two-layers GCN to derive high-level representations which can be formally expressed as:\n\nwhere Θ 1 , Θ 2 ∈ R 1×K are convolution kernels with stride 1, * denotes convolution operation, and BN represents the batch normalization layer. The node aggregation is performed with residual operation to obtain the hierarchical feature representations, which can be expressed as:\n\nwhere L = D -1 A d denotes the graph Laplacian. The hierarchical feature Z g is finally fed into a fully connected layer for final emotion classification. The final emotion prediction y is obtained via:\n\nThe model is optimized by minimizing the cross-entropy loss as follow:\n\nwhere y i,j is the ground-truth one-hot label of the i-th sample, and y i,j represents the prediction for class j.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Cross-View Fusion Module",
      "text": "While the DGCN can effectively capture global brain topology in EEG signals, local subtle patterns are equally critical for emotion recognition. To obtain stronger feature representation and mitigate the effects of intra-subject variability, this article proposes the cross-view fusion module which effectively integrates both global and spatial-local patterns.\n\n1) Spatial Feature Extraction: EEG signals are recorded from multiple scalp-mounted electrodes. Each electrode monitors electrical activity in specific brain regions. The spatial arrangement of these electrodes preserves the brain's intrinsic topographic organization. These spatial patterns in EEG data reveal critical insights into emotion-related functional connectivity and neural activation dynamics. It is natural to leverage such patterns to provide auxiliary information for more precise prediction.\n\nGiven the input EEG signals X ∈ R N ×C×B , this article first performs spatial projection Φ(•) based on physical electrode arrangements  [38] ,  [43] :\n\nwhere S represents the spatial form of X. We apply a threelayer CNN encoder to obtain the deep spatial feature Z s from S. To preserve fine-grained spatial resolution, we omit final pooling operation and flatten spatial feature in channel dimension. Finally, the deep spatial feature Z s ∈ R N ×D is utilized for further fusion.\n\n2) Cross-view Fusion Mechanism: The significant inherent intra-subject variance in EEG signals introduces a critical challenge for fusing heterogeneous views. Existing approaches typically process these views independently or employ simple fusion strategies, failing to capture their complex interdependency and potentially introducing feature misalignment. Moreover, inter-subject variability further complicates the fusion since the relationship between the two views may shift across subjects. Drawing inspiration from dynamic fusion strategies  [44] ,  [45]  in other fields, this article proposes a novel cross-view fusion mechanism that dynamically generates a unified representation from heterogeneous views for unseen subjects, thereby bridging this gap.\n\nTo establish space compatibility for cross-view interaction, we apply a learnable transformation to project the spatial feature Z s into the semantic space of Z g as follow:\n\nThe aligned representation Z s is then concatenated with Z g to form the joint representation Z c = [ Z s ; Z g ]. Then, the cross-view dependencies Z △ between Z s and Z g are captured through a multi-head self-attention mechanism formulated as:\n\nL denotes the number of attention heads. d h is the dimensionality of key. And W o is the learnable projection matrix aggregating the head outputs. The dependencies dynamically recalibrate Z s through residual refinement Z′ s = Zs + Z △ . The unified representation Z u is obtained through channelwise concatenation:\n\nTo reduce the effect of inter-subject variance in view fusion, we reconceptualize the multi-head self-attention parameters as subject-agnostic meta parameters optimized via modelagnostic meta-learning. These parameters undergo episodic training through model-agnostic meta-learning paradigm  [41] , enforcing generalization across different subjects through bilevel optimization. Since the few-shot adapter module inherently involves this optimization framework, we will systematically elaborate on the optimization strategy in the subsequent subsection.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Few-Shot Adapter Module",
      "text": "Acquiring a subject-agnostic unified representation via the CVF module remains insufficient due to the pronounced intersubject variability in EEG signals. While unified representations provide comprehensive features, they necessitate further refinement to capture discriminative subject-specific emotional patterns. Inspired by adapter-based architectures  [46]  in CV, this work devises a novel few-shot adapter integrated with meta-learning principles to preserves individualized emotional features.\n\nThis article enhances conventional residual adapters through strategic integration of batch normalization layers to statistically regularizes inter-subject variance as follows:\n\nwhere W l1 ∈ R d h ×d and W l2 ∈ R d×d h (d ≪ d h ) enforce a bottleneck for efficient adaptation. The stacked batch normalization layers statistically regularize cross-subject distribution shifts by whitening feature statistics. For clarity, we omit bias terms in Eq.  (11) . The predicted emotion can be computed using a fully connected layer:\n\nTo enable the few-shot adapter to rapidly and effectively capture subject-specific emotional cues, we incorporate MAML's optimization approach  [41]  into its training. The adapter parameters θ A are jointly optimized with the multihead self-attention parameters θ M and the prediction layer θ Wc via bi-level optimization. Our key innovation lies in formulating the adaptation process as a partial meta-learning problem. We treat each subject as a task T under the MAML paradigm. Since the MAML paradigm follows episodic learning, we randomly sample n subjects per episode for bi-level optimization. For each subject T i ∼ p(T ), we randomly sample a support set D sup i and a query set D que i . Let the meta-optimized parameters Θ = {θ M , θ A , θ Wc }. The whole model parameters Θ all = {θ gcn , θ cnn , Θ} where θ gcn and θ cnn represent the pre-trained weights of the GCN and CNN encoders, respectively. The parameters Θ are optimized to subject-specific parameters Θ ′ i via m-steps gradient descent on D sup i in the inner loop:\n\nwhere α the inner-loop learning rate. In this study, we set α = 0.01. Through n inner-loop optimizations performed on n subjects, we obtain n sets of subject-specific model parameters. Our objective is to identify an optimal initialization plane that enables efficient convergence to subject-optimal parameters through few-step gradient updates. To achieve this, we should evaluates the performance of inner-loop optimized models while systematically aggregating second-order gradient information across n subjects. This meta-optimization in outerloop is formally expressed as:\n\nHere, Θ ′ all i = {θ gcn , θ cnn , Θ ′ i }. Notably, the labels of D que i is available for the source subjects. The meta-optimization updates whole model parameters through second-order gradients:\n\nwith β = 0.001 denoting the outer loop learning rate. The proposed framework yields a subject-agnostic parameter initialization Θ ′ all through N e episodes of bi-level optimization during the meta-training phase. The complete training procedure is formalized in Algorithm 1.\n\nDuring cross-subject evaluation, the proposed framework demonstrates efficient adaptation capability through its metalearned parameter initialization Θ ′ all . When encountering novel target subjects S t , only k-shot labeled EEG samples per class need to be collected as the support set D sup t to perform rapid parameter adaptation via inner-loop optimization:\n\nWhen k = 3, the proposed framework achieves performance comparable to state-of-the-art methods. for m = 1 to m-steps do 9:\n\nend for 11:\n\nend for 13:\n\nΘ ′ all ← Θ all -β∇ Θ all 15: end for 16: return Θ ′ all IV. PERFORMANCE EVALUATION The benchmarks are described in Section IV-A, followed by a detailed implementation of the proposed approach in Section IV-B. Comparative experimental results with different methods are presented in Sections IV-C1 and IV-C2, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "This article evaluates the performance of the proposed approach across three well-established EEG emotion recognition benchmarks, including SEED  [47] , SEED-IV  [48] , and SEED-V  [49] . A detailed description of these benchmarks is provided below:\n\n• SEED  [47]",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experiment Setup",
      "text": "Network Architecture: This article employs DGCN and Conv-3 as the main backbone. Following previous work  [42] , the DGCN encoder utilizes a two-layer residual architecture without incorporating K-order Chebyshev polynomials. For this branch, DE features from five frequency bands are concatenated to form the input of size N × 62 × 5, where N is the number of samples. Conv-3 consists of 3 blocks. Each block contains a 3 × 3 kernel, a batch normalization layer, a ReLU activation layer, and a 2 × 2 max-pooling layer. To enable effective spatial learning from EEG signals, the 62channel DE feature is first projected onto a 2D grid following the physical electrode arrangement on a 9 × 9 map  [38] . This spatial representation is subsequently interpolated to 32 × 32 resolution. The final input size of the Conv-3 branch is N × 32 × 32 × 5.\n\nEvaluation Protocol: This article conducted cross-subject experiments to evaluate the generalization performance of FACE. Following previous studies, this article employed a leave-one-subject-out (LOSO) strategy. In LOSO, each subject serves as the target for testing, while the remaining subjects are used for training. The final performance is reported as the average across all target subjects. Given the few-shot learning paradigm, the model requires a few labeled samples from the target subject. This article randomly shuffles the target subject's samples and selected K samples per class for meta-training, with the remaining samples used for testing. Despite the limited sample size (approximately 1%/5%/8% of samples in SEED/SEED-IV/SEED-V datasets under the 10shot setting), the proposed method maintains comparability with domain adaptation/generalization-based approaches. To mitigate the risk of overfitting, this article repeated the experiment 200 times for each subject and reported the average performance.\n\nImplementation Details: Recent work has shown that pretrained models for whole-subject classification have improved the transferability of novel subjects  [15] . This article utilizes a two-stage approach involving pre-training and few-shot ∼ indicates an approximation. Since sample amounts vary across subjects, SDDA uses half of the target labeled samples for semi-supervised learning, allowing only an estimation of labeled target samples. learning for training. For the pre-training stage, this article performs conventional whole-subject emotion classification. The model minimizes the smooth cross-entropy  [50]  loss across all source subjects under a standard supervised learning scheme. We employ the Adam optimizer with a learning rate 0.001 and train the model from scratch for 50 epochs. After pre-training, the few-shot learning paradigm is adopted. The model is further optimized using the Adam optimizer with a learning rate 0.001 over 50 episodes. For each episode, we randomly sample N subjects. From each subject, (K + Q) samples are randomly selected, where K samples are used for a 10-step fast update, and Q samples are used to evaluate the updated model. The smooth cross-entropy loss is computed across the N subjects to optimize the model, aiming to learn a good initialization that generalizes across all source subjects. In our experiments, N is optimally set to 2, K is chosen from {1, 3, 5, 10}, and Q is set to 20 during training and to the remaining samples of the target subject during testing. All experiments are implemented using the PyTorch framework and conducted on an AMD Instinct MI250 GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Comparison With State-Of-The-Art Approaches",
      "text": "This study presents a comprehensive comparison between the proposed method and existing state-of-the-art approaches, including three supervised learning methods (IAG  [26] , GMSS  [20] , and PGCN  [19] ), three semi-supervised learning methods (SDDA  [8] , SSML  [27] , and FSA-TSP  [28] ), three domain adaptation-based methods (MS-MDA  [7] , UDDA  [8] , and Grop  [5] ), four domain generalization-based methods (GMoE  [52] , MOGE  [34] , GDDN  [4] , and Ugan  [33] ) and four few shot learning methods (Meta-baseline  [7] , MMR  [35] , MAML-EEG  [14] , and SIML  [15] ). All experiments were conducted on three widely adopted EEG-based emotion recognition benchmarks (SEED, SEED-IV, and SEED-V). This article reproduced some methods based on open-source code. To ensure fair comparisons and maintain consistency in experimental settings, DGCN was uniformly adopted as the backbone for the few-shot learning methods. Since 5-shot is a widely adopted setting in few-shot learning, most experiments in this article use the 5-shot model as the base. The performance evaluations were carried out on the SEED and SEED-IV benchmarks, as outlined in Section IV-C1, and on the SEED-V benchmark, as detailed in Section IV-C2.\n\n1) Results on SEED and SEED-IV: Table I presents a comprehensive comparison of various methods on SEED and SEED-IV datasets. Our method demonstrates competitive performance with state-of-the-art methods. Specifically, FACE achieves an impressive accuracy of 96.72% on SEED and 95.95% on SEED-IV under the 10-shot setting. Furthermore, the proposed method consistently outperforms all competing approaches in the 5-shot setting, proving the effectiveness of the few-shot learning paradigm and the superiority of the proposed framework. However, the semi-supervised method FSA-TSP exhibits slightly better performance under 3-shot setting compared to our FACE method. This can be attributed to the ability of semi-supervised methods, such as FSA-TSP, to utilize both labeled and large amounts of unlabeled samples during training, thereby improving their performance in low-data scenarios. When the number of labeled samples increases to 5-shot, this performance gap is eliminated. These results demonstrate that while semi-supervised methods benefit from unlabeled data, our approach achieves exceptional performance even with limited labeled data, highlighting the effectiveness of the proposed method in small-sample learning scenarios and its strong generalization capability.\n\n2) Results on SEED-V: Since the SEED-V dataset was released only recently, there are relatively few comparative methods available, and the dataset itself is relatively small in scale. Therefore, this article conducts a separate comparative analysis on this dataset and includes additional experiments under the 1-shot setting. Table  II  demonstrates the effectiveness of the proposed method on the SEED-V dataset. Under the 1-shot setting, the proposed method already achieves competitive performance. Furthermore, in the 3-shot setting, the proposed approach surpasses all other approaches. In the 10-shot setting, the proposed method achieves a high accuracy of 98.95% using only approximately 8% of the labeled target domain samples. These results consistently highlight the superiority and strong generalization of the proposed method in few-shot learning scenarios.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Discussion",
      "text": "This section presents a series of ablation studies and visualizations to assess the contribution of each module. Section V-A introduces the module-wise ablation experiments, while Sections V-C and V-B examine the effects of the FSA and CVF modules, respectively. Additionally, Sections V-D and V-E analyze the influence of the number of shots and the hyperparameter L corresponding to the number of attention layers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Module-Wise Ablation Study",
      "text": "Table III presents the module-wise ablation studies of the proposed method on the SEED, SEED-IV, and SEED-V datasets. Across different shot settings, incorporating the CVF or FSA module alone generally improves performance compared to the baseline. This demonstrates that subjectspecific optimization, whether applied to feature fusion or logit prediction, benefits the model performance. The optimal results are achieved when both CVF and FSA modules are employed. The main reason is that the CVF module provides comprehensive subject-specific features, facilitating more effective adaptation by the FSA module. In this way, the model can learn subject-specific emotional cues more efficiently, thereby improving overall generalization performance.\n\nThe statistical analysis in Fig.  3  employs pairwise t-tests to quantify module effectiveness under the 5-shot setting. The asterisk ( * ) indicates 0.01<p ≤ 0.05 while the four asterisks (****) denote p ≤ 0.0001, with a higher number of asterisks representing a more significant improvement. Notably, the consistent presence of multiple asterisks across all datasets confirms that both CVF and FSA components contribute substantially to performance improvement, with pvalues falling below 0.0001 in most cases. This evidence highlights the effectiveness of the proposed module.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Discussion On Impact Of Cvf Module",
      "text": "To rigorously validate the effectiveness of the CVF module, comparative experiments are conducted against conventional fusion strategies across three benchmarks. Table  IV , the CVF module demonstrates superior performance by effectively capturing complementary multi-view features, whereas selfattention and cross-attention mechanisms exhibit performance degradation due to inter-view feature distribution incompatibility. This limitation is mitigated through the CVF's view fusion mechanism, which selectively preserves view-specific discriminative patterns while suppressing inter-modal conflicts. Furthermore, ablation studies reveal a non-negligible performance decline upon removing the meta-learning component implemented via MAML. This empirically validates its critical role in coordinating cross-view feature fusion. Fig.  4  presents the t-SNE  [55]  visualization to illustrate the impact of the CVF module on feature representation. The model without the CVF module exhibits substantial inter-class overlap with ambiguous decision boundaries, indicating that the learned feature representations fail to fully distinguish between different emotions. In contrast, integrating the CVF module results in more compact and well-separated clusters, suggesting that CVF enhances the model's discriminative capability. This improvement is particularly evident in the SEED-IV dataset (Fig.  4  (c) vs. Fig.  4(d) ), where the cluster separation and alignment between ground truth markers and predicted colors become more pronounced.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Discussion On The Impact Of The Fsa Module",
      "text": "This subsection conducts comparative experiments and visual analysis to investigate the impact of the FSA module. Firstly, this article compares the proposed FSA module with the adaptation methods  [46]  commonly used in the CV and NLP fields. As demonstrated in Table V, our FSA module enables more effective subject-specific calibration, resulting in superior performance improvements. Secondly, this article visualizes the brain topographic maps before and after 5-shot adaptation using the FSA module, as illustrated in Fig.  5 . Specifically, this article employs the shapley additive global importance  [56]  to quantify the importance of different channels under various emotional states. The fine-tuned model exhibits highly consistent and similar activated regions under the same emotions across different datasets. Meanwhile, for the negative emotions, the right hemisphere (particularly the right anterior/right prefrontal area) presents more prominent red or warm-colored regions. For neutral emotions, the overall brain region activation distribution appears relatively dispersed without conspicuous highly activated concentrated areas. In addition, positive emotions display stronger activation in the left hemisphere (especially in the left prefrontal lobe activity). These observations are consistent with findings from neuroscience research  [57] -  [59] , thereby validating the effectiveness of the FSA module.  When the number of shots reaches 10, the accuracy across all three datasets converges at approximately 99%, indicating that providing 10 labeled samples for fine-tuning is sufficient to achieve promising performance. Moreover, the shaded regions surrounding each performance curve quantify the standard deviation across multiple subjects. The SEED-V dataset achieves superior accuracy and exhibits relatively low variance, which may be attributed to its higher data quality.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "E. Parameter Sensitivity Analysis",
      "text": "Table VI presents the model performance with different numbers of attention layers (L) across various datasets and few-shot settings. As the number of attention layers changes from 1 to 5 under different shot conditions, the performance shows some fluctuations. However, these changes do not seem to follow a clear upward or downward trend directly related to the increase in the number of attention layers. Although there are differences between different settings, no significant pattern emerges indicating a strong impact on the number of attention Consequently, these findings suggest that different numbers of attention layers do not produce significant performance variations across the datasets and shot conditions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This article presented the FACE framework for cross-subject EEG emotion recognition, effectively addressing the challenges associated with inter-subject and intra-subject variance.\n\nThe proposed framework demonstrated remarkable capability in capturing subtle emotion-discriminative patterns while mitigating overfitting risks under data scarcity constraints. Specifically, FACE integrated a cross-view fusion module to merge heterogeneous EEG views into a unified representation and a few-shot adapter module to facilitate rapid, subject-specific emotional feature adjustment. Comprehensive experiments on three benchmarks validated the superiority of the proposed method over state-of-the-art approaches. Furthermore, our results demonstrated that fine-tuning with approximately 10 samples per class from a new subject can achieve an emotion classification accuracy of up to 98%. We anticipate this work will inspire further research into few-shot cross-subject BCI interface tasks. Future investigations will focus on few-shot multimodal EEG emotion recognition  [60]  and extend the few-shot learning paradigm to broader affective BCI applications  [61] .",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of training data and processes between Few-Shot Learning",
      "page": 1
    },
    {
      "caption": "Figure 1: ). Nonetheless, the scarcity",
      "page": 1
    },
    {
      "caption": "Figure 2: FACE consists of two core components:",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of the proposed FACE architecture in testing stage. For each unseen subject, a few labeled EEG signals are collected for rapid partial",
      "page": 4
    },
    {
      "caption": "Figure 3: employs pairwise t-tests",
      "page": 8
    },
    {
      "caption": "Figure 3: Statistical comparison of different components on the (a) SEED, (b)",
      "page": 8
    },
    {
      "caption": "Figure 4: presents the t-SNE [55] visualization to illustrate",
      "page": 9
    },
    {
      "caption": "Figure 4: (c) vs. Fig. 4(d)), where the cluster",
      "page": 9
    },
    {
      "caption": "Figure 5: Specifically, this article employs the shapley additive global",
      "page": 9
    },
    {
      "caption": "Figure 6: illustrates the accuracy trends of SEED, SEED-",
      "page": 9
    },
    {
      "caption": "Figure 4: t-SNE visualization of the SEED and SEED-IV datasets with and without the CVF module under the 5-shot setting. (a) Baseline w/o CVF module on",
      "page": 10
    },
    {
      "caption": "Figure 5: Brain topographic maps illustrating the effects of 5-shot adaptation on (a) the SEED and (b) the SEED-IV dataset. The first row depicts the state",
      "page": 10
    },
    {
      "caption": "Figure 6: Accuracy Trends of SEED, SEED-IV and SEED-V across Different",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Linear\nBN\nELU\nLinear\nBN\nELU\ncls. head\nFast updated subject-specific Parameter (10-steps) \n2）Few Shot Adapter Module\n1）Cross-View Fusion Module\nMulti-Head\nC\nC\n+\n+\nAttention\nFeature Adjustment\nFeature Fusion": "Spatial\nLearnable\nConcat\nAdd\n+\nC\nProjection\nTransformation\nOperation\nOperation"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Supervised Learning",
          "#Num of unlabeled\ntarget samples": "(cid:37)",
          "#Num of\nlabeled\ntarget samples": "(cid:37)",
          "SEED\nMean / STD (%)": "86.30 / 6.91",
          "SEED-IV\nMean / STD (%)": "62.64 / 10.25"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "86.5 / 6.22",
          "SEED-IV\nMean / STD (%)": "73.48 / 7.41"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "84.59 / 8.68",
          "SEED-IV\nMean / STD (%)": "73.69 / 7.16"
        },
        {
          "Method": "Semi-Supervised learning",
          "#Num of unlabeled\ntarget samples": "ALL",
          "#Num of\nlabeled\ntarget samples": "∼225/∼35",
          "SEED\nMean / STD (%)": "91.08 / 7.70",
          "SEED-IV\nMean / STD (%)": "81.58 / 8.72"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "10",
          "SEED\nMean / STD (%)": "88.59",
          "SEED-IV\nMean / STD (%)": "-"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "3",
          "SEED\nMean / STD (%)": "93.55 / 5.03",
          "SEED-IV\nMean / STD (%)": "87.96 / 5.18"
        },
        {
          "Method": "Domain Adaptation",
          "#Num of unlabeled\ntarget samples": "ALL",
          "#Num of\nlabeled\ntarget samples": "(cid:37)",
          "SEED\nMean / STD (%)": "89.63 / 6.79",
          "SEED-IV\nMean / STD (%)": "59.34 / 5.48"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "88.10 / 6.54",
          "SEED-IV\nMean / STD (%)": "73.14 / 9.43"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "91.58 / 4.02",
          "SEED-IV\nMean / STD (%)": "75.63 / 9.20"
        },
        {
          "Method": "Domain Generalization",
          "#Num of unlabeled\ntarget samples": "(cid:37)",
          "#Num of\nlabeled\ntarget samples": "(cid:37)",
          "SEED\nMean / STD (%)": "84.60 / 9.30",
          "SEED-IV\nMean / STD (%)": "71.20 / 8.50"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "88.00 / 4.50",
          "SEED-IV\nMean / STD (%)": "74.30 / 6.10"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "92.54 / 3.65",
          "SEED-IV\nMean / STD (%)": "75.65 / 5.47"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "",
          "SEED\nMean / STD (%)": "93.11 / 4.13",
          "SEED-IV\nMean / STD (%)": "77.16 / 4.73"
        },
        {
          "Method": "Few shot Learning",
          "#Num of unlabeled\ntarget samples": "(cid:37)",
          "#Num of\nlabeled\ntarget samples": "5",
          "SEED\nMean / STD (%)": "84.39 / 8.18",
          "SEED-IV\nMean / STD (%)": "81.38 / 4.96"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "5",
          "SEED\nMean / STD (%)": "84.69 / 7.80",
          "SEED-IV\nMean / STD (%)": "83.89 / 4.04"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "5",
          "SEED\nMean / STD (%)": "79.54 / 8.70",
          "SEED-IV\nMean / STD (%)": "84.95 / 3.94"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "5",
          "SEED\nMean / STD (%)": "88.13 / 5.63",
          "SEED-IV\nMean / STD (%)": "85.70 / 3.48"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "3",
          "SEED\nMean / STD (%)": "91.66 / 3.72",
          "SEED-IV\nMean / STD (%)": "83.91 / 3.87"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "5",
          "SEED\nMean / STD (%)": "93.96 / 2.70",
          "SEED-IV\nMean / STD (%)": "89.51 / 3.13"
        },
        {
          "Method": "",
          "#Num of unlabeled\ntarget samples": "",
          "#Num of\nlabeled\ntarget samples": "10",
          "SEED\nMean / STD (%)": "96.72 / 1.81",
          "SEED-IV\nMean / STD (%)": "95.95 / 1.47"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "IAG [26]",
          "#NUS": "(cid:37)",
          "#NLS": "(cid:37)",
          "SEED-V\nMean / STD (%)": "59.68 / 9.44"
        },
        {
          "Method": "Progressive GCN [53]",
          "#NUS": "",
          "#NLS": "",
          "SEED-V\nMean / STD (%)": "71.40 / 9.43"
        },
        {
          "Method": "PGCN∗ [19]",
          "#NUS": "",
          "#NLS": "",
          "SEED-V\nMean / STD (%)": "65.21 / 10.10"
        },
        {
          "Method": "JAGP [54]",
          "#NUS": "All",
          "#NLS": "(cid:37)",
          "SEED-V\nMean / STD (%)": "75.43 / 7.57"
        },
        {
          "Method": "GMoE [52]",
          "#NUS": "(cid:37)",
          "#NLS": "(cid:37)",
          "SEED-V\nMean / STD (%)": "76.30 / 9.20"
        },
        {
          "Method": "Ugan∗ [33]",
          "#NUS": "",
          "#NLS": "",
          "SEED-V\nMean / STD (%)": "77.12 / 10.87"
        },
        {
          "Method": "MoGE [34]",
          "#NUS": "",
          "#NLS": "",
          "SEED-V\nMean / STD (%)": "81.80 / 10.00"
        },
        {
          "Method": "MetaBaseline∗ [7]",
          "#NUS": "(cid:37)",
          "#NLS": "5",
          "SEED-V\nMean / STD (%)": "85.16 / 5.64"
        },
        {
          "Method": "MMR∗ [35]",
          "#NUS": "",
          "#NLS": "5",
          "SEED-V\nMean / STD (%)": "82.85 / 5.10"
        },
        {
          "Method": "SIML∗ [15]",
          "#NUS": "",
          "#NLS": "5",
          "SEED-V\nMean / STD (%)": "84.88 / 4.59"
        },
        {
          "Method": "FACE (Ours)",
          "#NUS": "(cid:37)",
          "#NLS": "1",
          "SEED-V\nMean / STD (%)": "75.74 / 7.00"
        },
        {
          "Method": "",
          "#NUS": "",
          "#NLS": "3",
          "SEED-V\nMean / STD (%)": "89.55 / 2.98"
        },
        {
          "Method": "",
          "#NUS": "",
          "#NLS": "5",
          "SEED-V\nMean / STD (%)": "95.20 / 1.42"
        },
        {
          "Method": "",
          "#NUS": "",
          "#NLS": "10",
          "SEED-V\nMean / STD (%)": "98.95 / 0.36"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Encoding of multi-modal emotional information via personalized skinintegrated wireless facial interface",
      "authors": [
        "J Lee",
        "H Jang",
        "Y Jang",
        "H Song",
        "S Lee",
        "P Lee",
        "J Kim"
      ],
      "year": "2024",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "2",
      "title": "The role of functional emotion circuits in distinct dimensions of psychopathology in youth",
      "authors": [
        "V Karl",
        "H Engen",
        "D Beck",
        "L Norbom",
        "L Ferschmann",
        "E Aksnes",
        "R Kjelkenes",
        "I Voldsbekk",
        "O Andreassen",
        "D Alnaes"
      ],
      "year": "2024",
      "venue": "Translational Psychiatry"
    },
    {
      "citation_id": "3",
      "title": "EEG based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "4",
      "title": "GDDN: Graph domain disentanglement network for generalizable eeg emotion recognition",
      "authors": [
        "B Chen",
        "C Chen",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Grop: Graph orthogonal purification network for eeg emotion recognition",
      "authors": [
        "M Wu",
        "C Chen",
        "B Chen",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "A survey of unsupervised deep domain adaptation",
      "authors": [
        "G Wilson",
        "D Cook"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "7",
      "title": "Meta-baseline: Exploring simple meta-learning for few-shot learning",
      "authors": [
        "Y Chen",
        "Z Liu",
        "H Xu",
        "T Darrell",
        "X Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "8",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin",
        "C Fan",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "9",
      "title": "A review of domain adaptation without target labels",
      "authors": [
        "W Kouw",
        "M Loog"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Model-based domain generalization",
      "authors": [
        "A Robey",
        "G Pappas",
        "H Hassani"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Domain generalization through metalearning: A survey",
      "authors": [
        "A Khoee",
        "Y Yu",
        "R Feldt"
      ],
      "year": "2024",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "12",
      "title": "Meta-learning approaches for few-shot learning: A survey of recent advances",
      "authors": [
        "H Gharoun",
        "F Momenifar",
        "F Chen",
        "A Gandomi"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "13",
      "title": "Freaml: A frequencydomain adaptive meta-learning framework for eeg-based emotion recognition",
      "authors": [
        "L Wang",
        "J Zhu",
        "L Du",
        "B Jin",
        "X Wei"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "14",
      "title": "Model-agnostic meta-learning for eeg-based inter-subject emotion recognition",
      "authors": [
        "C Chen",
        "H Fang",
        "Y Yang",
        "Y Zhou"
      ],
      "year": "2025",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "15",
      "title": "Subject-independent meta-learning framework towards optimal training of eeg-based classifiers",
      "authors": [
        "H Ng",
        "C Guan"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "16",
      "title": "Data generation for enhancing eeg-based emotion recognition: Extracting time-invariant and subject-invariant components with contrastive learning",
      "authors": [
        "Z Wan",
        "Q Yu",
        "W Dai",
        "S Li",
        "J Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "17",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "18",
      "title": "Fusing frequency-domain features and brain connectivity features for crosssubject emotion recognition",
      "authors": [
        "C Chen",
        "Z Li",
        "F Wan",
        "L Xu",
        "A Bezerianos",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "19",
      "title": "Pgcn: Pyramidal graph convolutional network for eeg emotion recognition",
      "authors": [
        "M Jin",
        "C Du",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Gmss: Graph-based multi-task self-supervised learning for eeg emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Spatialtemporal feature fusion neural network for eeg-based emotion recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "J Zhang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "22",
      "title": "A network model of the emotional brain",
      "authors": [
        "L Pessoa"
      ],
      "year": "2017",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "23",
      "title": "A domain-general brain network underlying emotional and cognitive interference processing: evidence from coordinate-based and functional connectivity meta-analyses",
      "authors": [
        "T Chen",
        "B Becker",
        "J Camilleri",
        "L Wang",
        "S Yu",
        "S Eickhoff",
        "C Feng"
      ],
      "year": "2018",
      "venue": "Brain Structure and Function"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari",
        "A Shoeibi",
        "M Khodatars",
        "S Bagherzadeh",
        "A Shalbaf",
        "D García",
        "J Gorriz",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "25",
      "title": "Human emotion recognition from eeg-based brain-computer interface using machine learning: a comprehensive review",
      "authors": [
        "E Houssein",
        "A Hammad",
        "A Ali"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "26",
      "title": "Instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "A novel semi-supervised meta learning method for subject-transfer brain-computer interface",
      "authors": [
        "J Li",
        "F Wang",
        "H Huang",
        "F Qi",
        "J Pan"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "28",
      "title": "Cross-subject eeg-based emotion recognition via semisupervised multisource joint distribution adaptation",
      "authors": [
        "M Jiménez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "29",
      "title": "Domain adaptation and generalization of functional medical data: A systematic survey of brain data",
      "authors": [
        "G Sarafraz",
        "A Behnamnia",
        "M Hosseinzadeh",
        "A Balapour",
        "A Meghrazi",
        "H Rabiee"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "30",
      "title": "Transit-eeg -a framework for cross-subject classification with subject specific adaptation",
      "authors": [
        "C Ahuja",
        "D Sethia"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "31",
      "title": "Integrating structured biological data by kernel maximum mean discrepancy",
      "authors": [
        "K Borgwardt",
        "A Gretton",
        "M Rasch",
        "H.-P Kriegel",
        "B Schölkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "32",
      "title": "Two-phase prototypical contrastive domain generalization for cross-subject eeg-based emotion recognition",
      "authors": [
        "H Cai",
        "J Pan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Ugan: Uncertainty-guided graph augmentation network for eeg emotion recognition",
      "authors": [
        "B Chen",
        "C Chen",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "34",
      "title": "Moge: Mixture of graph experts for cross-subject emotion recognition via decomposing eeg",
      "authors": [
        "X.-H Liu",
        "W.-B Jiang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "35",
      "title": "Improving generalization of meta-learning with inverted regularization at innerlevel",
      "authors": [
        "L Wang",
        "S Zhou",
        "S Zhang",
        "X Chu",
        "H Chang",
        "W Zhu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Robust saliencyaware distillation for few-shot fine-grained visual recognition",
      "authors": [
        "H Liu",
        "C Chen",
        "X Gong",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Cross-subject eeg emotion recognition using domain adaptive few-shot learning networks",
      "authors": [
        "R Ning",
        "C Chen",
        "T Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE international conference on bioinformatics and biomedicine (BIBM)"
    },
    {
      "citation_id": "39",
      "title": "Few-shot learning for fine-grained emotion recognition using physiological signals",
      "authors": [
        "T Zhang",
        "A Ali",
        "A Hanjalic",
        "P Cesar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Metaemotionnet: Spatial-spectral-temporal-based attention 3d dense network with meta-learning for eeg emotion recognition",
      "authors": [
        "X Ning",
        "J Wang",
        "Y Lin",
        "X Cai",
        "H Chen",
        "H Gou",
        "X Li",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "41",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "42",
      "title": "A dual-branch dynamic graph convolution based adaptive transformer feature fusion network for eeg emotion recognition",
      "authors": [
        "M Sun",
        "W Cui",
        "S Yu",
        "H Han",
        "B Hu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "An attention-based multi-domain bi-hemisphere discrepancy feature fusion model for eeg emotion recognition",
      "authors": [
        "L Gong",
        "W Chen",
        "D Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "44",
      "title": "Sgbev: satellite-guided bev fusion for cross-view semantic segmentation",
      "authors": [
        "J Ye",
        "Q Luo",
        "J Yu",
        "H Zhong",
        "Z Zheng",
        "C He",
        "W Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Multi-view cross-fusion transformer based on kinetic features for noninvasive blood glucose measurement using ppg signal",
      "authors": [
        "S Chen",
        "F Qin",
        "X Ma",
        "J Wei",
        "Y.-T Zhang",
        "Y Zhang",
        "E Jovanov"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "46",
      "title": "Cross-domain few-shot learning with task-specific adapters",
      "authors": [
        "W.-H Li",
        "X Liu",
        "H Bilen"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "48",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "49",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "50",
      "title": "When does label smoothing help?",
      "authors": [
        "R Müller",
        "S Kornblith",
        "G Hinton"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "51",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "52",
      "title": "Sparse mixture-of-experts are domain generalizable learners",
      "authors": [
        "B Li",
        "Y Shen",
        "J Yang",
        "Y Wang",
        "J Ren",
        "T Che",
        "J Zhang",
        "Z Liu"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "53",
      "title": "Progressive graph convolution network for eeg emotion recognition",
      "authors": [
        "Y Zhou",
        "F Li",
        "Y Li",
        "Y Ji",
        "G Shi",
        "W Zheng",
        "L Zhang",
        "Y Chen",
        "R Cheng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "54",
      "title": "Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from eeg signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "56",
      "title": "Understanding global feature contributions with additive importance measures",
      "authors": [
        "I Covert",
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Amygdala-prefrontal cortex functional connectivity during threat-induced anxiety and goal distraction",
      "authors": [
        "A Gold",
        "R Morey",
        "G Mccarthy"
      ],
      "year": "2015",
      "venue": "Biological psychiatry"
    },
    {
      "citation_id": "58",
      "title": "Decoding the nature of emotion in the brain",
      "authors": [
        "P Kragel",
        "K Labar"
      ],
      "year": "2016",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "59",
      "title": "The effects of prefrontal tdcs and hf-trns on the processing of positive and negative emotions evoked by video clips in first-and third-person",
      "authors": [
        "P La Malva",
        "A Di Crosta",
        "G Prete",
        "I Ceccato",
        "M Gatti",
        "E D'intino",
        "L Tommasi",
        "N Mammarella",
        "R Palumbo",
        "A Di Domenico"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "60",
      "title": "Cross-cultural emotion recognition with eeg and eye movement signals based on multiple stacked broad learning system",
      "authors": [
        "X Gong",
        "C Chen",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "61",
      "title": "He received Macau FDCT Natural Science Award three times and a First-rank Guangdong Province Scientific and Technology Advancement Award in 2019. His current research interests include cybernetics, computational intelligence, and systems. Tong Zhang (M'16-SM'24) received the degree in software engineering from Sun Yat-sen University",
      "authors": [
        "L Oganesian",
        "M Shanechi"
      ],
      "year": "2014",
      "venue": "followed by serving as the IEEE Systems, Man, and Cybernetics Society President from 2012 to 2013. Currently, he serves as an deputy director of CAAI Transactions on AI, an Associate Editor of the IEEE Transactions on AI, IEEE Trans on SMC: Systems, and IEEE Transactions on Fuzzy Systems, an Associate Editor of China Sciences: Information Sciences"
    }
  ]
}