{
  "paper_id": "2411.10060v1",
  "title": "Cmath: Cross-Modality Augmented Transformer With Hierarchical Variational Distillation For Multimodal Emotion Recognition In Conversation",
  "published": "2024-11-15T09:23:02Z",
  "authors": [
    "Xiaofei Zhu",
    "Jiawei Cheng",
    "Zhou Yang",
    "Zhuo Chen",
    "Qingyang Wang",
    "Jianfeng Yao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversation (MER) aims to accurately identify emotions in conversational utterances by integrating multimodal information. Previous methods usually treat multimodal information as equal quality and employ symmetric architectures to conduct multimodal fusion. However, in reality, the quality of different modalities usually varies considerably, and utilizing a symmetric architecture is difficult to accurately recognize conversational emotions when dealing with uneven modal information. Furthermore, fusing multi-modality information in a single granularity may fail to adequately integrate modal information, exacerbating the inaccuracy in emotion recognition. In this paper, we propose a novel Cross-Modality Augmented Transformer with Hierarchical Variational Distillation, called CMATH, which consists of two major components, i.e., Multimodal Interaction Fusion and Hierarchical Variational Distillation. The former is comprised of two submodules, including Modality Reconstruction and Cross-Modality Augmented Transformer (CMA-Transformer), where Modality Reconstruction focuses on obtaining high-quality compressed representation of each modality, and CMA-Transformer adopts an asymmetric fusion strategy which treats one modality as the central modality and takes others as auxiliary modalities. The latter first designs a variational fusion network to fuse the fine-grained representations learned by CMA-Transformer into a coarse-grained representations. Then, it introduces a hierarchical distillation framework to maintain the consistency between modality representations with different granularities. Experiments on the IEMOCAP and MELD datasets demonstrate that our proposed model outperforms previous state-of-the-art baselines. Implementation codes can be available at https://github.com/ cjw-MER/CMATH.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) aims to identify the corresponding emotion of each utterance in a conversation. It has become a critical problem in recent years due to its potential applications in social media analysis  (Kumar, Dogra and Dabas, 2015) , recommendation systems  (Xu, Yang, Han, Wang, Zhuang and Xiong, 2018; Zheng, Zhao, Zhu and Qian, 2022) , healthcare services  (Pujol, Mora and MartÃ­nez, 2019) , and affective computing systems  (Zhou, Gao, Li and Shum, 2020) .\n\nEarly research works usually assume that the emotion of the current utterance depends on its surrounding utterances as well as the speaker's emotional state. DialogueRNN  (Majumder, Poria, Hazarika, Mihalcea, Gelbukh and Cambria, 2019 ) presents a RNN-based neural network to keep track of individual party states. DialogueGCN  (Ghosal, Majumder, Poria, Chhaya and Gelbukh, 2019 ) models inter-and self-party dependency with a graph convolutional network to capture emotional inertia of individual speakers. Although these methods demonstrates impressive performance in emotion recognition, they only focus on unique modality (i.e., textual modality).\n\nIn recent years, multimodal emotion recognition in conversation has attracted considerable attention which attempts to model emotions of utterances by utilizing multiple modality information, such as textual, audio, and visual cues. Recent works mainly employ transformer mechanisms  (Lian, Liu and Tao, 2021; Zou, Huang and Shen, 2023)  or interaction graphs  (Hu, Liu, Zhao and Jin, 2021; Hu, Hou, Wei, Jiang and Mo, 2022; Nguyen, Nguyen, Le and Ha, 2024; Tu, Xie, Liang, Wang and Xu, 2024)  to model interactive relationships across different modalities  (Ma, Wang, Lin, Zhang, Zhang and Xu, 2024) .  Lian et al. (2021)  apply a cross-modal transformer to capture intra-and inter-modal interactions.  Zou et al. (2023)  introduce a prompt transformer for cross-modal information interaction.  Hu et al. (2021)  propose a multimodal fused graph convolutional network to explore both multimodal and long-distance contextual information. Hu et al.  (Hu et al., 2022)   interactive graph network to enhance intra-and cross-modal interactions.\n\nDespite previous works have demonstrated promising results, they still suffer from several limitations: Firstly, they usually treat different modalities as equivalent quality and employ symmetric architectures to fuse information in these modalities. However, in reality, modality qualities would be various, and the symmetric fusion strategy inevitable leads to inferior performance. Secondly, existing fusion methods mainly focus on fusing multimodal information in a unique information granularity by employing a unique-stage fusion strategy. They ignore the rich multi-granularity information embedded in different modalities, including the fine-grained feature representation and the coarse-grained semantic representation.\n\nTo address the aforementioned issues, this paper proposes a novel Cross-Modality Augmented Transformer with Hierarchical Variational Distillation (CMATH). It consists of two major components: Multimodal Interaction Fusion module and Hierarchical Variational Distillation module. In the Multimodal Interaction Fusion module, we introduce a cross-modality augmented transformer (CMA-Transformer) for fine-grained feature-fusion. Different from previous symmetric fusion strategy, CMA-Transformer adopts an asymmetric fusion strategy, which treats one modality as the central modality and the others as auxiliary ones. To be specific, it attempts to enhance the quality of the central modality by exploring interaction information from these auxiliary modalities. We employ CMA-Transformer for each modality by taking it as the central modality, and obtain its augmented representation. In the Hierarchical Variational Distillation Module, we first introduce a variational fusion network for coarse-grained semantic information fusion, which takes the above augmented fine-grained feature representation for each modality as input and learns a multimodal Gaussian distribution. Then, we introduce a Hierarchical Distillation strategy to maintain the consistency between modality representations with different granularities to further enhance the effectiveness of the semantic distribution integration.\n\nTo validate the effectiveness of the proposed model CMATH, we conducted extensive experiments on two widely used datasest, i.e., IEMOCAP and MELD. Experimental results demonstrate that CMATH significantly outperforms all baseline methods and achieves the state-of-the-art performance. Specifically, the proposed method obtains 73.90% and 73.96% in terms of two metrices ( i.e., accuracy and weighted F1-score) on the dataset IEMOCAP, with the relative performance improvement of 4.84% and 4.55% over the best performance baseline AdalGN, respectively. We further carry out experiments to analyze the effectiveness of each component of CMATH in depth in order to explore their contribution to the performance of multimodal emotion recognition in conversation. In summary, the main contributions of this work are as follows:\n\nâ€¢ We propose to treat different modalities as nonequivalent quality during the fusing process and propose a novel CMA-Transformer which fuses multi-modality in an asymmetric strategy. We focus on enhancing the quality of each modality by taking it as the central modality and exploiting the remaining modalities as auxiliary information.\n\nâ€¢ We introduce a hierarchical variational distillation framework, which first leverages a light variational fusion module to extract coarse-grained semantic information by fusing the Gaussion distribution of each modality, and then applies a hierarchical distillation strategy to maintain the consistency between modality representations with different information granularities.\n\nâ€¢ We validate the effectiveness of our proposed method on two widely used benchmark datasets, i.e., IEMO-CAP and MELD. Experimental results demonstrate its superiority over existing state-of-the-art methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Previous research efforts  (Majumder et al., 2019; Ghosal et al., 2019; Ma, Wang, Lin, Pan, Zhang and Yang, 2022)  on emotion recognition in conversation (ERC) mainly focus on unique modality, i.e., textual modality.  Majumder et al. (2019)  propose an attentive RNN called DialogueRNN which employs three GRUs, i.e., global GRU, party GRU and emotion GRU, to model the speaker, the preceding context and the preceding emotion.  Ghosal et al. (2019)  extend DialogueRNN by proposing DialogueGCN which explores the emotional inertia of each speaker via capturing both intra-and inter-speaker dependencies. Moreover, it incorporates the relative position of context utterances from the target utterance to model the influence of both past and future utterances.  Ma et al. (2022)  develop MVN which explores the emotion representation of the query utterance based on two dependencies, i.e., the word-level dependencies among utterances and the utterance-level dependencies in the context.\n\nSince a unique modality would provide insufficient information as compared to multimodal for ERC, recently there has been growing interest in multimodal emotion recognition in conversation. Despite an early concatenation operation can be applied for capturing multimodal features for the above-mentioned methods, they neglect the rich interaction information across different modalities and result in suboptimal results. To address the issue,  Lian et al. (2021)  utilize the transformer-based structure to capture intra-and inter-modal interactions among different modalities, and model temporal information in the utterance by considering both word-level lexical features and segment-level acoustic features.  Hu et al. (2021)  propose a multimodal fused graph convolutional network named MMGCN by creating a graph based on all modalities. It builds the connected graph in each modality and constructs edge connections between nodes across different modalities.  Hu et al. (2022)  leverage graph convolution operation to aggregate dynamics of contextual information of both intra-and inter-modality in a specific semantic space, and capture the intrinsic sequential patterns of contextual information in adjacent semantic space. By control information flow between layers, this method can reduce redundancy and boost complementarity between modalities.  Yang et al. (2023)  propose the self-adaptive context and modal-interaction modeling framework, which attempts to model different ranges of context dependency as well as captures the specific contribution of each modality.  Zou et al. (2023)  leverages audio and visual modalities as the prompt information for textual modality and designs a multimodal prompt transformer for cross-modal information interaction. Then a hybrid contrastive learning strategy is incorporated to improve the performance for less sample labels.  Nguyen et al. (2024)  attempt to integrate multimodal features by employing Directed Acyclic Graph (DAG), and introduce the curriculum learning strategy to handle emotional shifts and data imbalance.  Tu et al. (2024)  proposes an adaptive interactive graph network called AdaIGN by developing two selection policies, i.e., NSP and ESP, which learns a selection pattern for nodes and edges in a multimodal heterogeneous graph. Then a directed graph based fusion is adopted to prevent the influence of current utterance by future ones.\n\nThe main difference between these works and our proposed approach are two-folds. Firstly, existing works usually focus on fusing multimodal information in a symmetric framework which assumes different modalities have equivalent quality. Due to the varying quality of different modalities in real-world scenarios, we argue that it is more reasonable to conduct multimodal fusion in an asymmetric manner.\n\nTo the end, we propose a novel cross-modality augmented transformer (CMA-Transformer) by treating one modality as the central modality and the others as auxiliary ones. Secondly, different from existing works, we propose to learn modality representations with different granularities and develop a novel hierarchical distillation strategy to boost the effectiveness of the semantic distribution integration by maintaining the consistency between them.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preliminaries",
      "text": "In this section, we introduce the task definition and unimodal feature extraction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Definition",
      "text": "In the task of multimodal emotion recognition in conversation, a conversation is defined as a sequence of utterances ğ‘ˆ = {ğ‘¢ 1 , ğ‘¢ 2 , â‹¯ , ğ‘¢ ğ‘› } uttered by ğ‘š speakers, where ğ‘› is the number of utterances. The ğ‘–-th utterance representation u ğ‘– is represented by three different modalities denoted as\n\nare the corresponding representations of textual, audio, visual modality, respectively. ğ‘‘ ğ‘¡ , ğ‘‘ ğ‘ and ğ‘‘ ğ‘£ are the dimentions of the three modalities. The corresponding speaker of each utterance ğ‘¢ ğ‘– is denoted by ğ‘  ğœ™(ğ‘¢ ğ‘– ) where ğœ™(ğ‘¢ ğ‘– ) indicates the speaker index of ğ‘¢ ğ‘– . The emotion label of the ğ‘–-th utterance ğ‘¢ ğ‘– is ğ‘¦ ğ‘– âˆˆ ğ¶, where ğ¶ is the set of the emotion labels. The goal of the task is to predict the emotion label ğ‘¦ ğ‘– for a given utterance ğ‘¢ ğ‘– based on the multimodal information of utterances as well as the corresponding speaker signals in the conversation .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Unimodal Feature Extraction",
      "text": "Pre-trained models are typically trained on large-scale datasets, which enables them to capture rich semantic information. Therefore, we utilize pre-trained models to extract feature representations for each modality:\n\nTextual Modality: we utilize the RoBERTa-Large  (Liu, Ott, Goyal, Du, Joshi, Chen and Stoyanov, 2019)  for extracting textual features. RoBERTa is an enhancement of the BERT, which utilizes a multi-layer transformer to learn textual representations. For feature representation, we adopt the embeddings of the [CLS] tokens in the final layer of RoBERTa, which is a feature vector with a dimensionality of 1024.\n\nAudio Modality: we utilize the OpenSMlLE Toolkit  (Eyben, WÃ¶llmer and Schuller, 2010)  to extract acoustic features. OpenSMlLE serves as a versatile feature extraction framework designed for signal processing, which allows for the configuration of modular feature extraction components via a scriptable console application. After processing with OpenSMlLE, the dimensionality of the acoustic feature representation is reduced to 1582 for the IEMOCAP dataset  (Busso, Bulut, Lee, Kazemzadeh, Mower, Kim, Chang, Lee and Narayanan, 2008)  and 300 for the MELD dataset  (Poria, Hazarika, Majumder, Naik, Cambria and Mihalcea, 2019) .\n\nVisual Modality: we utilize the DenseNet  (Iandola, Moskewicz, Karayev and Keutzer, 2014)  for the extraction of visual features. DenseNet is a convolutional neural network (CNN) that is known for its high efficiency. DenseNet outputs a feature representation with a dimensionality of 342.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we introduce the details of CMATH which consists of two major components, including Multimodal Interaction Fusion module (in Section 4.1) and Hierarchical Variational Distillation module (in Section 4.2). The architecture of our proposed model is shown in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Interaction Fusion",
      "text": "Hierarchical Variational Distillation",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Interaction Fusion",
      "text": "The multimodal interaction fusion module consists of two submodules, i.e., Modality Reconstruction and Cross-Modality Augmented Transformer (CMA-Transformer).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modality Reconstruction",
      "text": "To ensure the representations of different modalities lie in the same space as well as achieve high-quality compressed representation of each modality, we introduce the modality reconstruction submodule, which is a simplified version of U-Net  (Ronneberger, Fischer and Brox, 2015) . Specifically, we adopt a 1D convolutional layer for each modality to conduct down-sampling operation and obtain their corresponding compressed representations in the common space. Then another 1D convolutional layer is leveraged for each modality to conduct up-sampling operation and reconstruct the input representation. Formally, we have:\n\nwhere\n\n, U ğ‘š is the raw utterance representation of the modality ğ‘š, ğ‘˜ ğ‘š is the kernel size. ğ‘‘ and ğ‘‘ ğ‘š are the corresponding dimension size of the common space and the modality ğ‘š. The reconstruction loss is defined as:\n\nwhere â€–â‹…â€– 2 ğ¹ is the squared Frobenius norm. Since speaker and positional information are crucial for emotion recognition in conversation  (Kim and Vossen, 2021) , we integrate them into modality features. To be specific, for each speaker ğ‘  ğ‘— , ğ‘— âˆˆ {1, â‹¯ , ğ‘š}, we obtain its corresponding representation s ğ‘— âˆˆ â„ ğ‘‘ as follows:\n\nwhere V ğ‘  âˆˆ â„ ğ‘‘Ã—ğ‘š is the trainable embedding matrix, and o(ğ‘  ğ‘— ) âˆˆ â„ ğ‘š is the one-hot vector of speaker ğ‘  ğ‘— . The speaker embedding in the conversation is denoted as\n\nFor positional information, we employ sinusoidal positional encoding to encode positions within the conversation, which is defined as follows:\n\n),\n\n),\n\nwhere P = {p 1 , â‹¯ , p ğ‘› } âˆˆ â„ ğ‘›Ã—ğ‘‘ , ğ‘– is the positional index of an utterance and ğ‘˜ is the dimension. Therefore, we obtain the sequence representation of modality ğ‘š by adding the latent embedding U â€² ğ‘š , the speaker embedding S and the positional embedding P:\n\nwhere H ğ‘š âˆˆ â„ ğ‘›Ã—ğ‘‘ is the augmented representation for modality ğ‘š.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Cma-Transformer",
      "text": "Previous works usually assume different modalities have equivalent quality and focus on fusing multi-model information based on a symmetric architecture  (Lian et al., 2021; Yang et al., 2023; Mao, Liu, Wang, Gao and Li, 2021) . However, the qualities of different modalities would be varying and resulting in a suboptimal performance of these models. To the end, we develop an asymmetric fusion strategy named Cross-Modality Augmented Transformer (CMA-Transformer), which treats one modality as the central modality and takes others as auxiliary modalities. CMA-Transformer aims to enhance the representation of the central modality by exploiting information from these auxiliary modalities. Since the textual modality derived CMA-Transformer have the same structure with the two other modalities (i.e., audio and visual) derived CMA-Transformers, we mainly focus on explaining the CMA-Transformer framework centered on the textual modality.\n\nFor the textual modality, we treat it as the centralmodality and enhance its representation by exploring signals from the audio modality (i.e., auxiliary modality):\n\nwhere Hğ‘â†’ğ‘¡ , Hğ‘â†’ğ‘¡ , H ğ‘â†’ğ‘¡ âˆˆ â„ ğ‘›Ã—ğ‘‘ , ğ¶ğ´(â‹…), ğ‘ğ‘œğ‘Ÿğ‘š(â‹…) and ğ¹ ğ¹ ğ‘(â‹…) indicate cross-attention, normalization and feedforward network, respectively.\n\nSimilarly, we also explore signals from the visual modality (i.e., auxiliary modality) to boost the representation of the textual modality:\n\nwhere Hğ‘£â†’ğ‘¡ , Hğ‘£â†’ğ‘¡ ,\n\nSince the textual modality is the central modality, we first strengthen its representation via self-attention  (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez and Polosukhin, 2017) , and then inject information from other two modalities into the textual modality:\n\nwhere Hğ‘¡â†’ğ‘¡ , Hğ‘¡â†’ğ‘¡ , H ğ‘¡â†’ğ‘¡ âˆˆ â„ ğ‘›Ã—ğ‘‘ .\n\nFinally, we adopt the gating fusion mechanism to adaptively fuse these representations, making the final representation of the textual modality:\n\nWe represent the process of the CMA-Transformer framework centered on the textual modality, i.e., Equations (9-12), in a unified form:\n\nwhere H â€² ğ‘¡ âˆˆ â„ ğ‘›Ã—ğ‘‘ , and ğœƒ ğ‘¡ are trainable parameters. Similarly, we can also obtain the enhanced representations centered on audio and visual modalities as follows:\n\nwhere",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Hierarchical Variational Distillation",
      "text": "In this section, we introduce the Hierarchical Variational Distillation module, which consists of two submodules, i.e., the Variational Fusion Network and the Hierarchical Distillation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Variational Fusion Network",
      "text": "After we obtain the augmented representations for each modality based on CMA transformer, we first apply a variational fusion network to fuse these fine-grained representations for each modality into a coarse-grained representations. To be specific, we assume the latent representations of each modality are represented by a Gaussian distribution îˆº ğ‘š (ğ ğ‘š , ğˆ 2 ğ‘š ), which takes H â€² ğ‘š as input and outputs its corresponding mean ğ ğ‘š âˆˆ â„ ğ‘›Ã—ğ‘‘ and standard deviation ğˆ 2 ğ‘š âˆˆ â„ ğ‘›Ã—ğ‘‘ of each modality:\n\nwhere ğ‘€ğ¿ğ‘ƒ ğœ‡ ğ‘š (â‹…), ğ‘€ğ¿ğ‘ƒ ğœ ğ‘š (â‹…) represent two multi-layer perceptrons.\n\nThen, we assume these Gaussian distributions of different modalities are independent and integrate them into a multimodal Gaussian distribution îˆº ğ‘¥ (ğ ğ‘¥ , ğˆ 2 ğ‘¥ ). Specifically, we obtain ğ ğ‘¥ âˆˆ â„ ğ‘›Ã—ğ‘‘ and ğˆ ğ‘¥ âˆˆ â„ ğ‘›Ã—ğ‘‘ by simply averaging the means and standard deviations of all Gaussian distributions:\n\nThe representation sampled from the learned multimodal Gaussian distribution can be regarded as the fused representations of all modalities. Since the sampling operation is not differentiable, we adopt the reparameterization trick  (Kingma and Welling, 2014)  for allowing back-propagation of gradients during the training stage. Specifically, the fused multimodal representations H â€² ğ‘¥ âˆˆ â„ ğ‘›Ã—ğ‘‘ is generated as follows:\n\nwhere ğœ– is the random noise sampled from a normal distribution îˆº (0, I).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hierarchical Distillation",
      "text": "With the fine-grained representations H â€² ğ‘š learned by CMA-Transformer for each modality ğ‘š and the coarsegrained representation H â€² ğ‘¥ learned by the variational fusion network on hand, we introduce the hierarchical distillation submodule to maintain the consistency between modality representations with different granularities. To be specific, the hierarchical distillation paradigm is comprised of a lowlevel distillation and a high-level distillation. The former is designed to minimize the difference between semantic logits of teacher H â€² ğ‘¥ and student H â€² ğ‘š . The latter is developed to minimize the KL-Divergence between decision logits of teacher Å¶ğ‘¥ and student teacher Å¶ğ‘š .\n\nLow-Level Distillation: the objective of the low-level distillation is to impose consistency constraints between modalities on the semantic representation spaces. Specifically, we treat H â€² ğ‘¥ as teacher and H â€² ğ‘š as student, and adopt Mean Squared Error (MSE) loss  (Wang and Bovik, 2009)  as the low-level distillation loss:\n\nwhere â€–â‹…â€– 2 ğ¹ is the squared Frobenius norm. High-Level Distillation: the objective of high-level distillation is to maintain consistency between modalities on the decision representation spaces. We first obtain the classification results of modalities based on different granularities:\n\nwhere Classifier ğ‘š (â‹…) and Classifier ğ‘¥ (â‹…) are composed of a fully connected layer followed by a softmax layer. We utilize the cross-entropy loss for training these classifiers. In particular, the cross-entropy loss between the predicted label distribution Å¶ğ‘š and the true label distribution ğ‘Œ is defined as follows:\n\nwhere Å·ğ‘š ğ‘– âˆˆ Å¶ğ‘š is the predicted label for the ğ‘–-th utterance ğ‘¢ ğ‘– based on the fine-grained representations H â€² ğ‘š , and ğ‘¦ ğ‘– âˆˆ ğ‘Œ is the true label for the ğ‘–-th utterance ğ‘¢ ğ‘– . Similarly, we define the cross-entropy loss between the predicted label distribution Å¶ğ‘¥ and the true label distribution ğ‘Œ as follows:\n\nwhere Å·ğ‘¥ ğ‘– âˆˆ Å¶ğ‘¥ is the predicted label for the ğ‘–-th utterance ğ‘¢ ğ‘– based on the coarse-grained representations H â€² ğ‘¥ . The overall cross-entropy loss is defined as:\n\nThen, we adopt KL-Divergence  (Hershey and Olsen, 2007)  loss as the high-level distillation loss, where Å¶ğ‘¥ and Å¶ğ‘š are treated as teacher and student, respectively. Formally, we have:\n\n),\n\n(23)\n\nFinally, the overall loss is defined as follows:\n\nwhere ğ›¾ 1 and ğ›¾ 2 are hyperparameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets And Evaluations",
      "text": "We conduct experiments on two well-known benchmark datasets, i.e., IEMOCAP  (Busso et al., 2008)  and MELD  (Poria et al., 2019) , and the statistics of these datasets are detailed in Table  1 .\n\nIEMOCAP. This dataset comprises two-way conversations involving ten speakers, with a total of 151 conversations and 7,433 utterances. It is segmented into five sessions, with the first four utilized for training and the final session reserved for testing. Following  (Ma et al., 2022) , we randomly select 20% of the training set as the validation set. Each utterance within the dataset is annotated with one of six emotions, including Happy, Sad, Neutral, Angry, Excited, Frustrated.\n\nMELD. Different from IEMOCAP, MELD is a multispeaker conversation dataset with three or more speakers in a conversation. It is collected from the TV series Friends, which includes 1,433 conversations and 13,708 utterances. Each utterance is categorized under one of the seven emotions, i.e., Neutral, Surprise, Fear, Sadness, Joy, Disgust, Angry.\n\nEvaluation Metrics. We evaluate the model's performance based on two metrics, i.e., the overall accuracy (ACC) and the weighted average F1-score (W-F1). Additionally, we provide the F1-score for each emotion category to offer a comprehensive assessment of performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "We conduct a comprehensive comparison of our proposed model with various SOTA baseline methods:\n\nâ€¢ DialogueRNN(  Majumder et al., 2019) : It models the speaker, the preceding context and the preceding emotion via three gated recurrent units, including a global GRU, a party GRU and an emotion GRU.\n\nâ€¢ DialogueGCN  (Ghosal et al., 2019) : This baseline considers the inter-speaker dependency and the intraspeaker dependency to capture emotional influence between different speakers and emotional inertia of individual speakers, respectively. The graph convolution network is adopted to model contextual information among distant utterances.\n\nâ€¢ MMGCN  (Hu et al., 2021) : MMGCN applies a multimodal fused graph convolutional network to explore the multimodal information and capture distant contextual information. It can effectively model multimodal dependencies as well as inter-and intra-speaker dependencies.\n\nâ€¢ CTNet  (Lian et al., 2021) : It utilizes transformer-based structures to model intra-modal and cross-modal interactions of different modalities. A single-modal transformer is developed to capture temporal dependencies among unimodal features, and a cross-modal transformer is designed to learn cross-modal interactions on the unaligned multimodal features.\n\nâ€¢ MM-DFN  (Hu et al., 2022) : It attempts to reduce redundancy and enhance complementarity between modalities by designing a graph-based dynamic fusion module to merge multimodal context information. It employs the graph convolution operation to aggregate intra-and inter-modality contextual information in a specific space, and adopts the gating mechanism to learn intrinsic sequential patterns of contextual information in adjacent semantic spaces.\n\nâ€¢ SCMM  (Yang et al., 2023) : It aims to model the varying difficulty of each utterance and the specific contribution of each modality. A self-adaptive path selection strategy is adopted to select an appropriate path to obtain utterance representation.\n\nâ€¢ CMCF-SRNet(Zhang and Li, 2023): It designs two transformers, i.e., cross-modal locality-constrained transformer and graph-based semantic refinement transformer, to explore the multimodal interaction and semantic relationship information among utterances.\n\nâ€¢ MultiDAG  (Nguyen et al., 2024) : It applies directed acyclic graph to fuse multimodal features within a unified framework. Then the curriculum learning is leveraged to facilitate the learning process by gradually presenting training samples in a meaningful order to address emotional shift issues and imbalanced data.\n\nâ€¢ AdaIGN  (Tu et al., 2024) : It presents an adaptive interactive graph network to balance the intra-and interspeaker emotion dependency as well as mitigate the issue of conflicting emotion across different modalities.\n\nIt further adopts a directed graph based fusion strategy to prevent the influence of current utterance by future ones.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "We implement the proposed model using Pytorch on NVIDIA RTX 2080Ti and adopt Adam  (Kingma and Ba, 2015)  as the optimizer with an initial learning rate of 3.0e-3 for IEMOCAP and 2.0e-4 for MELD. The batch size is 16 for IEMOCAP and 4 for MELD. We set ğ›¾ 1 and ğ›¾ 2 to 1.0 and 1.8 for IEMOCAP, 1.0 and 1.2 for MELD. Note that the two baselines DialogueRNN and DialogueGCN are initially developed for unimodel scenario, we apply an early concatenation operation for fusing multimodal features in their implementation. All results are averages of 5 runs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Overall Performances",
      "text": "Table  2  and table 3 report the experimental results of all baseline models and our proposed method on the IEMOCAP and MELD datasets, respectively. The best and second best results are bolded and underlined, respectively. From the results, we can observe that both DialogueRNN and Dia-logueGCN obtain the worst performance. This is because they are initially designed for single modality. Although an early concatenation fusion is utilized in their implementations, the complex interaction information among different modalities are not well explored by them. Compared to DialogueRNN and DialogueGCN, these graph-based or transformer-based baselines demonstrate superior performances as they can better explore the complex multimodal interaction information. Among all baselines, AdaIGN obtains the best performance in most cases. Our proposed method is better than all baselines. To be specific, on the IEMOCAP dataset, our model outperforms the best performing baseline AdaIGN by 3.41% and 3.22% in terms of the metrics ACC and W-F1, respectively. We further conduct significance test between our model and the best performing baseline, and the results suggest that the improvement of CMATH over AdaIGN is significant on both datasets with ğ‘ < 0.05 based on t-test. In addition, we also analyze the detailed results on each emotion category, and the results show that our model achieves superior or competitive performances on most emotion categories.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we conduct the ablation study to investigate the contribution of each component in our proposed model. Specifically, we introduce the following variants of CMATH to perform the comparison: â€¢ w/o MR: We replace the modality reconstruction module by a fully-connected layer, and still keep the speaker and positional information.\n\nâ€¢ w/o CMA-T: We ignore the asymmetric fusion operation by discarding the CMA Transformer.\n\nâ€¢ w/o LLD: We remove the low-level distillation part, and only consider the high-level distillation.\n\nâ€¢ w/o HLD: We discard the high-level distillation part, and only keep the low-level distillation.\n\nâ€¢ w/o HD: We ignore the consistency operation by discarding the hierarchical distillation.\n\nThe experimental results are shown in Table  4  and we can observe that CMATH achieves the best performance compared to each variant, which indicates the rationality of its design. Specifically, the modality reconstruction module helps to improve the performance by obtaining highquality compressed representation of each modality. Moreover, there is a significant performance decline if we remove the CMA-Transformer module, which demonstrates the effectiveness of incorporating the asymmetric fusion strategy. The performance drops on the IEMOCAP dataset is larger than on the MELD dataset. The reason may be attributed to the conversation length in the IEMOCAP dataset is considerably longer than those in the MELD dataset (as shown in Table  1 ), and CMA-Transformer is able to explore more interaction signals on IEMOCAP than on MELD. In addition, removing either the low-level distillation or the high-level distillation will result in performance degradation, which demonstrates that each distillation constraint plays a critical role in our model. Besides, removing both low-and high-level distillation parts will further lead to performance degradation compared to removing each of them.\n\nIt indicates that the low-and high-level distillation parts can compensates each other, and maintaining the consistency between modality representations with different granularities are crucial for the proposed model.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Different Modality Settings",
      "text": "In order to evaluate the effectiveness of our model under different modality settings, we remove one or two modalities and the results are shown in Table  5 . We have the following observations. First, the performance of CMATH over the textual modality is significantly superior to the other two modalities according to unimodal results, which indicates that textual features play the most important role in the ERC task. Among all modalities, the performance on visual modality is the worst which reflects that its quality is relatively lower than other modalities. Second, when we implement CMATH based on any combination of two modalities, we can observe that the corresponding performance is much better than its counterpart solely on a unique modality. Third, the best performance is achieved when all three modalities are leveraged simultaneously. This result confirms that our model can reasonably integrating information from different modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Representation Visualization",
      "text": "To further verify the superiority of our proposed model CMATH, we adopt t-SNE to illustrate learned embeddings by CMATH and MultiDAG 1 on the IEMOCAP dataset. Figure  3  presents the results, from which we can find that the embeddings learned MultiDAG are not satisfactory as many nodes with different labels are mixed together. In contrast, our method is clearly superior to MultiDAG as the learned embeddings are more distinguishable, which 1 We do not compare with AdaIGN as its code is not available.  demonstrates a higher intra-class tightness and clear interclass boundaries. The results validates the effectiveness of our proposed method.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Embedding Dimension",
      "text": "In this section, we explore how the size of the embedding dimension affects the performance of our proposed model. We vary it from 100 to 600 with a step size of 100. Figure  4  shows the performance of CMATH with respect to different dimension sizes. For IEMOCAP, in the beginning, the performance of CMATH improves gradually as a higher size of embedding dimension is adopted, and reaches to the peak when the dimension size equals to 500. After that, the performance starts to decline as the dimension size continues to increase. For MELD, we can observe a similar trend of performance, where the best performance is obtained when the dimension size equals to 400. The results suggests that the proposed model can produce the state-of-the-art performance in a relatively lower-dimensional space.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Impact Of Conversation Length",
      "text": "To investigate the performance of our proposed method with different conversation lengths, we group test conversations into three different categories with respect to their lengths. Specifically, for IEMOCAP, the conversation categories are defined as C1 (1-39 utterances), C2 (40-69 utterances), and C3 (over 70 utterances). For MELD, the conversation categories are defined as C1 (1-9 utterances), C2 (10-20 utterances), and C3 (over 20 utterances).\n\nFigure  5  presents the performance of CMATH and Mul-tiDAG with respect to different conversation categories. We can observe that our proposed model CMATH consistently outperforms MultiDAG across all categories on both datasets. Specifically, for IEMOCAP, the relative improvements of CMATH over MultiDAG on the three categories are 3.86%, 3.74% and 5.41%, respectively. For MELD, the relative performance improvements of CMATH over Multi-DAG are 4.15%, 2.67% and 4.19% on the three categories. The experimental results verify that our proposed model achieves better and more stable performance for various conversation lengths.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Hyperparameter Analysis",
      "text": "In this section, we explore the effects of hyperparameters ğ›¾ 1 and ğ›¾ 2 on the model performance, where ğ›¾ 1 and ğ›¾ 2 control the low-and high-level distillation losses, respectively. A higher ğ›¾ 1 (ğ›¾ 2 ) signifies a higher intensity of fine-grained (coarse-grained) information constraint between modalities. We perform grid search over combinations of ğ›¾ 1 and ğ›¾ 2 by varying them from 1.0 to 2.0 with a step of 0.2, and the results are depicted in Figure  6 . We observe that our proposed model achieves the best performance when ğ›¾ 1 = 1.0 (1.0) and ğ›¾ 2 = 1.8 (1.2) on the IEMOCAP (MELD) dataset in terms of the metric W-F1. This result indicates that imposing consistency constraints between modalities on the decision representation spaces. (i.e., high-level distillation) is relatively more important compared to maintaining consistency between modalities on the semantic representation spaces. It also verifies the effectiveness of developing the hierarchical distillation mechanism in our model, which can maintain the consistency between modality representations with different granularities.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Error Analysis",
      "text": "In this section, we report the confusion matrix of CMATH in Figure  7 . For both datasets, we can observe a high concentration of items on the diagonal line which reflects the performance of our model over each class. Moreover, we can also observe some misclassified samples for different classes. To be specific, on the IEMOCAP dataset, we observe that there are 48 misclassified samples for the class \"Angry\", among which most of them (i.e., 42 samples) are misclassified to the class \"Frustrated\". This may be attributed to the high similarity between the two classes. Similar observations can be found for the pair of classes, i.e., \"Happy\" and \"Excited\". On the MELD dataset, we find that most of the misclassified samples for each classes fall into the class \"Neutral\". The reason may be that the number of samples of the class \"Neutral\" is overwhelmingly larger than other classes which makes the model biased to this dominant class.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a novel model named CMATH for multimodal emotion recognition in conversation. We develop a cross-modality augmented transformer for finegrained feature fusion, and then propose a variational fusion network for coarse-grained semantic information fusion. After that, we introduce a Hierarchical Distillation module, which consists of both low-and high-level distillations, to maintain the consistency between modality representations with different granularities. We conducted extensive experiments on two public datasets, and the results demonstrate that our proposed model is significantly superior to the stateof-the-art baselines.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Credit Authorship Contribution Statement",
      "text": "Xiaofei Zhu: Conceptualization of this study, Methodology, Writing -original draft & review & editing, Supervision. Jiawei Cheng: Conceptualization of this study, Methodology, Software, Writing -original draft. Zhou Yang: Supervision. Zhuo Chen: Supervision. Qingyang Wang: Supervision. Jianfeng Yao: Supervision.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a multimodal conversation scenario,",
      "page": 2
    },
    {
      "caption": "Figure 2: Zhu et al.: Preprint submitted to Elsevier",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of our proposed model CMATH, which consists of two modules: (1) Multimodal Interaction Fusion, which",
      "page": 4
    },
    {
      "caption": "Figure 3: presents the results, from which we can find that",
      "page": 9
    },
    {
      "caption": "Figure 3: The t-SNE visualizations of learned embeddings by",
      "page": 9
    },
    {
      "caption": "Figure 4: Performance of the proposed model with different",
      "page": 9
    },
    {
      "caption": "Figure 5: Performance of the proposed model with different",
      "page": 10
    },
    {
      "caption": "Figure 5: presents the performance of CMATH and Mul-",
      "page": 10
    },
    {
      "caption": "Figure 6: We observe that our",
      "page": 10
    },
    {
      "caption": "Figure 7: For both datasets, we can observe a high",
      "page": 10
    },
    {
      "caption": "Figure 6: Parameter analysis of ğ›¾1 and ğ›¾2 on the two datasets.",
      "page": 11
    },
    {
      "caption": "Figure 7: Confusion matrices for the proposed model on the two datasets.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 5: Experiments for exploring the effect of a unique modality as",
      "data": [
        {
          "Column_1": "ACC W-F1 A",
          "unique modality as\nes.\nMELD\nCC W-F1\n.26 65.85\n.19 41.08\n.12 31.27\n.93 66.56\n.89 66.63\n.43 42.04 MultiDAG CMATH\nFigure 3: The t-SNE visualizations of learned embeddings by\n.34 66.93\nCMATH and MultiDAG on the IEMOCAP dataset.\nrisabletoexplore\n76": "",
          "Column_3": ""
        },
        {
          "Column_1": "67.98 67.81 67\n50.57 52.9 49\n27.73 30.02 48\n70.55 70.58 67\n68.45 68.37 67\n61.92 61.84 47",
          "unique modality as\nes.\nMELD\nCC W-F1\n.26 65.85\n.19 41.08\n.12 31.27\n.93 66.56\n.89 66.63\n.43 42.04 MultiDAG CMATH\nFigure 3: The t-SNE visualizations of learned embeddings by\n.34 66.93\nCMATH and MultiDAG on the IEMOCAP dataset.\nrisabletoexplore\n76": "",
          "Column_3": ""
        },
        {
          "Column_1": "",
          "unique modality as\nes.\nMELD\nCC W-F1\n.26 65.85\n.19 41.08\n.12 31.27\n.93 66.56\n.89 66.63\n.43 42.04 MultiDAG CMATH\nFigure 3: The t-SNE visualizations of learned embeddings by\n.34 66.93\nCMATH and MultiDAG on the IEMOCAP dataset.\nrisabletoexplore\n76": "",
          "Column_3": "66.63\n42.04 MultiDAG CMATH\nFigure 3: The t-SNE visualizations of learned embeddings by\n66.93\nCMATH and MultiDAG on the IEMOCAP dataset.\nbletoexplore\n76"
        },
        {
          "Column_1": "",
          "unique modality as\nes.\nMELD\nCC W-F1\n.26 65.85\n.19 41.08\n.12 31.27\n.93 66.56\n.89 66.63\n.43 42.04 MultiDAG CMATH\nFigure 3: The t-SNE visualizations of learned embeddings by\n.34 66.93\nCMATH and MultiDAG on the IEMOCAP dataset.\nrisabletoexplore\n76": ".34",
          "Column_3": ""
        },
        {
          "Column_1": "",
          "unique modality as\nes.\nMELD\nCC W-F1\n.26 65.85\n.19 41.08\n.12 31.27\n.93 66.56\n.89 66.63\n.43 42.04 MultiDAG CMATH\nFigure 3: The t-SNE visualizations of learned embeddings by\n.34 66.93\nCMATH and MultiDAG on the IEMOCAP dataset.\nrisabletoexplore\n76": "risa",
          "Column_3": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP MELD\nFigure 6: Parameter analysis of ğ›¾ and ğ›¾ on the two datasets.\n1 2": "IEMOCAP MELD\nFigure 7: Confusion matrices for the proposed model on the two datasets.\nAcknowledgement Eyben, F., WÃ¶llmer, M., Schuller, B., 2010. Opensmile: The munich\nversatileandfastopen-sourceaudiofeatureextractor,in:Proceedingsof"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M WÃ¶llmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Approximating the kullback leibler divergence between gaussian mixture models",
      "authors": [
        "J Hershey",
        "P Olsen"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "Densenet: Implementing efficient convnet descriptor pyramids",
      "authors": [
        "F Iandola",
        "M Moskewicz",
        "S Karayev",
        "K Keutzer"
      ],
      "year": "2014",
      "venue": "Densenet: Implementing efficient convnet descriptor pyramids"
    },
    {
      "citation_id": "8",
      "title": "EmoBERTa: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "9",
      "title": "Adam: a method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "Proceedings of 2nd International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Emotion analysis of twitter using opinion mining",
      "authors": [
        "A Kumar",
        "P Dogra"
      ],
      "year": "2015",
      "venue": "Porceedings of the 18th International Conference on Contemporary Computing"
    },
    {
      "citation_id": "12",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "14",
      "title": "A multi-view network for real-time emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "X Pan",
        "Y Zhang",
        "Z Yang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "15",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "DialogueRNN: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "17",
      "title": "DialogueTRM: Exploring multi-modal emotional dynamics in a conversation",
      "authors": [
        "Y Mao",
        "G Liu",
        "X Wang",
        "W Gao",
        "X Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "18",
      "title": "Curriculum learning meets directed acyclic graph for multimodal emotion recognition",
      "authors": [
        "C Nguyen",
        "C Nguyen",
        "D Le",
        "Q Ha"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation"
    },
    {
      "citation_id": "19",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition to improve e-healthcare systems in smart cities",
      "authors": [
        "F Pujol",
        "H Mora",
        "A MartÃ­nez"
      ],
      "year": "2019",
      "venue": "Research & Innovation Forum"
    },
    {
      "citation_id": "21",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "Medical Image Computing and Computer-Assisted Intervention"
    },
    {
      "citation_id": "22",
      "title": "Adaptive graph learning for multimodal conversational emotion detection",
      "authors": [
        "G Tu",
        "T Xie",
        "B Liang",
        "H Wang",
        "R Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 30th Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Mean squared error: Love it or leave it? a new look at signal fidelity measures",
      "authors": [
        "Z Wang",
        "A Bovik"
      ],
      "year": "2009",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "25",
      "title": "Exploiting the sentimental bias between ratings and reviews for enhancing recommendation",
      "authors": [
        "Y Xu",
        "Y Yang",
        "J Han",
        "E Wang",
        "F Zhuang",
        "H Xiong"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Data Mining"
    },
    {
      "citation_id": "26",
      "title": "Self-adaptive context and modal-interaction modeling for multimodal emotion recognition",
      "authors": [
        "H Yang",
        "X Gao",
        "J Wu",
        "T Gan",
        "N Ding",
        "F Jiang",
        "L Nie"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "27",
      "title": "A cross-modality context fusion and semantic refinement network for emotion recognition in conversation",
      "authors": [
        "X Zhang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "PERD: Personalized emoji recommendation with dynamic user preference",
      "authors": [
        "X Zheng",
        "G Zhao",
        "L Zhu",
        "X Qian"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "29",
      "title": "The design and implementation of xiaoice, an empathetic social chatbot",
      "authors": [
        "L Zhou",
        "J Gao",
        "D Li",
        "H Shum"
      ],
      "year": "2020",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Multimodal prompt transformer with hybrid contrastive learning for emotion recognition in conversation",
      "authors": [
        "S Zou",
        "X Huang",
        "X Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    }
  ]
}