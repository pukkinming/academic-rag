{
  "paper_id": "2207.11482v1",
  "title": "Multimodal Emotion Recognition With Modality-Pairwise Unsupervised Contrastive Loss",
  "published": "2022-07-23T10:11:24Z",
  "authors": [
    "Riccardo Franceschini",
    "Enrico Fini",
    "Cigdem Beyan",
    "Alessandro Conti",
    "Federica Arrigoni",
    "Elisa Ricci"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is involved in several real-world applications. With an increase in available modalities, automatic understanding of emotions is being performed more accurately. The success in Multimodal Emotion Recognition (MER), primarily relies on the supervised learning paradigm. However, data annotation is expensive, time-consuming, and as emotion expression and perception depends on several factors (e.g., age, gender, culture) obtaining labels with a high reliability is hard. Motivated by these, we focus on unsupervised feature learning for MER. We consider discrete emotions, and as modalities text, audio and vision are used. Our method, as being based on contrastive loss between pairwise modalities, is the first attempt in MER literature. Our end-to-end feature learning approach has several differences (and advantages) compared to existing MER methods: i) it is unsupervised, so the learning is lack of data labelling cost; ii) it does not require data spatial augmentation, modality alignment, large number of batch size or epochs; iii) it applies data fusion only at inference; and iv) it does not require backbones pre-trained on emotion recognition task. The experiments on benchmark datasets show that our method outperforms several baseline approaches and unsupervised learning methods applied in MER. Particularly, it even surpasses a few supervised MER state-of-the-art.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion is a key factor driving people's actions and thoughts, and a fundamental part of the human verbal and nonverbal communication. Automated emotion recognition is an important aspect of many applications, including social assistive robots  [1] , smart systems to work in customer service  [2] , health-care  [3] , education  [4] , and automated-driving cars  [5] . However, it is a highly challenging problem due to the complex nature of emotion expression and perception, which are hard to generalize as being dependent on several factors such as age  [6] , gender  [7] , cultural background  [8] , and personality traits  [9] . Furthermore, as humans can express their emotions across various modalities (e.g., language, facial expressions, gestures, and speech), it is essential to effectively model the interactions between these modalities, containing complementary but also (possibly) redundant information  [10] .\n\nThe majority of works mainly concentrated on unimodal learning of emotions  [11] ,  [12] ,  [13] , i.e., processing a single modality. Although there exist breakthrough achievements by unimodal emotion recognition, due to the aforementioned multimodal nature of emotion expression, such models remain incapable in some circumstances. On the other hand, multimodal emotion recognition (MER) holds the challenges of multimodal machine learning, e.g., representing the data to be able to exploit the complementarity and redundancy of modalities, data translation among modalities, co-learning, modality alignment (e.g., capturing temporal information) and data fusion (see  [10]  for details). Like most intelligent systems, the advancements in deep learning have enhanced MER, particularly, by utilizing the abundance of data availability. Studies in this field (e.g.,  [14] ,  [15] ,  [16] ,  [17] ) so far, treat the learning process with the supervised way, thus require an intense labor for annotations.\n\nThis paper addresses the problem of perceived multimodal emotion recognition when the emotions are represented as discrete categories and, more importantly, we learn the features in an unsupervised fashion. Motivated by the fact that contrastive learning has shown accurate and robust performance in many domains (e.g.,  [18] ,  [19] ), we adapt the contrastive loss function  [20]  to perform pairwise modality feature learning. To the best of our knowledge, this is the first time contrastive loss is adapted for MER. Our approach learns feature embeddings in an end-to-end fashion (see  [21]  for the definition), and differs from the prior works in terms of several aspects, which are described as follows. i) Modality exploitation. Our method leverages different modalities in a contrastive learning framework. Given a data sample represented in terms of multiple modalities, our aim is to push the embeddings of two modalities of the same sequence to be close to each other while pulling the embeddings of the same two modalities of different sequences to be apart. Note that the sequences that are being pulled apart can be from the same class. But, herein we do not use the class labels, thus we only aim to make the representations of the same sequence across modalities similar (as close as possible) to each other. ii) Data translation & co-learning. We contrast the feature embedding of one modality with another modality when both are belonging to the same data sample. This can be seen as an analogy to performing data translation and ultimately colearning. Unlike existing contrastive learning approaches (e.g.,  [22] ,  [19] ), we do not require data spatial augmentation (e.g., random crops, blurs or color distortions). Also, different from approaches  [18] ,  [23] ,  [24]  relying on heavy data augmentations as well as large number of batch sizes and epochs, our method is much more affordable. iii) Modality alignment. The outputs of different sensors might have different (but fixed) sample rates. However, this is not valid for text, which makes obtaining word-aligned sequences not so obvious  [16] . Still, multimodal data alignment is an imperative step to perform an effective MER for several methods (e.g.,  [25] ,  [26] ), resulting in the real-world application of such methods challenging. In contrast, our method does not require perfectly aligned modalities. We considered both aligned samples and a mixture of aligned/misaligned samples in our experiments (Sec. IV-A). iv) Data fusion. It is applied here only at inference via the concatenation of learned feature representations. This is different from the MER state-of-the-art (SOTA) applying data fusion both in training and testing  [27] ,  [26] ,  [28] ,  [29] . v) Data labelling. Our method is free from data labeling cost by being an unsupervised feature learning approach. Note that there exist a few number of unsupervised approaches in the same and/or related topics, e.g., speech emotion recognition  [30] ,  [31] , facial emotion recognition  [32] , facial expression intensity estimation  [33] , and multimodal sentiment and emotion analysis  [25] . However, our method involves the deep architectures either pre-trained on tasks different from emotion recognition (e.g., action recognition) or not pre-trained. This aspect introduces a potential to apply the proposed method to the related downstream tasks, e.g., multimodal sentiment analysis and social interaction analysis, without the need of customization. Some approaches (e.g.,  [34] ,  [35] ,  [36] ), instead, could supply the desired performance (e.g., outperforming the best of all methods of comparison time) if and only if they are pre-trained on large emotion datasets having the same emotion labels as in the test set.\n\nTo validate the effectiveness of our method, experiments were realized on two multimodal emotion datasets. Results show that the proposed method outperforms prior unsupervised MER approaches and several baselines. Moreover, despite performing unsupervised feature learning, our method even surpasses some of the fully-supervised MER methods. To summarize, the main contributions of this study are: (1) presenting a novel unsupervised multimodal feature learning approach, (2) being the first study adapting the contrastive loss for MER, and (3) improving the emotion recognition results compared to unsupervised feature learning MER SOTA. The code of the proposed method is available at https://github.com/ ricfrr/mpuc-mer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Several methods for multimodal emotion recognition (MER) were proposed, as detailed in the recent survey papers:  [37] ,  [38] . In this section, our summary is regarding discrete MER research modeling text, visual and acoustic modalities, as we tested our method on that context. Early works adapt classifiers like SVMs, Linear and Logistic Regression  [39] ,  [40]  while, by the time bigger datasets were developed, deep learning architectures were also explored. For example,  [27]  is based on CNNs, and  [26] ,  [28]  use RNNs. Some recent studies  [41] ,  [14] ,  [16]  adopt Transformers.\n\nGhaleb et al.  [42]  apply deep metric learning in which a LSTM component models the variations of the emotions as a function of time. That is different from late fusion of modalities  [27] ,  [28]  or building temporal features to extract global information by assuming that emotions are expressed simultaneously  [26] . Late fusion is favorably applied by concatenating the learned features of all modalities in  [27] ,  [28]  or with a pairwise scheme in  [26] . Instead, the authors of M3ER  [29]  propose a data-driven multiplicative fusion method to combine the modalities, which learns to emphasize the more reliable cues and suppresses the others by integrating Canonical Correlation Analysis as a pre-processing step. Differently, Zadeh et al.  [43]  present Graph-MFN, which synchronizes the multimodal sequences by storing intra-modality and crossmodality interactions through time with a graph structure. Attention mechanism has been exploited by several works as well  [44] ,  [45] ,  [41] ,  [46] ,  [47] ,  [15] ,  [17] ,  [48] ,  [21] ,  [49] . For example, Dai et al.  [21]  present MESM that is composed of sparse cross-modal attention mechanism attached to the joint learning of multimodal features.\n\nThere are a lot of attempts applying end-to-end learning  [27] ,  [26] ,  [50] ,  [51] , but only  [21]  compared a fully end-toend method (defined as jointly optimizing feature extraction and feature learning stages  [21] ) with the two-phase pipelines (i.e., feature extraction is independent from multimodal learning). Indeed, it is very common in the MER litreature to apply the feature extraction step separately. This is performed on each modality by using either hand-crafted formulations  [52] ,  [53] ,  [29] ,  [26] ,  [27] ,  [43] ) and/or deep learning architectures  [42] ,  [26] ,  [27] . As example of acoustic features; Log-Mel spectrogram  [27] , pitch, voiced/unvoiced segmenting features  [26] ,  [43] ,  [29] , MFCCs  [28] ,  [26] ,  [43] ,  [29] , features extracted from SoundNet  [42] ) can be given. On the other hand, various backbones such as VGG16  [28] , I3D  [42] , FaceNet  [42]  as well as facial features; facial landmarks and facial action units extracted by OpenFace  [43] ,  [29]  are among the most popular visual features. For text, Glove embeddings  [54]  have been frequently utilized  [26] ,  [43] ,  [29] ,  [41] ,  [55] ,  [16] ,  [14] , while Transformers are used as the backbone  [41] ,  [55] ,  [16] ,  [14] ,  [49]  or LSTMs are trained with the extracted word embeddings  [43] ,  [29] .\n\nAmong the aforementioned approaches,  [45] ,  [55]  use text and audio,  [53] ,  [48] ,  [42] ,  [27] ,  [28] ,  [50] ,  [52]  use video and audio, and all others use text, audio and video together. It is worth noting that these techniques are all supervised. Recently, Khare et al.  [49]  investigated the usage of large unlabeled multimodal datasets for pre-training a cross-modal transformer, which is then fine-tuned for the emotion recognition task. In detail, the VoxCeleb dataset  [56] , composed of 1.1 million videos that are associated to emotions  [57] , is used to pre-train the multimodal transformer. Then, the decoder layer is removed, and an average pooling and additional fully connected layers are added to fine-tune the model for emotion recognition task. Unlike  [49] , we do not rely on auxiliary large-scale datasets to pre-train our model, and both the feature learning and inference are performed on the same datasets, which are much smaller than the VoxCeleb dataset  [56] .\n\nOur learned features are frozen such that we do not apply any fine-tuning as in  [49] . This is an important difference because some studies  [58] ,  [59]  have shown that, compared to using frozen features that are learned in an unsupervised fashion, fine-tuning can bring up to 17.5% improvement for the downstream task. However, following the fine-tuning approach would not keep the feature learning methodology \"entirely unsupervised\", as it requires the labels of the downstream task. Moreover, our model is applicable with different modality combinations, whereas text is an anchor modality in  [49] .\n\nThe MER litreature is very limited in terms of fully unsupervised feature learning approaches. Very recently, a Convolutional Autoencoder architecture is presented in  [25] . Despite being very different from our method in terms of the architecture,  [25]  is still our \"direct competitor\" by having the following common aspects with the proposed method: i) performing unsupervised feature learning without fine-tuning, ii) being independent to the number of modalities and modality combinations, and iii) not being task-specific.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Our Approach",
      "text": "An overview of our approach is given in Fig.  1 . First, the multimodal features are learned with an unsupervised way (Sec. III-B). Then, the downstream task (discrete emotion recognition) is performed (Sec. III-C). Sec. III-A describes the modalities and Sec. III-D presents the implementation details.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Modalities",
      "text": "The modalities and backbones we utilize are described as follows.\n\nText. The word vectors are extracted from transcripts with the Glove word embeddings  [54] , following the procedure in  [43] . As the backbone, we use the Transformer in  [60] , which is one of the SOTA architectures of language processing.\n\nVisual. We rely on two sources of visual data. One of them is the facial images extracted by MTCNN face detector  [61]  (unless faces are supplied by the dataset) from RGB video frames. As the backbone associated to the facial images, the R(2+1)D architecture  [62]  pre-trained on Kinetics-400 dataset  [63]  is used. The other visual data is the facial landmarks detected by the method in  [64]  (unless it is provided by the dataset used), and the associated backbone is Spatio-Temporal Graph Neural Network (ST-GCN)  [65] .\n\nAcoustic. Mel-spectograms are extracted with the same procedure and settings in  [66] ,  [41] ,  [55]  with Librosa Python Library  [67]  using 80 filter banks and by selecting one frame for every 16 frames. The dimension of the mel-spectograms is fixed to 128. We adapt Time Convoluted Network (TCN)  [68]  such that it takes mel-spectrograms as the input.\n\nAs seen, each modality has its own backbone, which have been chosen as being the SOTA architectures for diverse applications of language, visual and acoustic data processing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Unsupervised Multimodal Feature Learning",
      "text": "The proposed method includes separate multi-layer projection heads onto each backbone defined in Sec. III-A. All projection heads have the same structure such that they are composed of fully-connected layers (f c), where the first layer is followed by a ReLU activation function (f c 1 +ReLU +f c 2 ). This structure is motivated by SimCLR  [18] , which shows that a nonlinear projection head contributes to the performance more than a linear projection head, and its contribution is even more compared to not including any projection layer.\n\nWe adapt the CLIP fashion  [69]  training, without using any labels of the downstream task (i.e., emotion recognition).\n\nGiven a data sample represented by a sequence of observations in multiple modalities, our aim is to make the embeddings of two modalities of the same sequence (positives) close to each other, and make the embeddings of the same two modalities of different sequences (negatives) apart from each other. This is repeated for all possible pairs of modalities. Notice that negative samples might belong to the same class (i.e. exhibit the same emotion). However, herein, we assume that the class labels are not available, and we resort to instance discrimination with contrastive learning which encourages the model to produce invariant representations and align the latent spaces of all the modalities.\n\nMore formally, the contrastive loss function for a pair of modalities (m,n) has the following form:\n\nz denotes the embedding after the projection, i, j are indices of samples in the current batch of size N , τ is the temperature parameter (scalar),\n\ndenotes the dot product between 2 -normalized vectors u and v (i.e., cosine similarity). Eq. (  1 ) is computed across all samples i in the batch, resulting in L m,n = N i=1 L m,n i . In addition, we minimize this loss for each possible pairs of modalities. Notice that, since the negatives are drawn from only one modality (see denominator in Eq. (  1 )), the loss is asymmetric, i.e., L m,n is not equal to L n,m . Therefore, our final loss function (Eq. (  2 )) includes the loss obtained from all the permutations of two elements drawn with replacement from the set of modalities M:\n\nNote that, we found empirically that only contrasting different modalities (i.e. when m = n) produces better representations. In addition, we perform temporal augmentations (see Sec. III-D for details) to the sequences in order to avoid overfitting and improve performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Discrete Emotion Recognition",
      "text": "Following the common practice  [18] ,  [70] , in order to perform the downstream task (i.e., discrete emotion recognition), we discard the projection layers (described in Sec. III-B) and use the 512-dimensional feature representation extracted from each backbone. The extracted features are concatenated (e.g., for 3 modalities, the combined vector holds 3×512 number of features) and given to a prediction layer, that shares the same design with the projection heads (i.e., f c+RELU+f c) where its output is the emotion classes. The aforementioned prediction layers are trained with the emotion labels using the cross entropy loss and a variant of it (see Sec. IV-B for details).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Implementation Details",
      "text": "The training is performed with the SGD optimizer with the momentum of 0.9 and the weight decay of 0.001. All models are trained with the batch size of 32 (or 64) while the batch size of our downstream task is 64 (or 128). The learning rate is initialized as 0.001. We create a linear scheduler to vary the learning rate over the training process such that at every 5 epochs for CMU-MOSEI  [43]  and every 100 for RAVDESS  [71] , we multiply the learning rate with 0.9 (notice that RAVDESS dataset is much smaller than CMU-MOSEI). We do not apply any \"spatial\" data augmentation (e.g., random crops, blurs or color distortions), but data sampling can have overlapping sequences. For example, a video segment from t to t+10, and another video segment from t+5 to t+15 can be used in the same training. This is referred as augmentation in the temporal dimension. We set the number of epochs to 2000, but we also define a patience parameter such that: if after 100 consecutive epochs the validation performance does not change, then we stop the training. In practice, the maximum number of epochs was never been reached because the patience parameter stopped the training before. The temperature scalar τ is taken as 0.07.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets And Evaluation Metrics",
      "text": "We used the speech part of RAVDESS dataset  [71] , containing 2880 audio-visual recordings acted by 24 professional actors pronouncing two lexically identical statements. Each recording was labeled in terms of one of the eight categorical emotions (anger, happiness, disgust, fear, surprise, sadness, calmness and neutral), while the emotions were expressed with two intensity (normal or strong). RAVDESS is classbalanced except the neutral class, which was elicited 50% less time than the other emotion classes. We adapted two crossvalidation settings following the methods  [42] ,  [48] ,  [27] ,  [28] ,  [13] ,  [72] ,  [44] ,  [53] ,  [12] ,  [52] . The first setting considers the identities of the actors such that the training (validation) and the corresponding testing k-folds have no overlap in terms of actors (shown as actor-split= hereafter). The second setting,  instead, applies standard k-fold cross-validation (i.e., actor-split=). In both settings, k was taken as 10 and the reported results are in terms of accuracy (ACC), which is averaged over the 10-folds, supplying fair comparisons with the MER SOTA  [42] ,  [48] ,  [27] ,  [28] ,  [44] ,  [53] ,  [52] . As the same statements are being repeated by the actors in RAVDESS dataset  [71] , the proposed method (as well as the SOTA) are based only on visual and acoustic modalities.\n\nThe CMU-MOSEI  [43]  is the largest multimodal in-thewild dataset in the MER domain. It consists of more than 23K utterances, belonging to more than 1000 speakers, collected from YouTube videos. Each utterance is labeled with six emotions: happiness, sadness, anger, fear, disgust, and surprise with a [0,3] Likert scale for the presence of each emotion class. Following  [43] ,  [21] ,  [41] ,  [29] ,  [26] ,  [46] ,  [14] ,  [15] ,  [16] ,  [17] , the emotions were treated as either present or not present (i.e., binary classification), while more than one emotion can be present at the same time, making the task a multi-label problem. There exist (≈ 3000) not-correctly aligned sequences across the modalities. As our approach does not require strict data alignment, we used all sequences as supplied in CMU-MOSEI SDK  [73] . In other words, we did not apply any data cleaning, e.g., as in  [21] . We also used the recommended dataset split and the evaluation metrics in  [43] , namely weighted accuracy  [74]  (w-ACC) and F1-measure.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Comparisons With The Baseline Methods",
      "text": "We compare the proposed approach with the following baseline methods. These baselines are all supervised such that cross-entropy and binary cross-entropy losses were used for RAVDESS  [71]  and CMU-MOSEI  [43] , respectively. The corresponding results are given in Tables  I  and II . Unimodal Learning. Each modality was trained with its associated backbone (described in Sec. III-A) followed by two fully connected (f c) layers with a ReLU activation function. The best results were obtained with the following parameter settings. For acoustic data, the learning rate was initialized with 0.001 and decreased by multiplying it with 0.9 at every 10 epochs. The batch size was 32 and number of epoch was 100. For facial images, the learning rate was 0.01, number of epoch was 150 and the momentum was 0.9. For facial landmarks, the learning rate was 0.001, momentum was 0.9 and the number of epochs was set as the proposed method with patience parameter. Late Fusion. Recall that late fusion was applied by several SOTA methods, e.g.,  [27] ,  [28] ,  [26] . Given the modalities and the backbones described, we concatenated the feature embeddings of each modality, and fed them to a shallow network composed of two f c layers with a ReLU activation function. The batch size was taken as 32, the number of epochs was set by the patience parameter, the learning rate and momentum were taken as 0.001 and 0.9, respectively. Attention Mechanism. As mention in Sec. II, attention mechanism has been frequently applied in MER, hence we adapted it as a baseline too. We first concatenated the feature embeddings obtained from each modality (512 features extracted from each backbone as in our method) and then applied the multi-head attention mechanism of  [60] . The batch size was 64, the learning rate was 0.001, and the number of epochs was set to 2000 with the patience parameter described in Sec. III-D. The same scheduler as the proposed method was used.\n\nAs seen in Table  I , our unsupervised feature learning method outperforms all of the supervised baselines when acoustic and facial landmarks are involved. It is notable that, in the visual domain, the facial landmarks are more effective than the facial images. Out of all baseline methods, late fusion and attention mechanism surpass the unimodal setups, while attention mechanism achieves slightly better results than the late fusion. Overall, all methods perform better in the actor-split= setting compared to their actor-split= counterpart. This is perhaps as a result of having more training data in the actor-split= setting. With reference to Table  I , we have further investigated the contribution of used modalities with respect to different emotions by inspecting the confusion matrices. Our observation is that there is no particular modality or a pair of modality which performs better for a specific Given the better performances of late fusion and attention mechanism compared to unimodal learning in Table  I , we inherited them to test on CMU-MOSEI dataset  [43]  when four modalities (text, facial images, acoustic and facial landmarks) are used. Additionally, in order to investigate the contribution of the text modality, we compare the results of the proposed method with the performance of the proposed method when the text is discarded (shown as wout/ text). The corresponding results can be seen in Table  II . Our method outperforms the baselines for all emotion classes (especially for surprise) as well as on average (see Table  II ). Also, the performances of our method do not fluctuate across different emotion classes, meaning that our method generalize better than the baseline methods. In overall there exist a drop of 11.2% and 41.73% for w-ACC and F1-measure, respectively, when the text modality is discarded from the pipeline of the proposed method, showing the positive contribution of the text modality.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Comparisons With The State-Of-The-Art Methods",
      "text": "We compare our approach with several SOTA MER methods. Concerning RAVDESS  [71] , the performances are given in Table  III . The fact that \"human performance\" is not 100% presents the difficulty of MER task. It is remarkable that our approach surpasses several supervised competitors:  [42] ,  [48] ,  [44] ,  [28]  with a margin of 2-35% despite working in a more difficult (unsupervised) setting. It also performs on par with supervised approaches:  [27] ,  [52] . The results for CMU-MOSEI  [43]  are given in Table  IV . There exist a very recent unsupervised feature learning approach (namely CAE-LR  [25] ) tested on CMU-MOSEI  [43]  for multimodal sentiment analysis. CAE-LR  [25]  achieved the best results for multimodal sentiment analysis compared to other unsupervised counterparts. Motivated by this, we adapted the authors' code for MER. Instead of applying Logistic Regression, we performed Linear Evaluation  [58] , which is the common protocol for unsupervised learning if the downstream task is classification (notice that we apply it for the proposed method as well, i.e., the prediction layer). For all emotion classes and on overall, our method achieves much better results than CAE-LR  [25] , showing the effectiveness of the contrastive loss in multimodal setting compared to convolutional autoencoders. It is worth noting that, on average, our method is better than several fully supervised techniques: MESM  [21] , FE2E  [21] , Graph-MFN  [43] ,  [51] , CIA  [46] . Considering that these methods integrate relatively complex supervised techniques; attention mechanisms, transformers, graphs, the better performance of our method is very promising.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "We presented an unsupervised multimodal feature learning approach, which was tested on discrete emotion recognition. Our method is a pioneer in the MER litreature, being based on pairwise contrastive learning. Experiments show that the performance of our approach is better than the supervised baselines and unsupervised counterpart, while being competitive to several complex supervised SOTA and even surpassing a few. Being an unsupervised feature learning method, the proposed approach is transferable to other domains without retraining (not even tuning) the representation model itself.\n\nThe proposed method keeps the modality pairings the same for all data (i.e., emotions) and the way we learn the features gives equal importance to each modality. An alternative could be having different modality pairings for different emotion classes. This will be further investigated as future work.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Summary of our approach. We ﬁrst learn the multimodal features in an unsupervised fashion, then the downstream task (discrete emotion recognition)",
      "page": 3
    },
    {
      "caption": "Figure 1: First, the",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Unimodal": "Unimodal",
          "(cid:51)": "",
          "Column_4": "(cid:51)",
          "Column_5": "",
          "60.80": "58.50"
        },
        {
          "Unimodal": "Unimodal",
          "(cid:51)": "",
          "Column_4": "",
          "Column_5": "(cid:51)",
          "60.80": "62.05"
        },
        {
          "Unimodal": "LateFusion",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "60.80": "64.10"
        },
        {
          "Unimodal": "AttentionMec.",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "60.80": "65.40"
        },
        {
          "Unimodal": "Ours",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "",
          "60.80": "63.78"
        },
        {
          "Unimodal": "Ours",
          "(cid:51)": "",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "60.80": "77.10"
        },
        {
          "Unimodal": "Ours",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "60.80": "78.54"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Unimodal": "Unimodal",
          "Column_2": "",
          "(cid:51)": "",
          "Column_4": "(cid:51)",
          "Column_5": "",
          "72.80": "75.90"
        },
        {
          "Unimodal": "Unimodal",
          "Column_2": "",
          "(cid:51)": "",
          "Column_4": "",
          "Column_5": "(cid:51)",
          "72.80": "76.35"
        },
        {
          "Unimodal": "LateFusion",
          "Column_2": "",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "72.80": "80.72"
        },
        {
          "Unimodal": "AttentionMec.",
          "Column_2": "",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "72.80": "81.80"
        },
        {
          "Unimodal": "Ours",
          "Column_2": "",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "",
          "72.80": "80.32"
        },
        {
          "Unimodal": "Ours",
          "Column_2": "",
          "(cid:51)": "",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "72.80": "89.50"
        },
        {
          "Unimodal": "Ours",
          "Column_2": "",
          "(cid:51)": "(cid:51)",
          "Column_4": "(cid:51)",
          "Column_5": "(cid:51)",
          "72.80": "93.17"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "LateFusion",
          "w-ACC": "54.60",
          "F1": "30.50"
        },
        {
          "Column_1": "AttentionMec.",
          "w-ACC": "55.90",
          "F1": "38.00"
        },
        {
          "Column_1": "Ourswout/text",
          "w-ACC": "55.50",
          "F1": "27.77"
        },
        {
          "Column_1": "Ours",
          "w-ACC": "66.70",
          "F1": "69.50"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Humanperformance[71]": "Ghalebetal.[42]",
          "-": "Supervised",
          "80.00": "67.70"
        },
        {
          "Humanperformance[71]": "Ghalebetal.[48]",
          "-": "Supervised",
          "80.00": "69.40"
        },
        {
          "Humanperformance[71]": "Ghalebetal.[48](w/ATT)",
          "-": "Supervised",
          "80.00": "76.30"
        },
        {
          "Humanperformance[71]": "Radoietal.[27]",
          "-": "Supervised",
          "80.00": "78.70"
        },
        {
          "Humanperformance[71]": "Ours",
          "-": "Unsupervised",
          "80.00": "78.54"
        },
        {
          "Humanperformance[71]": "Beardetal.[44]",
          "-": "Supervised",
          "80.00": "58.30"
        },
        {
          "Humanperformance[71]": "Songetal.[28]",
          "-": "Supervised",
          "80.00": "90.00"
        },
        {
          "Humanperformance[71]": "Tiwarietal.[52]",
          "-": "Supervised",
          "80.00": "93.30"
        },
        {
          "Humanperformance[71]": "Ours",
          "-": "Unsupervised",
          "80.00": "93.17"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CAE-LR[25]": "Ours",
          "Column_2": "68.82",
          "64.70": "",
          "Column_4": "",
          "Column_5": "69.20",
          "65.60": "",
          "Column_7": "",
          "Column_8": "62.93",
          "53.20": "",
          "Column_10": "",
          "Column_11": "55.70",
          "55.60": "",
          "Column_13": "",
          "Column_14": "67.91",
          "61.80": "",
          "Column_16": "",
          "Column_17": "70.09",
          "61.90": "",
          "Column_19": "",
          "Column_20": "62.93",
          "57.10": "",
          "Column_22": "",
          "Column_23": "72.73",
          "70.70": "",
          "Column_25": "",
          "Column_26": "72.91",
          "69.00": "",
          "Column_28": "",
          "Column_29": "74.25",
          "70.10": "",
          "Column_31": "",
          "Column_32": "64.49",
          "60.40": "",
          "Column_34": "",
          "Column_35": "74.85",
          "69.20": "",
          "Column_37": "",
          "Column_38": "66.70",
          "61.03": "",
          "Column_40": "",
          "Column_41": "69.50",
          "65.52": "",
          "Column_43": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MESM[21]": "Zhangetal.[15]",
          "Column_2": "71.70",
          "64.10": "",
          "Column_4": "",
          "72.30": "–",
          "Column_6": "",
          "Column_7": "",
          "63.00": "64.30",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "–",
          "46.60": "",
          "Column_13": "",
          "Column_14": "",
          "66.80": "67.00",
          "Column_16": "",
          "Column_17": "–",
          "49.30": "",
          "Column_19": "",
          "65.70": "",
          "Column_21": "62.30",
          "Column_22": "",
          "Column_23": "–",
          "27.20": "",
          "Column_25": "",
          "75.60": "",
          "Column_27": "72.50",
          "Column_28": "",
          "Column_29": "–",
          "56.40": "",
          "Column_31": "",
          "65.80": "64.60",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "–",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "–",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "FE2E[21]",
          "Column_2": "",
          "64.10": "65.40",
          "Column_4": "",
          "72.30": "72.60",
          "Column_6": "",
          "Column_7": "",
          "63.00": "65.20",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "46.60": "49.00",
          "Column_13": "",
          "Column_14": "",
          "66.80": "67.60",
          "Column_16": "",
          "Column_17": "",
          "49.30": "49.60",
          "Column_19": "",
          "65.70": "66.70",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "27.20": "29.10",
          "Column_25": "",
          "75.60": "77.70",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "56.40": "57.10",
          "Column_31": "",
          "65.80": "",
          "Column_33": "63.80",
          "Column_34": "",
          "Column_35": "",
          "28.90": "26.80",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "46.80": "47.40",
          "Column_43": ""
        },
        {
          "MESM[21]": "Graph-MFN[43]",
          "Column_2": "",
          "64.10": "66.30",
          "Column_4": "",
          "72.30": "",
          "Column_6": "66.30",
          "Column_7": "",
          "63.00": "",
          "Column_9": "60.40",
          "Column_10": "",
          "Column_11": "66.90",
          "46.60": "",
          "Column_13": "",
          "Column_14": "",
          "66.80": "",
          "Column_16": "",
          "Column_17": "72.80",
          "49.30": "",
          "Column_19": "",
          "65.70": "",
          "Column_21": "53.70",
          "Column_22": "",
          "Column_23": "85.50",
          "27.20": "",
          "Column_25": "",
          "75.60": "",
          "Column_27": "69.10",
          "Column_28": "",
          "Column_29": "76.60",
          "56.40": "",
          "Column_31": "",
          "65.80": "",
          "Column_33": "62.00",
          "Column_34": "",
          "Column_35": "89.90",
          "28.90": "",
          "Column_37": "",
          "Column_39": "62.35",
          "Column_40": "",
          "Column_41": "76.33",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "Delbroucketal.[41]",
          "Column_2": "–",
          "64.10": "",
          "Column_4": "",
          "72.30": "",
          "Column_6": "64.00",
          "Column_7": "",
          "63.00": "–",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "67.90",
          "46.60": "",
          "Column_13": "",
          "Column_14": "–",
          "66.80": "–",
          "Column_16": "",
          "Column_17": "74.70",
          "49.30": "",
          "Column_19": "",
          "65.70": "–",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "86.10",
          "27.20": "",
          "Column_25": "",
          "75.60": "–",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "83.60",
          "56.40": "",
          "Column_31": "",
          "65.80": "–",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "84.00",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "76.72",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "Huynhetal.[51]",
          "Column_2": "",
          "64.10": "62.70",
          "Column_4": "",
          "72.30": "",
          "Column_6": "63.00",
          "Column_7": "",
          "63.00": "",
          "Column_9": "54.40",
          "Column_10": "",
          "Column_11": "69.70",
          "46.60": "",
          "Column_13": "",
          "Column_14": "",
          "66.80": "",
          "Column_16": "",
          "Column_17": "74.30",
          "49.30": "",
          "Column_19": "",
          "65.70": "",
          "Column_21": "50.60",
          "Column_22": "",
          "Column_23": "85.70",
          "27.20": "",
          "Column_25": "",
          "75.60": "",
          "Column_27": "66.00",
          "Column_28": "",
          "Column_29": "81.30",
          "56.40": "",
          "Column_31": "",
          "65.80": "",
          "Column_33": "52.90",
          "Column_34": "",
          "Column_35": "86.40",
          "28.90": "",
          "Column_37": "",
          "Column_39": "57.70",
          "Column_40": "",
          "Column_41": "76.73",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "Khareetal.[49]",
          "Column_2": "",
          "64.10": "68.10",
          "Column_4": "",
          "72.30": "",
          "Column_6": "68.20",
          "Column_7": "",
          "63.00": "64.30",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "72.40",
          "46.60": "",
          "Column_13": "",
          "Column_14": "",
          "66.80": "66.90",
          "Column_16": "",
          "Column_17": "74.80",
          "49.30": "",
          "Column_19": "",
          "65.70": "65.10",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "87.70",
          "27.20": "",
          "Column_25": "",
          "75.60": "73.60",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "82.40",
          "56.40": "",
          "Column_31": "",
          "65.80": "",
          "Column_33": "63.00",
          "Column_34": "",
          "Column_35": "86.60",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "78.68",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "CIA[46]",
          "Column_2": "",
          "64.10": "51.90",
          "Column_4": "",
          "72.30": "71.30",
          "Column_6": "",
          "Column_7": "",
          "63.00": "",
          "Column_9": "61.80",
          "Column_10": "",
          "Column_11": "72.90",
          "46.60": "",
          "Column_13": "",
          "Column_14": "",
          "66.80": "",
          "Column_16": "",
          "Column_17": "74.70",
          "49.30": "",
          "Column_19": "",
          "65.70": "",
          "Column_21": "58.20",
          "Column_22": "",
          "Column_23": "86.00",
          "27.20": "",
          "Column_25": "",
          "75.60": "74.10",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "81.80",
          "56.40": "",
          "Column_31": "",
          "65.80": "",
          "Column_33": "63.90",
          "Column_34": "",
          "Column_35": "87.80",
          "28.90": "",
          "Column_37": "",
          "Column_39": "62.88",
          "Column_40": "",
          "Column_41": "79.08",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "Tsaietal.[14]",
          "Column_2": "71.00",
          "64.10": "",
          "Column_4": "",
          "72.30": "71.00",
          "Column_6": "",
          "Column_7": "",
          "63.00": "75.00",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "72.10",
          "46.60": "",
          "Column_13": "",
          "Column_14": "78.30",
          "66.80": "81.58",
          "Column_16": "",
          "Column_17": "75.00",
          "49.30": "",
          "Column_19": "",
          "65.70": "90.50",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "86.10",
          "27.20": "",
          "Column_25": "",
          "75.60": "83.00",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "82.50",
          "56.40": "",
          "Column_31": "",
          "65.80": "91.70",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "87.80",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "79.08",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "Wenetal.[16]",
          "Column_2": "72.50",
          "64.10": "",
          "Column_4": "",
          "72.30": "72.60",
          "Column_6": "",
          "Column_7": "",
          "63.00": "75.60",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "70.70",
          "46.60": "",
          "Column_13": "",
          "Column_14": "77.10",
          "66.80": "82.08",
          "Column_16": "",
          "Column_17": "74.90",
          "49.30": "",
          "Column_19": "",
          "65.70": "90.60",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "86.10",
          "27.20": "",
          "Column_25": "",
          "75.60": "85.00",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "83.20",
          "56.40": "",
          "Column_31": "",
          "65.80": "91.70",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "87.80",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "79.22",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "Shenoyetal.[26]",
          "Column_2": "70.00",
          "64.10": "",
          "Column_4": "",
          "72.30": "",
          "Column_6": "68.40",
          "Column_7": "",
          "63.00": "76.10",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "74.50",
          "46.60": "",
          "Column_13": "",
          "Column_14": "83.10",
          "66.80": "82.77",
          "Column_16": "",
          "Column_17": "80.90",
          "49.30": "",
          "Column_19": "",
          "65.70": "87.40",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "84.00",
          "27.20": "",
          "Column_25": "",
          "75.60": "90.30",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "87.30",
          "56.40": "",
          "Column_31": "",
          "65.80": "89.70",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "87.00",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "80.35",
          "46.80": "",
          "Column_43": ""
        },
        {
          "MESM[21]": "M3ER[29]",
          "Column_2": "–",
          "64.10": "",
          "Column_4": "",
          "72.30": "78.00",
          "Column_6": "",
          "Column_7": "",
          "63.00": "–",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "87.30",
          "46.60": "",
          "Column_13": "",
          "Column_14": "–",
          "66.80": "–",
          "Column_16": "",
          "Column_17": "81.60",
          "49.30": "",
          "Column_19": "",
          "65.70": "–",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "93.20",
          "27.20": "",
          "Column_25": "",
          "75.60": "–",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "84.40",
          "56.40": "",
          "Column_31": "",
          "65.80": "–",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "91.80",
          "28.90": "",
          "Column_37": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "86.05",
          "46.80": "",
          "Column_43": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "SPRING: Socially pertinent robots in gerontological healthcare",
      "venue": "SPRING: Socially pertinent robots in gerontological healthcare"
    },
    {
      "citation_id": "2",
      "title": "Detecting anger in automated voice portal dialogs",
      "authors": [
        "F Burkhardt",
        "J Ajmera",
        "R Englert",
        "J Stegmann",
        "W Burleson"
      ],
      "year": "2006",
      "venue": "Detecting anger in automated voice portal dialogs"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition for healthcare surveillance systems using neural networks: A survey",
      "authors": [
        "M Dhuheir",
        "A Albaseer",
        "E Baccour",
        "A Erbad",
        "M Abdallah",
        "M Hamdi"
      ],
      "year": "2021",
      "venue": "Emotion recognition for healthcare surveillance systems using neural networks: A survey"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in e-learning systems",
      "authors": [
        "O Hammoumi",
        "F Benmarrakchi",
        "N Ouherrou",
        "J Kafi",
        "A Hore"
      ],
      "year": "2018",
      "venue": "ICMCS"
    },
    {
      "citation_id": "5",
      "title": "Smart emotion recognition framework: A secured iovt perspective",
      "authors": [
        "P Paikrao",
        "A Mukherjee",
        "D Jain",
        "P Chatterjee",
        "W Alnumay"
      ],
      "year": "2021",
      "venue": "IEEE Consumer Electronics Magazine"
    },
    {
      "citation_id": "6",
      "title": "Age-and genderrelated variations of emotion recognition in pseudowords and faces",
      "authors": [
        "L Demenescu",
        "K Mathiak",
        "K Mathiak"
      ],
      "year": "2014",
      "venue": "Experimental Aging Research"
    },
    {
      "citation_id": "7",
      "title": "Sex differences in facial emotion perception ability across the lifespan",
      "authors": [
        "S Olderbak",
        "O Wilhelm",
        "A Hildebrandt",
        "J Quoidbach"
      ],
      "year": "2019",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "8",
      "title": "Emotion perception across cultures: the role of cognitive mechanisms",
      "authors": [
        "J Engelmann",
        "M Pogosyan"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "9",
      "title": "Exploring the effects of personality traits on the perception of from prosody",
      "authors": [
        "D Furnes",
        "H Berg",
        "R Mitchell",
        "S Paulmann"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "10",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrusaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "11",
      "title": "Modeling multiple temporal scales of full-body movements for",
      "authors": [
        "C Beyan",
        "S Karumuri",
        "G Volpe"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Facial expression recognition in videos: An cnn-lstm based model for video classification",
      "authors": [
        "M Abdullah",
        "M Ahmad",
        "D Han"
      ],
      "venue": "Facial expression recognition in videos: An cnn-lstm based model for video classification"
    },
    {
      "citation_id": "13",
      "title": "Dynamic emotion modeling with learnable graphs and graph inception network",
      "authors": [
        "A Shirian",
        "S Tripathi",
        "T Guha"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Multimodal routing: Improving local and global interpretability of multimodal language analysis",
      "authors": [
        "Y.-H Tsai",
        "M Ma",
        "M Yang",
        "R Salakhutdinov",
        "L.-P Morency"
      ],
      "venue": "Multimodal routing: Improving local and global interpretability of multimodal language analysis"
    },
    {
      "citation_id": "15",
      "title": "Multi-modal language analysis with hierarchical interaction-level and selection-level attentions",
      "authors": [
        "D Zhang",
        "L Wu",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IEEE ICME"
    },
    {
      "citation_id": "16",
      "title": "Cross-modal dynamic convolution for multi-modal emotion recognition",
      "authors": [
        "H Wen",
        "S You",
        "Y Fu"
      ],
      "year": "2021",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "17",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "venue": "A simple framework for contrastive learning of visual representations"
    },
    {
      "citation_id": "19",
      "title": "Cocon: Cooperative-contrastive learning",
      "authors": [
        "N Rai",
        "E Adeli",
        "K.-H Lee",
        "A Gaidon",
        "J Niebles"
      ],
      "year": "2021",
      "venue": "IEEE/CVF CVPR Workshops"
    },
    {
      "citation_id": "20",
      "title": "Contrastive predictive coding of audio with an adversary",
      "authors": [
        "L Wang",
        "K Kawakami",
        "A Van Den Oord"
      ],
      "year": "2020",
      "venue": "Contrastive predictive coding of audio with an adversary"
    },
    {
      "citation_id": "21",
      "title": "Multimodal end-toend sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Multimodal end-toend sparse model for emotion recognition"
    },
    {
      "citation_id": "22",
      "title": "Augmented skeleton based contrastive action learning with momentum lstm for unsupervised action recognition",
      "authors": [
        "H Rao",
        "S Xu",
        "X Hu",
        "J Cheng",
        "B Hu"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "23",
      "title": "Big self-supervised models are strong semi-supervised learners",
      "authors": [
        "T Chen",
        "S Kornblith",
        "K Swersky",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Big self-supervised models are strong semi-supervised learners"
    },
    {
      "citation_id": "24",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "venue": "Contrastive unsupervised learning for speech emotion recognition"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised multimodal language representations using convolutional autoencoders",
      "authors": [
        "P Koromilas",
        "T Giannakopoulos"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "26",
      "title": "Multilogue-net: A context-aware RNN for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "A Shenoy",
        "A Sardana"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language"
    },
    {
      "citation_id": "27",
      "title": "An end-toend emotion recognition framework based on temporal aggregation of multimodal information",
      "authors": [
        "A Radoi",
        "A Birhala",
        "N.-C Ristea",
        "L.-C Dutu"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "28",
      "title": "Video-audio emotion recognition based on feature fusion deep learning method",
      "authors": [
        "Y Song",
        "Y Cai",
        "L Tan"
      ],
      "year": "2021",
      "venue": "IEEE Int. Midwest Symposium on Circuits and Systems (MWSCAS)"
    },
    {
      "citation_id": "29",
      "title": "M3ER: multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "30",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "31",
      "title": "Transformer based unsupervised pre-training for acoustic representation learning",
      "authors": [
        "R Zhang",
        "H Wu",
        "W Li",
        "D Jiang",
        "W Zou",
        "X Li"
      ],
      "venue": "Transformer based unsupervised pre-training for acoustic representation learning"
    },
    {
      "citation_id": "32",
      "title": "Unsupervised emotion recognition algorithm based on improved deep belief model in combination with probabilistic linear discriminant analysis",
      "authors": [
        "Y Xiao",
        "D Wang",
        "L Hou"
      ],
      "year": "2019",
      "venue": "Personal Ubiquitous Comput"
    },
    {
      "citation_id": "33",
      "title": "Unsupervised features for facial expression intensity estimation over time",
      "authors": [
        "M Awiszus",
        "S Grashof",
        "F Kuhnke",
        "J Ostermann"
      ],
      "year": "2018",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "34",
      "title": "Deep multi-task learning to recognise subtle facial expressions of mental states",
      "authors": [
        "G Hu",
        "L Liu",
        "Y Yuan",
        "Z Yu",
        "Y Hua",
        "Z Zhang",
        "F Shen",
        "L Shao",
        "T Hospedales",
        "N Robertson",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "35",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "IEEE SISY"
    },
    {
      "citation_id": "36",
      "title": "Fusing visual attention cnn and bag of visual words for cross-corpus speech emotion recognition",
      "authors": [
        "M Seo",
        "M Kim"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "38",
      "title": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
      "authors": [
        "G Sharma",
        "A Dhall"
      ],
      "year": "2021",
      "venue": "A Survey on Automatic Multimodal Emotion Recognition in the Wild"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition through multiple modalities: Face, body gesture, speech",
      "authors": [
        "G Castellano",
        "L Kessous",
        "G Caridakis"
      ],
      "year": "2008",
      "venue": "Emotion recognition through multiple modalities: Face, body gesture, speech"
    },
    {
      "citation_id": "40",
      "title": "Multiple kernel learning for emotion recognition in the wild",
      "authors": [
        "K Sikka",
        "K Dykstra",
        "S Sathyanarayana",
        "G Littlewort",
        "M Bartlett"
      ],
      "year": "2013",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "41",
      "title": "A transformerbased joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "J.-B Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "42",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "Multimodal and temporal perception of audio-visual cues for emotion recognition"
    },
    {
      "citation_id": "43",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "44",
      "title": "Multi-modal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "R Beard",
        "R Das",
        "R Ng",
        "P Gopalakrishnan",
        "L Eerens",
        "P Swietojanski",
        "O Miksik"
      ],
      "venue": "Multi-modal sequence fusion via recursive attention for emotion recognition"
    },
    {
      "citation_id": "45",
      "title": "Convolutional attention networks for multimodal emotion recognition from speech and text data",
      "authors": [
        "W Choi",
        "K Song",
        "C Lee"
      ],
      "year": "2018",
      "venue": "Grand Challenge and Workshop on Human Multimodal Language"
    },
    {
      "citation_id": "46",
      "title": "Contextaware interactive attention for multi-modal sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "M Akhtar",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "47",
      "title": "A deep multi-task contextual attention framework for multi-modal affect analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "A Ekbal"
      ],
      "year": "2020",
      "venue": "ACM Trans. Knowl. Discov. Data"
    },
    {
      "citation_id": "48",
      "title": "Multimodal attentionmechanism for temporal emotion recognition",
      "authors": [
        "E Ghaleb",
        "J Niehues",
        "S Asteriadis"
      ],
      "year": "2020",
      "venue": "IEEE ICIP"
    },
    {
      "citation_id": "49",
      "title": "Self-supervised learning with cross-modal transformers for emotion recognition",
      "authors": [
        "A Khare",
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "50",
      "title": "Multi-modal residual perceptron network for audio-video emotion recognition",
      "authors": [
        "X Chang",
        "W Skarbek"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "51",
      "title": "End-to-end learning for multimodal emotion recognition in video with adaptive loss",
      "authors": [
        "V Huynh",
        "H.-J Yang",
        "G.-S Lee",
        "S.-H Kim"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "52",
      "title": "Multimodal emotion recognition using sda-lda algorithm in video clips",
      "authors": [
        "P Tiwari",
        "H Rathod",
        "S Thakkar",
        "A Darji"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "53",
      "title": "Bimodal emotion recognition using deep belief network",
      "authors": [
        "A Jaratrotkamjorn",
        "A Choksuriwong"
      ],
      "year": "2019",
      "venue": "Int. Computer Science and Engineering Conference"
    },
    {
      "citation_id": "54",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "55",
      "title": "Modulated fusion using transformer for linguistic-acoustic emotion recognition",
      "authors": [
        "J.-B Delbrouck",
        "N Tits",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "First International Workshop on Natural Language Processing Beyond Text"
    },
    {
      "citation_id": "56",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "ACMMM"
    },
    {
      "citation_id": "58",
      "title": "Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance",
      "authors": [
        "G Paoletti",
        "J Cavazza",
        "C Beyan",
        "A Del Bue"
      ],
      "venue": "Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance"
    },
    {
      "citation_id": "59",
      "title": "3d human action representation learning via cross-view consistency pursuit",
      "authors": [
        "L Linguo",
        "W Minsi",
        "N Bingbing",
        "W Hang",
        "Y Jiancheng",
        "Z Wenjun"
      ],
      "venue": "3d human action representation learning via cross-view consistency pursuit"
    },
    {
      "citation_id": "60",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "61",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "62",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "63",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev",
        "M Suleyman",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "64",
      "title": "How far are we from solving the 2d and 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "How far are we from solving the 2d and 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)"
    },
    {
      "citation_id": "65",
      "title": "Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting",
      "authors": [
        "B Yu",
        "H Yin",
        "Z Zhu"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "66",
      "title": "Efficiently trainable textto-speech system based on deep convolutional networks with guided attention",
      "authors": [
        "H Tachibana",
        "K Uenoyama",
        "S Aihara"
      ],
      "year": "2018",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "67",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "68",
      "title": "Asteroid: the PyTorch-based audio source separation toolkit for researchers",
      "authors": [
        "M Pariente",
        "S Cornell",
        "J Cosentino",
        "S Sivasankaran",
        "E Tzinis",
        "J Heitkaemper",
        "M Olvera",
        "F.-R Stöter",
        "M Hu",
        "J Martín-Doñas",
        "D Ditter",
        "A Frank",
        "A Deleforge",
        "E Vincent"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "69",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim"
      ],
      "venue": "ICML, 2021"
    },
    {
      "citation_id": "70",
      "title": "Spatiotemporal contrastive video representation learning",
      "authors": [
        "R Qian",
        "T Meng",
        "B Gong",
        "M.-H Yang",
        "H Wang",
        "S Belongie",
        "Y Cui"
      ],
      "year": "2021",
      "venue": "Spatiotemporal contrastive video representation learning"
    },
    {
      "citation_id": "71",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "72",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Hitkul",
        "Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "73",
      "title": "CMU Multimodal SDK",
      "venue": "CMU Multimodal SDK"
    },
    {
      "citation_id": "74",
      "title": "Combating human trafficking with multimodal deep models",
      "authors": [
        "E Tong",
        "A Zadeh",
        "C Jones",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Combating human trafficking with multimodal deep models"
    }
  ]
}