{
  "paper_id": "2408.00780v1",
  "title": "In-Depth Analysis Of Emotion Recognition Through Knowledge-Based Large Language Models",
  "published": "2024-07-17T06:39:51Z",
  "authors": [
    "Bin Han",
    "Cleo Yau",
    "Su Lei",
    "Jonathan Gratch"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in social situations is a complex task that requires integrating information from both facial expressions and the situational context. While traditional approaches to automatic emotion recognition have focused on decontextualized signals, recent research emphasizes the importance of context in shaping emotion perceptions. This paper contributes to the emerging field of context-based emotion recognition by leveraging psychological theories of human emotion perception to inform the design of automated methods. We propose an approach that combines emotion recognition methods with Bayesian Cue Integration (BCI) to integrate emotion inferences from decontextualized facial expressions and contextual knowledge inferred via Large-language Models. We test this approach in the context of interpreting facial expressions during a social task, the prisoner's dilemma. Our results provide clear support for BCI across a range of automatic emotion recognition methods. The best automated method achieved results comparable to human observers, suggesting the potential for this approach to advance the field of affective computing. \n I. OVERVIEW In this work, we propose a general approach to context-dependent emotion recognition  [1] . This approach consists of three main steps. The first step is to predict the emotions that people are likely to perceive from an emotional expression without context (see Section II). The second step involves predicting the emotions that people are likely to perceive from a situational description. Finally, we combine these separate sources of information using psychologically-inspired models, such as Bayesian Cue Integration [2] (see Section III). We test this approach with an emotional social task called the prisoner's dilemma (game details and corpus are explained in  [1] ).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Emotion Probability Given By Face",
      "text": "This section focuses on emotion distribution estimated from facial cues alone. We compare three alternatives for automatically recognizing emotions from decontextualized videos.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Lstm Model Details",
      "text": "We train an LSTM model that incorporates dynamic features using human context-free annotations as the ground truth. The model utilizes various input features, including sequences of Action Units (AUs), facial optical flows, gaze, and head pose data. Detailed descriptions of the model architecture are provided in Table  I   We treat context-free human annotations as ground truth (human annotation results) when people judge emotions without situational context. Figure  3  shows the emotional probability distribution for three facial emotion recognition methods: FACET, EAC, and LSTM. FACET and EAC overestimate joy across all conditions. FACET and EAC do not recognize surprise well, which could significantly affect their performance (see Table  3  in  [1] ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Emotion Probability Given By Face And Context",
      "text": "We compare alternative methods for integrating facial and contextual cues for context-aware emotion recognition. We explore three methods: BCI, GPT-4, and NN.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Gpt Prompt For Integration",
      "text": "LLMs are known for their ability to understand various situational cues and perform well, especially in terms of emotional capability  [3] . We provided the LLM with situational knowledge and outcome to obtain emotional probabilities. This method utilizes the advanced capabilities of GPT-4 to directly generate a context-aware emotion probability distribution. The integration process involves GPT-4, which incorporates a representation of P (e|f ) estimated by a context-free facial emotion recognition method. Additionally, we crafted prompts to integrate both context and face cues to the LLM. We used GPT-4.  1  . The Figure  2  shows the GPT prompt. The prompt contains four main components:\n\n• A general description of the prisoner's dilemma game  [4] .\n\n• The game outcome of each turn (CC,DC,CD, and DD).\n\n• Representation of P (e|f ) estimated by a LSTM.\n\n• A request for the emotional distribution (Basic emotion).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Neural Network Integration",
      "text": "In this section, we discuss the Neural Network Integration (NNI) method. To improve integration, a simple neural network was trained using LSTM emotion distribution results (P (e|f )) and GPT-4 emotion distribution results (P (e|c)) as inputs, with context-based results (P (e|c, f )) as the ground truth. The NN architecture included a dense layer with 100 ReLU neurons and a softmax output layer. Using the Adam optimizer and a custom KL Divergence loss function, the model was trained over 1000 epochs. Cross-validation with 5 splits ensured robust evaluation. Table  II  shows the result of the comparison. To assess the performance of models, we employ three standard metrics. KLD  [5]  and RMSE are standard metrics to compare the distance between two probability distributions  [6] ,  [7] . Additionally, we adopt F1 (weighted) to evaluate performance if the model was forced to provide a single label. The NN's performance was somewhat worse than the BCI and GPT-4 integration results. It was only compared with the LSTM because both were fine-tuned on split-steal data, unlike the pre-trained models.  Fig.  3  illustrates the emotional distributions when integrating facial and contextual cue for emotion recognition. Methods integrating FACET and EAC show a propensity to overestimate joy in the CD. In addition, integrations with GPT-4 yield distributions that are more closely aligned with human context-based estimations than those with GPT-3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Face+Context (Integration",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Performance A. Evaluation By Context (Kld, Rmse, And F1-Score)",
      "text": "We assess the performance of knowledge-based emotion recognition against human context-based perception distributions. The LSTM+GPT-4 (BCI) shows robust performance across all contexts, mirroring the trend observed in human-to-human (BCI) assessments. Notably, most methods displayed weaker performance in CD and DD contexts, whereas LSTM+GPT-4 (BCI) and human-to-human (BCI) demonstrated relatively better performance in these contexts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overall",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Confusion Matrix",
      "text": "Fig.  4  presents the confusion matrix for facial emotion recognition methods using human context-free annotations as the ground truth. All three methods tend to predict emotions as Joy. Fig.  5  shows the confusion matrix for facial and contextual emotion recognition. When integrating context, the methods tend to predict a more diverse range of emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. How Integration Improves Performance?",
      "text": "We conduct an analysis to assess how BCI enhances recognition performance. Fig.  6  illustrates the variation in recognition performance according to different game outcomes.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: shows the emotional probability distribution for three facial emotion recognition methods: FACET,",
      "page": 2
    },
    {
      "caption": "Figure 1: Emotional probability distribution for P(e|f).",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the GPT prompt. The prompt contains four main components:",
      "page": 2
    },
    {
      "caption": "Figure 2: GPT Prompt for P(e|c, f): Integrating facial cue and the game outcome to predict Player A’s emotion.",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the emotional distributions when integrating facial and contextual cue for emotion recognition. Methods",
      "page": 3
    },
    {
      "caption": "Figure 3: Emotional probability distribution for P(e|c, f).",
      "page": 4
    },
    {
      "caption": "Figure 4: presents the confusion matrix for facial emotion recognition methods using human context-free annotations as the",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the confusion matrix for facial and contextual",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrix for facial emotion recognition. Human context-free is used as a ground truth (J stands for Joy, N for",
      "page": 5
    },
    {
      "caption": "Figure 5: Confusion matrix for facial and contextual emotion recognition. Human context-based is used as a ground truth.",
      "page": 6
    },
    {
      "caption": "Figure 6: illustrates the variation in recognition",
      "page": 6
    },
    {
      "caption": "Figure 6: Enhancement in model performance (RMSE and KLD) through facial and context integration.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face+Context\n(Integration)": "FACET+GPT-3 (BCI)\nFACET+GPT-4 (BCI)\nEAC+GPT-3 (BCI)\nEAC+GPT-4 (BCI)\nLSTM+GPT-3 (BCI)\nLSTM+GPT-4 (BCI)\nHuman+Human (BCI)\nFACET (GPT-4)\nEAC (GPT-4)\nLSTM (GPT-4)\nLSTM+GPT-4 (NNI)",
          "KLD(↓)\nRMSE(↓)\nF1(↑)": "1.713\n0.215\n0.525\n1.829\n0.200\n0.565\n1.340\n0.200\n0.519\n1.330\n0.210\n0.527\n0.809\n0.162\n0.454\n0.346\n0.104\n0.649\n0.092\n0.782\n0.441\n0.648\n0.150\n0.528\n0.597\n0.155\n0.503\n0.354\n0.112\n0.530\n0.580\n0.151\n0.151"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Overall": "1.713\n1.829\n1.341\n1.330\n0.809\n0.346\n0.441\n0.648\n0.597\n0.354",
          "CC\nDC\nCD\nDD": "1.301\n1.158\n2.407\n1.986\n1.147\n1.946\n1.948\n2.275\n1.152\n0.819\n2.024\n1.367\n0.886\n1.486\n1.495\n1.453\n0.394\n0.935\n0.385\n1.521\n0.222\n0.376\n0.389\n0.395\n0.235\n0.379\n0.59\n0.561\n0.549\n0.557\n0.663\n0.824\n0.491\n0.688\n0.621\n0.588\n0.179\n0.291\n0.538\n0.407"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Overall": "0.215\n0.2\n0.2\n0.21\n0.162\n0.104\n0.092\n0.15\n0.155\n0.112",
          "CC\nDC\nCD\nDD": "0.134\n0.189\n0.272\n0.267\n0.132\n0.175\n0.254\n0.238\n0.137\n0.163\n0.282\n0.218\n0.138\n0.194\n0.26\n0.251\n0.097\n0.241\n0.108\n0.202\n0.068\n0.113\n0.116\n0.117\n0.056\n0.088\n0.106\n0.117\n0.15\n0.14\n0.153\n0.159\n0.1484\n0.1626\n0.1518\n0.1587\n0.076\n0.098\n0.145\n0.131"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Overall": "0.525\n0.565\n0.519\n0.527\n0.454\n0.649\n0.782\n0.528\n0.503\n0.530",
          "CC\nDC\nCD\nDD": "0.882\n0.676\n0.333\n0.300\n0.861\n0.838\n0.375\n0.338\n0.861\n0.711\n0.324\n0.181\n0.861\n0.773\n0.323\n0.328\n0.882\n0.045\n0.390\n0.058\n0.861\n0.804\n0.192\n0.510\n0.954\n0.795\n0.753\n0.602\n0.861\n0.782\n0.343\n0.289\n0.861\n0.750\n0.333\n0.267\n0.861\n0.773\n0.142\n0.333"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Knowledge-based emotion recognition using large language models",
      "authors": [
        "B Han",
        "S Lei",
        "C Yau",
        "J Gratch"
      ],
      "year": "2024",
      "venue": "2024 12th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "2",
      "title": "Affective cognition: Exploring lay theories of emotion",
      "authors": [
        "D Ong",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2015",
      "venue": "Cognition"
    },
    {
      "citation_id": "3",
      "title": "Is gpt a computational model of emotion?",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "4",
      "title": "Prisoner's dilemma: A study in conflict and cooperation",
      "authors": [
        "A Rapoport",
        "A Chammah"
      ],
      "year": "1965",
      "venue": "Prisoner's dilemma: A study in conflict and cooperation"
    },
    {
      "citation_id": "5",
      "title": "On information and sufficiency",
      "authors": [
        "S Kullback",
        "R Leibler"
      ],
      "year": "1951",
      "venue": "The annals of mathematical statistics"
    },
    {
      "citation_id": "6",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Approximating discrete probability distribution of image emotions by multi-modal features fusion",
      "authors": [
        "S Zhao",
        "G Ding",
        "Y Gao",
        "J Han"
      ],
      "year": "2017",
      "venue": "Transfer"
    }
  ]
}