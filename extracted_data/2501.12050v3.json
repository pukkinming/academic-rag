{
  "paper_id": "2501.12050v3",
  "title": "Representation Learning With Parameterised Quantum Circuits For Advancing Speech Emotion Recognition",
  "published": "2025-01-21T11:23:38Z",
  "authors": [
    "Thejan Rajapakshe",
    "Rajib Rana",
    "Farina Riaz",
    "Sara Khalifa",
    "Björn W. Schuller"
  ],
  "keywords": [
    "quantum machine learning",
    "deep learning",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Quantum machine learning (QML) offers a promising avenue for advancing representation learning in complex signal domains. In this study, we investigate the use of parameterised quantum circuits (PQCs) for speech emotion recognition (SER)-a challenging task due to the subtle temporal variations and overlapping affective states in vocal signals. We propose a hybrid quantum-classical architecture that integrates PQCs into a conventional convolutional neural network (CNN), leveraging quantum properties such as superposition and entanglement to enrich emotional feature representations. Experimental evaluations on three benchmark datasets-IEMOCAP, RECOLA, and MSP-IMPROV-demonstrate that our hybrid model achieves improved classification performance relative to a purely classical CNN baseline, with over 50% reduction in trainable parameters. This work provides early evidence of the potential for QML to enhance emotion recognition and lays the foundation for future quantum-enabled affective computing systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Speech Emotion Recognition (SER) has experienced significant advancements through the adoption of representation learning, a paradigm where models automatically extract meaningful features directly from raw speech data. Deep learning has been instrumental in this shift, enabling the development of models capable of learning rich and task-specific representations, surpassing the limitations of traditional handengineered features. However, despite improving accuracy and adaptability, deep learning-based methods face challenges, including high computational demands and difficulties in capturing intricate relationships within high-dimensional feature spaces. Quantum machine learning (QML) offers a promising solution by leveraging quantum circuits to efficiently process and represent complex, high-dimensional data with fewer resources. This section explores recent advancements in representation learning for SER and examines how QML can address the computational and modelling challenges of classical deep learning approaches.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Representation Learning In Ser",
      "text": "Representation learning has emerged as a powerful paradigm for extracting meaningful features from speech data, enabling significant advancements in SER systems.\n\n1) Automatic Feature Extraction and Task Specificity: Conventional feature engineering requires prior domain knowledge to design features, which may not generalise well across datasets or tasks. Representation learning, in contrast, automates feature extraction by learning task-relevant features directly from raw audio signals. Trigeorgis et al. demonstrated that deep convolutional recurrent networks could extract hierarchical features from raw waveforms, outperforming traditional MFCC-based systems  [9] . This approach eliminates the need for manual feature design, reducing human intervention and improving adaptability to diverse datasets  [10] ,  [11] ,  [12] .\n\n2) Enhanced Representational Power: Deep learning architectures like Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks learn temporal and spectral dependencies, enhancing their ability to model subtle emotional cues  [13] ,  [14] . Satt et al.  [15]  found that deep models with spectrogram-based inputs significantly outperformed systems using hand-crafted features to detect speech emotions. Zhao et al.  [16]  introduced two CNN-LSTM architectures: one utilising 1D CNNs for raw audio input and the other employing 2D CNNs for Mel-spectrogram inputs. They reported achieving an accuracy of 52.14% on the IEMOCAP database for speaker-independent SER using the 2D CNN-LSTM model.\n\nFurther advancing classical methods, researchers have also explored complex architectures to address multiple SER challenges simultaneously. For instance, Daneshfar and Kabudian proposed a system using a hierarchical multi-layer sparse auto-encoder combined with an Extreme Learning Machine (ELM)  [2] . Their approach utilised a rich set of spectral and spectro-temporal features and notably introduced a new adaptive weighting method specifically to counteract the problem of data imbalance, a persistent challenge in SER datasets. This highlights a direction in classical SER research focused on creating sophisticated, multi-stage processing pipelines to extract discriminative features and build robust classifiers.\n\n3) Scalability Across Languages and Domains: Handcrafted features often need to be tailored for specific languages or domains, limiting their scalability. Representation learning, however, leverages large-scale pre-training to learn universal speech representations applicable across multiple tasks and languages. Huang et al.  [17]  highlighted that attention-based models trained on multilingual datasets generalise well, enabling SER systems to scale efficiently. Mirsamadi et al.  [18]  demonstrated that deep recurrent neural networks can effectively learn and aggregate frame-level acoustic features into compact utterance-level representations for improved emotion recognition. Additionally, they proposed a local attentionbased pooling strategy to focus on emotionally salient regions of speech, achieving superior performance on the IEMOCAP corpus compared to existing methods.\n\n4) Robustness to Limited Labelled Data: Representation learning, especially self-supervised approaches, reduces the reliance on labelled data, which is often scarce in SER. Models pre-trained on large unlabelled datasets, such as HuBERT  [19] , can be fine-tuned with limited labelled data, making them ideal for settings with sparse annotations. This adaptability is a significant advantage over traditional methods requiring extensive labelled data for optimal performance. 5) Challenges in Representation Learning for SER: Computational Demands and High-Dimensional Modelling: Representation learning has brought transformative advancements to SER, enabling the automatic extraction of taskrelevant features and outperforming traditional hand-crafted approaches  [20] ,  [21] . However, the underlying deep learning models are computationally intensive, leading to significant resource demands, especially during training. Moreover, the classical deep learning models face challenges in efficiently modelling complex correlations and entanglements in highdimensional feature spaces, which are critical for capturing subtle emotional cues in speech  [22] .\n\n6) QML addressing the Challenges in Representation Learning for SER : QML offers a promising alternative to address these issues. Quantum circuits, by leveraging principles of superposition and entanglement, have the potential to represent and process exponentially large feature spaces with fewer resources compared to classical systems  [23] . This capability could enable more efficient representation learning for SER, particularly in scenarios with limited labelled data or high-dimensional inputs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Quantum Machine Learning For Ser And Related Fields",
      "text": "Early QML research focused on foundational algorithms like the quantum support vector machine  [24] ,  [25]  and the Harrow-Hassidim-Lloyd (HHL) algorithm for solving linear systems  [25] . With the advent of noisy intermediate-scale quantum (NISQ) devices, practical quantum-enhanced models, such as PQCs, have become feasible  [26] . These advancements suggest that QML can potentially overcome the limitations of classical deep learning, particularly in modelling highdimensional data and reducing resource requirements.\n\n1) Parameterised Quantum Circuits (PQCs): QML has also shown promise in augmenting traditional neural network structures through Quantum Neural Networks (QNNs). By employing PQCs as neural layers, QNNs can encode quantum states while exploiting quantum advantages for specific tasks. Farhi and Neven pioneered quantum-enhanced learning frameworks for combinatorial optimisation  [8] , and Schuld and Killoran demonstrated how quantum circuits can efficiently process and generalise high-dimensional data  [27] , showcasing their suitability for complex learning tasks. While none of the above studies focused on SER, Qu et al. introduced a Quantum Federated Learning (QFL) algorithm for emotion recognition in speech. Their method emphasised privacy and robustness in noisy environments by employing quantum minimal gated unit RNNs  [28] . However, a key shortcoming of this work is that it did not focus on representation learning. By not leveraging the feature representation capabilities of PQCs, the model's ability to capture the complex, high-dimensional dependencies inherent in emotional speech data remained unexplored, limiting its potential effectiveness for the core SER task.\n\n2) Hybrid Classical-Quantum Architectures: Hybrid quantum-classical architectures have been explored to enhance model expressiveness and training efficiency. These architectures integrate the representational strengths of classical deep learning with the computational power of quantum systems  [29] ,  [30] . Hybrid models are particularly suited for tasks where classical models face limitations, as quantum layers can introduce novel features and expand the solution space. Other explorations into hybrid quantumclassical architectures have shown promise but have not yet unlocked a quantum advantage for SER. For instance, Thejha et al. applied a hybrid quantum-classical CNN to general speech recognition, achieving performance merely comparable to classical CNNs without demonstrating a clear benefit  [31] . Likewise, work by Esposito et al. on audio classification for cough detection  [32]  also failed to investigate representation learning for SER. These studies, while foundational, stopped short of demonstrating that QML could either outperform classical methods or effectively model the nuanced feature representations required for robust emotion recognition. Similarly, Norval and Wang proposed a Quantum AI-based approach for SER, but its practical viability was limited, achieving a low accuracy of only 30% on a custom dataset  [33] . Their approach, which relied on basic quantum encoding and variational algorithms without exploring PQCs or representation learning, underscores the significant performance challenges that early QML-SER models faced.\n\nAn area of significant interest is the application of hybrid architectures in CNNs, leading to the development of quantum CNNs (QCNNs). QCNNs effectively process structured data by utilising quantum circuits to perform convolution-like operations, as demonstrated by Cong et al.  [34] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Research Gap",
      "text": "CASIA  [36]  RAVDESS  [37]  EMO-DB  [38]  This Paper ✓ ✓ ✓ ✓\n\nIEMOCAP  [39]  RECOLA  [40]  MSP-Improv  [41]  This paper aims to investigate the of integration QML techniques, specifically PQCs and hybrid architectures, into representation learning frameworks to address these gaps and drive advancements in the field of SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Contextualising Quantum Machine Learning",
      "text": "FOR SPEECH EMOTION RECOGNITION In this study, we conduct a preliminary investigation to evaluate the applicability of Quantum Machine Learning (QML) with Parameterised Quantum Circuits (PQCs) for Speech Emotion Recognition (SER) tasks. By leveraging the quantum properties of superposition and entanglement, we hypothesise that QML can learn more complex and abstract representations from speech data, which are crucial for improving SER performance.\n\nA simplified architecture of the proposed hybrid Classical-Quantum model is illustrated in Figure  1 . In this model, the input spectrograms of speech audio data are first processed through a CNN-based representation learning block to generate classical representations. These representations are subsequently passed through a quantum representation learning block, where quantum operations are performed. Finally, the learnt quantum representations are fed into a classification block to predict the emotional state.\n\nThe Quantum Representation Learning block incorporates a quantum layer composed of three key components: the Quantum Embedding Component, the Quantum Circuit Layer, and the Quantum Measurement Component. This architecture allows the block to accept classical inputs, transform them into quantum states, and output classical representations, facilitating seamless representation transition in the hybrid classicalquantum model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Preamble",
      "text": "Quantum Embedding refers to the process of encoding classical speech features into quantum states within the quantum Hilbert space. This step enables classical audio features to interface with quantum systems and serves as the foundation for quantum machine learning algorithms  [27] . To achieve quantum embeddings, PQCs or predefined encoding schemes are employed to transform speech feature inputs into quantum states that can be manipulated by quantum gates.\n\nA Quantum Circuit Layer consists of a series of quantum gates that act on qubits to perform specific transformations or computations. Quantum circuits layers form the computational backbone of QNNs, enabling quantum systems to process information and learn. Two main types of circuits are utilised: Static Quantum Circuits -These circuits have fixed configurations with pre-specified gate sequences and no adjustable parameters. Static circuits typically perform predefined operations, such as feature extraction or classical-to-quantum encoding, without further tuning during the training process. Parameterised Quantum Circuits -PQCs include variable parameters -such as rotation angles in quantum gates -that are optimised iteratively during training  [26] . As the trainable core of QNNs, PQCs enable dynamic adaptation to input data by minimising a defined cost function.\n\nQuantum Measurement is the process of extracting classical information from quantum states by projecting them onto specific basis states  [25] . In QNNs, measurements occur at the end of the quantum circuit to produce outputs such as classifications, probabilities, or projections. The choice of measurement basis significantly influences the interpretability and effectiveness of the results. In this study, we employ the following quantum measurement methods: PauliZ, Z, PauliX, and Probability. The choice of measurement is critical as it determines the nature of the classical data passed to the subsequent layers of our model.\n\nPauliZ Measurement -calculates the expectation value of the Pauli Z operator, which is the average value of its eigenvalues (+1 for state |0⟩ and -1 for state |1⟩). This results in a continuous output value in the range [-1, +1]. This continuous output can be interpreted as a probabilistic score or a \"soft\" classification from the quantum circuit. For an SER task, this allows the model to express the degree to which an emotional feature is present, rather than making a hard decision. This nuanced output is then fed into the classical neural network  for further processing to reach a final classification  [42] .\n\nZ Measurement -is a measurement in the computational basis, which forces the qubit into a definite state of either |0⟩ or |1⟩. This process, known as state collapse, yields a classical binary outcome (a 0 or a 1)!  [43] . In the context of binary SER classification (e.g., 'High' vs. 'Low' valence), this provides a direct, decisive classification from the quantum layer. This binary output can then be used by the subsequent classical layers, potentially simplifying the final classification decision.\n\nThe fundamental difference lies in their output: PauliZ provides a continuous, probabilistic value, while Z provides a discrete, binary outcome. This distinction directly affects how the SER model interprets the quantum-processed features, with PauliZ offering a more nuanced, probabilistic representation and Z providing a more direct, binary classification.\n\nPauliX Measurement -involves projecting the qubit state onto the eigenstates of the Pauli X operator:\n\n2 (eigenvalue -1) This measurement essentially determines the \"flip basis\" (analogous to flipping the computational basis). The Mathematical representation of the PauliX measurement is as follows.\n\nProbability Measurement -In quantum mechanics, the probability of obtaining a specific measurement outcome is determined by the Born rule  [44] . If a quantum state |ψ⟩ is measured with respect to a specific basis (e.g., Z or X), the probability of observing a particular eigenstate |ϕ⟩ is given by Equation 1\n\nwhere |⟨ϕ|ψ⟩| is the overlap between the measured state and the eigenstate.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Model Architecture",
      "text": "Figure  2  illustrates the detailed architecture employed in this study. The model builds upon the foundational SER architecture proposed by Issa et al.  [45] . This architecture was selected for its simplicity, as it avoids the complexities introduced by components such as transformers, attention mechanisms, and LSTM networks. By focusing on a streamlined architecture, we aim to isolate and evaluate the specific contribution of the quantum layer to the SER task.\n\nWe acknowledge the limitations of this approach. The baseline CNN model does not incorporate more complex components like LSTM layers for temporal modeling or attention mechanisms, which have been shown to enhance SER performance by enabling the model to dynamically focus on the most emotionally salient segments of speech  [46] . This choice represents a critical trade-off: while a more sophisticated classical architecture could potentially yield higher overall accuracy, it would also introduce confounding variables, making it more difficult to isolate and interpret the specific impact of the quantum components. Our primary goal in this study is to establish a clear and foundational understanding of the quantum layer's capabilities. By benchmarking against a simpler, wellunderstood classical model, we can more directly attribute performance gains to the quantum representation learning, thereby balancing the pursuit of absolute performance with the need for clear, interpretable results.\n\nThe architecture consists of three primary blocks: a CNNbased classical representation learning block, a quantum representation learning block, and a classification block. The input spectrograms are initially processed by the CNN block, which extracts classical feature maps using convolutional operations. These intermediate representations are subsequently forwarded to the quantum representation learning block for further processing. To ensure compatibility between the classical and quantum components, a classical-to-quantum adaptor is introduced. This adaptor reshapes, flattens, and reformats the CNN output to match the input requirements of the quantum embedding layer.\n\nThe quantum representation learning block consists of three interconnected modules:\n\n1) Quantum Embedding Module: Converts the classical input feature maps into quantum embeddings using quantum embedding algorithms. This step ensures that the input data is represented as quantum states in the Hilbert space, enabling manipulation by quantum gates. 2) Quantum Circuit Layer (U): Utilises PQCs to process the quantum embeddings. PQCs include adjustable parameters (e.g., rotation angles) that are optimised during training to enhance the SER task. These circuits enable the model to learn complex and abstract representations by leveraging the unique properties of quantum systems such as superposition and entanglement. 3) Quantum Measurement Module: Extracts classical information from the processed quantum states by projecting them onto specific basis states. This step produces interpretable outputs, such as probabilities or projections, that are subsequently passed to the classification block. In this study, as stated, we employ four quantum measurement methods -PauliZ, PauliX, Probability, and Z -to project quantum states onto classical outcomes. These methods are critical for bridging the quantum-to-classical transition, enabling the integration of quantum features into the SER task.\n\nTo construct the quantum embedding module, we utilise three embedding algorithms -Angle Embedding, Amplitude Embedding, and IQP Embedding -as described in Havlíček et al.  [47] . These embeddings map input features into quantum vectors of n qubits (n = 8 in this study). The resulting quantum vectors are processed through the quantum circuit layer, which applies quantum rotations and other transformations. For this layer, we employ Strongly Entangling Layers  [48]  and Random Layers as circuit designs. These circuits facilitate rich entanglement among qubits, enhancing the model's capacity to capture intricate dependencies in the data.\n\nFigure  3  illustrates the quantum circuit of a strongly entangling layer with four qubits. This configuration exemplifies how entanglement is incorporated into the quantum layer to improve the model's ability to learn nuanced emotional features from speech data. The design of this four-qubit circuit, which uses sequential controlled-NOT (CNOT) gates to link the qubits, systematically builds up correlations between them. This robust entanglement is critical for the model's ability to represent the complex, interdependent relationships among the various acoustic features that characterise emotional speech.\n\nFig.  3 : Quantum Circuit Diagram of a Strongly Entangling Layer of four qubits IV. EXPERIMENTAL SETUP In this section, we outline the datasets, input features, and model configurations used in this study. The com-plete Python codebase for the experiments and data preprocessing is publicly accessible on GitHub under the repository QuantumMachineLearning-for-SER 1  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "To robustly evaluate the proposed hybrid classical-quantum model, we selected three benchmark datasets commonly used in the SER field: IEMOCAP  [39] , MSP-Improv  [41] , and RECOLA  [40]  1) IEMOCAP: is a widely used dataset containing 12 hours of audio-visual data from dyadic interactions between actors. The sessions include both scripted and improvised scenarios designed to elicit specific emotions. While the emotions are acted, the improvisational nature of many interactions introduces a degree of spontaneity not found in purely scripted datasets. By including IEMOCAP, we test our model's performance on more archetypal and clearly expressed emotions, which serves as a crucial baseline for SER tasks. In this study, we focus on the audio from the improvised sessions and use the valence annotations, which range from 1 to 5, to evaluate the model's ability to capture nuanced emotional states.\n\n2) MSP-Improv: is a valuable resource designed to bridge the gap between acted and naturalistic data. It features actors improvising scenarios that are specifically designed to elicit genuine emotional responses in a controlled setting. This methodology aims to produce more realistic emotional expressions than simple script-reading, providing a unique test case for our model. The dataset contains 20 predetermined scripts, allowing for the study of key emotions like happiness, sadness, anger, and neutrality. Using MSP-Improv allows us to evaluate our model's effectiveness on lexically controlled but emotionally varied speech.\n\n3) RECOLA: consists of recordings of spontaneous, naturalistic interactions between participants collaborating on a task remotely. The emotional expressions in RECOLA are not acted or elicited, but arise naturally from the interaction, making them more subtle and representative of real-world conversational speech. This presents a significant challenge for any SER model. By evaluating on RECOLA's valence annotations, we specifically test our model's capacity to recognise emotions in spontaneous and unscripted contexts, which is a key objective for advancing the field of SER.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Input Features",
      "text": "Mel-Frequency Cepstral Coefficients (MFCCs) are commonly used as input features in SER studies  [49] ,  [50] ,  [51] ,  [52] . While MFCCs capture the spectral characteristics of an audio signal, they do not offer a direct visual representation of the signal's structure. In contrast, Mel-spectrograms provide a two-dimensional visual representation that more intuitively reflects the temporal and spectral patterns of the audio. For this preliminary study, the interpretability of input features in relation to the audio signal is crucial; hence, Mel-spectrograms were chosen as the input features for our model.\n\nTo ensure uniformity across the dataset, each audio utterance was resampled to a sampling rate of 22 kHz and standardised to a duration of 3 seconds, either by truncating or zero-padding as necessary. Mel-spectrograms were then extracted using a window size of 2048 samples and a hop size of 512 samples, resulting in spectrograms with consistent dimensions for input to the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Model Configuration",
      "text": "Hyperparameters such as Quantum Measurement methods, Quantum Embedding algorithms, and Quantum Circuit Layers were tuned alongside traditional training parameters, including the optimiser, learning rate, and weight decay. To identify the optimal combination of these hyperparameters, we employed a grid search approach  [53] . A comprehensive summary of the hyperparameters and their respective values used in the grid search process is provided in Table  II . The models were implemented using the PyTorch framework, with the quantum components simulated using the Pen-nyLane library. All training and evaluation experiments were conducted on a high-performance computing node equipped with an NVIDIA A100 40GB GPU, a 60-core CPU, and 120GB of RAM. Given the extensive hyperparameter space, the grid search was computationally intensive, requiring significant runtime to systematically evaluate all possible configurations and identify the optimal model for each experiment.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Baseline Classical Model",
      "text": "To ensure a fair and rigorous comparison, we performed parallel experiments using a purely classical deep learning approach. The classical DNN model was designed to closely mirror the architecture of the hybrid classical-quantum model, with the key distinction being the exclusion of the 'Quantum Representation Learning Block'. The architecture of the classical model used in this study is illustrated in Figure  4 . This experimental setup facilitates a direct comparison between the hybrid classical-quantum approach and traditional deep learning methods, allowing us to isolate and evaluate the impact of the quantum components on model performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Evaluation Metrics",
      "text": "We evaluate model performance using the Unweighted Average Recall (UAR, %), a widely adopted metric in speechbased machine learning research  [54] ,  [55] ,  [56] ,  [57] . UAR is",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Experiments",
      "text": "We designed our experiments to address two primary scenarios: binary classification and multi-class emotion classification.\n\n• Binary Classification: The objective of this task was to classify input speech samples into one of two categories: \"High\" or \"Low\" valence. For this evaluation, we utilised the IEMOCAP and RECOLA datasets, which are wellsuited for valence-based classification. • Multi-Class Emotion Classification: This task aimed to identify the specific emotion conveyed in the input speech, selecting from the categories: happy, angry, sad, or neutral. For this purpose, we employed the IEMOCAP and MSP-Improv datasets, which offer comprehensive annotations for a diverse range of emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Evaluation",
      "text": "This section presents the results of the experiments conducted in our study. The results of the binary classification are detailed in subsection V-A, while those of the multi-class classification are provided in subsection V-B. Performance is evaluated using UAR%, while the number of trainable parameters is reported to indicate the relative complexity of each model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Binary Classification",
      "text": "In this section, we evaluate the binary classification performance of the hybrid classical-quantum models within the SER domain. The experiments were conducted using the IEMO-CAP and RECOLA datasets. The objective was to classify speech emotion into one of two dimensional labels: Valence -High or Low.\n\n1) IEMOCAP: The IEMOCAP dataset is annotated with both categorical and dimensional emotional labels. For the valence dimension, annotations range from 1 to 5. We categorised valence as \"Low\" for values less than 3 and \"High\" for values 3 and above.   Analysing the UAR% results, the hybrid classical-quantum model outperforms the identical classical DNN model. This suggests that the inclusion of the \"Quantum representation learning block\" contributes significantly to the observed performance improvement  [31] ,  [32] .\n\nAlso, the hybrid classical-quantum model has nearly as half of the number of parameters which indicates the reduced training complexity of the model.\n\n2) RECOLA: The RECOLA dataset is an annotated resource containing multiple markers for emotion recognition. For this binary classification task in SER, we focused on the valence dimension. Valence in the RECOLA dataset is annotated with positive and negative values. We categorised negative valence values as \"Low\" and positive valence values as \"High\".\n\nFigure  5 (b) presents the UAR% and the number of trainable parameters for each hybrid classical-quantum model and its corresponding classical model. The best-performing quantum model, selected via grid search, is configured with the following hyper-parameters:\n\n• Learning Rate: 0.00001 • Optimiser: Stochastic Gradient Descent (SGD) with zero weight decay\n\n• Quantum Layer: Amplitude Embedding, Strongly Entangling Layers for the Quantum Circuit, and PauliX for Quantum Measurement From the results, the hybrid classical-quantum model demonstrates superior performance compared to its classical counterpart, achieving this with nearly half the number of trainable parameters.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Multi-Class Emotion Classification",
      "text": "In this section, we assess the multi-class emotion classification performance of the hybrid classical-quantum models in the SER domain. The experiments were conducted using the IEMOCAP and MSP-Improv datasets. The goal was to classify speech emotions into one of four categorical labels: Angry, Happy, Neutral, and Sad.\n\n1) IEMOCAP: In this scenario, we utilise the categorical annotations of the IEMOCAP dataset.\n\nFigure  5 (c) presents the performance comparison between the best quantum model and its classical counterpart.\n\nWhen comparing Figure  5 (a) and Figure  5 (c), it is evident that the UAR% for the multi-class classification task is lower. This decline in performance can be attributed to the increased complexity of the SER task and the higher number of output classes. However, the quantum model demonstrates higher accuracy compared to the corresponding classical model in this multi-class scenario.\n\nThe selected model employs the following hyperparameters:\n\n• Learning Rate: 0.001 • Optimiser: Stochastic Gradient Descent (SGD) with zero weight decay • Quantum Layer: Angle embedding, random layers as the quantum circuit, and a combination of Z and PauliZ measurements for quantum measurement When compared to the binary classification experiments on the IEMOCAP dataset, the same quantum layer configuration is observed for both binary and multi-class scenarios. This suggests that the parameters of the quantum layer should be tailored to the underlying data distribution.\n\n2) MSP-Improv: We use four categorical emotions (Angry, Happy, Neutral, and Sad) annotated in the MSP-Improve dataset. The task of this classification model is to correctly classify the emotion embedded in the audio.\n\nFigure  5 (d) compares the performance and the complexity of the best quantum model selected by the grid search and the corresponding classical model. The hyper-parameters selected by grid search for the best performing quantum model are:\n\n• Learning Rate: 0.0001 • Optimiser: AdaGrad optimiser with zero weight decay • Quantum Layer: Angle embedding, random layers as the quantum circuit, and PauliZ measurements for quantum measurement",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Summary Of The Results",
      "text": "A summary of the results obtained by the experiments in this study is tabulated in the Table  III . The results demonstrate the effectiveness of the hybrid classical-quantum model for SER tasks, highlighting superior performance and reduced complexity compared to classical models. For binary classification on the IEMOCAP dataset, the hybrid model achieved a UAR of 64.68%, outperforming the classical model's 61.36%, with 50.34% fewer trainable parameters.\n\nSimilarly, for the RECOLA dataset, the hybrid model recorded a UAR of 80.85%, considerably higher than the classical model's 74.42%, while maintaining the same parameter efficiency. In multi-class classification, the hybrid model also outperformed its classical counterpart, achieving a UAR of 55.93% on the IEMOCAP dataset (versus 52.54%) and 34.60% on MSP-Improv (versus 32.78%), again with reduced complexity. Across all tasks, optimal configurations for the hybrid model consistently included zero weight decay and specific quantum embeddings, such as angle or amplitude embedding, paired with random or strongly entangling quantum layers. These results highlight the potential of integrating quantum feature extraction into SER models, offering improved accuracy and efficiency in handling complex emotional data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Discussion And Future Work",
      "text": "The results of the study highlight how well the proposed hybrid classical-quantum framework can handle the challenges that come with SER. The work shows that combining PQCs with a traditional CNN substantially decreases model complexity while improving classification performance on both binary and multi-class problems.\n\nFor binary classification tasks, the hybrid models demonstrated higher accuracies on both the IEMOCAP and RECOLA datasets, consistently outperforming their classical counterparts in terms of UAR(%). The quantum layer's potential to utilise entanglement and superposition, which enhance feature representation and capture subtle relationships among speech components, contributes for these improvements. Furthermore, the hybrid approach's effectiveness is demonstrated by a reduction in the number of trainable parameters, which makes it an intriguing option for situations with limited resources.\n\nThe hybrid framework had slightly lower UAR(%) values than binary tasks, but it still performed better for multi-class classification. The higher complexity involved in identifying more subtle emotional states is consistent with this outcome. The quantum-enhanced model proved stable in spite of this difficulty, especially when tested on the MSP-Improv dataset, which includes a variety of emotional expressions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Key Observations",
      "text": "Key observations observed throughout this study are;\n\n• The experiments reveal that quantum embeddings and circuit configurations (e.g., Angle Embedding and Strongly Entangling Layers) are critical to achieving optimal performance. The observed consistency in selected quantum components across binary and multi-class tasks suggests a potential universality of these configurations for SER tasks. • The absence of weight decay in the best-performing models indicates that quantum layers inherently provide sufficient regularisation. • The hybrid models' reduced parameter count underscores their suitability for deployment in real-world scenarios where computational resources and energy efficiency are critical constraints.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Lessons Learnt",
      "text": "The development of the hybrid classical-quantum framework presented in this paper involved a significant exploration of different architectural and methodological approaches. Before arriving at the final design, several alternative strategies were implemented and evaluated, providing valuable insights into the challenges and opportunities of integrating QML with SER. These explorations highlighted the complexities of leveraging quantum properties effectively and guided the iterative refinement of the model towards improved performance and efficiency.\n\nOur initial investigations focused on incorporating static quantum circuits within established deep learning architectures. We hypothesised that these circuits, despite their fixed structure, could offer advantages in feature extraction due to their ability to exploit quantum phenomena like superposition. A variety of circuit configurations, including those based on 2-CNOT, 3-CNOT, and 4-CNOT gates (Figure  6 ), were designed and tested. The output of these static quantum circuits, essentially quantum measurements representing extracted features, were then passed as input to various classical deep learning models. These included standard CNNs, LSTMs for capturing temporal dependencies, multi-kernel CNNs to explore different receptive fields, and combinations of CNNs and LSTMs (Figure  7  (a)). However, across these various architectures and circuit designs, the performance on benchmark datasets like IEMOCAP remained consistently below 49% accuracy. This suggested that the static nature of these circuits limited their ability to adapt to the nuances and complexities of emotional expression in speech data.\n\nSubsequently, we explored the potential of fusion-based models, aiming to combine the strengths of both quantum and classical processing. Two primary fusion strategies were investigated: early fusion and decision-level fusion. In the early fusion approach, the outputs of the static quantum circuits and the classical CNNs were concatenated before being passed to a final classification layer (Figure  7 (b) ). This aimed to leverage both quantum and classical features in the decision-making process. The decision-level fusion approach, on the other hand, involved training separate quantum and classical models, and their respective classification outputs were then combined using techniques like averaging or weighted averaging. Despite the conceptual appeal of these fusion strategies, the observed performance gains were marginal, with accuracies hovering around 44% on the IEMOCAP dataset. This suggested that simply combining static quantum computations with classical deep learning was insufficient to capture the intricate relationships within emotional speech data.\n\nFurther experimentation delved into the role of regularisation within these hybrid models. Specifically, we investigated the impact of L2 regularisation, commonly used to prevent over-fitting in classical deep learning. We observed a clear trend: increasing the weight decay (the hyperparameter controlling L2 regularisation) improved the performance of the classical CNN components. However, the same trend was not observed for the quantum components, suggesting that they might possess inherent regularisation properties due to the constraints of the quantum Hilbert space.\n\nThe culmination of these explorations led to the realisation that the fixed nature of static quantum circuits was a limiting factor in achieving optimal performance. This prompted the shift towards PQCs, which allow for adaptive learning through the optimisation of circuit parameters. The flexibility of PQCs, combined with carefully chosen quantum embeddings and circuit architectures, proved to be the key to unlocking the potential of QML for SER, as demonstrated by the improved results presented in this study. This journey emphasises the significance of systematic exploration and iterative refinement in the nascent field of QML, laying the groundwork for future research to further optimise and expand upon these promising initial discoveries.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Limitations Of The Proposed Model",
      "text": "While the hybrid framework offers promising results, several limitations merit discussion. First, the study relies on simulated quantum environments, which may not fully capture the hardware-related challenges and noise associated with physical quantum systems. Future work should validate these findings on real quantum devices to assess their practical applicability.\n\nAdditionally, the current approach uses fixed configurations for the classical CNN component. Exploring alternative architectures, such as transformer-based models, could further enhance performance by leveraging their strengths in capturing long-term dependencies.\n\nFinally, the scope of datasets used in this study, though diverse, does not encompass all linguistic and cultural variations in emotional speech. Expanding the evaluation to include",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This paper has introduced a novel hybrid classical-quantum framework for Speech Emotion Recognition (SER), leveraging the power of Parameterised Quantum Circuits (PQCs) integrated within a Convolutional Neural Network (CNN) architecture. By harnessing the principles of quantum superposition and entanglement, the proposed model demonstrates a significant improvement in both accuracy and model efficiency.\n\nThe experimental results across three widely used benchmark datasets -IEMOCAP, RECOLA, and MSP-Improvconsistently showcase the superior performance of the hybrid classical-quantum model compared to its purely classical counterpart. Notably, the hybrid models achieve enhanced accuracy in both binary and multi-class emotion classification tasks, while simultaneously reducing the number of trainable parameters by 50.34%. This reduction in model complexity translates to lower computational overhead and improved energy efficiency, rendering these quantum-enhanced methods more suitable for practical real-world applications.\n\nFurthermore, the study reveals crucial insights into optimal configurations for hybrid classical-quantum models in SER. The experiments suggest that specific quantum embeddings, such as Angle and Amplitude embedding, coupled with strongly entangling or random quantum circuit layers, and a summation of Pauli Z measurements consistently contribute to superior performance. The absence of weight decay in these optimal configurations implies that L2 regularisation might not be necessary for such hybrid models. Notably, the optimal configurations consistently emerge with zero weight decay. This directly supports the observation from Section VI-A, which indicates that the quantum layers inherently provide sufficient regularisation. This behaviour suggests that classical techniques like L2 regularisation may be redundant, potentially due to the intrinsic properties and constraints of the quantum Hilbert space, simplifying the training process. These findings provide valuable guidance for future research and development in quantum-enhanced SER.\n\nWhile these results are promising, the study also acknowledges certain limitations. Firstly, the experiments were performed using simulated quantum environments. Future studies should validate these findings on physical quantum devices. Secondly, the current implementation uses a fixed architecture for the classical CNN component; exploring alternative architectures could be an avenue for further improvement. Finally, the dataset diversity should be expanded in future studies to better understand the models' generalisation capacity across various linguistic and cultural contexts.\n\nIn summary, this study demonstrates the potential of quantum machine learning to revolutionise the field of SER. By integrating PQCs with classical CNNs, our proposed framework achieves improved accuracy and reduced complexity, paving the way for the development of more robust and efficient SER models for real-world applications. The future work includes extending these experiments on a quantum hardware and implementing more robust feature extraction methods for the CNN model.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: By combining the expressive power of quantum cir-",
      "page": 1
    },
    {
      "caption": "Figure 1: A simplified architecture of the proposed Hybrid",
      "page": 2
    },
    {
      "caption": "Figure 1: In this model, the",
      "page": 4
    },
    {
      "caption": "Figure 2: Overview of the model architecture used in this study. Input features pass through a CNN representation learning",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the detailed architecture employed in this",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the quantum circuit of a strongly entan-",
      "page": 6
    },
    {
      "caption": "Figure 3: Quantum Circuit Diagram of a Strongly Entangling",
      "page": 6
    },
    {
      "caption": "Figure 4: Architecture of the classical model used in this study.",
      "page": 7
    },
    {
      "caption": "Figure 5: Comparison of UAR (%) and Number of Parameters with Hybrid classical-quantum model and Classical Model in the",
      "page": 8
    },
    {
      "caption": "Figure 5: (a) illustrates the UAR% and the number of",
      "page": 8
    },
    {
      "caption": "Figure 5: (b) presents the UAR% and the number of trainable",
      "page": 8
    },
    {
      "caption": "Figure 5: (c) presents the performance comparison between",
      "page": 9
    },
    {
      "caption": "Figure 5: (a) and Figure 5(c), it is evident",
      "page": 9
    },
    {
      "caption": "Figure 5: (d) compares the performance and the complexity",
      "page": 9
    },
    {
      "caption": "Figure 6: 2-CNOT, 3-CNOT, and 4-CNOT quantum circuits we",
      "page": 10
    },
    {
      "caption": "Figure 6: ), were designed",
      "page": 10
    },
    {
      "caption": "Figure 7: (a)). However, across these various architectures and",
      "page": 10
    },
    {
      "caption": "Figure 7: (b)). This aimed to leverage",
      "page": 10
    },
    {
      "caption": "Figure 7: Two of the model architectures employed in our experiments involving static quantum circuits.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Research Study": "",
          "Focus": "Hybrid Classical-\nQ. Model",
          "Dataset": ""
        },
        {
          "Research Study": "Esposito\net\nal.\n2022 [32]",
          "Focus": "✓",
          "Dataset": "DiCOVA [35]"
        },
        {
          "Research Study": "Thejha\net\nal.\n2023 [31]",
          "Focus": "✓",
          "Dataset": "Not Available"
        },
        {
          "Research Study": "Norval & Wang\n2024 [33]",
          "Focus": "✗",
          "Dataset": "Custom Dataset"
        },
        {
          "Research Study": "Qu\net\nal.\n2024 [28]",
          "Focus": "✗",
          "Dataset": "CASIA [36]\nRAVDESS [37]\nEMO-DB [38]"
        },
        {
          "Research Study": "This Paper",
          "Focus": "✓",
          "Dataset": "IEMOCAP [39]\nRECOLA [40]\nMSP-Improv [41]"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hyper Parameter": "Learning Rate",
          "Values": "0.001, 0.001, 0.00001"
        },
        {
          "Hyper Parameter": "Optimiser",
          "Values": "Adam,\nSGD, RMSProp, AdaDelta, Ada-\nGrad"
        },
        {
          "Hyper Parameter": "Weight Decay",
          "Values": "0, 0.01, 0.001"
        },
        {
          "Hyper Parameter": "Q.Embedding",
          "Values": "Angle Embedding, Amplitude Embedding,\nIQP Embedding"
        },
        {
          "Hyper Parameter": "Q.Circuit Layer",
          "Values": "Random Layers, Strongly Entangling Lay-\ners"
        },
        {
          "Hyper Parameter": "Q.Measurements",
          "Values": "PauliZ, PauliX, Z + PauliZ, Probability"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hyper Parameters": "Learning Rate\nOptimiser\nWeight Decay\nQ.Embedding\nQ.Circuit Layer\nQ.Measurement",
          "Dataset\nIEMOCAP - Binary\nRECOLA - Binary\nIEMOCAP - 4 Class\nMSP-Improv - 4 Class": "0.00001\n0.00001\n0.001\n0.0001\nAdam\nSGD\nSGD\nAdaGrad\n0\n0\n0\n0\nAngle Embedding\nAmplitude Embedding\nAngle Embedding\nAngle Embedding\nRandom Layers\nStrongly Entangling Layers\nRandom Layers\nRandom Layers\nZ + PauliZ\nPauliX\nZ + PauliZ\nPauliZ"
        },
        {
          "Hyper Parameters": "UAR (%)\n- Quantum\nUAR (%)\n- Classical",
          "Dataset\nIEMOCAP - Binary\nRECOLA - Binary\nIEMOCAP - 4 Class\nMSP-Improv - 4 Class": "64.68 ± 3.34\n80.85 ± 4.45\n55.93 ± 4.62\n34.60 ± 5.19\n61.36 ± 3.21\n74.42 ± 5.28\n52.54 ± 7.56\n32.78 ± 4.32"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of speech emotion recognition in natural environment",
      "authors": [
        "M Shah Fahad",
        "A Ranjan",
        "J Yadav",
        "A Deepak"
      ],
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Speech Emotion Recognition Using Multi-Layer Sparse Auto-Encoder Extreme Learning Machine and Spectral/Spectro-Temporal Features with New Weighting Method for Data Imbalance",
      "authors": [
        "F Daneshfar",
        "S Kabudian"
      ],
      "year": "2021",
      "venue": "ICCKE 2021 -11th International Conference on Computer Engineering and Knowledge"
    },
    {
      "citation_id": "3",
      "title": "A review on speech emotion recognition: A survey, recent advances, challenges, and the influence of noise",
      "authors": [
        "S George",
        "P Ilyas"
      ],
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "4",
      "title": "The role of entanglement for enhancing the efficiency of quantum kernels towards classification",
      "authors": [
        "D Sharma",
        "P Singh",
        "A Kumar"
      ],
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "5",
      "title": "Transition role of entangled data in quantum machine learning",
      "authors": [
        "X Wang",
        "Y Du",
        "Z Tu",
        "Y Luo",
        "X Yuan",
        "D Tao"
      ],
      "year": "2024",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "6",
      "title": "Exploring the Power of Entangled Data in Quantum Machine Learning",
      "authors": [
        "X Wang",
        "Y Du",
        "Z Tu",
        "Y Luo",
        "X Yuan",
        "D Tao"
      ],
      "venue": "Wuhan University Journal of Natural Sciences"
    },
    {
      "citation_id": "7",
      "title": "Entanglementenhanced Quantum Reinforcement Learning: an Application using Single-Photons",
      "authors": [
        "J Gaspar",
        "A Bergerault",
        "V Apostolou",
        "A Ricou"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Quantum Computing and Engineering (QCE)"
    },
    {
      "citation_id": "8",
      "title": "Classification with Quantum Neural Networks on Near Term Processors",
      "authors": [
        "E Farhi",
        "H Neven"
      ],
      "year": "2018",
      "venue": "Classification with Quantum Neural Networks on Near Term Processors"
    },
    {
      "citation_id": "9",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics"
    },
    {
      "citation_id": "10",
      "title": "Real time speech emotion recognition using RGB image classification and transfer learning",
      "authors": [
        "M Stolar",
        "M Lech",
        "R Bolia",
        "M Skinner"
      ],
      "year": "2017",
      "venue": "11th International Conference on Signal Processing and Communication Systems"
    },
    {
      "citation_id": "11",
      "title": "GFRN-SEA: Global-Aware Feature Representation Network for Speech Emotion Analysis",
      "authors": [
        "L Pan",
        "Q Wang"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Multimodal Emotion Recognition from Raw Audio with Sinc-convolution",
      "authors": [
        "X Zhang",
        "W Fu",
        "M Liang"
      ],
      "year": "2024",
      "venue": "Multimodal Emotion Recognition from Raw Audio with Sinc-convolution"
    },
    {
      "citation_id": "13",
      "title": "CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "14",
      "title": "An ensemble 1D-CNN-LSTM-GRU model with data augmentation for speech emotion recognition",
      "authors": [
        "M Ahmed",
        "S Islam",
        "A Islam",
        "S Shatabda"
      ],
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "15",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "17",
      "title": "Speech Emotion Classification Using Attention-Based LSTM",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings"
    },
    {
      "citation_id": "19",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2021"
    },
    {
      "citation_id": "20",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "Y Bu",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "IJCAI International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Survey of Deep Representation Learning for Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Deep learning approaches for speech emotion recognition: state of the art and research challenges",
      "authors": [
        "R Jahangir",
        "Y Teh",
        "F Hanif",
        "G Mujtaba"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-020-09874-7"
    },
    {
      "citation_id": "23",
      "title": "Quantum convolutional neural network based on variational quantum circuits",
      "authors": [
        "L Gong",
        "J Pei",
        "T Zhang",
        "N Zhou"
      ],
      "venue": "Optics Communications"
    },
    {
      "citation_id": "24",
      "title": "Quantum Support Vector Machine for Big Data Classification",
      "authors": [
        "P Rebentrost",
        "M Mohseni",
        "S Lloyd"
      ],
      "venue": "Physical Review Letters"
    },
    {
      "citation_id": "25",
      "title": "Quantum machine learning",
      "authors": [
        "J Biamonte",
        "P Wittek",
        "N Pancotti",
        "P Rebentrost",
        "N Wiebe",
        "S Lloyd"
      ],
      "year": "2017",
      "venue": "Nature"
    },
    {
      "citation_id": "26",
      "title": "Variational quantum algorithms",
      "authors": [
        "M Cerezo",
        "A Arrasmith",
        "R Babbush",
        "S Benjamin",
        "S Endo",
        "K Fujii",
        "J Mcclean",
        "K Mitarai",
        "X Yuan",
        "L Cincio",
        "P Coles"
      ],
      "year": "2021",
      "venue": "Nature Reviews Physics"
    },
    {
      "citation_id": "27",
      "title": "Quantum Machine Learning in Feature Hilbert Spaces",
      "authors": [
        "M Schuld",
        "N Killoran"
      ],
      "venue": "Physical Review Letters"
    },
    {
      "citation_id": "28",
      "title": "QFSM: A Novel Quantum Federated Learning Algorithm for Speech Emotion Recognition With Minimal Gated Unit in 5G IoV",
      "authors": [
        "Z Qu",
        "Z Chen",
        "S Dehdashti",
        "P Tiwari"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Intelligent Vehicles",
      "doi": "10.1109/TIV.2024.3370398"
    },
    {
      "citation_id": "29",
      "title": "Hybrid Quantum-Classical Convolutional Neural Networks",
      "authors": [
        "J Liu",
        "K Lim",
        "K Wood",
        "W Huang",
        "C Guo",
        "H.-L Huang"
      ],
      "year": "2019",
      "venue": "Science China: Physics, Mechanics and Astronomy",
      "doi": "10.1007/s11433-021-1734-3"
    },
    {
      "citation_id": "30",
      "title": "Quantum classical hybrid convolutional neural networks for breast cancer diagnosis",
      "authors": [
        "Q Xiang",
        "D Li",
        "Z Hu",
        "Y Yuan",
        "Y Sun",
        "Y Zhu",
        "Y Fu",
        "Y Jiang",
        "X Hua"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "31",
      "title": "Speech Recognition Using Quantum Convolutional Neural Network",
      "authors": [
        "B Thejha",
        "S Yogeswari",
        "A Vishalli",
        "J Jeyalakshmi"
      ],
      "year": "2023",
      "venue": "Proceedings of 8th IEEE International Conference on Science, Technology, Engineering and Mathematics"
    },
    {
      "citation_id": "32",
      "title": "Quantum Machine Learning for Audio Classification with Applications to Healthcare",
      "authors": [
        "M Esposito",
        "G Uehara",
        "A Spanias"
      ],
      "year": "2022",
      "venue": "13th International Conference on Information, Intelligence, Systems and Applications"
    },
    {
      "citation_id": "33",
      "title": "Quantum AI in Speech Emotion Recognition",
      "authors": [
        "M Norval",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "PREPRINT (Version 1)"
    },
    {
      "citation_id": "34",
      "title": "Quantum convolutional neural networks",
      "authors": [
        "I Cong",
        "S Choi",
        "M Lukin"
      ],
      "year": "2019",
      "venue": "Nature Physics"
    },
    {
      "citation_id": "35",
      "title": "DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics",
      "authors": [
        "A Muguli",
        "L Pinto",
        "R Nirmala",
        "N Sharma",
        "P Krishnan",
        "P Ghoshy",
        "R Kumar",
        "S Bhat",
        "S Chetupalli",
        "S Ganapathy",
        "S Ramoji",
        "V Nanda"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "36",
      "title": "Design of Speech Corpus for Mandarin Text to Speech",
      "authors": [
        "J Tao",
        "F Liu",
        "M Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "Design of Speech Corpus for Mandarin Text to Speech"
    },
    {
      "citation_id": "37",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "38",
      "title": "A Database of German Emotional Speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A Database of German Emotional Speech"
    },
    {
      "citation_id": "39",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "40",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "41",
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "A Multi-Classification Hybrid Quantum Neural Network Using an All-Qubit Multi-Observable Measurement Strategy",
      "authors": [
        "Y Zeng",
        "H Wang",
        "J He",
        "Q Huang",
        "S Chang"
      ],
      "venue": "Entropy"
    },
    {
      "citation_id": "43",
      "title": "QuaLITi: Quantum Machine Learning Hardware Selection for Inferencing with Top-Tier Performance",
      "authors": [
        "K Phalak",
        "S Ghosh"
      ],
      "year": "2024",
      "venue": "QuaLITi: Quantum Machine Learning Hardware Selection for Inferencing with Top-Tier Performance"
    },
    {
      "citation_id": "44",
      "title": "Zur Quantenmechanik der Stoßvorgänge",
      "authors": [
        "M Born"
      ],
      "year": "1926",
      "venue": "Zeitschrift für Physik",
      "doi": "10.1007/BF01397477"
    },
    {
      "citation_id": "45",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "46",
      "title": "The Impact of Attention Mechanisms on Speech Emotion Recognition",
      "authors": [
        "S Chen",
        "M Zhang",
        "X Yang",
        "Z Zhao",
        "T Zou",
        "X Sun"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "47",
      "title": "Supervised learning with quantum-enhanced feature spaces",
      "authors": [
        "V Havlíček",
        "A Córcoles",
        "K Temme",
        "A Harrow",
        "A Kandala",
        "J Chow",
        "J Gambetta"
      ],
      "year": "2019",
      "venue": "Nature"
    },
    {
      "citation_id": "48",
      "title": "Circuit-centric quantum classifiers",
      "authors": [
        "M Schuld",
        "A Bocharov",
        "K Svore",
        "N Wiebe"
      ],
      "venue": "Physical Review A"
    },
    {
      "citation_id": "49",
      "title": "Speech based human emotion recognition using MFCC",
      "authors": [
        "M Likitha",
        "S Gupta",
        "K Hasitha",
        "A Raju"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 International Conference on Wireless Communications, Signal Processing and Networking"
    },
    {
      "citation_id": "50",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "51",
      "title": "Speech Emotion Recognition using MFCC, GFCC, Chromagram and RMSE features",
      "authors": [
        "H Patni",
        "A Jagtap",
        "V Bhoyar",
        "A Gupta"
      ],
      "year": "2021",
      "venue": "Proceedings of the 8th International Conference on Signal Processing and Integrated Networks, SPIN 2021"
    },
    {
      "citation_id": "52",
      "title": "Speech emotion recognition using ANN on MFCC features",
      "authors": [
        "H Dolka",
        "M Xavier",
        "S Juliet"
      ],
      "venue": "2021 3rd International Conference on Signal Processing and Communication"
    },
    {
      "citation_id": "53",
      "title": "A tutorial on adaptive design optimization",
      "authors": [
        "J Myung",
        "D Cavagnaro",
        "M Pitt"
      ],
      "year": "2013",
      "venue": "Journal of Mathematical Psychology"
    },
    {
      "citation_id": "54",
      "title": "Domain adaptation for speech emotion recognition by sharing priors between related source and target classes",
      "authors": [
        "Q Mao",
        "W Xue",
        "Q Rao",
        "F Zhang",
        "Y Zhan"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "55",
      "title": "Cross-Corpus Speech Emotion Recognition Based on Few-Shot Learning and Domain Adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "56",
      "title": "TC-Net: A Modest & Lightweight Emotion Recognition System Using Temporal Convolution Network",
      "authors": [
        "M Ishaq",
        "M Khan",
        "S Kwon"
      ],
      "year": "2023",
      "venue": "Computer Systems Science and Engineering"
    },
    {
      "citation_id": "57",
      "title": "MSER: Multimodal speech emotion recognition using cross-attention with deep fusion",
      "authors": [
        "M Khan",
        "W Gueaieb",
        "A Saddik",
        "S Kwon"
      ],
      "venue": "Expert Systems with Applications"
    }
  ]
}