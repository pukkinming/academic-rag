{
  "paper_id": "2510.04688v1",
  "title": "A Study On The Data Distribution Gap In Music Emotion Recognition",
  "published": "2025-10-06T10:57:05Z",
  "authors": [
    "Joann Ching",
    "Gerhard Widmer"
  ],
  "keywords": [
    "Music emotion recognition",
    "emotion modeling",
    "data distribution gap"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music Emotion Recognition (MER) is a task deeply connected to human perception, relying heavily on subjective annotations collected from contributors. Prior studies tend to focus on specific musical styles rather than incorporating a diverse range of genres, such as rock and classical, within a single framework. In this paper, we address the task of recognizing emotion from audio content by investigating five datasets with dimensional emotion annotations -EmoMusic, DEAM, PMEmo, WTC, and WCMED -which span various musical styles. We demonstrate the problem of out-of-distribution generalization in a systematic experiment. By closely looking at multiple data and feature sets, we provide insight into genre-emotion relationships in existing data and examine potential genre dominance and dataset biases in certain feature representations. Based on these experiments, we arrive at a simple yet effective framework that combines embeddings extracted from the Jukebox model with chroma features and demonstrate how, alongside a combination of several diverse training sets, this permits us to train models with substantially improved cross-dataset generalization capabilities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The relationship between music and emotion has been a long-investigated topic in MIR. Music emotions can be categorized into two types: \"perceived\" and \"induced\"  [39] . Perceived emotion refers to the emotion conveyed by the music itself  [15] , while induced emotion describes the feelings invoked in the listener  [20] . In this work, we focus solely on perceived emotion.\n\nMusic Emotion Recognition (MER) research generally follows two approaches: (1) regression-based  [40, 5, 25, 19]  or  (2)  classification-based  [41, 35, 19] . These approaches correspond to different annotation formats, which are either dimensional (e.g., valence-arousal) or categorical (or discrete) (e.g., happy or sad)  [13] . Russell's  [32]  circumplex model, with its two proposed dimensions, valence (positivity of emotional responses) and arousal (emotional intensity), is often considered less ambiguous than categorical labels  [40] , as it captures emotional variation on a continuous scale rather than assigning music to a fixed set of discrete adjectives; we adopt this emotion model in this study.\n\nDespite significant research efforts, quantifying emotion in music remains a complex challenge due to the subjectivity of human perception, which varies across individuals based on factors such as cultural background and personal preference. Over the years, researchers have invested significant time and effort into obtaining reliable emotion annotations. However, these are often collected under specific conditions, such as a focus on particular musical styles or genres  [34, 2, 12, 42, 14] . As a result, there are inherent limitations to their generalizability to other unseen musical styles. Previous studies have shown that different musical genres evoke distinct emotional responses  [11] , highlighting the challenge to the universality of emotion annotations across genres. This challenge becomes even more pronounced when considering multiple datasets where the annotation process was conducted under varying instructions and settings. The effect is particularly significant in audio-based models, as opposed to symbolic representations (e.g., MIDI), since audio retains all expressive qualities, including timbre, which strongly influences emotion perception  [38, 17, 28] . Additionally, in dimensional MER, datasets are often compiled using different valence-arousal scales, making the integration challenging and possibly obscuring genre-specific emotional patterns.\n\nIn this work, we analyze the data distribution gap and genre bias in commonly used MER datasets. To investigate this, we start by determining a suitable input feature representation for the task of MER, which leads us to focus on Jukebox embeddings  3  . We demonstrate the data distribution gap with a systematic cross-dataset experiment. To better understand genre-emotion relationships in existing data, we analyze distribution divergences between datasets in both audio content and emotion annotations. Additionally, based on feature clustering, we uncover potential genre dominance and dataset biases in certain feature representations. Finally, we identify chroma features as a very simple complement to Jukebox embeddings that, combined with diversified training datasets, improves and stabilizes in-and out-of-distribution MER performance, enhancing adaptability to unseen data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Prior work in dimensional music emotion recognition has focused on identifying relevant audio features, drawing from psychological theories  [24] , music domain knowledge  [22, 40, 30] , and multi-modal sources such as MIDI and lyrics  [25, 29] . To examine the relationship between these features and emotion labels, tools such as PsySound  [3] , MARSYAS  [36] , and MIRToolBox  [21]  are often used  [1, 27] . However, some of these are no longer maintained, making feature extraction and reproducibility increasingly difficult.\n\nWith the rise of deep audio embeddings such as VGGish  [33]  and OpenL3  [8]  in the MIR field, researchers have explored their use in MER tasks  [19] . More recently, large-scale models trained on massive datasets have shown strong performance and transferability across tasks  [23, 37, 16] . Castellon et al.  [4]  demonstrated the effectiveness of the Jukebox embeddings  [9]  in MER, outperforming other input types on EmoMusic, despite them not yet being commonly used in MIR tasks. Based on these results (and our own preliminary experiments), Jukebox embeddings will be used as the central feature set in our experiments (see Section 4).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "As mentioned in Section 1, we focus on datasets that provide dimensional annotations of perceived emotions, with the additional constraint that such remain publicly available. Among the widely used datasets since 2020  [18] , EmoMusic, DEAM, and PMEmo are relatively well-established for MER tasks. The distribution of genre labels officially provided by EmoMusic and DEAM is plotted in Appendix A and documents a somewhat selective focus on popular genres (e.g. Country, Electronic, Jazz, and Pop).  4  To investigate the performance on stylistically distant music, we will further include WTC and WCMED, both of which consist of Western classical piano excerpts, rather than merely expanding the corpus with large-scale commercial music datasets. Finally, for datasets that provide both static and dynamic annotations, only the static data is considered in our study. Below is a brief overview of the datasets:\n\n1. EmoMusic (E)  [34]  was developed for the \"Emotion in Music\" task at MediaEval 2013.  5  The dataset consists of 744 audio recordings, each with a duration of 45 seconds, spanning a variety of musical styles, such as Country, Blues, Electronic, and Rock. The emotion annotations for the corresponding 45-second clips are on a continuous scale ranging from -1 to +1. 2. DEAM (D)  [2]  extends the EmoMusic dataset, aggregating recordings from the MediaEval task between 2013 and 2015.  6  With a total of 1,802 tracks, it encompasses EmoMusic as a subset. 3. PMEmo (P)  [42]  contains emotion annotations for 794 songs collected from the Billboard Hot 100, the iTunes Top 100 Songs (USA), and the UK Top 40 Singles Chart. Of these, 767 tracks include valence-arousal labels, annotated on manually selected chorus excerpts with values ranging from 0 to +1. The duration of these excerpts varies from 11-88 seconds. 4. WTC (W1)  [7]  provides valence-arousal annotations for six different performances, by six renowned pianists, of Bach's Well-Tempered Clavier (WTC)\n\nBook 1. The dataset maintains stylistic coherence, featuring compositions evenly distributed across all 24 possible major and minor keys, each represented by a prelude-fugue pair. In total, it contains 288 recordings, with participants rating valence on a scale of -5 to +5 and arousal on a scale of 0 to 100. 5. WCMED (W2)  [14]  comprises 200 royalty-free audio recordings of Western classical repertoire, 79 of which are solo piano pieces by composers such as Bach, Beethoven, Chopin, Mozart, Rachmaninoff, collected from the Saarland Music Dataset (SMD)  [26] . Each annotated excerpt ranges from 8 to 20 seconds in duration. Instead of absolute ratings, annotations were obtained through a ranking-based crowd-sourcing experiment, with participants performing pairwise comparisons. The resulting rankings span from 0 to 400 for both valence and arousal. While the investigation of adaptability in pairwise and direct annotations should be further studied, we include this set due to its representation of the Western classical music style.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we analyze the data distribution gap and genre bias in several steps, as outlined in the introduction. To establish a suitable audio feature set for the subsequent investigations, we evaluate various feature representations in an MER task on the MediaEval EmoMusic dataset, leading us to focus on Jukebox embeddings (Section 4.1). Using this feature set, we then demonstrate and quantify the data distribution gap with a cross-dataset experiment (Section 4.2). Section 4.3 analyzes distribution divergences between datasets in terms of both audio content and emotion annotations. Finally, we identify chroma features as a stabilizing factor and, in Section 4.4 test how and to what extent the combination of Jukebox embeddings with chroma features improves in-distribution performance and out-of-distribution generalization.\n\nThe underlying model architecture that we use for all subsequent learning experiments is a simple feedforward neural network (MLP) using Mean Squared Error (MSE) as the loss function. The input feature dimension is variable, depending on the type of input feature. The model consists of two hidden layers with ReLU activation, where the first hidden layer has 1024 units and the second has 512. To prevent overfitting, dropout is applied at the input and after each hidden layer. Finally, the model outputs two separate predictions, one for valence and one for arousal.\n\nIn terms of inputs, audio segment lengths vary across datasets (see Section 3 for details) to ensure that the emotion remains consistent throughout the annotated audio clip. During Jukebox embedding extraction, a random 25-second segment is selected from each annotated clip; segments shorter than this length are padded. As a result, audio clips of arbitrary lengths always produce a fixedsize input.\n\nFor all datasets considered in this work, the emotion annotations are independently normalized to the range of -1 and +1 by dataset. We follow the experimental setup of previous works and split each dataset into training, validation, and test sets using a ratio of 8 : 1 : 1. This consistent split is applied across all experiments, whether using individual datasets or combining multiple. The exception is when a set is used as out-of-distribution test set, in which case the whole dataset is utilised.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Identifying The Best Audio Features",
      "text": "Considering recent trends in learning representations with foundation models, we evaluate the effectiveness of Jukebox embeddings by comparing them against hand-crafted features (Chroma, MFCCs), data-learned features (Mid-Level Features), and embeddings from foundation models (MERT, Music2Latent, Encodec), using a consistent experimental setup as the previous step with minimal parameter tuning for each feature type (Table  1 ).\n\nTo ensure comparability, we follow the pipeline of Castellon et al.  [4]  to extract embeddings from the 36th (middle) layer of the Jukebox-5B model, yielding 4800-dimensional vectors. Performance is evaluated using the coefficient of determination (R 2 ), a standard regression metric in MER to quantify how well the model explains variance in valence and arousal. The hand-crafted features are computed using Librosa's CQT-chromagram to capture tonal and harmonic content and Mel-frequency cepstral coefficients for the short-term spectral variations and temporal evolution. To enrich these features, we compute first-, second-, and third-order differences along the time axis, which are concatenated into a final feature vector. For data-driven approaches, Mid-level Features and foundation model embeddings are obtained using their respective open-source implementations (see references in Table  1 ). Results show that Jukebox embeddings outperform all alternative feature representations, and will thus serve as our primary feature set moving forward.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Gap: Cross-Dataset Prediction",
      "text": "To assess the model's adaptability to unseen datasets, we conduct a series of systematic cross-dataset evaluations, using Jukebox embeddings as input features for all datasets, with a consistent model structure. Due to content similarities between EmoMusic and DEAM, training and testing interchangeably on these datasets yields relatively good results. However, performance drops significantly when evaluated on other datasets (see Table  2 ), which implies a severe data distribution gap. Since Jukebox is trained as a generative model capable of producing music with coherent tempo, genre, instrumentation, and key, its embeddings likely encode musically relevant information, making them effective in distinguishing patterns across datasets.\n\nTo further investigate, we apply t-SNE to the Jukebox embeddings from all datasets to visualize their distributions (Fig.  1 ). The embeddings form distinct clusters by dataset, with the exception of EmoMusic and DEAM, which overlap due to shared content. The same visualization procedure is applied to all other feature types; all, except chroma, show similar dataset-wise separability (see Appendix B).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analyzing The Gap",
      "text": "The universality of emotion annotations across genres is particularly important in our study, as we aim to investigate the potential of integrating annotations from multiple datasets for MER. However, as different genres can evoke distinct emotional responses  [11] , this raises concerns about annotation consistency across datasets. Because these annotations are independently normalized -due to varying annotation scales across datasets -this process may obscure differences in the underlying data distributions. Consequently, there may be a mismatch between the musical content and the seemingly aligned emotion annotations. To examine this relationship, we compute the Wasserstein distance and Jensen-Shannon divergence between all dataset pairs, comparing both the data content distributions and the emotion annotation distributions. The Wasserstein distance quantifies distributional shifts by measuring the optimal transport cost The results are shown in Table  3 . To interpret the pairwise comparisons, consider the EmoMusic column as an example. When compared to DEAM, the data distributions (column \"Data\") are highly similar (WD=0.03, JS=0.02) but the annotation distributions (column \"Annotation\") show a slightly larger divergence (WD=0.05, JS=0.13), suggesting differences in emotion interpretation. On the other hand, when compared to PMEmo, data distributions exhibit a relatively small shift (WD=0.20, JS=0.15), while the annotation distributions indicate a stronger similarity in emotion labels (WD=0.11, JS=0.03).\n\nInterestingly (still in the EmoMusic column), comparison with the two classical music datasets reveals notable differences. The largest shift in data distribution, indicating a substantial difference in content, occurred between EmoMusic and WCMED (WD=1.71, JS=0.46), while the annotations remained relatively close (WD=0.14, JS=0.02). Despite the pronounced content differences, the distribution of emotion labels across these datasets seems to be strikingly similar. WTC, on the other hand, though coming under the same genre label as WCMED (\"Classical\"; and indeed, both datasets contain solo piano music only, but from different composers) shows only a moderate shift in data distribution to EmoMusic (WD=0.37, JS=0.25), but a notable difference in the interpretation of emotions, as shown by the notable difference in the emotion annotations (WD=0.12, JS=0.49). These findings not only highlight the differing associations of classical music datasets with EmoMusic but also suggest that even when music content varies profoundly, they may still point toward similar emotional representations.\n\nTo examine the factors contributing to the separation of Jukebox embeddings by dataset (Fig.  1 ), we apply k-means clustering to the Jukebox feature vectors to see if there are associations between certain features and certain datasets or genres. Motivated by preliminary findings suggesting that Chroma features have a stabilizing effect across datasets, we include Chroma features in the same clustering analysis. Indeed, it is quite reasonable to assume that Chroma features might be generally informative, as the perception of emotion in music is often linked to harmonic content, also in the literature. More precisely, we perform k-means clustering on the combined Jukebox and Chroma feature vectors extracted from the union of all datasets, and analyze how strongly each dataset and genre is represented in each resulting cluster. The lower half of Fig.  2(b)  and (c ) suggests that the Jukebox embeddings capture genre-specific characteristics, which is further supported by visual similarity between the clustering in Fig.  1 . In contrast, clusters formed using Chroma features show relatively uniform distributions over datasets and genres, indicating lower sensitivity to genre-specific differences. These findings suggest that combining Jukebox embeddings with Chroma features may mitigate dataset and genre bias in MER model training.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Bridging The Gap?",
      "text": "As the final step in bridging the gap between datasets and genres, we revisit and combine the previous findings. Generally, we wish to introduce and combine data from a broader spectrum of musical styles, to train MER models that generalize better. But instead of combining all five datasets, we focus on three representing distinct styles: EmoMusic, PMEmo, and WTC. This allows us to perform out-of-distribution testing using the remaining two datasets. In addition, based on the findings presented above, and confirmed also relative to other feature sets in Fig.  4  in the Appendix B, we add the Chroma features to the input representation. As per our clustering experiment, they seem to be less dataset-and genre-specific and might have a regularizing effect. More precisely,   we concatenate the 4800-dimensional Jukebox input vector with a 72-dimension vector derived from the 12 Chroma values, including the mean and standard deviation of their first-and second-order derivatives over time.\n\nAs shown in Table  4 , adding Chroma features alone already substantially improves generalization to the out-of-domain Classical dataset WCMED, albeit at the cost of a slight drop in performance on the in-domain EmoMusic test set. Broadening the stylistic coverage of the training data by including PMEmo and WTC yields additional improvement, lifting the results on both the now more diverse in-domain test set (EmoMusic+PMEmo+WTC) and -very substantially -on the out-of-domain WCMED collection. DEAM, which is closely related to EmoMusic, also benefits from Chroma features and gains further improvement with a diversified training set.\n\nTo contextualize our results, the first two rows of Table  4  report scores from  [34]  and  [19] , both of which trained and tested their models on EmoMusic using the same R 2 metric for valence and arousal, making their results directly comparable to ours. The prior state of the art on this dataset would be  [4] , who first applied Jukebox embeddings to MER tasks. In our replication of their baseline setup (row 3 of Table  4 ), we obtain R 2 scores of 0.674, 0.708, 0.640. Furthermore, to isolate the impact of Chroma features, we conducted an experiment Table  4 : Results on in-and out-of-distribution emotion recognition. The first two lines report the results of  [34]  and  [19] ; we only cite the corresponding numbers from these papers and did not reproduce their results. Testing with out-of-distribution datasets is only performed in our own experiments, from line 3 onwards. The combined dataset is the union of EmoMusic, PMEmo, and WTC.   4 ). While this setup already improves performance on out-ofdistribution datasets compared to using only EmoMusic for training, the gains are notably smaller than those achieved by incorporating Chroma features.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "With this project, we hope to have heightened the awareness, in the MER research community, of the problem of style-and genre-specificity of current MER models. We documented the problem with a systematic cross-dataset prediction experiment. Based on a series of experiments and analyses, we finally arrived at the combination of Jukebox embeddings and Chroma features as a simple, but apparently robust baseline representation which, in combination with a diversification of training data, permits us to train models that generalize substantially better to out-of-distribution data. We propose this representation and the results of the last experiment as a new baseline for the MER community.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Appendix",
      "text": "Figure  3  in this document provides the genre distributions for the datasets Emo-Music and DEAM, as officially reported. While both datasets contain a broad variety of genres, they are predominantly focused on Country, Electronic, Jazz, and Pop. Although \"Classical\" appears to have the largest genre representation, we found pieces labeled as \"Classical\" to be somewhat arbitrary. For this reason, we expand the corpus for the experiments to include datasets consisting exclusively of Western classical music.\n\nAs mentioned in Section 4.2 of the main text, a clear pattern emerges when visualizing the embedding distributions with t-SNE: the embeddings are wellseparated by dataset. Since Jukebox is a generative model trained to produce songs with consistent tempo, genre, instrumentation, and key, its embeddings likely capture musically relevant information, such as tonal, rhythmic, and timbral properties. To examine whether such distinctive clustering patterns are unique to Jukebox or also present in other embeddings without these properties, we apply the same visualization method to all other feature types considered in this study (Fig.  4 ). Additionally, we calculate the inter-centroid distance, as well as the average and variance, as a metric to compare the spread and relationships between datasets. These results are visualized as heatmaps, which show the distance between each dataset pair. Ideally, if the feature is not strongly distinguishable by dataset, we would expect a lower mean inter-centroid distance and variance. The plots reveal that all features, with the exception of Chroma features, exhibit a similar pattern: PMEmo predominantly occupies one side of the 2D plane, the two Western classical sets dominate the other side, and EmoMusic closely overlaps with DEAM due to their shared content. The inter-centroid distances between dataset pairs for all features also showed the same pattern. Therefore, suggesting that the bias observed in the Jukebox embeddings is not unique to that feature set.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The embeddings form distinct",
      "page": 6
    },
    {
      "caption": "Figure 1: Jukebox embeddings of each dataset visualized using t-SNE, with the",
      "page": 7
    },
    {
      "caption": "Figure 1: ), we apply k-means clustering to the Jukebox feature vectors",
      "page": 8
    },
    {
      "caption": "Figure 1: In contrast, clusters formed using Chroma features show relatively uniform dis-",
      "page": 8
    },
    {
      "caption": "Figure 4: in the Appendix B, we add the Chroma features to the",
      "page": 8
    },
    {
      "caption": "Figure 2: Visualization of K-means clustering on Chroma (top) and Jukebox (bot-",
      "page": 9
    },
    {
      "caption": "Figure 3: in this document provides the genre distributions for the datasets Emo-",
      "page": 11
    },
    {
      "caption": "Figure 4: ). Additionally, we calculate the inter-centroid distance, as",
      "page": 11
    },
    {
      "caption": "Figure 3: Genre distributions of EmoMusic and DEAM with the officially provided",
      "page": 11
    },
    {
      "caption": "Figure 4: t-SNE visualization of all embeddings considered in the task, along with",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 3: The Wasserstein distance (WD) and Jensen-Shannon divergence (JS)",
      "data": [
        {
          "EmoMusic": "Data\nAnnot.",
          "DEAM": "Data\nAnnot.",
          "PMEmo": "Data\nAnnot.",
          "WTC": "Data\nAnnot."
        },
        {
          "EmoMusic": "-\n-\n0.03/0.02 0.05/0.13\n0.20/0.15 0.11/0.03 0.19/0.14 0.10/0.16\nW1 0.37/0.25 0.12/0.49 0.45/0.31 0.18/0.47 0.37/0.26 0.15/0.60\nW2 1.71/0.46 0.14/0.02 1.74/0.47 0.19/0.05 1.71/0.46 0.15/0.11 1.59/0.43 0.13/0.51",
          "DEAM": "-\n-\n-\n-",
          "PMEmo": "-\n-\n-\n-\n-\n-",
          "WTC": "-\n-\n-\n-\n-\n-\n-\n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Results on in- and out-of-distribution emotion recognition. The first",
      "data": [
        {
          "Input": "Hand-crafted [34]",
          "Training": "EmoMusic",
          "Testing": "EmoMusic",
          "Avg.\nA.\nV.": "-\n0.54\n0.07"
        },
        {
          "Input": "OpenL3 [19]",
          "Training": "EmoMusic",
          "Testing": "EmoMusic",
          "Avg.\nA.\nV.": "-\n0.67\n0.56"
        },
        {
          "Input": "Jukebox",
          "Training": "EmoMusic",
          "Testing": "EmoMusic\nDEAM\nWCMED",
          "Avg.\nA.\nV.": "0.674\n0.708\n0.640\n0.454\n0.490\n0.418\n-0.835\n-1.115\n-0.555"
        },
        {
          "Input": "",
          "Training": "Combined",
          "Testing": "Combined\nDEAM\nWCMED",
          "Avg.\nA.\nV.": "0.632\n0.685\n0.580\n0.619\n0.618\n0.620\n0.082\n0.336\n-0.172"
        },
        {
          "Input": "Jukebox+Chroma",
          "Training": "EmoMusic",
          "Testing": "EmoMusic\nDEAM\nWCMED",
          "Avg.\nA.\nV.": "0.651\n0.692\n0.610\n0.479\n0.528\n0.430.\n0.002\n0.232\n-0.228"
        },
        {
          "Input": "",
          "Training": "Combined",
          "Testing": "Combined\nDEAM\nWCMED",
          "Avg.\nA.\nV.": "0.684\n0.745\n0.622\n0.830\n0.826\n0.835\n0.277\n0.366\n0.188"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "MIRUtrecht participation in mediaeval 2013: Emotion in music task",
      "authors": [
        "A Aljanaki",
        "F Wiering",
        "R Veltkamp"
      ],
      "year": "2013",
      "venue": "MultiMediaeval Benchmark Proceedings"
    },
    {
      "citation_id": "2",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "A Aljanaki",
        "Y Yang",
        "M Soleymani"
      ],
      "year": "2017",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "3",
      "title": "Psysound: A computer program for psychoacoustical analysis",
      "authors": [
        "D Cabrera"
      ],
      "year": "1999",
      "venue": "Australian Acoustical Society Conference"
    },
    {
      "citation_id": "4",
      "title": "Codified audio language modeling learns useful representations for music information retrieval",
      "authors": [
        "R Castellon",
        "C Donahue",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Proc. 22nd International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "5",
      "title": "Regression-based music emotion prediction using triplet neural networks",
      "authors": [
        "K Cheuk"
      ],
      "year": "2020",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "6",
      "title": "Towards explainable music emotion recognition: The route via mid-level features",
      "authors": [
        "S Chowdhury",
        "A Vall",
        "V Haunschmid",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Proc. 20th International Society for Music Information Retrieval Conference (ISMIR"
    },
    {
      "citation_id": "7",
      "title": "On perceived emotion in expressive piano performance: Further experimental evidence for the relevance of mid-level perceptual features",
      "authors": [
        "S Chowdhury",
        "G Widmer"
      ],
      "year": "2021",
      "venue": "Proc. 22nd International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "8",
      "title": "Look, listen and learn more: Design choices for deep audio embeddings",
      "authors": [
        "J Cramer",
        "H Wu",
        "J Salamon",
        "J Bello"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Jukebox: A generative model for music",
      "authors": [
        "P Dhariwal"
      ],
      "year": "2020",
      "venue": "Jukebox: A generative model for music",
      "arxiv": "arXiv:2005.00341"
    },
    {
      "citation_id": "10",
      "title": "High fidelity neural audio compression",
      "authors": [
        "A Défossez",
        "J Copet",
        "G Synnaeve",
        "Y Adi"
      ],
      "year": "2022",
      "venue": "High fidelity neural audio compression",
      "arxiv": "arXiv:2210.13438"
    },
    {
      "citation_id": "11",
      "title": "Are the emotions expressed in music genre-specific? An audio-based evaluation of datasets spanning classical, film, pop and mixed genres",
      "authors": [
        "T Eerola"
      ],
      "year": "2011",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "12",
      "title": "Music and emotion stimulus sets consisting of film soundtracks",
      "authors": [
        "T Eerola"
      ],
      "year": "2019",
      "venue": "OSF"
    },
    {
      "citation_id": "13",
      "title": "Basic emotions. in Handbook of Cognition and Emotion, 1st ed",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. in Handbook of Cognition and Emotion, 1st ed"
    },
    {
      "citation_id": "14",
      "title": "A comparative study of western and chinese classical music based on soundscape models",
      "authors": [
        "J Fan",
        "Y Yang",
        "K Dong",
        "P Pasquier"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Emotion perceived and emotion felt: Same or different?",
      "authors": [
        "A Gabrielsson"
      ],
      "year": "2001",
      "venue": "Musicae Scientiae"
    },
    {
      "citation_id": "16",
      "title": "LLark: A multimodal instruction-following language model for music",
      "authors": [
        "J Gardner",
        "S Durand",
        "D Stoller",
        "R Bittner"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "It's not what you play, it's how you play it: Timbre affects perception of emotion in music",
      "authors": [
        "J Hailstone"
      ],
      "year": "2009",
      "venue": "Quarterly journal of experimental psychology"
    },
    {
      "citation_id": "18",
      "title": "Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges",
      "authors": [
        "J Kang",
        "D Herremans"
      ],
      "year": "2024",
      "venue": "Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges",
      "arxiv": "arXiv:2406.08809"
    },
    {
      "citation_id": "19",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "E Koh",
        "S Dubnov"
      ],
      "year": "2021",
      "venue": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "arxiv": "arXiv:2104.06517"
    },
    {
      "citation_id": "20",
      "title": "Using music to induce emotions: Influences of musical preference and absorption",
      "authors": [
        "G Kreutz",
        "U Ott",
        "D Teichmann",
        "P Osawa",
        "D Vaitl"
      ],
      "year": "2007",
      "venue": "Psychology of Music"
    },
    {
      "citation_id": "21",
      "title": "A matlab toolbox for musical feature extraction from audio",
      "authors": [
        "O Lartillot",
        "P Toiviainen"
      ],
      "year": "2007",
      "venue": "Proc. of the 10th International Conference of Digital Audio Effects"
    },
    {
      "citation_id": "22",
      "title": "Audio music mood classification using support vector machine",
      "authors": [
        "C Laurier",
        "P Herrera"
      ],
      "year": "2007",
      "venue": "Audio music mood classification using support vector machine"
    },
    {
      "citation_id": "23",
      "title": "MERT: Acoustic music understanding model with large-scale selfsupervised training",
      "authors": [
        "Y Li"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations (ICLR"
    },
    {
      "citation_id": "24",
      "title": "Automatic mood detection and tracking of music audio signals",
      "authors": [
        "L Lu",
        "D Liu",
        "H Zhang"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Emotionally-relevant features for classification and regression of music lyrics",
      "authors": [
        "R Malheiro",
        "R Panda",
        "P Gomes",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Saarland music data (SMD)",
      "authors": [
        "M Müller",
        "V Konz",
        "W Bogler",
        "V Arifi-Müller"
      ],
      "year": "2011",
      "venue": "Late-Breaking and Demo Session of the International Conference on Music Information Retrieval"
    },
    {
      "citation_id": "27",
      "title": "Novel audio features for music emotion recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Audio features for music emotion recognition: A survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "R Panda",
        "R Malheiro",
        "B Rocha",
        "A Oliveira",
        "R Paiva"
      ],
      "year": "2013",
      "venue": "10th International symposium on computer music multidisciplinary research (CMMR)"
    },
    {
      "citation_id": "30",
      "title": "Using support vector machines for automatic mood tracking in audio music",
      "authors": [
        "R Panda",
        "R Paiva"
      ],
      "year": "2011",
      "venue": "th AES Convention"
    },
    {
      "citation_id": "31",
      "title": "Music2Latent: Consistency autoencoders for latent audio compression",
      "authors": [
        "M Pasini",
        "S Lattner",
        "G Fazekas"
      ],
      "year": "2024",
      "venue": "Proc. 25th International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "32",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "33",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition"
    },
    {
      "citation_id": "34",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "M Soleymani",
        "M Caro",
        "E Schmidt",
        "C Sha",
        "Y Yang"
      ],
      "year": "2013",
      "venue": "1000 songs for emotional analysis of music"
    },
    {
      "citation_id": "35",
      "title": "Evaluation of musical features for emotion classification",
      "authors": [
        "Y Song",
        "S Dixon",
        "M Pearce"
      ],
      "year": "2012",
      "venue": "Proc. 13th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "36",
      "title": "MARSYAS: a framework for audio analysis",
      "authors": [
        "G Tzanetakis",
        "P Cook"
      ],
      "year": "2002",
      "venue": "Organised Sound"
    },
    {
      "citation_id": "37",
      "title": "A foundation model for music informatics",
      "authors": [
        "M Won",
        "Y Hung",
        "D Le"
      ],
      "year": "2024",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "The correspondence of music emotion and timbre in sustained musical instrument sounds",
      "authors": [
        "B Wu",
        "A Horner",
        "C Lee"
      ],
      "year": "2014",
      "venue": "AES: Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "39",
      "title": "Music Emotion Recognition",
      "authors": [
        "Y Yang",
        "H Chen"
      ],
      "year": "2011",
      "venue": "Music Emotion Recognition"
    },
    {
      "citation_id": "40",
      "title": "A regression approach to music emotion recognition",
      "authors": [
        "Y Yang",
        "Y Lin",
        "Y Su",
        "H Chen"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Music emotion classification: a fuzzy approach",
      "authors": [
        "Y Yang",
        "C Liu",
        "H Chen"
      ],
      "year": "2006",
      "venue": "Music emotion classification: a fuzzy approach"
    },
    {
      "citation_id": "42",
      "title": "The PMEmo dataset for music emotion recognition",
      "authors": [
        "K Zhang",
        "H Zhang",
        "S Li",
        "C Yang",
        "L Sun"
      ],
      "year": "2018",
      "venue": "Proc. ACM on International Conference on Multimedia Retrieval"
    }
  ]
}