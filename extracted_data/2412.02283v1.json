{
  "paper_id": "2412.02283v1",
  "title": "Vr Based Emotion Recognition Using Deep Multimodal Fusion With Biosignals Across Multiple Anatomical Domains",
  "published": "2024-12-03T08:59:12Z",
  "authors": [
    "Pubudu L. Indrasiri",
    "Bipasha Kashyap",
    "Chandima Kolambahewage",
    "Bahareh Nakisa",
    "Kiran Ijaz",
    "Pubudu N. Pathirana"
  ],
  "keywords": [
    "Emotion recognition",
    "valance",
    "arousal",
    "multimodel",
    "multi-domain",
    "LSTM",
    "multi-scaled attention",
    "SE block"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is significantly enhanced by integrating multimodal biosignals and IMU data from multiple domains. In this paper, we introduce a novel multi-scale attention-based LSTM architecture, combined with Squeeze-and-Excitation (SE) blocks, by leveraging multi-domain signals from the head (Meta Quest Pro VR headset), trunk (Equivital Vest), and peripheral (Empatica Embrace Plus) during affect elicitation via visual stimuli. Signals from 23 participants were recorded, alongside self-assessed valence and arousal ratings after each stimulus. LSTM layers extract features from each modality, while multi-scale attention captures fine-grained temporal dependencies, and SE blocks recalibrate feature importance prior to classification. We assess which domain's signals carry the most distinctive emotional information during VR experiences, identifying key biosignals contributing to emotion detection. The proposed architecture, validated in a user study, demonstrates superior performance in classifying valance and arousal level (high / low), showcasing the efficacy of multi-domain and multimodal fusion with biosignals (e.g., TEMP, EDA) with IMU data (e.g., accelerometer) for emotion recognition in real-world applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "In recent years, affect recognition has garnered significant attention, particularly with the rise of multimodal data sources and advanced deep learning techniques  [12] ,  [13] ,  [14] . Numerous studies have explored various approaches for recognizing affective states using physiological signals and additional modalities such as motion or environmental sensors.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Detection Using Physiological Signals",
      "text": "Physiological signals have been extensively explored in the field of affective computing for emotion detection, given their strong correlation with emotional states. Among the most commonly utilized signals is the electrocardiogram (ECG), which provides insights into heart rate variability (HRV), a key indicator of autonomic nervous system activity and emotional arousal  [2] ,  [6] ,  [7] . Electrodermal activity (EDA), also referred to as galvanic skin response (GSR), measures skin conductance and reflects sympathetic nervous system responses to emotional stimuli, making it a vital tool for detecting arousal and stress  [1] ,  [7] . Additionally, blood volume pulse (BVP), typically measured via photoplethysmography (PPG), has been widely used to assess heart rate and peripheral blood flow, contributing to the detection of emotional responses related to stress and anxiety  [1] . Other crucial signals include respiration rate (RR), which varies with emotional states such as fear or relaxation, and electromyography (EMG), which captures facial muscle movements linked to expressions of emotion  [15] . Moreover, electroencephalogram (EEG) has been extensively employed to monitor brainwave activity, offering insights into the valence and arousal dimensions of emotion  [2]  ,  [7] ,  [3] ,  [16] . Finally, skin temperature (ST) and heart rate (HR) have also proven useful in detecting stress and emotional arousal, further contributing to a holistic understanding of affective states  [1] . These signals, often used in combination, provide a rich, multimodal approach to accurately capturing and classifying complex emotional experiences.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Detection Using Motion Signals",
      "text": "Emotion recognition research based on human motion has attracted significant attention within the field of humancomputer interaction, particularly with the advancements in IMU sensors, which have facilitated efficient and seamless interaction between humans and devices  [8] ,  [9] ,  [17] ,  [18] . IMU sensors, such as accelerometers (ACC) and gyroscopes (GYRO), capture crucial motion-related data that can be used to infer emotional states. Recent studies have explored the use of wearable inertial devices attached to various body parts to collect emotional information through human movement. Hashmi et al.  [8]  employed a smartphone worn on the chest to record human gait signals and used spectro-temporal features from stride signals for emotion recognition, identifying six basic emotions using Random Forest and SVM classifiers. Similarly, Gravina et al.  [9]  leveraged sensor-level and featurelevel fusion to monitor in-seat activities, which reflect psychological and emotional states, though the handcrafted features required labour-intensive design. Chang et al.  [19]  used intelligent inertial sensors to recognize emotions during badminton play, though prolonged activity resulted in physical fatigue that affected the accuracy of emotion detection. Furthermore, recent advancements such as those proposed by Feng et al.  [20]  have integrated motion and emotion recognition into intelligent wearable systems using multi-sensor fusion, enabling real-time interaction and feedback in virtual environments via Digital Twin technology, significantly enhancing user experience and emotion recognition accuracy. These studies highlight the increasing potential of IMU-based motion signals in emotion recognition, though challenges such as feature extraction complexity and physical fatigue remain areas of ongoing research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Emotion Detection Using Eye Tracking Data",
      "text": "Humans interact with their environment, with each elicited emotion being closely tied to its specific context  [21] . Ptaszynski et al.  [22]  highlighted the importance of incorporating contextual analysis into emotion processing. Eye-tracking involves identifying a user's gaze point or focus on a specific visual stimulus. An eye-tracker, a device designed for this purpose, measures eye position and movements  [23] . As a sensor technology, eye-tracking is versatile, applicable in diverse configurations and uses, as demonstrated by Singh et al.  [24] . Furthermore, other studies explore eye movements as potential indicators for recognizing emotions  [10] ,  [11] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Multimodal Emotion Detection",
      "text": "Multimodal fusion has captured significant interest across various research domains due to its broad applicability in areas such as emotion recognition, event detection, image segmentation, and video classification  [25] . Fusion strategies are traditionally categorized by the fusion level, including: 1) feature-level fusion (early fusion), 2) decision-level fusion (late fusion), and 3) hybrid multimodal fusion. With advancements in deep learning, an increasing number of researchers are leveraging deep learning frameworks to enhance multimodal fusion approaches.\n\nFeature-level fusion is a widely adopted approach for integrating multiple modalities, where features from each modality are combined into a high-dimensional representation and then processed as a unified input by models  [26] ,  [3] ,  [11] ,  [1] .\n\nDecision-level fusion, by contrast, involves the use of multiple classifiers and their aggregation, frequently through ensemble learning techniques  [27] . This approach merges individual classifier outputs into a single decision. Techniques such as MAX fusion, SUM fusion, and fuzzy integral fusion are commonly employed in multimodal emotion recognition, highlighting the complementary nature of EEG and eye movement features through confusion matrix analysis  [28] ,  [29] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Emotion Detection Using Multi-Domain Sensors",
      "text": "Emotion recognition using IMU sensors and biosensors in VR environments has emerged as a critical area of research, with a significant focus on analyzing signals from isolated domains such as head, trunk, and peripheral sensors. Historically, emotion detection efforts have been domain-specific  [5] . For example, head-mounted sensors, predominantly integrated into VR headsets, capture signals like eye movement and facial EMG to infer emotional states. Trunk-mounted sensors have primarily focused on physiological signals such as HRV and respiration, while peripheral devices, particularly wrist-worn sensors, monitor EDA, skin temperature, and accelerometry. While these single-domain approaches have demonstrated effectiveness in capturing emotional cues, they inherently limit the understanding of how emotions are represented across the body in different regions  [1] ,  [5] .\n\nHowever, a critical gap in the current literature is the lack of comparative studies that examine the relative effectiveness of these domains (head, trunk, and peripheral) in emotion recognition tasks. Specifically, no substantial research has been conducted to systematically compare the emotional information captured by sensors in these distinct body regions within the same experimental setup. Furthermore, the literature has not adequately explored the potential benefits of multimodal sensor fusion, where signals from multiple domains are combined to exploit complementary features and improve emotion detection accuracy. This omission leaves a gap in understanding the holistic contribution of multi-domain signals in representing complex emotional states, particularly in immersive VR settings.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "F. Motivations And Contributions",
      "text": "Building on the identified gaps in single-domain emotion recognition, this study aims to leverage the potential of multidomain sensor fusion in VR environments. By systematically comparing signals from the head, trunk, and peripheral regions, we seek to determine whether the combination of signals from these diverse domains can provide a more comprehensive and accurate representation of emotional states. Additionally, this work explores the specific contributions of each domain and evaluates the benefits of integrating these signals for more precise emotion detection.\n\nThis study advances multi-domain inference in VR emotion recognition by presenting a framework, within the same experimental setup, that:\n\n• Identifies Key Emotional Domains: Although emotion originates from the central nervous system, emerging research supports the idea that physiological signals from various body domains such as the head  [10] ,  [11] , trunk  [2] ,  [6] ,  [7] , and peripheral [?],  [19]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experimental Protocol A. Video Stimuli Selection",
      "text": "Thirteen 360-degree videos were sourced from the public database described in  [32] , specifically chosen for their ability to evoke targeted emotional responses. Due to the varied lengths of these original videos, three domain experts-two psychologists and one computer scientist-selected specific intervals within each video that best represented the intended emotional state for viewers  [30] ,  [31] . This standardization ensured uniform stimulus duration across all videos, reducing variability in the experimental setup and maintaining an efficient overall study duration.\n\nTable  I  lists the videos used, detailing the selected intervals and associated emotional quadrants (LALV = low arousal, low valence; LAHV = low arousal, high valence; HALV = high arousal, low valence; HAHV = high arousal, high valence). Video clips were categorized by valence and arousal levels, grouping clips of similar emotional tones within the same quadrant. Participants were presented with a printed Self-Assessment Manikin (SAM) scale reference, ratings < 4 as low valence (LV) and low arousal (LA), and ratings > 4 as high valence (HV) and high arousal (HA) and instructed to rate each video group on these dimensions after viewing, allowing for consistent assessment of emotional responses (Figure  1 ).   [30] . FOR THE REMAINING 5 VR CLIPS, QUADRANT ASSIGNMENTS WERE NOT GIVEN  [31]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Participants",
      "text": "In this study, a cohort of 23 healthy participants was recruited through digital and physical advertisements displayed across social media platforms and within the university campus. A substantial portion of the sample (75.0%, n=18) comprised male participants, while the remaining 25.0% (n=6) were female. The age of participants ranged from 21 to 52 years, yielding a mean age of 28.9 years with a standard deviation of 6.4 years. All participants were screened to confirm the absence of any pre-existing cardiovascular or psychological conditions, ensuring a baseline of physiological and psychological health across the sample.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Experimental Setup",
      "text": "Three commercially available devices-the Equivital Vest, Empatica Embrace Plus Watch, and Meta Quest Pro VR headset were employed to collect a range of physiological and behavioural measurements from participants. The Equivital Vest (specifically, Equivital EQ02+ LifeMonitor) was utilized to capture electrocardiogram (ECG) data along with accelerometer readings, while the Empatica Watch (specifically, Empatica E4 Watch recorded EDA, skin temperature, heart rate, and accelerometer data. The Equivital GSR add-on accessory was used to capture GSR (Galvanic Skin Response) activity. The Meta Quest Pro VR headset, in addition to delivering immersive 3D 360-degree video stimuli, provided critical eyetracking data, capturing participant gaze patterns and reactions in real-time. Within the same experimental set-up, sensory data from the trunk, peripheral (wrist), and head domains were captured using the Equivital Vest, Empatica Embrace Plus Watch, and Meta Quest Pro VR headset, respectively. Figure  2 .(b) provides a schematic overview of the data collection architecture.\n\nTo achieve temporal precision across all devices in the experimental setup, the Host Computer and Meta Quest Pro were synchronized via a shared Network Time Protocol (NTP) server. This alignment ensured uniform timekeeping, critical for data accuracy. Additionally, the control applications for the Equivital Vest and Empatica Watch, running on the host computer, upheld the same time synchronization protocol, reinforcing the cohesion across the system. Data recording on both the vest and watch commenced automatically upon activation and continued until device removal and shutdown. User control over the Meta Quest Pro was enabled through a wireless keyboard, while the research team monitored the participant's perspective in real-time through the device's live video streaming capability. A custom-built VR video player and data logger application developed using Unity and C#, facilitated seamless, real-time data capture from the headset's integrated sensors. Figure  2 .(c) illustrates the architecture of this custom VR application.\n\nThis multi-modal data collection framework, which incorporated physiological signals, immersive environments, and eye-tracking data, provided a robust platform for evaluating participant responses and behaviours under controlled conditions.\n\nThe VR application architecture is structured into three core modules to optimize immersive and interactive data collection: the VR Environment, Data Acquisition, and Data Logger  modules. The VR Environment serves as the primary user interface, offering real-time interaction capabilities for both participants and researchers. The Data Acquisition module systematically gathers data from multiple headset sensors, while the Data Logger archives this information efficiently, ensuring that data is available for post-experiment analysis.\n\nThe Video Player component enables the playback of audiovisual stimuli by loading predesignated media files directly from the headset's file system, delivering an immersive, synchronized audiovisual experience.\n\nAn Input Manager module tracks inputs from a wireless keyboard, allowing the researcher to control playback and data logging functions dynamically. Through predefined keyboard shortcuts, the researcher can navigate media clips, initiate play/pause, skip content, insert event markers, and terminate the application. The Feedback Manager enhances usability by providing a responsive visual overlay, confirming command execution (e.g., a pause icon during a video pause).\n\nWithin the Data Acquisition module, the system utilizes the Meta XR SDK to interface with various headset sensors. The Head Mount Device Tracker provides real-time updates on the headset's spatial position and orientation relative to an auto-calibrated reference point. Left and right Controller Trackers capture hand movements' positional and rotational data relative to the headset, while the Eye Gaze Trackers record gaze orientation and fixation points, offering insights into visual attention patterns. The Marker Input Handler logs event markers initiated via the keyboard in CSV format, streamlining data synchronization and subsequent analysis.\n\nThe Data Logger module consists of two sub-components: the Audio Recorder and CSV Writer. The Audio Recorder captures high-fidelity audio input from the headset's microphones, saving it in .wav format. Simultaneously, the CSV Writer logs all sensor data with precise timestamps in a structured .csv format, ensuring thorough temporal alignment of multimodal data.\n\nThis multi-layered architecture enables rigorous data acquisition, reliable interaction, and enhanced data integrity, establishing the application as a scalable and robust tool for immersive experimental research.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Data Collection",
      "text": "Upon arrival, participants were briefed on the study's objectives and protocol. Researchers ensured accurate equipment fitting after providing informed consent and completing a demographic survey.\n\nParticipants watched thirteen videos, completing the SAM Manikin Questionnaire after each video to rate their emotional responses on a scale from 1 to 7 for both valence and arousal. This process of viewing videos and completing the questionnaire was repeated across all video sets. Researchers then assisted with equipment removal, ensuring standardized data collection for robust, synchronized multi-modal analysis. The data collection process is illustrated in Fig 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Data Analysis And Preprocessing",
      "text": "The system accepts raw data from Empatica Embrace Plus Watch, Equivital Vest and Meta Quest Pro VR Head Set as inputs.Through the data preprocessing module, the heterogeneous inputs can be formatted into specific representations, which can be effectively used in the following feature extraction network module.\n\nThe  The VR headset predominantly tracks eye movement, with left and right eye positions (LEyeposition, REyeposition) measured in three-dimensional space (x, y, z coordinates). These eye position signals provide precise spatial coordinates at specific time points, enabling us to capture gaze dynamics and spatial orientation during each experimental task.\n\nFor peripheral domain signal preprocessing, we applied tailored filtering techniques to ensure high-quality data. Accelerometer signals were filtered using a 0.5-20 Hz bandpass filter to eliminate low-frequency drift and high-frequency noise, preserving relevant motion information. Regarding EDA preprocessing, upsampling from 4 to 64 Hz was performed to make both signals at the equal sampling frequency  [33] .\n\nA Butterworth filter with a cutoff frequency of 0.5 Hz is often used. Apply a bandpass filter between 0.5 Hz and 4 Hz to remove noise for BVP signal processing. Use a moving average to reduce the noise from the temperature readings.\n\nFor trunk domain data, ECG signals, a 0.5-45 Hz band-pass filter was used to retain key waveform features (P, QRS, T waves) while removing artifacts and noise. Same as the watch, accelerometer signals are filtered using a 0.5-20 Hz band-pass filter. However, GSR data were unreliable for five participants; removing them would cause data insufficiency. Thus, GSR was excluded to maintain consistency across participants.\n\nFor each physiological, IMU, and eye-tracking data input, we first applied z-score normalization to standardize the data, ensuring that all signals were on a comparable scale. Following this, we selected the last 40 seconds of data from both the peripheral and trunk domains. This selection was based on an analysis of the signals, which indicated that the final segment of the video stimuli most consistently elicited emotional responses. The extraction of this uniform 40-second segment was necessary to maintain equal input lengths across modalities, a critical requirement for the proposed deep learning architecture.\n\nAfter segmentation, we applied a sliding window technique with a 2-second window and a 50% overlap to the physiological and IMU signals. This windowing strategy effectively partitioned the input data stream into multiple overlapping windows, ensuring fine-grained temporal resolution while preserving temporal context. Each segmented window was then represented as a 2D array with dimensions T × F , where T is the number of time windows (time stamps), and F represents the size of the data (features) in one window, determined by the sampling frequency and window duration. Specifically, for signals from the Empatica Embrace Plus, F is set to 128, based on its sampling rate of 64 Hz, while for the Equivital vest, F is 512, reflecting its sampling rate of 256 Hz.\n\nFor the Meta Quest Pro VR headset data, we extracted the final 2000 coordinates from each axis of both the left and right eye positions (x, y, z for each eye). The data was then segmented into overlapping windows using a sliding window approach, with 50% overlap, dividing the 2000 coordinates into windows of F =200 coordinates each. This segmentation process provided structured 2D inputs for the proposed architecture, similar to the representation used for the physiological and IMU data, ensuring compatibility across all input modalities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. System Architecture",
      "text": "The high-level structure of our proposed emotion recognition system, EMO-MSASE: Emotion Detection using Multi-Scale Attention and Squeeze-and-Excitation is depicted in Fig.  3 , highlighting only the best-performing signals from each device, as determined through experimental evaluation. The system is composed of three core components: data preprocessing, feature extraction, and emotion recognition. Each of these components plays a crucial role in the overall system, and their detailed descriptions are provided in the subsequent sections. EMO-MSASE enhances the framework presented in  [1] , which used a simple attention mechanism. In contrast, we introduce multi-scale attention (MSA), which allows the model to capture patterns across multiple temporal scales, thereby improving its ability to handle varying signal dynamics. Additionally, the integration of Squeeze-and-Excitation (SE) blocks helps recalibrate features by focusing on the most important features for each domain before constructing the fused feature vector from three domains, further enhancing the model's performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Multi-Scale Attention Lstm Feature Extraction",
      "text": "In this stage, we employed an LSTM architecture augmented with a multi-scale attention mechanism to act as a high-level feature extractor. The LSTM model  [34] , a specialized variant of recurrent neural networks (RNNs), is adept at capturing long-term dependencies within sequential data. The core elements of the LSTM structure include the cell state and gating mechanisms; the cell state functions as a \"memory\" that carries information across time steps, while the gating structures control which information is retained or discarded. Specifically, the LSTM comprises three gate types: the forget gate, input gate, and output gate. The forget gate selectively removes irrelevant information from prior cell states, the input gate updates the cell state with new data from the current input, and the output gate produces the final output based on the updated cell state. Utilizing the LSTM enables us to capture temporal relationships within each input modality, yielding a detailed representation. Additionally, recognizing that not all sub-samples contribute equally to the final recognition task (for example, not all frames in a video are equally informative), we incorporated an attention mechanism  [35] . This approach assigns relative importance weights to different sub-samples, allowing for the fusion of these weighted sub-samples into a final informative feature vector. The attention mechanism thereby enhances our ability to capture both temporal and spatial dependencies by differentially weighting each sub-sample, while the fused feature vector, being more compact, also reduces training time requirements  [36] .\n\nThe proposed architecture is structured around a multidomain, multi-modality framework, incorporating three distinct domains: the trunk, peripheral (wrist), and head, represented by the Equivital Vest, Empatica Embrace Plus Watch, and Meta Quest Pro VR headset, respectively. We denote these domains as D V (Equivital Vest), D W (Empatica Embrace Plus), and D H (Meta Quest Pro). Each domain-specific device has multiple modalities to capture diverse physiological and motion-related signals across these domains. For the watch domain, the modalities are denoted as X i W , where i indexes the different modalities, for example, accelerometer data in the x, y, and z directions. Similarly, the modalities for the vest domain are represented as X j V and for the VR headset as X k H , with j and k indexing the respective modalities within each domain.\n\nEach modality follows a consistent feature extraction process using an LSTM + multi-scale attention (MSA) architecture as shown in Fig.  4 . Let X ∈ R T ×F represent the input signal for a given modality, where T is the number of timesteps and F is the number of features per timestep. The LSTM processes this sequence and outputs a sequence of hidden states H ∈ R T ×H , where H is the size of the hidden state. Mathematically, the output of the LSTM can be represented as:\n\nHere, h denotes the hidden state vector at each timestep, capturing features extracted by the LSTM.\n\nThe multi-scale attention (MSA) mechanism is then applied to the LSTM output, operating at three different temporal scales: short-term, medium-term, and long-term. The attention weights for each scale are generated by comparing the hidden states to a learnable context vector, u ∈ R H , which is initialized randomly and trained to prioritize the most informative timesteps for each modality. The attention score α i for each hidden state h i is computed as:\n\nThe learnable context vector u helps highlight important information by guiding the focus on particular timesteps based on the context of the task.\n\nFor the short-term attention, attention weights α i are computed over all T timesteps, producing a weighted sum of the hidden states:\n\nFor the medium-term attention, adjacent timesteps are merged, reducing the sequence length to T medium = T 2 , and attention is applied over the merged sequence with weights β i :\n\nSimilarly, for long-term attention, the sequence is further reduced to T long = T 3 , and attention is applied over this coarser representation with weights γ i :\n\nThe outputs from the short-term, medium-term, and longterm attention mechanisms are concatenated to form the final feature vector, called the Concatenated Attention Vector (CAV) for the modality:\n\nWithin each domain, all modality-specific feature vectors are concatenated to form a domain-specific feature vector. For example, in the watch domain:\n\nwhere M W is the number of modalities in the watch domain. The same process is followed for the vest domain (v V ) and the VR headset domain (v H ).\n\nAfter concatenating the modality-specific feature vectors for each domain, a Squeeze-and-Excitation (SE) block  [37]  is applied to each concatenated domain-specific feature vector (v device ). First, a global average pooling operation is applied to the concatenated domain-specific feature vector:\n\nz device is the result of this global average pooling applied to the concatenated feature vector v device for a specific device (watch, vest, or VR headset).\n\nNext, two fully connected layers with ReLU and sigmoid activations generate recalibration weights for each device:\n\ns device represents the recalibration weights generated for a specific device after applying two fully connected layers to z device . The recalibration weights are then applied to the device-specific feature vector:\n\n. The recalibrated domain-specific feature vectors are then concatenated to form a global feature vector:\n\n. Finally, the global feature vector is passed through a fully connected layer for classification, with softmax activation for multi-class classification:\n\nHere, ŷ represents the predicted class probabilities for C emotion classes.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Deep Learning Setup For The Proposed Architecture",
      "text": "For feature extraction, each device utilizes a dedicated Attention-LSTM network. The LSTM network comprises two layers, each with 128 hidden units. The model is trained using binary cross-entropy loss with the AdamW optimizer for effective weight updates, a learning rate of 0.001, and a batch size of 16. Training is conducted over 50 epochs, with early stopping based on validation performance to prevent overfitting. All data analytics and deep learning model building and testing were conducted on a 13th-generation i7 workstation equipped with an RTX 4080 GPU and 32GB of RAM.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Results",
      "text": "In this section, we present the performance of EMO-MSASE. Our study leverages data from three distinct domains, head (Meta Quest Pro VR headset), trunk (Equivital Vest), and peripheral (Empatica Embrace Plus watch) to advance emotion recognition. These domains contribute multiple modalities, including IMU data, ECG, temperature, EDA, BVP, and eye movements as in Table  II , which are utilized to assess valence and arousal. While these devices offer a wide array of signals, we selected only the most relevant modalities for our analysis, focused on their significance in emotion detection. We collected a total of 299 samples for each modality from 23 participants. Each participant watched thirteen videos, completing the SAM Manikin Questionnaire after each video to rate their emotional responses on a scale from 1 to 7 for both valence and arousal. As indicated in Table  I , we classified ratings < 4 as low valence (LV) and low arousal (LA), and ratings > 4 as high valence (HV) and high arousal (HA), forming the first ground truth (G1) for valence (G1 V) and arousal (G1 A) respectively. The second ground truth (G2) of valence (G2 V) and arousal (G2 A) levels is derived from external ratings (three domain experts, two psychologists and one computer scientist) based on established criteria from previous research  [30] -  [32] . In constructing the deep learning model, four distinct cases of label assignment were systematically applied to the collected dataset: (1) the actual response of each participant for each video was used as the label (general), (2) for each video, a final label was determined by the majority response across all 23 participants (majority) as in Table  I , (3) only the responses from male participants were used (males only), and (  4 ) the label values provided by the selected reference papers (G2 V and G2 A) for each video to assign labels for each video. Cases (  1 ), (2), and (3) are subclasses of the general labels (G1 V and G1 A).\n\nThe efficacy of our proposed deep learning architecture for valence and arousal detection was rigorously evaluated through two distinct cross-validation techniques: (1) Group K-Fold cross-validation and (2) Leave-One-Subject-Out (LOSO) cross-validation. In Group K-Fold cross-validation, participants were divided into two distinct groups for training and testing, ensuring that data from the same participant was not used in both the training and testing sets to prevent data leakage. This approach maintains subject independence during model evaluation. In LOSO cross-validation, the model was trained on data from all but one participant, with the left-out participant's data used for testing. This process was repeated for each participant, allowing for a robust assessment of the model's generalizability across individuals. For Analysis V-A, V-B, V-C, we employed Group K-Fold cross-validation with 5 folds. The mean accuracy across the 5 folds was computed to provide a robust and comprehensive evaluation of the model's performance. To further enhance the evaluation, Analysis V-C was additionally assessed using (LOSO) cross-validation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Uni-Modality Performance Analysis",
      "text": "We began by independently evaluating the performance of each signal modality within each domain, as shown in Table  II , for the detection of valence and arousal.   under G 2 labeling. However, when using G 1 labels, the signals from the trunk and peripheral domains exhibit lower accuracy across all three scenarios (general, majority, and males). Upon closer examination, the accuracy for trunk and peripheral signals is slightly improved under the majority labeling case compared to the other two. In contrast, the head domain exhibits significantly higher performance when majority labeling is employed. Notably, in this scenario, head domain signals outperform those from the trunk and peripheral domains. Specifically, L EP Y and R EP Y independently achieve close to 70% accuracy for valence detection, indicating strong predictive power for these signals in alignment with the majority labeling. Unlike valence, arousal detection demonstrates superior performance with the majority labeling across most signals from all three domains. A similar trend is observed for both valence and arousal in the head domain, where L EP Y and R EP Y independently achieve nearly 70% accuracy. In contrast, G2 V labeling yields comparatively lower performance for signals in the trunk and peripheral domains, though the head domain performance remains consistent with that of the majority labeling. LAT ACC emerges as the most predictive signal in the trunk domain, while EDA stands out as the best-performing signal in the peripheral domain.\n\nIn conclusion, majority labeling consistently performs well for both valence and arousal detection when applied to our SAM ratings.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Single Domain Performance Analysis",
      "text": "In this experiment, we conducted an independent evaluation of each domain's performance using their respective modal-ities for valence and arousal detection. For the peripheral domain, we initially considered four key modalities: IMU data (accelerometer readings along the X, Y, and Z axes), BVP, EDA, and TEMP. These six raw signals across four modalities yielded 64 possible combinations. Table  III  showcases the top four combinations for both valence and arousal detection. For the trunk domain, we analyzed two key modalities: electrocardiogram (ECG) and IMU data. The ECG data was captured from two leads, referred to as ECG1 and ECG2, while the IMU data included lateral acceleration (LAT ACC), longitudinal acceleration (LONG ACC), and vertical acceleration (VERT ACC). The top five performing combinations for valence and arousal detection are summarized in Table  IV . For the eye-tracking data, under the eye modality, we utilized six features: LEyeposition x, LEyeposition y, LEyeposition z, REyeposition x, REyeposition y, and REyeposition z. The results of the top-performing combinations are presented in Table  V .\n\nAs shown in Tables III and IV, higher accuracy values for both valence and arousal were consistently observed with G2 V and G2 A labeling, except in the trunk domain. For arousal detection, the trunk domain achieved the highest accuracy under majority labeling. In contrast, when using G1 V and G1 A labels, valence accuracy was moderate across all three cases (general, majority, and males) in both the peripheral and trunk domains. Overall, majority labeling yielded the highest accuracy for both valence and arousal in the trunk and peripheral domains when using G1 V and G1 A labels from the collected data. In analyzing the top-performing signals combinations for the watch, the combination of ACC Z, EDA, and TEMP consistently demonstrated superior performance For the eye-tracking data from the VR headset, we achieved significantly higher valence and arousal accuracy compared to other domains across all four cases as in Table  V . In the general case, valence accuracy for the other domains was 55% ± 3, while the head data yielded a notably higher accuracy of 60% ± 3. Under majority labeling, valence accuracy remained at 57% ± 4, whereas the head data improved substantially to 66% ± 3. For male participants, valance accuracy was 54% ± 3, compared to a significantly higher 64% ± 3 for head data. Arousal accuracy followed a similar trend, with 55% ± 4 in the general case and 57% ± 3 for the head domain. In the majority case, the accuracy was 57% ± 4 for the peripheral and 56% ± 4 for the trunk, while the eye-tracking data demonstrated a significant improvement, achieving 68% ± 3. For male participants, arousal accuracy was 56% ± 5 for both the peripheral and trunk, but notably higher for the eye-tracking data at 65% ± 3. The optimal signal combination accross all cases for the head domain was found to consist of five key eye signals: LEyeposition x, LEyeposition y, LEyeposition z, REyeposition y, and REyeposition z.\n\nUpon analyzing the final outcomes from all three domains, we observed that, for both the peripheral and trunk domains, the majority case performed slightly better than the general and males-only cases. However, for the VR data, the majority case demonstrated significantly superior performance. Based on these findings, we selected the best-performing modality combinations from each domain and utilized them to design the multi-domain architecture, focusing exclusively on the majority case. The results of this multi-domain architecture are presented in the following subsection.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Multi-Domain Fusion Performance Analysis",
      "text": "The results presented in Table VI clearly demonstrate the superiority of multi-domain fusion over single-domain approaches in both valence and arousal detection. While the head domain consistently outperforms both the peripheral and trunk domains individually, achieving accuracies as high as 71.05% for G1 V and 73.22% for G2 A, the fusion of multiple domains significantly enhances performance. Notably, the combination of trunk and head domains delivers the highest accuracy for G2 V, reaching 87.14%, while the full fusion of all three domains (peripheral, trunk, and head) results in the best overall performance for valence detection in G1 V, with 80.57%. For arousal detection, the head domain alone achieves the best individual performance, but fusion strategies, particularly those combining head and",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Ablation Study",
      "text": "Table VII presents the results of the ablation experiments conducted on the proposed multimodal fusion framework. In these experiments, LSTMSA denotes the use of self-attention with LSTM for feature extraction, LSTMMSA represents the application of a multi-scale attention mechanism with LSTM in the fusion layer, and proposed architecture, EMO-MSASE indicates the integration of both multi-scale attention and a Squeeze-and-Excitation (SE) block for each domain prior to domain fusion.\n\nThe results provide clear evidence of the benefits introduced by the EMO-MSASE architecture compared to the LSTMSA and LSTMMSA models. Across all domain combinations and both valence and arousal detection, EMO-MSASE consistently outperforms the other models. For valence detection, the fusion of trunk and head signals achieves the highest accuracy of 87.14% for G2 V, significantly surpassing the best results from LSTMMSA (84.28%) and LSTMSA (80.56%). Similarly, in arousal detection, EMO-MSASE delivers superior performance, with 65.15% accuracy for G1 A and 63.53% for G2 A when combining all domains, again outperforming LSTMMSA and LSTMSA. The addition of the multi-scale attention mechanism, along with the Squeeze-and-Excitation (SE) blocks, enhances the model's capacity to capture complex patterns across domains, leading to the observed improvements in accuracy. Notably, the combination of head and peripheral domains in EMO-MSASE also shows strong performance, particularly in valence detection, highlighting the importance of multi-modal fusion in this task. These results underline the efficacy of integrating domain-specific SE blocks and multi-scale attention, positioning EMO-MSASE as the most effective model in the presented framework.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "E. Loso And Different Fusion Technique Results",
      "text": "As detailed in II-D, we explored various fusion techniques for integrating multi-modal data, with a primary focus on feature-level fusion in our proposed deep learning architecture. To further enhance the analytical robustness, we also employed decision-level fusion methods, utilizing both sum and max strategies. The sum fusion technique aggregates the probabilistic outputs from multiple classifiers, summing the probabilities for each emotion and selecting the label with the highest cumulative probability. In contrast, the max strategy identifies the highest individual probability across classifiers and designates that as the final prediction. In this analysis, we focus exclusively on the scenario involving the combination of all three domains. Additionally, we computed the Leave-One-Subject-Out (LOSO) accuracy score to further assess the model's generalizability.  The results in Table VIII highlight the performance loss associated with decision-level fusion (DL) techniques compared to modality-level fusion (ML) for both valence and arousal detection using the EMO-MSASE framework. For valence detection, ML shows superior performance, and switching to DL-sum leads to an accuracy loss of 6.64% (from 86.77% to 80.13%) for G2 V, while DL-max results in an even larger drop of 12.56% (from 86.77% to 74.21%). Similarly, for arousal detection, ML significantly outperforms both DL methods, with DL-sum and DL-max incurring accuracy losses of 8.91% and 12.39%, respectively, when compared to ML for G1 A. These results demonstrate that while decision-level fusion can still achieve reasonable performance, it introduces notable accuracy degradation compared to the more comprehensive integration provided by modality-level fusion. This reinforces the advantage of ML in achieving more reliable and accurate emotion detection across multiple domains.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Fusion",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vi. Discussion",
      "text": "In this section, we present key findings derived from our experimental analysis. As shown in Fig.  5 , G2 V labelling yields superior performance for valence detection, whereas G1 with majority labelling proves more effective for arousal detection, outperforming G2 A. Overall, majority labelling consistently demonstrates robust results for both valence and arousal when considering participant ratings (G1) from the SAM scale. Another critical observation is the superior performance of head domain signals compared to those from the trunk and peripheral domains in both valence and arousal detection. When evaluating single-domain performance, the head domain exhibits markedly better accuracy for both metrics, aligning with the trends observed in unimodal analysis, where majority labelling again outperforms other labelling strategies. For multi-domain performance analysis, we selected the best-performing signal combinations from each domain. The results indicate that combining signals from all three domains yields the highest accuracy for valence detection. However, for arousal, the head domain alone produces the best performance. Furthermore, the ablation study highlights the substantial impact of incorporating multi-scale attention mechanisms and Squeeze-and-Excitation (SE) blocks into the model architecture, significantly enhancing accuracy compared to conventional approaches. Finally, the evaluation of different fusion techniques reveals that modality-level fusion consistently outperforms decision-level fusion strategies within the proposed EMO-MSASE framework, confirming the efficacy of our fusion strategy for multi-modal emotion detection.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vii. Limitations",
      "text": "This study has certain limitations. First, the dataset size is relatively small, comprising only 23 participants. Given the potential for subjective variability in participant-reported emotions in response to the same video stimuli, the limited number of participants may result in insufficient data for the model to effectively learn and generalize emotion patterns. In future work, we aim to expand the participant pool to mitigate this limitation. Additionally, when collecting data, we grouped the first eight videos into four pairs, having participants watch two videos consecutively and then provide a single emotion rating. For analysis, we applied the same rating to both videos in each pair. However, we recognize that this approach may not fully capture the emotional nuances participants experience, as they might have felt different or mixed emotions while watching each video. In future experiments, we plan to show one video at a time and allow participants to provide distinct ratings for each video, thereby improving the granularity and accuracy of the collected data. Furthermore, while our model incorporates attention mechanisms to enhance performance, there is room for improvement. Future work should focus on developing more rigorous and sophisticated attention mechanisms to further improve the accuracy of emotion detection across multiple modalities.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "This study presented EMO-MSASE, an advanced multimodal deep learning architecture for the classification of valence and arousal. By leveraging multi-scale attention mechanisms and Squeeze-and-Excitation (SE) blocks within a multidomain framework, our approach demonstrated the capability to capture nuanced temporal features and enhance signal integration across head, trunk, and peripheral domains. Our work encompassed the full workflow, from hardware selection, system setup, and configuration, to participant recruitment, data acquisition, data cleaning, and preprocessing, ensuring a robust and reliable dataset aligned with real-world applications. Through rigorous evaluations, including Group K-Fold and Leave-One-Subject-Out cross-validation, we showed that integrating modalities from these domains significantly improved emotion classification accuracy, particularly when combining head and trunk data for valence and relying on head data for arousal.\n\nOur findings revealed that majority labeling, explained in Table  I  consistently enhanced classification accuracy, with valence detection achieving the highest accuracy under expertbased (G2) labeling, while arousal performed optimally with participant ratings (G1). The ablation study further confirmed the impact of multi-scale attention and SE blocks, with EMO-MSASE surpassing traditional LSTM with selfattention (LSTMSA) and LSTM with multi-scale attention (LSTMMSA) approaches in capturing complex, domainspecific patterns.\n\nMoreover, the superior performance of modality-level fusion over decision-level fusion underscores the benefit of deeper multimodal integration in emotion recognition. The EMO-MSASE framework not only advances the capabilities of affective computing but also provides a robust, scalable approach for applications in adaptive virtual environments, and humancomputer interaction. These results highlight the potential of EMO-MSASE to set a new benchmark in multimodal emotion recognition, paving the way for further exploration and innovation in multi-domain deep learning architectures.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Data collection arrangement for a single participant.",
      "page": 4
    },
    {
      "caption": "Figure 2: (b) provides a schematic overview of the data collection",
      "page": 4
    },
    {
      "caption": "Figure 2: (c) illustrates the architecture of",
      "page": 4
    },
    {
      "caption": "Figure 2: End-to-end data collection setup. (a) a participant wearing the Equivital Vest (inside his shirt), Empatica EmbracePlus and Meta Quest Pro VR headset",
      "page": 5
    },
    {
      "caption": "Figure 1: E. Data Analysis and Preprocessing",
      "page": 5
    },
    {
      "caption": "Figure 3: , highlighting only the best-performing signals from each",
      "page": 6
    },
    {
      "caption": "Figure 3: Proposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM",
      "page": 7
    },
    {
      "caption": "Figure 4: Proposed multi-attention (MSA) based LSTM feature extractor for a",
      "page": 8
    },
    {
      "caption": "Figure 4: Let X ∈RT ×F represent the input",
      "page": 8
    },
    {
      "caption": "Figure 5: illustrates the results of individual signal performance for",
      "page": 9
    },
    {
      "caption": "Figure 5: Comparison of unimodal valence and arousal accuracy across four different cases (general, majority, males only and G2) for three domains. Blue",
      "page": 10
    },
    {
      "caption": "Figure 3: Combination",
      "page": 11
    },
    {
      "caption": "Figure 5: , G2 V labelling",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kolambahewage, Member,": "Abstract—Emotion\nrecognition\nis\nsignificantly\nenhanced\nby",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "nature necessitate more sophisticated approaches, prompting a"
        },
        {
          "Kolambahewage, Member,": "integrating multimodal biosignals\nand IMU data\nfrom multi-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "shift\ntoward multimodal systems that\nintegrate data from mul-"
        },
        {
          "Kolambahewage, Member,": "ple\ndomains.\nIn\nthis\npaper, we\nintroduce\na\nnovel multi-scale",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "tiple sources to improve classification accuracy and robustness."
        },
        {
          "Kolambahewage, Member,": "attention-based LSTM architecture, combined with Squeeze-and-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "Physiological\nsignals—such as electrocardiograms\n(ECG),"
        },
        {
          "Kolambahewage, Member,": "Excitation (SE) blocks, by leveraging multi-domain signals from",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "the head (Meta Quest Pro VR headset),\ntrunk (Equivital Vest),",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "electrodermal\nactivity\n(EDA),\nand\nheart\nrate\n(HR)—have"
        },
        {
          "Kolambahewage, Member,": "and peripheral (Empatica Embrace Plus) during affect elicitation",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "demonstrated\nstrong\ncorrelations\nwith\nemotional\narousal,"
        },
        {
          "Kolambahewage, Member,": "via visual\nstimuli. Signals\nfrom 23 participants were\nrecorded,",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "stress, and other affective states\n[2],\n[6],\n[1]\n,\n[7]. Similarly,"
        },
        {
          "Kolambahewage, Member,": "alongside\nself-assessed valence\nand arousal\nratings\nafter\neach",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "motion data\ncaptured by inertial measurement units\n(IMUs)"
        },
        {
          "Kolambahewage, Member,": "stimulus. LSTM layers\nextract\nfeatures\nfrom each modality,",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "has been employed to infer emotions through analysis of body"
        },
        {
          "Kolambahewage, Member,": "while multi-scale\nattention captures fine-grained temporal de-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "pendencies, and SE blocks recalibrate feature importance prior",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "movements [8], [9]. Eye-tracking, which provides insight\ninto"
        },
        {
          "Kolambahewage, Member,": "to\nclassification. We\nassess which\ndomain’s\nsignals\ncarry\nthe",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "gaze behavior and visual\nfocus,\nfurther contributes contextual"
        },
        {
          "Kolambahewage, Member,": "most distinctive\nemotional\ninformation during VR experiences,",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "understanding\nof\nemotional\nstates\n[10],\n[11]. Despite\nthe"
        },
        {
          "Kolambahewage, Member,": "identifying key biosignals contributing to emotion detection. The",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "strengths\nof\nthese\nindividual modalities,\neach\nhas\ninherent"
        },
        {
          "Kolambahewage, Member,": "proposed architecture, validated in a user\nstudy, demonstrates",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "limitations when used in isolation. For example, while phys-"
        },
        {
          "Kolambahewage, Member,": "superior performance\nin classifying\nvalance\nand arousal\nlevel",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "(high /\nlow), showcasing the efficacy of multi-domain and multi-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "iological\nsignals\ncapture\nautonomic nervous\nsystem activity,"
        },
        {
          "Kolambahewage, Member,": "modal\nfusion with\nbiosignals\n(e.g., TEMP, EDA) with\nIMU",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "they do not account\nfor contextual or environmental\nfactors"
        },
        {
          "Kolambahewage, Member,": "data (e.g., accelerometer)\nfor emotion recognition in real-world",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "that may be captured through motion data or gaze tracking."
        },
        {
          "Kolambahewage, Member,": "applications.",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "To address\nthese\nlimitations, multimodal data\nintegration"
        },
        {
          "Kolambahewage, Member,": "Index Terms—Emotion recognition,\nvalance,\narousal, multi-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "has\ngained\nsignificant\nattention\n[3],\n[11],\n[1]. Multimodal"
        },
        {
          "Kolambahewage, Member,": "model, multi-domain, LSTM, multi-scaled attention, SE block",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "systems\nleverage\nthe\ncomplementary\nstrengths\nof\ndifferent"
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "data streams, providing a more comprehensive understanding"
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "of\nemotional\nstates. Recent\nadvances\nin deep learning have"
        },
        {
          "Kolambahewage, Member,": "I.\nINTRODUCTION",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "enabled\nthe\nefficient\nfusion\nof\nhigh-dimensional\ndata\nfrom"
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "multiple modalities,\noffering\nsignificant\nimprovements\nover"
        },
        {
          "Kolambahewage, Member,": "research",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "E MOTION recognition has emerged as a critical",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "traditional machine learning techniques\nin feature extraction"
        },
        {
          "Kolambahewage, Member,": "in\ndeep",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "and pattern recognition."
        },
        {
          "Kolambahewage, Member,": "learning\nand\nthe\nincreasing\navailability\nof multimodal\ndata",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "In\nthis work, we\npropose\na\nnovel\nemotion\nrecognition"
        },
        {
          "Kolambahewage, Member,": "sources\n[1],\n[2],\n[3],\n[4]. Affect\nrecognition\nis\ncentral\nto",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "framework that integrates data from three distinct devices, each"
        },
        {
          "Kolambahewage, Member,": "various\napplications\nin\nhuman-computer\ninteraction\n(HCI),",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "corresponding to a different physiological and motion domain:"
        },
        {
          "Kolambahewage, Member,": "healthcare, gaming, and adaptive systems, where an accurate",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "the trunk, head, and peripheral. The devices employed include"
        },
        {
          "Kolambahewage, Member,": "understanding of\nemotional\nstates\ncan significantly enhance",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "a wearable vest (trunk domain), a virtual reality (VR) headset"
        },
        {
          "Kolambahewage, Member,": "user\nexperience\nand\nsystem responsiveness.\nTraditionally,",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "(head domain), and a wrist-worn device (peripheral domain)."
        },
        {
          "Kolambahewage, Member,": "emotion detection has relied on unimodal data streams, such",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "Each device contributes unique data modalities—physiological"
        },
        {
          "Kolambahewage, Member,": "as physiological signals, motion data, or facial expressions [5].",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "signals (e.g., ECG and HR) from the vest, motion data from the"
        },
        {
          "Kolambahewage, Member,": "However, human emotions’\ninherent complexity and dynamic",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "wrist via accelerometers, and feature-extracted data from the"
        },
        {
          "Kolambahewage, Member,": "Pubudu\nL.\nIndrasiri\nand\nBipasha\nKashyap\nand\nPubudu\nN.\nPathi-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "VR headset. By incorporating these multimodal inputs, we aim"
        },
        {
          "Kolambahewage, Member,": "rana\nare\nwith\nthe\nSchool\nof\nEngineering,\nDeakin\nUniversity, Waurn",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "to capture emotional states with greater precision,\nleveraging"
        },
        {
          "Kolambahewage, Member,": "Ponds, VIC 3216, Australia\n(e-mails:\n{pranpatidewage,\nb.kashyap,\npub-",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "the complementary nature of data across the trunk, head, and"
        },
        {
          "Kolambahewage, Member,": "udu.pathirana}@deakin.edu.au).",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "Bahareh Nakisa is with the School of\nInformation Technology, Faculty of",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "peripheral domains."
        },
        {
          "Kolambahewage, Member,": "Science Engineering and Built Environment, Deakin University, Burwood,",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "This multi-domain\napproach\naddresses\nseveral\nkey\nchal-"
        },
        {
          "Kolambahewage, Member,": "VIC 3125, Australia (e-mail: bahar.nakisa@deakin.edu)",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "lenges in affect recognition. By capturing physiological signals"
        },
        {
          "Kolambahewage, Member,": "Kiran Ijaz is with the Wellbeing-supportive Technology Laboratory, School",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "of Electrical and Information Engineering, The University of Sydney, NSW",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "from the trunk, motion data from the wrist, and gaze behaviour"
        },
        {
          "Kolambahewage, Member,": "2006 Australi\n(e-mail: kiran.ijaz@sydney.edu.au)",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "from the head, we construct a more holistic view of emotional"
        },
        {
          "Kolambahewage, Member,": "Chandima Kolambahewage is with the School of Engineering, Faculty of",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "states,\nthus\novercoming\nthe\nlimitations\nof\nsingle-modality"
        },
        {
          "Kolambahewage, Member,": "Science Engineering and Built Environment, Deakin University, Waurn Ponds,",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": ""
        },
        {
          "Kolambahewage, Member,": "VIC 3216, Australia (e-mail: c.kolambahewage@deakin.edu)",
          "IEEE\nIEEE, Bahareh Nakisa, Kiran Ijaz, Pubudu N. Pathirana, Senior Member,": "systems. Additionally,\nthe fusion of these modalities enables a"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "more accurate classification of complex emotional\nresponses,",
          "2": "B. Emotion Detection Using Motion Signals"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "particularly in dynamic or\ninteractive\nenvironments\nsuch as",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Emotion recognition research based on human motion has"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "virtual\nreality.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "attracted\nsignificant\nattention within\nthe\nfield\nof\nhuman-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "This\npaper\npresents\nthe methodology\nfor\ncollecting\nand",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "computer\ninteraction, particularly with the\nadvancements\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "processing data from three critical domains: peripheral,\ntrunk,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "IMU sensors, which\nhave\nfacilitated\nefficient\nand\nseamless"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\nhead,\nand\nthe\ndevelopment\nof\na\ndeep\nlearning-based",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "interaction between humans and devices\n[8],\n[9],\n[17],\n[18]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal architecture for valence and arousal detection. Our",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "IMU sensors,\nsuch as accelerometers\n(ACC) and gyroscopes"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "proposed\narchitecture\nintegrates Long\nShort-Term Memory",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "(GYRO), capture crucial motion-related data that can be used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(LSTM) networks with multi-scale attention mechanisms\nfor",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "to infer\nemotional\nstates. Recent\nstudies have\nexplored the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature extraction from unimodal signals within each domain.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "use of wearable inertial devices attached to various body parts"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Domain-wise\nfusion\nis\nthen\napplied,\nfollowed\nby Squeeze-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "to collect\nemotional\ninformation through human movement."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and-Excitation (SE) blocks to dynamically recalibrate domain",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Hashmi et al. [8] employed a smartphone worn on the chest\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "outputs. The recalibrated outputs from the three domains are",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "record human gait signals and used spectro-temporal\nfeatures"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subsequently fused into a multimodal vector, which is passed",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "from stride\nsignals\nfor\nemotion recognition,\nidentifying six"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to the output layer for emotion classification. We provide com-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "basic\nemotions\nusing Random Forest\nand SVM classifiers."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "prehensive experimental results that highlight the effectiveness",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Similarly, Gravina et al. [9] leveraged sensor-level and feature-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nthis multi-domain, multimodal\napproach,\nshowcasing\nits",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "level fusion to monitor in-seat activities, which reflect psycho-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "superiority over unimodal systems in terms of both accuracy",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "logical and emotional states,\nthough the handcrafted features"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and robustness, with substantial performance gains in emotion",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "required labour-intensive design. Chang et al. [19] used intelli-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "detection.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "gent\ninertial sensors to recognize emotions during badminton"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "play,\nthough prolonged activity resulted in physical\nfatigue"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "II. RELATED WORKS",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "that affected the accuracy of emotion detection. Furthermore,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In\nrecent\nyears,\naffect\nrecognition\nhas\ngarnered\nsignifi-",
          "2": "recent advancements such as those proposed by Feng et al. [20]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cant\nattention, particularly with the\nrise of multimodal data",
          "2": "have integrated motion and emotion recognition into intelligent"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sources\nand\nadvanced\ndeep\nlearning\ntechniques\n[12],\n[13],",
          "2": "wearable systems using multi-sensor fusion, enabling real-time"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[14]. Numerous studies have explored various approaches for",
          "2": "interaction and feedback in virtual environments via Digital"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognizing\naffective\nstates\nusing\nphysiological\nsignals\nand",
          "2": "Twin technology, significantly enhancing user experience and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "additional modalities such as motion or environmental sensors.",
          "2": "emotion\nrecognition\naccuracy. These\nstudies\nhighlight\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "increasing potential of\nIMU-based motion signals in emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "recognition, though challenges such as feature extraction com-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Emotion Detection Using Physiological Signals",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "plexity and physical fatigue remain areas of ongoing research."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Physiological signals have been extensively explored in the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "field of affective computing for emotion detection, given their",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "C. Emotion Detection Using Eye Tracking Data"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "strong\ncorrelation with\nemotional\nstates. Among\nthe most",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "commonly\nutilized\nsignals\nis\nthe\nelectrocardiogram (ECG),",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Humans interact with their environment, with each elicited"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which provides insights into heart rate variability (HRV), a key",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "emotion being closely tied to its specific context [21]. Ptaszyn-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "indicator of autonomic nervous system activity and emotional",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "ski et al. [22] highlighted the importance of incorporating con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arousal\n[2],\n[6],\n[7]. Electrodermal\nactivity (EDA),\nalso re-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "textual analysis into emotion processing. Eye-tracking involves"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ferred to as galvanic skin response (GSR), measures skin con-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "identifying a user’s gaze point or\nfocus on a specific visual"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ductance and reflects sympathetic nervous system responses to",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "stimulus. An eye-tracker, a device designed for\nthis purpose,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotional stimuli, making it a vital\ntool\nfor detecting arousal",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "measures\neye\nposition\nand movements\n[23]. As\na\nsensor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and stress\n[1],\n[7]. Additionally, blood volume pulse (BVP),",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "technology,\neye-tracking\nis\nversatile,\napplicable\nin\ndiverse"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "typically measured via photoplethysmography (PPG), has been",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "configurations and uses, as demonstrated by Singh et al. [24]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "widely used to assess heart\nrate\nand peripheral blood flow,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Furthermore, other studies explore eye movements as potential"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "contributing to the detection of emotional responses related to",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "indicators for\nrecognizing emotions [10],\n[11]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "stress and anxiety [1]. Other crucial signals include respiration",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "rate\n(RR), which varies with emotional\nstates\nsuch as\nfear",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "D. Multimodal Emotion Detection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "or\nrelaxation, and electromyography (EMG), which captures",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "facial muscle movements\nlinked\nto\nexpressions\nof\nemotion",
          "2": "Multimodal\nfusion has captured significant\ninterest across"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[15]. Moreover,\nelectroencephalogram (EEG)\nhas\nbeen\nex-",
          "2": "various\nresearch\ndomains\ndue\nto\nits\nbroad\napplicability\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tensively\nemployed\nto monitor\nbrainwave\nactivity,\noffering",
          "2": "areas\nsuch\nas\nemotion\nrecognition,\nevent\ndetection,\nimage"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "insights\ninto the valence and arousal dimensions of emotion",
          "2": "segmentation, and video classification [25]. Fusion strategies"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[2] , [7], [3], [16]. Finally, skin temperature (ST) and heart rate",
          "2": "are\ntraditionally\ncategorized\nby\nthe\nfusion\nlevel,\nincluding:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(HR) have also proven useful in detecting stress and emotional",
          "2": "1)\nfeature-level\nfusion (early fusion), 2) decision-level\nfusion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arousal,\nfurther\ncontributing\nto\na\nholistic\nunderstanding\nof",
          "2": "(late fusion), and 3) hybrid multimodal fusion. With advance-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "affective states [1]. These signals, often used in combination,",
          "2": "ments\nin deep learning, an increasing number of\nresearchers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "provide a rich, multimodal approach to accurately capturing",
          "2": "are\nleveraging deep learning frameworks\nto enhance multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and classifying complex emotional experiences.",
          "2": "modal\nfusion approaches."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Feature-level\nfusion is a widely adopted approach for\ninte-",
          "3": "•\nIdentifies Key Emotional Domains: Although\nemotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "grating multiple modalities, where features from each modality",
          "3": "originates\nfrom the\ncentral\nnervous\nsystem,\nemerging"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "are combined into a high-dimensional representation and then",
          "3": "research supports the idea that physiological signals from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "processed as a unified input by models [26],\n[3],\n[11],\n[1].",
          "3": "various body domains such as the head [10],\n[11],\ntrunk"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Decision-level\nfusion,\nby\ncontrast,\ninvolves\nthe\nuse\nof",
          "3": "[2],\n[6],\n[7],\nand\nperipheral\n[?],\n[19]\nsystems,\noffer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multiple classifiers and their aggregation,\nfrequently through",
          "3": "unique\nand\ncomplementary\nemotional\ninsights. When"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ensemble\nlearning\ntechniques\n[27]. This\napproach merges",
          "3": "these signals are integrated, they significantly enhance the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "individual classifier outputs into a single decision. Techniques",
          "3": "accuracy and robustness of emotion recognition systems."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "such as MAX fusion, SUM fusion, and fuzzy integral\nfusion",
          "3": "Our\ncomprehensive\nanalysis\nof\nthese\ndomains within"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "are commonly employed in multimodal emotion recognition,",
          "3": "the\nsame\nexperimental\nsetup\nelucidates which\nsignals"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "highlighting the complementary nature of EEG and eye move-",
          "3": "contribute most effectively to emotional\ninference during"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ment\nfeatures through confusion matrix analysis [28],\n[29].",
          "3": "VR experiences, ultimately advancing the precision of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "emotion recognition in VR environments."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "E. Emotion Detection Using Multi-Domain Sensors",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "•\nPerforms Biosignal Importance Analysis: Using advanced"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Emotion recognition using IMU sensors and biosensors in",
          "3": "techniques\nsuch as deep learning, we pinpoint\nspecific"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "VR environments has emerged as a critical area of\nresearch,",
          "3": "biosignals within the most\ninformative domain that con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with a\nsignificant\nfocus on analyzing signals\nfrom isolated",
          "3": "tribute most significantly to emotion recognition, enhanc-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domains such as head,\ntrunk, and peripheral sensors. Histori-",
          "3": "ing model\ninterpretability and optimization."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cally, emotion detection efforts have been domain-specific [5].",
          "3": "•\nPropose a novel deep learning-based multi-modal archi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For example, head-mounted sensors, predominantly integrated",
          "3": "tecture\nthat\nleverages multi-scale\nattention mechanisms"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "into VR headsets, capture signals like eye movement and facial",
          "3": "and Squeeze-and-Excitation (SE) blocks. It captures fine-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EMG to infer emotional\nstates. Trunk-mounted sensors have",
          "3": "grained dependencies across modalities and dynamically"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "primarily focused on physiological signals such as HRV and",
          "3": "recalibrates\nfeature representations\nto enhance accuracy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "respiration, while peripheral devices, particularly wrist-worn",
          "3": "and robustness in emotion detection."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sensors, monitor EDA,\nskin temperature, and accelerometry.",
          "3": "• Enables Multimodal Data Fusion: We assess the perfor-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "While these single-domain approaches have demonstrated ef-",
          "3": "mance of a fused multi-domain model to evaluate whether"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fectiveness in capturing emotional cues,\nthey inherently limit",
          "3": "combining data across domains yields superior accuracy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the understanding of how emotions are represented across the",
          "3": "for\nemotion\ndetection\ncompared\nto\nsingle-domain\nap-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "body in different\nregions [1],\n[5].",
          "3": "proaches."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "However,\na\ncritical\ngap\nin\nthe\ncurrent\nliterature\nis\nthe",
          "3": "This multi-domain framework provides a scalable and pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lack of\ncomparative\nstudies\nthat\nexamine\nthe\nrelative\neffec-",
          "3": "cise\ntool\nfor VR emotion recognition research, with impli-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tiveness\nof\nthese\ndomains\n(head,\ntrunk,\nand\nperipheral)\nin",
          "3": "cations\nfor\nenhanced\naffective\ncomputing,\nuser\nexperience"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition tasks. Specifically, no substantial research",
          "3": "analysis, and behavioural studies in immersive environments."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "has been conducted to systematically compare the emotional",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "III. EXPERIMENTAL PROTOCOL"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information\ncaptured\nby\nsensors\nin\nthese\ndistinct\nbody\nre-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "A. Video Stimuli Selection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gions within the\nsame\nexperimental\nsetup. Furthermore,\nthe",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "literature has not\nadequately explored the potential benefits",
          "3": "Thirteen 360-degree videos were sourced from the public"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of multimodal\nsensor\nfusion, where\nsignals\nfrom multiple",
          "3": "database described in [32], specifically chosen for their ability"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domains are combined to exploit complementary features and",
          "3": "to\nevoke\ntargeted\nemotional\nresponses. Due\nto\nthe\nvaried"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "improve emotion detection accuracy. This omission leaves a",
          "3": "lengths of\nthese original videos,\nthree domain experts—two"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gap in understanding the holistic contribution of multi-domain",
          "3": "psychologists\nand\none\ncomputer\nscientist—selected\nspecific"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals in representing complex emotional states, particularly",
          "3": "intervals within each video that best\nrepresented the intended"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in immersive VR settings.",
          "3": "emotional\nstate\nfor viewers\n[30],\n[31]. This\nstandardization"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "ensured uniform stimulus duration across\nall videos,\nreduc-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F\n. Motivations and Contributions",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "ing variability in the experimental\nsetup and maintaining an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Building on the identified gaps\nin single-domain emotion",
          "3": "efficient overall study duration."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,\nthis study aims to leverage the potential of multi-",
          "3": "Table I lists the videos used, detailing the selected intervals"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "domain\nsensor\nfusion\nin VR environments. By\nsystemati-",
          "3": "and associated emotional quadrants (LALV = low arousal, low"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cally\ncomparing\nsignals\nfrom the\nhead,\ntrunk,\nand\nperiph-",
          "3": "valence; LAHV = low arousal, high valence; HALV = high"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eral\nregions, we seek to determine whether\nthe combination",
          "3": "arousal,\nlow valence; HAHV = high arousal, high valence)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nsignals\nfrom these diverse domains\ncan provide\na more",
          "3": "Video clips were categorized by valence and arousal\nlevels,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "comprehensive and accurate representation of emotional states.",
          "3": "grouping\nclips\nof\nsimilar\nemotional\ntones within\nthe\nsame"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Additionally,\nthis work explores\nthe specific contributions of",
          "3": "quadrant.\nParticipants were\npresented with\na\nprinted\nSelf-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "each domain and evaluates\nthe benefits of\nintegrating these",
          "3": "Assessment Manikin (SAM)\nscale reference,\nratings < 4 as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals for more precise emotion detection.",
          "3": "low valence (LV) and low arousal\n(LA), and ratings > 4 as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "This\nstudy advances multi-domain inference\nin VR emo-",
          "3": "high valence (HV) and high arousal (HA) and instructed to rate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion recognition by presenting a framework, within the same",
          "3": "each video group on these dimensions after viewing, allowing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "experimental setup,\nthat:",
          "3": "for consistent assessment of emotional\nresponses (Figure 1)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": ""
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": ""
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": ""
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Title (ID)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": ""
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Jailbreak (68)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "War knows no nation (20)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "The displaced (18)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Solitary confinement\n(16)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Walk the tight\nrope (69)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Puppies host SourceFed for a day (50)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Malaekahana Sunrise (32)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Great ocean road (22)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "The fight\nto save threatened species (12)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Instant Caribbean vacation (23)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Seagulls (27)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "The margins (13)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": ""
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": "Through Mowgli’s Eyes (73)"
        },
        {
          "AROUSAL), AND RATINGS FROM 4-7 WERE CLASSIFIED AS HIGH (HV FOR VALENCE, HA FOR AROUSAL). THE MAJORITY LABEL FROM PARTICIPANTS’": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "B. Participants"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "In\nthis\nstudy,\na\ncohort\nof\n23\nhealthy\nparticipants was"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "recruited\nthrough\ndigital\nand\nphysical\nadvertisements\ndis-"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "played across social media platforms and within the university"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "campus. A substantial portion of\nthe\nsample\n(75.0%, n=18)"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "comprised male participants, while the remaining 25.0% (n=6)"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "were female. The age of participants\nranged from 21 to 52"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "years,\nyielding\na mean\nage\nof\n28.9\nyears with\na\nstandard"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "deviation\nof\n6.4\nyears. All\nparticipants were\nscreened\nto"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "confirm the\nabsence\nof\nany\npre-existing\ncardiovascular\nor"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "psychological conditions, ensuring a baseline of physiological"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "and psychological health across the sample."
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "C. Experimental Setup"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": ""
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "Three commercially available devices—the Equivital Vest,"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "Empatica Embrace Plus Watch, and Meta Quest Pro VR head-"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "set were employed to collect a range of physiological and be-"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "havioural measurements from participants. The Equivital Vest"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "(specifically, Equivital EQ02+ LifeMonitor) was utilized to"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "capture electrocardiogram (ECG) data along with accelerom-"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "eter\nreadings, while\nthe Empatica Watch\n(specifically, Em-"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "patica E4 Watch recorded EDA, skin temperature, heart\nrate,"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "and accelerometer data. The Equivital GSR add-on accessory"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "was used to capture GSR (Galvanic Skin Response) activity."
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "The Meta Quest Pro VR headset,\nin addition to delivering"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "immersive 3D 360-degree video stimuli, provided critical eye-"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "tracking data, capturing participant gaze patterns and reactions"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "in real-time. Within the same experimental set-up, sensory data"
        },
        {
          "Fig. 1. Data collection arrangement\nfor a single participant.": "from the\ntrunk,\nperipheral\n(wrist),\nand\nhead\ndomains were"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)": "(b)"
        },
        {
          "(a)": "Fig. 2. End-to-end data collection setup. (a) a participant wearing the Equivital Vest (inside his shirt), Empatica EmbracePlus and Meta Quest Pro VR headset"
        },
        {
          "(a)": "with two controllers in his hands. Gel electrodes for measuring GSR activity are attached to the middle and ring fingers of"
        },
        {
          "(a)": "asked to comfortably move around and interact with the VR content while standing,"
        },
        {
          "(a)": "VR application architecture."
        },
        {
          "(a)": "modules. The VR Environment\nserves\nas\nthe\nprimary\nuser"
        },
        {
          "(a)": "interface, offering real-time\ninteraction capabilities\nfor both"
        },
        {
          "(a)": "participants\nand\nresearchers. The Data Acquisition module"
        },
        {
          "(a)": "systematically\ngathers\ndata\nfrom multiple\nheadset\nsensors,"
        },
        {
          "(a)": "while\nthe Data Logger\narchives\nthis\ninformation efficiently,"
        },
        {
          "(a)": "ensuring that data is available for post-experiment analysis."
        },
        {
          "(a)": "The Video Player component enables\nthe playback of au-"
        },
        {
          "(a)": "diovisual stimuli by loading predesignated media files directly"
        },
        {
          "(a)": "from the headset’s file system, delivering an immersive, syn-"
        },
        {
          "(a)": "chronized audiovisual experience."
        },
        {
          "(a)": "An Input Manager module\ntracks\ninputs\nfrom a wireless"
        },
        {
          "(a)": "keyboard, allowing the researcher to control playback and data"
        },
        {
          "(a)": "logging functions dynamically. Through predefined keyboard"
        },
        {
          "(a)": "shortcuts,\nthe\nresearcher\ncan\nnavigate media\nclips,\ninitiate"
        },
        {
          "(a)": "play/pause,\nskip content,\ninsert event markers, and terminate"
        },
        {
          "(a)": "the application. The Feedback Manager enhances usability by"
        },
        {
          "(a)": "providing a\nresponsive visual overlay,\nconfirming command"
        },
        {
          "(a)": "execution (e.g., a pause icon during a video pause)."
        },
        {
          "(a)": "Within\nthe Data Acquisition module,\nthe\nsystem utilizes"
        },
        {
          "(a)": "the Meta XR SDK to interface with various headset sensors."
        },
        {
          "(a)": "The Head Mount Device Tracker provides\nreal-time updates"
        },
        {
          "(a)": "on the headset’s\nspatial position and orientation relative\nto"
        },
        {
          "(a)": "an auto-calibrated reference point. Left\nand right Controller"
        },
        {
          "(a)": "Trackers\ncapture hand movements’ positional\nand rotational"
        },
        {
          "(a)": "data relative to the headset, while the Eye Gaze Trackers record"
        },
        {
          "(a)": "gaze\norientation\nand\nfixation\npoints,\noffering\ninsights\ninto"
        },
        {
          "(a)": "visual attention patterns. The Marker Input Handler logs event"
        },
        {
          "(a)": "markers initiated via the keyboard in CSV format, streamlining"
        },
        {
          "(a)": "data synchronization and subsequent analysis."
        },
        {
          "(a)": "The Data Logger module consists of\ntwo sub-components:"
        },
        {
          "(a)": "the Audio Recorder and CSV Writer. The Audio Recorder cap-"
        },
        {
          "(a)": "tures high-fidelity audio input from the headset’s microphones,"
        },
        {
          "(a)": "saving it\nin .wav format. Simultaneously,\nthe CSV Writer logs"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TABLE II",
          "6": "tailored filtering techniques\nto ensure high-quality data. Ac-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "THE DIFFERENT MODALITIES COLLECTED FROM THREE DOMAINS, THE",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "celerometer\nsignals were filtered\nusing\na\n0.5-20 Hz\nband-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "PERIPHERAL (EMPATICA EMBRACEPLUS), TRUNK (EQUIVITAL VEST),",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "pass filter to eliminate low-frequency drift and high-frequency"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "AND HEAD (META QUEST PRO VR HEADSET), ALONG WITH THEIR FULL",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "NAMES AND CORRESPONDING ABBREVIATIONS AND SAMPLING",
          "6": "noise, preserving relevant motion information. Regarding EDA"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "FREQUENCIES (IN BRACKETS). ONLY THE ABBREVIATIONS ARE USED",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "preprocessing, upsampling from 4 to 64 Hz was performed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "THROUGHOUT THIS PAPER.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "to make both signals\nat\nthe\nequal\nsampling frequency [33]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Peripheral\nTrunk\nHead",
          "6": "A Butterworth filter with\na\ncutoff\nfrequency\nof\n0.5 Hz\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(Empatica EmpracePlus)\n(Equivital Vest)\n(Meta Quest Pro)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "often used. Apply a bandpass filter between 0.5 Hz\nand 4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Accelerometer x\nx\nElectrocardiogram Lead 1\nLEyeposition",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(ACC X, 64 Hz)\n(ECG1, 256 Hz)\n(L EP X)",
          "6": "Hz to remove noise for BVP signal processing. Use a moving"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Accelerometer y\nElectrocardiogram Lead 2\nLEyeposition\ny",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "average to reduce the noise from the temperature readings."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(ACC Y, 64 Hz)\n(ECG2, 256 Hz)\n(L EP Y)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Accelerometer\nz\nLateral Accelerometer\nLEyeposition\nz",
          "6": "For trunk domain data, ECG signals, a 0.5-45 Hz band-pass"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(ACC Z, 64 Hz)\n(LAT ACC, 256 Hz)\n(L EP Z)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "filter was used to retain key waveform features\n(P, QRS, T"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Temperature\nLongitudinal Accelerometer\nREyeposition x",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(TEMP)\n(LONG ACC, 256 Hz)\n(R EP X)",
          "6": "waves) while removing artifacts and noise. Same as the watch,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Electrodermal Activity\nVertical Accelerometer\nREyeposition y",
          "6": "accelerometer signals are filtered using a 0.5-20 Hz band-pass"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(EDA, 4 Hz)\n(VERT ACC, 256 Hz)\n(R EP Y)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "filter. However, GSR data were unreliable for five participants;"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Blood Volume Pulse\nGalvanic Skin Response\nREyeposition z",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(BVP, 64 Hz)\n(GSR, 256 Hz)\n(R EP Z)",
          "6": "removing them would cause data insufficiency. Thus, GSR was"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "excluded to maintain consistency across participants."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "For each physiological,\nIMU, and eye-tracking data input,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "skin conductance (EDA), blood volume pulse (BVP), and skin",
          "6": "we first applied z-score normalization to standardize the data,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "temperature (TEMP). The ECG, or electrocardiogram, records",
          "6": "ensuring that all signals were on a comparable scale. Following"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the electrical activity of the heart over time, providing detailed",
          "6": "this, we\nselected the\nlast 40 seconds of data\nfrom both the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information about\nthe heart’s rhythm and rate by detecting the",
          "6": "peripheral and trunk domains. This selection was based on an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "depolarization and repolarization of\ncardiac muscles. Blood",
          "6": "analysis of the signals, which indicated that\nthe final segment"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "volume pressure (BVP), measured via a photoplethysmogra-",
          "6": "of\nthe\nvideo\nstimuli most\nconsistently\nelicited\nemotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "phy (PPG)\nsensor embedded in the watch,\nreflects\nthe pulse",
          "6": "responses. The extraction of\nthis uniform 40-second segment"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "wave of\nthe heart and the volume of blood flowing through",
          "6": "was necessary to maintain equal\ninput\nlengths across modal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vessels,\noffering\na\nnon-invasive\ninsight\ninto\ncardiovascular",
          "6": "ities,\na\ncritical\nrequirement\nfor\nthe\nproposed\ndeep\nlearning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dynamics. EDA, or electrodermal activity, captures variations",
          "6": "architecture."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in the\nskin’s\nelectrical\nconductance, which is\ninfluenced by",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "After segmentation, we applied a sliding window technique"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sweat gland activity and serves as a physiological marker\nfor",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "with a 2-second window and a 50% overlap to the physio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotional and arousal states. This signal\nis monitored by ap-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "logical and IMU signals. This windowing strategy effectively"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "plying a low-level current between two electrodes on the skin,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "partitioned\nthe\ninput\ndata\nstream into multiple\noverlapping"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "providing\nvaluable\ndata\nabout\nthe\nbody’s\nelectrodermal\nre-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "windows, ensuring fine-grained temporal resolution while pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sponse to external stimuli. Skin temperature (TEMP) is another",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "serving temporal context. Each segmented window was\nthen"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "critical metric, measured using a thermopile infrared sensor,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "represented as a 2D array with dimensions T × F , where T is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which helps monitor\nthe body’s\nthermoregulatory response.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the number of time windows (time stamps), and F represents"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Vascular\nresponses,\nsuch as vasodilation or vasoconstriction,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the size of\nthe data (features)\nin one window, determined by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "result\nin corresponding changes\nin skin temperature, which",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the sampling frequency and window duration. Specifically, for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "can indicate the body’s reaction to various environmental and",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "signals\nfrom the Empatica Embrace Plus, F is\nset\nto 128,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "physiological\nfactors.\nIn contrast,\nIMU data involves motion-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "based on its\nsampling rate of 64 Hz, while for\nthe Equivital"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "related\nsignals\ncaptured\nby\naccelerometers, which measure",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "vest, F is 512,\nreflecting its sampling rate of 256 Hz."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "acceleration\nalong\nthree\naxes\nx,\ny,\nand\nz\non\nthe watch,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "For\nthe Meta Quest Pro VR headset\ndata, we\nextracted"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "providing insights\ninto movement\ndynamics.\nIn addition to",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the final\n2000\ncoordinates\nfrom each\naxis\nof\nboth\nthe\nleft"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the Empatica EmbracePlus\ndata,\nthe Equivital Vest\noffers",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "and\nright\neye\npositions\n(x,\ny,\nz\nfor\neach\neye). The\ndata"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "complementary\naccelerometer measurements,\nincluding\nthe",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "was\nthen segmented into overlapping windows using a slid-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lateral accelerometer, which measures side-to-side movement,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "ing window approach, with 50% overlap, dividing the 2000"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the\nlongitudinal\naccelerometer, which captures\nforward and",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "coordinates\ninto windows of F =200 coordinates\neach. This"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "backward motion, and the vertical accelerometer, which tracks",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "segmentation process provided structured 2D inputs\nfor\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "up-and-down movements. Together,\nthese physiological\nand",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "proposed architecture,\nsimilar\nto the\nrepresentation used for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "motion-related signals provide a comprehensive understanding",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the physiological and IMU data, ensuring compatibility across"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nthe user’s physical and emotional states.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "all\ninput modalities."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The VR headset predominantly tracks eye movement, with",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "left\nand\nright\neye\npositions\n(LEyeposition, REyeposition)",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "IV. SYSTEM ARCHITECTURE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "measured\nin\nthree-dimensional\nspace\n(x,\ny,\nz\ncoordinates).",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These eye position signals provide precise spatial coordinates",
          "6": "The high-level structure of our proposed emotion recogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "at specific time points, enabling us to capture gaze dynamics",
          "6": "tion system, EMO-MSASE: Emotion Detection using Multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and spatial orientation during each experimental\ntask.",
          "6": "Scale Attention and Squeeze-and-Excitation is depicted in Fig."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For\nperipheral\ndomain\nsignal\npreprocessing, we\napplied",
          "6": "3,\nhighlighting\nonly\nthe\nbest-performing\nsignals\nfrom each"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "models and squeeze and excitation block. Only best performing modalities from each domain are outlined."
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "A. Multi-Scale Attention LSTM Feature Extraction\ndevice,\nas determined through experimental\nevaluation. The"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "system is composed of\nthree core components: data prepro-"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "In\nthis\nstage, we\nemployed\nan LSTM architecture\naug-"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "cessing,\nfeature extraction, and emotion recognition. Each of"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "mented with a multi-scale\nattention mechanism to act\nas\na"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "these components plays a crucial\nrole in the overall\nsystem,"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "high-level feature extractor. The LSTM model [34], a special-"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "and their detailed descriptions are provided in the subsequent"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "ized variant of\nrecurrent neural networks (RNNs),\nis adept at"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "sections. EMO-MSASE enhances the framework presented in"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "capturing long-term dependencies within sequential data. The"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "[1], which used a simple attention mechanism. In contrast, we"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "core elements of the LSTM structure include the cell state and"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "introduce multi-scale attention (MSA), which allows the model"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "gating mechanisms;\nthe\ncell\nstate\nfunctions\nas\na\n“memory”"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "to capture patterns\nacross multiple\ntemporal\nscales,\nthereby"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "that\ncarries\ninformation across\ntime\nsteps, while\nthe gating"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "improving its ability to handle varying signal dynamics. Addi-"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "structures control which information is retained or discarded."
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "tionally, the integration of Squeeze-and-Excitation (SE) blocks"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "Specifically,\nthe LSTM comprises three gate types:\nthe forget"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "helps\nrecalibrate features by focusing on the most\nimportant"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "gate,\ninput gate, and output gate. The forget gate selectively"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "features for each domain before constructing the fused feature"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "removes irrelevant\ninformation from prior cell states,\nthe input"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "vector\nfrom three\ndomains,\nfurther\nenhancing\nthe model’s"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "gate updates the cell state with new data from the current input,"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "performance."
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "and the output gate produces\nthe final output based on the"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "updated cell state. Utilizing the LSTM enables us to capture"
        },
        {
          "Fig. 3.\nProposed multi-domain leveraged multimodal deep learning architecture for emotion (valance, arousal) classification using multi-scale attention, LSTM": "temporal\nrelationships within each input modality, yielding a"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "signal for a given modality, where T is the number of timesteps"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "and F is\nthe number of\nfeatures per\ntimestep. The LSTM"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "processes\nthis\nsequence\nand\noutputs\na\nsequence\nof\nhidden"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "states H ∈ RT ×H , where H is\nthe size of\nthe hidden state."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention out",
          "8": "Mathematically,\nthe output of\nthe LSTM can be represented"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "as:"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "exp(u⊤ · hi)": "αi ="
        },
        {
          "exp(u⊤ · hi)": "(cid:80)T"
        },
        {
          "exp(u⊤ · hi)": "j=1 exp(u⊤ · hj)"
        },
        {
          "exp(u⊤ · hi)": "The\nlearnable\ncontext vector u helps highlight\nimportant"
        },
        {
          "exp(u⊤ · hi)": "information by guiding the focus on particular timesteps based"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "on the context of\nthe task."
        },
        {
          "exp(u⊤ · hi)": "For\nthe short-term attention, attention weights αi are com-"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "puted over all T timesteps, producing a weighted sum of\nthe"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "hidden states:"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "T(cid:88) i\nαihi,\nvshort =\nvshort ∈ RH ."
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "=1"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "For the medium-term attention, adjacent timesteps are merged,"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "reducing the sequence length to Tmedium = T\n2 , and attention is"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "applied over\nthe merged sequence with weights βi:"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "Tmedium(cid:88)"
        },
        {
          "exp(u⊤ · hi)": "vmedium =\nβihmerged,i,\nvmedium ∈ RH ."
        },
        {
          "exp(u⊤ · hi)": "i=1"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "Similarly,\nfor\nlong-term attention,\nthe\nsequence\nis\nfurther"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "reduced to Tlong = T\n3 , and attention is applied over this coarser"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "representation with weights γi:"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "Tlong"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "vlong =\nγihmerged,i,\nvlong ∈ RH ."
        },
        {
          "exp(u⊤ · hi)": "(cid:88) i"
        },
        {
          "exp(u⊤ · hi)": "=1"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "The outputs\nfrom the short-term, medium-term, and long-"
        },
        {
          "exp(u⊤ · hi)": "term attention mechanisms are concatenated to form the final"
        },
        {
          "exp(u⊤ · hi)": "feature vector, called the Concatenated Attention Vector (CAV)"
        },
        {
          "exp(u⊤ · hi)": "for\nthe modality:"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "CAV = vmodality = [vshort, vmedium, vlong],\nvmodality ∈ R3H ."
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "Within each domain,\nall modality-specific\nfeature vectors"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "are concatenated to form a domain-specific feature vector. For"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "example,\nin the watch domain:"
        },
        {
          "exp(u⊤ · hi)": ""
        },
        {
          "exp(u⊤ · hi)": "],\nvW = [v1\nvW ∈ RMW ×3H ,\nW , v2\nW , . . . , vMW"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where MW is the number of modalities in the watch domain.",
          "9": "wide\narray\nof\nsignals, we\nselected\nonly\nthe most\nrelevant"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The same process\nis\nfollowed for\nthe vest domain (vV ) and",
          "9": "modalities\nfor our\nanalysis,\nfocused on their\nsignificance\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the VR headset domain (vH ).",
          "9": "emotion detection. We\ncollected a\ntotal of 299 samples\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "After concatenating the modality-specific feature vectors for",
          "9": "each modality from 23 participants. Each participant watched"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "each\ndomain,\na Squeeze-and-Excitation\n(SE)\nblock\n[37]\nis",
          "9": "thirteen videos, completing the SAM Manikin Questionnaire"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "applied to each concatenated domain-specific\nfeature vector",
          "9": "after each video to rate their emotional\nresponses on a scale"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(vdevice). First, a global average pooling operation is applied",
          "9": "from 1 to 7 for both valence\nand arousal. As\nindicated in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to the concatenated domain-specific feature vector:",
          "9": "Table I, we classified ratings < 4 as low valence (LV) and low"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "arousal\n(LA), and ratings > 4 as high valence (HV) and high"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "zdevice = GlobalAvgPool(vdevice),\nzdevice ∈ R3H .",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "arousal\n(HA),\nforming the first ground truth (G1)\nfor valence"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "(G1 A)\nrespectively. The second ground\n(G1 V) and arousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is\nthe result of\nthis global average pooling applied to\nzdevice",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "truth\n(G2)\nof\nvalence\n(G2 V)\nand\narousal\n(G2 A)\nlevels"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the\nfor\na\nspecific device\nconcatenated feature vector vdevice",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "is derived from external\nratings\n(three domain experts,\ntwo"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(watch, vest, or VR headset).",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "psychologists and one computer scientist) based on established"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Next,\ntwo fully connected layers with ReLU and sigmoid",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "criteria from previous research [30]–[32].\nIn constructing the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "activations generate recalibration weights for each device:",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "deep learning model,\nfour distinct cases of\nlabel assignment"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sdevice = σ(W2 · ReLU(W1 · zdevice)),\nsdevice ∈ R3H .",
          "9": "were systematically applied to the collected dataset:\n(1)\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "actual\nresponse of each participant\nfor each video was used"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "represents\nthe\nrecalibration weights\ngenerated\nfor\nsdevice",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "as\nthe\nlabel\n(general),\n(2)\nfor\neach video,\na final\nlabel was"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a\nspecific device\nafter\napplying two fully connected layers",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "determined by the majority response across all 23 participants"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recalibration weights\nare\nthen applied to the\nto zdevice. The",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "(majority)\nas\nin Table\nI,\n(3) only the\nresponses\nfrom male"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "device-specific feature vector:",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "participants were used (males only), and (4)\nthe label values"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vSE\nvSE",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "device = sdevice · vdevice,\ndevice ∈ RMW ×3H .",
          "9": "provided by the selected reference papers (G2 V and G2 A)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "for each video to assign labels for each video. Cases (1),\n(2),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The\nrecalibrated domain-specific\nfeature vectors\nare\nthen",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "and (3) are subclasses of the general labels (G1 V and G1 A)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "concatenated to form a global\nfeature vector:",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "The\nefficacy\nof\nour\nproposed\ndeep\nlearning\narchitecture"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vglobal ∈ R(MW +MV +MH )×3H .",
          "9": "for\nvalence\nand\narousal\ndetection was\nrigorously\nevaluated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W , vSE\nV , vSE\nH ],",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "through two distinct cross-validation techniques: (1) Group K-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Finally,\nthe global\nfeature vector\nis passed through a fully",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "Fold cross-validation and (2) Leave-One-Subject-Out (LOSO)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "connected layer\nfor classification, with softmax activation for",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "cross-validation.\nIn Group K-Fold\ncross-validation,\npartici-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multi-class classification:",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "pants were divided into two distinct groups\nfor\ntraining and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "testing, ensuring that data from the same participant was not"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "y ∈ RC.\ny = softmax(Woutput · vglobal + boutput),",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "used\nin\nboth\nthe\ntraining\nand\ntesting\nsets\nto\nprevent\ndata"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Here,\ny represents\nthe\npredicted\nclass\nprobabilities\nfor C",
          "9": "leakage. This approach maintains subject independence during"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion classes.",
          "9": "model evaluation.\nIn LOSO cross-validation,\nthe model was"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "trained on data from all but one participant, with the left-out"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "participant’s data used for\ntesting. This process was repeated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Deep Learning Setup For the Proposed Architecture",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "for each participant, allowing for a robust assessment of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For\nfeature\nextraction,\neach\ndevice\nutilizes\na\ndedicated",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "model’s generalizability across individuals. For Analysis V-A,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Attention-LSTM network. The LSTM network comprises two",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "V-B, V-C, we employed Group K-Fold cross-validation with 5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "layers,\neach with\n128\nhidden\nunits. The model\nis\ntrained",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "folds. The mean accuracy across the 5 folds was computed to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using binary cross-entropy loss with the AdamW optimizer for",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "provide a robust and comprehensive evaluation of the model’s"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "effective weight updates, a learning rate of 0.001, and a batch",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "performance. To further enhance the evaluation, Analysis V-C"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "size of 16. Training is conducted over 50 epochs, with early",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "was additionally assessed using (LOSO) cross-validation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "stopping based on validation performance to prevent overfit-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ting. All data analytics and deep learning model building and",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "testing were\nconducted on a 13th-generation i7 workstation",
          "9": "A. Uni-modality Performance Analysis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "equipped with an RTX 4080 GPU and 32GB of RAM.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "We\nbegan\nby\nindependently\nevaluating\nthe\nperformance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "of\neach\nsignal modality within\neach\ndomain,\nas\nshown\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "RESULTS\nV.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "Table\nII,\nfor\nthe\ndetection\nof\nvalence\nand\narousal.\nFig\n5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In\nthis\nsection, we\npresent\nthe\nperformance\nof EMO-",
          "9": "illustrates\nthe\nresults\nof\nindividual\nsignal\nperformance\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MSASE. Our\nstudy\nleverages\ndata\nfrom three\ndistinct\ndo-",
          "9": "both\nvalence\n(a)\nand\narousal\n(b)\ncross\nfour\ndistinct\ncases."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mains, head (Meta Quest Pro VR headset),\ntrunk (Equivital",
          "9": "In the context of valence detection,\nthe model demonstrates"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Vest), and peripheral\n(Empatica Embrace Plus watch)\nto ad-",
          "9": "robust performance\nacross\nalmost\nall\nsignals\nfrom the\nthree"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vance emotion recognition. These domains contribute multiple",
          "9": "domains when using G 2 labels, consistently achieving over"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modalities,\nincluding\nIMU data, ECG,\ntemperature, EDA,",
          "9": "70% accuracy. Notably,\nLAT ACC from the\ntrunk,\nEDA"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "BVP, and eye movements as\nin Table II, which are utilized",
          "9": "and TEMP from the\nperipheral\ndomain,\nand L EP Y and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to\nassess\nvalence\nand\narousal. While\nthese\ndevices\noffer\na",
          "9": "R EP Y from the\nhead\ndomain\nperform particularly well"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Males"
        },
        {
          "TABLE III": "0.542831"
        },
        {
          "TABLE III": "0.551434"
        },
        {
          "TABLE III": "0.555967"
        },
        {
          "TABLE III": "0.521739"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "under G 2\nlabeling. However, when\nusing G 1\nlabels,\nthe",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "ities\nfor\nvalence\nand\narousal\ndetection. For\nthe\nperipheral"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "signals\nfrom the trunk and peripheral domains exhibit\nlower",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "domain, we initially considered four key modalities: IMU data"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "accuracy\nacross\nall\nthree\nscenarios\n(general, majority,\nand",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "(accelerometer\nreadings\nalong the X, Y,\nand Z axes), BVP,"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "males). Upon closer examination,\nthe accuracy for\ntrunk and",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "EDA, and TEMP. These six raw signals across four modalities"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "peripheral\nsignals\nis\nslightly\nimproved\nunder\nthe majority",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "yielded\n64\npossible\ncombinations. Table\nIII\nshowcases\nthe"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "labeling\ncase\ncompared\nto\nthe\nother\ntwo.\nIn\ncontrast,\nthe",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "top\nfour\ncombinations\nfor\nboth\nvalence\nand\narousal\ndetec-"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "head domain exhibits\nsignificantly higher performance when",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "tion. For\nthe trunk domain, we analyzed two key modalities:"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "majority labeling is employed. Notably,\nin this scenario, head",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "electrocardiogram (ECG) and IMU data. The ECG data was"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "domain signals outperform those from the trunk and peripheral",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "captured\nfrom two\nleads,\nreferred\nto\nas ECG1\nand ECG2,"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "domains. Specifically, L EP Y and R EP Y independently",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "while the IMU data included lateral acceleration (LAT ACC),"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "achieve close to 70% accuracy for valence detection, indicating",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "longitudinal acceleration (LONG ACC), and vertical acceler-"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "strong predictive power for these signals in alignment with the",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "ation (VERT ACC). The top five performing combinations for"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "majority labeling.",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "valence and arousal detection are summarized in Table IV. For"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "Unlike valence, arousal detection demonstrates superior per-",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "the eye-tracking data, under\nthe eye modality, we utilized six"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "formance with the majority labeling across most signals from",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "features:\nLEyeposition x,\nLEyeposition y,\nLEyeposition z,"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "all three domains. A similar trend is observed for both valence",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "REyeposition x, REyeposition\ny,\nand REyeposition z. The"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "and arousal\nin the head domain, where L EP Y and R EP Y",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "results of\nthe\ntop-performing combinations\nare presented in"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "independently achieve nearly 70% accuracy. In contrast, G2 V",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "Table V."
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "labeling yields comparatively lower performance for signals in",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": ""
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "As\nshown\nin Tables\nIII\nand\nIV,\nhigher\naccuracy\nvalues"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "the\ntrunk and peripheral domains,\nthough the head domain",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": ""
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "for both valence and arousal were consistently observed with"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "performance\nremains\nconsistent with\nthat\nof\nthe majority",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": ""
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "except\nin the\ntrunk domain. For\nG2 V and G2 A labeling,"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "in\nlabeling. LAT ACC emerges as the most predictive signal",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": ""
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "arousal detection,\nthe trunk domain achieved the highest ac-"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "the trunk domain, while EDA stands out as the best-performing",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": ""
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "curacy under majority labeling. In contrast, when using G1 V"
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "signal\nin the peripheral domain.",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": ""
        },
        {
          "0.618365\nACC Z, EDA, TEMP\n0.568475\n0.521739": "",
          "0.611476\n0.735819\n0.598701\n0.581499\n0.605254": "and G1 A labels, valence accuracy was moderate across all"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Trunk signal Combinations",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "LAT ACC, LONG ACC",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ECG1, LAT ACC, LONG ACC",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ECG2, LAT ACC, LONG ACC",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "LAT ACC, LONG ACC, VERT ACC",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "VR headset modality Combination",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "11": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Valence"
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Majority"
        },
        {
          "TABLE V": "0.659614"
        },
        {
          "TABLE V": "0.687317"
        },
        {
          "TABLE V": "0.646172"
        },
        {
          "TABLE V": "0.663179"
        },
        {
          "TABLE V": "0.676973"
        },
        {
          "TABLE V": "0.663063"
        },
        {
          "TABLE V": "0.673641"
        },
        {
          "TABLE V": "0.670076"
        },
        {
          "TABLE V": "0.666628"
        },
        {
          "TABLE V": "0.625132"
        },
        {
          "TABLE V": "0.697545"
        },
        {
          "TABLE V": "0.680245"
        },
        {
          "TABLE V": "0.710570"
        },
        {
          "TABLE V": "0.669959"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "significantly higher valence",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "to other domains across all",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "general case, valence accuracy for the other domains was 55%",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "± 3, while the head data yielded a notably higher accuracy of",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "60% ± 3. Under majority labeling, valence accuracy remained",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "at 57% ± 4, whereas the head data improved substantially to",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "66% ± 3. For male participants, valance accuracy was 54% ±",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "3, compared to a significantly higher 64% ± 3 for head data.",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "Arousal accuracy followed a similar trend, with 55% ± 4 in the",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "general case and 57% ± 3 for the head domain. In the majority",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "case,",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "± 4 for",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "a\nsignificant",
          "results across all conditions.": "achieving"
        },
        {
          "yielding robust": "",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "participants,",
          "results across all conditions.": "accuracy was"
        },
        {
          "yielding robust": "peripheral and trunk, but notably higher",
          "results across all conditions.": ""
        },
        {
          "yielding robust": "data at 65% ± 3. The optimal signal combination accross all",
          "results across all conditions.": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": "Models"
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": "LSTMSA"
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": "LSTMMSA"
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": "EMO-MSASE"
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        },
        {
          "ABLATION STUDY RESULTS FOR VALENCE (G1 V, G2 V) AND AROUSAL (G1 A, G2 A) DETECTION. THE TABLE PRESENTS ACCURACY AND RECALL": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "feature-level\nfusion in our proposed deep learning architec-"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "ture. To\nfurther\nenhance\nthe\nanalytical\nrobustness, we\nalso"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "employed decision-level\nfusion methods, utilizing both sum"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "and max strategies. The sum fusion technique aggregates the"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "probabilistic outputs\nfrom multiple\nclassifiers,\nsumming the"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "probabilities for each emotion and selecting the label with the"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "highest cumulative probability.\nIn contrast,\nthe max strategy"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "identifies\nthe highest\nindividual probability across classifiers"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "and designates that as the final prediction. In this analysis, we"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "focus exclusively on the scenario involving the combination"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "of all\nthree domains. Additionally, we computed the Leave-"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "One-Subject-Out (LOSO) accuracy score to further assess the"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "model’s generalizability."
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "TABLE VIII"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "PERFORMANCE OF THREE DOMAIN COMBINATION FOR VALENCE (G1 V,"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "G2 V) AND AROUSAL (G1 A, G2 A) DETECTION USING"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "EMO-MSASE. THE TABLE INCLUDES RESULTS FOR BOTH K-FOLD AND"
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": "LOSO EVALUATION STRATEGIES."
        },
        {
          "for\nintegrating multi-modal\ndata, with\na\nprimary\nfocus\non": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "notable accuracy degradation compared to the more compre-",
          "13": "developing more rigorous and sophisticated attention mecha-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hensive\nintegration\nprovided\nby modality-level\nfusion. This",
          "13": "nisms\nto further\nimprove the accuracy of emotion detection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reinforces\nthe\nadvantage of ML in achieving more\nreliable",
          "13": "across multiple modalities."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and accurate emotion detection across multiple domains.",
          "13": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "13": "VIII. CONCLUSION"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": "REFERENCES"
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": "[1] K. Yang, C. Wang, Y. Gu,\nZ."
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": "G. Wadley,\nand J. Goncalves,\n“Behavioral"
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": "2021."
        },
        {
          "Built Environment": "[2]",
          "(SEBE-2023-20).": "S. Katsigiannis\nand N. Ramzan,"
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": "recognition through eeg and ecg signals"
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": ""
        },
        {
          "Built Environment": "",
          "(SEBE-2023-20).": "vol. 22, no. 1, pp. 98–107, 2017."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[3]\nS. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,",
          "14": "International\nJoint Conference\non Artificial\nIntelligence\n(IJCAI-09)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "T. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis;",
          "14": "AAAI, 2009, pp. 1469–1474."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using physiological signals,” IEEE transactions on affective computing,",
          "14": "[23]\nT. Strandvall, “Eye tracking in human-computer interaction and usability"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 3, no. 1, pp. 18–31, 2011.",
          "14": "research,” in Human-Computer Interaction–INTERACT 2009: 12th IFIP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[4] X. Zhang, J. Liu, J. Shen, S. Li, K. Hou, B. Hu, J. Gao, and T. Zhang,",
          "14": "TC 13 International Conference, Uppsala, Sweden, August 24-28, 2009,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Emotion recognition from multimodal physiological\nsignals using a",
          "14": "Proceedings, Part\nII 12.\nSpringer, 2009, pp. 936–937."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE\ntransactions\non\nregularized\ndeep\nfusion\nof\nkernel machine,”",
          "14": "[24] H. Singh\nand\nJ. Singh,\n“Human\neye\ntracking\nand\nrelated\nissues: A"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cybernetics, vol. 51, no. 9, pp. 4386–4399, 2020.",
          "14": "review,” International Journal of Scientific and Research Publications,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[5] B. Pan, K. Hirota, Z. Jia, and Y. Dai, “A review of multimodal emotion",
          "14": "vol. 2, no. 9, pp. 1–9, 2012."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition from datasets, preprocessing, features, and fusion methods,”",
          "14": "[25] D. Lahat, T. Adali, and C. Jutten, “Multimodal data fusion: an overview"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Neurocomputing, p. 126866, 2023.",
          "14": "of methods, challenges, and prospects,” Proceedings of\nthe IEEE, vol."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[6]\nT. Fan, S. Qiu, Z. Wang, H. Zhao,\nJ.\nJiang, Y. Wang,\nJ. Xu, T. Sun,",
          "14": "103, no. 9, pp. 1449–1477, 2015."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and N. Jiang, “A new deep convolutional neural network incorporating",
          "14": "[26] Y. Lu, W.-L. Zheng, B. Li, and B.-L. Lu, “Combining eye movements"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in\nattentional mechanisms\nfor\necg\nemotion\nrecognition,” Computers",
          "14": "and eeg to enhance emotion recognition.” in IJCAI, vol. 15.\nBuenos"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Biology and Medicine, vol. 159, p. 106938, 2023.",
          "14": "Aires, 2015, pp. 1170–1176."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[7]\nT. Song, W. Zheng, C. Lu, Y. Zong, X. Zhang,\nand Z. Cui,\n“Mped:",
          "14": "[27]\nZ.-H. Zhou, Ensemble methods:\nfoundations and algorithms.\nCRC"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A multi-modal\nphysiological\nemotion\ndatabase\nfor\ndiscrete\nemotion",
          "14": "press, 2012."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” IEEE Access, vol. 7, pp. 12 177–12 191, 2019.",
          "14": "[28] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[8] M. A. Hashmi, Q. Riaz, M. Zeeshan, M. Shahzad,\nand M. M. Fraz,",
          "14": "performance\nand robustness of multimodal deep learning models\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Motion reveal emotions:\nidentifying emotions from human walk using",
          "14": "multimodal emotion recognition,” IEEE Transactions on Cognitive and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "chest mounted smartphone,” IEEE Sensors Journal, vol. 20, no. 22, pp.",
          "14": "Developmental Systems, vol. 14, no. 2, pp. 715–729, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "13 511–13 522, 2020.",
          "14": "[29] K.-S. Song, Y.-H. Nho,\nJ.-H. Seo,\nand D.-s. Kwon,\n“Decision-level"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[9] C. Ma, W. Li, R. Gravina, J. Du, Q. Li, and G. Fortino, “Smart cushion-",
          "14": "fusion method for emotion recognition using multimodal emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based activity recognition: Prompting users to maintain a healthy seated",
          "14": "nition information,” in 2018 15th international conference on ubiquitous"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "posture,” IEEE Systems, Man, and Cybernetics Magazine, vol. 6, no. 4,",
          "14": "robots (UR).\nIEEE, 2018, pp. 472–476."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 6–14, 2020.",
          "14": "[30]\nJ.-N. Voigt-Antons, E. Lehtonen, A. P. Palacios, D. Ali, T. Kojic, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[10]\nS. Alghowinem, M. AlShehri, R. Goecke, and M. Wagner, “Exploring",
          "14": "S. M¨oller, “Comparing emotional states induced by 360 videos via head-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eye activity as an indication of emotional\nstates using an eye-tracking",
          "14": "mounted display and computer\nscreen,”\nin 2020 twelfth international"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for Science and Information: Extended\nsensor,” in Intelligent Systems",
          "14": "conference on quality of multimedia experience (QoMEX).\nIEEE, 2020,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Selected Results from the Science and Information Conference 2013.",
          "14": "pp. 1–6."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Springer, 2014, pp. 261–276.",
          "14": "[31] Q. Guimard, F. Robert, C. Bauce, A. Ducreux, L. Sassatelli, H.-Y. Wu,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[11]\nL. Mou, C. Zhou, P. Zhao, B. Nakisa, M. N. Rastgoo, R.\nJain,\nand",
          "14": "M. Winckler,\nand A. Gros,\n“Pem360: A dataset of 360 videos with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "W. Gao, “Driver stress detection via multimodal fusion using attention-",
          "14": "continuous\nphysiological measurements,\nsubjective\nemotional\nratings"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based cnn-lstm,” Expert Systems with Applications, vol. 173, p. 114693,",
          "14": "and motion traces,” in Proceedings of the 13th ACM Multimedia Systems"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2021.",
          "14": "Conference, 2022, pp. 252–258."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[12] C. M. T. Khan, N. A. Ab Aziz, J. E. Raja, S. W. B. Nawawi, and P. Rani,",
          "14": "[32] B. J. Li, J. N. Bailenson, A. Pines, W. J. Greenleaf, and L. M. Williams,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Evaluation of machine\nlearning algorithms\nfor\nemotions\nrecognition",
          "14": "“A public database of\nimmersive vr videos with corresponding ratings"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using electrocardiogram,” Emerging Science Journal, vol. 7, no. 1, pp.",
          "14": "of arousal, valence, and correlations between head movements and self"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "147–161, 2022.",
          "14": "report measures,” Frontiers in psychology, vol. 8, p. 2116, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[13]\nP. Sarkar and A. Etemad, “Self-supervised ecg representation learning",
          "14": "[33] V. Chandra, A. Priyarup, and D. Sethia, “Comparative study of physio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for emotion recognition,” IEEE Transactions on Affective Computing,",
          "14": "logical signals from empatica e4 wristband for stress classification,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 13, no. 3, pp. 1541–1554, 2020.",
          "14": "Advances\nin Computing and Data Sciences: 5th International Confer-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[14]\nS. Siriwardhana, T. Kaluarachchi, M. Billinghurst, and S. Nanayakkara,",
          "14": "ence, ICACDS 2021, Nashik, India, April 23–24, 2021, Revised Selected"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Multimodal emotion recognition with transformer-based self supervised",
          "14": "Papers, Part\nII 5.\nSpringer, 2021, pp. 218–229."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature fusion,” Ieee Access, vol. 8, pp. 176 274–176 285, 2020.",
          "14": "Supervised\n[34] A. Graves\nand A. Graves,\n“Long\nshort-term memory,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[15] U. Sarkar, S. Pal, S. Nag, S. Sanyal, A. Banerjee, R. Sengupta,\nand",
          "14": "sequence labelling with recurrent neural networks, pp. 37–45, 2012."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. Ghosh, “A simultaneous eeg and emg study to quantify emotions from",
          "14": "[35] D. Bahdanau, “Neural machine translation by jointly learning to align"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hindustani classical music,” in Recent Developments in Acoustics: Select",
          "14": "and translate,” arXiv preprint arXiv:1409.0473, 2014."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proceedings of\nthe 46th National Symposium on Acoustics.\nSpringer,",
          "14": "[36] X. Zhang, F. Zhuang, W. Li, H. Ying, H. Xiong,\nand S. Lu,\n“Infer-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2021, pp. 285–299.",
          "14": "ring mood instability via\nsmartphone\nsensing: A multi-view learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[16] B. Nakisa, M. N. Rastgoo, D.\nTjondronegoro,\nand V. Chandran,",
          "14": "the 27th ACM International Conference\napproach,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Evolutionary\ncomputation\nalgorithms\nfor\nfeature\nselection\nof\neeg-",
          "14": "on Multimedia, 2019, pp. 1401–1409."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based emotion recognition using mobile sensors,” Expert Systems with",
          "14": "[37]\nJ. Hu, L. Shen,\nand G. Sun,\n“Squeeze-and-excitation\nnetworks,”\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Applications, vol. 93, pp. 143–155, 2018.",
          "14": "Proceedings of\nthe\nIEEE conference on computer\nvision and pattern"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[17]\nZ. Leng, M. Jung, S. Hwang, S. Oh, L. Zhang, T. Pl¨otz, and K. Kim,",
          "14": "recognition, 2018, pp. 7132–7141."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Emotion\nrecognition\non\nthe\ngo: Utilizing wearable\nimus\nfor\nper-",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the 2024 on ACM\nsonalized emotion recognition,”\nin Companion of",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "International Joint Conference on Pervasive and Ubiquitous Computing,",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2024, pp. 537–544.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[18] M. H. Rahmani, M. Symons, O. Sobhani, R. Berkvens, and M. Weyn,",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Emowear: Wearable\nphysiological\nand motion\ndataset\nfor\nemotion",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition and context awareness,” Scientific Data, vol. 11, no. 1, p.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "648, 2024.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] C. Chang, K. Chen, J. Cao, Q. Wu, and H. Chen, “[retracted] analyzing",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the effect of badminton on physical health and emotion recognition on",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the account of smart sensors,” Applied Bionics and Biomechanics, vol.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2022, no. 1, p. 8349448, 2022.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20]\nF. Yu, C. Yu, Z. Tian, X. Liu,\nJ. Cao, L. Liu, C. Du, and M.\nJiang,",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Intelligent wearable system with motion and emotion recognition based",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on digital\ntwin technology,” IEEE Internet of Things Journal, 2024.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21] D. Valtchanov and M. Hancock, “Enviropulse: Providing feedback about",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the expected affective valence of the environment,” in Proceedings of the",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "33rd Annual ACM Conference on human factors in computing systems,",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2015, pp. 2073–2082.",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22] M. Ptaszynski, P. Dybala, W. Shi, R. Rzepka, and K. Araki, “Towards",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "context aware emotional\nintelligence in machines: computing contextual",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the Twenty-First\nappropriateness of affective states,” in Proceedings of",
          "14": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Behavioral and physiological signalsbased deep multimodal approach for mobile emotion recognition",
      "authors": [
        "K Yang",
        "C Wang",
        "Y Gu",
        "Z Sarsenbayeva",
        "B Tag",
        "T Dingler",
        "G Wadley",
        "J Goncalves"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "3",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen",
        "S Li",
        "K Hou",
        "B Hu",
        "J Gao",
        "T Zhang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "5",
      "title": "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods",
      "authors": [
        "B Pan",
        "K Hirota",
        "Z Jia",
        "Y Dai"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "A new deep convolutional neural network incorporating attentional mechanisms for ecg emotion recognition",
      "authors": [
        "T Fan",
        "S Qiu",
        "Z Wang",
        "H Zhao",
        "J Jiang",
        "Y Wang",
        "J Xu",
        "T Sun",
        "N Jiang"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "7",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Motion reveal emotions: identifying emotions from human walk using chest mounted smartphone",
      "authors": [
        "M Hashmi",
        "Q Riaz",
        "M Zeeshan",
        "M Shahzad",
        "M Fraz"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "9",
      "title": "Smart cushionbased activity recognition: Prompting users to maintain a healthy seated posture",
      "authors": [
        "C Ma",
        "W Li",
        "R Gravina",
        "J Du",
        "Q Li",
        "G Fortino"
      ],
      "year": "2020",
      "venue": "IEEE Systems, Man, and Cybernetics Magazine"
    },
    {
      "citation_id": "10",
      "title": "Exploring eye activity as an indication of emotional states using an eye-tracking sensor",
      "authors": [
        "S Alghowinem",
        "M Alshehri",
        "R Goecke",
        "M Wagner"
      ],
      "year": "2013",
      "venue": "Intelligent Systems for Science and Information: Extended and Selected Results from the Science and Information Conference"
    },
    {
      "citation_id": "11",
      "title": "Driver stress detection via multimodal fusion using attentionbased cnn-lstm",
      "authors": [
        "L Mou",
        "C Zhou",
        "P Zhao",
        "B Nakisa",
        "M Rastgoo",
        "R Jain",
        "W Gao"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "12",
      "title": "Evaluation of machine learning algorithms for emotions recognition using electrocardiogram",
      "authors": [
        "C Khan",
        "N Aziz",
        "J Raja",
        "S Nawawi",
        "P Rani"
      ],
      "year": "2022",
      "venue": "Emerging Science Journal"
    },
    {
      "citation_id": "13",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "15",
      "title": "A simultaneous eeg and emg study to quantify emotions from hindustani classical music",
      "authors": [
        "U Sarkar",
        "S Pal",
        "S Nag",
        "S Sanyal",
        "A Banerjee",
        "R Sengupta",
        "D Ghosh"
      ],
      "year": "2021",
      "venue": "Recent Developments in Acoustics: Select Proceedings of the 46th National Symposium on Acoustics"
    },
    {
      "citation_id": "16",
      "title": "Evolutionary computation algorithms for feature selection of eegbased emotion recognition using mobile sensors",
      "authors": [
        "B Nakisa",
        "M Rastgoo",
        "D Tjondronegoro",
        "V Chandran"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition on the go: Utilizing wearable imus for personalized emotion recognition",
      "authors": [
        "Z Leng",
        "M Jung",
        "S Hwang",
        "S Oh",
        "L Zhang",
        "T Plötz",
        "K Kim"
      ],
      "year": "2024",
      "venue": "Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "18",
      "title": "Emowear: Wearable physiological and motion dataset for emotion recognition and context awareness",
      "authors": [
        "M Rahmani",
        "M Symons",
        "O Sobhani",
        "R Berkvens",
        "M Weyn"
      ],
      "year": "2024",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "19",
      "title": "] analyzing the effect of badminton on physical health and emotion recognition on the account of smart sensors",
      "authors": [
        "C Chang",
        "K Chen",
        "J Cao",
        "Q Wu",
        "H Chen"
      ],
      "year": "2022",
      "venue": "Applied Bionics and Biomechanics"
    },
    {
      "citation_id": "20",
      "title": "Intelligent wearable system with motion and emotion recognition based on digital twin technology",
      "authors": [
        "F Yu",
        "C Yu",
        "Z Tian",
        "X Liu",
        "J Cao",
        "L Liu",
        "C Du",
        "M Jiang"
      ],
      "year": "2024",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "21",
      "title": "Enviropulse: Providing feedback about the expected affective valence of the environment",
      "authors": [
        "D Valtchanov",
        "M Hancock"
      ],
      "year": "2015",
      "venue": "Proceedings of the 33rd Annual ACM Conference on human factors in computing systems"
    },
    {
      "citation_id": "22",
      "title": "Towards context aware emotional intelligence in machines: computing contextual appropriateness of affective states",
      "authors": [
        "M Ptaszynski",
        "P Dybala",
        "W Shi",
        "R Rzepka",
        "K Araki"
      ],
      "year": "2009",
      "venue": "Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI-09"
    },
    {
      "citation_id": "23",
      "title": "Eye tracking in human-computer interaction and usability research",
      "authors": [
        "T Strandvall"
      ],
      "year": "2009",
      "venue": "Human-Computer Interaction-INTERACT 2009: 12th IFIP TC 13 International Conference"
    },
    {
      "citation_id": "24",
      "title": "Human eye tracking and related issues: A review",
      "authors": [
        "H Singh",
        "J Singh"
      ],
      "year": "2012",
      "venue": "International Journal of Scientific and Research Publications"
    },
    {
      "citation_id": "25",
      "title": "Multimodal data fusion: an overview of methods, challenges, and prospects",
      "authors": [
        "D Lahat",
        "T Adali",
        "C Jutten"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "26",
      "title": "Combining eye movements and eeg to enhance emotion recognition",
      "authors": [
        "Y Lu",
        "W.-L Zheng",
        "B Li",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IJCAI"
    },
    {
      "citation_id": "27",
      "title": "Ensemble methods: foundations and algorithms",
      "authors": [
        "Z.-H Zhou"
      ],
      "year": "2012",
      "venue": "Ensemble methods: foundations and algorithms"
    },
    {
      "citation_id": "28",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "29",
      "title": "Decision-level fusion method for emotion recognition using multimodal emotion recognition information",
      "authors": [
        "K.-S Song",
        "Y.-H Nho",
        "J.-H Seo",
        "D.-S Kwon"
      ],
      "year": "2018",
      "venue": "2018 15th international conference on ubiquitous robots (UR)"
    },
    {
      "citation_id": "30",
      "title": "Comparing emotional states induced by 360 videos via headmounted display and computer screen",
      "authors": [
        "J.-N Voigt-Antons",
        "E Lehtonen",
        "A Palacios",
        "D Ali",
        "T Kojic",
        "S Möller"
      ],
      "year": "2020",
      "venue": "2020 twelfth international conference on quality of multimedia experience (QoMEX)"
    },
    {
      "citation_id": "31",
      "title": "Pem360: A dataset of 360 videos with continuous physiological measurements, subjective emotional ratings and motion traces",
      "authors": [
        "Q Guimard",
        "F Robert",
        "C Bauce",
        "A Ducreux",
        "L Sassatelli",
        "H.-Y Wu",
        "M Winckler",
        "A Gros"
      ],
      "year": "2022",
      "venue": "Proceedings of the 13th ACM Multimedia Systems Conference"
    },
    {
      "citation_id": "32",
      "title": "A public database of immersive vr videos with corresponding ratings of arousal, valence, and correlations between head movements and self report measures",
      "authors": [
        "B Li",
        "J Bailenson",
        "A Pines",
        "W Greenleaf",
        "L Williams"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "33",
      "title": "Comparative study of physiological signals from empatica e4 wristband for stress classification",
      "authors": [
        "V Chandra",
        "A Priyarup",
        "D Sethia"
      ],
      "year": "2021",
      "venue": "Advances in Computing and Data Sciences: 5th International Conference, ICACDS 2021"
    },
    {
      "citation_id": "34",
      "title": "Supervised sequence labelling with recurrent neural networks",
      "authors": [
        "A Graves",
        "A Graves"
      ],
      "year": "2012",
      "venue": "Supervised sequence labelling with recurrent neural networks"
    },
    {
      "citation_id": "35",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "36",
      "title": "Inferring mood instability via smartphone sensing: A multi-view learning approach",
      "authors": [
        "X Zhang",
        "F Zhuang",
        "W Li",
        "H Ying",
        "H Xiong",
        "S Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}