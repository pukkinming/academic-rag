{
  "paper_id": "2108.11626v3",
  "title": "Compm: Context Modeling With Speaker'S Pre-Trained Memory Tracking For Emotion Recognition In Conversation",
  "published": "2021-08-26T07:45:09Z",
  "authors": [
    "Joosung Lee",
    "Wooin Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speaker's pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-theart among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods. Our code is available on github 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As the number of applications such as interactive chatbots or social media that are used by many users has recently increased dramatically, Emotion Recognition in Conversation (ERC) plays a more important role in natural language processing, and as a proof, a lot of research  (Poria et al., 2019; Zhang et al., 2019; Ghosal et al., 2020; Jiao et al., 2020)  has been conducted on the task.\n\nThe ERC module increases the quality of empathetic conversations with the users and can be 1 https://github.com/rungjoo/CoMPM Figure  1 : An example of MELD dataset utilized when sending tailored push messages to the users  (Shin et al., 2019; Zandie and Mahoor, 2020; Lin et al., 2020) . In addition, emotion recognition can be effectively used for opinion mining, recommender systems, and healthcare systems where it can improve the service qualities by providing personalized results. As these interactive machines increase, the ERC module plays an increasingly important role. Figure  1  is an example of a conversation in which two speakers are angry at each other. The emotion of speaker B's utterance (\"How'd you get to that?\") is angry. If the system does not take into account previous utterances, it is difficult to properly recognize emotions. Like the previous studies  (Ghosal et al., 2020) , we show that the utterance-level emotion recognition, which does not consider the previous utterance, have limitations and experiments result in poor performances.\n\nTherefore, recent studies are attempting to recognize emotions while taking into account the previous utterances. Representatively, Dia-logueRNN  (Majumder et al., 2019)  recognizes the present emotion by tracking context from the previous utterances and the speaker's emotion. AGHMN  (Jiao et al., 2020)  considers the previous utterances through memory summarizing using GRU with attention.\n\nMany recent studies use external knowledge to improve the ERC performance. However, this exter-nal knowledge is often only available in English. In order to utilize the previous methods in languages of other countries, it is expensive and difficult to utilize because external knowledge data must be newly constructed. In recent NLP studies, due to the effectiveness of the pre-trained language model, it has already been developed in many countries. Since pre-trained language models are trained by unsupervised learning, these models are relatively usable approaches regardless of language types.  Petroni et al. (2019)  introduces that these language models can be used as knowledge bases and have many advantages over the structured knowledge bases. Based on these studies, we eliminate the dependence on structured external data used in cutting-edge systems and use a pre-trained language model as a feature extractor of knowledge.\n\nCoMPM, introduced in this paper, is composed of two modules that take into account previous utterances in dialogue. (1) The first is a context embedding module (CoM) that reflects all previous utterances as context. CoM is an auto-regressive model that predicts the current emotion through attention between the previous utterances of the conversation and the current utterance. (2) The second is a pre-trained memory module (PM) that extracts memory from utterances. We use the output of the pre-trained language model as the memory embedding where the utterances are passed into the language model. We use the PM to help predict the emotion of the speaker by taking into account the speaker's linguistic preferences and characteristics.\n\nWe experiment on 4 different English ERC datasets. Multi-party datasets are MELD  (Poria et al., 2019)  and  EmoryNLP (Zahiri and Choi, 2018) , and dyadic datasets are IEMOCAP  (Busso et al., 2008)  and DailyDialog  (Li et al., 2017) . CoMPM achieves the first or second performance according to the evaluation metric compared to all previous systems. We perform an ablation study on each module to show that the proposed approach is effective. Further experiments also show that our approach can be used in other languages and show the performance of CoMPM when the number of data is limited.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Many recent studies use external knowledge to improve the ERC performance. KET  (Zhong et al., 2019)  is used as external knowledge based on ConceptNet  (Speer et al., 2017)  and emotion lex-icon NRC_VAD  (Mohammad, 2018)  as the commonsense knowledge. ConceptNet is a knowledge graph that connects words and phrases in natural language using labeled edges. NRC_VAD Lexicon has human ratings of valence, arousal, and dominance for more than 20,000 English words. COSMIC  (Ghosal et al., 2020)  and Psychological  (Li et al., 2021)  improve the performance of emotion recognition by extracting commonsense knowledge of the previous utterances. Commonsense knowledge feature is extracted and leveraged with COMET  (Bosselut et al., 2019)  trained with ATOMIC (The Atlas of Machine Commonsense)  (Sap et al., 2019) . ATOMIC has 9 sentence relation types with inferential if-then commonsense knowledge expressed in text. ToDKAT  (Zhu et al., 2021)  improves performance by combining commonsense knowledge using COMET and topic discovery using VHRED  (Serban et al., 2017)  to the model.\n\nEkman  (Ekman, 1992 ) constructs taxonomy of six common emotions (Joy, Sadness, Fear, Anger, Surprise, and Disgust) from human facial expressions. In addition, Ekman explains that a multimodal view is important for multiple emotions recognition. The multi-modal data such as MELD and IEMOCAP are some of the available standard datasets for emotion recognition and they are composed of text, speech and vision-based data.  Datcu and Rothkrantz (2014)  uses speech and visual information to recognize emotions, and  (Alm et al., 2005)  attempts to recognize emotions based on text information. MELD and ICON  (Hazarika et al., 2018a)  show that the more multi-modal information is used, the better the performance and the text information plays the most important role. Multimodal information is not always given in most social media, especially in chatbot systems where they are mainly composed of text-based systems. In this work, we design and introduce a text-based emotion recognition system using neural networks.\n\nIn the previous studies, such  as Hazarika et al. (2018b) ;  Zadeh et al. (2017) ;  Majumder et al. (2019) , most works focused on dyadic-party conversation. However, as the multi-party conversation datasets including MELD and EmoryNLP have become available, a lot of recent research is being conducted on multi-party dialogues such as  Zhang et al. (2019) ;  Jiao et al. (2020) ;  Ghosal et al. (2020) . In general, the multi-party conversations have higher speaker dependency than the dyadic-party dialogues, therefore have more conditions to consider and result in poor performance.  Zhou et al. (2018) ;  Zhang et al. (2018a)  shows that commonsense knowledge is important for understanding conversations and generating appropriate responses.  Liu et al. (2020)  reports that the lack of external knowledge makes it difficult to classify implicit emotions from the conversation history. EDA  (Bothe et al., 2020)  expands the multi-modal emotion datasets by extracting dialog acts from MELD and IEMOCAP and finds out that there is a correlation between dialogue acts and emotion labels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Approach",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Statement",
      "text": "In a conversation, M sequential utterances are given as [(u 1 , p u 1 ), (u 2 , p u 2 ), ..., (u M , p u M )]. u i is the utterance which the speaker p u i uttered, where p u i is one of the conversation participants. While p u i and p u j (i = j) can be the same speaker, the minimum number of the unique conversation participants should be 2 or more. The ERC is a task of predicting the emotion e t of u t , the utterance of the t-th turn, given the previous utterances h t = {u 1 , ..., u t-1 }. Emotions are labeled as one of the predefined classes depending on the dataset, and the emotions we experimented with are either 6 or 7. We also experimented with a sentiment classification dataset which provides sentiment labels consisting of positive, negative and neutral.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Overview",
      "text": "Figure  2  shows an overview of our model. Our ERC neural network model is composed of two modules. The first is CoM which catches the underlying effect of all previous utterances on the current speaker's emotions. Therefore, we propose a context model to handle the relationship between the current and the previous utterances. The second one is PM that leverages only the speaker's previous utterances, through which we want to reflect the speaker's knowledge.\n\nIf the CoM and PM are based on different backbones, we consider them to be unaligned with respect to each other's output representations. Therefore, we design the PM to follow CoM so that the output representations of CoM and PM can mutually understand each other. If CoM and PM are based on different architectures, CoMPM is trained to understand each other's representations by matching dimensions using W p in Equation  4 . The combination of CoM and PM is described in Section 4.5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Com: Context Embedding Module",
      "text": "The context embedding module predicts e t by considering all of the utterances before the t-th turn as the dialogue context. The example in Figure  2  shows how the model predicts the emotion of u 6 uttered by s A , given a conversation of three participants (s A , s B , s C ). The previous utterances are\n\n} and e 6 is predicted while considering the relationship between u 6 and h 6 .\n\nWe consider multi-party conversations where 2 or more speakers are involved. A special token <s P > is introduced to distinguish participants in the conversation and to handle the speaker's dependency where P is the set of participants. In other words, the same special token appears before the utterances of the same speaker.\n\nWe use an Transformer encoder as a context model. In many natural language processing tasks, the effectiveness of the pre-trained language model has been proven, and we also set the initial state of the model to RoBERTa  (Liu et al., 2019) . RoBERTa is an unsupervised pre-trained model with largescale open-domain corpora of unlabeled text.\n\nWe use the embedding of the special token <cls> to predict emotion. The <cls> token is concatenated at the beginning of the input and the output of the context model is as follows:\n\nwhere P :t-1 is the set of speakers in the previous turns. c t ∈ R 1×hc and h c is the dimension of CoM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pm: Pre-Trained Memory Module",
      "text": "External knowledge is known to play an important role in understanding conversation. Pre-trained language models can be trained on numerous corpora and be used as an external knowledge base. Inspired by previous studies that the speaker's knowledge helps to judge emotions, we extract and track pre-trained memory from the speaker's previous utterances to utilize the emotions of the current utterance u t . If the speaker has never appeared before the current turn, the result of the pre-trained memory is considered a zero vector. Since <cls> is mostly used for the task of classifying sentences, we use the embedding output of the <cls> token as a vector representing the utterance as follows:\n\nwhere p u i = p S , S is the speaker of the current utterance. k i ∈ R 1×h k and h k is the dimension of PM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Compm: Combination Of Com And Pm",
      "text": "We combine CoM and PM to predict the speaker's emotion. In many dialogue systems  (Zhang et al., 2018b; Ma et al., 2019) , it is known that utterances close to the current turn are important for response. Therefore, we assume that utterances close to the current utterance will be important in emotional recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Tracking Method",
      "text": "We use k i tracking method using GRU. The tracking method assumes that the importance of all previous speaker utterances to the current emotion is not equal and varies with the distance of the current utterance. In other words, since the flow of conversation changes as it progresses, the effect on emotion may differ depending on the distance from the current utterance. We track and capture the sequential position information of k i using a unidirectional GRU:\n\nwhere t is the turn index of the current utterance, n is the number of previous utterances of the speaker, and i s (s = 1, 2, ..., n) is each turn uttered. kt t ∈ R 1×hc is the output of k in and as a result, the knowledge of distant utterance is diluted and the effect on the current utterance is reduced. GRU is composed of 2-layers, the dimension of the output vector is h c , and the dropout is set to 0.3 during training. Finally, the output vector o t is obtained by adding kt t and c t in Equation  4 .\n\nwhere, W p is a matrix that projects the pretrained memory to the dimension of the context output, and is used only when PM and CoM are different pre-trained language models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Prediction",
      "text": "Softmax is applied to the vector multiplied by o t and the linear matrix W o ∈ R he×hc to obtain the probability distribution of emotion classes, where h e is the number of emotion classes. e t is the predicted emotion class that corresponds to the index of the largest probability from the emotion class distribution.\n\nThe objective is to minimize the cross entropy loss so that e t is the same as the ground truth emotional label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "We experiment on four benchmark datasets. MELD  (Poria et al., 2019)  and EmoryNLP (Zahiri and Choi, 2018) are multi-party datasets, while IEMOCAP  (Busso et al., 2008)  and DailyDialog  (Li et al., 2017)  are dyadic-party datasets. The statistics of the dataset are shown in Table  1 .\n\nIEMOCAP is a dataset involving 10 speakers, and each conversation involves 2 speakers and the emotion-inventory is given as \"happy, sad, angry, excited, frustrated and neutral\". The train and development dataset is a conversation involving the previous eight speakers, and the train and development are divided into random splits at a ratio of 9:1. The test dataset is a conversation involving two later speakers.\n\nDailyDialog is a dataset of daily conversations between two speakers and the emotion-inventory is given as \"anger, disgust, fear, joy, surprise, sadness and neutral\". Since more than 82% of the data are tagged as neutral, neutral emotions are excluded when evaluating systems with Micro-F1 as did in the previous studies.\n\nMELD is a dataset based on Friends TV show and provides two taxonomy: emotion and sentiment. MELD's emotion-inventory is given as \"anger, disgust, sadness, joy, surprise, fear and neutrality\" following Ekman  (Ekman, 1992)  and sentiment-inventory is given as \"positive, negative and neutral\". EmoryNLP, like MELD, is also a dataset based on Friends TV show, but the emotion-inventory is given as \"joyful, peaceful, powerful, scared, mad, sad and neutral\". Sentiment labels are not provided, but sentiment classes can be grouped as follows: positive: {joyful, peaceful, powerful}, negative: {scared, mad, sad}, neutral: {neutral}",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training Setup",
      "text": "We use the pre-trained model from the huggingface library 2  . The optimizer is AdamW and the learning rate is 1e-5 as an initial value. The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping. We select the model with the best performance on the validation set. All experiments are conducted on one V100 GPU with 32GB memory.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Previous Method",
      "text": "We show that the proposed approach is effective by comparing it with various baselines and the stateof-the-art methods.\n\nKET  (Zhong et al., 2019 ) is a Knowledge Enriched Transformer that reflects contextual utterances with a hierarchical self-attention and leverages external commonsense knowledge by using a context-aware affective graph attention mechanism.\n\nDialogueRNN  (Majumder et al., 2019)  uses a GRU network to keep track of the individual party states in the conversation to predict emotions. This model assumes that there are three factors in emotion prediction: the speaker, the context from the preceding utterances and the emotion of the preceding utterances. Also,  Ghosal et al. (2020)  shows the performance of RoBERTa+DialogueRNN when the vectors of the tokens are extracted with a pretrained RoBERTa.\n\nRGAT+P  (Ishiwatari et al., 2020)  (relational graph attention networks) proposes relational position encodings with sequential information reflecting the relational graph structure, which shows that both the speaker dependency and the sequential information can be captured.\n\nHiTrans  (Li et al., 2020)   Trans utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer.\n\nCOSMIC  (Ghosal et al., 2020)  incorporates different elements of commonsense such as mental states, events and causal relations, and learns the relations between participants in the conversation. This model uses pre-trained RoBERTa as a feature extractor and leverages COMET trained with ATOMIC as the commonsense knowledge.  (Sun et al., 2021)  proposes a discourse-aware graph neural network that utilizes self-speaker dependency of interlocutors as a relational convolution and informative cues of dependent utterances as a gated convolution.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ermc-Disgcn",
      "text": "Psychological  (Li et al., 2021)  uses commonsense knowledge as enrich edges and processes it with graph transformer. Psychological performs emotion recognition by utilizing intention of utterances from not only past contexts but also future context.\n\nDialogueCRN  (Hu et al., 2021)  introduces an intuitive retrieving process, the reasoning module, which understands both situation-level and speakerlevel contexts.\n\nToDKAT  (Zhu et al., 2021)  proposes a language model with topic detection added, and improves performance by combining it with commonsense knowledge. The performance of ToDKAT in MELD was re-released on github 3 .\n\n3 https://github.com/something678/TodKat",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Result And Analysis",
      "text": "Table  2  shows the performance of the previous methods and our models. CoM used alone does not leverage PM and predicts emotions by considering only the dialogue context. PM used alone is not used as a memory module, but the same backbone is used. PM used alone predicts emotion only with the utterance of the current turn without considering the context. CoMPM is a model in which both CoM and PM parameters are updated in the initial state of the pre-trained LM. CoMPM(f) is a model in which PM parameters are frozen in the initial state (pre-trained LM) and is not trained further, and CoMPM(s) is a model in which PM is trained from scratch. CoMPM(k) is a model in which PM is trained on ConceptNet. Following previous studies, we use the average vector for each token in PM(k) as the feature of the utterance. We use the pre-trained model provided by the site 4  as PM(k).\n\nThe effect of PM can be confirmed through the performance comparison between CoM and CoMPM, and the effect of CoM can be confirmed by comparing the results of CoM and PM. Since PM does not consider context, it showed worse performance than CoM, and the performance gap is larger in the IEMOCAP dataset with a higher average number of conversation turns. As a result, we show that the combination of CoM and PM is effective in achieving better performance.\n\nWe confirm the effect of PM structure in the model through the performance of CoMPM(s).\n\nIf PM parameters are not frozen and are instead randomly initialized (i.e. PM(s)), the performance deteriorates. CoMPM(s) performs worse than CoMPM, and even performs worse than CoM on the other datasets except for MELD. That is, PM(s) cannot be regarded as a pre-trained memory because the parameters are randomly initialized, and simply increasing the model complexity does not help to improve the performance. CoMPM(f) shows similar performance to CoMPM and achieves better performance depending on the data. PM(f) is not fine-tuned on the data, but it extracts general pre-trained memory from a pretrained language model. The comparison between PM and PM(f) will be further described in Section 4.6. In addition, CoMPM(k) shows better performance than CoM, PM, and CoMPM(s) except for IEMOCAP. In IEMOCAP, CoMPM(k) has lower performance than CoM. For all datasets, CoMPM(k) performs slightly worse than CoMPM. In other words, ConceptNet improves the performance of CoMPM, but is not as effective as pretrained memory. As a result, we regard pre-trained memory as compressed knowledge, which can play a role similar to external knowledge used in cuttingedge systems.\n\nThe best performance of our approaches is CoMPM or CoMPM(f), both of which combine pre-trained memory. We achieve state-of-the-art performance among all systems that do not leverage structured external data and achieve the first or second performance even including systems that leverage external data. Therefore, our approach can be extended to other languages without structured external data as well, which is described in Section 4.7.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Combinations Of Com And Pm",
      "text": "We experiment with the effect of pre-trained memory of different language models. To eliminate the influence of the PM structure, we freeze the parameters of PM and use it as a feature extractor. Table  3  shows the performance of the pretrained memory extracted by the different language models. If PM and CoM are based on different backbones, the pre-trained memory is projected through W p as the dimension of the context output. RoBERTa+BERT and RoBERTa+GPT2 (combination of CoM and PM(f)) have lower performance than RoBERTa+RoBERTa, which is inferred because pre-trained memory of RoBERTa contains richer information than BERT and GPT2. Since there is a lot of training data in the diallydialog and W p is fine-tuned to the data to mutually understand the pre-trained memory and context representation. Therefore, we infer that performance does not decrease even if the PM changes from the dailydialog. However, even if other PMs are used, the performance is improved compared to using only CoM, so the pre-trained memory of other language models is also effective for emotion recognition.\n\nBERT+RoBERTa has a larger performance decrease than RoBERTa+BERT. In particular, in IEMOCAP data with a long average number of turns in the context, the performance deteriorates significantly. In addition, the performance of BERT+RoBERTa is lower than CoM (RoBERTa), which supports that the performance of CoM is a more important factor than the use of pre-trained memory. In other words, we confirm that CoM is more important than PM in our system for performance, and it is effective to focus on context modeling rather than external knowledge in the study of emotion recognition in conversation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Training With Less Data",
      "text": "CoMPM is an approach that eliminates dependence on external sources and is easily extensible to any language. However, the insufficient number of emotional data available in other countries (or actual service) remains a problem. Therefore, we conduct additional experiments according to the use ratio of training data in MELD and EmoryNLP, where there is neither too much nor too little data. Figure  3  shows the performance of the model according to the ratio of the training data. In MELD and EmoryNLP, even if only 60% and 80% are used, respectively, the performance decreases by only 3 points.  The value in parentheses is the performance difference from the original CoMPM(f) (RoBERTa + RoBERTa). We use the bert-large-uncased and GPT2-medium versions.\n\nTable  2  shows that CoMPM(f) achieves better performance than CoMPM in the emotion classification of IMEOCAP and EmoryNLP, which has fewer training data than other settings. On the other hand, if there is a lot of training data, CoMPM shows better performance. Figure  3  shows that as the number of data decreases, CoMPM(f) shows better results than CoMPM, which indicates that it is better to freeze the parameters of PM when the number of training data is insufficient. Therefore, if there is a lot of training data in the real-world application, CoMPM is expected to achieve good performance, otherwise it is CoMPM(f).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Erc In Other Languages",
      "text": "Previous studies mostly utilize external knowledge to improve performance, but these approaches require additional publicly available data, which are mainly available for English. Indeed, structured knowledge and ERC data are lacking in other languages. Our approach can be extended to other languages without building additional external knowledge and achieves better performance than simply using a pre-trained model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Korean Dataset",
      "text": "We constructed data composed of two speakers in Korean, and emotion-inventory is given as \"surprise, fear, ambiguous, sad, disgust, joy, bored, embarrassed, neutral\". The total number of sessions is 1000, and the average number of utterance turns is 13.4. We use the data randomly divided into train:dev:test in a ratio of 8:1:1. This dataset is for actual service and is not released to the public.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results In The Korean Dataset",
      "text": "In Korean, our results are shown in Table  4 . The backbone of CoM and PM is Korean-BERT owned by the company, respectively. In the Korean dataset, like the English dataset, the performance is good in the order of CoMPM, CoM, and PM. Our approach simply shows a significant performance improvement on baselines that are fine-tuned to the language model and works well for other languages as well as for English.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "We propose CoMPM that leverages pre-trained memory using a pre-trained language model. CoMPM consists of a context embedding module (CoM) and a pre-trained memory module (PM), and the experimental results show that each module is effective in improving the performance. CoMPM outperforms baselines on both dyadic-party and multi-party datasets and achieves state-of-the-art among systems that do not use external knowledge.\n\nIn addition, CoMPM achieves performance comparable to cutting-edge systems that leverage structured external knowledge, which is the effect of pre-trained memory of the language model. By combining other pre-trained memories, we find that the pre-trained memory extracted with RoBERTa is richer and more effective than the pre-trained memory extracted with BERT or GPT2. Since we believe that pre-trained memory is proportional to the performance of a language model, a language model with a large training corpus and many parameters is considered to be more effective. However, we find that context modeling is more important than pre-trained memory for emotion recognition in conversation, and future research will focus on context modeling.\n\nAdditionally, our approach achieves competitive performance and does not require externally structured data. Therefore, we show that it can be easily extended to Korean as well as English, and it is expected to be effective in other countries.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of MELD dataset",
      "page": 1
    },
    {
      "caption": "Figure 1: is an example of a conversation in which",
      "page": 1
    },
    {
      "caption": "Figure 2: shows an overview of our model. Our",
      "page": 3
    },
    {
      "caption": "Figure 2: shows how the model predicts the emotion of u6",
      "page": 3
    },
    {
      "caption": "Figure 2: Our model consists of two modules: a context embedding module and a pre-trained memory module.",
      "page": 4
    },
    {
      "caption": "Figure 3: Performance according to the size of training",
      "page": 7
    },
    {
      "caption": "Figure 3: shows that as",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 2: shows that CoMPM(f) achieves better Korean",
      "data": [
        {
          "RoBERTa": "RoBERTa",
          "BERT": "GPT2",
          "65.93\n(-3.53)": "68.54\n(-0.92)",
          "52.74\n59.97\n(+1.07)\n(+0.95)": "50.68\n59.61\n(-0.99)\n(+0.59)",
          "65.41\n(-0.36)": "65.58\n(-0.19)",
          "37.25\n(-1.68)": "36.39\n(-2.54)"
        },
        {
          "RoBERTa": "BERT",
          "BERT": "RoBERTa",
          "65.93\n(-3.53)": "62.69\n(-6.77)",
          "52.74\n59.97\n(+1.07)\n(+0.95)": "48.99\n57.34\n(-2.68)\n(-1.68)",
          "65.41\n(-0.36)": "63.79\n(-1.98)",
          "37.25\n(-1.68)": "35.47\n(-3.46)"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions from text: Machine learning for text-based emotion prediction",
      "authors": [
        "Cecilia Ovesdotter Alm",
        "Dan Roth",
        "Richard Sproat"
      ],
      "year": "2005",
      "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1470"
    },
    {
      "citation_id": "3",
      "title": "EDA: Enriching emotional dialogue acts using an ensemble of neural annotators",
      "authors": [
        "Chandrakant Bothe",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Semantic audiovisual data fusion for automatic emotion recognition",
      "authors": [
        "D Datcu",
        "Rothkrantz"
      ],
      "year": "2014",
      "venue": "Semantic audiovisual data fusion for automatic emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "7",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "8",
      "title": "2018a. ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "9",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "10",
      "title": "Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "11",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "12",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020"
    },
    {
      "citation_id": "13",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "14",
      "title": "HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.370"
    },
    {
      "citation_id": "15",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Caire: An end-to-end empathetic chatbot",
      "authors": [
        "Zhaojiang Lin",
        "Peng Xu",
        "Genta Indra Winata",
        "Farhad Bin Siddique",
        "Zihan Liu",
        "Jamin Shin",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i09.7098"
    },
    {
      "citation_id": "17",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "18",
      "title": "Towards conversational recommendation over multi-type dialogs",
      "authors": [
        "Zeming Liu",
        "Haifeng Wang",
        "Zheng-Yu Niu",
        "Hua Wu",
        "Wanxiang Che",
        "Ting Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.98"
    },
    {
      "citation_id": "19",
      "title": "TripleNet: Triple attention network for multiturn response selection in retrieval-based chatbots",
      "authors": [
        "Wentao Ma",
        "Yiming Cui",
        "Nan Shao",
        "Su He",
        "Wei-Nan Zhang",
        "Ting Liu",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
      "doi": "10.18653/v1/K19-1069"
    },
    {
      "citation_id": "20",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "21",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1017"
    },
    {
      "citation_id": "22",
      "title": "Language models as knowledge bases?",
      "authors": [
        "Fabio Petroni",
        "Tim Rocktäschel",
        "Sebastian Riedel",
        "Patrick Lewis",
        "Anton Bakhtin",
        "Yuxiang Wu",
        "Alexander Miller"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1250"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "25",
      "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
      "authors": [
        "Maarten Sap",
        "Le Ronan",
        "Emily Bras",
        "Chandra Allaway",
        "Nicholas Bhagavatula",
        "Hannah Lourie",
        "Brendan Rashkin",
        "Noah Roof",
        "Yejin Smith",
        "Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33013027"
    },
    {
      "citation_id": "26",
      "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "authors": [
        "Iulian Serban",
        "Alessandro Sordoni",
        "Ryan Lowe",
        "Laurent Charlin",
        "Joelle Pineau",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Happybot: Generating empathetic dialogue responses by improving user experience lookahead",
      "authors": [
        "Jamin Shin",
        "Peng Xu",
        "Andrea Madotto",
        "Pascale Fung"
      ],
      "year": "2019",
      "venue": "Happybot: Generating empathetic dialogue responses by improving user experience lookahead",
      "arxiv": "arXiv:1906.08487"
    },
    {
      "citation_id": "28",
      "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "authors": [
        "Robyn Speer",
        "Joshua Chin",
        "Catherine Havasi"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17"
    },
    {
      "citation_id": "29",
      "title": "A discourse-aware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Yang Sun",
        "Nan Yu",
        "Guohong Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "30",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "31",
      "title": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Emptransfo: A multi-head transformer architecture for creating empathetic dialog systems",
      "authors": [
        "Rohola Zandie",
        "Mohammad Mahoor"
      ],
      "year": "2020",
      "venue": "Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference, Originally to be held in North"
    },
    {
      "citation_id": "33",
      "title": "Modeling both context-and speakersensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
      "doi": "10.24963/ijcai.2019/752"
    },
    {
      "citation_id": "34",
      "title": "Text emotion distribution learning via multi-task convolutional neural network",
      "authors": [
        "Yuxiang Zhang",
        "Jiamei Fu",
        "Dongyu She",
        "Ying Zhang",
        "Senzhang Wang",
        "Jufeng Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18",
      "doi": "10.24963/ijcai.2018/639"
    },
    {
      "citation_id": "35",
      "title": "Modeling multiturn conversation with deep utterance aggregation",
      "authors": [
        "Zhuosheng Zhang",
        "Jiangtong Li",
        "Pengfei Zhu",
        "Hai Zhao",
        "Gongshen Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao",
        "Lin-Hao Zhou",
        "Tom Young",
        "Minlie Huang",
        "Haizhou Zhao",
        "Jingfang Xu",
        "Xiaoyan Zhu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.24963/ijcai.2018/643"
    },
    {
      "citation_id": "37",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}