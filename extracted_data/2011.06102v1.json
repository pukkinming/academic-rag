{
  "paper_id": "2011.06102v1",
  "title": "Improving Multimodal Accuracy Through Modality Pre-Training And Attention",
  "published": "2020-11-11T22:31:27Z",
  "authors": [
    "Aya Abdelsalam Ismail",
    "Mahmudul Hasan",
    "Faisal Ishtiaq"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Training a multimodal network is challenging and it requires complex architectures to achieve reasonable performance. We show that one reason for this phenomena is the difference between the convergence rate of various modalities. We address this by pre-training modality-specific sub-networks in multimodal architectures independently before end-to-end training of the entire network. Furthermore, we show that the addition of an attention mechanism between sub-networks after pretraining helps identify the most important modality during ambiguous scenarios boosting the performance. We demonstrate that by performing these two tricks a simple network can achieve similar performance to a complicated architecture that is significantly more expensive to train on multiple tasks including sentiment analysis, emotion recognition, and speaker trait recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal learning  [1]  means using data from different modalities such as acoustic, visual and language information to perform a certain task. It involves understanding the role each modality plays in the task (intra-modal dynamics) and how modalities interact with each other (inter-modality dynamics). Previous work on multimodal learning focuses on increasing model complexity to boost accuracy. Such sophisticated architectures are needed to overcome the difficulty of training multimodal networks  [2, 3] . We believe, the main reason behind this training difficulty is that various modalities tend to converge and generalize at different rates (shown in the first column of Figure  2 ). Training multimodal architecture end-to-end with randomly initialized sub-networks produces a sub-optimal solution.\n\nWe address the shortcomings above by taking a two-step approach while training the multimodal network: (1) Each sub-network is pre-trained independently to learn intra-modal dynamics (allowing each modality to convergence at its own rate). (2) Different modalities are then fused by an attention mechanism (that decides which modalities are most important in a particular example at test time) and the entire network is then fine-tuned end-to-end. The entire architecture is shown in Figure  1  We empirically show that training of multimodal architecture end-to-end from scratch will bias the network towards the modality that has a faster convergence rate. We then show that adding the pre-training step sufficiently improves the performance of multimodal networks. We evaluate our proposed training methods on three publicly available datasets for three multimodal tasks. Our model with simple modality-specific sub-networks shows competitive performance when compared to other models on all three tasks.\n\nPreprint. Under review.    2 ) An attention module that decides which modality is important in a particular example.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Sub-Networks:",
      "text": "The goal of sub-networks is to learn intra-modal dynamics and produce an embedding for each modality that will be used by the attention block. We use a bidirectional Long-short Term Memory (LSTM)  [4]  for each sub-network. LSTMs have been widely used in modeling temporal data in many tasks including text classification  [5] , video classification  [6] , and voice activity detection  [7] . Note that LSTM can be replaced with any neural architecture and the sub-networks are not required to have the same architecture.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality-Specific Pre-Training:",
      "text": "The modality-specific sub-networks are first pre-trained independently on the task of interest. This is done for two reasons: (1) Pre-training allows better learning of intra-modal dynamics since the learning process is not affected by any other modalities at this time. (2) Data from different modalities have different convergence rates, if we train the network end-to-end without the pre-training step, the network tends to focus on the modality that converges faster ignoring others.\n\nThe softmax layer at the end of the modality-specific sub-network is removed after pre-training and the last hidden layer is used as a feature representation of that modality and is given as the input to the attention block as shown in Figure  1 .\n\nFormally, let the input to the language sub-network be denoted as L = [l 0 , . . . , l t , . . . , , l n ] where l t is the t th word in the sentence and l is the feature embedding. Similarly, the input to acoustic and visual sub-network is denoted as A and V respectively. The output of the sub-networks is the last hidden state of modality-specific network can be written as:\n\nAttention Block: Hidden layers from sub-networks h l , h a , and h v each have a same dimension of N . Matrix H is the input to attention block where H = [h l , h a , h v ]; H has a dimension of m × N , where m is the number of modalities. The output of the attention block is a weight matrix A.\n\nwhere k is a hyper-parameter; b 1 and b 2 are bias parameter and the sof tmax() ensures that all the computed weights are sum to 1. In the attention matrix A = [w l , w a , w v ], weights w l , w a , and w v represent importance of different modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Output Of Man:",
      "text": "The hidden layer of each sub-network is multiplied by its weights, which is then concatenated together and passed to a fully connected layer and a sof tmax layer for the final classification. The entire architecture is then trained end-to-end together, which fine-tunes the sub-networks in the process. This helps to capture both intra-modal and inter-modal dynamics.\n\nfollowing: (a) The performance of MAN in terms of accuracy, complexity, and training time when compared to other networks. (b) The effect of pre-training the sub-networks versus end-to-end training of the entire network for multimodal networks. (c) The importance of attention module in differentiating between important and ambiguous modalities. Detailed description of datasets, baselines and experiments is available in the supplementary material. Datasets: We selected three publicly available multimodal datasets that contain spoken language, acoustic and visual information. Multimodal Sentiment Analysis CMU MOSI dataset  [8] , Multimodal Emotion Recognition CMU MOSEI dataset  [9]  and Multimodal Speaker Trait Recognition Persuasion Opinion Multimodal POM dataset  [10] .\n\nComputational Descriptors: Language Features: GloVe  [11]  word embeddings are used to convert the transcripts into word embeddings. Acoustic Features: COVAREP  [12]  acoustic analysis framework is used to extract low level acoustic features. Visual Features: Facial expression analysis toolkit FACET  [13]  is used as visual feature extractor.\n\nBaseline Models We compare the performance of MAN to a variety of models for multimodal language analysis; all models are trained using the same feature embeddings: LF-LSTM Late Fusion LSTM, MAF Modality Attention Fusion  [14] , TFN Tensor Fusion Network  [15] , LMF Low-rank Multimodal Fusion  [16]  , MFN Memory Fusion Network  [17] , MFM Multimodal Factorization Model  [18]  and RAVEN Recurrent Attended Variation Embedding Network  [19] .\n\nEvaluation Metrics For regression, we report Mean Absolute Error. For classification accuracy, we report A C where C is the number of classes. In addition, we report the mean epoch training time in seconds and inference time in milliseconds.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "Performance of MAN We show the results in Table  1 . There is no single model that always outperform others in all metrics across different tasks, in terms of accuracy and MAE. MAN shows competitive performance when compared against the other models. Generally best performing models are RAVEN and MAN; while MAN requires less training time. Why do we need modality-specific pre-training? Different modalities converge and generalize at different rates. Consider Figure  2  (a), the 1 st column shows how language modality converges faster than the acoustic and the visual modality for MOSI dataset. In this case, end-to-end training leads to favoring language modality as shown in the 2 nd column, where the attention module assigns higher weights to the language. As the network continues to learn, the effect of acoustic and visual modalities tend to diminish over time. If we pre-train the sub-networks and then fine-tune the entire architecture, we eliminate the bias introduced by the difference in convergence rate of different modalities, as shown in the 3 rd column; where network learns that both language and visual modalities are equally important in this task. For MOSEI and POM (the 1 st column of Figures  2(b ) and 2(c) respectively), the differences in convergence rate among the modalities are not as significant as the MOSI dataset. However, training without pre-training the sub-networks leads to a biased distribution of the weights among the modalities as shown in the 2 nd column. This is fixed when we pre-train the sub-networks as shown in the 3 rd column, which eventually leads to better performance.   Why is Attention needed? The attention module produces a single weight for each modality pointing out which modality is important in a particular example allowing the network to focus on the feature representation of that modality which results in a boost in accuracy as shown in Table  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cmu-Mosi Cmu-Mosei Pom",
      "text": "In addition, attention weights can also be used as a method of model interpretation  [20] [21] [22] [23] [24] . This in not the case in many methods such as tensor factorization, features from different modalities are multiplied together so it becomes unclear which modality is the most important. In Figure  3 , shows an example from MOSI dataset on sentiment analysis task, where the speaker appears smiling and laughing; however, the text is negative and model was able to attend on the correct modality. More examples are available in the supplementary material.\n\nFigure  3 : MAN correctly identifies language as the most important modality.\n\nBetter word embeddings better accuracy Table  3  show the difference in accuracy on MOSI when replacing the text sub-network with BERT  [25] . Since BERT produces better word embeddings than GloVe replacing text sub-network with BERT improves accuracy. Similar, to GloVe, the addition of pre-training step along with the attention module before end-to-end training improves the accuracy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we identify the difference in the convergence rate of various modalities as one of the reasons behind the difficulty of training multimodal networks. We find that this can be overcome by pre-training individual modality-specific sub-networks followed by end-to-end fine-tuning of the entire network. Finally, we can improve the accuracy further through an attention module that attends to different modalities after pre-training.\n\nEarly Fusion concatenates input-level feature from different modalities. The fused features are then passed to a generic model. Intra-modal dynamics are not modeled explicitly when using this method of fusion  [36, 37, 30] . Early fusion methods can preserve the temporal nature of data by using recurrent neural networks, or totally ignores the time factor when using classical machine learning methods such as Support Vector Machines  [8, 32] . Due to the lack of specific intra-modal models, this fusion method tends to overfit on small datasets  [38] .\n\nLate Fusion models each modality separately, then the outputs are combined in the decision phase. This can be done by majority voting , weighted averaging  [39] , or the output of modality networks are combined with a joint neural network that is trained to make finial decision  [40] .\n\nMulti-view learning is a broader class of methods that aim to learn both intra-modal dynamics and inter-modality dynamics jointly. To place our work in the context of prior research we divide multi-view learning into the following subcategories: (a) Non-attention based fusion methods: this includes tensor-based multimodal representations created from expensive tensor product  [15]  or low-rank tensors approximation  [16] , generative representation learning  [41, 1] , word-level factorized multimodal representation  [18]  and variations to word representations by word-level fusion with nonverbal features  [19] . Such methods (excluding  [16] ) are rather slow and suffer from exponentially increasing computational complexity as the number of modalities increase. (b) Attention based fusion methods: The work in  [17, 42]  used attention to discover interactions between modalities through time. Method proposed in  [43]  decomposes the fusion problem into multiple stages and focuses on a subset of multimodal signals at each stage. In  [14] , authors apply both feature attention and modality attention to classify utterance-level speech data.\n\nMAN belongs to the attention based fusion subcategory; however, we differ from current methods by our ability to view modalities independently and quantize how each modality influences the final decision of the network. Additionally, the pre-training step allows our model to produce competitive results when compared to more complex architectures.",
      "page_start": 4,
      "page_end": 8
    },
    {
      "section_name": "Detailed Description Of The Experiments",
      "text": "Datasets Details of each dataset is described in the section below and data split is shown in Table  4 .\n\nEach datasets that contain spoken language, acoustic and visual information. Multimodal Sentiment Analysis: CMU-MOSI dataset  [8]  is a collection of 93 review videos in English with 2199 utterance segments. Each utterance is annotated with sentiment by five individual annotators in the range  [-3,3] , where -3 indicates highly negative and 3 indicates highly positive.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset",
      "text": "Multimodal Emotion Recognition: CMU-MOSEI dataset  [9]  is a collection of 3,229 videos spanning over 23,000 utterances from more than 1,000 online YouTube speakers. Each utterance is annotated with sentiment in the range [-3, 3] similar to CMU-MOSI dataset. In addition, utterances are annotated for Ekman emotions  [44]  of happiness, sadness, anger, fear, disgust, and surprise with values in range [0, 3].\n\nMultimodal Speaker Trait Recognition: POM Persuasion Opinion Multimodal dataset  [10]  contains 1,000 movie review videos annotated for 16 different speaker traits: confidence, passion, voice pleasant, dominance, credibility, vividness, expertise, entertaining, reserved, trusting, relaxed, outgoing, thorough, nervous, humorous, and persuasive.\n\nFeature Embedding Following prior practice  [16, 43, 45, 19] , we use features provided by CMU-Multimodal SDK.  1 • Language Features: Pre-trained word embeddings GloVe  [11]  are used to convert the transcripts of videos into sequence of 300-dimensional word embeddings.\n\n• Acoustic Features: COVAREP  [12]  acoustic analysis framework is used to extract low level acoustic features. The features include pitch tracking, polarity estimation, glottal closure instants, spectral envelope, glottal flow estimation and 12 Melfrequency cepstral coefficients along with other features.\n\n• Visual Features: Facial expression analysis toolkit FACET 2  is used as visual feature extractor. Features include facial action units, facial landmarks, head pose, gaze tracking and HOG features.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baseline Models",
      "text": "Late Fusion: A sub-network is learned for each modality such as language, acoustic, and visual. As a baseline, we use a bidirectional LSTM for each sub-network. The last hidden layer from sub-networks are concatenated together and passed to a dense layer. This is referred to as LF-LSTM.\n\nHybrid Attention based Multimodal Network (MAF)  [14]  a deep multimodal network with both feature attention and modality attention. The hybrid attention architecture helps the system focus on learning informative representations for both modality-specific feature extraction and model fusion.\n\nTensor Fusion Network (TFN)  [15]  learns intra-modality and inter-modality dynamics by creating a multi-dimensional tensor that captures unimodal, bimodal and trimodal interactions across modalities.\n\nLow-rank Multimodal Fusion (LMF)  [16]  performs multimodal fusion using low-rank tensors to improve efficiency.\n\nMemory Fusion Network (MFN)  [17]  a multi-view sequential learning neural network that accounts for both view-specific and cross-view interactions through time. View-specific interactions are learned through assigning an LSTM function to each view. The cross-view interactions are identified using a special attention mechanism and summarized through time with a multi-view gated Memory.\n\nMultimodal Factorization Model (MFM)  [18]  a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors then optimizes for a joint generative-discriminative objective across multimodal data and labels.\n\nRecurrent Attended Variation Embedding Network (RAVEN)  [19]  a model for human multimodal language that considers the fine-grained structure of nonverbal subword sequences and dynamically shifts the word representations based on these nonverbal cues.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Training Models",
      "text": "All models are trained on NVIDIA Tesla K80 GPU. For each dataset, we use the hyper-parameters reported by the baseline methods for that particular dataset. If the hyper-parameters were not reported in the paper, we perform an extensive grid search to find the best performing hyper-parameters. We use the same grid search method to find the best hyper-parameters for our method as well.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Understanding The Importance Of Different Modalities Through Attention",
      "text": "Human's ability to communicate effectively comes from their understanding of different modalities  [46] . The context in which a modality occurs changes its underlying meaning; a laugh can be both happy or sarcastic. In such cases, not all modalities are equally important for revealing the meaning of human interaction. Consider examples in Figure  4  for an emotion classification task: In example (a), we can easily perceive that the emotion is fear as the visual signal is very prominent. In example (b), visual signals are ambiguous and not useful; however, by looking at the sentence, the word \"Wow\" allows us to conclude that the emotion is surprise. In example (c), visual and language signals are misleading and acoustic features help make correct perception. Humans can identify the important modality in cases where some modalities are ambiguous or may lead to wrong judgment. The goal of the attention block in MAN is to mimic human's ability to understand the context of multimodal data by identifying the importance of modality. In Figure  3 , we showed an example from MOSI where MAN can successfully detect the essence of different modalities. Figure  5  shows examples from MOSEI and POM.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: ). Training multimodal architecture end-to-end with randomly initialized sub-networks produces a",
      "page": 1
    },
    {
      "caption": "Figure 1: We empirically show that training of multimodal architecture end-to-end from scratch will bias the",
      "page": 1
    },
    {
      "caption": "Figure 1: Figure 1: An overview of the proposed Modality Aware Attention Network. MAN contains two main",
      "page": 2
    },
    {
      "caption": "Figure 1: Formally, let the input to the language sub-network be denoted as L = [l0, . . . , lt, . . . , , ln] where lt is",
      "page": 2
    },
    {
      "caption": "Figure 2: (a), the 1st column shows how language modality converges faster",
      "page": 3
    },
    {
      "caption": "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column",
      "page": 4
    },
    {
      "caption": "Figure 3: MAN correctly identiﬁes language as the most important modality.",
      "page": 4
    },
    {
      "caption": "Figure 4: Recent work recognizes the importance",
      "page": 7
    },
    {
      "caption": "Figure 4: for an emotion classiﬁcation task: In example (a),",
      "page": 9
    },
    {
      "caption": "Figure 4: Video examples for an emotion classiﬁcation task. (a) Visual images are enough to make",
      "page": 9
    },
    {
      "caption": "Figure 3: , we showed an example",
      "page": 10
    },
    {
      "caption": "Figure 5: shows examples from MOSEI and POM. Figure 5 (a) An example from MOSEI dataset on emotion",
      "page": 10
    },
    {
      "caption": "Figure 5: (b) An example from POM dataset on personality trait recognition. Here the",
      "page": 10
    },
    {
      "caption": "Figure 5: Video examples from three datasets on different classiﬁcation tasks; in each example we",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Training a multimodal network is challenging and it requires complex architectures"
        },
        {
          "Abstract": "to achieve reasonable performance. We show that one reason for this phenomena"
        },
        {
          "Abstract": "is the difference between the convergence rate of various modalities. We address"
        },
        {
          "Abstract": "this by pre-training modality-speciﬁc sub-networks in multimodal architectures"
        },
        {
          "Abstract": "independently before end-to-end training of the entire network. Furthermore, we"
        },
        {
          "Abstract": "show that the addition of an attention mechanism between sub-networks after pre-"
        },
        {
          "Abstract": "training helps identify the most important modality during ambiguous scenarios"
        },
        {
          "Abstract": "boosting the performance. We demonstrate that by performing these two tricks"
        },
        {
          "Abstract": "a simple network can achieve similar performance to a complicated architecture"
        },
        {
          "Abstract": "that is signiﬁcantly more expensive to train on multiple tasks including sentiment"
        },
        {
          "Abstract": "analysis, emotion recognition, and speaker trait recognition."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": "Multimodal\nlearning [1] means using data from different modalities such as acoustic, visual and"
        },
        {
          "1\nIntroduction": "language information to perform a certain task.\nIt\ninvolves understanding the role each modality"
        },
        {
          "1\nIntroduction": "plays in the task (intra-modal dynamics) and how modalities interact with each other (inter-modality"
        },
        {
          "1\nIntroduction": "dynamics).\nPrevious work on multimodal\nlearning focuses on increasing model complexity to"
        },
        {
          "1\nIntroduction": "boost accuracy. Such sophisticated architectures are needed to overcome the difﬁculty of training"
        },
        {
          "1\nIntroduction": "multimodal networks [2, 3]. We believe, the main reason behind this training difﬁculty is that various"
        },
        {
          "1\nIntroduction": "modalities tend to converge and generalize at different rates (shown in the ﬁrst column of Figure"
        },
        {
          "1\nIntroduction": "2). Training multimodal architecture end-to-end with randomly initialized sub-networks produces a"
        },
        {
          "1\nIntroduction": "sub-optimal solution."
        },
        {
          "1\nIntroduction": "We address the shortcomings above by taking a two-step approach while training the multimodal"
        },
        {
          "1\nIntroduction": "network: (1) Each sub-network is pre-trained independently to learn intra-modal dynamics (allowing"
        },
        {
          "1\nIntroduction": "each modality to convergence at its own rate). (2) Different modalities are then fused by an attention"
        },
        {
          "1\nIntroduction": "mechanism (that decides which modalities are most important in a particular example at test time)"
        },
        {
          "1\nIntroduction": "and the entire network is then ﬁne-tuned end-to-end. The entire architecture is shown in Figure 1"
        },
        {
          "1\nIntroduction": "We empirically show that training of multimodal architecture end-to-end from scratch will bias the"
        },
        {
          "1\nIntroduction": "network towards the modality that has a faster convergence rate. We then show that adding the"
        },
        {
          "1\nIntroduction": "pre-training step sufﬁciently improves the performance of multimodal networks. We evaluate our"
        },
        {
          "1\nIntroduction": "proposed training methods on three publicly available datasets for three multimodal tasks. Our model"
        },
        {
          "1\nIntroduction": "with simple modality-speciﬁc sub-networks shows competitive performance when compared to other"
        },
        {
          "1\nIntroduction": "models on all three tasks."
        },
        {
          "1\nIntroduction": "Preprint. Under review."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nModality Attention Network with Two Step Training": "Modality Attention Network (MAN) has two main components:\n(1) Sub-networks -\nthese are"
        },
        {
          "2\nModality Attention Network with Two Step Training": "independently trained bidirectional LSTM for each modality such as text, audio, and video for"
        },
        {
          "2\nModality Attention Network with Two Step Training": "emotion recognition task.\n(2) Attention Block - this block estimates a relative salience score for"
        },
        {
          "2\nModality Attention Network with Two Step Training": "each modality. MAN overview is shown in Figure 1."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Figure 1: An overview of the proposed Modality Aware Attention Network. MAN contains two main"
        },
        {
          "2\nModality Attention Network with Two Step Training": "components: (1) Independently pre-trained sub-networks that learn the representation of different"
        },
        {
          "2\nModality Attention Network with Two Step Training": "modalities. (2) An attention module that decides which modality is important in a particular example."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Sub-networks: The goal of sub-networks is to learn intra-modal dynamics and produce an embedding"
        },
        {
          "2\nModality Attention Network with Two Step Training": "for each modality that will be used by the attention block. We use a bidirectional Long-short Term"
        },
        {
          "2\nModality Attention Network with Two Step Training": "Memory (LSTM) [4] for each sub-network. LSTMs have been widely used in modeling temporal data"
        },
        {
          "2\nModality Attention Network with Two Step Training": "in many tasks including text classiﬁcation [5], video classiﬁcation [6], and voice activity detection [7]."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Note that LSTM can be replaced with any neural architecture and the sub-networks are not required"
        },
        {
          "2\nModality Attention Network with Two Step Training": "to have the same architecture."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Modality-Speciﬁc Pre-training: The modality-speciﬁc sub-networks are ﬁrst pre-trained indepen-"
        },
        {
          "2\nModality Attention Network with Two Step Training": "dently on the task of interest. This is done for two reasons: (1) Pre-training allows better learning"
        },
        {
          "2\nModality Attention Network with Two Step Training": "of intra-modal dynamics since the learning process is not affected by any other modalities at this"
        },
        {
          "2\nModality Attention Network with Two Step Training": "time.\n(2) Data from different modalities have different convergence rates, if we train the network"
        },
        {
          "2\nModality Attention Network with Two Step Training": "end-to-end without the pre-training step, the network tends to focus on the modality that converges"
        },
        {
          "2\nModality Attention Network with Two Step Training": "faster ignoring others."
        },
        {
          "2\nModality Attention Network with Two Step Training": "The softmax layer at the end of the modality-speciﬁc sub-network is removed after pre-training and"
        },
        {
          "2\nModality Attention Network with Two Step Training": "the last hidden layer is used as a feature representation of that modality and is given as the input to"
        },
        {
          "2\nModality Attention Network with Two Step Training": "the attention block as shown in Figure 1."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Formally, let the input to the language sub-network be denoted as L = [l0, . . . , lt, . . . , , ln] where lt is"
        },
        {
          "2\nModality Attention Network with Two Step Training": "the tth word in the sentence and l is the feature embedding. Similarly, the input to acoustic and visual"
        },
        {
          "2\nModality Attention Network with Two Step Training": "sub-network is denoted as A and V respectively. The output of the sub-networks is the last hidden state"
        },
        {
          "2\nModality Attention Network with Two Step Training": "of modality-speciﬁc network can be written as: hl = Netl(L),\nha = Neta(A),\nhv = Netv(V )."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Attention Block: Hidden layers from sub-networks hl, ha, and hv each have a same dimension of"
        },
        {
          "2\nModality Attention Network with Two Step Training": "N . Matrix H is the input to attention block where H = [hl, ha, hv]; H has a dimension of m × N ,"
        },
        {
          "2\nModality Attention Network with Two Step Training": "where m is the number of modalities. The output of the attention block is a weight matrix A."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Attention matrix A is given as: sof tmax (cid:0)W2\n(cid:0)tanh (cid:0)W1H T + b1\n(cid:1)(cid:1) + b2\n(cid:1). Weight matrix W1 and"
        },
        {
          "2\nModality Attention Network with Two Step Training": "W2 have dimensions of k × N and 1 × k respectively, where k is a hyper-parameter; b1 and b2 are bias"
        },
        {
          "2\nModality Attention Network with Two Step Training": "parameter and the sof tmax() ensures that all the computed weights are sum to 1. In the attention"
        },
        {
          "2\nModality Attention Network with Two Step Training": "matrix A = [wl, wa, wv], weights wl, wa, and wv represent importance of different modalities."
        },
        {
          "2\nModality Attention Network with Two Step Training": "Output of MAN: The hidden layer of each sub-network is multiplied by its weights, which is"
        },
        {
          "2\nModality Attention Network with Two Step Training": "then concatenated together and passed to a fully connected layer and a sof tmax layer\nfor\nthe"
        },
        {
          "2\nModality Attention Network with Two Step Training": "ﬁnal classiﬁcation. The entire architecture is then trained end-to-end together, which ﬁne-tunes the"
        },
        {
          "2\nModality Attention Network with Two Step Training": "sub-networks in the process. This helps to capture both intra-modal and inter-modal dynamics."
        },
        {
          "2\nModality Attention Network with Two Step Training": "3\nExperiments"
        },
        {
          "2\nModality Attention Network with Two Step Training": "We evaluated the proposed framework on three real-world applications: sentiment analysis, emotion"
        },
        {
          "2\nModality Attention Network with Two Step Training": "recognition, and speaker trait recognition. The experiments have been designed to investigate the"
        },
        {
          "2\nModality Attention Network with Two Step Training": "2"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: There is no single model that always",
      "data": [
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "compared to other networks.\n(b) The effect of pre-training the sub-networks versus end-to-end"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "training of the entire network for multimodal networks.\n(c) The importance of attention module"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "in differentiating between important and ambiguous modalities. Detailed description of datasets,"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "baselines and experiments is available in the supplementary material."
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Datasets: We selected three publicly available multimodal datasets that contain spoken language,"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "acoustic and visual information. Multimodal Sentiment Analysis CMU MOSI dataset [8], Multimodal"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Emotion Recognition CMU MOSEI dataset [9] and Multimodal Speaker Trait Recognition Persuasion"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Opinion Multimodal POM dataset [10]."
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Computational Descriptors: Language Features: GloVe [11] word embeddings are used to convert"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "the transcripts into word embeddings. Acoustic Features: COVAREP [12] acoustic analysis frame-"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "work is used to extract low level acoustic features. Visual Features: Facial expression analysis toolkit"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "FACET [13] is used as visual feature extractor."
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Baseline Models We compare the performance of MAN to a variety of models for multimodal"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "language analysis; all models are trained using the same feature embeddings: LF-LSTM Late Fusion"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "LSTM, MAF Modality Attention Fusion [14], TFN Tensor Fusion Network [15], LMF Low-rank"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Multimodal Fusion [16] , MFN Memory Fusion Network[17], MFM Multimodal Factorization"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Model [18] and RAVEN Recurrent Attended Variation Embedding Network [19]."
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "Evaluation Metrics For regression, we report Mean Absolute Error. For classiﬁcation accuracy, we"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "report AC where C is the number of classes. In addition, we report the mean epoch training time in"
        },
        {
          "following: (a) The performance of MAN in terms of accuracy, complexity, and training time when": "seconds and inference time in milliseconds."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: There is no single model that always",
      "data": [
        {
          "competitive performance when compared against the other models. Generally best performing models": "are RAVEN and MAN; while MAN requires less training time."
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": ""
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "Method"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "LF-LSTM"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "MAF"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "TFN"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "LMF"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "MFN"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "MFM"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "RAVEN"
        },
        {
          "competitive performance when compared against the other models. Generally best performing models": "MAN"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: showsthatpre-trainingsub-networksbeforeend-to-endtrainingimprovestheaccuracyonboth.",
      "data": [
        {
          "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column": "shows the difference in converge rate of each modalities. The 2nd column shows the average attention"
        },
        {
          "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column": "weights over all examples per epochs. The 3rd column shows the the average attention weights over"
        },
        {
          "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column": "all examples per epochs if sub-networks we pre-trained before added to the end-to-end network."
        },
        {
          "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column": "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment"
        },
        {
          "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column": "with pre-training for LF-LSTM and MAF since they allow separation of sub-networks. Table 2"
        },
        {
          "Figure 2: The effect of pre-training multimodal networks on attention weights. The 1st column": "shows that pre-training sub-networks before end-to-end training improves the accuracy on both."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: showsthatpre-trainingsub-networksbeforeend-to-endtrainingimprovestheaccuracyonboth.",
      "data": [
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": ""
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": "shows that pre-training sub-networks before end-to-end training improves the accuracy on both."
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": ""
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": "LF-LSTM"
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": "0.747"
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": "0.762"
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": "Table 2: A2 with & without Pre-training."
        },
        {
          "The effect of pre-training on different multimodal architectures: We repeated MOSI experiment": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1]\nJiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal"
        },
        {
          "References": "deep learning.\nIn Proceedings of the 28th international conference on machine learning (ICML-11), pages"
        },
        {
          "References": "689–696, 2011."
        },
        {
          "References": "[2] Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classiﬁcation networks"
        },
        {
          "References": "hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages"
        },
        {
          "References": "12695–12705, 2020."
        },
        {
          "References": "[3] Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, and Davide Testuggine. Supervised multimodal bitrans-"
        },
        {
          "References": "formers for classifying images and text. arXiv preprint arXiv:1909.02950, 2019."
        },
        {
          "References": "IEEE Transactions on\n[4] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks."
        },
        {
          "References": "Signal Processing, 45(11):2673–2681, 1997."
        },
        {
          "References": "[5]\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn"
        },
        {
          "References": "Advances in neural information processing systems, pages 3104–3112, 2014."
        },
        {
          "References": "[6]\nJeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan,"
        },
        {
          "References": "Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and"
        },
        {
          "References": "description.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages"
        },
        {
          "References": "2625–2634, 2015."
        },
        {
          "References": "[7] Thad Hughes and Keir Mierle. Recurrent neural networks for voice activity detection.\nIn 2013 IEEE"
        },
        {
          "References": "International Conference on Acoustics, Speech and Signal Processing, pages 7378–7382. IEEE, 2013."
        },
        {
          "References": "[8] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Multimodal sentiment intensity"
        },
        {
          "References": "analysis in videos: Facial gestures and verbal messages.\nIEEE Intelligent Systems, 31(6):82–88, 2016."
        },
        {
          "References": "[9] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency."
        },
        {
          "References": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.\nIn"
        },
        {
          "References": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long"
        },
        {
          "References": "Papers), pages 2236–2246, 2018."
        },
        {
          "References": "[10] Sunghyun Park, Han Suk Shim, Moitreya Chatterjee, Kenji Sagae, and Louis-Philippe Morency. Com-"
        },
        {
          "References": "putational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction"
        },
        {
          "References": "approach.\nIn Proceedings of the 16th International Conference on Multimodal Interaction, pages 50–57."
        },
        {
          "References": "ACM, 2014."
        },
        {
          "References": "[11]\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word repre-"
        },
        {
          "References": "sentation.\nIn Proceedings of the 2014 conference on empirical methods in natural language processing"
        },
        {
          "References": "(EMNLP), pages 1532–1543, 2014."
        },
        {
          "References": "[12] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. Covarep—a collabora-"
        },
        {
          "References": "tive voice analysis repository for speech technologies.\nIn 2014 ieee international conference on acoustics,"
        },
        {
          "References": "speech and signal processing (icassp), pages 960–964. IEEE, 2014."
        },
        {
          "References": "[13]\niMotion, 2017. URL https://imotions.com."
        },
        {
          "References": "[14] Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen, Xinyu Li, and Ivan Marsic. Hybrid attention based"
        },
        {
          "References": "multimodal network for spoken language classiﬁcation. In Proceedings of the 27th International Conference"
        },
        {
          "References": "on Computational Linguistics, pages 2379–2390, 2018."
        },
        {
          "References": "[15] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion"
        },
        {
          "References": "network for multimodal sentiment analysis. arXiv preprint arXiv:1707.07250, 2017."
        },
        {
          "References": "[16] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-"
        },
        {
          "References": "Philippe Morency. Efﬁcient low-rank multimodal fusion with modality-speciﬁc factors. arXiv preprint"
        },
        {
          "References": "arXiv:1806.00064, 2018."
        },
        {
          "References": "[17] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe"
        },
        {
          "References": "Morency. Memory fusion network for multi-view sequential learning.\nIn Thirty-Second AAAI Conference"
        },
        {
          "References": "on Artiﬁcial Intelligence, 2018."
        },
        {
          "References": "[18] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov."
        },
        {
          "References": "Learning factorized multimodal representations. arXiv preprint arXiv:1806.06176, 2018."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "shift: Dynamically adjusting word representations using nonverbal behaviors.\nIn Proceedings of the AAAI"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "Conference on Artiﬁcial Intelligence, volume 33, pages 7216–7223, 2019."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[20] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel,"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "and Yoshua Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention.\nIn"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "International conference on machine learning, pages 2048–2057, 2015."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[21] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Koˇcisk`y, and Phil Blunsom. Reason-"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "ing about entailment with neural attention. arXiv preprint arXiv:1509.06664, 2015."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[22]\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "arXiv preprint arXiv:1612.08220, 2016."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[23]\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. Explainable prediction"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "of medical codes from clinical text. arXiv preprint arXiv:1802.05695, 2018."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[24]\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Generating token-level"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "explanations for natural language inference. arXiv preprint arXiv:1904.10717, 2019."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[25]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[26] Michael Grimm, Kristian Kroschel, and Shrikanth Narayanan. The vera am mittag german audio-visual"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "emotional speech database. In 2008 IEEE international conference on multimedia and expo, pages 865–868."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "IEEE, 2008."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[27] Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, et al. Collecting large, richly annotated"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "facial-expression databases from movies.\nIEEE multimedia, 19(3):34–41, 2012."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[28] Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne.\nIntroducing the recola multi-"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "modal corpus of remote collaborative and affective interactions. In 2013 10th IEEE international conference"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "and workshops on automatic face and gesture recognition (FG), pages 1–8. IEEE, 2013."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[29] Sanjay Bilakhia, Stavros Petridis, Anton Nijholt, and Maja Pantic. The mahnob mimicry database: A"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "database of naturalistic human interactions. Pattern recognition letters, 66:52–61, 2015."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[30] Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency, and Eric P Xing. Select-additive learning:"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "Improving generalization in multimodal sentiment analysis.\nIn 2017 IEEE International Conference on"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "Multimedia and Expo (ICME), pages 949–954. IEEE, 2017."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[31] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "Chang, Sungbok Lee, and Shrikanth S Narayanan.\nIemocap: Interactive emotional dyadic motion capture"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "database. Language resources and evaluation, 42(4):335, 2008."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[32] Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi.\nTowards multimodal sentiment analysis:"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "Harvesting opinions from the web.\nIn Proceedings of the 13th international conference on multimodal"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "interfaces, pages 169–176. ACM, 2011."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[33] Martin Wöllmer, Felix Weninger, Tobias Knaup, Björn Schuller, Congkai Sun, Kenji Sagae, and Louis-"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "IEEE Intelligent\nPhilippe Morency. Youtube movie reviews: Sentiment analysis in an audio-visual context."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "Systems, 28(3):46–53, 2013."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[34] Verónica Pérez-Rosas, Rada Mihalcea, and Louis-Philippe Morency. Utterance-level multimodal sentiment"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "the 51st Annual Meeting of\nthe Association for Computational Linguistics\nanalysis.\nIn Proceedings of"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "(Volume 1: Long Papers), pages 973–982, 2013."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[35]\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihal-"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "cea. Meld: A multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "arXiv:1810.02508, 2018."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[36] Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Amir Hussain. Convolutional mkl based multimodal"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "emotion recognition and sentiment analysis.\nIn 2016 IEEE 16th international conference on data mining"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "(ICDM), pages 439–448. IEEE, 2016."
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "[37]\nSoujanya Poria, Erik Cambria, and Alexander Gelbukh. Deep convolutional neural network textual features"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "and multiple kernel learning for utterance-level multimodal sentiment analysis.\nIn Proceedings of the 2015"
        },
        {
          "[19] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Words can": "conference on empirical methods in natural language processing, pages 2539–2544, 2015."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "2013."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[39] Torsten Wörtwein and Stefan Scherer. What really matters—an information gain analysis of questions and"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "reactions in automated ptsd screenings.\nIn 2017 Seventh International Conference on Affective Computing"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "and Intelligent Interaction (ACII), pages 15–20. IEEE, 2017."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[40] Behnaz Nojavanasghari, Deepak Gopinath,\nJayanth Koushik, Tadas Baltrušaitis, and Louis-Philippe"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "the 18th ACM\nMorency.\nDeep multimodal\nfusion for persuasiveness prediction.\nIn Proceedings of"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "International Conference on Multimodal Interaction, pages 284–288. ACM, 2016."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[41] Hai Pham, Thomas Manzini, Paul Pu Liang, and Barnabas Poczos.\nSeq2seq2sentiment: Multimodal"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "sequence to sequence models for sentiment analysis. arXiv preprint arXiv:1807.03915, 2018."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[42] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Multi-attention recurrent network for human communication comprehension.\nIn Thirty-Second AAAI"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Conference on Artiﬁcial Intelligence, 2018."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[43]\nPaul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-Philippe Morency. Multimodal language analysis with"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "recurrent multistage fusion. arXiv preprint arXiv:1808.03920, 2018."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Journal of\n[44] Paul Ekman, Wallace V Freisen, and Sonia Ancoli. Facial signs of emotional experience."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "personality and social psychology, 39(6):1125, 1980."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[45] Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen, Xinyu Li, and Ivan Marsic. Multimodal affective"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "analysis using hierarchical attention strategy with word-level alignment. arXiv preprint arXiv:1805.08660,"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "2018."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "[46] Stephen C Levinson and Judith Holler. The origin of human multi-modal communication. Philosophical"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Transactions of the Royal Society B: Biological Sciences, 369(1651):20130302, 2014."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Supplementary"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Related Work"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Previous work on multimodal reasoning focuses on audio-visual data [26–30]. Human commu-"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "nication uses both verbal and nonverbal modalities.\nIgnoring the contribution of text can cause"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "misinterpretation as shown in example (b) in Figure 4. Recent work recognizes the importance"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "of adding text as a modality, leading to a blossom of language-audio-visual datasets in tasks such"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "as emotion recognition and sentiment analysis [31–34, 8, 9, 35]. Models in previous work can be"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "divided into three major categories: (1) early fusion, (2) late fusion, and (3) multi-view learning."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Early Fusion concatenates input-level feature from different modalities. The fused features are then"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "passed to a generic model. Intra-modal dynamics are not modeled explicitly when using this method"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "of fusion [36, 37, 30]. Early fusion methods can preserve the temporal nature of data by using"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "recurrent neural networks, or totally ignores the time factor when using classical machine learning"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "methods such as Support Vector Machines [8, 32]. Due to the lack of speciﬁc intra-modal models,"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "this fusion method tends to overﬁt on small datasets [38]."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Late Fusion models each modality separately, then the outputs are combined in the decision phase."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "This can be done by majority voting , weighted averaging [39], or the output of modality networks"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "are combined with a joint neural network that is trained to make ﬁnial decision [40]."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "Multi-view learning is a broader class of methods that aim to learn both intra-modal dynamics"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "and inter-modality dynamics jointly. To place our work in the context of prior research we divide"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "(a) Non-attention based fusion methods:\nmulti-view learning into the following subcategories:"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "this includes tensor-based multimodal representations created from expensive tensor product [15] or"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "low-rank tensors approximation [16], generative representation learning [41, 1], word-level factorized"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "multimodal representation [18] and variations to word representations by word-level fusion with"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "nonverbal features [19]. Such methods (excluding [16]) are rather slow and suffer from exponentially"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "(b) Attention based\nincreasing computational complexity as the number of modalities increase."
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "fusion methods: The work in [17, 42] used attention to discover interactions between modalities"
        },
        {
          "[38] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634,": "through time. Method proposed in [43] decomposes the fusion problem into multiple stages and"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "and modality attention to classify utterance-level speech data."
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "MAN belongs to the attention based fusion subcategory; however, we differ from current methods"
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "by our ability to view modalities independently and quantize how each modality inﬂuences the ﬁnal"
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "decision of the network. Additionally, the pre-training step allows our model to produce competitive"
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "results when compared to more complex architectures."
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "Detailed description of the experiments"
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "Datasets"
        },
        {
          "focuses on a subset of multimodal signals at each stage. In [14], authors apply both feature attention": "Each datasets that contain spoken language, acoustic and visual information."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "Multimodal SDK.1"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "• Language Features: Pre-trained word embeddings GloVe [11] are used to convert\nthe"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "transcripts of videos into sequence of 300-dimensional word embeddings."
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "• Acoustic Features: COVAREP [12] acoustic analysis framework is used to extract\nlow"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "level acoustic features. The features include pitch tracking, polarity estimation, glottal"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "closure instants, spectral envelope, glottal ﬂow estimation and 12 Melfrequency cepstral"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "coefﬁcients along with other features."
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "• Visual Features:\nFacial expression analysis toolkit FACET2\nis used as visual\nfeature"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "extractor. Features include facial action units, facial landmarks, head pose, gaze tracking"
        },
        {
          "Feature Embedding\nFollowing prior practice [16, 43, 45, 19], we use features provided by CMU-": "and HOG features."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "feature attention and modality attention. The hybrid attention architecture helps the system focus on"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "learning informative representations for both modality-speciﬁc feature extraction and model fusion."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Tensor Fusion Network (TFN) [15] learns intra-modality and inter-modality dynamics by creating a"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "multi-dimensional tensor that captures unimodal, bimodal and trimodal interactions across modalities."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Low-rank Multimodal Fusion (LMF) [16] performs multimodal fusion using low-rank tensors to"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "improve efﬁciency."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Memory Fusion Network (MFN) [17] a multi-view sequential learning neural network that accounts"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "for both view-speciﬁc and cross-view interactions through time. View-speciﬁc interactions are learned"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "through assigning an LSTM function to each view. The cross-view interactions are identiﬁed using a"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "special attention mechanism and summarized through time with a multi-view gated Memory."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Multimodal Factorization Model (MFM) [18] a model that factorizes representations into two sets"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "of independent factors: multimodal discriminative and modality-speciﬁc generative factors then"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "optimizes for a joint generative-discriminative objective across multimodal data and labels."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Recurrent Attended Variation Embedding Network (RAVEN) [19] a model for human multimodal"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "language that considers the ﬁne-grained structure of nonverbal subword sequences and dynamically"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "shifts the word representations based on these nonverbal cues."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Training Models"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "All models are trained on NVIDIA Tesla K80 GPU. For each dataset, we use the hyper-parameters"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "reported by the baseline methods for that particular dataset. If the hyper-parameters were not reported"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "in the paper, we perform an extensive grid search to ﬁnd the best performing hyper-parameters. We"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "use the same grid search method to ﬁnd the best hyper-parameters for our method as well."
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Understanding the Importance of Different Modalities through Attention"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "Human’s ability to communicate effectively comes from their understanding of different modalities"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "[46]. The context in which a modality occurs changes its underlying meaning; a laugh can be both"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "happy or sarcastic. In such cases, not all modalities are equally important for revealing the meaning of"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "human interaction. Consider examples in Figure 4 for an emotion classiﬁcation task: In example (a),"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "we can easily perceive that the emotion is fear as the visual signal is very prominent. In example (b),"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "visual signals are ambiguous and not useful; however, by looking at the sentence, the word \"Wow\""
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "allows us to conclude that the emotion is surprise. In example (c), visual and language signals are"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "misleading and acoustic features help make correct perception. Humans can identify the important"
        },
        {
          "Hybrid Attention based Multimodal Network (MAF) [14] a deep multimodal network with both": "modality in cases where some modalities are ambiguous or may lead to wrong judgment."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": "multimodal data by identifying the importance of modality."
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": "from MOSI where MAN can successfully detect"
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": ""
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": ""
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": ""
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": ""
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": ""
        },
        {
          "The goal of the attention block in MAN is to mimic human’s ability to understand the context of": "makes it in some sense more explainable."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal deep learning",
      "authors": [
        "Jiquan Ngiam",
        "Aditya Khosla",
        "Mingyu Kim",
        "Juhan Nam",
        "Honglak Lee",
        "Andrew Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th international conference on machine learning (ICML-11)"
    },
    {
      "citation_id": "2",
      "title": "What makes training multi-modal classification networks hard?",
      "authors": [
        "Weiyao Wang",
        "Du Tran",
        "Matt Feiszli"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Supervised multimodal bitransformers for classifying images and text",
      "authors": [
        "Douwe Kiela",
        "Suvrat Bhooshan",
        "Hamed Firooz",
        "Davide Testuggine"
      ],
      "year": "2019",
      "venue": "Supervised multimodal bitransformers for classifying images and text",
      "arxiv": "arXiv:1909.02950"
    },
    {
      "citation_id": "4",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "Ilya Sutskever",
        "Oriol Vinyals",
        "Quoc V Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Long-term recurrent convolutional networks for visual recognition and description",
      "authors": [
        "Jeffrey Donahue",
        "Lisa Hendricks",
        "Sergio Guadarrama",
        "Marcus Rohrbach",
        "Subhashini Venugopalan",
        "Kate Saenko",
        "Trevor Darrell"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Recurrent neural networks for voice activity detection",
      "authors": [
        "Thad Hughes",
        "Keir Mierle"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "9",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
      "authors": [
        "Sunghyun Park",
        "Suk Han",
        "Moitreya Shim",
        "Kenji Chatterjee",
        "Louis-Philippe Sagae",
        "Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "12",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "14",
      "title": "Hybrid attention based multimodal network for spoken language classification",
      "authors": [
        "Yue Gu",
        "Kangning Yang",
        "Shiyu Fu",
        "Shuhong Chen",
        "Xinyu Li",
        "Ivan Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "16",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "17",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "19",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Reasoning about entailment with neural attention",
      "authors": [
        "Tim Rocktäschel",
        "Edward Grefenstette",
        "Karl Hermann",
        "Tomáš Kočiskỳ",
        "Phil Blunsom"
      ],
      "year": "2015",
      "venue": "Reasoning about entailment with neural attention",
      "arxiv": "arXiv:1509.06664"
    },
    {
      "citation_id": "22",
      "title": "Understanding neural networks through representation erasure",
      "authors": [
        "Jiwei Li",
        "Will Monroe",
        "Dan Jurafsky"
      ],
      "year": "2016",
      "venue": "Understanding neural networks through representation erasure",
      "arxiv": "arXiv:1612.08220"
    },
    {
      "citation_id": "23",
      "title": "Explainable prediction of medical codes from clinical text",
      "authors": [
        "James Mullenbach",
        "Sarah Wiegreffe",
        "Jon Duke",
        "Jimeng Sun",
        "Jacob Eisenstein"
      ],
      "year": "2018",
      "venue": "Explainable prediction of medical codes from clinical text",
      "arxiv": "arXiv:1802.05695"
    },
    {
      "citation_id": "24",
      "title": "Generating token-level explanations for natural language inference",
      "authors": [
        "James Thorne",
        "Andreas Vlachos",
        "Christos Christodoulopoulos",
        "Arpit Mittal"
      ],
      "year": "2019",
      "venue": "Generating token-level explanations for natural language inference",
      "arxiv": "arXiv:1904.10717"
    },
    {
      "citation_id": "25",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "26",
      "title": "The vera am mittag german audio-visual emotional speech database",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "2008 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "27",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "28",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "29",
      "title": "The mahnob mimicry database: A database of naturalistic human interactions",
      "authors": [
        "Sanjay Bilakhia",
        "Stavros Petridis",
        "Anton Nijholt",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "Pattern recognition letters"
    },
    {
      "citation_id": "30",
      "title": "Select-additive learning: Improving generalization in multimodal sentiment analysis",
      "authors": [
        "Haohan Wang",
        "Aaksha Meghawat",
        "Louis-Philippe Morency",
        "Eric Xing"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "31",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "32",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "33",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "Martin Wöllmer",
        "Felix Weninger",
        "Tobias Knaup",
        "Björn Schuller",
        "Congkai Sun",
        "Kenji Sagae",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "34",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "Verónica Pérez-Rosas",
        "Rada Mihalcea",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "36",
      "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Iti Chaturvedi",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "year": "2016",
      "venue": "2016 IEEE 16th international conference on data mining (ICDM)"
    },
    {
      "citation_id": "37",
      "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Alexander Gelbukh"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "38",
      "title": "A survey on multi-view learning",
      "authors": [
        "Chang Xu",
        "Dacheng Tao",
        "Chao Xu"
      ],
      "year": "2013",
      "venue": "A survey on multi-view learning",
      "arxiv": "arXiv:1304.5634"
    },
    {
      "citation_id": "39",
      "title": "What really matters-an information gain analysis of questions and reactions in automated ptsd screenings",
      "authors": [
        "Torsten Wörtwein",
        "Stefan Scherer"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "Behnaz Nojavanasghari",
        "Deepak Gopinath",
        "Jayanth Koushik",
        "Tadas Baltrušaitis",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "41",
      "title": "Seq2seq2sentiment: Multimodal sequence to sequence models for sentiment analysis",
      "authors": [
        "Hai Pham",
        "Thomas Manzini",
        "Paul Liang",
        "Barnabas Poczos"
      ],
      "year": "2018",
      "venue": "Seq2seq2sentiment: Multimodal sequence to sequence models for sentiment analysis",
      "arxiv": "arXiv:1807.03915"
    },
    {
      "citation_id": "42",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal language analysis with recurrent multistage fusion",
      "arxiv": "arXiv:1808.03920"
    },
    {
      "citation_id": "44",
      "title": "Facial signs of emotional experience",
      "authors": [
        "Paul Ekman",
        "Sonia Wallace V Freisen",
        "Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "45",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Yue Gu",
        "Kangning Yang",
        "Shiyu Fu",
        "Shuhong Chen",
        "Xinyu Li",
        "Ivan Marsic"
      ],
      "year": "2018",
      "venue": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "arxiv": "arXiv:1805.08660"
    },
    {
      "citation_id": "46",
      "title": "The origin of human multi-modal communication",
      "authors": [
        "C Stephen",
        "Judith Levinson",
        "Holler"
      ],
      "year": "2014",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    }
  ]
}