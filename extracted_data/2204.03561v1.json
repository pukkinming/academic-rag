{
  "paper_id": "2204.03561v1",
  "title": "Emotional Speech Recognition With Pre-Trained Deep Visual Models",
  "published": "2022-04-06T11:27:59Z",
  "authors": [
    "Waleed Ragheb",
    "Mehdi Mirzapour",
    "Ali Delfardi",
    "Hélène Jacquenet",
    "Lawrence Carbon"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a new methodology for emotional speech recognition using visual deep neural network models. We employ the transfer learning capabilities of the pre-trained computer vision deep models to have a mandate for the emotion recognition in speech task. In order to achieve that, we propose to use a composite set of acoustic features and a procedure to convert them into images. Besides, we present a training paradigm for these models taking into consideration the different characteristics between acoustic-based images and regular ones. In our experiments, we use the pre-trained VGG-16 model and test the overall methodology on the Berlin EMO-DB dataset for speakerindependent emotion recognition. We evaluate the proposed model on the full list of the seven emotions and the results set a new state-of-the-art.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the information technology revolution, it became mandatory -not just an option -for many computer systems to express and recognize effects and emotions to attain creative and intelligent behavior. The main purpose is to understand the emotional states expressed by the human subjects so that personalized responses can be delivered accordingly. Humans are still way ahead of machines in detecting and recognizing the different types of effects including emotions  (Alu et al., 2017) . Therefore, Emotional Intelligence (EI) is deemed as the turning point from moving from the narrow definition of Artificial Intelligence (AI) to a more general humanized AI. Speech signals are considered one of the main channels in human communications. Naturally, humans could effectively recognize the emotional aspects of speech signals.\n\nThe emotional state of the speech will not change the linguistics of the uttered speech, but it reflects many of the speaker's intents and other latent information about the mental and physical state and attitude  (Narayanan et Georgiou, 2013) . Therefore, empowering computer systems with speech emotional recognition features can have a significant impact on personalizing the user experience in many applications and sectors such as marketing, healthcare, customer satisfaction, gaming experience improvement, social media analysis and stress monitoring.  (Nassif et al., 2019; Proksch et al., 2019; Rouhi et al., 2019) .\n\nEarlier emotional speech recognition had some processes in common with automatic speech recognition. It involved many feature engineering steps that may play a substantial role in model selection and training paradigm. Acoustical speech features reported in the literature could be categorized into continuous, qualitative, spectral, and temporal features  (Bandela et Kumar, 2017) . At the time, most of the models were classical machine learning and statistical models. Most of these models train from scratch on a varied set of features or the original speech signal itself. Different pre-trained models have been released and become substantially available for many applications in Computer Vision and Natural Language Processing.\n\nAs for emotional speech recognition, some pre-trained transferable models such as spee-chVGG  (Beckmann et al., 2019)  have been introduced which act as feature extractor for different speech processing tasks. Although speechVGG has got its inspiration from VGG (Simonyan et Zisserman, 2014)-a well-known computer vision architecture-it is trained from scratch with the LibriSpeech dataset  (Panayotov et al., 2015) . We mainly focus on how an existing pre-trained computer vision model, such as VGG, can efficiently be fine-tuned in a different domain such as emotional speech recognition. This can reduce the cost of further expensive and exhaustive training for new domains and be beneficial for practical and industrial use cases.\n\nIn this work, we present an experimental study using one of the most powerful pre-trained visual models VGG to tackle the aforementioned problem. Our proposed methodology is : (i) to present a novel order of frequency-domain voice features that transform the speech acoustic signals into compound ready-to-use 3D images for existing pre-trained computer vision models ; (ii) to apply simple signal-level and frequency domain voice-level data augmentation techniques ; (iii) to introduce simple, and yet efficient mini-batch padding technique ; and finally, (iv) to fine-tune the VGG-16 (with batch-normalization) pre-trained model on classical image classification tasks. We have applied the proposed configurations and some of their variants on one of the most well-known datasets for emotional recognition (Berlin EmoDB  (Burkhardt et al., 2005) ) and the results are very competitive to the state-of-the-art and outperform many strong baselines. Our implementation is made available for public 1  .\n\nThe paper is organized as follows : in section 2 we present a literature review and the related works. In section 3, we introduce the proposed methodology including the considered acoustic features and all the variants of the used models. Section 4 addresses all the experimental setups and the results followed by brief discussions. Finally, we conclude the study and experiments in section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Traditional models proposed for emotional speech recognition and classification are based on the same models used for automatic speech recognition like HMM, GP, SVM ... etc (Has-FIG.  1  -The original VGG-16 architecture  (Simonyan et Zisserman, 2014)  san et  Damper, 2010; Lin et Wei, 2005; Azmy et al., 2013) . These models involve extensive feature engineering steps that are sensitive and may significantly affect the structure of the entire method  (Pandey et al., 2019) . With the development of the deep learning models, the speech recognition systems benefited from the end-to-end learning paradigm. This enables the model to learn all the steps from the input to the final output simultaneously including feature extraction. Similarly, the emotional models have followed the same course. There is a lot of effort and research on employing these algorithms to recognize emotions from speech. More specifically, some of these models used the ability of Convolutional Neural Networks (CNN) to learn features from input signals  (Bertero et Fung, 2017; Mekruksavanich et al., 2020) . Another type of model makes use of the sequential nature of the speech signals and utilized Recurrent Neural Networks (RNN) architectures like long short-term memory (LSTM)  (Tzinis et Potamianos, 2017; Fayek et al., 2017) . Some models combined both types of architectures like in ConvLSTM  (Kurpukdee et al., 2017) .\n\nRecently, there has been a breakthrough improvement in the transfer learning capabilities of deep models with the powerful pre-trained visual models like AlexNet, VGG, Yolo-2 ... etc  (Voulodimos et al., 2018) . The main idea is to train these models for large and different image classification tasks and transfer the feature selection parts of these models to be used in the downstream tasks. Figure  1  shows an example of these models and the one used in our experiments (VGG). This complies with the fact that speech signals could be represented as visual features. For instance, the Mel frequency Cepstral coefficient (MFCC), Log Frequency Power Coefficients (LFPC), and Log Mel Spectrogram could be considered as 2D/3D images that could carry emotion-related information  (Wang, 2014) . This will permit us to take advantage of the pre-trained visual models to extract visual features presented in the input acoustic features without the need for large datasets in an indirect supervision fashion. The work presented in this paper is to some extent related to a previous work in  (Zhang et al., 2017) . The authors use only Log Mel Spectrogram on three channels of deltas as the input features and a pre-trained AlexNet  (Krizhevsky et al., 2012)  as the visual model. This model extracts the visual feature representation of the input and then involves a linear SVM model for the target classification task.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Methodology",
      "text": "In this section, we present the preprocessing steps and the proposed set of acoustic features that we used. Besides, we introduce the architecture of the visual model applied in the experiments with all the considered variants.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acoustic Features",
      "text": "We tried different types of acoustic features to get the proper representation of the speech signals in the form of images. The frequency-domain features reveal a promising behavior. More specifically, we used the same spectral features reported in  (Issa et al., 2020)  and utilized the Librosa library  (McFee et al., 2015)  for the features extraction process. Additionally, we added one more feature and proposed a new method for integrating all these features into an image representation of the input speech signal. The complete set of used features are :\n\n1. Mel-frequency cepstral coefficients (MFCCs) 2. Mel-scaled spectrogram.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Power Spectrogram Or Chromagram",
      "text": "4. Sub-bands spectral contrast 5. Tonal centroid features (Tonnetz) 6. Average of the mel-scaled spectrogram of the harmonic and percussive components After removing small silence (pauses) in the speech signals, we compose the images by a normalized concatenation of all the previous features in three different channels like the Red-Green-Blue (RGB) decomposition of an image. In contrast to  (Issa et al., 2020) , we did not aggregate the time scale features for achieving a fixed sized vector. This is a crucial decision for fine-tuning the VGG models since they are data-greedy and aggregating the time-scale information (by summing or averaging functions) will eliminate a considerable amount of useful patterns resulting in accuracy reduction of our model. Nevertheless, this strategy has a side effect for some architectures : the resulting 3D images will vary in time-dimension axis. Our case study on EMO-DB dataset shows the size (3, X, 230) in which X varies in time axis between 50 and 700 depending on audio signal sizes. This, in principle, will not affect our model since VGG models require a minimum input size of the 3x32x32 which fits well with our settings.  show two examples of original speech visual features happiness and anger, respectively.\n\nIt is worth mentioning that the order of the features has an important role in getting an acceptable accuracy. To find the optimum order, we experimented with transforming our features to vectors by averaging values in the time axis, and then we have fed all the permutations of these compact vector features to a very simple logistic regression model classifier. We finally selected only a few candidates with the same accuracy. To make the final decision on the orders of features, we fine-tuned our VGG-16 model according to the shortlisted orders of features to get the best performance. This has given us the practical ground to find the best permutations of the order of the features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Training",
      "text": "For the visual deep model, we used the pre-trained version of 16-layer VGG architecture (VGG16)  (Simonyan et Zisserman, 2014)  which is proven to get the best performance in ILSVRC-2014 2 and became very popular in many image classification problems. The feature extraction layer groups are initialized from the pre-trained model weights, however, the classification layer group is fine-tuned from scratch with random weights. Moreover, we used the batch normalization variants of the VGG model for its effective regularization effect especially with training batches of smaller sizes. When creating the batches and before extracting the acoustic features, we applied signal-level padding to the maximum signal length in each training batch.\n\nMost of the available emotional speech datasets are small-sized. Accordingly, this was the primary motivation for relying on pre-trained models. In addition to that, and as most deep visual model training, we applied data augmentation. The idea here is not as simple as regular transformation done with the images -rotation, translation, flipping, cropping ... etc. Our proposed images in the considered problem are special types of images, so standard visual augmentation techniques will not be useful. Hence, we applied \"CutMix\" ;  (Yun et al., 2019)  a special augmentation and regularization strategy in which patches are cut and pasted among training images where the ground truth labels are also mixed proportionally in the area of the patches. \"CutMix\" efficiently uses training pixels and retains the regularization effect of regional dropout. Figure  2  shows two examples of original speech visual features (happiness and anger) representation before and after \"CutMix\".\n\nAs a regular classification deep learning model, we used cross entropy loss function with Adam optimizer  (Kingma et Ba, 2014) . We employed a learning rate of 10 -5 and a batch size of 16.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first present the used datasets. Then, we show the results of the proposed methodology before we end with some discussions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "In these experiments, we used Berlin EMO-DB  (Burkhardt et al., 2005)  with its speakerindependent configuration that contains a total of 535 emotional speech German utterances. Each speech signal is classified into one of the seven classes of emotions (Fear, Sadness, Dis- We split the dataset randomly into training and testing sets and preserve almost the same distributions of different emotions across both datasets as summarized in table  1",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "We tested the overall methodology described in section 3 to measure the classification performance on the test dataset. We use the accuracy metrics to enable the head-to-head comparison with the SOTA models and other reported baselines results. Moreover, we tested 6 possible variants of the model to enhance the ablation analysis of the proposed methodology components. The proposed variants and their definition are described as :   2  presents the results of all these variants which validate that the best performing model is corresponding to the complete proposed methodology. Besides, we compare this model (Model-A) to a set of strong baseline including the SOTA best-reported results for the considered dataset. We show this comparison in table  3 . Furthermore, we present the confusion matrix of our best model concerning all different classes in figure 4.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussions",
      "text": "By applying the overall methodology described in section 3, we have reached our best model (Model-A) with the 87.73 accuracy measure. Moreover, six other variants of the model (as in table 2) have been taken into consideration to enhance the ablation analysis of the proposed methodology. A quick analysis shows that mini-batch padding plays the most significant role Model-B shows the importance of using batch-normalization to make the final model better to some extent (batch-normalization was later introduced and added to VGG architectures).\n\nTable  3  shows our model outperformed the previous state-of-the-art results and many strong base-lines. Figure  4  shows the detailed confusion matrix for our best model result. It is worth mentioning that our analysis shows the importance of applying different components altogether (and not just only fine-tuning VGG-16) to outperform the state-of-the-art results.\n\nModel Accuracy (%) Badshah et. al.  (Badshah et al., 2017)  52.00 Wang et. al.  (Wang et al., 2015)  73.30 Lampropoulos et. at.  (Lampropoulos et Tsihrintzis, 2012)  83.93 Huangb et. al.  (Huang et al., 2014)  85.  20 Wu et. al. (Wu et al., 2011)  85.  80 Issa et. al. (Issa et al.,",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "Speech is one of the most preferred means in human communications. With the recent advances in speech technology and human/machine interaction, emotional speech recognition systems play an important role in bringing out emotionally intelligent behavior tasks. This study has focused on the VGG-16 (with batch normalization) pre-trained computer vision model and we have highlighted efficient components for fine-tuning VGG-16 for emotional speech The work presented in this paper could be extended to include more pre-trained computer vision deep models such as ResNet  (He et al., 2016) , EfficientNet  (Tan et Le, 2019) , ViT  (Dosovitskiy et al., 2020)  and Inceptionv3 (GoogLeNet)  (Szegedy et al., 2016) . Besides, extensive experiments can be performed on other emotional datasets like LSSED  (Fan et al., 2021) , IEMOCAP  (Busso et al., 2008) , and RAVDESS  (Livingstone et Russo, 2018)  . Moreover, it could be interesting to include other modalities for emotional recognition like text, images and videos.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Références",
      "text": "",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: – The original VGG-16 architecture (Simonyan et Zisserman, 2014)",
      "page": 3
    },
    {
      "caption": "Figure 1: shows an example of these models and the one used in our",
      "page": 3
    },
    {
      "caption": "Figure 2: shows two examples of original speech visual features (happiness",
      "page": 5
    },
    {
      "caption": "Figure 2: – Two examples of original speech visual features (happiness and anger) representation",
      "page": 6
    },
    {
      "caption": "Figure 3: – Example of a same speech uttered in different emotional states by a same speaker",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the detailed confusion matrix for our best model result. It is worth",
      "page": 8
    },
    {
      "caption": "Figure 4: – Confusion matrix of Model-A",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{ﬁrst.last}@contentside.com": "∗∗∗∗ CS Dept, Isfahan University, Isfahan, Iran"
        },
        {
          "{ﬁrst.last}@contentside.com": "alidelfardi@eng.ui.ac.ir"
        },
        {
          "{ﬁrst.last}@contentside.com": "Résumé.\nIn this paper, we propose a new methodology for emotional speech re-"
        },
        {
          "{ﬁrst.last}@contentside.com": "cognition using visual deep neural network models. We employ the transfer lear-"
        },
        {
          "{ﬁrst.last}@contentside.com": "ning capabilities of the pre-trained computer vision deep models to have a man-"
        },
        {
          "{ﬁrst.last}@contentside.com": "date for the emotion recognition in speech task. In order to achieve that, we pro-"
        },
        {
          "{ﬁrst.last}@contentside.com": "pose to use a composite set of acoustic features and a procedure to convert them"
        },
        {
          "{ﬁrst.last}@contentside.com": "into images. Besides, we present a training paradigm for\nthese models taking"
        },
        {
          "{ﬁrst.last}@contentside.com": "into consideration the different characteristics between acoustic-based images"
        },
        {
          "{ﬁrst.last}@contentside.com": "and regular ones.\nIn our experiments, we use the pre-trained VGG-16 model"
        },
        {
          "{ﬁrst.last}@contentside.com": "and test\nthe overall methodology on the Berlin EMO-DB dataset\nfor speaker-"
        },
        {
          "{ﬁrst.last}@contentside.com": "independent emotion recognition. We evaluate the proposed model on the full"
        },
        {
          "{ﬁrst.last}@contentside.com": "list of the seven emotions and the results set a new state-of-the-art."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": ""
        },
        {
          "1\nIntroduction": "many computer systems to express and recognize effects and emotions to attain creative and"
        },
        {
          "1\nIntroduction": "intelligent behavior. The main purpose is to understand the emotional states expressed by the"
        },
        {
          "1\nIntroduction": "human subjects so that personalized responses can be delivered accordingly. Humans are still"
        },
        {
          "1\nIntroduction": "way ahead of machines in detecting and recognizing the different"
        },
        {
          "1\nIntroduction": "emotions (Alu et al., 2017). Therefore, Emotional Intelligence (EI) is deemed as the turning"
        },
        {
          "1\nIntroduction": "point from moving from the narrow deﬁnition of Artiﬁcial Intelligence (AI) to a more general"
        },
        {
          "1\nIntroduction": "humanized AI. Speech signals are considered one of the main channels in human communica-"
        },
        {
          "1\nIntroduction": "tions. Naturally, humans could effectively recognize the emotional aspects of speech signals."
        },
        {
          "1\nIntroduction": "The emotional state of the speech will not change the linguistics of the uttered speech, but it re-"
        },
        {
          "1\nIntroduction": "ﬂects many of the speaker’s intents and other latent information about the mental and physical"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "state and attitude (Narayanan et Georgiou, 2013). Therefore, empowering computer systems"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "with speech emotional recognition features can have a signiﬁcant impact on personalizing the"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "user experience in many applications and sectors such as marketing, healthcare, customer satis-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "faction, gaming experience improvement, social media analysis and stress monitoring. (Nassif"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "et al., 2019; Proksch et al., 2019; Rouhi et al., 2019)."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Earlier emotional speech recognition had some processes in common with automatic speech"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "recognition. It involved many feature engineering steps that may play a substantial role in mo-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "del selection and training paradigm. Acoustical speech features reported in the literature could"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "be categorized into continuous, qualitative, spectral, and temporal features (Bandela et Kumar,"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "2017). At the time, most of the models were classical machine learning and statistical models."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Most of these models train from scratch on a varied set of features or the original speech signal"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "itself. Different pre-trained models have been released and become substantially available for"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "many applications in Computer Vision and Natural Language Processing."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "As for emotional speech recognition, some pre-trained transferable models such as spee-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "chVGG (Beckmann et al., 2019) have been introduced which act as feature extractor for dif-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "ferent speech processing tasks. Although speechVGG has got\nits inspiration from VGG (Si-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "monyan et Zisserman, 2014)– a well-known computer vision architecture– it\nis trained from"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "scratch with the LibriSpeech dataset\n(Panayotov et al., 2015). We mainly focus on how an"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "existing pre-trained computer vision model, such as VGG, can efﬁciently be ﬁne-tuned in a"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "different domain such as emotional speech recognition. This can reduce the cost of further ex-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "pensive and exhaustive training for new domains and be beneﬁcial for practical and industrial"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "use cases."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "In this work, we present an experimental study using one of the most powerful pre-trained"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "visual models VGG to tackle the aforementioned problem. Our proposed methodology is : (i)"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "to present a novel order of frequency-domain voice features that\ntransform the speech acous-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "tic signals into compound ready-to-use 3D images for existing pre-trained computer vision"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "models ;\n(ii)\nto apply simple signal-level and frequency domain voice-level data augmenta-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "tion techniques ; (iii) to introduce simple, and yet efﬁcient mini-batch padding technique ; and"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "ﬁnally, (iv) to ﬁne-tune the VGG-16 (with batch-normalization) pre-trained model on classi-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "cal image classiﬁcation tasks. We have applied the proposed conﬁgurations and some of their"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "variants on one of\nthe most well-known datasets for emotional\nrecognition (Berlin EmoDB"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "(Burkhardt et al., 2005)) and the results are very competitive to the state-of-the-art and outper-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "form many strong baselines. Our implementation is made available for public1."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "The paper is organized as follows : in section 2 we present a literature review and the related"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "works. In section 3, we introduce the proposed methodology including the considered acoustic"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "features and all the variants of the used models. Section 4 addresses all the experimental setups"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "and the results followed by brief discussions. Finally, we conclude the study and experiments"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "in section 5."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "san et Damper, 2010; Lin et Wei, 2005; Azmy et al., 2013). These models involve extensive"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "feature engineering steps that are sensitive and may signiﬁcantly affect\nthe structure of\nthe"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "entire method (Pandey et al., 2019). With the development of the deep learning models,\nthe"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "speech recognition systems beneﬁted from the end-to-end learning paradigm. This enables the"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "model to learn all the steps from the input to the ﬁnal output simultaneously including feature"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "extraction. Similarly,\nthe emotional models have followed the same course. There is a lot of"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "effort and research on employing these algorithms to recognize emotions from speech. More"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "speciﬁcally, some of these models used the ability of Convolutional Neural Networks (CNN)"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "to learn features\nfrom input signals\n(Bertero et Fung, 2017; Mekruksavanich et al., 2020)."
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "Another type of model makes use of the sequential nature of the speech signals and utilized"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "Recurrent Neural Networks (RNN) architectures like long short-term memory (LSTM) (Tzinis"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "et Potamianos, 2017; Fayek et al., 2017). Some models combined both types of architectures"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "like in ConvLSTM (Kurpukdee et al., 2017)."
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "Recently,\nthere has been a breakthrough improvement\nin the transfer learning capabilities"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "of deep models with the powerful pre-trained visual models like AlexNet, VGG, Yolo-2 ..."
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "etc (Voulodimos et al., 2018). The main idea is to train these models for large and different"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "image classiﬁcation tasks and transfer the feature selection parts of these models to be used"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "in the downstream tasks. Figure 1 shows an example of these models and the one used in our"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "experiments (VGG). This complies with the fact\nthat speech signals could be represented as"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "visual features. For instance, the Mel frequency Cepstral coefﬁcient (MFCC), Log Frequency"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "Power Coefﬁcients (LFPC), and Log Mel Spectrogram could be considered as 2D/3D images"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "that could carry emotion-related information (Wang, 2014). This will permit us to take advan-"
        },
        {
          "FIG. 1 – The original VGG-16 architecture (Simonyan et Zisserman, 2014)": "tage of the pre-trained visual models to extract visual features presented in the input acoustic"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "features without"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "sented in this paper is to some extent related to a previous work in (Zhang et al., 2017). The"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "authors use only Log Mel Spectrogram on three channels of deltas as the input features and"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "a pre-trained AlexNet (Krizhevsky et al., 2012) as the visual model. This model extracts the"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "visual feature representation of the input and then involves a linear SVM model for the target"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "classiﬁcation task."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "the Librosa library (McFee et al., 2015) for the features extraction process. Additionally, we"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "added one more feature and proposed a new method for integrating all"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "image representation of the input speech signal. The complete set of used features are :"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "1. Mel-frequency cepstral coefﬁcients (MFCCs)"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "2. Mel-scaled spectrogram."
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "3. Power spectrogram or Chromagram"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "4. Sub-bands spectral contrast"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "5. Tonal centroid features (Tonnetz)"
        },
        {
          "More speciﬁcally, we used the same spectral features reported in (Issa et al., 2020) and utilized": "6. Average of the mel-scaled spectrogram of the harmonic and percussive components"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W. Ragheb et al.": "to vectors by averaging values in the time axis, and then we have fed all\nthe permutations of"
        },
        {
          "W. Ragheb et al.": "these compact vector features to a very simple logistic regression model classiﬁer. We ﬁnally"
        },
        {
          "W. Ragheb et al.": "selected only a few candidates with the same accuracy. To make the ﬁnal decision on the orders"
        },
        {
          "W. Ragheb et al.": "of features, we ﬁne-tuned our VGG-16 model according to the shortlisted orders of features to"
        },
        {
          "W. Ragheb et al.": "get the best performance. This has given us the practical ground to ﬁnd the best permutations"
        },
        {
          "W. Ragheb et al.": "of the order of the features."
        },
        {
          "W. Ragheb et al.": "3.2\nModel Training"
        },
        {
          "W. Ragheb et al.": "For\nthe visual deep model, we used the pre-trained version of 16-layer VGG architec-"
        },
        {
          "W. Ragheb et al.": "ture (VGG16)\n(Simonyan et Zisserman, 2014) which is proven to get\nthe best performance"
        },
        {
          "W. Ragheb et al.": "in ILSVRC-20142 and became very popular in many image classiﬁcation problems. The fea-"
        },
        {
          "W. Ragheb et al.": "ture extraction layer groups are initialized from the pre-trained model weights, however,\nthe"
        },
        {
          "W. Ragheb et al.": "classiﬁcation layer group is ﬁne-tuned from scratch with random weights. Moreover, we used"
        },
        {
          "W. Ragheb et al.": "the batch normalization variants of the VGG model for its effective regularization effect espe-"
        },
        {
          "W. Ragheb et al.": "cially with training batches of smaller sizes. When creating the batches and before extracting"
        },
        {
          "W. Ragheb et al.": "the acoustic features, we applied signal-level padding to the maximum signal\nlength in each"
        },
        {
          "W. Ragheb et al.": "training batch."
        },
        {
          "W. Ragheb et al.": "Most of\nthe available emotional\nspeech datasets are small-sized. Accordingly,\nthis was"
        },
        {
          "W. Ragheb et al.": "the primary motivation for\nrelying on pre-trained models.\nIn addition to that, and as most"
        },
        {
          "W. Ragheb et al.": "deep visual model\ntraining, we applied data augmentation. The idea here is not as simple as"
        },
        {
          "W. Ragheb et al.": "regular transformation done with the images - rotation,\ntranslation, ﬂipping, cropping ... etc."
        },
        {
          "W. Ragheb et al.": "Our proposed images in the considered problem are special types of images, so standard visual"
        },
        {
          "W. Ragheb et al.": "augmentation techniques will not be useful. Hence, we applied \"CutMix\" ; (Yun et al., 2019)"
        },
        {
          "W. Ragheb et al.": "a special augmentation and regularization strategy in which patches are cut and pasted among"
        },
        {
          "W. Ragheb et al.": "training images where the ground truth labels are also mixed proportionally in the area of"
        },
        {
          "W. Ragheb et al.": "the patches. \"CutMix\" efﬁciently uses training pixels and retains the regularization effect of"
        },
        {
          "W. Ragheb et al.": "regional dropout. Figure 2 shows two examples of original speech visual features (happiness"
        },
        {
          "W. Ragheb et al.": "and anger) representation before and after \"CutMix\"."
        },
        {
          "W. Ragheb et al.": "As a regular classiﬁcation deep learning model, we used cross entropy loss function with"
        },
        {
          "W. Ragheb et al.": "Adam optimizer (Kingma et Ba, 2014). We employed a learning rate of 10−5 and a batch size"
        },
        {
          "W. Ragheb et al.": "of 16."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": "1"
        },
        {
          "(a) Happiness (Feaures)": "Time"
        },
        {
          "(a) Happiness (Feaures)": "(b) Anger (Feaures)"
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": "1"
        },
        {
          "(a) Happiness (Feaures)": "Time"
        },
        {
          "(a) Happiness (Feaures)": "(c) CutMixed (Happiness + Anger)"
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": ""
        },
        {
          "(a) Happiness (Feaures)": "1"
        },
        {
          "(a) Happiness (Feaures)": "Time"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classes": "Fear",
          "Train": "55",
          "Test": "14"
        },
        {
          "Classes": "Sadness",
          "Train": "50",
          "Test": "12"
        },
        {
          "Classes": "Disgust",
          "Train": "37",
          "Test": "9"
        },
        {
          "Classes": "Anger",
          "Train": "102",
          "Test": "25"
        },
        {
          "Classes": "Boredom",
          "Train": "65",
          "Test": "16"
        },
        {
          "Classes": "Neutral",
          "Train": "64",
          "Test": "15"
        },
        {
          "Classes": "Happiness",
          "Train": "56",
          "Test": "15"
        },
        {
          "Classes": "Total",
          "Train": "429",
          "Test": "106"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FIG. 3 – Example of a same speech uttered in different emotional states by a same speaker": "4.2\nResults"
        },
        {
          "FIG. 3 – Example of a same speech uttered in different emotional states by a same speaker": "We tested the overall methodology described in section 3 to measure the classiﬁcation"
        },
        {
          "FIG. 3 – Example of a same speech uttered in different emotional states by a same speaker": "performance on the test dataset. We use the accuracy metrics to enable the head-to-head com-"
        },
        {
          "FIG. 3 – Example of a same speech uttered in different emotional states by a same speaker": "parison with the SOTA models and other\nreported baselines results. Moreover, we tested 6"
        },
        {
          "FIG. 3 – Example of a same speech uttered in different emotional states by a same speaker": "possible variants of the model\nto enhance the ablation analysis of the proposed methodology"
        },
        {
          "FIG. 3 – Example of a same speech uttered in different emotional states by a same speaker": "components. The proposed variants and their deﬁnition are described as :"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "possible variants of the model": "components. The proposed variants and their deﬁnition are described as :",
          "to enhance the ablation analysis of the proposed methodology": ""
        },
        {
          "possible variants of the model": "1. Model-A : The complete model as described in section 3",
          "to enhance the ablation analysis of the proposed methodology": ""
        },
        {
          "possible variants of the model": "2. Model-B : The model without batch normalization",
          "to enhance the ablation analysis of the proposed methodology": ""
        },
        {
          "possible variants of the model": "3. Model-C : Excluding CutMix augmentation strategy",
          "to enhance the ablation analysis of the proposed methodology": ""
        },
        {
          "possible variants of the model": "4. Model-D : Excluding signal-level augmentation",
          "to enhance the ablation analysis of the proposed methodology": ""
        },
        {
          "possible variants of the model": "5. Model-E : Excluding both signal-level and CutMix augmentations",
          "to enhance the ablation analysis of the proposed methodology": ""
        },
        {
          "possible variants of the model": "6. Model-F : Applying the model excluding mini-batch padding",
          "to enhance the ablation analysis of the proposed methodology": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: shows our model outperformed the previous state-of-the-art results and many strong",
      "data": [
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model-A"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model-B"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model-C"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model-D"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model-E"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Model-F"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: shows our model outperformed the previous state-of-the-art results and many strong",
      "data": [
        {
          "Model-C\n83.02": "Model-D\n81.13"
        },
        {
          "Model-C\n83.02": "Model-E\n76.42"
        },
        {
          "Model-C\n83.02": "Model-F\n69.81"
        },
        {
          "Model-C\n83.02": "TAB. 2 – Classiﬁcation accuracy of the proposed model variants"
        },
        {
          "Model-C\n83.02": "as Model-F indicates around -18 percent drop in the accuracy by excluding it. Excluding both"
        },
        {
          "Model-C\n83.02": "signal-level and CutMix augmentations (Model-E) can reduce the performance of our model"
        },
        {
          "Model-C\n83.02": "by around 11 percent. This indicates the role of two-sided augmentations of the model"
        },
        {
          "Model-C\n83.02": "signal and image levels. Analysis of Model-C and Model-D show that stand-alone signal-level"
        },
        {
          "Model-C\n83.02": "augmentation is just a slightly better component than stand-alone CutMix augmentation as the"
        },
        {
          "Model-C\n83.02": "signal-level augmentation is only 2 percent ahead of CutMix augmentation in terms of accu-"
        },
        {
          "Model-C\n83.02": "racy measure. As discussed earlier, both augmentations can consistently strengthen each other."
        },
        {
          "Model-C\n83.02": "Model-B shows the importance of using batch-normalization to make the ﬁnal model better"
        },
        {
          "Model-C\n83.02": "to some extent\n(batch-normalization was later"
        },
        {
          "Model-C\n83.02": "Table 3 shows our model outperformed the previous state-of-the-art results and many strong"
        },
        {
          "Model-C\n83.02": "base-lines. Figure 4 shows the detailed confusion matrix for our best model result. It is worth"
        },
        {
          "Model-C\n83.02": "mentioning that our analysis shows the importance of applying different components altogether"
        },
        {
          "Model-C\n83.02": "(and not just only ﬁne-tuning VGG-16) to outperform the state-of-the-art results."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: shows our model outperformed the previous state-of-the-art results and many strong",
      "data": [
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "(and not just only ﬁne-tuning VGG-16) to outperform the state-of-the-art results."
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Model"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Badshah et. al. (Badshah et al., 2017)"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Wang et. al. (Wang et al., 2015)"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Lampropoulos et. at. (Lampropoulos et Tsihrintzis, 2012)"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Huangb et. al. (Huang et al., 2014)"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Wu et. al. (Wu et al., 2011)"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Issa et. al. (Issa et al., 2020) ∗"
        },
        {
          "mentioning that our analysis shows the importance of applying different components altogether": "Our best model (Model-A)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FIG. 4 – Confusion matrix of Model-A": "recognition. This has been achieved by applying a novel order of frequency-domain voice fea-"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "tures represented as ready-to-use 3D images ; signal-level and frequency-domain voice-level"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "data augmentation techniques ; and ﬁnally simple, and yet efﬁcient, mini-batch padding tech-"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "nique. We have outperformed the previous state-of-the-art results and many strong baselines."
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "The work presented in this paper could be extended to include more pre-trained com-"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "puter vision deep models such as ResNet\n(He et al., 2016), EfﬁcientNet(Tan et Le, 2019),"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "ViT(Dosovitskiy et al., 2020) and Inceptionv3 (GoogLeNet) (Szegedy et al., 2016). Besides,"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "extensive experiments can be performed on other emotional datasets like LSSED (Fan et al.,"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "2021),\nIEMOCAP (Busso et al., 2008), and RAVDESS (Livingstone et Russo, 2018)\n. Mo-"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "reover,\nit could be interesting to include other modalities for emotional recognition like text,"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "images and videos."
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "Références"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "Alu, D., E. Zoltan, et I. C. Stoica (2017). Voice based emotion recognition with convolutional"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "neural networks for companion robots. Science and Technology 20(3), 222–240."
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "Azmy, W. M., S. Abdou, et M. Shoman (2013). Arabic unit selection emotional speech syn-"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "thesis using blending data approach. International Journal of Computer Applications 81(8)."
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "Badshah, A. M., J. Ahmad, N. Rahim, et S. W. Baik (2017). Speech emotion recognition from"
        },
        {
          "FIG. 4 – Confusion matrix of Model-A": "spectrograms with deep convolutional neural network. In 2017 International Conference on"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Platform Technology and Service (PlatCon), pp. 1–5."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Bandela, S. R. et T. K. Kumar (2017). Stressed speech emotion recognition using feature fusion"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "of teager energy operator and mfcc.\nIn 2017 8th International Conference on Computing,"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Communication and Networking Technologies (ICCCNT), pp. 1–5. IEEE."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Beckmann, P., M. Kegler, H. Saltini, et M. Cernak (2019). Speech-vgg : A deep feature ex-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "tractor for speech processing. arXiv preprint arXiv :1910.09909."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Bertero, D. et P. Fung (2017). A ﬁrst\nlook into a convolutional neural network for speech"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "emotion detection.\nIn 2017 IEEE international conference on acoustics, speech and signal"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "processing (ICASSP), pp. 5115–5119. IEEE."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Burkhardt, F., A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss, et al. (2005). A database"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "of german emotional speech.\nIn Interspeech, Volume 5, pp. 1517–1520."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Busso, C., M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, et"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "S. S. Narayanan (2008).\nIemocap : Interactive emotional dyadic motion capture database."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Language resources and evaluation 42(4), 335–359."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Dosovitskiy, A., L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Deh-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "ghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020). An image is worth 16x16 words :"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Transformers for image recognition at scale. arXiv preprint arXiv :2010.11929."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Fan, W., X. Xu, X. Xing, W. Chen, et D. Huang (2021). Lssed : a large-scale dataset and bench-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "mark for speech emotion recognition. In ICASSP 2021-2021 IEEE International Conference"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "on Acoustics, Speech and Signal Processing (ICASSP), pp. 641–645. IEEE."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Fayek, H. M., M. Lech, et L. Cavedon (2017).\nEvaluating deep learning architectures for"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "speech emotion recognition. Neural Networks 92, 60–68."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Hassan, A. et R. Damper (2010). Multi-class and hierarchical svms for emotion recognition."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "pp. 2354–2357."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "He, K., X. Zhang, S. Ren, et J. Sun (2016). Deep residual\nlearning for image recognition.\nIn"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pp. 770–"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "778."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Huang, Z., M. Dong, Q. Mao, et Y. Zhan (2014). Speech emotion recognition using cnn. MM"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "’14, New York, NY, USA. Association for Computing Machinery."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Issa, D., M. F. Demirci, et A. Yazici (2020). Speech emotion recognition with deep convolu-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "tional neural networks. Biomedical Signal Processing and Control 59, 101894."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Kingma, D. P. et J. Ba (2014). Adam : A method for stochastic optimization. arXiv preprint"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "arXiv :1412.6980."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Krizhevsky, A., I. Sutskever, et G. E. Hinton (2012). Imagenet classiﬁcation with deep convolu-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "tional neural networks. Advances in neural information processing systems 25, 1097–1105."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Kurpukdee, N., T. Koriyama, T. Kobayashi, S. Kasuriya, C. Wutiwiwatchai, et P. Lamsrichan"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "(2017).\nSpeech emotion recognition using convolutional\nlong short-term memory neural"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "network and support vector machines.\nIn 2017 Asia-Paciﬁc Signal and Information Proces-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "sing Association Annual Summit and Conference (APSIPA ASC), pp. 1744–1749. IEEE."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Lampropoulos, A. S. et G. A. Tsihrintzis (2012). Evaluation of mpeg-7 descriptors for speech"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "emotional recognition.\nIn 2012 Eighth International Conference on Intelligent Information"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W. Ragheb et al.": "Hiding and Multimedia Signal Processing, pp. 98–101."
        },
        {
          "W. Ragheb et al.": "Lin, Y.-L. et G. Wei (2005).\nSpeech emotion recognition based on hmm and svm.\nIn 2005"
        },
        {
          "W. Ragheb et al.": "international conference on machine learning and cybernetics, Volume 8, pp. 4898–4901."
        },
        {
          "W. Ragheb et al.": "IEEE."
        },
        {
          "W. Ragheb et al.": "Livingstone, S. R. et F. A. Russo (2018).\nThe ryerson audio-visual database of emotional"
        },
        {
          "W. Ragheb et al.": "speech and song (ravdess) : A dynamic, multimodal set of facial and vocal expressions in"
        },
        {
          "W. Ragheb et al.": "north american english. PloS one 13(5), e0196391."
        },
        {
          "W. Ragheb et al.": "McFee, B., C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, et O. Nieto (2015)."
        },
        {
          "W. Ragheb et al.": "the 14th python in\nlibrosa : Audio and music signal analysis in python.\nIn Proceedings of"
        },
        {
          "W. Ragheb et al.": "science conference, Volume 8, pp. 18–25. Citeseer."
        },
        {
          "W. Ragheb et al.": "Mekruksavanich, S., A. Jitpattanakul, et N. Hnoohom (2020). Negative emotion recognition"
        },
        {
          "W. Ragheb et al.": "using deep learning for thai\nlanguage.\nIn 2020 Joint International Conference on Digital"
        },
        {
          "W. Ragheb et al.": "Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electro-"
        },
        {
          "W. Ragheb et al.": "nics, Computer and Telecommunications Engineering (ECTI DAMT & NCON), pp. 71–74."
        },
        {
          "W. Ragheb et al.": "IEEE."
        },
        {
          "W. Ragheb et al.": "Narayanan, S. et P. G. Georgiou (2013). Behavioral signal processing : Deriving human beha-"
        },
        {
          "W. Ragheb et al.": "vioral informatics from speech and language. Proceedings of the IEEE 101(5), 1203–1233."
        },
        {
          "W. Ragheb et al.": "Nassif, A. B., I. Shahin, I. Attili, M. Azzeh, et K. Shaalan (2019). Speech recognition using"
        },
        {
          "W. Ragheb et al.": "deep neural networks : A systematic review.\nIEEE access 7, 19143–19165."
        },
        {
          "W. Ragheb et al.": "Panayotov, V., G. Chen, D. Povey, et S. Khudanpur (2015). Librispeech : an asr corpus based"
        },
        {
          "W. Ragheb et al.": "on public domain audio books.\nIn 2015 IEEE international conference on acoustics, speech"
        },
        {
          "W. Ragheb et al.": "and signal processing (ICASSP), pp. 5206–5210. IEEE."
        },
        {
          "W. Ragheb et al.": "Pandey, S. K., H. Shekhawat, et S. Prasanna (2019).\nDeep learning techniques for speech"
        },
        {
          "W. Ragheb et al.": "emotion recognition : A review.\nIn 2019 29th International Conference Radioelektronika"
        },
        {
          "W. Ragheb et al.": "(RADIOELEKTRONIKA), pp. 1–6. IEEE."
        },
        {
          "W. Ragheb et al.": "Proksch, S.-O., C. Wratil, et J. Wäckerle (2019).\nTesting the validity of automatic speech"
        },
        {
          "W. Ragheb et al.": "recognition for political text analysis. Political Analysis 27(3), 339–359."
        },
        {
          "W. Ragheb et al.": "Rouhi, A., M. Spitale, F. Catania, G. Cosentino, M. Gelsomini, et F. Garzotto (2019). Emotify :"
        },
        {
          "W. Ragheb et al.": "emotional game for children with autism spectrum disorder based-on machine learning.\nIn"
        },
        {
          "W. Ragheb et al.": "Proceedings of\nthe 24th International Conference on Intelligent User Interfaces : Compa-"
        },
        {
          "W. Ragheb et al.": "nion, pp. 31–32."
        },
        {
          "W. Ragheb et al.": "Simonyan, K. et A. Zisserman (2014). Very deep convolutional networks for large-scale image"
        },
        {
          "W. Ragheb et al.": "recognition. arXiv preprint arXiv :1409.1556."
        },
        {
          "W. Ragheb et al.": "Szegedy, C., V. Vanhoucke, S. Ioffe, J. Shlens, et Z. Wojna (2016). Rethinking the inception"
        },
        {
          "W. Ragheb et al.": "architecture for computer vision. In Proceedings of the IEEE conference on computer vision"
        },
        {
          "W. Ragheb et al.": "and pattern recognition, pp. 2818–2826."
        },
        {
          "W. Ragheb et al.": "Tan, M. et Q. Le (2019).\nEfﬁcientnet\n: Rethinking model scaling for convolutional neural"
        },
        {
          "W. Ragheb et al.": "networks.\nIn International Conference on Machine Learning, pp. 6105–6114. PMLR."
        },
        {
          "W. Ragheb et al.": "Tzinis, E. et A. Potamianos (2017). Segment-based speech emotion recognition using recurrent"
        },
        {
          "W. Ragheb et al.": "neural networks.\nIn 2017 Seventh International Conference on Affective Computing and"
        },
        {
          "W. Ragheb et al.": "Intelligent Interaction (ACII), pp. 190–195. IEEE."
        },
        {
          "W. Ragheb et al.": "Voulodimos, A., N. Doulamis, A. Doulamis, et E. Protopapadakis (2018). Deep learning for"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "computer vision : A brief review.\nIn Computational Intelligence and Neuroscience, Volume"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "2018, pp. 1–13."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Wang, K., N. An, B. N. Li, Y. Zhang, et L. Li (2015). Speech emotion recognition using fourier"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "parameters.\nIEEE Transactions on affective computing 6(1), 69–75."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Wang, K.-C. (2014). The feature extraction based on texture image information for emotion"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "sensing in speech. Sensors 14(9), 16692–16714."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Wu, S., T. H. Falk, et W.-Y. Chan (2011). Automatic speech emotion recognition using modu-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "lation spectral features. Speech Communication 53(5), 768–785. Perceptual and Statistical"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Audition."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Yun, S., D. Han, S. J. Oh, S. Chun, J. Choe, et Y. Yoo (2019). Cutmix : Regularization stra-"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "the IEEE/CVF\ntegy to train strong classiﬁers with localizable features.\nIn Proceedings of"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "International Conference on Computer Vision, pp. 6023–6032."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "Zhang, S., S. Zhang, T. Huang, et W. Gao (2017).\nSpeech emotion recognition using deep"
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "IEEE Transac-\nconvolutional neural network and discriminant temporal pyramid matching."
        },
        {
          "Emotional Speech Recognition with Pre-trained Deep Visual Models": "tions on Multimedia 20(6), 1576–1590."
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Platform Technology and Service",
      "venue": "Platform Technology and Service"
    },
    {
      "citation_id": "2",
      "title": "Stressed speech emotion recognition using feature fusion of teager energy operator and mfcc",
      "authors": [
        "S Bandela",
        "T Kumar"
      ],
      "year": "2017",
      "venue": "2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)"
    },
    {
      "citation_id": "3",
      "title": "Speech-vgg : A deep feature extractor for speech processing",
      "authors": [
        "P Beckmann",
        "M Kegler",
        "H Saltini",
        "M Cernak"
      ],
      "year": "2019",
      "venue": "Speech-vgg : A deep feature extractor for speech processing",
      "arxiv": "arXiv:1910.09909"
    },
    {
      "citation_id": "4",
      "title": "A first look into a convolutional neural network for speech emotion detection",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Iemocap : Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "An image is worth 16x16 words : Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words : Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "8",
      "title": "Lssed : a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "Multi-class and hierarchical svms for emotion recognition",
      "authors": [
        "A Hassan",
        "R Damper"
      ],
      "year": "2010",
      "venue": "Multi-class and hierarchical svms for emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using cnn. MM '14",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using cnn. MM '14"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "14",
      "title": "Adam : A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam : A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "15",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using convolutional long short-term memory neural network and support vector machines",
      "authors": [
        "N Kurpukdee",
        "T Koriyama",
        "T Kobayashi",
        "S Kasuriya",
        "C Wutiwiwatchai",
        "P Lamsrichan"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "17",
      "title": "Evaluation of mpeg-7 descriptors for speech emotional recognition",
      "authors": [
        "A Lampropoulos",
        "G Tsihrintzis"
      ],
      "year": "2012",
      "venue": "2012 Eighth International Conference on Intelligent Information W. Ragheb et al. Hiding and Multimedia Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition based on hmm and svm",
      "authors": [
        "Y.-L Lin",
        "G Wei"
      ],
      "year": "2005",
      "venue": "2005 international conference on machine learning and cybernetics"
    },
    {
      "citation_id": "19",
      "title": "The ryerson audio-visual database of emotional speech song (ravdess) : A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "20",
      "title": "librosa : Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "21",
      "title": "Negative emotion recognition using deep learning for thai language",
      "authors": [
        "S Mekruksavanich",
        "A Jitpattanakul",
        "N Hnoohom"
      ],
      "year": "2020",
      "venue": "2020 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics"
    },
    {
      "citation_id": "22",
      "title": "Behavioral signal processing : Deriving human behavioral informatics from speech and language",
      "authors": [
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "23",
      "title": "Speech recognition using deep neural networks : A systematic review",
      "authors": [
        "A Nassif",
        "I Shahin",
        "I Attili",
        "M Azzeh",
        "K Shaalan"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "24",
      "title": "Librispeech : an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Deep learning techniques for speech emotion recognition : A review",
      "authors": [
        "S Pandey",
        "H Shekhawat",
        "S Prasanna"
      ],
      "year": "2019",
      "venue": "2019 29th International Conference Radioelektronika (RADIOELEKTRONIKA)"
    },
    {
      "citation_id": "26",
      "title": "Testing the validity of automatic speech recognition for political text analysis",
      "authors": [
        "S.-O Proksch",
        "C Wratil",
        "J Wäckerle"
      ],
      "year": "2019",
      "venue": "Political Analysis"
    },
    {
      "citation_id": "27",
      "title": "Emotify : emotional game for children with autism spectrum disorder based-on machine learning",
      "authors": [
        "A Rouhi",
        "M Spitale",
        "F Catania",
        "G Cosentino",
        "M Gelsomini",
        "F Garzotto"
      ],
      "year": "2019",
      "venue": "Proceedings of the 24th International Conference on Intelligent User Interfaces : Companion"
    },
    {
      "citation_id": "28",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "29",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Efficientnet : Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "Segment-based speech emotion recognition using recurrent neural networks",
      "authors": [
        "E Tzinis",
        "A Potamianos"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "32",
      "title": "Deep learning for Emotional Speech Recognition with Pre-trained Deep Visual Models computer vision : A brief review",
      "authors": [
        "A Voulodimos",
        "N Doulamis",
        "A Doulamis",
        "E Protopapadakis"
      ],
      "year": "2018",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "34",
      "title": "The feature extraction based on texture image information for emotion sensing in speech",
      "authors": [
        "K.-C Wang"
      ],
      "year": "2014",
      "venue": "Sensors"
    },
    {
      "citation_id": "35",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "36",
      "title": "Cutmix : Regularization strategy to train strong classifiers with localizable features",
      "authors": [
        "S Yun",
        "D Han",
        "S Oh",
        "S Chun",
        "J Choe",
        "Y Yoo"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "38",
      "title": "Summary Donner la traduction anglaise du résumé dans le préambule avec la commande \\summary{Your abstract",
      "venue": "Summary Donner la traduction anglaise du résumé dans le préambule avec la commande \\summary{Your abstract"
    }
  ]
}