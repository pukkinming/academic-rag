{
  "paper_id": "2509.15654v2",
  "title": "Emo-Rl: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model For Generalized Speech Emotion Recognition",
  "published": "2025-09-19T06:27:33Z",
  "authors": [
    "Pengcheng Li",
    "Botao Zhao",
    "Zuheng Kang",
    "Junqing Peng",
    "Xiaoyang Qu",
    "Yayun He",
    "Jianzong Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although Large Audio-Language Models (LALMs) have exhibited outstanding performance in auditory understanding, their performance in affective computing scenarios, particularly in emotion recognition, reasoning, and subtle sentiment differentiation, remains suboptimal. Recent advances in Reinforcement Learning (RL) have shown promise in improving LALMs' reasoning abilities. However, two critical challenges hinder the direct application of RL techniques to Speech Emotion Recognition (SER) tasks: (1) convergence instability caused by ambiguous emotional boundaries and (2) limited reasoning ability when using relatively small models (e.g., 7B-parameter architectures). To overcome these limitations, we introduce EMO-RL, a novel framework incorporating reinforcement learning with two key innovations: Emotion Similarity-Weighted Reward (ESWR) and Explicit Structured Reasoning (ESR). Built upon pretrained LALMs, our method employs group-relative policy optimization with emotion constraints. Comprehensive experiments demonstrate that our EMO-RL training strategies can significantly enhance the emotional reasoning capabilities of LALMs, attaining state-of-the-art results on both the MELD and IEMOCAP datasets, and cross-dataset experiments prove the strong superiority of generalization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a significant research direction in the field of affective computing, aiming to map speech signals to corresponding emotional labels through computational analysis. It plays an important role in various applications, including intelligent customer service  (Li and Lin, 2021) , mental health assessment  (Madanian et al., 2022) , and human-computer interaction  (Alsabhan, 2023) . † These authors contributed equally to this work. * Corresponding author: jzwang@188.com.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Standard Rl For Lalm",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emo-Rl For Lalm (Ours)",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "S Ta N D A R D",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Lalm Audio",
      "text": "What is the emotion in the audio?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Soft Decision By",
      "text": "much easier to learn.\n\ndense reward\n\nFigure  1 : The key ideas of our proposed Emo-RL. Compared with the standard RL, Emo-RL exploited emotion similarity-weighted reward and the explicit structured reasoning to improve the emotion recognition performance of LALM.\n\nIn SER task, prior studies predominantly rely on pre-trained speech models or perform fine-tuning on affective corpora to derive emotional representations, then train a classification head to implement emotion classification  (Li et al., 2023b; Chen et al., 2023) . However, the emotional representations extracted by these models can only capture the acoustic expressions of emotions, but lack a collaborative analysis of text semantics. These models have very limited generalization capability and lack explainability.\n\nWith the development of multi-modal large models, many powerful Large Audio-Language Models (LALMs) have emerged  (Kong et al., 2024) , among which Qwen2-Audio  (Chu et al., 2024)  is a representative example. It can follow user instructions to perform many downstream tasks, such as speech recognition, transcription, sound classification, and more. Although Qwen2-Audio demonstrates strong speech understanding capabilities, its performance on SER tasks remains limited. This limitation stems from its tendency to rely on shallow associations rather than multi-step reasoning that integrates textual semantics and auditory features across modalities, resulting in its limited accuracy and generalization on SER. Speech emotion recognition inherently constitutes a cognitive reasoning process that necessitates comprehensive analysis from multiple perspectives through stepby-step reasoning. When humans recognize emotions in speech, they often understand the specific content and keywords of speech and integrate this understanding based on acoustic features (such as pitch, voice quality, speech rate). For example, when someone says \"fed up\" with rapid speech, high volume, and sharp intonation, anger can be inferred.\n\nThese reasoning steps are beyond the capability of traditional audio feature extraction and classification head frameworks. Extensive research has shown that reinforcement learning can enhance the reasoning capabilities of LLM  (Guo et al., 2025; Team et al., 2025) . The effective deployment of reinforcement learning (RL) in SER encounters two fundamental limitations: convergence reliability issues primarily arising from ill-defined inter-class affective boundaries that induce gradient conflict during policy updates, compounded by insufficient affective reasoning capacity in under-parameterized architectures (e.g., 7B-parameter configurations).\n\nTo address these challenges, we adopt a psychological perspective by transforming the original right/wrong classification problem into a regression problem that accommodates varying degrees of correctness and error, through the introduction of an emotion-state-transition matrix (As illustrated in Figure  1 ). We implement an Emotion Similarity-Weighted Reward (ESWR) mechanism that progressively guides the policy model from simpler to more complex tasks. This approach initially teaches the model to distinguish between basic positive and negative emotions before advancing to finer-grained emotional distinctions. To further enhance the model's emotional reasoning capabilities, we incorporate Explicit Structured Reasoning (ESR) strategies during RL training. These strategies provide the model with guiding clues to help it more effectively differentiate between emotions, thereby improving its overall reasoning ability in SER tasks.\n\nBased on the ESWR and ESR, we exploited our emotion-rule-based RL method to fine-tune the LALM, and the contributions of this paper are summarized as follows:\n\n• We propose a SER pipeline via RL fine-tuning of a large audio and language model.\n\n• We introduce Emotion-rule based RL to improve the emotion recognition ability of LALM, leveraging emotion similarity-weighted rewards and explicit structured reasoning strategies.\n\n• Extensive experiments demonstrate that the proposed approach exhibits strong generalizability and achieves state-of-the-art performance.\n\n2 Related Works",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Generalized Speech Emotion Recognition",
      "text": "For SER, traditional approaches have focused on designing novel network architectures  (Zou et al., 2022; Li et al., 2023b)  based on classical neural networks. With the advancement of self-supervised learning, researchers have increasingly utilized pre-trained audio models like WavLM  (Chen et al., 2022) , Emotion2vec  (Ma et al., 2024b) , HuBERT  (Hsu et al., 2021) , and Whisper  (Radford et al., 2023)  to extract speech features or fine-tune these models on speech emotion datasets to obtain emotion-specific features  (Morais et al., 2022; Chen and Rudnicky, 2023) . Subsequently, a linear classification head is trained to perform emotion classification. These models have significantly enhanced speech emotion perception capabilities  (Li et al., 2023a) . For instance, the Vesper model  (Chen et al., 2024) , obtained by distilling the WavLM-large model with emotion data, has achieved promising results in SER tasks. However, the generalization capabilities of these models remain limited, and they lack collaborative analysis of text semantics and explainability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Large Audio-Language Models",
      "text": "Recent progress in multimodal large-scale language modeling has led to the emergence of numerous LALMs, such as Audio Flamingo  (Kong et al., 2024) , Qwen2-Audio  (Chu et al., 2024) ,and SALMONN  (Tang et al., 2023) , which have demonstrated strong audio understanding capabilities, with Qwen2-Audio even outperforming previous methods across the vast majority of audio-focused evaluation benchmark. These models typically comprise three main components: an Audio Encoder, a Large Language Model, and a modality connector that bridges them. These models are capable of directly processing cross-modal inputs, including audio (such as speech, environmental sounds, and music) and text prompts, and can generate the corresponding textual output. They are able to follow user instructions to perform a variety of downstream tasks, such as transcription, SER, and sound classification  (Wang et al., 2025; Waheed et al., 2024) . However, current LALM training mainly focuses on perception and basic QA tasks, lacking explicit multi-step reasoning. Thus, the potential of LALMs like Qwen2-Audio in complex audio reasoning tasks such as SER remains untapped. Enhancing their reasoning abilities in these advanced tasks is crucial.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Reinforcement Learning And Reasoning",
      "text": "Reinforcement learning (RL) plays a crucial role in advancing the reasoning abilities of LLMs and MLLMs. RLHF employs proximal policy optimization (PPO)  (Schulman et al., 2017)  alongside a trained reward mechanism to align LLMs with human preferences. Direct Preference Optimization (DPO)  (Rafailov et al., 2023)  bypasses reward modeling by learning from preference data directly, whereas Rejection Sampling Fine-tuning (RFT)  (Yuan et al., 2023)  strengthens reasoning through curated self-produced reasoning chains. Group Relative Policy Optimization (GRPO)  (Shao et al., 2024)  refines PPO by eliminating the critic component and utilizing group-level baseline averaging for advantage computation, achieving enhanced LLM reasoning with reduced computational overhead. The Hybrid GRPO variant  (Sane, 2025)  integrates GRPO's sampling mechanism with a trained value estimator, improving training stability and data utilization efficiency. Contemporary research demonstrates that Chain-of-Thought (COT) combined with RL substantially elevates LALM reasoning capabilities. Audio-CoT  (Ma et al., 2025)  pioneered COT integration in LALMs, though gains were modest without model parameter optimization. Audio-Reasoner  (Xie et al., 2025)  developed CoTA, an extensive synthetic corpus containing millions of question-answer instances with detailed reasoning trajectories, markedly advancing extended-context reasoning abilities. Xiaomi's implementation utilized GRPO optimization on the Qwen2-Audio-7B architecture for audio question-answering applications  (Li et al., 2025) , achieving notable improvements in reasoning precision. SARI  (Wen et al., 2025)  additionally combines systematic reasoning frameworks with pro-gressive reinforcement training curricula, establishing new benchmarks on MMAU and MMSU evaluations. Reward-based optimization frameworks have proven effective in boosting reasoning precision, demonstrating that reinforcement-driven strategies can maximize learning efficiency from constrained training datasets. Nevertheless, existing RL methodologies remain overly broad for specialized speech emotion applications. Consequently, developing emotion-rule-guided reinforcement strategies specifically tailored for SER tasks becomes essential.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "We use emotional audio question answering in Qwen2-Audio-7B-Instruct to implement SER. SER process through LALMs constitutes a parametric mapping process where: given a speech signal x and structured textual query Q containing multiplechoice options, their temporal-contextual concatenation forms the input prompt p = [x; Q]. The LALM, π θ , then generates emotion prediction ŷ through cross-modal understanding, formally expressed as:\n\nwhere S is a speech audio with a sampling rate of 16kHz, Q is the textual question prompt, and a is the generated response of LALM, including thinking and reasoning contents and the final selected answer, and ŷ denotes the predicted emotion label. This study aim to address two core challenges in SER through LALMs: (1) Enhancing the predictive accuracy of f θ via reinforcement learning using the training dataset D = {(x i , y i )} N i , where the N means the sample number. (2) Discovering optimal prompt formulations Q to improve the inference performance. Considering the parameter space of is infinite, we defined three experimentally validated designs as the Q space, Q, including implicit reasoning Q IR , explicit unstructured reasoning Q EUR , explicit structured reasoning Q ESR . Therefore, the target of this study could be defined as:\n\nwhere the R denotes the reward function.\n\nIn detail, we explore three reasoning strategies in EMO-RL training to evaluate the impact of reasoning patterns. We detail three patterns below: The overview of the Emo-RL for LALM to improve the generalized speech emotion recognition. Building on GRPO, we enhance emotion recognition via two improvements. First, we create an emotion-state-transition matrix from Plutchik's wheel of emotions  (Plutchik, 1982) , allowing the policy model to receive rewards for predicting similar emotions. Second, we introduce explicit structured reasoning to directly input human emotion recognition priors into the model. Through synthesis of these dual information streams, the system derives its conclusive assessment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion-Rule Based Rl Framework",
      "text": "We built our Emo-RL based on the GRPO framework for its efficiency and scalability. Unlike proximal policy optimization, which requires a computationally expensive value network, GRPO calculates relative advantages by comparing rewards within a group of sampled actions, reducing computational overhead and simplifying optimization. This makes GRPO particularly suitable for speech reasoning tasks. Similar to GRPO, the Emo-RL also has three main steps, sampling action groups, reward evaluation, and updating policy network with relative advantage and KL divergence (As shown in Figure  2 ).\n\nSampling Action Groups For each input state s = (x, Q) , where x is the speech encoding of the input audio and Q the textual encoding of the question, GRPO samples a group of actions (the generated response of LALM), {a 1 , a 2 , . . . , a G }, from the current policy π θ . The sampling process is:\n\nThis strategy ensures diverse responses, promoting exploration and preventing premature convergence.\n\nReward Evaluation. In our reinforcement learning framework, each sampled action a i is assigned a reward R(a i ) based on verifiable criteria, resulting in a reward set {r 1 , r 2 , . . . , r G }. For emotional speech reasoning tasks, the reward function R(a i ) combines two components: the reasoning format reward R format (a i ) and emotion accuracy reward R acc (a i ). The format reward ensures that the responses adhere to a structured format, thereby guiding the reasoning strategy of the policy network, π θ . The accuracy reward evaluates the correctness of the action a i , providing feedback to π θ on the extent to which the answer aligns with the correct response. The overall reward function is:\n\nUpdating Policy Network with Relative Advantage and KL divergence. The π θ is optimized by the Relative Advantage of rewards and KL divergence between π θ and reference model π ref . Firstly, policy Rewards are normalized within the sampled group to compute relative advantages {A 1 , A 2 , . . . , A G }, defined as:\n\nBased on these advantages, the policy is updated to reinforce actions with positive advantages and reduce the probability of less effective ones. To ensure stable RL learning, π θ updates are further constrained by minimizing the KL divergence between the updated and reference models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Rewards Mechanism Design",
      "text": "The EMO-RL framework implements dual reward mechanisms synergistically combining structural compliance enforcement and affective alignment optimization. Specifically, domainspecific response schemata are enforced through regular-expression pattern matching that validates three distinct reasoning pattern compliance rates (Q IR , Q EUR , Q ESR ), systematically enhancing explainability via cognitive transparency in decision pathways. Complementarily, the emotion similarity-weighted reward employs an emotion-state-transition matrix constructed through Plutchik's wheel of emotions  (Plutchik, 1982) , generating dense reward signals that precisely guide policy gradients through convex optimization landscapes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Reasoning Format Reward",
      "text": "This component ensures adherence to specific response formats across different reasoning strategies by implementing tailored format reward. We define three distinct reasoning format functions, including Implicit Reasoning (IR), Explicit Unstructured Reasoning (EUR), and Explicit Structured Reasoning (ESR), each requiring different format constraints.\n\nFor IR, which targets only answer generation, the reward is granted only when the final answer is both correct and correctly delimited by <answer> tags, as shown in Figure  2 . For EUR, which require explicit reasoning display, the format reward is granted when the response contains reasoning within <think> tags and the final answer within <answer> tags. ESR is similar to EUR, but with four additional format constraint tags.\n\nThe format reward function of ESR is as follows, and all format reward follow this rule. Each format reward function employs binary scoring based on regex pattern matching, where strict adherence to the specified format yields a reward score of 1, while any deviation results in a score of 0. This ensures consistent formatting across different reasoning strategies while maintaining the flexibility to accommodate strategy-specific requirements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Accuracy Reward",
      "text": "The conventional approach uses binary classification rewards (BCR), allocating a score of 1 to fully accurate responses and 0 to all others. However, this has limitations when applied to emotions, as it ignores the relationships between different emotion types. Emotions are inherently continuous and complex. Drawing from psychological emotion dimension theories  (Plutchik, 1982)  and other psychological knowledge, we comprehensively consider emotion valence (the positive or negative of emotions) and arousal (the intensity or activation level of emotions). We have meticu-lously designed an emotion-state-transition matrix S ∈ R C×C (C denote emotion categories) as:\n\nif yi or yi='neutral' 1 2 (cos (Pl (yi, yj)) + 1) , otherwise (7) Here, the y i and y j denotes two emotion types, and Pl(•, •) means the angles from each other on the Plutchik's wheel of emotions. Based on the S, we have the Emotion Similarity-Weighted Reward function, formulated as:\n\nwhere α is the partial matching coefficient, dynamically adjusting from 1 to 0 during training, and γ is the threshold of the contradictory emotion, which was set as 0.7 in this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "We evaluated model capabilities and generalization in speech emotion recognition (SER) using four datasets: MELD  (Poria et al., 2019)  (13 708 utterances from Friends, 7 emotions), IEMOCAP  (Busso et al., 2008)  (5,531 utterances from conversations, 4 emotions), RAVDESS (Livingstone and Russo, 2018) (4 800 audio-video recordings of speech and song, 8 emotions), and SAVEE  (Jackson and Haq, 2014)  (480 samples, 7 emotions).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Implementation Details",
      "text": "We use Qwen2-Audio-7B-instruct as the foundational backbone model for our experiments. The RL models are trained using eight NVIDIA RTX A6000 GPUs, each processing a per-device batchsize of 1 with gradient accumulation over 2 steps. Training proceeds for 300 optimisation steps under a learning rate of 1 × 10 -6 and a softmax temperature of 1.0. Each reinforcement learning optimization step generates 6 responses per sample. SFT models are optimized with AdamW at a learning rate of 1 × 10 -5 for five complete epochs. The optimal iteration results are selected for final analysis.",
      "page_start": 1,
      "page_end": 6
    },
    {
      "section_name": "Baselines And Metrics",
      "text": "We benchmark the proposed approach against state-of-the-art methods, which we group into three distinct categories: W/o-LALM, LALM, and LALM-FT. W/o-LALM and LALM-FT refer to models post-trained on the MELD training set, while LALM involves zero-shot inference using prompt strategies without task-specific fine-tuning.\n\n• W/o-LALM: We selected four advanced selfsupervised pre-trained audio models: Hu-BERT large  (Hsu et al., 2021) , data2vec 2.0 large  (Baevski et al., 2023) , WavLM large  (Chen et al., 2022) , Whisper large v3  (Radford et al., 2023)  and Emotion2vec  (Ma et al., 2024b) . Features from the last Transformer layer of these frozen pre-trained models were extracted to train the downstream linear layers with a hidden dimension of 256.\n\n• LALM: We directly use Qwen2-Audio  (Chu et al., 2024)  for SER tasks without additional training or fine-tuning, employing two prompt patterns: direct inference and chain-of-thought inference.\n\n• LALM-FT: We further trained Qwen2-Audio.\n\nTo evaluate different training methods, we compare models trained with supervised fine-tuning (SFT), GRPO  (Shao et al., 2024) , and EMO-RL. Additionally, we assess the impact of different reasoning strategies in EMO-RL: implicit reasoning, unstructured explicit reasoning, and structured explicit reasoning.\n\nIn our evaluation, we utilize three key metrics: Unweighted Accuracy (UA), Weighted Accuracy (WA), and Macro F1 Score (F1), to assess the performance of the SER task. WA reflects the overall accuracy of the model across all emotion classes. UA measures the average accuracy by considering each emotion class equally, regardless of its frequency in the dataset. The macro-F1 score, harmonic mean of precision and recall, furnishes a balanced and class-agnostic gauge of model efficacy, particularly in scenarios where there is an imbalance in the distribution of emotion classes.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Main Performance",
      "text": "As shown in Table  1 , the results demonstrate the effectiveness of COT in LALM, our proposed ESWR, and ESR. Effectiveness of COT in LALM: Using CoT prompts significantly enhances the zeroshot SER performance of LALMs. In fact, CoT enables Qwen2-Audio to approach the performance Table  1 : The comparison of the main performance metrics for various methods on the MELD and IEMOCAP datasets. Results for W/o-LALM methods are cited from the Emobox benchmark  (Ma et al., 2024a)  and  (Ma et al., 2024b) . The bold font indicates the best results among all models. The baseline here denotes the GRPO+IR, and the SOTA means the best results among W/o-LALM methods.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Type Model",
      "text": "Method MELD IEMOCAP UA(%) WA(%) F1(%) UA(%) WA(%) F1(%)\n\nHuBERT large  (Hsu et al., 2021)  Classification Head 24.13 46.37 24.99 67.42 66.69 67.24 WavLM large  (Chen et al., 2022)  Classification Head 28.18 49.31 29.11 69.47 69.07 69.29 data2vec 2.0 large  (Baevski et al., 2023) Classification Head 26.33 47.72 27.35 57.30 56.23 56.70  Whisper large V3  (Radford et al., 2023) Classification Head 31.54 51.89 32.95 73.54 72.86 73.11  Emotion2vec+ large  (Ma et al., 2024b)",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "/ Comparason",
      "text": "Ours VS SOTA ↑25.1 ↑34.0 ↑27.1 ↑18.9 ↑19.8 ↑19.6 Ours VS Baseline ↑24.9 ↑25.5 ↑26.0 ↑6.95 ↑10.9 ↑10.8 of the best W/o-LALM pre-trained audio models without any task-specific post-training. Effectiveness of ESWR: When training and testing on the same dataset, the direct use of GRPO achieves similar accuracy to SFT. This may be due to the MELD dataset containing considerable noise, resulting in the model's lower ability to recognize correct emotions. This leads to GRPO's binary rewards being too sparse, with 60% of accuracy rewards being 0, making it difficult for the model's update policy to stabilize. However, ESWR provides more dense and psychologically grounded reward signals, improving the model's emotional reasoning capability by consistently guiding it toward the correct emotional direction.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Effectiveness Of Esr Training Strategies.",
      "text": "Besides the ESWR method, compared to models trained with IR, models trained with EUR and ESR can both enhance emotional reasoning capabilities, improving accuracy on the MELD and IEMO-CAP test sets. Moreover, models with structured thinking capabilities achieve superior accuracy relative to models lacking structured reasoning mechanisms, indicating that structured reasoning helps models avoid errors. Through the above experiments, we have demonstrated that using the EMO-RL algorithm can significantly enhance the emotional reasoning capabilities of LALMs, achieving SOTA performance in SER tasks. Additionally, we found that our method yields greater improvements on datasets with more complex emotion labels, for example, the improvement of MELD, compared to IEMOCAP.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Generalizability",
      "text": "In practical scenarios, a model's ability to generalize emotion recognition to unseen individuals and unknown recording conditions is of paramount importance. To evaluate this capability, cross-dataset zero-shot testing offers an effective means of assessing a model's generalization in emotion recognition. We meticulously selected three diverse datasets: IEMOCAP, RAVDESS, and SAVEE. These datasets encompass a variety of sources, accents, and recording environments, enabling a comprehensive evaluation of the model's generalization and robustness across real-world scenarios.\n\nAs shown in Table  2 , the results of cross-datasets evaluation demonstrate that (1) Reinforcement",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation",
      "text": "To explore the quantitative ablation of the effects of the reward mechanism alone. As shown in Table  3 , we have supplemented the relevant quantitative ablation experiments on MELD dataset. For the methods in the first three rows, we only used the corresponding format reward without the accuracy reward. For the last two rows, we only used the corresponding accuracy reward without the format reward. These results demonstrate the efficiency of our proposed ESWR and ESR methods for speech emotion recognition.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Case Study",
      "text": "In Figure  3 , we show a case study that demonstrates the response results when testing the same speech sample after training with different methods. Models trained with IR seem to have lost many other abilities, such as not trying to think and reasoning, even though I asked them to do so.\n\nModels trained with EUR can generate flexible reasoning based on different speech inputs. They often analyze emotions primarily through acoustic features such as pitch, rhythm, and speed. While this approach is effective for simpler cases, it faces challenges with more complex scenarios due to the omission of critical semantic emotional details.\n\nIn contrast, models trained with ESR explicitly document the speaker's key content and acoustic  features, followed by a comprehensive analysis of both semantic and auditory information. This structured approach reduces the likelihood of overlooking key details, thereby enhancing the model's emotional reasoning capabilities.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces EMO-RL, a reinforcement learning framework that improves the emotionalreasoning capacity of large audio-language models for speech-emotion recognition. By incorporating emotion similarity-weighted reward, which integrates psychological prior knowledge into RL, and Explicit Structured Reasoning into our framework, EMO-RL effectively overcomes the challenges of convergence instability and limited reasoning ability in speech emotion recognition tasks. Comprehensive experiments demonstrate that EMO-RL not only improves the emotional reasoning capabilities of LALMs on the MELD and IEMOCAP datasets (compared with SOTA, achieving an UA improvement of 25.1% and 18.9%, respectively), but also shows excellent generalization across different datasets. This work signifies a step forward in applying reinforcement learning and large audiolanguage models to speech emotion recognition, paving the way for future speech affective computing research. Moreover, EMO-RL shows potential for enhancing multi-modal LLMs' emotion perception, bringing us closer to building truly emotional LLMs.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limitation",
      "text": "Our proposed method has certain limitations that warrant attention. Firstly, while our EMO-RL framework is designed to be versatile and applicable across a variety of multi-modal scenarios, including video, audio, and text, our current experimental scope has been limited to the speech modality alone. We have not yet incorporated visual elements such as images or videos into our experimental design. This restriction means that the full potential of our framework in multi-modal contexts remains unexplored. Secondly, although exploiting the LALMs for SER tasks has delivered promising results, it has also introduced challenges related to computational complexity and inference efficiency. The inference efficiency of our approach is comparatively lower than that of previous methods, which might affect its practicality for real-time applications. In the future, we will try to solve the above limitations.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ethical Considerations",
      "text": "The deployment of SER systems raises significant ethical concerns that build upon established frameworks for sentiment and emotion analysis. Privacy and consent represent primary issues, as SER extracts sensitive psychological information from vocal patterns often without users' explicit awareness, unlike voluntary text-based sentiment analysis. Additionally, SER systems exhibit systematic biases across demographic groups and may misinterpret cultural differences in emotional expression, with training datasets often lacking diverse representation-problems shared with broader emotion analysis research. The \"black box\" nature of deep learning-based systems also raises accountability concerns when informing decisions affecting individuals' lives. These considerations highlight the need for robust consent frameworks, diverse datasets, and ethical guidelines specific to speech emotion recognition that address the unique challenges of speech emotion detection.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The key ideas of our proposed Emo-RL. Com-",
      "page": 1
    },
    {
      "caption": "Figure 1: ). We implement an Emotion Similarity-",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of the Emo-RL for LALM to improve the generalized speech emotion recognition. Building",
      "page": 4
    },
    {
      "caption": "Figure 2: For EUR, which re-",
      "page": 5
    },
    {
      "caption": "Figure 3: , we show a case study that demon-",
      "page": 8
    },
    {
      "caption": "Figure 3: An example of the reasoning results of IR,",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Policy\nModel\ne\nl\nb\na\nn\ni\na\nr\nt\nLALM": "",
          "Sampling Action Groups": "<think>...</think><answer>...</answer>\n<think>...</think><answer>...</answer>\n...                ...\n<think>...</think><answer>...</answer>\nUpdating Policy Model\nKL Divergence\nReference\nLALM\nd\ne\nn\ni\na\nr\nt\ne\nModel\nr\np"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sDisgustAnger\ncalculate\nìï\níïî\nEmotion-State-Transition Matrix\nEmotion Similarity-Weighted Reward\nPlutchik’s wheel of emotions \nPredicted\nEmotion\nJ\nS\nF\nS\nˆ,\ny\n=\n1\n1\no\nu\nNeutral\ne\n(\n)\nr\na\np\nr\ny Trust\nr\ni\ns\n+\ne\nR\nS\nˆ,\ny\ng>\na\n=\n×\nS\ny\n,\ny\n(\n)\nESWR\npred\ntrue\n(\n)\nI\nS\nGroundtruth\nn\na\nt\nd\ne\nr\nn\ne\ne\ns\ns\nt\nS\nˆ,\ny\ng£\n0\n(\n)\nEmotion labels\nEmotion similarity matrix": "",
          "1 0\nExplicit Structured\nReasoning Reward\nif the format\nì\nmatches\nR\nESRQ\nESR\n= í\notherwise.\nî": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Question\nWhat  is  the  emotion  of  the  speaker  in  the \naudio? After thinking and reasoning, choose \nthe ﬁnal answer from the following options: \n+\n[‘neutral’, ‘happy’, ‘sad’, ‘angry’].\nInput audio\nGroundtruth: happy\nQuestion prompt": "Response"
        },
        {
          "Question\nWhat  is  the  emotion  of  the  speaker  in  the \naudio? After thinking and reasoning, choose \nthe ﬁnal answer from the following options: \n+\n[‘neutral’, ‘happy’, ‘sad’, ‘angry’].\nInput audio\nGroundtruth: happy\nQuestion prompt": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human-computer interaction with a real-time speech emotion recognition with ensembling techniques 1d convolution neural network and attention",
      "authors": [
        "Waleed Alsabhan"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "Efficient self-supervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "Alexei Baevski",
        "Arun Babu",
        "Wei-Ning Hsu",
        "Michael Auli"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "I: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Peihao Chen",
        "Xiangmin Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Dst: Deformable speech transformer for emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "9",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Dejian Daya Guo",
        "Haowei Yang",
        "Junxiao Zhang",
        "Ruoyu Song",
        "Runxin Zhang",
        "Qihao Xu",
        "Shirong Zhu",
        "Peiyi Ma",
        "Wang"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "10",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Surrey audiovisual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audiovisual expressed emotion (savee) database"
    },
    {
      "citation_id": "12",
      "title": "Audio flamingo: a novel audio language model with fewshot learning and dialogue abilities",
      "authors": [
        "Zhifeng Kong",
        "Arushi Goel",
        "Rohan Badlani",
        "Wei Ping",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "year": "2024",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "13",
      "title": "Reinforcement learning outperforms supervised fine-tuning: A case study on audio question answering",
      "authors": [
        "Gang Li",
        "Jizhong Liu",
        "Heinrich Dinkel",
        "Yadong Niu",
        "Junbo Zhang",
        "Jian Luan"
      ],
      "year": "2025",
      "venue": "Reinforcement learning outperforms supervised fine-tuning: A case study on audio question answering",
      "arxiv": "arXiv:2503.11197"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition for power customer service",
      "authors": [
        "Xutong Li",
        "Rongheng Lin"
      ],
      "year": "2021",
      "venue": "International Conference on Computer and Communications"
    },
    {
      "citation_id": "15",
      "title": "Exploration of a selfsupervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "16",
      "title": "Hengsheng Fan, and Xiangmin Xu. 2023b. Multiscale temporal transformer for speech emotion recognition",
      "authors": [
        "Zhipeng Li",
        "Xiaofen Xing",
        "Yuanbo Fang",
        "Weibin Zhang"
      ],
      "venue": "Hengsheng Fan, and Xiangmin Xu. 2023b. Multiscale temporal transformer for speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "2024a. Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "M Ma",
        "H Chen",
        "Z Zhang",
        "W Zheng",
        "X Chen",
        "J Li",
        "X Ye",
        "Chen",
        "Hain"
      ],
      "venue": "2024a. Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark"
    },
    {
      "citation_id": "19",
      "title": "Audio-cot: Exploring chainof-thought reasoning in large audio language model",
      "authors": [
        "Ziyang Ma",
        "Zhuo Chen",
        "Yuping Wang",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "year": "2025",
      "venue": "Audio-cot: Exploring chainof-thought reasoning in large audio language model",
      "arxiv": "arXiv:2501.07246"
    },
    {
      "citation_id": "20",
      "title": "2024b. emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "venue": "Findings of the Association for Computational Linguistics ACL"
    },
    {
      "citation_id": "21",
      "title": "Automatic speech emotion recognition using machine learning: digital transformation of mental health",
      "authors": [
        "David Samaneh Madanian",
        "Olayinka Parry",
        "Christian Adeleye",
        "Farhaan Poellabauer",
        "Shilpa Mirza",
        "Sandy Mathew",
        "Schneider"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Pacific Asia Conference on Information Systems"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1982",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "24",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "Rafael Rafailov",
        "Archit Sharma",
        "Eric Mitchell",
        "Christopher Manning",
        "Stefano Ermon",
        "Chelsea Finn"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Hybrid group relative policy optimization: A multi-sample approach to enhancing policy optimization",
      "authors": [
        "Soham Sane"
      ],
      "year": "2025",
      "venue": "Hybrid group relative policy optimization: A multi-sample approach to enhancing policy optimization",
      "arxiv": "arXiv:2502.01652"
    },
    {
      "citation_id": "28",
      "title": "Proximal policy optimization algorithms",
      "authors": [
        "John Schulman",
        "Filip Wolski",
        "Prafulla Dhariwal",
        "Alec Radford",
        "Oleg Klimov"
      ],
      "year": "2017",
      "venue": "Proximal policy optimization algorithms",
      "arxiv": "arXiv:1707.06347"
    },
    {
      "citation_id": "29",
      "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "authors": [
        "Zhihong Shao",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Junxiao Song"
      ],
      "year": "2024",
      "venue": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "arxiv": "arXiv:2402.03300"
    },
    {
      "citation_id": "30",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "M Zejun",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "The International Conference on Learning Representations"
    },
    {
      "citation_id": "31",
      "title": "Kimi k1. 5: Scaling reinforcement learning with llms",
      "authors": [
        "Kimi Team",
        "Angang Du",
        "Bofei Gao",
        "Bowei Xing",
        "Changjiu Jiang",
        "Cheng Chen",
        "Cheng Li",
        "Chenjun Xiao",
        "Chenzhuang Du"
      ],
      "year": "2025",
      "venue": "Kimi k1. 5: Scaling reinforcement learning with llms",
      "arxiv": "arXiv:2501.12599"
    },
    {
      "citation_id": "32",
      "title": "What do speech foundation models not learn",
      "authors": [
        "Abdul Waheed",
        "Hanin Atwany",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2024",
      "venue": "What do speech foundation models not learn",
      "arxiv": "arXiv:2410.12948"
    },
    {
      "citation_id": "33",
      "title": "Enabling auditory large language models for automatic speech quality evaluation",
      "authors": [
        "Siyin Wang",
        "Wenyi Yu",
        "Yudong Yang",
        "Changli Tang",
        "Yixuan Li",
        "Jimin Zhuang",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang"
      ],
      "year": "2025",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Sari: Structured audio reasoning via curriculum-guided reinforcement learning",
      "authors": [
        "Cheng Wen",
        "Tingwei Guo",
        "Shuaijiang Zhao",
        "Wei Zou",
        "Xiangang Li"
      ],
      "year": "2025",
      "venue": "Sari: Structured audio reasoning via curriculum-guided reinforcement learning",
      "arxiv": "arXiv:2504.15900"
    },
    {
      "citation_id": "35",
      "title": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "year": "2025",
      "venue": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "arxiv": "arXiv:2503.02318"
    },
    {
      "citation_id": "36",
      "title": "Scaling relationship on learning mathematical reasoning with large language models",
      "authors": [
        "Zheng Yuan",
        "Hongyi Yuan",
        "Chengpeng Li",
        "Guanting Dong",
        "Keming Lu",
        "Chuanqi Tan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Scaling relationship on learning mathematical reasoning with large language models",
      "arxiv": "arXiv:2308.01825"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}