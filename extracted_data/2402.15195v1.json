{
  "paper_id": "2402.15195v1",
  "title": "The Affecttoolbox: Affect Analysis For Everyone",
  "published": "2024-02-23T08:55:47Z",
  "authors": [
    "Silvan Mertes",
    "Dominik Schiller",
    "Michael Dietz",
    "Elisabeth André",
    "Florian Lingenfelser"
  ],
  "keywords": [
    "Affective Computing",
    "Social Signal Analysis",
    "Open Source",
    "Emotion Recognition",
    "Affect Recognition",
    "Multimodal Fusion",
    "Pleasure",
    "Valence",
    "Arousal",
    "Dominance"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the field of affective computing, where research continually advances at a rapid pace, the demand for userfriendly tools has become increasingly apparent. In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes. The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers. Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface. The architecture encompasses a variety of models for emotion recognition on multiple affective channels and modalities, as well as an elaborate fusion system to merge multi-modal assessments into a unified result. The entire system is open-sourced and will be publicly available to ensure easy integration into more complex applications through a well-structured, Python-based code base -therefore marking a substantial contribution toward advancing affective computing research and fostering a more collaborative and inclusive environment within this interdisciplinary field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective computing describes a field of research, which incorporates the recognition, interpretation and simulation of human emotions in software systems. In particular, the Affect-Toolbox aims at the automated recognition of affective statesa sub-discipline of machine learning and artificial intelligence research. As part of this ever-evolving realm, affect recognition is characterized by rapid pace of new developments and a continuous pushing of boundaries in research. The exploration of affective computing often involves addressing recurring tasks such as interpreting facial expressions, analysing speech prosody and sentiment or observing non-verbal behaviour in body movements and poses. However, despite the growing interest and demand in this field, existing tools and frameworks (e.g. Microsoft's Platform for Situated Intelligence  [32]  or Google's MediaPipe  [25] ) present challenges for swift development, as they require in-depth programming knowledge and are primarily aimed at power users or experienced developers.\n\nEven within more accessible systems, a discernible gap exists, as most of them are focusing predominantly on providing the input modalities and signals, while lacking essential built-in functionality for comprehensive affect analysis and recognition. This limitation underscores the need for more versatile, accessible, and comprehensive architectures that cater to a broader audience, including researchers and practitioners without extensive programming backgrounds but high interest and demand for most recent solutions. Addressing these challenges is pivotal to advancing affective computing research and fostering a more inclusive and collaborative environment in this dynamic and interdisciplinary field.\n\nTo close this gap, in this paper, we present the AffectToolbox, a comprehensive affect recognition software system, that is\n\n• Easy to use. No programming knowledge requiredall necessary functionality can easily be controlled by a graphical user interface. • Comprehensive. A variety of models for interpreting accessible affective channels is included.\n\n• Easy to integrate. The AffectToolbox can easily be integrated into more complex applications, using its builtin networking functionality, allowing for a seamless connection to proprietary software implementations. • Open source. The whole software system and source code will be made publicly available. 1",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "To give a foundation for the conceptualization and development of the AffectToolbox, in the following we give a brief overview on existing software that addresses related goals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Multi-Modal Sensor Integration And Synchronization",
      "text": "Apache Flink  [14]  is an open-source stream processing and batch processing framework for big data processing and analytics. It is designed to efficiently process large volumes of data in real-time and batch processing modes. Flink provides a programming model and runtime for the processing of data streams in a distributed and fault-tolerant manner.\n\nSiAM-dp  [15]  serves as a platform dedicated to the development of multi-modal dialogue systems, with a key emphasis on effortlessly integrating dispersed input and output devices within the realm of cyber-physical environments.\n\nThe Social Signal Interpretation Framework (SSI)  [12]  is an efficient framework that offers robust support for diverse sensor devices, filter and feature algorithms, and provides Fig.  1 . The modular architecture of the AffectToolbox (Section III). In its current state, audiovisual sensory devices (e.g. webcams) provide easy means to generate all considered data streams (i.e. audio, transcript, video and skeleton data). So called activity checks trigger the machine learning based analysis of respective modalities (Section III-D). The uni-modal results of applied affect recognition models are represented by a subset of pleasure, arousal and/or dominance scores. These unimodal emotional cues are the input for an event-driven fusion algorithm (Section III-E), which deduces a coherent affective state, represented in the continuous PAD emotional space (Section III-C).\n\nan extensive C++ API to add further functionality and train custom machine learning models. Meanwhile, for end users, SSI provides accessibility through an XML interface. To also be accessible on mobile devices, Damian et al. ported the core functionality of SSI to a Java-based framework which they called SSJ  [20] .\n\nBarz et al. introduced the Multisensor Pipeline (MSP)  [31] , a Python framework that serves as lightweight tool for prototyping multi-modal sensor pipelines. A similar goal was followed by Saffaryazdi et al.  [34] , who introduced a Python framework called Octopus Sensing -however, in contrast to MSP, which focuses more on real-time applications, Octopus Sensing specifically addresses multi-modal data collection.\n\nTo the best of the authors' knowledge, all of the frameworks mentioned above, although providing comprehensive functionality to track and synchronize a multitude of different sensor hardware, do not provide pre-trained machine learning models, which allow for an immediate in-depth affective analysis of the modalities. They are limited to providing an efficient and extensible infrastructure for further computations (which may include the definition and training of custom classification models).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multi-Modal Signal Analysis",
      "text": "Due to the ongoing growth of the field, some large companies have also recognised the economic potential of according systems recently. As such, efforts were taken to release software that also incorporates ready-to-use machine learning models. For example, Google released their MediaPipe framework in 2019. MediaPipe is an open-source framework developed by Google that provides a comprehensive solution for building machine learning-based applications for various multimedia tasks. It is designed to facilitate the development of applications that involve real-time processing of audio, video, and other sensor data  [25] . MediaPipe also includes a set of pretrained ML models, e.g., models for object detection, image segmentation or face detection. Microsoft developed the Platform for Situated Intelligence (psi)  [32]  -a versatile and open framework designed for development of integrative AI systems that leverage multi-modal capabilities while also incorporating various ML models for tasks such as voice activity or mouth shape detection.\n\nAlthough those systems offer a plenitude of possibilities, they are, due to their industry-driven origin, designed to target a broad range of developers. As such, when building an application for a specific use case, still much effort has to be invested. Besides that being a time-consuming factor, proficient programming knowledge is required to build the respective applications, which often can be a hindering factor -especially if the system is to be used in research projects where no computer science background is represented at all.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. System Architecture A. Overview",
      "text": "The AffectToolbox consists of a variety of independent components and modules that communicate to each other in a queue-based, multi-threaded runtime (Figure  1 ). Depending on how the components interact with the queue system, we categorize them into:\n\n• Input components: Receive data from outside the framework (e.g., camera, microphones) and write it into at least one queue. • Processing components: Read data from at least one queue, process it in various ways, and write new data in at least one queue (e.g., preprocessing algorithms, machine learning models, etc.) • Output components: Read data from at least one queue, but as a data sink do not write new data to internal queues (e.g., GUI components, network adapters, etc.)\n\nAs such, components can be easily added by defining an input queue that the new component should load its data from, and/or an output queue that the new component should write its data to, as well as the functionality that the component should serve. Note that each module can process data in its own frequency -the respective frequency and other component-specific parameters are interfaced in a way that they can be adjusted via code or directly in the graphical user interface (GUI). Fig.  2 . User interacting with a virtual agent, whose emotional behaviour is taking real-time input from the AffectToolbox into consideration.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Interface Design",
      "text": "The GUI design of the AffectToolbox is crucial for its accessibility across fields. We have carefully designed it in a way that basic and important functions are easily visible, avoiding confusion for new users by keeping unnecessary details out of the main design. However, note that all necessary configuration parameters can be adjusted in respective context menus in greater detail -as such, addressing the needs for slightly more sophisticated use cases as well. In the GUI, all modalities and potential data flows are always on displays. Specific modalities can be activated with a simple click, triggering all necessary connections and dependencies automatically. Visually highlighting the active data flow ensures that users see ongoing interactions in real-time. The GUI can be seen in Figure  2 , where the AffectToolbox is used to steer the emotional behaviour of a virtual agent.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Emotion Model",
      "text": "For the implementation of the AffectToolbox, a common representation of the emotional user state has to be chosen. All included classification models need to recognize towards a respective affective golden standard. Also, the result is communicated to the user and / or subsequent applications in the chosen format.\n\nA common choice for automatic affect recognition are preselected categories or labels. Categorical emotion models subsume affective states under discrete categories like happiness, sadness, surprise or anger. This bears the advantage, that there is a wide and common understanding of these discrete categories of basic emotions  [2] .\n\nUsage of discrete labels is however restricting, as many blended feelings and emotions cannot adequately be described by the chosen categories. A more precise way to describe emotions is to describe the experienced stimuli with the help of continuous scales within dimensional models: Lang et al.  [4]  suggest to characterize emotions along two continuous axes, pleasure and arousal. Mehrabian  [3]  proposes dominance as an additional measurements (PAD model). The pleasure scale describes the pleasantness of a given emotion. A high value indicates an enjoyable emotion such as joy or happiness, lower values are associated with negative emotions like sadness and fear. The arousal scale measures the agitation level of an emotion. Dominance further refines the model by adding assessment of the feelings of being in control of a situation as well as autonomy. These continuous representations are less intuitive but allow continuous blending between affective states. They describe multiple aspects of an emotion, the combination of stimuli's alignments on these scales defines single emotions (Figure  3 ), which makes a conversion from dimensional values to emotional labels possible. These characteristics make the dimensional PAD model our choice for handling the affective golden standard within the AffectToolbox.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Modalities",
      "text": "The AffectToolbox applies signal processing and deep learning methods to interpret recorded signals by learning a mapping between observed signals and affective states. In uni-modal affect classification, information from one social channel,e.g. vocal properties, are used to make assumptions about the current emotional condition of a user. But as the cues that describe emotional conditions are indeed encoded within multiple modalities, the classification process should incorporate as much multi-modal information as possible  [9] .\n\n1) Face Analysis: Facial expressions are considered the most expressive transmitter of human emotions. Concentration and training would be needed to mask the depiction of affect in one's face reliably. But even then it takes a certain amount of time to control the muscle reactions that were triggered by the underlying emotional state and the correct emotion is expressed during this short period  [7] . Analysis of facial expressions has greatly benefited from advances in image processing and resulted in reliable affect recognition models.\n\na) Camera Component: Being able to connect to a variety of different camera hardware setups is essential for the accessibility of the toolbox. As such, we rely on OpenCV's camera interface solutions.  2  By doing so, we ensure compatibility with the majority of existing hardware, as such supporting both standard and specialized use cases. According to a specified sample rate, our camera component takes an image frame and writes it to the respective input queue.\n\nb) Face Preprocessing: Before passing the camera data on to our machine learning models, we process them with two preprocessing components. First, we use the MediaPipe 3  face detection solution to extract the users' face. If a face is found, a bounding box is placed around it, which is used to crop the image such that only face-relevant information remains. If no face is found, the image is passed on as-is. Subsequently, we normalize the images with min-max normalization (min = -1.0, max = 1.0) and resize them to 224x224 pixel. c) Face Activity Detection: We apply MediaPipe's face mesh detection solution to the preprocessed camera data. The resulting face landmarks have two major benefits:\n\n• First, if no face mesh is detected at all, it is likely that no user is in the tracking area of the camera. Having that information, we can ignore the face modality for the respective timeframes. • Second, by analyzing the landmark coordinates, we can obtain information on whether the user is actually facing the camera and/or screen. Depending on the application context, we can use that information to appropriately downweigh the face modality.\n\nd) Deep Facial Analysis: For the analysis of facial expressions we use a custom model that is based on the MobileNetV2 architecture  [22] . Its modifications include the addition of two model heads to enable the recognition of continuous valence/arousal values and the detection of eight discrete emotion classes (Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Contempt). The reasoning behind these choices was to improve the recognition accuracy while maintaining a lightweight architecture for faster processing  [30] . During the training procedure, both tasks were learned simultaneously from the AffectNet dataset  [26] . Afterwards, the secondary model head for discrete emotion detection was discarded as it only served to stabilize the primary task. The resulting model uses the preprocessed camera data as input and provides predictions for valence/arousal values as output. While there exist public corpora with reliable valence and arousal annotations, the dominance dimension is less covered by respective resources. To also derive a dominance score from the facial modality, we chose a literature-based approach: Studies such as  [1]  describe observed relations between the three scales of the PAD model as well as emotional labels. Based on these insights, one can infer a perceived dominance level from recognized facial expressions described in the valence/arousal space (or inferred emotional label).\n\n2) Pose Analysis: Although early work like Caridakis et al.  [5]  describes the possibilities of expressivity features (mainly calculated on arm and head movements extracted from video sequences), the main assumption was that body movement only shows the intensity of emotions. However, in recent years studies have shown that dynamic body movement and gestures as well as static postures convey affective states of a monitored person. Affective concepts such as a person's current expressivity  [10]  or engagement in a conversation  [17]  can be reliably described by suitable movement features, body orientation, and posture. Spatial expansive poses and postures including e.g. a straightened spine are used to convey dominance and self-confidence and (though a problem of replicating respective studies persists) there are clear hints towards effects on self perception and emotional experience  [29] .\n\na) Skeleton Detection: The actions and positions of body, head, and limbs -also referred to as kinesics -can be derived from detected pose landmarks in images. As a reliable detection model, we chose to integrate the lightweight convolutional neural network BlazePose  [28] , with which we can define a skeleton overlay for each video frame to be further processed into pose features.\n\nb) Pose Features: A logical consequence of this rising interest in kinesics is the effort to develop a reliable coding system for body movement in emotion expression, just like the by-now-available standards for facial expressions. For this reason, Dael et al.  [11]  describe the Body Action and Posture System: The system distinguishes body posture units and body action units  [6]  with the first representing the general alignments of trunk, head and limbs to a resting configuration (e.g. arms crossed) and the latter one describing a local and short termed movement of head or arms (e.g. pointing gesture). We went for a comparable rule-based approach by deducing a perceived dominance score based on pose features calculated from the detected pose landmarks (skeleton). Features by now include head and body tilts as well as overall activation of a user. Further features are continuously developed.\n\n3) Voice Analysis: Human language encodes emotional information in a semantic as well as in a paralinguistic way  [16] . Paralinguistics refers to phenomena that accompany speech and do not consist of linguistic units such as sounds, words, sentences, etc., but give it an additional communicative aspect. This includes acoustic characteristics such as pitch, volume or speaking speed.Figure  5  shows the audio signal of a person's normal speech leading into an affective burst of laughter. The characteristics of the signal parts can be well differentiated. Semantics on the other hand describes the content and the grammatical format of utterances, including the arrangement and choices of words, phrases, and clauses. Although modern language analysis models show promising results in analyzing both aspects of language within a unified architecture  [37] , it is still advantageous to analyze paralinguistics and semantics separately for optimal performance and flexibility.\n\na) Microphone Component: To track the user's speech, we use PyAudio 4 . We chose PyAudio because it enables us to directly connect to a multitude of different hardware setups and allows for a large variety of different technical configuration options. The tracked audio data is written into the respective input queue in chunks of predefined size. As such, although we can keep the communication frequency with the microphone comparably low, we still have the whole audio signal in the queue system without any quality losses. By tweeking the chunk size and grabbing frequency, the application-dependent trade-off between low latency and resource efficiency can be controlled if necessary. 4 https://github.com/CristiFati/pyaudio Fig.  5 . Audio signal of a person's normal speech leading into an affective burst of laughter. The characteristics of the signal parts can be well differentiated. Further processing of the signal carves out detailed differences and enables the categorization of more subtle affective states. b) Voice Activity Detection: For affective voice analysis, it is crucial to know when the tracked audio signals contain actual speech and not only background noises. As such, we make use of the WebRTC voice activity detector  5  . We feed the audio by using a sliding window, resulting in continuous probability estimations for the presence of voice. We can use those probabilities to downweigh or even completely ignore all voice-related modalities.\n\nc) Paralinguistic Analysis: Paralinguistic analysis refers to the analysis of voice properties unrelated to speech's linguistic content. As such, characteristics like intonation, pitch or loudness play an important role here. However, in recent years, performing paralinguistic analysis in an endto-end manner, i.e., without handcrafting specific features, is becoming more and more standard  [23] . As such, for the AffectToolbox, we use a current state-of-the-art network architecture for assessing pleasure, arousal, and dominance from voice  [37] . The model is based on the popular wav2vec2 architecture  [27]  and has proven to obtain high performance on standard emotion recognition benchmark tasks  [37] .\n\n4) Sentiment Analysis: When analyzing emotions from spoken language, the paralinguistic analysis of speech is complemented by the semantic analysis of the spoken content. While the employed voice analysis model (III-D3) is already incorporating linguistic information to some extent  [35] , we argue that the integration of a dedicated sentiment analysis module is still adding value to the overall pipeline.\n\na) Speech-To-Text: In the preceding step, we must first convert the spoken language into a textual representation that can be processed by a sentiment analysis model. To this end we rely on the whisper model suite by Radford et el.  [36]  The model suite consists of an off-the-shelf encoder-decoder Transformer architecture as proposed by  [19]  at different scales. The novelty of the proposed work lies in the introduction of a weakly supervised multitask training procedure that relies heavily on large-scale data crawled from the web.\n\nThe authors found that their proposed training procedure enables the model to perform many speech processing pipeline tasks like multilingual speech recognition, speech translation, spoken language identification, and voice activity detection at the same time while also enabling the resulting models to be competitive with other supervised models but in a zero-shot transfer setting, i.e. on data for which the model has not seen any examples. All in all, these features make the model an ideal fit for the AffectToolbox.\n\nb) Sentiment: To analyze the sentiment of a spoken utterance we rely on the XLM roBERTa model from Barbieri et al.  [33]  The roBERTa model is a pretrained variant of the BERT  [21]  architecture introduced by Liu et al.  [24]  that relies upon an optimized training procedure to improve results. The XLM roBERTa multilingual model has been further trained on a vast amount of Twitter data and provides support for sentiment analysis in eight different languages. The authors chose the model architecture and datasets specifically to make it suitable for agent-human-interaction scenarios, which is also an ideal scenario for the AffectToolbox",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Fusion",
      "text": "Including multiple modalities in the affect recognition process is generally meant to enhance performance. In addition to the increase in information, the system as a whole becomes more robust: Modalities that temporarily -due to tracking failures or general lack of activity -do not contribute meaningful information can be substituted by other channels. The most impactful factor for the quality of a multi-modal affect recognition system is the ability to extract informative features and results from its single modalities (Section III-D). The fusion strategy to integrate this information into a coherent decision is however of equal importance for the design of the AffectToolbox.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1) Requirements:",
      "text": "A key distinction between considered fusion approaches relates to the modeling of temporally shifted occurrences of emotional cues throughout multiple affective channels. While simple fusion strategies mainly apply a synchronous strategy, which considers all information within a fixed time segment, more elaborate systems regard these asynchronous characteristics of emotional manifestations and try to model them within the fusion process. Multi-modal affective events, such as facial expressions or vocal bursts that manifest certain emotional states, are expected to occur at shifted points in time and we consequently need to asynchronously treat modalities. We can therefore formulate key requirements for the AffectToolbox fusion algorithm.\n\nTemporal Flow: If we recognize an affective cue in a modality, it enters the fusion process and influences the continuous result. The initial influence will diminish over time until the cue is ´no longer relevant and gets discarded. Current cues therefore have a stronger impact on the fusion process than the ones that lie further down the time axis.\n\nReinforcement and Attenuation: If complementary cues are detected in overlapping time-segments, they reinforce each other by amplifying their impact on the continuous fusion output. On the other hand, contradictory cues neutralize each other and therefore have a lesser negative and attenuating effect on the fusion result. This way, additional information from multiple modalities is more likely to enhance the overall classification performance.\n\nReal-Time Fusion Result: The AffectToolbox targets applications, that are reactive to current emotional states of a user. We consequently aim for near real-time recognition tasks on online input, and need the processing speed of all all algorithms to be suitable for this task. The result of the fusion scheme is calculated by temporal influences (expressed through diminishing weights) of registered cues and a current fusion result has to be available at any given point in time, guaranteeing access to the latest affective estimation for all subsequent components of an application. Fig.  6 . The fusion result is calculated from active events in fixed update steps: At each time frame, the influence of active event vectors is reduced based on the defined decay speed, expired lifetime, and the initial norm of the vector and weight of the event. Within the three-dimensional PAD space, the fusion result is drawn towards the center of mass specified by weighted event vectors.\n\n2) Implementation: We based the implementation of our fusion mechanism on preceding work done by Gilroy et al.  [8] , which represents emotions as a vector within a dimensional emotion model. We generalized this approach by designing an event-driven fusion scheme that operates in a user-defined vector space  [13] ,  [18] .\n\nAffective cues, recognized in observed modalities, are registered as events for an event-driven algorithm, which continuously calculates its multi-modal fusion result based on the currently active affective events (Figure  6 ). As the AffectToolbox is operating with the pleasure-arousaldominance emotion model, respective events are represented as three-dimensional PAD vectors and provided with several parameters: A score, which defines the position of the vector within the dimensional PAD model, is assigned for each axis in the event space. Each vector is given a weight parameter, which serves as a quantifier for its impact on the calculation of the fusion result. This way reliability of a modality or cue type can regulated. Finally, a decay speed parameter describes the average lifespan of cues extracted from the respective signal. It determines the time it takes for the event's influence to fully diminish and get discarded. Events with strong indications can be given longer decay times to prolong their influence on the result. Given these parameters, the fusion result can be calculated from active events at each update step: At each time frame, active event vectors e = 1 . . . E are decayed by multiplying each vector element with a decay factor that is calculated based on the defined decay speed, expired lifetime and the initial norm of the vector:\n\nIf the resulting norm of the decayed vector stays above zero, it remains active -otherwise the vector is discarded. Afterwards, the fusion point within the vector space is calculated from all active event vectors: For each dimension d = 1 . . . D of the vector space respective scores of active event vectors e = 1 . . . E (modified by their weight factor) are summed up.\n\nThe result is normalized by the sum of the weights of all contributing event vectors.\n\nThe final result itself is a vector that approaches the calculated fusion point within the PAD space with a predefined speed parameter (Figure  6 ). If no events remain active in the vector space, the fusion vector approaches a neutral state. The fusion vector serves as an additional means of smoothing: If we can assume that the thought affective state is unlikely to undergo quick changes, the vector can be defined to move slowly, making the algorithm more robust to occasional misclassifications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Discussion",
      "text": "We have by now seen how the AffectToolbox can analyze multi-modal behaviour and subsequently deduce the affective states of users. This capability offers a wide range of possible applications but also features some technical requirements and conceptual limitations that have to be taken into account.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Potential Use-Cases",
      "text": "Monitoring of affective user states is of great interest in theoretical research scenarios as well as practical software design. Psychological studies often include automated analysis and recording of proband behaviour. Several instances of the AffectToolbox can even be used to track emotional states of several study participants (e.g. dyadic or multi-person conversations, therapist and patient, etc.).\n\nIn most applications for which the AffectToolbox was specifically designed, the emotional analysis is used in a human-computer interaction (HCI) scenario, i.e. the interaction between human users and a virtual agent. User emotion can hereby serve as contextual information for the dialogue system to also include the affective intent of the user in steering the conversational flow. Also, the non-verbal behaviour of a virtual character can be designed in a very natural and reactive manner if insights into the current emotional condition of the human counterpart are available.\n\nA further use-case in the field of HCI is the adaptation of systems to user states. Techniques such as reinforcement learning can use e.g. PAD values as additional input to the knowledge space or in the calculation of the reward function. This way implicit feedback mechanisms greatly enhance the user experience.\n\nIn all described fields of application, study and system designers can benefit greatly from a comprehensive analysis tool, that is easy to set up and integrate into the broader infrastructure without the need to allocate resources to the implementation of the affect recognition pipeline. To this end, the AffectToolbox includes networking capabilities to broadcast its results in several formats (JSON, XML, etc.) and protocols (UDP, Kafka, etc.).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Technical Requirements",
      "text": "To hold on to the ease-of-use paradigm, we try to keep the technical requirements of the AffectToolbox within reasonable limits: Audiovisual sensory devices (e.g. webcams) provide easy means to generate all so far considered data streams (i.e. audio, transcript, video, and skeleton data) without reliance on expensive or complicated sensor hardware. The same approach is followed for our reliance on computing power. The AffectToolbox features an extensive list of tuning parameters (e.g. analysis frame rates, variable model sizes, etc.) to customize the system to the capabilities of available hardware. Of course, the best performance with a high frequency and accuracy of classification results is achieved with an up-to-date hardware setup, we can nevertheless achieve acceptable results with less costly and more available configurations. This is partly due to the fusion process, which is inherently able to interpolate within a sparse result space (Section III-E).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Limitations",
      "text": "Generally we consider the AffectToolbox to be able to recognize so-called displayed emotions. This means, thatat least with the audiovisual information used -there is no reliable way to differentiate between a genuine feeling or socially altered affective state. Whether the displayed and respectively measured emotions are felt or expressed for other reasons, like using them as conversation techniques, resulting PAD values will be the same from the system's point of view.\n\nWays to investigate deeper into the actually felt emotion mostly include context information and interpretation or additional physiological sensory equipment. Physiological responses (e.g. heart rate, skin conductivity, and respiration as well as eye gaze and pupil dilation) are more unconscious reactions to affect that are hard to control by untrained subjects, as they are regulated by the autonomous nervous system. Inclusion of this kind of analysis would on the one hand contradict the accessability paradigm of the AffectToolbox, on the other hand might be worth the effort to optionally offer further valuable user states such as stress, cognitive workload or user engagement (Section V).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion & Future Work",
      "text": "The AffectToolbox is able to offer multi-modal recognition of displayed user emotions in real-time. Results are given in fine-grained continuous pleasure-arousal-dominance levels. In contrast to related solutions, it is easy to apply, as no programming knowledge is required: A concise graphical user interface provides accessible configuration and customization possibilities. High-quality and ready-to-use recognition models are integrated into the toolbox for each modality, a sophisticated fusion algorithm has been implemented to handle the multi-modal affective cues and generate a coherent result. Integration into application setups is easy to accomplish as fusion results are continuously broadcasted and accessible at any point in time.\n\nSeveral research projects are currently using the AffectToolbox for the development of affect-sensitive studies and prototypes. Given the multi-disciplinary nature of these groups, we see numerous non-technical users applying the toolbox without major complications. Acceptance of the system underlines and reinforces the goal to provide an easy-to-handle solution to integrate affective computing into state-of-the-art HCI systems.\n\nThe toolbox is in active development, which first means keeping the system up-to-date with novel signal processing methods and machine learning models. Second, we are investigating potential the demand for additional affective states beyond PAD values. Offering insights into more medical and well-being-related user states such as stress levels together with emotional states seems to raise interest in potential user groups. Automatic measurement of engagement in interactions as well as estimations of cognitive workload are expected to be of great benefit for the adaptation of HCI systems to current user needs, e.g. in training or educational scenarios.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The modular architecture of the AffectToolbox (Section III). In its current state, audiovisual sensory devices (e.g. webcams) provide easy means to",
      "page": 2
    },
    {
      "caption": "Figure 1: ). Depending",
      "page": 2
    },
    {
      "caption": "Figure 2: User interacting with a virtual agent, whose emotional behaviour is",
      "page": 3
    },
    {
      "caption": "Figure 2: , where the AffectToolbox is used to steer",
      "page": 3
    },
    {
      "caption": "Figure 3: The three dimensional PAD model of affect. Continuous pleasure,",
      "page": 3
    },
    {
      "caption": "Figure 3: ), which makes a conversion from dimensional values to",
      "page": 3
    },
    {
      "caption": "Figure 4: Based on empirical studies, a dominance level can be derived from",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the audio signal of a person’s",
      "page": 5
    },
    {
      "caption": "Figure 5: Audio signal of a person’s normal speech leading into an affective burst",
      "page": 5
    },
    {
      "caption": "Figure 6: The fusion result is calculated from active events in fixed update",
      "page": 6
    },
    {
      "caption": "Figure 6: ). If no events remain active in the vector",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{forename.surname}@uni-a.de": "Abstract—In the field of affective computing, where research"
        },
        {
          "{forename.surname}@uni-a.de": "continually\nadvances\nat\na\nrapid\npace,\nthe\ndemand\nfor\nuser-"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "friendly tools has become increasingly apparent.\nIn this paper,"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "we\npresent\nthe AffectToolbox,\na\nnovel\nsoftware\nsystem that"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "aims to support researchers in developing affect-sensitive studies"
        },
        {
          "{forename.surname}@uni-a.de": "and prototypes. The proposed system addresses\nthe\nchallenges"
        },
        {
          "{forename.surname}@uni-a.de": "posed\nby\nexisting\nframeworks, which\noften\nrequire\nprofound"
        },
        {
          "{forename.surname}@uni-a.de": "programming\nknowledge\nand\ncater\nprimarily\nto\npower-users"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "or\nskilled\ndevelopers. Aiming\nto\nfacilitate\nease\nof\nuse,\nthe"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "AffectToolbox\nrequires no programming knowledge\nand offers"
        },
        {
          "{forename.surname}@uni-a.de": "its\nfunctionality to reliably analyze\nthe affective\nstate of users"
        },
        {
          "{forename.surname}@uni-a.de": "through an accessible graphical user interface. The architecture"
        },
        {
          "{forename.surname}@uni-a.de": "encompasses\na\nvariety\nof models\nfor\nemotion\nrecognition\non"
        },
        {
          "{forename.surname}@uni-a.de": "multiple affective\nchannels and modalities, as well as an elab-"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "orate\nfusion system to merge multi-modal\nassessments\ninto\na"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "unified result. The\nentire\nsystem is\nopen-sourced and will be"
        },
        {
          "{forename.surname}@uni-a.de": "publicly available to ensure easy integration into more complex"
        },
        {
          "{forename.surname}@uni-a.de": "applications\nthrough a well-structured, Python-based code base"
        },
        {
          "{forename.surname}@uni-a.de": "- therefore marking a substantial contribution toward advancing"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "affective computing research and fostering a more collaborative"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "and inclusive environment within this interdisciplinary field."
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "Index Terms—Affective Computing,\nSocial\nSignal Analysis,"
        },
        {
          "{forename.surname}@uni-a.de": "Open Source, Emotion Recognition, Affect Recognition, Multi-"
        },
        {
          "{forename.surname}@uni-a.de": "modal Fusion, Pleasure, Valence, Arousal, Dominance"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "I.\nINTRODUCTION"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "Affective\ncomputing\ndescribes\na field\nof\nresearch, which"
        },
        {
          "{forename.surname}@uni-a.de": "incorporates\nthe recognition,\ninterpretation and simulation of"
        },
        {
          "{forename.surname}@uni-a.de": "human emotions in software systems. In particular,\nthe Affect-"
        },
        {
          "{forename.surname}@uni-a.de": ""
        },
        {
          "{forename.surname}@uni-a.de": "Toolbox aims at\nthe automated recognition of affective states -"
        },
        {
          "{forename.surname}@uni-a.de": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "generate all considered data streams\n(i.e. audio,\ntranscript, video and skeleton data). So called activity checks\ntrigger\nthe machine learning based analysis"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "of\nrespective modalities (Section III-D). The uni-modal\nresults of applied affect\nrecognition models are represented by a subset of pleasure, arousal and/or"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "dominance scores. These unimodal emotional cues are the input for an event-driven fusion algorithm (Section III-E), which deduces a coherent affective state,"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "represented in the continuous PAD emotional space (Section III-C)."
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "developed by Google that provides a comprehensive solution\nan extensive C++ API\nto add further\nfunctionality and train"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "for building machine learning-based applications\nfor various\ncustom machine learning models. Meanwhile,\nfor end users,"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "multimedia tasks. It is designed to facilitate the development of\nSSI provides accessibility through an XML interface. To also"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "applications that\ninvolve real-time processing of audio, video,\nbe accessible on mobile devices, Damian et al. ported the core"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "and\nother\nsensor\ndata\n[25]. MediaPipe\nalso\nincludes\na\nset\nfunctionality of SSI\nto a\nJava-based framework which they"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "called SSJ [20].\nof pretrained ML models,\ne.g., models\nfor object detection,"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "Pipeline\nimage\nsegmentation\nor\nface\ndetection. Microsoft\ndeveloped\nBarz\net\nal.\nintroduced\nthe Multisensor\n(MSP)"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "the Platform for Situated Intelligence (psi)\n[32]\n- a versatile\n[31], a Python framework that\nserves as\nlightweight\ntool\nfor"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "and open framework designed for development of\nintegrative\nprototyping multi-modal sensor pipelines. A similar goal was"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "AI\nsystems\nthat\nleverage multi-modal capabilities while also\nfollowed by Saffaryazdi et al.\n[34], who introduced a Python"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "incorporating\nvarious ML models\nfor\ntasks\nsuch\nas\nvoice\nframework called Octopus Sensing - however,\nin contrast\nto"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "activity or mouth shape detection.\nMSP, which focuses more on real-time applications, Octopus"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "Although those\nsystems offer\na plenitude of possibilities,\nSensing specifically addresses multi-modal data collection."
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "they\nare,\ndue\nto\ntheir\nindustry-driven\norigin,\ndesigned\nto\nTo the best of the authors’ knowledge, all of the frameworks"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "target a broad range of developers. As\nsuch, when building\nmentioned above, although providing comprehensive function-"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "an application for\na\nspecific use\ncase,\nstill much effort has\nality to track and synchronize a multitude of different sensor"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "to be\ninvested. Besides\nthat being a\ntime-consuming factor,\nhardware, do not provide pre-trained machine learning models,"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "proficient\nprogramming\nknowledge\nis\nrequired\nto\nbuild\nthe\nwhich allow for an immediate in-depth affective analysis of"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "respective applications, which often can be a hindering factor\nthe modalities. They are limited to providing an efficient and"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "- especially if\nthe system is\nto be used in research projects\nextensible infrastructure for\nfurther computations (which may"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "where no computer science background is represented at all.\ninclude\nthe\ndefinition\nand\ntraining\nof\ncustom classification"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "models)."
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "III. SYSTEM ARCHITECTURE"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "B. Multi-modal Signal Analysis\nA. Overview"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "The AffectToolbox\nconsists\nof\na\nvariety\nof\nindependent\nDue to the ongoing growth of the field, some large compa-"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "components and modules\nthat communicate to each other\nin\nnies have also recognised the economic potential of accord-"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "a queue-based, multi-threaded runtime (Figure 1). Depending\ning systems\nrecently. As\nsuch, efforts were taken to release"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "software\nthat\nalso\nincorporates\nready-to-use machine\nlearn-\non how the components\ninteract with the queue system, we"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "categorize them into:\ning models. For\nexample, Google\nreleased their MediaPipe"
        },
        {
          "Fig. 1.\nThe modular architecture of\nthe AffectToolbox (Section III).\nIn its current state, audiovisual sensory devices (e.g. webcams) provide easy means to": "framework in 2019. MediaPipe is an open-source framework"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "selected categories or labels. Categorical emotion models sub-"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "sume affective states under discrete categories like happiness,"
        },
        {
          "A common choice for automatic affect recognition are pre-": "sadness,\nsurprise\nor\nanger. This\nbears\nthe\nadvantage,\nthat"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "there is a wide and common understanding of\nthese discrete"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "categories of basic emotions [2]."
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "Usage\nof\ndiscrete\nlabels\nis\nhowever\nrestricting,\nas many"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "blended feelings and emotions cannot adequately be described"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "by\nthe\nchosen\ncategories. A more\nprecise way\nto\ndescribe"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "emotions is to describe the experienced stimuli with the help of"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "continuous scales within dimensional models: Lang et al.\n[4]"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "suggest\nto characterize emotions along two continuous axes,"
        },
        {
          "A common choice for automatic affect recognition are pre-": ""
        },
        {
          "A common choice for automatic affect recognition are pre-": "pleasure and arousal. Mehrabian [3] proposes dominance as"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "•\nInput components: Receive data from outside the frame-": "work (e.g., camera, microphones) and write it into at least",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "For\nthe\nimplementation\nof\nthe AffectToolbox,\na\ncommon"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "one queue.",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "representation of\nthe emotional user\nstate has\nto be chosen."
        },
        {
          "•\nInput components: Receive data from outside the frame-": "• Processing\ncomponents: Read\ndata\nfrom at\nleast\none",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "All\nincluded classification models need to recognize towards"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "queue, process it in various ways, and write new data in at",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "a\nrespective\naffective\ngolden\nstandard. Also,\nthe\nresult\nis"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "least one queue (e.g., preprocessing algorithms, machine",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "communicated to the user\nand / or\nsubsequent\napplications"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "learning models, etc.)",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "in the chosen format."
        },
        {
          "•\nInput components: Receive data from outside the frame-": "• Output components: Read data from at\nleast one queue,",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "A common choice for automatic affect recognition are pre-"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "but\nas\na\ndata\nsink\ndo\nnot write\nnew data\nto\ninternal",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "selected categories or labels. Categorical emotion models sub-"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "queues (e.g., GUI components, network adapters, etc.)",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "sume affective states under discrete categories like happiness,"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "sadness,\nsurprise\nor\nanger. This\nbears\nthe\nadvantage,\nthat"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "As\nsuch, components can be easily added by defining an",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "there is a wide and common understanding of\nthese discrete"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "input\nqueue\nthat\nthe\nnew component\nshould\nload\nits\ndata",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "categories of basic emotions [2]."
        },
        {
          "•\nInput components: Receive data from outside the frame-": "from, and/or an output queue that\nthe new component should",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "Usage\nof\ndiscrete\nlabels\nis\nhowever\nrestricting,\nas many"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "write its data to, as well as the functionality that\nthe compo-",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "blended feelings and emotions cannot adequately be described"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "nent\nshould serve. Note\nthat\neach module\ncan process data",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "by\nthe\nchosen\ncategories. A more\nprecise way\nto\ndescribe"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "in\nits\nown\nfrequency\n-\nthe\nrespective\nfrequency\nand\nother",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "emotions is to describe the experienced stimuli with the help of"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "component-specific parameters\nare\ninterfaced in a way that",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "continuous scales within dimensional models: Lang et al.\n[4]"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "they can be adjusted via code or directly in the graphical user",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "suggest\nto characterize emotions along two continuous axes,"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "interface (GUI).",
          "C. Emotion Model": ""
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "pleasure and arousal. Mehrabian [3] proposes dominance as"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "an additional measurements (PAD model). The pleasure scale"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "describes\nthe pleasantness of a given emotion. A high value"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "indicates an enjoyable emotion such as joy or happiness, lower"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "values\nare\nassociated with\nnegative\nemotions\nlike\nsadness"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "and fear. The\narousal\nscale measures\nthe\nagitation level of"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "an emotion. Dominance further\nrefines\nthe model by adding"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "assessment of the feelings of being in control of a situation as"
        },
        {
          "•\nInput components: Receive data from outside the frame-": "",
          "C. Emotion Model": "well as autonomy."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dimensional PAD model our choice for handling the affective": "golden standard within the AffectToolbox.",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "expressions we\nuse\na\ncustom model\nthat\nis\nbased\non\nthe"
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "MobileNetV2 architecture [22].\nIts modifications\ninclude the"
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "addition\nof\ntwo model\nheads\nto\nenable\nthe\nrecognition\nof"
        },
        {
          "dimensional PAD model our choice for handling the affective": "D. Modalities",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "continuous valence/arousal values and the detection of eight"
        },
        {
          "dimensional PAD model our choice for handling the affective": "The AffectToolbox applies signal processing and deep learn-",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "discrete emotion classes (Neutral, Happy, Sad, Surprise, Fear,"
        },
        {
          "dimensional PAD model our choice for handling the affective": "ing methods\nto\ninterpret\nrecorded\nsignals\nby\nlearning\na",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "Disgust, Anger\nand Contempt). The\nreasoning behind these"
        },
        {
          "dimensional PAD model our choice for handling the affective": "mapping\nbetween\nobserved\nsignals\nand\naffective\nstates.\nIn",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "choices was to improve the recognition accuracy while main-"
        },
        {
          "dimensional PAD model our choice for handling the affective": "uni-modal\naffect\nclassification,\ninformation from one\nsocial",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "taining a\nlightweight\narchitecture\nfor\nfaster processing [30]."
        },
        {
          "dimensional PAD model our choice for handling the affective": "channel,e.g. vocal properties, are used to make assumptions",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "During\nthe\ntraining\nprocedure,\nboth\ntasks were\nlearned\nsi-"
        },
        {
          "dimensional PAD model our choice for handling the affective": "about\nthe\ncurrent\nemotional\ncondition of\na user. But\nas\nthe",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "multaneously\nfrom the AffectNet\ndataset\n[26]. Afterwards,"
        },
        {
          "dimensional PAD model our choice for handling the affective": "cues\nthat describe\nemotional\nconditions\nare\nindeed encoded",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "the secondary model head for discrete emotion detection was"
        },
        {
          "dimensional PAD model our choice for handling the affective": "within multiple modalities,\nthe\nclassification process\nshould",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "discarded as it only served to stabilize the primary task. The"
        },
        {
          "dimensional PAD model our choice for handling the affective": "incorporate as much multi-modal\ninformation as possible [9].",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "resulting model uses\nthe preprocessed camera data\nas\ninput"
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "and provides predictions for valence/arousal values as output."
        },
        {
          "dimensional PAD model our choice for handling the affective": "1) Face Analysis:\nFacial\nexpressions\nare\nconsidered\nthe",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "most expressive transmitter of human emotions. Concentration",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "and training would be needed to mask the depiction of affect",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "in one’s face reliably. But even then it\ntakes a certain amount",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "of\ntime\nto control\nthe muscle\nreactions\nthat were\ntriggered",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "by\nthe\nunderlying\nemotional\nstate\nand\nthe\ncorrect\nemotion",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "is\nexpressed during this\nshort period [7]. Analysis of\nfacial",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "expressions\nhas\ngreatly\nbenefited\nfrom advances\nin\nimage",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "processing and resulted in reliable affect\nrecognition models.",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "a) Camera Component:\nBeing\nable\nto\nconnect\nto\na",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "variety of different camera hardware setups is essential for the",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "accessibility of\nthe toolbox. As\nsuch, we rely on OpenCV’s",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "camera\ninterface\nsolutions.2 By\ndoing\nso, we\nensure\ncom-",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "patibility with\nthe majority\nof\nexisting\nhardware,\nas\nsuch",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "supporting both standard and specialized use cases. According",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "to a\nspecified sample\nrate, our\ncamera\ncomponent\ntakes\nan",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "image frame and writes it\nto the respective input queue.",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "Fig. 4.\nBased on empirical studies, a dominance level can be derived from"
        },
        {
          "dimensional PAD model our choice for handling the affective": "b) Face Preprocessing: Before passing the camera data",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "facial expressions described in the valence/arousal space or inferred emotional"
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "labels."
        },
        {
          "dimensional PAD model our choice for handling the affective": "on to our machine learning models, we process them with two",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "preprocessing components. First, we use the MediaPipe3\nface",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "While there exist public corpora with reliable valence and"
        },
        {
          "dimensional PAD model our choice for handling the affective": "detection solution to extract\nthe users’ face. If a face is found,",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "arousal annotations,\nthe dominance dimension is less covered"
        },
        {
          "dimensional PAD model our choice for handling the affective": "a bounding box is placed around it, which is used to crop",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "by\nrespective\nresources. To\nalso\nderive\na\ndominance\nscore"
        },
        {
          "dimensional PAD model our choice for handling the affective": "the image such that only face-relevant\ninformation remains. If",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "from the facial modality, we chose a literature-based approach:"
        },
        {
          "dimensional PAD model our choice for handling the affective": "no face is found,\nthe image is passed on as-is. Subsequently,",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "Studies\nsuch as\n[1] describe observed relations between the"
        },
        {
          "dimensional PAD model our choice for handling the affective": "we normalize the images with min-max normalization (min =",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "three scales of\nthe PAD model as well as emotional\nlabels."
        },
        {
          "dimensional PAD model our choice for handling the affective": "−1.0, max = 1.0) and resize them to 224x224 pixel.",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "Based on these insights, one can infer a perceived dominance"
        },
        {
          "dimensional PAD model our choice for handling the affective": "c) Face Activity Detection: We apply MediaPipe’s\nface",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "level\nfrom recognized\nfacial\nexpressions\ndescribed\nin\nthe"
        },
        {
          "dimensional PAD model our choice for handling the affective": "mesh detection solution to the preprocessed camera data. The",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "valence/arousal space (or\ninferred emotional\nlabel)."
        },
        {
          "dimensional PAD model our choice for handling the affective": "resulting face landmarks have two major benefits:",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": ""
        },
        {
          "dimensional PAD model our choice for handling the affective": "",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "2) Pose Analysis: Although early work like Caridakis et al."
        },
        {
          "dimensional PAD model our choice for handling the affective": "First,\nif no face mesh is detected at all,\nit\nis\nlikely that",
          "d) Deep\nFacial\nAnalysis:\nFor\nthe\nanalysis\nof\nfacial": "[5] describes the possibilities of expressivity features (mainly"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "towards effects on self perception and emotional experience"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "[29]."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "a) Skeleton Detection:\nThe\nactions\nand\npositions\nof"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "kinesics\nbody,\nhead,\nand\nlimbs\n-\nalso\nreferred\nto\nas\n-\ncan"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "be\nderived\nfrom detected\npose\nlandmarks\nin\nimages. As\na"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "reliable detection model, we chose to integrate the lightweight"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "convolutional neural network BlazePose [28], with which we"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "can define a skeleton overlay for each video frame to be further"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "processed into pose features."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "b) Pose Features: A logical consequence of\nthis\nrising"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "interest\nin kinesics\nis\nthe effort\nto develop a reliable coding"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "system for body movement\nin emotion expression,\njust\nlike"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "the by-now-available standards for facial expressions. For this"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "reason, Dael et al. [11] describe the Body Action and Posture"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "System: The\nsystem distinguishes\nbody\nposture\nunits\nand"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "body action units\n[6] with the first\nrepresenting the general"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "alignments of trunk, head and limbs to a resting configuration"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "(e.g. arms crossed) and the latter one describing a local and"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "short termed movement of head or arms (e.g. pointing gesture)."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "We went for a comparable rule-based approach by deducing a"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "perceived dominance score based on pose features calculated"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "from the detected pose landmarks (skeleton). Features by now"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "include head and body tilts as well as overall activation of a"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "user. Further\nfeatures are continuously developed."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "3) Voice Analysis: Human language encodes emotional\nin-"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "formation in a semantic as well as in a paralinguistic way [16]."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "Paralinguistics\nrefers\nto phenomena\nthat\naccompany speech"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "and do not consist of\nlinguistic units such as sounds, words,"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "sentences, etc., but give it an additional communicative aspect."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "This includes acoustic characteristics such as pitch, volume or"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "speaking speed.Figure 5 shows the audio signal of a person’s"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "normal speech leading into an affective burst of laughter. The"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "characteristics of\nthe signal parts can be well differentiated."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "Semantics on the other hand describes\nthe\ncontent\nand the"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "grammatical\nformat of utterances,\nincluding the arrangement"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "and choices of words, phrases, and clauses. Although modern"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "language analysis models show promising results in analyzing"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "both aspects of\nlanguage within a unified architecture [37],\nit"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "is still advantageous to analyze paralinguistics and semantics"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "separately for optimal performance and flexibility."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "a) Microphone Component: To track the user’s\nspeech,"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "we use PyAudio4. We chose PyAudio because it enables us to"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "directly connect to a multitude of different hardware setups and"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "allows for a large variety of different\ntechnical configuration"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "options. The tracked audio data is written into the respective"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "input queue in chunks of predefined size. As such, although we"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "can keep the communication frequency with the microphone"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "comparably low, we still have the whole audio signal\nin the"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "queue\nsystem without\nany\nquality\nlosses. By\ntweeking\nthe"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "chunk size and grabbing frequency,\nthe application-dependent"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "trade-off between low latency and resource efficiency can be"
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": "controlled if necessary."
        },
        {
          "replicating\nrespective\nstudies\npersists)\nthere\nare\nclear\nhints": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "enables the model to perform many speech processing pipeline",
          "Reinforcement and Attenuation:\nIf complementary cues": "are detected in overlapping time-segments, they reinforce each"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "tasks like multilingual speech recognition, speech translation,",
          "Reinforcement and Attenuation:\nIf complementary cues": "other\nby\namplifying\ntheir\nimpact\non\nthe\ncontinuous\nfusion"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "spoken language identification, and voice activity detection at",
          "Reinforcement and Attenuation:\nIf complementary cues": "output. On the other hand, contradictory cues neutralize each"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "the same time while also enabling the resulting models to be",
          "Reinforcement and Attenuation:\nIf complementary cues": "other\nand\ntherefore\nhave\na\nlesser\nnegative\nand\nattenuating"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "competitive with other\nsupervised models but\nin a zero-shot",
          "Reinforcement and Attenuation:\nIf complementary cues": "effect on the fusion result. This way, additional\ninformation"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "transfer setting,\ni.e. on data for which the model has not seen",
          "Reinforcement and Attenuation:\nIf complementary cues": "from multiple modalities is more likely to enhance the overall"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "any examples. All\nin all,\nthese features make the model an",
          "Reinforcement and Attenuation:\nIf complementary cues": "classification performance."
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "ideal fit\nfor\nthe AffectToolbox.",
          "Reinforcement and Attenuation:\nIf complementary cues": "AffectToolbox\nReal-Time\nFusion\nResult:\nThe\ntargets"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "b) Sentiment:\nTo\nanalyze\nthe\nsentiment\nof\na\nspoken",
          "Reinforcement and Attenuation:\nIf complementary cues": "applications,\nthat\nare\nreactive\nto current\nemotional\nstates of"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "utterance we rely on the XLM roBERTa model from Barbieri",
          "Reinforcement and Attenuation:\nIf complementary cues": "a user. We\nconsequently aim for near\nreal-time\nrecognition"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "et al.\n[33] The roBERTa model\nis a pretrained variant of\nthe",
          "Reinforcement and Attenuation:\nIf complementary cues": "tasks on online\ninput,\nand need the processing speed of\nall"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "BERT [21] architecture introduced by Liu et al. [24] that relies",
          "Reinforcement and Attenuation:\nIf complementary cues": "all algorithms\nto be suitable for\nthis\ntask. The result of\nthe"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "upon an optimized training procedure to improve results. The",
          "Reinforcement and Attenuation:\nIf complementary cues": "fusion scheme is calculated by temporal\ninfluences (expressed"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "XLM roBERTa multilingual model has been further\ntrained",
          "Reinforcement and Attenuation:\nIf complementary cues": "through diminishing weights) of registered cues and a current"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "on a vast\namount of Twitter data\nand provides\nsupport\nfor",
          "Reinforcement and Attenuation:\nIf complementary cues": "fusion result has\nto be available at any given point\nin time,"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "sentiment\nanalysis\nin eight different\nlanguages. The\nauthors",
          "Reinforcement and Attenuation:\nIf complementary cues": "guaranteeing access\nto the\nlatest\naffective\nestimation for\nall"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "chose the model architecture and datasets specifically to make",
          "Reinforcement and Attenuation:\nIf complementary cues": "subsequent components of an application."
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "it suitable for agent-human-interaction scenarios, which is also",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "an ideal scenario for\nthe AffectToolbox",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "E. Fusion",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "Including multiple modalities\nin\nthe\naffect\nrecognition",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "process\nis\ngenerally meant\nto\nenhance\nperformance.\nIn",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "addition to the increase in information,\nthe system as a whole",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "becomes more\nrobust: Modalities\nthat\ntemporarily - due\nto",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "tracking failures or general\nlack of activity - do not contribute",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "meaningful\ninformation can be substituted by other channels.",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "The most\nimpactful\nfactor\nfor\nthe quality of\na multi-modal",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "affect\nrecognition system is the ability to extract\ninformative",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "features and results from its single modalities (Section III-D).",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "The\nfusion\nstrategy\nto\nintegrate\nthis\ninformation\ninto\na",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "coherent\ndecision\nis\nhowever\nof\nequal\nimportance\nfor\nthe",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "",
          "Reinforcement and Attenuation:\nIf complementary cues": "Fig. 6.\nThe\nfusion result\nis\ncalculated from active\nevents\nin fixed update"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "design of\nthe AffectToolbox.",
          "Reinforcement and Attenuation:\nIf complementary cues": "steps: At each time frame,\nthe influence of active event vectors\nis\nreduced"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "",
          "Reinforcement and Attenuation:\nIf complementary cues": "based on the defined decay speed, expired lifetime, and the initial norm of"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "",
          "Reinforcement and Attenuation:\nIf complementary cues": "the vector and weight of the event. Within the three-dimensional PAD space,"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "1) Requirements:\nA key\ndistinction\nbetween\nconsidered",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "",
          "Reinforcement and Attenuation:\nIf complementary cues": "the fusion result\nis drawn towards the center of mass specified by weighted"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "fusion\napproaches\nrelates\nto\nthe modeling\nof\ntemporally",
          "Reinforcement and Attenuation:\nIf complementary cues": "event vectors."
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "shifted\noccurrences\nof\nemotional\ncues\nthroughout multiple",
          "Reinforcement and Attenuation:\nIf complementary cues": ""
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "affective\nchannels. While\nsimple\nfusion\nstrategies mainly",
          "Reinforcement and Attenuation:\nIf complementary cues": "2)\nImplementation: We based the\nimplementation of our"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "apply a synchronous strategy, which considers all\ninformation",
          "Reinforcement and Attenuation:\nIf complementary cues": "fusion mechanism on preceding work done by Gilroy et al. [8],"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "within a fixed time segment, more elaborate systems\nregard",
          "Reinforcement and Attenuation:\nIf complementary cues": "which represents emotions as a vector within a dimensional"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "these asynchronous characteristics of emotional manifestations",
          "Reinforcement and Attenuation:\nIf complementary cues": "emotion model. We generalized this\napproach by designing"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "and try to model\nthem within the fusion process. Multi-modal",
          "Reinforcement and Attenuation:\nIf complementary cues": "an event-driven fusion scheme that operates in a user-defined"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "affective\nevents,\nsuch\nas\nfacial\nexpressions\nor\nvocal\nbursts",
          "Reinforcement and Attenuation:\nIf complementary cues": "vector space [13],\n[18]."
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "that manifest certain emotional\nstates, are expected to occur",
          "Reinforcement and Attenuation:\nIf complementary cues": "Affective\ncues,\nrecognized\nin\nobserved modalities,\nare"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "at\nshifted\npoints\nin\ntime\nand we\nconsequently\nneed\nto",
          "Reinforcement and Attenuation:\nIf complementary cues": "registered\nas\nevents\nfor\nan\nevent-driven\nalgorithm, which"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "asynchronously treat modalities. We can therefore formulate",
          "Reinforcement and Attenuation:\nIf complementary cues": "continuously\ncalculates\nits multi-modal\nfusion\nresult\nbased"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "key requirements for\nthe AffectToolbox fusion algorithm.",
          "Reinforcement and Attenuation:\nIf complementary cues": "on\nthe\ncurrently\nactive\naffective\nevents\n(Figure\n6).\nAs"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "",
          "Reinforcement and Attenuation:\nIf complementary cues": "AffectToolbox\nthe\nis\noperating with\nthe\npleasure-arousal-"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "Temporal Flow:\nIf we\nrecognize\nan\naffective\ncue\nin\na",
          "Reinforcement and Attenuation:\nIf complementary cues": "dominance emotion model,\nrespective events are represented"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "modality,\nit\nenters\nthe\nfusion\nprocess\nand\ninfluences\nthe",
          "Reinforcement and Attenuation:\nIf complementary cues": "as\nthree-dimensional PAD vectors and provided with several"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "continuous result. The initial influence will diminish over time",
          "Reinforcement and Attenuation:\nIf complementary cues": "parameters: A score, which defines the position of\nthe vector"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "until the cue is ´no longer relevant and gets discarded. Current",
          "Reinforcement and Attenuation:\nIf complementary cues": "within the dimensional PAD model,\nis assigned for each axis"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "cues\ntherefore have a stronger\nimpact on the fusion process",
          "Reinforcement and Attenuation:\nIf complementary cues": "in the event\nspace. Each vector\nis given a weight parameter,"
        },
        {
          "The\nauthors\nfound\nthat\ntheir\nproposed\ntraining\nprocedure": "than the ones that\nlie further down the time axis.",
          "Reinforcement and Attenuation:\nIf complementary cues": "which serves as a quantifier\nfor\nits impact on the calculation"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "cue\ntype\ncan\nregulated.\nFinally,\na\ndecay\nspeed\nparameter",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "between human users and a virtual agent. User emotion can"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "describes\nthe\naverage\nlifespan\nof\ncues\nextracted\nfrom the",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "hereby serve as contextual information for the dialogue system"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "respective\nsignal.\nIt\ndetermines\nthe\ntime\nit\ntakes\nfor\nthe",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "to also include the affective intent of\nthe user\nin steering the"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "event’s\ninfluence to fully diminish and get discarded. Events",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "conversational flow. Also, the non-verbal behaviour of a virtual"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "with strong indications\ncan be given longer decay times\nto",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "character can be designed in a very natural and reactive manner"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "prolong their\ninfluence on the result.",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "if\ninsights into the current emotional condition of\nthe human"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "counterpart are available."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "Given these parameters,\nthe fusion result can be calculated",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "A further use-case\nin the field of HCI\nis\nthe\nadaptation"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "from active events at each update step: At each time frame,",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "of\nsystems\nto user\nstates. Techniques\nsuch as\nreinforcement"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "active event vectors e = 1 . . . E are decayed by multiplying each",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "learning can use e.g. PAD values\nas additional\ninput\nto the"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "vector element with a decay factor that\nis calculated based on",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "knowledge space or\nin the calculation of\nthe reward function."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "the defined decay speed, expired lifetime and the initial norm",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "This way implicit\nfeedback mechanisms greatly enhance the"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "of\nthe vector:",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "user experience."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "In\nall\ndescribed\nfields\nof\napplication,\nstudy\nand\nsystem"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "(1)\ndecaye = norme − (li f etimee ∗ speede)",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "designers can benefit greatly from a comprehensive analysis"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "If the resulting norm of the decayed vector stays above zero, it",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "tool,\nthat\nis\neasy\nto\nset\nup\nand\nintegrate\ninto\nthe\nbroader"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "remains active - otherwise the vector is discarded. Afterwards,",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "infrastructure without\nthe need to allocate\nresources\nto the"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "the\nfusion point within the vector\nspace\nis\ncalculated from",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "implementation of the affect recognition pipeline. To this end,"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "all\nactive\nevent vectors: For\neach dimension d = 1 . . . D of",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "the AffectToolbox includes networking capabilities to broadcast"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "the vector space respective scores of active event vectors e =",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "its results in several formats (JSON, XML, etc.) and protocols"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "1 . . . E (modified by their weight\nfactor) are summed up.",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "(UDP, Kafka, etc.)."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "B. Technical Requirements"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "E∑e\n(2)\n(event vectord,e ∗ weighte)\nf usion point(d1...D) =",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "=1",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "To hold on to the ease-of-use paradigm, we try to keep the"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "technical requirements of the AffectToolbox within reasonable"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "The\nresult\nis normalized by the\nsum of\nthe weights of\nall",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "limits: Audiovisual\nsensory devices\n(e.g. webcams) provide"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "contributing event vectors.",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "easy means to generate all so far considered data streams (i.e."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "audio,\ntranscript, video, and skeleton data) without reliance on"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "E∑e\n(3)\nweighte\nf usion point(d1...D) = mass(d1...D)/",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "=1",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "expensive or complicated sensor hardware."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "The\nsame\napproach is\nfollowed for our\nreliance on com-"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "The final result\nitself is a vector that approaches the calculated",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "puting power. The AffectToolbox features an extensive list of"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "fusion point within the PAD space with a predefined speed",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "tuning parameters\n(e.g. analysis\nframe rates, variable model"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "parameter\n(Figure 6).\nIf no events remain active in the vector",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "sizes,\netc.)\nto\ncustomize\nthe\nsystem to\nthe\ncapabilities\nof"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "space,\nthe fusion vector approaches a neutral state. The fusion",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "available\nhardware. Of\ncourse,\nthe\nbest\nperformance with"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "vector serves as an additional means of smoothing:\nIf we can",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "a\nhigh\nfrequency\nand\naccuracy\nof\nclassification\nresults\nis"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "assume that\nthe thought affective state is unlikely to undergo",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "achieved with an up-to-date hardware\nsetup, we\ncan never-"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "quick changes, the vector can be defined to move slowly, mak-",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "theless achieve acceptable results with less costly and more"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "ing the algorithm more robust\nto occasional misclassifications.",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "available\nconfigurations.\nThis\nis\npartly\ndue\nto\nthe\nfusion"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "IV. DISCUSSION",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "process, which is inherently able to interpolate within a sparse"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "result space (Section III-E)."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "We have by now seen how the AffectToolbox can analyze",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "multi-modal behaviour and subsequently deduce the affective",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "C. Limitations"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "states of users. This capability offers a wide range of possible",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "Generally we\nconsider\nthe AffectToolbox\nto\nbe\nable\nto"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "applications but also features some technical requirements and",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "displayed\nrecognize\nso-called\nemotions. This means,\nthat\n-"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "conceptual\nlimitations that have to be taken into account.",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "at\nleast with the\naudiovisual\ninformation used -\nthere\nis no"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "A. Potential Use-Cases",
          "human-computer interaction (HCI) scenario, i.e. the interaction": ""
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "reliable way\nto\ndifferentiate\nbetween\na\ngenuine\nfeeling\nor"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "Monitoring of\naffective user\nstates\nis of great\ninterest\nin",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "socially\naltered\naffective\nstate. Whether\nthe\ndisplayed\nand"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "theoretical\nresearch\nscenarios\nas well\nas\npractical\nsoftware",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "respectively measured emotions are felt or expressed for other"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "design. Psychological studies often include automated analysis",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "reasons,\nlike using them as conversation techniques,\nresulting"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "and recording of proband behaviour. Several\ninstances of\nthe",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "PAD values will be the same from the system’s point of view."
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "AffectToolbox\ncan\neven\nbe\nused\nto\ntrack\nemotional\nstates",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "Ways\nto\ninvestigate\ndeeper\ninto\nthe\nactually\nfelt\nemo-"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "of\nseveral\nstudy\nparticipants\n(e.g.\ndyadic\nor multi-person",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "tion mostly\ninclude\ncontext\ninformation\nand\ninterpretation"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "conversations,\ntherapist and patient, etc.).",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "or additional physiological\nsensory equipment. Physiological"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "In most\napplications\nfor which\nthe\nAffectToolbox was",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "responses\n(e.g. heart\nrate,\nskin conductivity,\nand respiration"
        },
        {
          "of\nthe\nfusion\nresult. This way\nreliability\nof\na modality\nor": "specifically\ndesigned,\nthe\nemotional\nanalysis\nis\nused\nin\na",
          "human-computer interaction (HCI) scenario, i.e. the interaction": "as well as eye gaze and pupil dilation) are more unconscious"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "jects, as they are regulated by the autonomous nervous system.",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "James A Russell and Albert Mehrabian. “Evidence for a"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "Inclusion\nof\nthis\nkind\nof\nanalysis would\non\nthe\none\nhand",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "three-factor theory of emotions”. In: Journal of research"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "contradict\nthe accessability paradigm of the AffectToolbox, on",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "in Personality 11.3 (1977), pp. 273–294."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "the other hand might be worth the effort\nto optionally offer",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Paul Ekman.\n“An\nargument\nfor\nbasic\nemotions”.\nIn:"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "further valuable user states such as stress, cognitive workload",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Cognition & Emotion 6.3 (1992), pp. 169–200."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "or user engagement\n(Section V).",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "A. Mehrabian.\n“Framework\nfor\na\ncomprehensive\nde-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "scription\nand measurement\nof\nemotional\nstates.”\nIn:"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "V. CONCLUSION & FUTURE WORK",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Genetic,\nsocial,\nand\ngeneral\npsychology monographs"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "The AffectToolbox is able to offer multi-modal\nrecognition",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "121.3 (1995), pp. 339–361.\nISSN: 8756-7547."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "of\ndisplayed\nuser\nemotions\nin\nreal-time. Results\nare\ngiven",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Peter Lang, Margaret Bradley, and B. Cuthbert. “Mo-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "in fine-grained continuous pleasure-arousal-dominance levels.",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "tivated\nattention: Affect,\nactivation,\nand\naction”.\nIn:"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "In\ncontrast\nto\nrelated\nsolutions,\nit\nis\neasy\nto\napply,\nas\nno",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Attention and orienting: Sensory and motivational pro-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "programming knowledge is required: A concise graphical user",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "cesses. Ed. by P. Lang, R. Simons,\nand M. Balaban."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "interface provides accessible configuration and customization",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Psychology Press, 1997, pp. 97–135."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "possibilities. High-quality and ready-to-use recognition models",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "G. Caridakis\net\nal.\n“Synthesizing gesture\nexpressivity"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "are integrated into the toolbox for each modality, a sophisti-",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "based on real sequences”.\nIn: LREC 2006 Conference."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "cated fusion algorithm has been implemented to handle\nthe",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "2006."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "multi-modal\naffective\ncues\nand\ngenerate\na\ncoherent\nresult.",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Jinni Harrigan, Robert Rosenthal,\nand Klaus Scherer."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "Integration into application setups\nis\neasy to accomplish as",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "New handbook of methods\nin nonverbal behavior\nre-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "fusion results are continuously broadcasted and accessible at",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "search. Oxford University Press, 2008."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "any point\nin time.",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "P. Ekman. The Philosophy of Deception: Lie Catching"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "Several research projects are currently using the AffectTool-",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "and Micro Expressions. Ed. Clancy Martin, Oxford"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "box for\nthe development of affect-sensitive studies and proto-",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "University Press, 2009."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "types. Given the multi-disciplinary nature of these groups, we",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Stephen Gilroy et al. “PAD-based multimodal affective"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "see numerous non-technical users applying the toolbox without",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "fusion”. In: International Conference on Affective Com-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "major complications. Acceptance of the system underlines and",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "puting\nand\nIntelligent\nInteraction\n(ACII).\n2009. DOI:"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "reinforces the goal to provide an easy-to-handle solution to in-",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "10 . 1109 / ACII . 2009 . 5349552. URL: http : / / tcts . fpms ."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "tegrate affective computing into state-of-the-art HCI systems.",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "ac.be/publications/papers/2009/icacii2009 ju.pdf."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "The\ntoolbox is\nin active development, which first means",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Zhihong Zeng\net\nal.\n“A Survey\nof Affect Recogni-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "keeping the\nsystem up-to-date with novel\nsignal processing",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "tion Methods: Audio, Visual, and Spontaneous Expres-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "methods\nand machine\nlearning models. Second, we\nare\nin-",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "sions”. In: IEEE Trans. Pattern Anal. Mach. Intell. 31.1"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "vestigating potential\nthe demand for additional affective states",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "(2009), pp. 39–58."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "beyond PAD values. Offering insights into more medical and",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "George Caridakis\net\nal.\n“A multimodal\ncorpus\nfor"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "well-being-related\nuser\nstates\nsuch\nas\nstress\nlevels\ntogether",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "gesture expressivity analysis”. In: International Confer-"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "with emotional states seems to raise interest\nin potential user",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "ence on Language Resources and Evaluation (LREC),"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "groups. Automatic measurement of engagement in interactions",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Multimodal Corpora: Advances\nin Capturing, Coding"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "as well as estimations of cognitive workload are expected to",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "and Analyzing Multimodality. Malta, 2010."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "be of great benefit for the adaptation of HCI systems to current",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Nele Dael, Marcello Mortillaro, and KlausR. Scherer."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "user needs, e.g.\nin training or educational scenarios.",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "“The Body Action and Posture Coding System (BAP):"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "ACKNOWLEDGMENTS",
          "REFERENCES": "Development\nand Reliability”. English.\nIn: Nonverbal"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Behavior\n36.2 (2012), pp. 97–121.\nISSN: 0191-5886."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "This research is funded by the Federal Ministry of Education",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "DOI: 10.1007/s10919-012-0130-0."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "and Research, Project MITHOS (grand agreement 16SV8687)",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Johannes Wagner et al. “The social signal\ninterpretation"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "and Project UBIDENZ (grand agreement 13GW0568F).",
          "REFERENCES": ""
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "(SSI)\nframework: multimodal\nsignal\nprocessing\nand"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "the 21st\nrecognition in real-time”.\nIn: Proceedings of"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "ACM international\nconference\non Multimedia.\n2013,"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "pp. 831–834."
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Florian Lingenfelser\net\nal.\n“An Event Driven Fusion"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Approach for Enjoyment Recognition in Real-time”. In:"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Proceedings of\nthe ACM International Conference on"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "Multimedia, MM’14, Orlando, FL, USA, November 03"
        },
        {
          "reactions\nto affect\nthat are hard to control by untrained sub-": "",
          "REFERENCES": "- 07, 2014. 2014, pp. 377–386."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "the\nprocessing in a single engine”.\nIn: The Bulletin of",
          "body postures”.\nIn: Social and Personality Psychology": "Compass 14 (July 2020). DOI: 10.1111/spc3.12559."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Technical Committee on Data Engineering 38.4 (2015).",
          "body postures”.\nIn: Social and Personality Psychology": "Dominik Schiller et al. “Relevance-based data masking:"
        },
        {
          "[14]": "[15]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Robert Neßelrath.\n“SiAM-dp: An\nopen\ndevelopment",
          "body postures”.\nIn: Social and Personality Psychology": "a model-agnostic transfer\nlearning approach for\nfacial"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "platform for massively multimodal dialogue systems in",
          "body postures”.\nIn: Social and Personality Psychology": "expression recognition”. In: Frontiers in Computer Sci-"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "cyber-physical environments”.\nIn:\n(2015).",
          "body postures”.\nIn: Social and Personality Psychology": "ence 2 (2020), p. 6."
        },
        {
          "[14]": "[16]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Johannes Wagner, Florian Lingenfelser,\nand Elisabeth",
          "body postures”.\nIn: Social and Personality Psychology": "Michael Barz et al. “Multisensor-pipeline: a lightweight,"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Andr´e.\n“Building\na Robust\nSystem for Multimodal",
          "body postures”.\nIn: Social and Personality Psychology": "flexible,\nand\nextensible\nframework\nfor\nbuilding"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Emotion Recognition”.\nIn:\ned. by Aruna Chakraborty",
          "body postures”.\nIn: Social and Personality Psychology": "Companion\nmultimodal-multisensor\ninterfaces”.\nIn:"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Amit Konar. Wiley, 2015. Chap. 15, pp. 379–410.",
          "body postures”.\nIn: Social and Personality Psychology": "Publication\nof\nthe\n2021\nInternational Conference\non"
        },
        {
          "[14]": "[17]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Tobias Baur, Dominik Schiller,\nand Elisabeth Andr´e.",
          "body postures”.\nIn: Social and Personality Psychology": "Multimodal\nInteraction. 2021, pp. 13–18."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "“Modeling Users Social Attitude\nin a Conversational",
          "body postures”.\nIn: Social and Personality Psychology": "Dan Bohus et al. “Platform for\nsituated intelligence”."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "System”.\nIn: Emotions and Personality in Personalized",
          "body postures”.\nIn: Social and Personality Psychology": "In: arXiv preprint arXiv:2103.15975 (2021)."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Services. Springer, 2016, pp. 181–199.",
          "body postures”.\nIn: Social and Personality Psychology": "Francesco Barbieri,\nLuis\nEspinosa Anke,\nand\nJose"
        },
        {
          "[14]": "[18]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Florian Lingenfelser\net al.\n“Asynchronous\nand Event-",
          "body postures”.\nIn: Social and Personality Psychology": "Camacho-Collados.\n“XLM-T: Multilingual Language"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "based Fusion Systems\nfor Affect Recognition on Nat-",
          "body postures”.\nIn: Social and Personality Psychology": "Models\nin\nTwitter\nfor\nSentiment Analysis\nand Be-"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "uralistic Data\nin\nComparison\nto\nConventional Ap-",
          "body postures”.\nIn: Social and Personality Psychology": "the Thirteenth Language Re-\nyond”. In: Proceedings of"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "proaches”. In: IEEE Transactions on Affective Comput-",
          "body postures”.\nIn: Social and Personality Psychology": "sources and Evaluation Conference. Marseille, France:"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "ing (2016).",
          "body postures”.\nIn: Social and Personality Psychology": "European Language Resources Association, June 2022,"
        },
        {
          "[14]": "[19]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Ashish Vaswani et al. “Attention is all you need”.\nIn:",
          "body postures”.\nIn: Social and Personality Psychology": "pp. 258–266. URL: https://aclanthology.org/2022.lrec-"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Advances in neural\ninformation processing systems 30",
          "body postures”.\nIn: Social and Personality Psychology": "1.27."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "(2017).",
          "body postures”.\nIn: Social and Personality Psychology": "Nastaran\nSaffaryazdi, Aidin Gharibnavaz,\nand Mark"
        },
        {
          "[14]": "[20]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Ionut Damian, Michael Dietz,\nand Elisabeth Andr´e.",
          "body postures”.\nIn: Social and Personality Psychology": "Billinghurst.\n“Octopus Sensing: A Python library for"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "“The SSJ\nframework: Augmenting social\ninteractions",
          "body postures”.\nIn: Social and Personality Psychology": "human behavior\nstudies”.\nIn: Journal of Open Source"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "using mobile signal processing and live feedback”.\nIn:",
          "body postures”.\nIn: Social and Personality Psychology": "Software 7.71 (2022), p. 4045."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Frontiers in ICT 5 (2018), p. 13.",
          "body postures”.\nIn: Social and Personality Psychology": "Andreas\nTriantafyllopoulos\net\nal.\n“Probing\nSpeech"
        },
        {
          "[14]": "[21]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Jacob Devlin\net\nal.\n“Bert: Pre-training\nof\ndeep\nbidi-",
          "body postures”.\nIn: Social and Personality Psychology": "Emotion\nRecognition\nTransformers\nfor\nLinguistic"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "rectional\ntransformers for\nlanguage understanding”.\nIn:",
          "body postures”.\nIn: Social and Personality Psychology": "Knowledge”.\nIn:\n(2022)."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "arXiv preprint arXiv:1810.04805 (2018).",
          "body postures”.\nIn: Social and Personality Psychology": "Alec Radford\net\nal.\n“Robust\nspeech\nrecognition\nvia"
        },
        {
          "[14]": "[22]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "M. Sandler et al. “MobileNetV2: Inverted Residuals and",
          "body postures”.\nIn: Social and Personality Psychology": "large-scale weak supervision”. In: International Confer-"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Linear Bottlenecks”.\nIn: 2018 IEEE/CVF Conference",
          "body postures”.\nIn: Social and Personality Psychology": "ence on Machine Learning. PMLR. 2023, pp. 28492–"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "on Computer Vision\nand Pattern Recognition.\n2018,",
          "body postures”.\nIn: Social and Personality Psychology": "28518."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "pp. 4510–4520. DOI: 10.1109/CVPR.2018.00474.",
          "body postures”.\nIn: Social and Personality Psychology": "Johannes Wagner et al. “Dawn of the transformer era in"
        },
        {
          "[14]": "[23]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Johannes Wagner et al. “Deep learning in paralinguistic",
          "body postures”.\nIn: Social and Personality Psychology": "speech emotion recognition: closing the valence gap”."
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "recognition tasks: Are hand-crafted features\nstill\nrele-",
          "body postures”.\nIn: Social and Personality Psychology": "In: IEEE Transactions on Pattern Analysis and Machine"
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "vant?” In:\n(2018).",
          "body postures”.\nIn: Social and Personality Psychology": "Intelligence (2023)."
        },
        {
          "[14]": "[24]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Yinhan\nLiu\net\nal.\n“Roberta:\nA\nrobustly\nopti-",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "arXiv\npreprint\nmized\nbert\npretraining\napproach”.\nIn:",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "arXiv:1907.11692 (2019).",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "[25]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Camillo\nLugaresi\net\nal.\n“Mediapipe: A framework",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "for building perception pipelines”.\nIn: arXiv preprint",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "arXiv:1906.08172 (2019).",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "[26]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "A. Mollahosseini, B. Hasani, and M. H. Mahoor. “Af-",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "fectNet: A Database for Facial Expression, Valence, and",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Arousal Computing in the Wild”. In: IEEE Transactions",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "on Affective Computing 10.1 (2019), pp. 18–31. DOI:",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "10.1109/TAFFC.2017.2740923.",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "[27]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Alexei Baevski et al. “wav2vec 2.0: A framework for",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "self-supervised learning of speech representations”.\nIn:",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Advances in neural\ninformation processing systems 33",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "(2020), pp. 12449–12460.",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "[28]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Valentin Bazarevsky et al. BlazePose: On-device Real-",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "time Body Pose\ntracking.\n2020.\narXiv:\n2006 . 10204",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "[cs.CV].",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "[29]",
          "Paris Carbone et al. “Apache flink: Stream and batch": "Robert K¨orner and Astrid Schuetz. “Dominance or pres-",
          "body postures”.\nIn: Social and Personality Psychology": ""
        },
        {
          "[14]": "",
          "Paris Carbone et al. “Apache flink: Stream and batch": "tige: A review of\nthe effects of power poses and other",
          "body postures”.\nIn: Social and Personality Psychology": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "2",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "3",
      "title": "Framework for a comprehensive description and measurement of emotional states",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1995",
      "venue": "Genetic, social, and general psychology monographs"
    },
    {
      "citation_id": "4",
      "title": "Motivated attention: Affect, activation, and action",
      "authors": [
        "Peter Lang",
        "Margaret Bradley",
        "B Cuthbert",
        "P Lang",
        "R Simons",
        "M Balaban"
      ],
      "year": "1997",
      "venue": "Attention and orienting: Sensory and motivational processes"
    },
    {
      "citation_id": "5",
      "title": "Synthesizing gesture expressivity based on real sequences",
      "authors": [
        "G Caridakis"
      ],
      "year": "2006",
      "venue": "LREC 2006 Conference"
    },
    {
      "citation_id": "6",
      "title": "New handbook of methods in nonverbal behavior research",
      "authors": [
        "Jinni Harrigan",
        "Robert Rosenthal",
        "Klaus Scherer"
      ],
      "year": "2008",
      "venue": "New handbook of methods in nonverbal behavior research"
    },
    {
      "citation_id": "7",
      "title": "The Philosophy of Deception: Lie Catching and Micro Expressions",
      "authors": [
        "P Ekman"
      ],
      "year": "2009",
      "venue": "The Philosophy of Deception: Lie Catching and Micro Expressions"
    },
    {
      "citation_id": "8",
      "title": "PAD-based multimodal affective fusion",
      "authors": [
        "Stephen Gilroy"
      ],
      "year": "2009",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII.2009.5349552"
    },
    {
      "citation_id": "9",
      "title": "A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions",
      "authors": [
        "Zhihong Zeng"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "10",
      "title": "A multimodal corpus for gesture expressivity analysis",
      "authors": [
        "George Caridakis"
      ],
      "year": "2010",
      "venue": "International Conference on Language Resources and Evaluation (LREC), Multimodal Corpora: Advances in Capturing, Coding and Analyzing Multimodality"
    },
    {
      "citation_id": "11",
      "title": "The Body Action and Posture Coding System (BAP): Development and Reliability",
      "authors": [
        "Nele Dael",
        "Marcello Mortillaro",
        "Klausr Scherer"
      ],
      "year": "2012",
      "venue": "Nonverbal Behavior",
      "doi": "10.1007/s10919-012-0130-0"
    },
    {
      "citation_id": "12",
      "title": "The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time",
      "authors": [
        "Johannes Wagner"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "An Event Driven Fusion Approach for Enjoyment Recognition in Real-time",
      "authors": [
        "Florian Lingenfelser"
      ],
      "year": "2014",
      "venue": "Proceedings of the ACM International Conference on Multimedia, MM'14"
    },
    {
      "citation_id": "14",
      "title": "Apache flink: Stream and batch processing in a single engine",
      "authors": [
        "Paris Carbone"
      ],
      "year": "2015",
      "venue": "The Bulletin of the Technical Committee on Data Engineering"
    },
    {
      "citation_id": "15",
      "title": "SiAM-dp: An open development platform for massively multimodal dialogue systems in cyber-physical environments",
      "authors": [
        "Robert Neßelrath"
      ],
      "year": "2015",
      "venue": "SiAM-dp: An open development platform for massively multimodal dialogue systems in cyber-physical environments"
    },
    {
      "citation_id": "16",
      "title": "Building a Robust System for Multimodal Emotion Recognition",
      "authors": [
        "Johannes Wagner",
        "Florian Lingenfelser",
        "Elisabeth André"
      ],
      "year": "2015",
      "venue": "Building a Robust System for Multimodal Emotion Recognition"
    },
    {
      "citation_id": "17",
      "title": "Modeling Users Social Attitude in a Conversational System",
      "authors": [
        "Tobias Baur",
        "Dominik Schiller",
        "Elisabeth André"
      ],
      "year": "2016",
      "venue": "Emotions and Personality in Personalized Services"
    },
    {
      "citation_id": "18",
      "title": "Asynchronous and Eventbased Fusion Systems for Affect Recognition on Naturalistic Data in Comparison to Conventional Approaches",
      "authors": [
        "Florian Lingenfelser"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "The SSJ framework: Augmenting social interactions using mobile signal processing and live feedback",
      "authors": [
        "Ionut Damian",
        "Michael Dietz",
        "Elisabeth André"
      ],
      "year": "2018",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "21",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "22",
      "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "authors": [
        "M Sandler"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00474"
    },
    {
      "citation_id": "23",
      "title": "Deep learning in paralinguistic recognition tasks: Are hand-crafted features still relevant?",
      "authors": [
        "Johannes Wagner"
      ],
      "year": "2018",
      "venue": "Deep learning in paralinguistic recognition tasks: Are hand-crafted features still relevant?"
    },
    {
      "citation_id": "24",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "25",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "Camillo Lugaresi"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "26",
      "title": "Af-fectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "27",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "On-device Realtime Body Pose tracking",
      "authors": [
        "Valentin Bazarevsky"
      ],
      "year": "2020",
      "venue": "On-device Realtime Body Pose tracking",
      "arxiv": "arXiv:2006.10204[cs.CV"
    },
    {
      "citation_id": "29",
      "title": "Dominance or prestige: A review of the effects of power poses and other body postures",
      "authors": [
        "Robert Körner",
        "Astrid Schuetz"
      ],
      "year": "2020",
      "venue": "Social and Personality Psychology Compass",
      "doi": "10.1111/spc3.12559"
    },
    {
      "citation_id": "30",
      "title": "Relevance-based data masking: a model-agnostic transfer learning approach for facial expression recognition",
      "authors": [
        "Dominik Schiller"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "31",
      "title": "Multisensor-pipeline: a lightweight, flexible, and extensible framework for building multimodal-multisensor interfaces",
      "authors": [
        "Michael Barz"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "32",
      "title": "Platform for situated intelligence",
      "authors": [
        "Dan Bohus"
      ],
      "year": "2021",
      "venue": "Platform for situated intelligence",
      "arxiv": "arXiv:2103.15975"
    },
    {
      "citation_id": "33",
      "title": "XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond",
      "authors": [
        "Francesco Barbieri",
        "Luis Espinosa Anke",
        "Jose Camacho-Collados"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "34",
      "title": "Octopus Sensing: A Python library for human behavior studies",
      "authors": [
        "Nastaran Saffaryazdi",
        "Aidin Gharibnavaz",
        "Mark Billinghurst"
      ],
      "year": "2022",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "35",
      "title": "Probing Speech Emotion Recognition Transformers for Linguistic Knowledge",
      "authors": [
        "Andreas Triantafyllopoulos"
      ],
      "year": "2022",
      "venue": "Probing Speech Emotion Recognition Transformers for Linguistic Knowledge"
    },
    {
      "citation_id": "36",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}