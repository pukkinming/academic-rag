{
  "paper_id": "2501.18164v6",
  "title": "Faster Convergence Of Riemannian Stochastic Gradient Descent With Increasing Batch Size",
  "published": "2025-01-30T06:23:28Z",
  "authors": [
    "Kanata Oowada",
    "Hideaki Iiduka"
  ],
  "keywords": [
    "Batch Size",
    "Stochastic First-order Oracle Complexity",
    "Learning Rate",
    "Riemannian Optimization",
    "Riemannian Stochastic Gradient Descent"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We theoretically analyzed the convergence behavior of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster convergence than using a constant batch size, not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. The convergence rate improves from O(T -1 + C) with a constant batch size to O(T -1 ) with an increasing batch size, where T denotes the total number of iterations and C is a constant. Using principal component analysis and low-rank matrix completion, we investigated, both theoretically and numerically, how an increasing batch size affects computational time as quantified by stochastic first-order oracle (SFO) complexity. An increasing batch size was found to reduce the SFO complexity of RSGD. Furthermore, an increasing batch size was found to offer the advantages of both small and large constant batch sizes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Stochastic gradient descent (SGD)  (Robbins and Monro, 1951 ) is a basic algorithm, widely used in machine learning  (Liu et al., 2021; He et al., 2016; Krizhevsky et al., 2012) . Riemannian stochastic gradient descent (RSGD) was introduced in  Bonnabel (2013) . Riemannian optimization  (Absil et al., 2008; Boumal, 2023) , which addresses RSGD and its variants, has attracted attention due to its potential application in various tasks, including many machine learning tasks. For example, it has been used in principal components analysis (PCA)  (Breloy et al., 2021; Liu and Boumal, 2020; Roy et al., 2018) , low-rank matrix completion  (Vandereycken, 2013; Boumal and Absil, 2015; Kasai and Mishra, 2016) , convolutional neural networks  (Wang et al., 2020; Huang et al., 2017) , and graph neural networks  (Zhu et al., 2020; Chami et al., 2019; Liu et al., 2019) . It has also been used in applications of optimal transportation theory  (Lin et al., 2020b; Weber and Sra, 2023) . It is well established that the performance of Euclidean SGD strongly depends on both the batch size (BS) and learning rate (LR) settings  (Goyal et al., 2017; Smith et al., 2018; Zhang et al., 2019; Lin et al., 2020a) . In the context of Riemannian SGD,  Ji et al. (2024) ;  Bonnabel (2013) ;  Kasai et al. (2019 Kasai et al. ( , 2018) )  addressed the use of a constant BS, and  Sakai and Iiduka (2025) ;  Han and Gao (2022)  addressed the use of an adaptive BS, although the latter did not consider RSGD specifically. In parallel, various decaying LR strategies, including cosine annealing  (Loshchilov and Hutter, 2017)  and polynomial decay  (Chen et al., 2018) , have been devised, and their effectiveness has been demonstrated for the Euclidean case. Motivated by these Table  1 : Comparison of RSGD convergence analyses, where α ∈ ( 1 2 , 1), C 1 ∈ (0, 1), C 2 and C are constants, and (x t ) is a sequence generated by RSGD. The usual stochastic approximation LR (s.a. LR) is defined as\n\nT is the total number of iterations, and T w is the number of warmup iterations (see Section 3.3). ∥G T ∥ 2 := min t∈{0,\n\n. 'Decay' includes LR schedules of the form η t = O(t -1 2 ). More detailed explanations of the convergence criteria are provided in Appendix F.\n\nprevious studies, we developed a novel convergence analysis of RSGD, proved that the convergence rate is improved with an increasing BS, and showed that the stochastic first-order oracle (SFO) complexity is reduced with an increasing BS. Our numerical results support our theoretical results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Previous Results And Our Contributions",
      "text": "Constraint:  Bonnabel (2013) ;  Zhang and Sra (2016) ;  Durmus et al. (2021) ;  Sakai and Iiduka (2024)  considered Hadamard manifolds or their submanifolds.  Tripuraneni et al. (2018)  treated Riemannian manifolds that satisfy the condition required to ensure a unique geodesic connecting any two points, including Hadamard manifolds.  Sakai and Iiduka (2025)  addressed embedded submanifolds of Euclidean space. In contrast, we considered general Riemannian manifolds that encompass all of the above conditions. Learning rate: Bonnabel (2013) considered s.a. LRs, and  Zhang and Sra (2016) ;  Tripuraneni et al. (2018) ;  Sakai and Iiduka (2024)  considered diminishing LRs of the form η t = 1 t a .  Hosseini and Sra (2020) ;  Durmus et al. (2021)  considered constant LRs.  Sakai and Iiduka (2025)  used both constant and diminishing LRs. We considered constant, diminishing, cosine annealing, and polynomial decay LRs, as well as their warm-up versions. Batch size: Because Bonnabel (2013) considered expected risk minimization using expectation instead of sample mean-as is customary in empirical risk minimization-the concept of BS is not generally applicable in the expected-risk-minimization setting.  Zhang and Sra (2016) ;  Tripuraneni et al. (2018) ;  Hosseini and Sra (2020) ;  Durmus et al. (2021) ;  Sakai and Iiduka (2024)  considered a constant BS.  Sakai and Iiduka (2025)  and this study considered an increasing BS. Furthermore, we numerically and theoretically compared using a constant BS with using an increasing BS on the basis of SFO complexity. Our contributions are as follows: (i) we theoretically showed that using an increasing BS yields a more favorable SFO complexity of O(ϵ -2 ), whereas using a constant BS equal to the critical BS yields a rate of O(ϵ -4 ); (ii) we numerically observed that an increasing BS yields both a better optimal solution-in terms of attaining a smaller gradient norm-and a shorter computational time than either a small or a large constant BS (see Section 4.3 for more details). Objective function:  Bonnabel (2013)  considered three times continuously differentiable functions, with both the gradient and Hessian uniformly bounded.  Tripuraneni et al. (2018)  treated functions that are twice continuously differential, subject to additional conditions on the Hessian. Other studies considered only once continuously differentiable functions.  Zhang and Sra (2016)  addressed convex and smooth functions, while  Durmus et al. (2021)  considered strongly convex and smooth functions.  Hosseini and Sra (2020) ;  Sakai and Iiduka (2024, 2025)  treated more general classes of functions, specifically nonconvex functions with bounded gradients. We treated nonconvex functions without bounded gradients. Convergence rate: A comparison of the previous and current RSGD convergence analyses is presented in Table  1 ; here we outline the key results.  Tripuraneni et al. (2018)  obtained a convergence rate of O(T -1 2 ) (with a criterion other than the gradient norm) using an LR of η = O(T -1 2 ); however, this rate decays to zero as T → ∞, making it unsuitable for practical use.  Durmus et al. (2021)  attained exponential convergence under strong convexity and smoothness assumptions.  Sakai and Iiduka (2025)  attained a rate of O(T -1 ) for several adaptive methods, including RSGD. Our work differs from  Sakai and Iiduka (2025)  in four key aspects : (i) we provide both theoretical and numerical evidence-based on SFO complexity-that using an increasing BS leads to better performance than using a constant BS; (ii) in addition to a constant LR and a diminishing LR, we addressed cosine annealing, polynomial decay, and warm-up LRs; (iii)  Sakai and Iiduka (2024)  focused on embedded submanifolds of Euclidean space unlike us; (iv) we do not assume bounded gradient norms, thereby expanding the range of applicable scenarios.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Contributions:",
      "text": "• We developed a convergence analysis of RSGD that incorporates an increasing BS, a cosine annealing LR, and a polynomial decay LR, under assumptions more general than those used in prior work. Our analysis shows that using an increasing BS improves the convergence rate of RSGD from\n\n).\n\n• We numerically and theoretically demonstrated-using PCA and low-rank matrix completion (LRMC) tasks-the advantage of an increasing BS in reducing computational time, as quantified by SFO complexity. Specifically, we observed that an increasing BS yields a better optimal solution (compared with a small constant BS) in terms of attaining a smaller gradient norm and a shorter computation time (compared with a large constant BS). These findings are consistent with our theoretical analysis, which shows that an increasing BS reduces the SFO complexity for achieving ∥G T ∥ 2 ≤ ϵ 2 from O(ϵ -4 )-with a constant BS set equal to the critical BS-to O(ϵ -2 ) in the experimental setting.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminaries",
      "text": "Let M be a Riemannian manifold and T x M denote the tangent space at x ∈ M. On T x M, the inner product of M is denoted by ⟨•, •⟩ x and induces the norm ∥ • ∥ x . For a smooth map f : M → R, we can define gradient gradf as a unique vector field that satisfies ∀v ∈ T x M : Df (x)[v] = ⟨gradf (x), v⟩ x . A Riemannian manifold is generally not equipped with vector addition, whereas in Euclidean space iterative methods are updated by addition.\n\nIterative updates are instead performed using alternative operations-such as retraction.\n\nDefinition 1 (Retraction) Let 0 x denote the zero element of Assumption 2 (Retraction Smoothness) Let f : M → R be a smooth map. Then there exists L r > 0, such that\n\nIn the Euclidean space setting, L-smoothness implies a property similar to retraction smoothness. The property corresponding to L-smoothness in Euclidean space is defined for\n\n, where Γ is the parallel transport from T y M to T x M, and d(•, •) is the Riemannian distance. This condition is sufficient for Assumption 2 with R := Exp (see  Boumal (2023, Corollary 10.54) ). This case is frequently used (e.g.,  Zhang and Sra (2016) ;  Criscitiello and Boumal (2023) ;  Kim and Yang (2022) ;  Liu et al. (2017) ). Other sufficient conditions for Assumption 2 were identified by  Kasai et al. (2018, Lemma 3.5 ) and  Sakai and Iiduka (2025, Proposition 3.2) . Now, we consider the empirical risk minimization problem such that\n\nwhere each f j : M → R is smooth and lower bounded. Therefore, f is also smooth and lower bounded. This assumption is often made for both Euclidean space and Riemannian space. The lower boundedness of f is essential for analyses using optimization theory because unbounded f may not have optimizers. We denote an optimal value of f by f ⋆ and let N denote the size of the dataset. In many machine learning tasks, either the dimension of model parameters x or the size of dataset N is large. Hence, we use the following minibatch gradient to efficiently approximate the gradient of f : gradf\n\n, where B represents a minibatch with size b, and\n\nThe following assumption is reasonable given this situation.\n\nAssumption 3 (Bounded Variance Estimator) The stochastic gradient given by a distribution Π is an unbiased estimator of the full gradient and has bounded variance:\n\nFor example, if f is lower bounded, satisfies Assumption 2, and Π is taken to be the uniform distribution over {1, . . . , N }, then Assumption 3 holds. RSGD is defined by x t+1 = R xt (-η t gradf Bt (x t )), and is a generalization of Euclidean SGD. Because (ξ i,t ) i,t are i.i.d. samples,\n\nNote that the BS (b t ) t may vary at each iteration. The detailed definitions of E ξ∼Π , V ξ∼Π , and E can be found in Appendix D.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Convergence Analysis",
      "text": "We begin by presenting a lemma that will be used in the convergence analysis of RSGD. Lemma 4 plays a central role is the following convergence analyses; its proof is provided in Appendix D.1.\n\nLemma 4 (Underlying Analysis) Let (x t ) t be a sequence generated by RSGD and let η max > 0. Consider a positive-valued sequence\n\nLr ). Then, under Assumptions 2 and 3, we obtain",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Case (I): Constant Bs; Constant Or Decaying Lr",
      "text": "In this case, we consider a BS (b t ) t and an LR (η t ) t such that b t = b and η t+1 ≤ η t . In particular, we present the following examples with constant or decaying LRs.\n\nConstant LR:\n\nDiminishing LR:\n\nCosine Annealing LR:\n\nPolynomial Decay LR:\n\nwhere η max and η min are positive values satisfying 0 ≤ η min ≤ η max < 2 Lr . Note that η max (resp. η min ) becomes the maximum (resp. minimum) value of η t ; namely,\n\nTheorem 5 We consider LRs (1), (  2 ), (3), and (4) and a constant BS b t = b > 0 under the assumptions of Lemma 4. Then, we obtain Diminishing LR (2) : min\n\nOtherwise (1), (3), (4) : min\n\nwhere Q 1 , Q 2 , Q1 , and Q2 are constants that do not depend on T .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Case (Ii): Increasing Bs; Constant Or Decaying Lr",
      "text": "In this case, we consider a BS (b t ) t and an LR (η t ) t such that b t ≤ b t+1 and η t+1 ≤ η t . We use the same examples as in Case (i) [(1), • • • , (4)], again with constant or decaying LRs, and a BS that increases every K ∈ N steps, where N is the set of positive integers. We let T denote the total number of iterations and define M := ⌊ T K ⌋ to represent the number of times the BS is increased. The BS, which takes the form of γ m b 0 or (am + b 0 ) p every K steps, is an example of an increasing BS. We can formalize the resulting BSs: for every m ∈ {0, . . . , M -1}, t ∈ S m := [mK, (m + 1)K)\n\nPolynomial Growth BS:\n\nwhere γ, c > 1, a > 0, and N 0 := N ∪ {0}.\n\nTheorem 6 We consider BSs (5) and (6) together with LRs (1), (  2 ), (  3 ) and (4) under the assumptions stated in Lemma 4. Then, the following results hold for both constant and increasing BSs.\n\nDiminishing LR (2) : min\n\nOtherwise (1), (  3 ), (4) : min\n\nwhere Q 1 , Q 2 , Q1 , and Q2 are constants that do not depend on T .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Case (Iii): Increasing Bs; Warm-Up Decaying Lr",
      "text": "In this case, we consider a BS (b t ) t and an LR (η t ) t such that b t ≤ b t+1 and η t ≤ η t+1 for (t ≤ T w -1) and η t+1 ≤ η t for (t ≥ T w ). As examples of an increasing warm-up LR, we consider an exponential growth LR and a polynomial growth LR, both increasing every K ′ steps. We set K, as defined in Case (ii), to be lK ′ , where l ∈ N (thus K > K ′ ). Namely, we consider a setting in which the BS is increased every l times the LR is increased. To formulate examples of an increasing LR, we define M ′ := ⌊ T K ′ ⌋ and formalize the LR: for every m ∈ {0, . .\n\nPolynomial Growth LR:\n\nwhere s > 0 and q > 1. Furthermore, we choose γ, δ > 1, and l ∈ N such that δ 2l < γ holds. Additionally, we set l w ∈ N such that T ≥ T w := l w K ′ ≥ lK ′ . The examples of an increasing BS used in this case are an exponential growth BS (5) and a polynomial growth  BS (6) . As examples of a warm-up LR, we use LRs that are increased using the exponential growth LR (7) and the polynomial growth LR (8) corresponding respectively to the exponential and polynomial growth BSs for the first T w steps and then decreased using the constant LR (1), the diminishing LR (2), the cosine annealing LR (3), or the polynomial decay LR (4) for the remaining T -T w steps. Note that η max := η Tw-1 . A more detailed version of Theorem 7 is provided in Appendix D.4.\n\nTheorem 7 We consider BSs (5) and (6) together with warm-up LRs (7) and (8) with decay parts given by (1), (  2 ), (3), or (4) under the assumptions stated in Lemma 4. Then, the following results hold for both constant and increasing BSs: (I) Decay part [Diminishing (2)]:  3 ), (4)]:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Case (Iv): Constant Bs; Warm-Up Decaying Lr",
      "text": "We consider a BS (b t ) t and an LR (η t ) t such that b t = b and η t ≤ η t+1 for (t ≤ T w -1) and η t+1 ≤ η t for (t ≥ T w ). As examples of a warm-up LR, we use the exponential growth LR (7) and the polynomial growth LR (8) for the first T w steps and then the constant LR (1), the diminishing LR (2), the cosine annealing LR (3), or the polynomial decay LR (4) for the remaining T -T w steps. The other conditions are the same as those in Case (iii). A more detailed version of Theorem 8 is provided in Appendix D.5.\n\nTheorem 8 We consider a constant BS b t = b > 0 and warm-up LRs (7) and (8) with decay parts given by (1), (  2 ), (3), or (4) under the assumptions stated in Lemma 4. Then, we obtain the following results: (I) Decay part [Diminishing (2)]:",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Numerical Experiment",
      "text": "We experimentally evaluated the performance of RSGD for the two types of BSs and various types of LRs introduced in Section 3. The experiments were run on an iMac (Intel Core i5, 2017) running the macOS Ventura operating system (ver. 13.7.1). The algorithms were written in Python (3.12.7) using the NumPy (1.26.0) and Matplotlib (3.9.1) packages. The Python code is available at https://github.com/iiduka-researches/RSGD_acml2025. git. We set p = 2.0 in (4) and η min := 0. In Cases (i) and (ii), we used an initial LR η max selected from {0.5, 0.1, 0.05, 0.01, 0.005}. In Case (ii), we set K = 1000, γ = 3.0, and a = c = 2.0. All the plots of the objective function values presented in this section are provided in Appendices A.2 and E. Those for Cases (iii) and (iv) are provided in Appendix G.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Principal Component Analysis",
      "text": "We can formulate the PCA problem as an optimization problem on the Stiefel manifold  (Kasai et al., 2019) ; for a given dataset\n\nWe set r = 10 and used the COIL100  (Nene et al., 1996)  and  MNIST (LeCun et al., 1998)  datasets. The Columbia Object Image Library (COIL100) dataset contains 7200 color camera images of 100 objects (72 poses per object) taken from different angles. We resized the images to 32 × 32 pixels and transformed each one into a 1024 (= 32 2 ) dimensional vector. Hence, we set (N, n, r) = (7200, 1024, 10). The MNIST dataset contains 60, 000 28 × 28-pixel grayscale images of handwritten digits 0 to 9. We transformed each image into a 784  (= 28 2 ) dimensional vector and normalized each pixel to the range [0, 1]. Hence, we set (N, n, r) = (60000, 784, 10). Furthermore, we used a constant BS with b t := 2 10 , an exponential growth BS with an initial value b 0 := 3 5 , and a polynomial growth BS with an initial value b 0 := 30.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Low-Rank Matrix Completion",
      "text": "The LRMC problem involves completing an incomplete matrix Z = (z 1 , . . . , z N ) ∈ R n×N ; Ω denotes a set of indices for which we know the entries in Z. For a ∈ R n , we define P Ω i (a) such that the j-th element is a j if (i, j) ∈ Ω and 0 otherwise. For U ∈ R n×r , z ∈ R n , q j (U, z) := argmin a∈R r ∥P Ω j (U a -z)∥. We can now formulate the LRMC problem as the following optimization problem on the Grassmann manifold  (Boumal and Absil, 2015) :\n\n, where Gr(r, n) := St(r, n)/O(r). We set r = 10 and used the MovieLens-1M (Harper and  Konstan, 2016)  and Jester datasets  (Goldberg et al., 2001) . The MovieLens-1M dataset contains 1, 000, 209 ratings given by 6040 users on 3952 movies. Every rating lies in  [0, 5] . We normalized each rating to the range [0, 1]. Hence, we set (N, n, r) = (3952, 6040, 10). The Jester dataset contains ratings of 100 jokes from 24, 983 users. Every rating is bounded by the range  [-10, 10 ]. Hence, we set (N, n, r) = (24983, 100, 10). Furthermore, we used a constant BS b t := 2 8 , an exponential growth BS with an initial value of b 0 := 3 4 , and a polynomial growth BS with an initial value of b 0 := 14.\n\nThe performances in terms of the gradient norm of the objective function versus the number of iterations for LRs (1), (  2 ), (3), and (4) on the COIL100, MNIST, MovieLens-1M, and Jester datasets are plotted in Figures  1, 2, 3,  and 4 , respectively. Achieving a small gradient norm was better with an increasing BS than with a constant BS. Among the increasing BSs, the exponential growth BS outperformed the polynomial BS. Because the magnitude of increase in the exponential BS is O(γ m ) and that in the polynomial BS is O(m), these numerical results indicate that a larger rate of increase in BS leads to better performance.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Comparison Of Computational Time Between Constant Bs And Increasing Bs Versus Sfo Complexity",
      "text": "What are the differences between using a large constant BS, a small constant BS, and an increasing BS? We numerically investigated this question on the basis of SFO complexity, defined as the number of stochastic gradient evaluations executed over T iterations  (Agarwal and Bottou, 2015; Shallue et al., 2019; Sato and Iiduka, 2023) . For a constant BS b, SFO complexity is represented by bT . For other BSs, it can be computed numerically, serves as  in order from left to right. A cosine annealing LR was used except for COIL100, for which a constant LR was used. For 'BS increases= 0,' a constant BS b = b 0 was used. For 'BS increases= 3' and 'BS increases= 6,' the BS was increased 3 and 6 times, respectively, in accordance with the exponential growth BS.\n\na proxy for computational time. Figure  5  plots the gradient norm of the objective function versus SFO complexity for PCA and LRMC. Each curve corresponds to RSGD for 3000 steps under one of the following settings: constant BS, BS tripled every 1000 steps (three increases in total), or BS tripled every 500 steps (six increases in total). These settings follow the update formula for constant or exponential growth BS. Additional experimental details are provided in the caption of Figure  5 . As shown in the figure, an increasing BS combines the advantages of both small and large BSs-namely shorter computational time and convergence to a solution with a smaller final gradient norm.\n\nAs shown in Figure  5 , although both full and large constant BSs lead to optima achieving a small gradient norm, they require more SFO complexity. In contrast, while small constant BSs require less SFO complexity, they do not lead to optima achieving a small gradient norm. As shown in Table  2 , the computational time (CPU time) of an increasing BS is shorter than that of both large and full constant BSs. Between small and large constant BSs, there is a trade-off between convergence to a solution with a smaller final gradient norm and a shorter computational time. Our results show that an increasing BS balances this trade-off effectively.\n\nWhy is increasing BS better? Our theoretical analyses provide an answer to this question. From Theorem 5, the SFO complexity of a constant BS with the critical BS for achieving ∥G T ∥ 2 ≤ ϵ 2 is O(ϵ -4 ). From Theorem 6, the SFO complexity of an exponential growth BS for achieving\n\n) in our strict theoretical setting-with fixed K (number of steps for each BS) and dynamic M (number of BS increases)-the assumptions in this experiment differ. Specifically, K depended on the value assigned to M , and our analysis can be applied to this experimental setting. Under these conditions, the SFO complexity of an exponential growth BS for achieving ∥G T ∥ 2 ≤ ϵ 2 is O(ϵ -2 ), which is superior to O(ϵ -4 ) and consistent with our experimental results. This provides our theoretical justification: increasing BS achieves lower SFO complexity than a constant BS. Summarizing the above, we obtain the following theorem. The derivation is provided in Appendix A.   Table  3 : Let U EVD be the solution to PCA by eigenvalue decomposition, and let U RSGD be the solution to PCA by RSGD. Then, the differences between the representations of the subspaces spanned by the principal components of U EVD and U RSGD are given by l := ∥U EVD U ⊤ EVD -U RSGD U ⊤ RSGD ∥. For comparison with the results for COIL100, we introduce x such that l = 9.487 + x × 10 -3 , and for MNIST, we introduce y such that l = 0.4 + y × 10 -4 . M is the number of BS increments.\n\nTheorem 9 Let ϵ > 0, and consider an LR scheduler other than the diminishing LR one. Under the assumptions of Theorems 5 and 6, the SFO complexity for achieving ∥G T ∥ 2 < ϵ 2 is O(ϵ -4 ) for a constant BS and O(ϵ -2 ) for an increasing BS.\n\nRemark 10 Under the rigorous conditions of our theory, M → ∞ implies that T = M K + const. → ∞ with fixed K. In contrast, the conditions of our experiment do not permit an increase in BS to achieve T → ∞ because it would require K → ∞ for T to diverge with fixed M , which means that the number of steps for the initial BS is infinite. However, as is done in many experimental setups, including ours, T is predetermined as a finite value before conducting the experiment. This setting is equivalent to using the number of steps as the stopping criterion. Under this practical setting, the conditions of our experiment are meaningful because T < ∞ implies K < ∞ with a fixed M . On the other hand, by considering the conditions of our proofs, our theoretical analysis can also be applied to calculate SFO complexities, even within our experimental setting. From these reasons, our theoretical analysis can support the numerical results. To enhance understanding, the derivation of the SFO complexity for an increasing BS can be found in Appendix A.\n\nObjective Function Values: Generalization performance is an important issue, and the objective function value serves as a criterion for measuring it. As shown in part of Appendix A.2, although the exponential growth BS performed better than or equal to the constant BS with respect to the objective function values, the differences in the values were small. One possible reason for this is that the objective function may be flat around the optimal solution. To test this hypothesis, we conducted an experiment comparing the solution obtained by RSGD with that obtained by eigenvalue decomposition in PCA. When the data dimensionality is not large, PCA can obtain the exact solution by eigenvalue decomposition, and the difference from the approximate solution obtained by RSGD can be used as a measure of generalization. As shown in Table  3 , the difference is small, which indicates that the objective functions of PCA with COIL100 and MNIST are flat around the optimal (exact) solution. Conversely, by considering optimization problems in which the objective functions do not have this property, the superiority of an increasing BS in terms of generalization performance can be revealed more clearly. In fact, this has already been demonstrated in LRMC with MovieLens-1M, as shown at the far left of Figure  6  (the corresponding results for the other datasets are provided in Appendix A.2). For further verification of this, we formulated a new optimization problem on the sphere S n-1 : minimize\n\n|⟨x j , w⟩|, where {x j } N j=1 is a dataset uniformly sampled from S n-1 (see Appendix C for more details). We set (N, n) = (7200, 1024) and (60000, 1024) and used the cosine annealing LR with an initial LR η max = 0.01. The BS was configured in exactly the same way as that for the COIL100 and MNIST datasets in Section 4.3. As in the two central graphs of Figure  6 , an increasing BS achieved superior performance in terms of the objective function value for this proposed problem. Although the order of the differences (vertical axis) O(10 -2 ) -O(10 -1 ) may appear small, it is 10-100 times larger than that observed for PCA because the differences in the values shown in Table 3 are on the order of O(10 -4 ) -O(10 -3 ). These results further confirm the validity of our hypothesis for this problem. Given these results, we suggest that an increasing BS should also improve generalization performance. Turning to a different topic, the objective function of our proposed optimization problem has an unbounded gradient norm (see Appendix C), and the corresponding experiments numerically confirmed that our theoretical results are applicable even in this case. The graphs of the gradient norm for our proposed problem are provided in Appendix C.1.\n\nGuidelines for setting γ, b 0 and M in the exponential growth BS : From Theorem 6, ∥G T ∥ 2 = O(1\n\n) hold. These theoretical results suggest a trade-off: for smaller gradient norm, γ, b 0 , and M should be assigned larger values while for smaller SFO complexity, γ, b 0 , and M should be assigned smaller values. The far-right plot in Figure  6  illustrates the trade-off between the gradient norm and SFO complexity achieved experimentally on MNIST (PCA) using the cosine annealing LR with an initial LR η max = 0.01, where (b 0 , M ) = (247, 6), while varying γ among {2.0, 2.5, 3.0, 3.5, 4.0}. For b 0 , this trade-off can be observed by comparing the three entries under 'BS increases= 3' or the two entries under 'BS increases= 6' in Figure  5 . For M , which represents the value 'BS increases', this trade-off can be observed by comparing the purple entry with the pink entry in Figure  5 . From these results, we were able to show, both theoretically and experimentally, that for each hyperparameter the trade-off between the gradient norm and SFO complexity holds. Figure  5  shows that an increasing BS with small b 0 and large M (= 6) leads to multiple optima characterized by a small gradient norm and relatively low SFO complexity, indicating that well-balanced configurations were achieved by taking into account the trade-offs inherent in b 0 and M individually. The relevant derivation is provided in Appendix B.",
      "page_start": 9,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "Our theoretical analysis, conducted with several learning rate schedules, including cosine annealing and polynomial decay, demonstrated that using an increasing batch size rather than a constant batch size improves the convergence rate of Riemannian stochastic gradient descent. This result is supported by our experimental results. Furthermore, an increasing batch size yields optima characterized by a smaller gradient norm within a shorter computational time owing to reductions in the stochastic first-order oracle complexity. These findings indicate that an increasing batch size combines the advantages of both small and large constant batch size. Due to the nature of the experimental tasks, we were unable to directly investigate the effect on generalization performance. However, evaluation via the objective function value suggests that an increasing batch size also enhances generalization performance. We believe our results, which clarify one aspect of the effectiveness of an increasing batch size for generalization performance, provide valuable insight into this topic.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Appendix A. Calculation Of Sfo Complexity",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.1. Sfo Complexity With Constant Bs And Increasing Bs",
      "text": "We compute the SFO complexity required to achieve ∥G T ∥ 2 ≤ ϵ 2 (ϵ > 0), as described in Section 4.3.\n\n[Constant BS] From Theorem 5, the number of iterations T required to achieve\n\nBecause the SFO complexity of constant BS can be represented by SFO const (b) = bT , the SFO const (b) required to achieve ∥G T ∥ 2 ≤ ϵ 2 is given by\n\nwe have the critical BS of constant BS:\n\n).\n\n[Increasing BS] From Theorem 6, the number of iterations T required to achieve ∥G T ∥ 2 ≤ ϵ 2 is given by\n\nWe assume a setting in which T = M K, meaning that by the T -th iteration, the BS has been increased M times, and RSGD has been updated for K steps at each BS. If the exact value of T takes the form T = M K +k (1 ≤ k ≤ K -1), we can set T to M (K +1) to ensure that ∥G T ∥ 2 ≤ ϵ 2 still holds. Similarly, if T takes the form T = M K-k (1 ≤ k ≤ K -1), we can set T to M K to satisfy the same condition. Hence, our assumption that T = M K is reasonable. SFO complexity for increasing BS (i.e., exponential growth BS) can be represented as\n\nUnder our theoretical assumption that K is fixed and M is dynamic, and with M = T K , the SFO complexity of increasing BS for achieving\n\nTo calculate SFO complexity under our experimental setting, we must reconsider the proof D.3 because it relies on letting M → ∞, which is not permitted in our experimental setting. This issue can, however, be resolved by the following discussion. The key is to replace the evaluation at the penultimate inequality of (9) with\n\n.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Thus,",
      "text": "implies the same result with Theorem 6:\n\nBecause Q2 contains K, as shown by the formal statement of Theorem 6 (with a constant LR and an exponential growth BS), we rearrange Q2 for calculating SFO complexity under our experimental setting as\n\nwhere Q3 is defined through this substitution. Then, we obtain\n\nTherefore, under our experimental setting-where K is determined by fixed M -and with K = T M , the SFO complexity of increasing BS for achieving\n\nThe calculations in this section are for the case of a constant LR, but it is immediately clear from Theorems 5 and 6 that exactly the same results hold for a cosine annealing LR, which was also used in our experiments.\n\nA  For the COIL100 (PCA) and MovieLens-1M (LRMC), performance was better with an increasing BS than with a constant BS. Although the differences in the objective function values for the MNIST (PCA) and Jester (LRMC) are small (one possible reason for this is that the objective function may be flat around the optimal solution), the performance with an increasing BS was equal to or better than that with a constant BS. A more detailed discussion of this hypothesis is presented in Section 4.3.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Appendix B. Trade-Offs For Each Hyperparameter Of Exponential Growth Bs Scheduler",
      "text": "We calculate the trade-offs for each hyperparameter of the exponential growth BS.\n\n[γ]: From Theorem 6,\n\nand SFO incr ϵ = O(γ M ) hold, and γ M is clearly monotonically increasing with respect to γ. Let f (γ\n\n(γ-1) 2 < 0, f is monotonically decreasing. Thus, for a smaller gradient norm, γ should be set to a larger value, while for a smaller SFO complexity, γ should be set to a smaller value.\n\n, g is monotonically decreasing for 1 < x ≤ √ 3, and monotonically increasing for x ≥ √ 3. Now, because x > 1 and b 0 ∈ N, it suffices to consider only the case x ≥ 2, in which g is monotonically increasing. Summarizing the above and considering b 0 ∈ dom g, we find that, for a smaller gradient norm, b 0 should be set to a larger value, while for a smaller SFO complexity, b 0 should be set to a smaller value.\n\n[M ]:\n\nx 2\n\n, h is monotonically increasing for x ≥ 1 ln γ when γ ≥ e. Thus, when we set γ ≥ e (γ was set to 3 in our experiments except for the far right plot in Figure  6 ), we find that, for a smaller gradient norm, M should be set to a larger value, while for a smaller SFO complexity, M should be set to a smaller value. Finally, we provide a remark on the setting of b 0 in our experiments. Although the initial BS (b 0 ) appears different for each dataset, the values were similarly determined using the same method to enable a comparison of the effectiveness between a small constant BS, a large constant BS, and an increasing BS. The N was set to the total number of data points in each dataset. For a large initial BS, it was set to 3 k , where k is the largest integer such that 3 k < N . For a medium-size initial BS, it was set to 3 k-3 . For the smallest initial BS, it was set to 3 k-5 . For example, because N = 7200 for the COIL100 dataset, the large, the medium-size small, and the smallest initial BSs were set to 3 8 = 6561, 3 5 = 243, 3 3 = 27, respectively.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Appendix C. Our Proposed Optimization Problem",
      "text": "Our proposed optimization problem:\n\n{x j } N j=1 is a dataset uniformly sampled from S n-1 . N is a total number of data points in the dataset, and n is a dimension of each one. The objective function defined above has unbounded gradient norms, which allows us to experimentally verify that our theoretical results are indeed applicable to functions with unbounded gradient norms. In our experiment, R x (d) := x+d ∥x+d∥ was used as a retraction for S n-1 . Proposition 11 The gradient norm of the objective function for this problem f is not bounded.\n\nSign(⟨x j , w⟩)|⟨x j , w⟩| -1 2 x j is obtained by\n\nSign(⟨x j , w⟩)|⟨x j , w⟩| -1 2 x i j ,\n\nSign(⟨x j , w⟩)|⟨x j , w⟩| -1 2 (I -ww ⊤ )x j .\n\nFix one data x i such that x i ̸ = 0. Let u ∈ S n-1 be a vector obtained by orthonormalizing x i via the Gram-Schmidt process. We consider a sequence (w t ) t∈N such that w t :=\n\nBy simple calculations,\n\nhold. Now, we define a set Λ satisfying {x j } j∈Λ ⊂ {x j } N j=1 such that ⟨x j , u⟩ = 0. We find ⟨x j , w t ⟩ = ⟨x j ,x i ⟩ t∥x i ∥ ( ∀ j ∈ Λ), and Λ ̸ = ∅ because of i ∈ Λ. Note that, for all j ̸ ∈ Λ, ⟨x j , u⟩ ̸ = 0 holds, which yields ⟨x j , w t ⟩ ̸ = 0. From these observations, if ⟨x j , x i ⟩ ̸ = 0 ( ∀ j ∈ Λ),\n\nholds. These complete the proof.  As shown in the plot of the norm of objective function gradient versus SFO complexity 8 for our proposed problem, performance was better with an increasing BS than with either a small or large constant BS. This result can be explained by Theorem 9, which shows that an increasing BS reduces SFO complexity compared with a constant BS.",
      "page_start": 21,
      "page_end": 23
    },
    {
      "section_name": "Appendix D. Proofs Of Our Lemma And Theorems",
      "text": "Recall that Π is a {1, • • • , N }-valued probability distribution. Thus, we consider the probability space ({1,\n\nFor a random variable ξ distributed as Π and a function g :\n\nHence, the variance is defined as\n\n∈ R be a function, and let x t be a point at the t-th iteration generated by the updating rule of RSGD. We can introduce a natural extension of the expectation notation to a multivariate random variable:\n\nThe following result serves as a preliminary for Lemma 4.\n\nLemma 12 (Descent Lemma) Let (x t ) t be a sequence generated by RSGD and (η t ) t be a positive-valued sequence. Then, under Assumptions 2 and 3, we obtain\n\nProof Under Assumption 2, we start with\n\nand\n\nTaking the total expectation of the above equations, we have",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "D.1. Proof Of Lemma 4",
      "text": "On the basis of Lemma 12, Lemma 4 is proven as follows.\n\nProof Taking η max < 2 Lr into consideration, we start with\n\nBy taking the summation on both sides of the inequality for Lemma 12 and evaluating it using the above inequality, we obtain",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "D.2. Proof Of Theorem 5",
      "text": "Proof We set b t = b in Lemma 4.\n\n[Constant LR (1)] From Lemma 4, we have\n\n[Diminishing LR (2)] From Lemma 4, we have\n\n[Cosine Annealing LR (3)] From Lemma 4, we have\n\n[Polynomial Decay LR (4)] We start by recalling the Riemann integral of g(t) := (1 -t) p ≥ 0 (0 ≤ t ≤ 1). U (P T ) :=\n\nIn fact, the integral value exists. Thus, we obtain\n\nSimilarly, if we define L(P T ) as T\n\nHence,\n\nholds. By replacing g with (1 + t) 2p (0 ≤ t ≤ 1) and using the same logic, we obtain\n\nTherefore, considering Lemma 4,",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "D.3. Proof Of Theorem 6",
      "text": "Proof The evaluations in all the following cases are based on Lemma 4 and use estimations of T -1 t=0 η t in the proof of Theorem 5. [Exponential Growth BS (5) and Constant LR (1)] From the fact that sums of positive term series are the supremum of their finite sums, we have\n\nwhich implies\n\n[Exponential Growth BS (5) and Diminishing LR (2)] Using (9), we have\n\n[Exponential Growth BS (5) and Cosine Annealing LR (3)] From (  9 ) and | cos x| ≤ 1, we have\n\n[Exponential Growth BS (5) and Polynomial Decay LR (4)] From (9) and t ≤ T , we have\n\n[Polynomial Growth BS (6) and Constant LR (1)] We set a := a ∧ b 0 . Considering c > 1, we have\n\n[Polynomial Growth BS (6) and Diminishing LR (2)] From (10), we have\n\n[Polynomial Growth BS (6) and Cosine Annealing LR (3)] From (10) and the previous analysis for Cosine Annealing LR (3), we have\n\n[Polynomial Growth BS (6) and Polynomial Decay LR (4)] From (10) and the previous analysis for Polynomial Decay LR (4), we have",
      "page_start": 28,
      "page_end": 30
    },
    {
      "section_name": "D.4. Proof Of Theorem 7",
      "text": "Theorem 13 (Detailed Version of Theorem 7) We consider BSs (5) and (6) and warm-up LRs (7) and (8) with decay parts given by (1), (  2 ), (3), or (4) under the assumptions of Lemma 4. Then, the following holds for both constant and increasing BSs.\n\n• Decay part: Diminishing (2)\n\n• Decay part: Otherwise (1), (  3 ), (  4 )\n\nwhere Q 1 , Q 2 , Q1 , and Q2 are constants that do not depend on T .\n\nProof The evaluations in all these cases are based on Lemma 4. We start with an exponential growth BS and a warm-up LR. From l, l w ∈ N, and l w ≥ l, there exist α, β ∈ N ∪ {0} such that l w = αl + β. Note that we define summations from 0 to -1 as 0. Furthermore, from ∀u, v > 0 : 0 ≤ u 2 + v 2 + uv holds.\n\n[LR Decay Part: Constant\n\n[LR Decay Part: Diminishing (2)] Similarly, we have\n\n[",
      "page_start": 31,
      "page_end": 31
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Norm of the gradient of the objective function versus the number of iterations for",
      "page": 8
    },
    {
      "caption": "Figure 2: Norm of the gradient of the objective function versus the number of iterations for",
      "page": 8
    },
    {
      "caption": "Figure 3: Norm of the gradient of the objective function versus the number of iterations for",
      "page": 8
    },
    {
      "caption": "Figure 4: Norm of the gradient of the objective function versus the number of iterations for",
      "page": 9
    },
    {
      "caption": "Figure 5: Norm of objective function gradient versus SFO complexity. Datasets used were",
      "page": 10
    },
    {
      "caption": "Figure 5: plots the gradient norm of the objective function",
      "page": 10
    },
    {
      "caption": "Figure 5: As shown in the figure, an increasing BS",
      "page": 10
    },
    {
      "caption": "Figure 5: , although both full and large constant BSs lead to optima achieving",
      "page": 10
    },
    {
      "caption": "Figure 6: The plot on the far left depicts the objective function value (loss) for MNIST",
      "page": 11
    },
    {
      "caption": "Figure 5: The plot in the middle left (resp.",
      "page": 11
    },
    {
      "caption": "Figure 6: (the corresponding results for the other datasets are provided in Appendix A.2).",
      "page": 12
    },
    {
      "caption": "Figure 6: , an increasing BS achieved superior",
      "page": 12
    },
    {
      "caption": "Figure 6: illustrates the trade-off",
      "page": 12
    },
    {
      "caption": "Figure 5: For M, which represents the value ‘BS increases’, this trade-off can be",
      "page": 12
    },
    {
      "caption": "Figure 5: From these results,",
      "page": 12
    },
    {
      "caption": "Figure 5: shows that",
      "page": 12
    },
    {
      "caption": "Figure 7: Objective function value (loss) versus SFO complexity on COIL100 (PCA),",
      "page": 19
    },
    {
      "caption": "Figure 6: ), we find that, for a smaller gradient norm, M should be set to a",
      "page": 20
    },
    {
      "caption": "Figure 8: Norm of objective function gradient versus SFO complexity. Plot on left (resp.",
      "page": 23
    },
    {
      "caption": "Figure 9: Objective function value (loss) versus number of iterations for LRs (1), (2), (3),",
      "page": 35
    },
    {
      "caption": "Figure 10: Objective function value (loss) versus number of iterations for LRs (1), (2), (3),",
      "page": 35
    },
    {
      "caption": "Figure 11: Objective function value (loss) versus number of iterations for LRs (1), (2), (3),",
      "page": 36
    },
    {
      "caption": "Figure 12: Objective function value (loss) versus number of iterations for LRs (1), (2), (3),",
      "page": 36
    },
    {
      "caption": "Figure 13: Norm of the gradient of the objective function versus number of iterations for",
      "page": 37
    },
    {
      "caption": "Figure 14: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 38
    },
    {
      "caption": "Figure 15: Norm of the gradient of the objective function versus number of iterations for",
      "page": 38
    },
    {
      "caption": "Figure 16: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 38
    },
    {
      "caption": "Figure 17: Norm of the gradient of the objective function versus number of iterations for",
      "page": 39
    },
    {
      "caption": "Figure 18: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 39
    },
    {
      "caption": "Figure 19: Norm of the gradient of the objective function versus number of iterations for",
      "page": 39
    },
    {
      "caption": "Figure 20: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 40
    },
    {
      "caption": "Figure 21: Norm of the gradient of the objective function versus number of iterations for",
      "page": 40
    },
    {
      "caption": "Figure 22: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 40
    },
    {
      "caption": "Figure 23: Norm of the gradient of the objective function versus number of iterations for",
      "page": 41
    },
    {
      "caption": "Figure 24: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 41
    },
    {
      "caption": "Figure 25: Norm of the gradient of the objective function versus number of iterations for",
      "page": 41
    },
    {
      "caption": "Figure 26: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 42
    },
    {
      "caption": "Figure 27: Norm of the gradient of the objective function versus number of iterations for",
      "page": 42
    },
    {
      "caption": "Figure 28: Objective function value (loss) versus number of iterations for warm-up LRs",
      "page": 42
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ConstantBS/PolyLR\nExpoGrowthBS/PolyLR": "PolyGrowthBS/PolyLR"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: , the computational time (CPU time) of an increasing BS is shorter",
      "data": [
        {
          "b_0=27, BS_increases=0\nb_0=243, BS_increases=0\nb_0=6561, BS_increases=0\nb_0=7200, BS_increases=0\nb_0=27, BS_increases=6\nb_0=30, BS_increases=6": "b_0=27, BS_increases=3\nb_0=729, BS_increases=3\nb_0=800, BS_increases=3"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: , the computational time (CPU time) of an increasing BS is shorter",
      "data": [
        {
          "b_0=9, BS_increases=0\nb_0=81, BS_increases=0": "b_0=2187, BS_increases=0\nb_0=3952, BS_increases=0\nb_0=9, BS_increases=6\nb_0=17, BS_increases=6\nb_0=9, BS_increases=3\nb_0=243, BS_increases=3\nb_0=440, BS_increases=3"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: , the computational time (CPU time) of an increasing BS is shorter",
      "data": [
        {
          "b_0=243, BS_increases=0\nb_0=2187, BS_increases=0\nb_0=59049, BS_increases=0\nb_0=60000, BS_increases=0\nb_0=243, BS_increases=6\nb_0=247, BS_increases=6\nb_0=243, BS_increases=3\nb_0=2223, BS_increases=3": "b_0=6561, BS_increases=3"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "b_0=9, BS_increases=0\nb_0=81, BS_increases=0\nb_0=2187, BS_increases=0\nb_0=3952, BS_increases=0\nb_0=9, BS_increases=6\nb_0=17, BS_increases=6\nb_0=9, BS_increases=3": "b_0=243, BS_increases=3\nb_0=440, BS_increases=3"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Optimization Algorithms on Matrix Manifolds",
      "authors": [
        "P.-A Absil",
        "R Mahony",
        "R Sepulchre"
      ],
      "year": "2008",
      "venue": "Optimization Algorithms on Matrix Manifolds"
    },
    {
      "citation_id": "2",
      "title": "A lower bound for the optimization of finite sums",
      "authors": [
        "Alekh Agarwal",
        "Leon Bottou"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Stochastic gradient descent on Riemannian manifolds",
      "authors": [
        "Silvère Bonnabel"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Automatic Control"
    },
    {
      "citation_id": "4",
      "title": "An introduction to optimization on smooth manifolds",
      "authors": [
        "Nicolas Boumal"
      ],
      "year": "2023",
      "venue": "An introduction to optimization on smooth manifolds"
    },
    {
      "citation_id": "5",
      "title": "Low-rank matrix completion via preconditioned optimization on the Grassmann manifold",
      "authors": [
        "Nicolas Boumal",
        "P.-A Absil"
      ],
      "year": "2015",
      "venue": "Linear Algebla and its Applications"
    },
    {
      "citation_id": "6",
      "title": "Majorizationminimization on the Stiefel manifold with application to robust sparse PCA",
      "authors": [
        "Arnaud Breloy",
        "Sandeep Kumar",
        "Ying Sun",
        "Daniel Palomar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Hyperbolic graph convolutional neural networks",
      "authors": [
        "Ines Chami",
        "Zhitao Ying",
        "Christopher Ré",
        "Jure Leskovec"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs",
      "authors": [
        "Liang-Chieh Chen",
        "George Papandreou",
        "Iasonas Kokkinos",
        "Kevin Murphy",
        "Alan Yuille"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Curvature and complexity: Better lower bounds for geodesically convex optimization",
      "authors": [
        "Christopher Criscitiello",
        "Nicolas Boumal"
      ],
      "year": "2023",
      "venue": "The Thirty Sixth Annual Conference on Learning Theory"
    },
    {
      "citation_id": "10",
      "title": "On Riemannian stochastic approximation schemes with fixed step-size",
      "authors": [
        "Alain Durmus",
        "Pablo Jiménez",
        "Eric Moulines",
        "Salem Said"
      ],
      "year": "2021",
      "venue": "Proceedings of The 24th International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "11",
      "title": "Eigentaste: A constant time collaborative filtering algorithm",
      "authors": [
        "Y Kenneth",
        "Theresa Goldberg",
        "Dhruv Roeder",
        "Chris Gupta",
        "Perkins"
      ],
      "year": "2001",
      "venue": "Information Retrieval"
    },
    {
      "citation_id": "12",
      "title": "Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour",
      "authors": [
        "Priya Goyal",
        "Piotr Dollár",
        "Ross Girshick",
        "Pieter Noordhuis",
        "Lukasz Wesolowski",
        "Aapo Kyrola",
        "Andrew Tulloch"
      ],
      "year": "2017",
      "venue": "Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour",
      "arxiv": "arXiv:1706.02677"
    },
    {
      "citation_id": "13",
      "title": "Improved variance reduction methods for Riemannian nonconvex optimization",
      "authors": [
        "Andi Han",
        "Junbin Gao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "14",
      "title": "The movielens datasets: History and context",
      "authors": [
        "Maxwell Harper",
        "Joseph Konstan"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "An alternative to EM for Gaussian mixture models: Batch and stochastic Riemannian optimization",
      "authors": [
        "Reshad Hosseini",
        "Suvrit Sra"
      ],
      "year": "2020",
      "venue": "Mathematical Programming"
    },
    {
      "citation_id": "17",
      "title": "Deep learning on Lie groups for skeleton-based action recognition",
      "authors": [
        "Zhiwu Huang",
        "Chengde Wan",
        "Thomas Probst",
        "Luc Van Gool"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Adaptive Riemannian stochastic gradient descent and reparameterization for Gaussian mixture model fitting",
      "authors": [
        "Chunlin Ji",
        "Yuhao Fu",
        "Ping He"
      ],
      "year": "2024",
      "venue": "Proceedings of the 15th Asian Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "Low-rank tensor completion: A Riemannian manifold preconditioning approach",
      "authors": [
        "Hiroyuki Kasai",
        "Bamdev Mishra"
      ],
      "year": "2016",
      "venue": "Proceedings of the 33nd International Conference on Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "Riemannian stochastic recursive gradient algorithm",
      "authors": [
        "Hiroyuki Kasai",
        "Hiroyuki Sato",
        "Bamdev Mishra"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Riemannian adaptive stochastic gradient algorithms on matrix manifolds",
      "authors": [
        "Hiroyuki Kasai",
        "Pratik Jawanpuria",
        "Bamdev Mishra"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning"
    },
    {
      "citation_id": "22",
      "title": "Nesterov acceleration for Riemannian optimization",
      "authors": [
        "Jungbin Kim",
        "Insoon Yang"
      ],
      "year": "2022",
      "venue": "Nesterov acceleration for Riemannian optimization",
      "arxiv": "arXiv:2202.02036"
    },
    {
      "citation_id": "23",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "The MNIST database of handwritten digits",
      "authors": [
        "Yann Lecun",
        "Corinna Cortes",
        "C Burges"
      ],
      "year": "1998",
      "venue": "The MNIST database of handwritten digits"
    },
    {
      "citation_id": "25",
      "title": "Don't use large minibatches, use local SGD",
      "authors": [
        "Tao Lin",
        "Sebastian Stich",
        "Kshitij Kumar",
        "Martin Patel",
        "Jaggi"
      ],
      "year": "2020",
      "venue": "8th International Conference on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Projection robust Wasserstein distance and Riemannian optimization",
      "authors": [
        "Tianyi Lin",
        "Chenyou Fan",
        "Nhat Ho",
        "Marco Cuturi",
        "Michael Jordan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Simple algorithms for optimization on Riemannian manifolds with constraints",
      "authors": [
        "Changshuo Liu",
        "Nicolas Boumal"
      ],
      "year": "2020",
      "venue": "Applied Mathematics & Optimization"
    },
    {
      "citation_id": "28",
      "title": "Hyperbolic graph neural networks",
      "authors": [
        "Qi Liu",
        "Maximilian Nickel",
        "Douwe Kiela"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Accelerated first-order methods for geodesically convex optimization on Riemannian manifolds",
      "authors": [
        "Yuanyuan Liu",
        "Fanhua Shang",
        "James Cheng",
        "Hong Cheng",
        "Licheng Jiao"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "30",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "31",
      "title": "SGDR: Stochastic gradient descent with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "32",
      "title": "Columbia object image library (COIL-100)",
      "authors": [
        "Shree Sameer A Nene",
        "Hiroshi Nayar",
        "Murase"
      ],
      "year": "1996",
      "venue": "Columbia object image library (COIL-100)"
    },
    {
      "citation_id": "33",
      "title": "A stochastic approximation method",
      "authors": [
        "Herbert Robbins",
        "Sutton Monro"
      ],
      "year": "1951",
      "venue": "The Annals of Mathematical Statistics"
    },
    {
      "citation_id": "34",
      "title": "Geometry aware constrained optimization techniques for deep learning",
      "authors": [
        "Soumava Kumar",
        "Zakaria Mhammedi",
        "Mehrtash Harandi"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "Convergence of Riemannian stochastic gradient descent on hadamard manifold",
      "authors": [
        "Hiroyuki Sakai",
        "Hideaki Iiduka"
      ],
      "year": "2024",
      "venue": "Pacific Journal of Optimization"
    },
    {
      "citation_id": "36",
      "title": "Naoki Sato and Hideaki Iiduka. Existence and estimation of critical batch size for training generative adversarial networks with two time-scale update rule",
      "authors": [
        "Hiroyuki Sakai",
        "Hideaki Iiduka"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Measuring the effects of data parallelism on neural network training",
      "authors": [
        "Christopher Shallue",
        "Jaehoon Lee",
        "Joseph Antognini",
        "Jascha Sohl-Dickstein",
        "Roy Frostig",
        "George Dahl"
      ],
      "year": "2019",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "38",
      "title": "Don't decay the learning rate, increase the batch size",
      "authors": [
        "L Samuel",
        "Pieter-Jan Smith",
        "Chris Kindermans",
        "Ying",
        "V Quoc",
        "Le"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations"
    },
    {
      "citation_id": "39",
      "title": "Averaging stochastic gradient descent on Riemannian manifolds",
      "authors": [
        "Nilesh Tripuraneni",
        "Nicolas Flammarion",
        "Francis Bach",
        "Michael Jordan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 31st Conference On Learning Theory"
    },
    {
      "citation_id": "40",
      "title": "Low-rank matrix completion by Riemannian optimization",
      "authors": [
        "Bart Vandereycken"
      ],
      "year": "2013",
      "venue": "SIAM Journal on Optimization"
    },
    {
      "citation_id": "41",
      "title": "Orthogonal convolutional neural networks",
      "authors": [
        "Jiayun Wang",
        "Yubei Chen",
        "Rudrasis Chakraborty",
        "Stella Yu"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "42",
      "title": "Riemannian optimization via Frank-Wolfe methods",
      "authors": [
        "Melanie Weber",
        "Suvrit Sra"
      ],
      "year": "2023",
      "venue": "Mathematical Programming"
    },
    {
      "citation_id": "43",
      "title": "Which algorithmic choices matter at which batch sizes? Insights from a noisy quadratic model",
      "authors": [
        "Guodong Zhang",
        "Lala Li",
        "Zachary Nado",
        "James Martens",
        "Sushant Sachdeva",
        "George Dahl",
        "Christopher Shallue",
        "Roger Grosse"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "First-order methods for geodesically convex optimization",
      "authors": [
        "Hongyi Zhang",
        "Suvrit Sra"
      ],
      "year": "2016",
      "venue": "29th Annual Conference on Learning Theory"
    },
    {
      "citation_id": "45",
      "title": "Graph geometry interaction learning",
      "authors": [
        "Shichao Zhu",
        "Shirui Pan",
        "Chuan Zhou",
        "Jia Wu",
        "Yanan Cao",
        "Bin Wang"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}