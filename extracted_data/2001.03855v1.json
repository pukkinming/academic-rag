{
  "paper_id": "2001.03855v1",
  "title": "Hyperparameters Optimization For Deep Learning Based Emotion Prediction For Human Robot Interaction",
  "published": "2020-01-12T05:25:02Z",
  "authors": [
    "Shruti Jaiswal",
    "Gora Chand Nandi"
  ],
  "keywords": [
    "Emotion Classification",
    "Optimized Deel Learning",
    "Convolutional Neural Network",
    "Inception Module",
    "Human Robot Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "To enable humanoid robots to share our social space we need to develop technology for easy interaction with the robots using multiple modes such as speech, gestures and share our emotions with them. We have targeted this research towards addressing the core issue of emotion recognition problem which would require less computation resources and much lesser number of network hyperparameters which will be more adaptive to be computed on low resourced social robots for real time communication. More specifically, here we have proposed an Inception module based Convolutional Neural Network Architecture which has achieved improved accuracy of upto 6% improvement over the existing network architecture for emotion classification when combinedly tested over multiple datasets when tried over humanoid robots in real -time. Our proposed model is reducing the trainable Hyperparameters to an extent of 94% as compared to vanilla CNN model which clearly indicates that it can be used in real time based application such as human robot interaction. Rigorous experiments have been performed to validate our methodology which is sufficiently robust and could achieve high level of accuracy. Finally, the model is implemented in a humanoid robot, NAO in real time and robustness of the model is evaluated.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Obots have now entered human life. An era has reached where human beings have got a helping hand such that human just needs to give direction to robots and get the work done. In such a scenario, it will be an add-on for us if robots also understand our emotion. Human face is a complex structure which can be manipulative at times. Predicting facial expression by looking at the image can be complex even for human. Facial expressions depends on many factors, like, the context in which the person is talking, the sentiment of the statement person is speaking or listening to, and the entire situation of the scenario. Hence all these factors makes predicting emotion just by looking at the image could be more difficult to achieve accuracy. When a robot is capable to predict human emotion, it can enter in a person's life in a more comfortable manner that means it can come up with collaborative behavior with human. It would be easier for a personto connect to a robot. Then robots can be customized as per person's need which can be useful in conducting therapies for child, taking care of elderly peopleor even living with person as a daily routine. When it comes to human facial expression by looking at an image, human performance itself has achieved 65± 5 % accuracy  [1]  that was researched over Fer2013  [2]  dataset. This was computed by manually predicting expression from every image of the dataset. Problem of making the robot predict human emotion, in order to stay with them in a collaborative manner, many ways have been achieved to solve it. Initially, this complex problem was solved in an early stage in year 2000-2004 when researchers in  [3]  manually extracted out features of the emotion dataset images so that when fed to neural network, it can predict the emotion. Much research has been done on feature extraction since then like extracting upper face action unit, lower face action units, including eyebrows, lips, check structure, and chin lifted, eyes shape and many others. Then these these features are used to predict human emotion. Even neurophysical data is also used to predict human emotion as in  [4] . In addition to that, deep learning has replaced the process of manually extracting features or manually extracting action units; one such work is done in  [3] . Facial action recognition is done similar grounds using static face images in  [5] . In this paper, we have proposed a model to predict human emotion in a more optimized and accurate manner.\n\nDetecting human emotion is still a crucial task to solve and also complex to make it real time implementable. To take up this challenge, in this paper, we have proposed a model to make emotion detection more accurate over many variations of the dataset and also more real times implementable with less time complexity over existing models and hence have implemented a humanoid robot NAO to be able to predict human emotion while it is talking to the person. Main contributions of this paper are:\n\n---------------- Further, the paper is structured with analysis of previous research for this application in the following section. After that in section 3 problem formulation is explained. Section 4 has proposed methodlogy, it explains Inception module, feature extraction, facial emotion recognition details. Here, we have also discussed Implementation feasibility of our proposed model on humanoid robot. In section 5, we have described details of all the dataset used to generalize the model. Finally section 6 has experiment details and result analysis. And lastly in section 7 and 8, we have summarized our paper with conclusion and future work of our proposed method respectively.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Analysis Of Previous Research",
      "text": "Lots of research is done in past few years, starting from basic neural network to predict human emotion. Researchers in  [6]  have used CNN to detect Action Units (AUs) for the faces in the emotion dataset to predict target emotion. This method extracts AUs from image sequence to predict final emotion state. This processing via CNN over sequence of images increases the complexity of proposed model.Authors in  [7]  have built a probabilistic model to generate human emotion on android robots using AUs. Researcher in  [8]  have used Support Vector Machine (SVM) to classify seven different emotion using Gabor wavelet. This method is still complex in implementation on a real-time system. To optimize complexity of a CNN for image classification task, authors in  [9]  have build a classifer using Principal Component Analysis (PCA), to reduce the dimensionality and then CNN to extract the features. Such a model is good with reduced complexity for image classifer applications, but when it comes to detecting emotion, which is highly vibrant in nature, reducing dimensionality at this early stage will lead to reduction in efficiency. In  [10]  also, authors have built a model to predict human emotion using PCA + Artificial Neural Network (ANN). Using ANN for image data would enhance parameter requirement and hence increased complexity. When analysed over L1 and L2 regularization to extract image features is done in  [11] . Researchers in  [13] , have improved on AlexNet performace to predict human emotion by varying the number of convolution layers as per the application need. In  [14] , authors have added SVM for classification purposes of features extracted via AlexNet model. Representational autoencoders and CNN model is built in  [15]  to predict human emotion.\n\nIn recent years, efforts are done to optimize and improve efficiency for emotion prediction using deep learning techniques. Authors in  [16]  have used CNN to extract out features and then implemented SVM for classification over translation invariant features and not over hand crafted features as used in earlier research. Recently, EEG dataset is also used to predict human emotion in  [17]  using DEAP dataset. Here, deep neural network and CNN model are built and analysed over EEG data for emotion prediction rather than facial images. Further in  [18]  paper, authors have built a model using genetic algorithm (GA) + SVM for EEG signals dataset. They have done the classification on two dimensional emotional model, arousal and valence. Electrodemal Activity [EDA] signals are also used for emotion classification as in  [19] . Classification here is done using SVM classifier. An overall review about emotion classifiers, its accuracy achieved over various datasets and complexity issues are discussed in  [20] . This review focusses on various hybrid deep learning approaches for emotion classification problem involving CNN, Recurrent Neural Network (RNN) especially LSTM to capture sequential frames of images to predict emotion.\n\nLater in  [21] , researchers have incorporated facial landmarks to loss function of classification process. Using facial landmarks helped differentiate between local and global features to help classify the variation in emotion. Action Units are also predicted for emotion classification. In  [22] , AUs prediction is automated by extracting spatial and temporal representations from the data. Spatial representations are extracted using CNN model and temporal represenations are computed using Long Short Term Memory (LSTM) model. Here, LSTMs are stacked on top of CNN to extract temporal representation after pulling spatial representations. Then combination of AUs brings out the emotion of the person. The network based on CNN proposed by  [23]  worked to make the network more generalizable by introducing more non-linearity to the model by adding four inception layers to the model. This contribution helped in generalizing the model to a better extent and can cover up more unknown faces, or unknown variant of faces with emotion. CNN is tried and improved by different ways depending on the requirement and application by working upon layers, regularization, loss function, activation function and computation speed. Such analysis is done in  [24] . In  [25] , authors have further tried to improve upon the network by adding non-linearity in the system and worked upon the complexity by added global average pooling in place of fully connected layers.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Formulation",
      "text": "To solve the real time implementational need for a social personal robot in order to predict human emotion for collaborative behavior, we have proposed a CNN model based on Inception modeule concept and have verified its real time complexity. Requirement of hyperparameters to be trained in a deep learning model is huge resulting in requirement of huge computational resources. Hence main contribution of this research is optimized model to predict human emotion which is robust in nature (has been verified over seven datasets) .This model is implemented over humanoid robot NAO and we have tested its response time and true predictions percentage. A comparison is done between state of the art existing models and our proposed model on metrics of number of parameters, number of weighted layers, training accuracy percentage and test accuracy percentage.Also the model is tested over seven different datasets to validate its robustness. A comparison is done in response time of the robot to predict human emotion while the robot if talking to the person.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Methodology",
      "text": "Our proposed model is based on Inception module, and is evaluated on criteria of accuracy, generalizability, response time in humanoid robot and computational speed while training and testing of the model. Inception module is used to introduce non-linearity to the system and reduce dimension as and when required. We have targeted to reduce dimension gradually down the network as we go deep, so as to not miss any feature in the beginning. Architecture of the network is shown in Fig.  1 . We have tested our proposed model on seven different datasets namely, Fer2013, JAFFE, CK dataset, CFD, Impa-face3D dataset, Affectnet and a custom dataset built in our lab. It is tested over seven class labels namely, \"happy\", \"sad\", \"angry\", \"neutral\", \"surprise\", \"fear\" and \"disgust\". To optimize the model, we have worked on batch normalization  [26] , 1×1 convolution  [26] , depthwise separable convolutions and global average pooling (GAP)  [26] . The use of Gap reduces the parameter requirement to a great extent by approximately 85%. Finally the network is tested over humanoid robot, NAO along with vanilla CNN model with almost same number of layers and performances of both situations are compared and analyzed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Inception Module",
      "text": "Inception module  [26]  helps reduce the complexity as it uses 1×1 kernal size before using any bigger size kernel and also reduces channels thereby reducing the computation. Also, adding 1×1 filter in parallel with other filters help extract other variants in the features which higher size filter convolution misses out. These two benefits are utilised in our model along with few other concept which will be explained subsequently to make it computation efficient and still be able to extract out the best feature set and meet state of the art accuracy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "Extracting features from face dataset to predict the emotion is a crucial task. It is difficult to achieve or improve accuracy in the sense that slight variation in the angle of lips, tilted eyebrows etc can result in change in the prediction of class or emotion in our case and lead to misclassification. We have increased the filter size gradually in the network as we go deep, so that dimensionality reduces with depth so that we don't miss out relevant feature in the beginning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Emotion Recognition",
      "text": "The proposed network architecture is shown in Fig.  1 . In the first layer, Convolution 1X1 is used to reduce dimensionality and computation and also introduce more non linearity in the system. After that, separable convolution is implemented 2 times in a loop of 4 with max pool operation as shown in architecture. Along with separable convolution, 1X1 convolution is added with the result as residue as shown in the architecture. This is done considering any missing feature that can be brought up from previous layers.\n\nIn the architecture, a part is repeated four times with increasing number of filters used each time. Number of filters for convolution is increased from 32 to 64 to 128 and finally to 256. Depth in the feature map is increased gradually so that best of the features can be extracted. Dimensionality is reduced gradually so that relevant feature is not missing with increasing convolution operations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Parameter Reduction Strategy",
      "text": "We have introduced many strategies to reduce parameter requirement in our model. In the beginning layer, 1X1 convolution is used to reduce the dimension of the input image and introduce non linearity in the dataset for better generalization of the network. Further separable convolution is used which reduces the number of multiplications in every epoch. After feature extraction in Convolution layers, classification task is performed. For classification, instead of Fully Connected layers, we have used Global Average Pooling which globally extracts out the average of the features. So average of the best features is highlighted out to perform the classification using Softmax function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reduction In Computational Resources",
      "text": "Use of separable convolution reduced the number of multiplication required in every epoch by a factor of 15 (approx.) as compared to convolution operation. Batch normalization is used which reduces the range of values hence introducing simpler multiplications. Also use of Global Average Pool instead of fully connected layers reduces all the computation of training the parameters of the dense layers. Global Average Pool performs average pooling operation over final set of feature map which only involves very few multiplications. In the last convolution layers, we have generated 7 channels of feature map as we have 7 classes. Global average pooling averages the 7 feature map and Softmax then classifies it to the most dominant emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance And Accuracy Trade Off",
      "text": "Use of separable convolution reduced the number of multiplication required in every epoch by a factor of 15 (approx.) as compared to convolution operation. Batch normalization is used which reduces the range of values hence introducing simpler multiplications. Also use of Global Average Pool instead of fully connected layers reduces all the computation of training the parameters of the dense layers. Global Average Pool performs average pooling operation over final set of feature map which only involves very few multiplications. In the last convolution layers, we have generated 7 channels of feature map as we have 7 classes. Global average pooling averages the 7 feature map and Softmax then classifies it to the most dominant emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation To Check Its Real Time Applicability On Humanoid Robot, Nao",
      "text": "Personal Robots can be made more customizable when it understands the human emotion along with conversation and visualization. It will help train/ treat elderly, child or other needy person to get their personal work done in a better way instead of robots becoming annoying for human. For our experiment, we have used NAO Humanoid Robot, though our application is not limited to NAO or to humanoid robots. NAO is a humanoid robot which is mobile, agile and Interactive. It has 25 degree of freedom. For vision, NAO has 2 cameras, one on the forehead center and other on the lips. We have used forehead camera of NAO to make it predict emotion of the person to whom NAO is looking at.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Variations In The Dataset",
      "text": "Datasets used for training, validation and testing our model is FER2013 dataset which comprises of 28709 training samples from 7 class labels and 3589 images for testing the network. Labels in the dataset are 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. Testing is done on different datasets to verify the robustness of the model. For testing the network, JAFFE dataset  [27]  and custom dataset are used. JAFFE dataset contains 230 images of 10 Japanese female models all posed for 7 different emotions. Custom dataset consists of 2000 images. Out of which 840 images are posed for 7 emotions, 700 images are collected with spontaneous emotions by showing movie clips, and collected during different moments of people gathering at different occasions. 300 collected from internet with different emotion labels, and 160 images are non-face images for adding generality in the dataset. Many variants in the dataset is used like different light conditions, face makeup, Asian and non-Asian faces, Japanese face, different background, all age groups and even non face images in order to achieve better generalization.All the datasets used for the experiment are described below showing number of test and train images for each class label. And it shows the subject count considered for dataset telling that inclusion of all these dataset assured addition of every variation to the set thereby network should be able to handle every kind of data well, instead of performing best for any one particular type of dataset and failing for others.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fer2013 Dataset",
      "text": "As explained earlier, this dataset is used for training and testing of our model. This dataset has 32,297 images in total and is divided in 28709 train and 3589 test images. We have verified manually that this data consists of non face images as well to embrace genrality in the network.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Jaffe",
      "text": "JAFFE is quite old small dataset for emotions consisting of 213 images of 7 emotion labels. We have considered this also for testing as it is used by many researchers to create a benchmark of results, hence comparitive confusion matrix is also build over this dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ck Dataset [28]",
      "text": "This dataset is relevant for our experiment in the sense that it has consecutive frames of images to predict an emotion. We are also performing similar experiment while implementing our model in humanoid robot, NAO, which captures 10 frames per second to predict human emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cfd [29]",
      "text": "This is Chicago Face dataset, it has 1250 images of categories: neutral, happy (with open face), happy (with close face), angry and fearful. We have verified this dataset using 4 labels: happy, neutral, angry and fear.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Impa-Face3D Dataset [30]",
      "text": "This dataset consists of 266 images from 38 subjects of 7 emotions which we are considering in our experiment. When our model is tested over this dataset, it has achieved 77% of accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Affectnet [31]",
      "text": "Affectnet dataset has 420,299 images of 11 categories namely, neutral, happy, sad, surprise, fear, disgust, anger, contempt, none, uncertain, and non-face. Out of which we are labeling it to first 7 categories and rest are considered under other category in our case.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Custom Dataset Created In Our Lab",
      "text": "We have created a dataset of 2000 images consisting of posed and spontaneous images of 7 different emotions using 12 subjects and further collected group face images of the subjects to classify emotions in them and test our model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Details And Results Analysis",
      "text": "We have used seven different datasets for our processing as discussed earlier. This experiment testing on various datasets is done to ensure robustness of the network and to check if that network can generalize to all the variation in the face dataset whether it be light effects, face changes in different region of the world, angle of the face and many other variations. After proof validation of our model, we have tested it on humanoid robot, NAO. We have also tested vanilla CNN model for comparison and to show real time effectiveness of our model over basic model. The architecture for the model is shown in the fig.  1 . The model is based on inception module  [26] , involving 1X1 Con-volutions and Global Average Pooling to reduce the number of trainable hyperparameters. In this network, separable Convolutions are used to reduce the hyperparameter requirement. 1X1 Convolutions are used to further reduce complexity and fully connected layer is replaced with global average pooling to further reduce most of the trainable parameters in order to make it real time implementable. Few samples of the datasets used for training and testing from various datasets mentioned above are shown in Fig.  2 . In that figure, first row has samples of Fer2013 dataset, second row has CFD dataset, third row is sample from AffectNet and finally samples of JAFFE dataset are shown. Fig.  3  shows the custom data created in our lab consisting of 2000 images used for testing purposes. Rigorous training and testing has been performed to verify the validity of the trained model. Images were taken in different background and light conditions. And mostly all the seven emotions were captured for different person. Label was created with manual judgement which also might have discrepancy, as human accuracy to judge an emotion is 65±5%. Some unlabeled data was also generated to test our network, like when face is not too clear with light conditions, or non face image shown in Fig.  3 . No class data images are added in the dataset for further generalization of the network. The network was trained on GPU server in which training was done several times in order to freeze the hyperparameters and the layer structure which can provide most efficient result with least computational cost. As the Occam Razor's principle, the simplest algorithm solving the same problem is the best one. As the fully connected layer in a CNN model has mostly 90 % of the parameters of the entire network, so using Global Average Pooling  [26]  has reduced the excess parameters. Further parameters are more reduced by adding 1X1 convolutions which can drastically reduce the parameter requirement and using separable convolutions which further reduces the parameters to be trained. Using depth-wise separable convolutions further reduces parameter. It works as convolving as two sequential layers (on the name of one Convolution), namely depth-wise convolution and point-wise convolution as shown in Fig 3 . Point-wise convolution is basically 1X1 convolution in order to reduce the parameter requirement. Say if we have input of 16 channels and Convolution operation is performed using 32 3X3 filters, in that case all the 16 channels are traversed by every 32 channels of 3X3 size, so the number of parameters are: 16 X 32 X 3X 3 = 4608 While when separable convolution is used, it traverses 16 channels by 1 3X3 filter and also 16 channels by 32 1X1 filters, thereby resultant number of parameters becomes:\n\nHence the number of required parameters reduces to a great extent performing the same task achieving desired accuracy. I Fig.  3 . Depthwise Separable convolution illustrating how it reduces complexity achieving the same goal.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup With Humanoid Robot, Nao",
      "text": "After intense training and testing on all the possible variations of input, the network is implemented on NAO humanoid robot. NAO forehead camera is used as input for the network. The maximum number of predictions out of 10 frames per minute of NAO vision is predicted as final emotion of the person to which NAO is talking to. This has been repeated every 2 minutes during the experiment. 15 subjects were used in the experiment to talk to NAO who tried making all the 7 emotions after every 2 minutes while talking to NAO as shown in Fig.  4 . Each subject made the emotions two times to evaluate the efficiency of the system. The efficiency in the experiment is increased as compared to that in the network as here maximum prediction is considered out of 10 consequent predictions, leading to correct prediction most of the time.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Result And Analysis With Extracted Features, Complexity & Accuracy",
      "text": "After evaluating the model over various datasets, the accuracy achieved on Fer2013 dataset is 72% whose plot is shown in the Fig.  5   Fig.  6  shows the results obtained in our custom dataset build in our lab. Its accuracy is approximately 64% -68%. The network built is able to extract out faces from group photo and predict emotion from it as shown in the figure. For face detection in our model we have used haarcascade_frontalface model designed by OpenCV which detects the frontal face. This algorithm is based on Viola Janes detection algorithm trained on face and non-faceimages to identify a face. Clearly effiecieny of our model is also affected by efficiency achieved in the face detection model. Comparison on confusion matrix for JAFFE dataset is shown in Table  1 , and 2 of Fig.  7  for Vanilla CNN, and our model respectively. These tables show that Vanilla CNN (a variant of AlexNet) lagged in prediction of angry and disgust emotion at all and also percentage accuracy for other emotions were also lesser as compared to our model. Confusion matrix for CFD dataset is shown in Fig.  8 . This dataset has 4 classes namely, neutral, angry, happy, fear and some non class images. When tested on our network, other three labels: surprise, sad, and disgust if predicted are considered as non-class as these doesnot exists in the dataset. So confusion matrix is framed accordingly. It is achieving 74.45 % accuracy as shown in the disgram. We have also tested our model on AffectNet dataset. In this, we have taken random 700 images from manually annotated validation set and achieved 65% accuracy as shown in confusion matrix in Fig.  9 . Affectnet dataset has 10 classes consisting of contempt, none, uncertain and non-face images which are additional than 7 emotions considered by us. So all other additional emotion if predicted or falsely predicted is considered in class \"other\" as shown in the diagram. The efficiency obtained on NAO is further improved from that predicted by the trained model, as the maximum predicted emotion is finalized out of 10 frames in a minute. Results of happy and sad emotion predicted by the NAO robot is shown in Fig.  10 , where the robot is trying to initiate a conversation with human based on the predicted emotion in the choreographe simulation software. Here NAOQi api is used for speech generation of the robot and to capture images for emotion prediction using the NAO forehead camera.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Reduction In Complexity And Its Real Time Implementation Ability",
      "text": "Here we are comparing the time complexity of vanilla CNN model, and our proposed model. To create similar conditions for comparison, both the models were created with global average pooling for classification purpose hence we donnot need to compare fully connected layer parameters. Also for CNN, time complexity of the model computed using  [32]  is:\n\nwhere, is number of filters in l th layer is size of the filter i.e. 5 for 5×5 filter is ouput feature map spatial size for layer l. For simplicity in computation, we have considered only the theoretical complexity and hence can be omitted. Hence, our equation for computation becomes: As stated in  [32] , this computed time complexity applies to both training and testing. Training time is roughly three times that of test time per image. Testing an image needs one forward propogation of the model, where as training one image needs one forward and twice backward propagation as described in  [32] . Also use of Depthwise Separable Convolution in our model rather than Convolution reduces the number of multiplication as explained in fig 3. Use of seperable convolution reduces the computation by a large amount (8 times approximately lowered in our example explained earlier), which futher adds to the fact that our model is lowered in computation as compared to other models.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Time Complexity Of Vanilla Cnn Model",
      "text": "The Vanilla CNN model is framed with 12 layers of Convolution operation with batch normalization and ReLU activation function. The numbers of layers match with that of our model in order to maintain the equality to compare. The fully connected layers are replaced with global average pooling so that all we need to compare is just convolution operation and its complexity. Using layers input and output number and size of filters its time complexity can be computed as: T",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Time Complexity Of Our Proposed Model",
      "text": "Our proposed model, architecture explained in figure  1 , having almost same number of convolution operations with global average pool at the end, having time compexity computed as:\n\nClearly, as compared with equation (3), complexity is almost reduced to one third of its value as shown in (4) and hence implementable in real time systems which we have explored subsequently.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Real Time Implementation On Humanoid Robot, Nao",
      "text": "The real time experiment setup is shown in fig 4 . Here you can see the subject is sitting in front of the robot and talking to it. Here only human emotion prediction using face image is used, hence whatever emotion is posed by the person, robot tries to start the conversation with such mood in order to make it feel more natural to connect and communicate. Response time is computed for each emotion on different subjects and has been compared with Vanilla CNN model response time when implemented in NAO robot. Accuracy in prediction and response time is compared for Vanilla CNN and our model and comparison table is shown in Table  1 . Here we have obtained minimum of 0.45 seconds and maximum of 1.02 seconds on an average for all 7 emotions when experimented over 10 subjects. Whereas when Vanilla CNN is used for the same experiment, almost all the emotions were predicted with longer delay and prediction rate is also reduced. This response time and prediction rate was computed by making the person sit in front of the robot while holding that emotion till Robot predicts it (right or wrong) with a threshold of 1.30 seconds. Flow diagram on how the experiment was performed to compute response time and prediction rate is shown in Fig.  11 . During the experiment, 10 frames per second was captuered after every 2 minutes and if any emotion is dominating in those frames, NAO speaks out about the emotion to check the well being of the person it is talking to. This can be used in many applications like personal robots, treating elderly people, treating autistic patients, physchological treatments of kids and elderly people etc.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In order to understand human's sentiment in a statement, robot need to be able to understand sentiments in text, but to relate to the context in which it is being said, understanding facial expression will also help to a great extent. Hence to make humanoid robots understand people in a more efficient manner, we have made an effort such that facial expression can be estimated in a real time manner. This will make robots connect to human in a better way, and human can rely or get more comfortable in presence of robots. Even this way human can express themselves to robots and stay more personalized.\n\nOur proposed model is tried, tested with humanoid robot and improved responses are shown. Our network achieved 94% reduction in parameter requirement, leading to reduction in complexity and an overall improvement in accuracy of upto 6% is reached when implemented over humanoid robotswhen compared over latency and response time.We have shown the improvement in real time system, i. e., humanoid robot, NAO.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Future Work",
      "text": "Our proposed network once embedded with humanoid robots can judge person's emotion and deal the situation accordingly. Further multi modal communication can be added in robot to make it more social and understandable. Affectnet dataset had further more classes like content, no-emotion, no face, etc. which can be further incorporated in the model. Sentiment of a same statement might be different at different times and situation, which can be predicted clearly if facial emotion is also known at that instance. Also researchers in  [33]  have mentioned various mixed emotions like \"angrily disgusted\", \"sadly angry\", etc, we are looking forward to add these too so that clearer prediction can be made for a better collaborative behavior. Our future work is directed to improve our models performance on humanoid robot NAO. Two camera are embedded in NAO robot one on the forehead and other in position of the mouth. These can take images in a range of resolution from 160 X 120 up to 1280 X 960. NAOQi framework described in  [34] , handles the vision api which can be used for both the cameras to improve the performance and try to extract out the best features from both the images of same instance. Authors in  [35]  have predicted finger pointing direction using single RGB camera. As our robot is already integrated with emotion classifier, adding gesture and speech to robot will make it more smooth and genuine. Hence personal robots can work in applications like psychological counseling, kids handling, elderly care etc. with more ease.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We have tested our proposed model on seven different datasets namely, Fer2013, JAFFE,",
      "page": 3
    },
    {
      "caption": "Figure 1: Architecture of our proposed model. (filter size is increased in the loop from 3 to 5 to 7  and then to 9 to gradually lower the dimension",
      "page": 4
    },
    {
      "caption": "Figure 1: In the first layer, Convolution 1X1 is used to reduce dimension-",
      "page": 4
    },
    {
      "caption": "Figure 2: Image samples of some datasets. First row has samples of Fer2013 dataset, second row has CFD dataset, third row is sample from",
      "page": 6
    },
    {
      "caption": "Figure 1: The model is based on inception module [26], involving 1X1 Con-",
      "page": 6
    },
    {
      "caption": "Figure 2: In that figure, first row has samples of Fer2013 dataset, second",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the custom data created in our lab consisting of 2000 images used for testing purposes. Rigorous training",
      "page": 7
    },
    {
      "caption": "Figure 3: No class data images are added in the dataset for further generalization of the network.",
      "page": 7
    },
    {
      "caption": "Figure 3: Point-wise",
      "page": 7
    },
    {
      "caption": "Figure 3: Depthwise Separable convolution illustrating how it re-",
      "page": 7
    },
    {
      "caption": "Figure 4: Each subject made the emo-",
      "page": 7
    },
    {
      "caption": "Figure 4: Experimental setup diagram.",
      "page": 8
    },
    {
      "caption": "Figure 5: (a). Fig. 5(b) shows the decrease in the loss showing improvement in the performance of the training pro-",
      "page": 8
    },
    {
      "caption": "Figure 5: a) Accuracy plot for the network which stabilizes to 72%",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the results obtained in our custom dataset build in our lab. Its accuracy is approximately 64% - 68%. The",
      "page": 8
    },
    {
      "caption": "Figure 7: for Vanilla CNN, and our mod-",
      "page": 8
    },
    {
      "caption": "Figure 8: This dataset has 4 classes namely, neutral, angry, happy, fear and",
      "page": 8
    },
    {
      "caption": "Figure 9: Affectnet dataset has 10 classes consisting of contempt, none, uncertain and non-face images which are addi-",
      "page": 8
    },
    {
      "caption": "Figure 6: Result of emotion classifier obtained from all the datasets used for verification.",
      "page": 9
    },
    {
      "caption": "Figure 10: , where the robot is trying to initiate a conversation with human based on the predicted emotion",
      "page": 9
    },
    {
      "caption": "Figure 7: Comparison in the Confusion matrix on JAFFE dataset us-",
      "page": 10
    },
    {
      "caption": "Figure 8: Confusion Matrix Performance of our network on CFD da-",
      "page": 10
    },
    {
      "caption": "Figure 9: Confusion Matrix performance of our model in AffectNet",
      "page": 10
    },
    {
      "caption": "Figure 10: NAO Robot predicting the emotion of the person it is",
      "page": 11
    },
    {
      "caption": "Figure 3: Use of seperable convolution reduces the computation by a large amount (8 times ap-",
      "page": 11
    },
    {
      "caption": "Figure 1: , having almost same number of convolution operations with",
      "page": 11
    },
    {
      "caption": "Figure 4: Here you can see the subject is sitting in front of the robot and talk-",
      "page": 11
    },
    {
      "caption": "Figure 11: Flow diagram showing how NAO (Humanoid Robot) talks to the person about his/ her emotion using our algorithm.",
      "page": 12
    },
    {
      "caption": "Figure 11: During the experiment, 10 frames per second was captuered after every 2 minutes and if any emotion is dominat-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "I  \n \nFig.  8.  Confusion  Matrix  Performance  of  our  network  on  CFD  da-\ntaset.": "I   \nFig.  9.  Confusion  Matrix  performance  of  our  model  in  AffectNet \ndataset (subset)."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: Average Response Time and prediction rate (using 10 subjects) on NAO when Vanilla CNN and our model are implemented for",
      "data": [
        {
          "Vanilla CNN": "Response \ntime (sec)",
          "Proposed Model": "Response \nTime"
        },
        {
          "Vanilla CNN": "0.98",
          "Proposed Model": "0.45"
        },
        {
          "Vanilla CNN": "1.03",
          "Proposed Model": "0.49"
        },
        {
          "Vanilla CNN": "1.26",
          "Proposed Model": "0.72"
        },
        {
          "Vanilla CNN": "NA",
          "Proposed Model": "0.86"
        },
        {
          "Vanilla CNN": "1.29",
          "Proposed Model": "0.63"
        },
        {
          "Vanilla CNN": "0.96",
          "Proposed Model": "1.02"
        },
        {
          "Vanilla CNN": "1.58",
          "Proposed Model": "0.91"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Social Robots for People with Developmental Disabilities: A User Study on Design Features of a Graphical User Interface",
      "authors": [
        "Xiaodong Wu",
        "Lyn Bartram"
      ],
      "year": "2018",
      "venue": "Social Robots for People with Developmental Disabilities: A User Study on Design Features of a Graphical User Interface",
      "arxiv": "arXiv:1808.00121"
    },
    {
      "citation_id": "2",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Feature Extraction for Emotion Recognition and Modelling Using Neurophysiological Data",
      "authors": [
        "A Samara",
        "M Menezes",
        "L Galway"
      ],
      "year": "2016",
      "venue": "15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security"
    },
    {
      "citation_id": "4",
      "title": "Facial action recognition for facial expression analysis from staticfaceimages",
      "authors": [
        "M Pantic",
        "J Rothkrantz"
      ],
      "year": "2004",
      "venue": "IEEE Transactionson Systems, Man and Cybernetics"
    },
    {
      "citation_id": "5",
      "title": "Facial emotion recognition using deep convolutional networks",
      "authors": [
        "M Mohammadpour",
        "H Khaliliardali",
        "S Hashemi",
        "M Alyannezhadi"
      ],
      "year": "2017",
      "venue": "2017 IEEE 4th International Conference on Knowledge-Based Engineering and Innovation (KBEI)"
    },
    {
      "citation_id": "6",
      "title": "Facial Expression Generation of an Android Robot based on probabilistic Model",
      "authors": [
        "H Hyung",
        "D Lee",
        "H Yoon",
        "D Choi",
        "D Lee",
        "M Hur"
      ],
      "year": "2018",
      "venue": "th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "7",
      "title": "Facial Expression Recognition Based On SVM in E-Learning",
      "authors": [
        "L Chen",
        "C Zhou",
        "L Shen"
      ],
      "year": "2012",
      "venue": "Proceedings of 2012 International Conference on Future Computer"
    },
    {
      "citation_id": "8",
      "title": "Convolutional Neural Network Based on Principal Component Analysis Initialization for Image Classification",
      "authors": [
        "X Ren",
        "H Guo",
        "G He",
        "X Xu",
        "C Di",
        "S Li"
      ],
      "year": "2016",
      "venue": "IEEE First International Conference on Data Science in Cyberspace (DSC)"
    },
    {
      "citation_id": "9",
      "title": "Multimodal emotion recognition with evolutionary computation for humanrobot interaction",
      "authors": [
        "Perez-Gaspar",
        "Luis-Alberto",
        "Caballero-Morales",
        "Santiago-Omar",
        "Felipe Trujillo-Romero"
      ],
      "year": "2016",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "10",
      "title": "Investigation on the Effect of L1 an L2 Regularization on Image Features Extracted Using Restricted Boltzmann Machine",
      "authors": [
        "S Jaiswal",
        "A Mehta",
        "G Nandi"
      ],
      "year": "2018",
      "venue": "2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS)"
    },
    {
      "citation_id": "11",
      "title": "Convolution neural network for automatic facial expression recognition",
      "authors": [
        "X Chen",
        "X Yang",
        "M Wang",
        "J Zou"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Applied System Innovation (ICASI)"
    },
    {
      "citation_id": "12",
      "title": "Convolutional Neural Networks for Facial Expression Recognition",
      "authors": [
        "S Alizadeh",
        "A Fazel"
      ],
      "year": "2017",
      "venue": "Convolutional Neural Networks for Facial Expression Recognition"
    },
    {
      "citation_id": "13",
      "title": "Facial Expression Classification Using Convolutional Neural Network and Support Vector Machine",
      "authors": [
        "V Pilla",
        "H Medeiros"
      ],
      "venue": "Facial Expression Classification Using Convolutional Neural Network and Support Vector Machine"
    },
    {
      "citation_id": "14",
      "title": "Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units",
      "authors": [
        "P Dachapally"
      ],
      "year": "2017",
      "venue": "Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units"
    },
    {
      "citation_id": "15",
      "title": "Learned features are better for Ethnicity Classifications",
      "authors": [
        "I Anwar",
        "N Islam"
      ],
      "year": "2017",
      "venue": "Learned features are better for Ethnicity Classifications"
    },
    {
      "citation_id": "16",
      "title": "Using deep and convolutional neural networks for accurate emotion classification on DEAP dataset",
      "authors": [
        "S Tripathi",
        "S Acharya",
        "R Sharma",
        "S Mittal",
        "S Bhattacharya"
      ],
      "year": "2017",
      "venue": "Proc. IAAI"
    },
    {
      "citation_id": "17",
      "title": "Improving bci-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "18",
      "title": "A wavelet-based approach to emotion classification using EDA signals",
      "authors": [
        "H Feng",
        "H Golshan",
        "M Mahoor"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "19",
      "title": "A Brief Review of Facial Emotion Recognition Based on Visual Information",
      "authors": [
        "Byoung Chulko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "20",
      "title": "I know how you feel: Emotion recognition with facial landmarks",
      "authors": [
        "I Tautkute",
        "T Trzcinski",
        "A Bielski"
      ],
      "year": "2018",
      "venue": "I know how you feel: Emotion recognition with facial landmarks"
    },
    {
      "citation_id": "21",
      "title": "Learning spatial and temporal cues for multi-label facial action unit detection",
      "authors": [
        "W Chu",
        "F Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "22",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Winter Conference on Application of Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Recent advances in convolutional neural networks",
      "authors": [
        "J Gu",
        "Z Wang",
        "J Kuen",
        "L Ma",
        "A Shahroudy",
        "B Shuai",
        "T Liu",
        "X Wang",
        "L Wang",
        "G Wang"
      ],
      "year": "2017",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "24",
      "title": "Real-time convolutional neural networks for emotion and gender classification",
      "authors": [
        "O Arriaga",
        "M Valdenegro-Toro",
        "P Plöger"
      ],
      "year": "2017",
      "venue": "Real-time convolutional neural networks for emotion and gender classification",
      "arxiv": "arXiv:1710.07557"
    },
    {
      "citation_id": "25",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "Coding Facial Expressions with Gabor Wavelets, 3rd IEEE International Conference on Automatic Face and Gesture Recognition",
      "authors": [
        "J Michael",
        "Shigeru Lyons",
        "Miyuki Akamatsu",
        "Jiro Kamachi",
        "Gyoba"
      ],
      "year": "1998",
      "venue": "Coding Facial Expressions with Gabor Wavelets, 3rd IEEE International Conference on Automatic Face and Gesture Recognition",
      "doi": "10.1109/AFGR.1998.670949"
    },
    {
      "citation_id": "27",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Y Tian"
      ],
      "year": "2000",
      "venue": "Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition (FG'00)"
    },
    {
      "citation_id": "28",
      "title": "The Chicago Face Database: A Free Stimulus Set of Faces and Norming Data",
      "authors": [
        "Correll Ma",
        "Wittenbrink"
      ],
      "year": "2015",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "29",
      "title": "Banco de Dados de Faces 3D: IMPA-FACE3D",
      "authors": [
        "Jesus Mena-Chalco",
        "Roberto Marcondes",
        "Luiz Velho"
      ],
      "year": "2008",
      "venue": "Banco de Dados de Faces 3D: IMPA-FACE3D"
    },
    {
      "citation_id": "30",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Convolutional neural networks at constrained time cost",
      "authors": [
        "K He",
        "J Sun"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "A deep learning approach for object recognition with NAO soccer robots",
      "authors": [
        "D Albani",
        "A Youssef",
        "V Suriani",
        "D Nardi",
        "D Bloisi"
      ],
      "year": "2017",
      "venue": "RoboCup 2016. LNCS (LNAI)",
      "doi": "10.1007/978-3-319-68792-6_33"
    },
    {
      "citation_id": "33",
      "title": "Deep Learning based Command Pointing direction estimation using a single RGB Camera",
      "authors": [
        "S Jaiswal",
        "P Mishra",
        "G Nandi"
      ],
      "year": "2018",
      "venue": "2018 5 th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)"
    }
  ]
}