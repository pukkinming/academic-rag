{
  "paper_id": "2304.14714v1",
  "title": "Sged: A Benchmark Dataset For Performance Evaluation Of Spiking Gesture Emotion Recognition",
  "published": "2023-04-28T09:32:09Z",
  "authors": [
    "Binqiang Wang",
    "Gang Dong",
    "Yaqian Zhao",
    "Rengang Li",
    "Lu Cao",
    "Lihua Lu"
  ],
  "keywords": [
    "Homogeneous multimodal data",
    "Spiking neural network",
    "Event dataset",
    "Affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the field of affective computing, researchers in the community have promoted the performance of models and algorithms by using the complementarity of multimodal information. However, the emergence of more and more modal information makes the development of datasets unable to keep up with the progress of existing modal sensing equipment. Collecting and studying multimodal data is a complex and significant work. To supplement the challenge of partial missing of community data, we collected and labeled a new homogeneous multimodal gesture emotion recognition dataset based on the analysis of the existing data sets. This data set complements the defects of homogeneous multimodal data and provides a new research direction for emotion recognition. Moreover, we propose a pseudo dual-flow network based on this dataset, and verify the application potential of this dataset in the affective computing community. The experimental results demonstrate that it is feasible to use the traditional visual information and spiking visual information based on homogeneous multimodal data for visual emotion recognition. The dataset is available at https://github.com/201528014227051/SGED",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition aims to enhance the stability and accuracy of emotion recognition by using the complementarity of multimodal information  [1] . The existing multimodal emotional tasks mostly include image, text, voice and other information  [2] . Through the interactive fusion of these information, we can make up for the lack of single mode data in some aspects of information. Just as the human five senses can better perceive the external world through cooperation, the collaborative fusion of multimodal data also improves the performance of emotion recognition tasks. It also provides technical reference for other fields based on multimodal information processing, such as matching, tracking, and brain imaging classification  [3, 4, 5] .\n\nAccording to different sources of multimodal information, this paper classifies multimodal data into two categories: homogeneous multimodal data and heterogeneous multimodal data. Homogeneous multimodal data refers to data from unified information sources, such as visual effects from different angles or devices  [6] . Heterogeneous multimodal data refers to data from different information sources, such as multimodal data from vision and sound  [7] . Due to the characteristics of heterogeneous multimodal data from different information sources, the data are more complementary, but the data form is different, so the processing mode between them is different. Homogeneous multimodal data generally come from different forms of the same information source, such as the combination of the popular ordinary RGB video information and depth camera, or the data acquisition form different from the traditional single RGB video  [6] .\n\nThe research goal of homogeneous multimodality is to mine the task-related information contained in the data as much as possible from multiple angles under the same information source. Some examples is shown in Fig.  1  to show the difference between heterogeneous multimodal data and homogeneous multimodal data. It is worth noting that the information after homogeneous multimodal fusion can also be used as a single input in heterogeneous multimodal.\n\nThe information processing source of this paper is data from vision. Traditional video data includes videos containing a sequences of ordered pictures.\n\nVideo can capture dynamic information relative to pictures, and can better predict tasks by capturing the context of time. Video from a single data source can only record information from one angle, and there is a visual dead angle.\n\nIn order to record scene data more comprehensively, some researchers proposed to use multiple camera data and then model 3D data for emotion recognition  [6] . From another perspective, this paper fully exploits the potential of vision, records data of different format, and provides algorithms. Visual transmission is more about posture information. Some scholars have made relevant preliminary research on dynamic posture, but this paper has carried out more systematic exploration and analysis in the field of emotion recognition. The performance of ordinary cameras in capturing scenes will decline under low illumination, fast motion, high dynamic range, etc. This decline in the performance of the input will directly affect the accuracy of subsequent tasks. The event flow data can just overcome these defects and provide a more robust recognition result.\n\nIn order to fully capture the dynamic information contained in the gesture, in addition to the ordinary video frame data, another modal data form used here is event stream data. The feature of event stream data is that it can capture the dynamic information in the picture, and only record the position and polarity of changes in the picture  [8] . Moreover, the time resolution of event stream data recording is generally very high, and the dynamic range is larger. Considering these characteristics, we decided to adopt the homogeneous multimodal form of video frames and event streams for emotion recognition research. If two different devices are used, the problem of scene registration should also be considered, because the location of different devices is different.\n\nIn order to avoid this difficulty, we adopted a collection device that can output video frames and event stream data at the same time, avoiding the error caused by the registration process from the hardware level  [9] .\n\nThe emotion recognition technology based on pure vision can be applied to situations where other modal information is not available, such as when the distance is relatively long and the surrounding noise is particularly large, only visual information is effective. Sign language is one of the most important means of information exchange without voice mode, especially for the hearing-impaired.\n\nHowever, the learning of professional sign language needs huge manpower, so it is positioned as ordinary daily communication gestures at the application level.\n\nThat is, gestures that can be distinguished by people's common sense without special sign language learning can fill the gap in the field of multimodal emotion recognition data.\n\nBy analyzing the data characteristics of the two modes, the video frame data contains more semantic information, while the event stream data contains more dynamic information. Artificial neural network can effectively model semantic information, while spiking neural network also has natural advantages for event flow modeling. So the main contributions of this paper are as follows:\n\n• A new multimodal emotion recognition data paradigm is proposed, which includes video frames and event streams data collection and a labeling algorithm to find sample split index of stream data according to the annotation of video frames.\n\n• Based on the characteristics of the data, the emotion recognition dataset based on dynamic gesture information is constructed. As far as we know, this is the first multimodal dataset containing emotion information posture in the community.\n\n• we propose a new pseudo-double-stream network, which can be input in two streams: video frame data is processed by artificial neural network, and event stream data is processed by spiking neural network. The experimental results validate the effectiveness of multi inputs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "The organization form of event stream is to record whether the brightness of the scene changes and the location of the change by imitating the mechanism of retina periphery, which is different from the regular cumulative exposure of the general camera for the whole scene. The characteristics of event data are asynchronous and sparse, which can provide input for real-time applications with low latency  [10] . Here, we first describe the relevant contents of the event dataset, and then introduce the algorithms and applications related to the event data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Event Dataset",
      "text": "In order to obtain event data, the existing popular data collection devices include DVS (Dynamic Vision Sensor)  [8] , DAVIS (Dynamic and Active-pixel VIsion Sensor)  [9] , ATIS (Asynchronous Time-based Image Sensor)  [11] , and CeleX  [12] . From the perspective of equipment availability, DVS and DAVIS are relatively good choices. Of the two, DVS can only record pure event stream data, while DAVIS can also obtain corresponding video frame data with rich semantic information. Therefore, DAVIS (specifically, DAVIS346) is selected as the acquisition device in this paper.\n\nDue to the inherent dynamic characteristics of gesture, dynamic vision camera has been applied to vision tasks for a long time. Mueggler et al. proposed a simulator of event data that can be used for multiple tasks, which can provide data for the research of multiple tasks, such as pose estimation, visual odometry, and SLAM (Simultaneous Localization And Mapping)  [13] . However, this data is different from the real world data due to the simulation characteristics.\n\nSo more researchers use physical cameras to collect event data. Most of the existing event data sets are captured using DVS  [14, 15, 16] , while the data sets captured using DAVIS are still few. Scheerlinck et al. proposed to use DAVIS to record color event data sets, but the shooting method is to shoot static scenes by shaking the camera, and the focus is not gestures  [17] .\n\nLungu et al. proposed a dataset containing three gestures: rock, scissors and paper  [14] . In the same year, Amir et al. released DvsGesture, the categories include hand waving (both arms), large straight arm rotations (both arms, clockwise and counterclockwise), forearm rolling (forward and backward), air guitar, air drums, and an \"Other\" gesture  [18] . Subsequently, Chen et al. released a dataset called Neuro ConGD Dataset, the gestures include beckoning, fingersnap, ok, push-hand (down, left, right, up), rotate-outward, swipe (left, right, up), tap-index, thumbs-up, zoom (in, out)  [15] . Maro et al. published a dataset, which is designed for the gestures controlling a smartphone, so when collecting data, one hand holding the smartphone while the other hand performs the movement  [19] . In the same year, the SL-Animals-DVS dataset was released, which focuses on using gestures to convey different animal information  [16] . The datasets mentioned above are all single-mode DVS or ATIS of event data, and the information and emotion transmitted are not specifically targeted. The new data set proposed in this paper makes up for these defects in the field. The relevant information of the above datasets is shown in Table  1 . It can be seen that this paper is not only relatively larger in data scale, but also more homogeneous video frame data in data mode than other datasets. The proposed SGED (Spiking Gestures Emotion Dataset) is also presented in Table  1 , and the details progress of constructing the dataset will be introduced in Section 3",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Algorithms And Applications",
      "text": "The processing of event data can be divided into two steps. The first step is to encode the event data, and then apply it on the basis of coding. Many scholars have begun to explore various applications based on event data, in- and SNN scheme. The front part of the network uses SNN to process the input event data, and the back end uses ANN to obtain the final task output  [30] .\n\nWith the popularity of Transformer, Zhang et al. added event data to it to form a spiking Transformer for target tracking  [31] .\n\nIn order to build three-dimensional information, Andreopoulos et al. uses two DAVIS cameras, one left and one right, to build a low power, high throughput, fully event-based stereo system  [32] . In addition, in order to reduce the energy consumption including matrix vector multiplication calculation, resistive memory array is also a possible choice  [33] . It is also an important application to add visual event data to autonomous driving system  [34, 35] . Although the research scope of this paper is visual event data, by the way, some scholars have studied event-based silicon cochlea  [36] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Proposed Dataset",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Basic Information",
      "text": "To construct the multi-modal dataset containing both ordinary vision information and dynamic vision information, DAVIS346 (Dynamic and Active Pixel  Ten volunteers are involved in our experiments. For each volunteer, deliberately designed emotional gesture is conducted in different light conditions and body positions. There are totally nine categories: ok, hello, no, kill, victory, good, yes, love, and fighting. These emotion gestures are conveyed by gestures of body and the details are described in Table  2 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Labeling Process",
      "text": "The labeling process of multimodal data based on video frames and event streams is different from that of traditional multimodal data. Take the traditional multimodal annotation of video and audio as an example. It is assumed that video and audio are synchronized. When the original video is labeled, the same timestamp can be directly used to label the audio. However, this is not the case for video frame data and event stream data, which is caused by the difference in time resolution between the two. The output format of DAVIS346\n\nis not a traditional video, but a simple video frame, which is saved in the form of compression and segmentation; On the other hand, the event stream is also saved in a tensor way, and cannot be directly aligned with the video frame for visualization. Due to the difference in time resolution, the number of frames in video frame data and the number of events in event stream data differ greatly.\n\nIn order to complete the labeling of the collected data, this paper proposes a binary search algorithm based on the scaling factor to align two different types of data, so as to obtain the labeling serial number of the event flow data. The specific labeling process is shown in Algorithm 1.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Algorithm 1 Scaling Binary Search Algorithm",
      "text": "Input: The time stamp corresponding to the video tag point, stored in a list with length M -1 (split into M segments), denoted as T agList. Time information in event tensor data, stored in a list with length N , denoted as T imeList. Scaling factor α.\n\nOutput: Time index of different categories, stored in a list, denoted as OutIndex.\n\nFunction ScalingBinarySearch(List,l,r,t,α):",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Statistical Analysis",
      "text": "To gain an intuitive understanding of the dataset as a whole, we conducted a simple statistical analysis of the dataset to find the prior information contained in it. The homogeneous multimodal dataset contains two parts of data, including video frame data and event stream data. For video frame data, the main parameter is the number of video frames in each sample. Here, we counted the number of samples containing different video frames, which are presented by histogram. As shown in Figure  2 , it can be found that most of the sample video frames are concentrated in 0-100 frames. In this way, when designing the input of video frame module, we can use this 100 as a priori information to truncate the input video frame sequence.\n\nAlthough the data was collected according to the average category setting, in the labeling process, due to the setting of a redundant category, there is a phenomenon of category imbalance. As shown in Figure  3 , the first nine categories are basically about 200 samples, while the tenth category is 688 samples.\n\nFor the convenience of processing, the tenth sample was directly removed in the experimental part. To illustrate the balance of sample data from a time length perspective, we have counted the duration of different sample event data. From Figure  4 , it can be seen that the overall duration of most categories of events is maintained at about 440s after adding on the entire data set. The gesture 'love' takes a long time, probably because the gesture content is relatively complex, and the complex embodiment can be seen from the description in Table  2 . In addition, the long time of 'other' is mainly accumulated by quantity, so we will not pay special attention here.\n\nTo gain a deeper understanding of the distribution law of event data, we have made statistics on positive and negative events respectively. We made statistics on the number of positive and negative events contained in each sample in each category, and showed the mean and outlier information through the box diagram. As shown in Figures  5  and 6 , first of all, it can be found that the number of events (whether positive or negative) contained in different types of emotional postures is different. Secondly, we can predict the number of events contained in different emotional postures from the generation process of gestures. For example, the number of events contained in emotional postures containing complex movements ('love') is more than that contained in emotional postures containing only head movements ('yes' and 'no'). Moreover, the distribution of positive and negative events is positively correlated, that is to say, the sample with a large number of positive events corresponds to a large number of negative events. This is because in the process of gesture operation, it basically belongs to rigid body motion, and the trigger of events is positive and negatives imultaneously in most conditions, with differences in location.\n\nTo visually present the representation of data in the dataset, we have visualized the video frame data and corresponding event data. As shown in Figure  7 , the first column represents one of three video frame sequences with different poses. The second column represents the schematic diagram of the change of event stream data in two-dimensional space over time (from yellow to blue represents the beginning to end of the event). It can be seen that the place  where the event is concentrated always appears in the place where the action occurs. The third column represents the distribution of positive and negative events along with events, where blue represents positive events and red represents negative events. It can be seen that positive events are concentrated in the space where the hand appears.\n\nIt is worth noting that considering that the video frame information involves privacy data such as human faces and postures, we conducted a questionnaire survey on volunteers before releasing the data. According to the questionnaire, we published video frame data for agreed volunteers. For other data, in order to facilitate subsequent scientific research, we provided corresponding video frame feature data.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Methods",
      "text": "To achieve gesture emotion recognition based on homogeneous multimodal input data, we designed a model framework called homogeneous pseudo dual flow network. Here, the concept of dual flow networks  [37]  is borrowed and",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Video Frame Data Preprocessing",
      "text": "In this section, we introduce the preprocessing operations of video data. Because the obtained video sequence data is traditional RGB three-channel data, according to existing mainstream feature extraction methods, it is possible to directly extract three-dimensional features or first extract video frame features, and then use sequence modeling tools, such as LSTM, to model temporal features. In order to enhance the flexibility of subsequent model design and maximize respect for the privacy of volunteers without affecting subsequent model research, we adopt the second scheme here. For the extraction of video frame features, we choose the model, EfficientNet, from the perspective of efficiency.\n\nIn this way, the video frame sequence eventually becomes a sequence composed of feature vectors, and the above process can be expressed as:\n\nwhere f EN represents the feature extraction of EfficientNet, vp i is the feature vector of the video frame v i , which is used as the input of the video timing feature extraction module.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Event Stream Data Preprocessing",
      "text": "This section describes how to preprocess event data. The recording format of event data directly output by DAVIS346 is a four-dimensional vector sequence, as previously mentioned. This recording format is very similar to the recording format of point clouds (except for two-dimensional plane space, the third-dimensional space is a temporal dimension and the fourth-dimensional space records polarity information), so it can be processed using a similar encoding method for point clouds, but this can cause problems such as loss of spatial characteristics of the data, size mismatch, and so on. Therefore, the existing event-based processing algorithms try to map the sequence of events back to the original two-dimensional space, which is commonly referred to as the spike plane. The event can be restored to the two-dimensional coordinate position of the corresponding spike plane based on the spatial location of the event. After direct mapping, the original point cloud data is restored to spike plane sequences. At this point, there is a challenge that the number of spike planes is too large, which can lead to excessive subsequent computation. To solve this problem, researchers have proposed a solution to compress the spike plane. There are two main schemes available, one is to compress based on a fixed time interval, and the other is to compress based on the number of events. Due to the uncertainty of the occurrence of events, there may be situations where no events occur for a period of time depending on the time interval, resulting in an invalid spike plane (i.e., all data is zero). Therefore, compression based on a fixed number of events is used here.\n\nSpecifically, first, the number of all events included in a sample is counted, without distinguishing the polarity of the event. Then, according to a preset K, representing the number of dense spike planes after data preprocessing. It should be noted that due to two different polarity events, the dense spike plane is essentially a three-dimensional tensor with a third dimension of 2. To represent that the data is obtained by compressing the original spike plane sequence, it is called a dense spike plane. The numerical value in the dense spike plane is obtained by separately counting the number of corresponding positive and negative spike events. So far, the processing of the original event data stream has been completed, and dense spike plane sequence data has been obtained.\n\nThe formal description of the above process is as follows:\n\nwhere f dense denotes the compress from spike planes to dense spike planes, sp 1 , ..., sp K are the dense spike planes. Note the for s j , the channel number is 1 which means the positive and negative events are mixed.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Video Timing Feature Extraction Module",
      "text": "In this section, we model temporal dependencies in video frame sequences.\n\nThe output results obtained in the video frame data processing stage are processed separately for each frame, and the extracted feature vectors contain the semantic information of a single frame. In order to capture timing information between consecutive frames, it is necessary to model specifically. Here, the classic time series modeling model LSTM is used to provide a baseline. LSTM can control the compression of effective information from front to back through a gating mechanism, using the output data of the last step of LSTM as the temporal feature representation of the video. The timing modeling process of LSTM can be expressed as follows:\n\nwhere h last is the output of last hidden states of LSTM.\n\nIt is important to note that LSTM is used here to verify the effectiveness of the complementarity of homogeneous multimodal data, and can be completely replaced by other temporal modeling tools, such as Transformer.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Event Stream Spiking Modeling Module",
      "text": "This section focuses on modeling event data, which has unique spike characteristics that make it well-suited for spiking neural networks. This approach is commonly used for gesture recognition with event cameras. In this work, we employ a classic framework for modeling event flow data, with the key modification being the adjustment of the number of intermediate-layer neurons to match the output types  [18] . The spiking neural network topology includes standard convolution, pooling, and fully connected layers, which are used to spatially compress input data and extract meaningful features. The main difference in our approach is the configuration of the number of spiking neurons in the fully connected layer prior to the output layer.\n\nThe specific spiking neuron model uses LIF (Leaky Integrate-and-Fire)  [38] .\n\nAn effective event flow feature extraction process can be expressed as:\n\nwhere DvsGesture is the network structure in  [18] , s DG is the output spike array whose size is equal to the number of category. Note that s DG is after averaged with K, and direct used to compute the MSE loss.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Gesture Emotion Recognition Module",
      "text": "This section describes how to synthesize the output of two different networks to obtain the final result. For spiking neural networks, we average pool the outputs of final spiking neurons output by the corresponding module (i.e. Event stream spiking modeling module) to reduce the dimension to the emotional category domain space. For the LSTM structure, we take the hidden state of the final step output as input and map it to the emotional output space through two fully connected layer with dropout rate 0.5. Due to the structure of the two branches involved, the output fusion method is directly used here.\n\nwhere f o represents two fully connected layer with dropout rate 0.5.\n\nIn addition, in experiments, we found that spiking neural networks need to use MSE losses to achieve good performance, while artificial neural networks are more effective in using cross entropy losses for this type of classification problem. Therefore, here, we balance the contribution of two different outputs by setting a hyperparameter and ultimately output the recognized gesture's emotional tendencies.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Experimental Setup",
      "text": "We will conduct experimental verification on the dataset released in this work. Because the goal of this paper is to identify the emotions involved in gestures, we ultimately abstract the task into a three class problem. According to the description in Table  2  and the introduction to the SGED in the previous section, it can be found that the number of neutral and negative samples is close, while the number of positive samples is more than the other two combined. Thus we add class number to weight different class when computing cross entropy loss.\n\nThis point reminds us that when carrying out model evaluation, we cannot simply measure the quality of the model based on accuracy, but should consider the imbalance of the sample.\n\nThe number of dense spike planes K is set to 12. Adam is used to optimize the parameters. The comparison method used is the classic gesture recognition The split of data sets can have different effects on experimental results, so when we first released the dataset, we gave a variety of different methods for splitting the dataset and chose one of them to conduct the experiment. It is worth noting that due to the limitations on the amount of data currently released, the distribution of real data may differ from that of the collected dataset. In the future, this problem can be alleviated by collecting datasets that contain more samples.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Experimental Results",
      "text": "The experimental results on the SGED are shown in the Table  3 . From the perspective of indicators, our proposed method has higher accuracy, weighted recall, and weighted F1 than the baseline method. Specifically, the accuracy of our method is 64.5%, which is higher than the accuracy of DvsGesture that is 63.4%. As the accuracy is computed by the overall samples without considering the imbalance distribution of SGED, more in-depth and detailed analysis is needed. For weighted precision metric, the DvsGesture is better than ours, this phenomenon may caused by the DvsGesture's tendency to predict positive categories with more samples, which can be verified again by the confusion matrix later. Our method takes into account the imbalance of samples, so the performance indicators of the method proposed in this paper are better on indicators weighted recall and weighted F1.\n\nTo more intuitively see the distribution of different categories of predictions, Figure  9  shows the confusion matrix for the results of the two methods. From the confusion matrix, it can be seen that the advantage of the method proposed in this paper comes from the correct prediction of neutral and negative gesture emotions. For positive gesture emotions, our performance is not comparable to the baseline method. This reason is attributed to the uneven distribution of gesture emotion categories. Although we considered the factors of category imbalance in the design process of the loss function, there are still models that tend to output positive gesture emotion results. We believe that this is also one of the challenging features of the dataset released in this paper.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper proposes a homogeneous multimodal gesture emotion recognition dataset that includes event stream data and video frame data. Furthermore, a pseudo dual stream structured network is proposed to provide a benchmark solution. This dataset is a combination of the field of affective computing and the research field of gesture recognition. For affective computing, homogeneous multimodal data can be added to existing multimodal data, enriching data diversity, and leveraging the high dynamic range characteristics of event cameras. For spiking neural networks, the homogenous modal data we provide synchronously can promote the research of the spiking neurons' encoding ability for gesture emotion information. The future research goal can be to improve the fusion mechanism of two homogeneous multimodal data, thereby ultimately improving the accuracy of gesture emotion recognition. In addition, deploying on neuromorphic hardware to achieve low-power solutions is also one of the future research directions  [39] .",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Acknowledgments",
      "text": "This work was supported by the Natural Science Foundation of Shandong Province (No. ZR2021QF145). We would like to thank all those who have contributed to the process of dataset construction. As the first author, I would like to thank my precious sun, Wenshi Wang, for coming to my life.",
      "page_start": 24,
      "page_end": 24
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: to show the dif-",
      "page": 2
    },
    {
      "caption": "Figure 1: The examples of diﬀerent multi-modal data.",
      "page": 3
    },
    {
      "caption": "Figure 2: Distribution of sample number and frame number of video frame data.",
      "page": 12
    },
    {
      "caption": "Figure 2: , it can be found that most of the sample video",
      "page": 12
    },
    {
      "caption": "Figure 3: , the ﬁrst nine cate-",
      "page": 12
    },
    {
      "caption": "Figure 3: Distribution of sample number and labels.",
      "page": 13
    },
    {
      "caption": "Figure 4: , it can be seen that the overall duration of most categories of events is",
      "page": 13
    },
    {
      "caption": "Figure 4: Distribution of events’ time summation of diﬀerent categories (the unite of time is",
      "page": 14
    },
    {
      "caption": "Figure 7: , the ﬁrst column represents one of three video frame sequences with diﬀer-",
      "page": 14
    },
    {
      "caption": "Figure 5: Box diagram of the number of positive events.",
      "page": 15
    },
    {
      "caption": "Figure 6: Box diagram of the number of negative events.",
      "page": 15
    },
    {
      "caption": "Figure 7: Three examples from SGED.",
      "page": 16
    },
    {
      "caption": "Figure 8: The framework of the proposed method",
      "page": 17
    },
    {
      "caption": "Figure 9: (a) The confusion matrix of the results of the baseline method. (b) The confusion",
      "page": 23
    },
    {
      "caption": "Figure 9: shows the confusion matrix for the results of the two methods. From",
      "page": 23
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset Name/\nContributor": "ROSHAMBO17 [14]",
          "Device": "DVS",
          "Category\nnumber": "3",
          "Sample\nnumber": "5 million",
          "Spatial\nresolution": "64×64"
        },
        {
          "Dataset Name/\nContributor": "DvsGesture [18]",
          "Device": "DVS128",
          "Category\nnumber": "10+1(Other)",
          "Sample\nnumber": "1342",
          "Spatial\nresolution": "128×128"
        },
        {
          "Dataset Name/\nContributor": "Neuro ConGD Dataset [15]",
          "Device": "DVS",
          "Category\nnumber": "16+1(blank)",
          "Sample\nnumber": "2040",
          "Spatial\nresolution": "128×128"
        },
        {
          "Dataset Name/\nContributor": "Maro and Benosman [19]",
          "Device": "ATIS",
          "Category\nnumber": "6",
          "Sample\nnumber": "1621",
          "Spatial\nresolution": "None"
        },
        {
          "Dataset Name/\nContributor": "SL-Animals-DVS [16]",
          "Device": "DVS",
          "Category\nnumber": "19",
          "Sample\nnumber": "1100",
          "Spatial\nresolution": "None"
        },
        {
          "Dataset Name/\nContributor": "SGED",
          "Device": "DAVIS346",
          "Category\nnumber": "9+1(Other)",
          "Sample\nnumber": "2500",
          "Spatial\nresolution": "346×260"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "aﬀective\ndisposition": "Neutral",
          "emotional\ngestures": "ok",
          "Gesture details": "The thumb and index ﬁnger form a circle,\nand the remaining ﬁngers naturally open"
        },
        {
          "aﬀective\ndisposition": "",
          "emotional\ngestures": "hello",
          "Gesture details": "Combine the ﬁve ﬁngers, extend the palm,\nand raise the hand to the eyebrow"
        },
        {
          "aﬀective\ndisposition": "Negative",
          "emotional\ngestures": "no",
          "Gesture details": "Shake your head left and right"
        },
        {
          "aﬀective\ndisposition": "",
          "emotional\ngestures": "kill",
          "Gesture details": "The palm slides left and right in front of the neck"
        },
        {
          "aﬀective\ndisposition": "Positive",
          "emotional\ngestures": "victory",
          "Gesture details": "Palm outward,\nextend the index ﬁnger and middle ﬁnger in a ’V’ shape"
        },
        {
          "aﬀective\ndisposition": "",
          "emotional\ngestures": "good",
          "Gesture details": "Thumbs up, other ﬁngers clenched"
        },
        {
          "aﬀective\ndisposition": "",
          "emotional\ngestures": "yes",
          "Gesture details": "Nod up and down"
        },
        {
          "aﬀective\ndisposition": "",
          "emotional\ngestures": "love",
          "Gesture details": "Stretch out the thumb,\nindex ﬁnger, and the little ﬁnger,\nthe palm outward, and the thumb and index ﬁnger are in an ’L’ shape"
        },
        {
          "aﬀective\ndisposition": "",
          "emotional\ngestures": "ﬁghting",
          "Gesture details": "One hand or two hands, clench the ﬁst and raise the hand,\nwith the center of the ﬁst inward,\nthe forearm vertical to the ground, move down slightly, and then stop"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: . From the",
      "data": [
        {
          "Accuracy": "63.4",
          "Weighted\nPrecision": "66.0",
          "Weighted\nRecall": "63.4",
          "Weighted\nF1": "60.8"
        },
        {
          "Accuracy": "64.5",
          "Weighted\nPrecision": "64.1",
          "Weighted\nRecall": "64.5",
          "Weighted\nF1": "64.1"
        }
      ],
      "page": 22
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Non-uniform attention network for multi-modal sentiment analysis",
      "authors": [
        "B Wang",
        "G Dong",
        "Y Zhao",
        "R Li",
        "Q Cao",
        "Y Chao"
      ],
      "year": "2022",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "2",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "3",
      "title": "Disentangled representation learning for cross-modal biometric matching",
      "authors": [
        "H Ning",
        "X Zheng",
        "X Lu",
        "Y Yuan"
      ],
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "A context-supported deep learning framework for multimodal brain imaging classification",
      "authors": [
        "J Jiang",
        "A Fares",
        "S.-H Zhong"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "5",
      "title": "Unsupervised rgbt object tracking with attentional multi-modal feature fusion",
      "authors": [
        "S Li",
        "R Yao",
        "Y Zhou",
        "H Zhu",
        "B Liu",
        "J Zhao",
        "Z Shao"
      ],
      "year": "2023",
      "venue": "Unsupervised rgbt object tracking with attentional multi-modal feature fusion"
    },
    {
      "citation_id": "6",
      "title": "4dme: A spontaneous 4d micro-expression dataset with multimodalities",
      "authors": [
        "X Li",
        "S Cheng",
        "Y Li",
        "M Behzad",
        "J Shen",
        "S Zafeiriou",
        "M Pantic",
        "G Zhao"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Sound active attention framework for remote sensing image captioning",
      "authors": [
        "X Lu",
        "B Wang",
        "X Zheng"
      ],
      "year": "1985",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "8",
      "title": "A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor",
      "authors": [
        "P Lichtsteiner",
        "C Posch",
        "T Delbruck"
      ],
      "year": "2008",
      "venue": "IEEE journal of solid-state circuits"
    },
    {
      "citation_id": "9",
      "title": "A 240× 180 130 db 3 µs latency global shutter spatiotemporal vision sensor",
      "authors": [
        "C Brandli",
        "R Berner",
        "M Yang",
        "S.-C Liu",
        "T Delbruck"
      ],
      "year": "2014",
      "venue": "IEEE Journal of Solid-State Circuits"
    },
    {
      "citation_id": "10",
      "title": "Ultra-high temporal resolution visual reconstruction from a fovea-like spike camera via spiking neuron model",
      "authors": [
        "L Zhu",
        "S Dong",
        "J Li",
        "T Huang",
        "Y Tian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Event-based tone mapping for asynchronous time-based image sensor",
      "authors": [
        "C Simon Chane",
        "S.-H Ieng",
        "C Posch",
        "R Benosman"
      ],
      "year": "2016",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "12",
      "title": "Live demonstration: A 768× 640 pixels 200meps dynamic vision sensor",
      "authors": [
        "M Guo",
        "J Huang",
        "S Chen"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
      "citation_id": "13",
      "title": "The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and slam",
      "authors": [
        "E Mueggler",
        "H Rebecq",
        "G Gallego",
        "T Delbruck",
        "D Scaramuzza"
      ],
      "year": "2017",
      "venue": "The International Journal of Robotics Research"
    },
    {
      "citation_id": "14",
      "title": "Live demonstration: Convolutional neural network driven by dynamic vision sensor playing roshambo",
      "authors": [
        "I.-A Lungu",
        "F Corradi",
        "T Delbrück"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
      "citation_id": "15",
      "title": "Flgr: Fixed length gists representation learning for rnn-hmm hybrid-based neuromorphic continuous gesture recognition",
      "authors": [
        "G Chen",
        "J Chen",
        "M Lienen",
        "J Conradt",
        "F Röhrbein",
        "A Knoll"
      ],
      "year": "2019",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Introduction and analysis of an event-based sign language dataset",
      "authors": [
        "A Vasudevan",
        "P Negri",
        "B Linares-Barranco",
        "T Serrano-Gotarredona"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "17",
      "title": "Scaramuzza, Ced: Color event camera dataset",
      "authors": [
        "C Scheerlinck",
        "H Rebecq",
        "T Stoffregen",
        "N Barnes",
        "R Mahony"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "18",
      "title": "A low power, fully eventbased gesture recognition system",
      "authors": [
        "A Amir",
        "B Taba",
        "D Berg",
        "T Melano",
        "J Mckinstry",
        "C Di Nolfo",
        "T Nayak",
        "A Andreopoulos",
        "G Garreau",
        "M Mendoza"
      ],
      "year": "2017",
      "venue": "A low power, fully eventbased gesture recognition system"
    },
    {
      "citation_id": "19",
      "title": "Event-based gesture recognition with dynamic background suppression using smartphone computational capabilities",
      "authors": [
        "J.-M Maro",
        "S.-H Ieng",
        "R Benosman"
      ],
      "year": "2020",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "20",
      "title": "Event-based feature detection, recognition and classification",
      "authors": [
        "G Cohen"
      ],
      "year": "2015",
      "venue": "Event-based feature detection, recognition and classification"
    },
    {
      "citation_id": "21",
      "title": "Event-based moving object detection and tracking",
      "authors": [
        "A Mitrokhin",
        "C Fermüller",
        "C Parameshwara",
        "Y Aloimonos"
      ],
      "year": "2018",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "22",
      "title": "Temporal binary representation for event-based action recognition",
      "authors": [
        "S Innocenti",
        "F Becattini",
        "F Pernici",
        "A Del",
        "Bimbo"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "23",
      "title": "Event-based asynchronous sparse convolutional networks",
      "authors": [
        "N Messikommer",
        "D Gehrig",
        "A Loquercio",
        "D Scaramuzza"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "24",
      "title": "Hots: a hierarchy of event-based time-surfaces for pattern recognition",
      "authors": [
        "X Lagorce",
        "G Orchard",
        "F Galluppi",
        "B Shi",
        "R Benosman"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Event-based gesture recognition through a hierarchy of time-surfaces for fpga",
      "authors": [
        "R Tapiador-Morales",
        "J.-M Maro",
        "A Jimenez-Fernandez",
        "G Jimenez-Moreno",
        "R Benosman",
        "A Linares-Barranco"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "26",
      "title": "Hand-gesture recognition based on emg and event-based camera sensor fusion: A benchmark in neuromorphic computing",
      "authors": [
        "E Ceolini",
        "C Frenkel",
        "S Shrestha",
        "G Taverni",
        "L Khacef",
        "M Payvand",
        "E Donati"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "27",
      "title": "Low latency event-based filtering and feature extraction for dynamic vision sensors in real-time fpga applications",
      "authors": [
        "A Linares-Barranco",
        "F Perez-Pena",
        "D Moeys",
        "F Gomez-Rodriguez",
        "G Jimenez-Moreno",
        "S.-C Liu",
        "T Delbruck"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "28",
      "title": "Neuromorphic event-based slip detection and suppression in robotic grasping and manipulation",
      "authors": [
        "R Muthusamy",
        "X Huang",
        "Y Zweiri",
        "L Seneviratne",
        "D Gan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "29",
      "title": "Event-based action recognition using motion information and spiking neural networks",
      "authors": [
        "Q Liu",
        "D Xing",
        "H Tang",
        "D Ma",
        "G Pan"
      ],
      "year": "2021",
      "venue": "IJCAI"
    },
    {
      "citation_id": "30",
      "title": "Hybrid snn-ann: energyefficient classification and object detection for event-based vision",
      "authors": [
        "A Kugele",
        "T Pfeil",
        "M Pfeiffer",
        "E Chicca"
      ],
      "year": "2021",
      "venue": "Pattern Recognition: 43rd DAGM German Conference, DAGM GCPR 2021"
    },
    {
      "citation_id": "31",
      "title": "Spiking transformers for event-based single object tracking",
      "authors": [
        "J Zhang",
        "B Dong",
        "H Zhang",
        "J Ding",
        "F Heide",
        "B Yin",
        "X Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "A low power, high throughput, fully event-based stereo system",
      "authors": [
        "A Andreopoulos",
        "H Kashyap",
        "T Nayak",
        "A Amir",
        "M Flickner"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "Echo state graph neural networks with analogue random resistive memory arrays",
      "authors": [
        "S Wang",
        "Y Li",
        "D Wang",
        "W Zhang",
        "X Chen",
        "D Dong",
        "S Wang",
        "X Zhang",
        "P Lin",
        "C Gallicchio"
      ],
      "year": "2023",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Eventbased vision meets deep learning on steering prediction for self-driving cars",
      "authors": [
        "A Maqueda",
        "A Loquercio",
        "G Gallego",
        "N García",
        "D Scaramuzza"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Event-based neuromorphic vision for autonomous driving: A paradigm shift for bio-inspired visual sensing and perception",
      "authors": [
        "G Chen",
        "H Cao",
        "J Conradt",
        "H Tang",
        "F Rohrbein",
        "A Knoll"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "36",
      "title": "Real-time speaker identification using the aerear2 event-based silicon cochlea",
      "authors": [
        "C.-H Li",
        "T Delbruck",
        "S.-C Liu"
      ],
      "year": "2012",
      "venue": "IEEE international symposium on circuits and systems (ISCAS)"
    },
    {
      "citation_id": "37",
      "title": "High-efficiency low-power microdefect detection in photovoltaic cells via a field programmable gate array-accelerated dual-flow network",
      "authors": [
        "H Wang",
        "H Chen",
        "B Wang",
        "Y Jin",
        "G Li",
        "Y Kan"
      ],
      "year": "2022",
      "venue": "Applied Energy"
    },
    {
      "citation_id": "38",
      "title": "Spiking emotions: Dynamic vision emotion recognition using spiking neural networks",
      "authors": [
        "B Wang",
        "G Dong",
        "Y Zhao",
        "R Li",
        "H Yang",
        "W Yin",
        "L Liang"
      ],
      "year": "2022",
      "venue": "Spiking emotions: Dynamic vision emotion recognition using spiking neural networks"
    },
    {
      "citation_id": "39",
      "title": "Loihi: A neuromorphic manycore processor with on-chip learning",
      "authors": [
        "M Davies",
        "N Srinivasa",
        "T.-H Lin",
        "G Chinya",
        "Y Cao",
        "S Choday",
        "G Dimou",
        "P Joshi",
        "N Imam",
        "S Jain"
      ],
      "year": "2018",
      "venue": "Ieee Micro"
    }
  ]
}